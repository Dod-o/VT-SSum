{
    "id": "sr3xaed65iomqpfitn4xmxdgtj2lzcdm",
    "title": "Combinatorial prediction games",
    "info": {
        "author": [
            "Nicol\u00f2 Cesa-Bianchi, University of Milan"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Discrete Optimization",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_cesa_bianchi_games/",
    "segmentation": [
        [
            "Then to the organizers for inviting me here.",
            "Thanks to everybody for sticking around until so late.",
            "So this is not going to be a submodular talk.",
            "And I don't know whether this is good or bad, but OK, this is a list of contributors that I was fortunate enough to work with this SMB back.",
            "Shame Kakada Gabor logo.",
            "She and I just realized I forgot to Geneva Diebert He's not here around so it's OK.",
            "It's good though."
        ],
        [
            "Forgot.",
            "Alright, so let me start with an example where of just so you understand what I'm talking about, so this is a combinatorial sequential prediction game, so you want to.",
            "You want to predict certain combinatorial structure in a class in order to minimize a certain notion of costs across time.",
            "So this is an example in which you want to keep a low cost spanning tree on a graph, and this games.",
            "This prediction games are sequential, so they proceed in the steps and at every time step.",
            "Given this graph which you know."
        ],
        [
            "There is some adversary that hidden from the player who is trying to predict the communal structures, puts some hidden costs on the edges of this graph.",
            "So without seeing this car."
        ],
        [
            "Also, the player chooses a tree.",
            "Because his job is to maintain a good spanning tree or over this graph, and after the tree."
        ],
        [
            "The trees chosen, the player suffer a loss which is linear in the edges that make up the tree.",
            "So in this case, if you sum up the costs of the edges, you should get 22.",
            "If I'm not mistaken, until yesterday was 17, but then I realized is that 22 OK, OK, and this game goes on and on.",
            "So at the next step at the adversary will put the different edge costs on the edges again hidden from the player the player.",
            "On the just by knowing what he lost on the previous trials will choose a different spanning tree and will suffer a new cost.",
            "OK, in the."
        ],
        [
            "So let me make things more precise.",
            "Now I want to specify what are, what is it?",
            "Is that the player is observing and what is the the criterion that the player is trying to fulfill in this game.",
            "So first of all, there are three nice feedback models that can be started in this sequential combinatorial games.",
            "First of all is the full information criterion.",
            "After choosing the tree, the player gets to see all the edge costs.",
            "OK."
        ],
        [
            "And the second model is Semi Bandit which was also mentioned in Spotlight Talk before today is when the player only seats only gets to see the costs on the edges that belong to the city that he has chosen.",
            "But he has information of the cost of every single edge that is part."
        ],
        [
            "The tree and the third model is bandit model and this is the hardest case in which the the player has a feedback after each.",
            "Each round of the game only gets to see the other cost of the tree as chosen so that the sum of the customer judges, but it doesn't see the cost.",
            "An individual edges, just overall just the overall cost.",
            "And so if you see the information is very different than the three models you see, the whole entire cost of all the edges, just the cost of the edges were chosen, or just the number which is the sum of those costs."
        ],
        [
            "OK, now to be a little little bit more formal.",
            "In general you can play that.",
            "You can play this game over trees, but over any other class of combinatorial structures.",
            "And that's why we call these communitarian prediction games.",
            "So in a more general setting you will have a Class S of combinatorial structures, for instance spanning trees of a given graph.",
            "So again, the adversary chooses loss vector, which is the previous case was where the cost associated to all the edges of the graph.",
            "So they were going to be the number of the edges in our graph.",
            "In the previous example.",
            "Now the player chooses an action which is an element, for instance spanning tree.",
            "In the previous example.",
            "And then gets to see the either the loss vector, entire loss vector and full information feedback model or the partial loss vector of those element that belongs to the.",
            "Action has chosen in the semi bandit case or just the number which is the sum of the loss components associated to the agency's chosen.",
            "So I should say here that it, but you should have guessed it by now, 'cause you've seen examples.",
            "I'm representing actions with with incidence vectors so that it is natural to represent the cost associated to an action as a dinner product between the loss vector.",
            "And the incidence vector that represents the action in the set.",
            "So again, I represent I represent the spanning tree as the incident vector of the edges that belong to the spanning tree.",
            "Given all the edges of the graph, OK.",
            "So in the I can represent with incident vectors.",
            "So we represent class of actions as classes of incident vectors over the Boolean hypercube.",
            "You mentioned E OK and what's important here is that the loss of each action attende is linear over the incidence vector.",
            "As I said before, so again, the loss of a tree.",
            "This is not working and more the loss of a tree is the sum of its edge costs, and I'm assuming also a scaling scaling assumption such that the bees inner products are at most one.",
            "For all the actions in my set, OK.",
            "So."
        ],
        [
            "Now what I'm supposed to do here?",
            "What is the player?",
            "What is the goal of the player in this game?",
            "So in a if you play this game over and over, choosing again over and over sets.",
            "Action from the action set and incurring losses, then the player over a sequence of choices in the action set will incur up to time TA certain cumulative loss, which is the sum of these inner products between the actually have chosen and the loss vectors.",
            "This is irrespective to the feedback model in all the three feedback models.",
            "And the goal, given a certain sequence of choices of the adversary, is to minimize the difference between the cumulative loss of the player and the cumulative loss of the best fixed action in the action set.",
            "So this is the cumulative loss that a player would have suffered if he had chosen consistently the single best action isn't set.",
            "So for instance, the single best spanning tree.",
            "On the graph for the specific sequence of losses.",
            "So of course this is something this minimization cannot be done in this model, because the smaller is sequential, so the even in the full information model, the player doesn't get to see in advance all the sequence of loss vectors, But the sequence of loss vectors is revealed incrementally to the player, so I would like to control this quantity in all the three feedback models.",
            "OK, and to bound it as a function of T."
        ],
        [
            "Because I have this scaling assumption."
        ],
        [
            "And that it's it's sensible to require that I would like this regret that to grow sublinearly overtime.",
            "So that because it's trivial I mean, your instantaneous regret can be sort of a constant is at most one on each time step OK?",
            "Because I have this assumption here.",
            "So I'm I'm.",
            "I'm happy if I'm able to prove that this difference will grow sublinearly overtime.",
            "OK, so this."
        ],
        [
            "This is the Philippines, studied a lot, and so I'm going to serve a number of results for the different feedback models, and I'm going to sketch a couple of algorithms have been proposed, and then at the end of the talk I'm going to give you some more recent results that concerning the combinatorial bandit feedback model.",
            "So this is a template for players that in this case we're going to be randomized because we are in an adversarial.",
            "Prediction models so players should be randomized in order to have a chance to stand a chance against the versary.",
            "So the player General Player will maintain a set of weights on the coordinates of the of the game.",
            "OK, so for instance set of weights over the edges of the graph.",
            "In the case of the spanning tree.",
            "And then that will obtain it from this set of weights at distribution over the actions.",
            "OK, we see ways of obtaining a distribution over the action set from weight assignment over the coordinates, and then will draw an action from the distribution of the action set.",
            "And this will be the action played and then it will occur depending on the feedback model, it will have to come up with an estimate of the actual.",
            "Loss vector, which in the semi bandit and bandit model is not observed, is only observed in the full information model and it will use this estimate of the loss vector in order to update the assignment of weights to the coordinates and this will allow him to come up with a different distribution for the next round of the game.",
            "So this will go over and over.",
            "So now in typically will have a different unbiased loss estimates for the weights.",
            "So this game is completely deterministic.",
            "The adversary we don't have any assumption on the way the adversary is generating the loss vectors, so the.",
            "Player is using its own internal randomization to come up with the estimates that will be unbiased in the expectation with respect to the players on randomization.",
            "OK, so in the case of full information, there's nothing to estimate.",
            "We do observe the entire loss vector, so the estimate concise with the loss vector which we get to see.",
            "So we get to see the cost of all the edges of our graphing cases spanning trees in the semi bandit case we only get to see the last component of the elements.",
            "All the action sets of, let's say, of the edges that we make part of our spanning tree in the in the spanning tree case.",
            "So the components of the incident vector that are one in general and we normalize this by the weighted succeeded with the coordinate.",
            "So this this is a statistic because we can compute it in the sample space of the players.",
            "Randomization OK and in the bandit remember them.",
            "And if we just get to see this scalar quantity here, which is the projection of the last vector onto the action that we have chosen, and we use a sort of a least square estimate.",
            "Where this PT is the pseudo inverse of the correlation matrix of the actions under the sampling distribution that we use the time T OK so this looks very much like a sort of at least square estimate, and this turns out to be a good thing to use in the bandit case.",
            "OK, so we have unbiased estimate for our losses and we have a general way of playing this game.",
            "So now we want to see how to.",
            "Go about computing these weights and doing the updates.",
            "The updates OK, so please interrupt me if you have any questions.",
            "Yes, is there.",
            "If you go back to the last slide there is this estimation of the most vector you suddenly used PT which you can choose in many ways.",
            "Would there be an advantage to choose meat in that particular way?",
            "Yeah, OK, I will give you 2 algorithms that specify both.",
            "Joint way of choosing PT NWT and, of course, this estimate will make sense for specific choices of PT OK."
        ],
        [
            "OK, so let's see one way.",
            "One way of playing this game and.",
            "Is the so called expanded hedge algorithm, which is the.",
            "Generalization to combinatorial sequential prediction problems of the hedge algorithms.",
            "That, or the experts algorithm, that using exponential weights.",
            "So the idea here is that it is very simple.",
            "So you remember we had weights over coordinates.",
            "This was the main element of our our playing strategy and this capital L had the cumulative loss estimates for each coordinate.",
            "So essentially we penalize the coordinate exponentially according to the cumulative.",
            "Estimated loss suffered by that coordinate.",
            "So in the case of the full information feedback, we do know how much specific edge of our graph has the overall cost of a specific edge of our graph.",
            "Because we can see it in the summer information model, we only we only saw the only so the cost of a single Ledger over all the times that we chose that edge in our as part of our action and in the bandit case we just have some.",
            "We don't have a way of serving this directly.",
            "OK, and then we can use this to create a weight assignment over actions to watch for two actions of our action sets by again taking exponential weighting where the weight given to an action is the projection of our of the cumulative loss.",
            "I estimate onto the action vector the incidence vector of the action OK and right.",
            "So now in we can play this.",
            "We can normalize this distribution and use this distribution to pick our actions and we can plug in our estimates for the let's say for the full information over the semi bandit information case in the bandit case we have to be a little bit.",
            "More clever and that this algorithm spender data is known as dramatic hate hedge and in the since the variance of this loss estimates would be much higher in the bandit case because we get much less feedback.",
            "You remember we only get to see this color information and which is the overall cost of our action.",
            "So we have to be more careful.",
            "And instead of playing the normalized playing.",
            "Using the probability given by the normalized wait OK, we played we play a probability which is a mix of the normalized weight and then another distribution which is helpful in order to control the variance of the last estimator.",
            "So we need some additional variance control in the feedback model where we get very little feedback OK.",
            "So this essentially is a way of."
        ],
        [
            "Using this player because we know how to compute P and we have a way of expressing that."
        ],
        [
            "View in terms of sums of lost estimates so we can run this player an gets results for using the previous last estimates for the Old Tree bandit models OK. Now the second algorithm."
        ],
        [
            "It has been proposed is a variant of mirror dissent.",
            "So this is a completely different idea and the idea is that you don't.",
            "You don't compute a distribution over, you know directly, maintain a distribution over the actions, but what you maintain is a point inside the convex Hull of the action set, and then you do a gradient descent or a mirror dissent inside the convex Hull of the action set using a specific potential function.",
            "So a potential function is a parameter of this of the mirror descent algorithm and helps you too.",
            "Specially Peps who gives you a way to control the variance.",
            "Again, it gives you a way to control the variance of the estimates and to any potential.",
            "We can also set a Bregman divergences, which is a measure of distance, and for instance if the if the potential function which is always destructive potential strictly convex function is the squared loss is A squared Euclidean norm, then the Bregman divergences will be just squared Euclidean distance.",
            "If you have any other.",
            "Is strictly convex.",
            "A potential function?",
            "The Bregman divergences will be suitable generalization of the squared Euclidean distance.",
            "So in general this this is a distance which doesn't obey the triangle inequality.",
            "A may not be even symmetric.",
            "OK."
        ],
        [
            "So now this is a nice slide that I borrowed from Sebastian.",
            "I'm not so good at drawing slicing later, so this is a sketch to explain how this online stochastic mirror dissent works.",
            "So you have a wait.",
            "And now this weight is is a point inside the convex Hull of your action set.",
            "And here is the simplex over the action set.",
            "So this is where your probability distribution over actions will be elements of this space and your weights will be elements of this space, and then you define the potential function over some domain which contains the convex Hull of your action set.",
            "So now what you do?"
        ],
        [
            "It's at any point that you have some probability distribution of their actions which you use to draw your action at time T. This chorus."
        ],
        [
            "Points to a point in the convex Hull of your action set, and once you get your feedback."
        ],
        [
            "OK, you will use your loss estimate depending on the feedback model to update during a mirror dissent gradient step to update your weight and this is the update rule for the mirror dissent, which is the gradient of the potential function.",
            "I'm not going into details, so this is a standard mirror dissent strategies, which is the most common strategy for online convex optimization, so this update might take you out of the.",
            "Comic sites, so you do."
        ],
        [
            "Projection step using the Bregman divergences which takes you back inside the convex Hull, and then at this new weight."
        ],
        [
            "Will give you a new probabilities by decomposing this weight onto a sum onto.",
            "By representing this weight as a convex combination of the sum of the point or some of the vertices of this polytope, you will get the point.",
            "PT in the simplex, so this will be a distribution over your the corners of this polytope, which are your actions OK?",
            "This is a convex Hull over the action set, yes.",
            "This is not the unique way.",
            "Now to define PT.",
            "There is no unique way, but this is the way I choose for this algorithm, and so PT is chosen so to satisfy this condition here, which is what we need.",
            "And there is a I mean.",
            "There is no unique you may you may represent the PT in different way given this is what but I want PT to satisfy this equation."
        ],
        [
            "So now as Francis was observing, yes, we want to express a point inside the convex Hull of our action set as a distribution over the actions, and then we need also to draw an action according to this distribution.",
            "So character dude category theorem ensures that we can find.",
            "A point in the simplex which has a small support over which is supported by a small number of actions, and this would also solve the sampling problem because in that case it would be easy to sample an action which is supported by sample and action according to a distribution which is supported by a small number of actions.",
            "However, computing this set is generally intractable, and this is the reason why this algorithm is in general.",
            "Intractable.",
            "And however, in many interesting, specific interesting cases, for instance, if the convex Hull of our action set as a polynomial number of faces, then we can find the in polynomial time distribution with that.",
            "Satisfying that condition, which is supported by a small number of actions so we can draw easily from that one.",
            "So this is for instance, a way with which you can use to.",
            "That we have a way so that you can use this algorithm for.",
            "For instance, for drawing paths in a graph.",
            "OK, because you can represent using the Max flow decomposition, you can represent a path using a small number of.",
            "Edges in your graph.",
            "OK, sorry you can represent.",
            "You can represent a distribution over.",
            "You can represent an L."
        ],
        [
            "I meant in your convex."
        ],
        [
            "As a distribution over a small number of paths in your graph, and this is how you can.",
            "Draw a path in polynomial time."
        ],
        [
            "OK, so these two algorithms have various instances that very special cases and very instances that have been proposed in the past.",
            "So as I mentioned, the hedge algorithm.",
            "So this is for playing actions that are not.",
            "Notorial elements of a combinatorial space, so by classical case I mean the case in which you are.",
            "Your actions are the.",
            "Orphans of the space.",
            "So you just have a number of actions equal to the number of dimensions in your space.",
            "And each action is there is a coordinating your space.",
            "And so this is the you have the the hedge algorithm and then you have the X3 algorithm, the bandit case and the algorithm.",
            "Also in the bandit case that have been proposed and also in the past.",
            "So these are all instances of the expanded edge algorithm.",
            "That was mentioned before, OK. And then we also have special cases of the mirror descent algorithm.",
            "For instance the algorithm that has been proposed three years ago that uses self concordant functions and a slightly different way of estimating losses loss vectors.",
            "As in order to direct the."
        ],
        [
            "Dissent, the gradient descent inside the convex Hull of actions.",
            "OK, so there's a."
        ],
        [
            "While the there's a wide range of special cases and.",
            "This there are two main families of algorithms, so one is this."
        ],
        [
            "Mirror dissent, in which you make this gradient descent inside the convex Hull, and then you decompose and then this is a different exam."
        ],
        [
            "Which is the expanded edge in which you more explicitly maintain a distribution over the corners of this action set, and the distribution is characterized by exponential weights.",
            "This expanded edge algorithm can be viewed as a as a sort of in a strange way as an instance of mirror dissent.",
            "But this is more convenient to view it as a completely separate strategy.",
            "OK.",
            "So."
        ],
        [
            "Right, so this is a nice table summarizing the some known regret bounds.",
            "OK, so you remember this is the.",
            "This is the difference between the cumulative cost to suffered by the player and the cumulative cost of the single best actions in your action set.",
            "After T plays OK, so here you don't.",
            "You don't see the size of the action set, which I'm assuming to be exponential in the number in the dimension of the problem.",
            "For instance, typically have an exponential number of spanning trees in the graph, so this is not.",
            "This is just going to affect the bounds by.",
            "Constant factors becausw.",
            "Just by constant factors.",
            "So it's just important that it's exponential indeed.",
            "So you see here that refeed that models so we have the full feedback models in which you fully observe the loss vector at each round, and then you see here that the two algorithms expanded edge and the online mirror dissent achieve the same regret bound.",
            "You see, it's a sub linear in time is it grows with the square root of time and it depends.",
            "Also, the square root of the number of dimensions in the problem.",
            "So this is irrespective to the specific shape of the action set and irrespective to the way the adversary is giving the loss vectors OK and there is a matching lower bound that shows that this is essentially the best you can get.",
            "OK, in the semi bandit case where you were the player observes the costs of the let's say the edges of the graph that belong to a spanning tree in the spanning tree case of, for instance, the edges of the graph that belong to the path in the graph.",
            "If it is a routing problem then you then these arguments have different regrets.",
            "So expanded edge depends linearly in the dimension.",
            "And the square root of time and online online mirror dissent has for a specific choice of the of the potential function and has a better regret bound which is matching the lower bound and is also matching the full information cases.",
            "So it's essentially this says that in the semi bandit the cost of estimating the missing information is.",
            "Not affecting the regret.",
            "OK, in this adversarial model.",
            "So the interesting situation is in the in the bandit case, we just where the player just observes a scalar, which is the cost of the action is chosen, and here there is a lower bound of this form in the.",
            "The two algorithms are not directly compareable.",
            "Expanded Edge has bound which as this form here.",
            "In the D online mirror dissent as abound of this form, where this bound is gotten using the self concordance at Concorde and potential function, which depends on a parameter Theta and in general Theta is can be order of the dimension of the problem, so that essentially in general these two bounds will have the same order of magnitude, so there's a.",
            "So you see, there's a gap here, and I'm in the second part of this talk I'm going to.",
            "Address a way to delete it to close this gap by showing a different different implementation of expanded hedge of this pended edge algorithm OK."
        ],
        [
            "So right so let me now focus on the on the bandit feedback model and focus on the expanded add edge are going for bandits, which is called the geometric hedge.",
            "So again, I'm maintaining.",
            "I'm in a full band in the bandit model, so I just observed this scalar here and I'm taking the algorithm is maintaining this last estimate OK?",
            "Where again at this PT is the.",
            "Expectation is the correlation matrix of the actions under the sampling distribution PT.",
            "OK, I'm maintaining the weights and just repeating what I wrote in a previous light and maintaining the weights over the coordinates in my using composing these weights by making the product of the coordinates that belong to a specific action to come up with the distribution of actions.",
            "And then I am a sampling from this distribution and I'm using this correction which is mixing, mixing exploracion distribution over the actions, which is good too.",
            "Reduce the variance so I haven't told you how to choose this exploration distribution at Angle to address the question now, and I also would like to point out the fact that this algorithm is also not in general is not efficient.",
            "Just like online mirror dissent, becausw, you have the problem of sampling and action over an exponential set of over an exponential set over S. So that you have the problem with somebody or something and action according to this distribution is potentially intractable, OK?",
            "So we will discuss the special cases in which this is possible.",
            "OK, so right."
        ],
        [
            "There is a general bound for this algorithm which is parameterized with respect to the exploration distribution.",
            "OK, so mu here."
        ],
        [
            "Will be this exploration distribution which is the mixing term in the sampling distribution which is helps to control the variance of these estimates in every coordinate in every direction.",
            "This will be the hard part to get a better bound for the bandit case.",
            "OK, good."
        ],
        [
            "So now there is a general regret bound that depends.",
            "You see, there's a square.",
            "There is the number of dimension here.",
            "There's this square root of time, and then there is this term here, which is the key term which depends on the size of the radius of your action set.",
            "In the Euclid inclusion rate radius of your action set, and the smallest eigenvalue of the correlation matrix of your action set under the.",
            "Exploration distribution.",
            "OK, so this tells you somehow that you would like to.",
            "You know you would like the spectrum of this matrix here to be as uniform as possible, so you don't want to have a skewered.",
            "Metrics here.",
            "OK, so in non uniform spectrum and the reason why you get this term here in the regret is because this term here this ratio is proportional to the variance of your last estimate.",
            "OK, so this is the term responsible."
        ],
        [
            "Or the variance in this loss estimate here?",
            "OK."
        ],
        [
            "So this is the price that the price that you have to pay in order to in order to.",
            "A compensate the fact that we are not observing enough information.",
            "OK, so the amazing thing you can still get the square root of T dependence.",
            "So now the question is I should come up with an exploration distribution that on my action set.",
            "So this is the random variable that varies over the action set.",
            "Give me a spectrum set of eigenvalues as uniform as possible.",
            "OK, indeed, when we are, whenever we are able to prove this so that this ratio becomes a constant, then we get is bounded.",
            "This square root of T which is the which matches the lower bound.",
            "So whenever this is true, which means that the action space.",
            "OK, so now what is an easy exploration distribution?",
            "OK, let's try.",
            "We can just explore it uniformly, so basically."
        ],
        [
            "We now we our sampling distribution will have two parts.",
            "This part which is the exploitation part which focuses on the actions on the actions that contains many coordinates that suffered suffered little loss in the past, at least estimated loss.",
            "And then we add a mixing distribution exploration distribution which is uniform over the actual space.",
            "OK, this is very simple, so we tried this."
        ],
        [
            "And what this means?",
            "OK, since this distribution is uniform, it means that if the actual space is essentially isotropic.",
            "So the distribution is uniform, and if the actual spaces isotropic, I hope that the eigenvalues will be of these metrics will be more or less all the same."
        ],
        [
            "So, so there are lots of interesting cases of structures where this you get to this effect.",
            "So these are this is for instance you see here.",
            "Permutation spanning trees cuts of a click Hamiltonian, cycles, subsets of a set of the elements, so these are all classes of actions, tutorial actions, in which you can go and compute the minimum eigenvalue."
        ],
        [
            "Of this matrix for the uniform distribution."
        ],
        [
            "And this turns out to be just the right magnitude.",
            "In order to achieve the."
        ],
        [
            "Optimal bound.",
            "This bound here so becausw."
        ],
        [
            "You when you plug in the quantities the relevant quantities here."
        ],
        [
            "You get exactly this dependence."
        ],
        [
            "So in those cases we are lucky."
        ],
        [
            "And then you can go about the sampling problem and right in some cases by using a darker algorithms developed in the literature, you can prove that you can efficiently sample actions from your sampling distribution PT in polynomial time by using."
        ],
        [
            "Random Walk on the corners of the."
        ],
        [
            "On the corners of the hypercube.",
            "OK, so in some cases this algorithm can be efficient and optimal."
        ],
        [
            "OK, so there is some bad cases, for instance that the routing problem is a bad case.",
            "So you see here this is agreed that this is your graph and you want to route from this source to this destination.",
            "So these are all possible paths you can take.",
            "And of course some edges will be contained just in a very few paths, whereas some edges other edges will contain an exponential number of paths.",
            "So now if you take a uniform exploration over this grid.",
            "It means that those edges will be sampled much more often, so your estimate your last estimates will be bad.",
            "We have a high variance on those edges, so this is a typical case in which uniform exploration doesn't work, there's an."
        ],
        [
            "Nice trick."
        ],
        [
            "That you can use to control your variance and to address these specific."
        ],
        [
            "Problems, let's see what's the trick.",
            "So remember, the actions are incidence vectors of the corners of the Boolean after cube, and you have a problem when the actual set is cured is not isotropic like in this case."
        ],
        [
            "OK, because we have like a marginal actions over here, these paths are marginal."
        ],
        [
            "So now you want to get a little help by changing the way represent the problem.",
            "So this is a linear.",
            "This is a linear problem, so you can.",
            "You can change the basis in which you represent your loss estimates.",
            "OK, so you would like to choose a base of RDA under which the actual set looks more isotropic and then you would like to run this geometric algorithm with loss estimates, computed this basis, and then you hope that this.",
            "Can help to cure the problem of skewered action sets."
        ],
        [
            "So one way which was proposed in the past and gets you so this gets you the."
        ],
        [
            "And over here, which is not optimal, because there is a gap, is through the use of various."
        ],
        [
            "Centric spanners this is a very nice idea so very centric.",
            "Spanners are actually a subset."
        ],
        [
            "Of actions in your action set and if your action sets action, set span spans Rd, you will find the D actions and the actions will spend the whole action set, so will be a basis.",
            "It won't be necessarily an orthogonal basis for your action set, but The thing is that in the in the coordinate of this basis all actions will have a small coordinates, so specifically."
        ],
        [
            "And exploration barycentric spanners is such that all actions will have in the basis of debris centric spanners coordinates between minus one and one.",
            "OK, so there is always existed very center spanners for any action set.",
            "This is a very nice proof due to Kleinberg and the other book.",
            "And now the trick is that you do exploration over uniformly over the spanners.",
            "And this buys you two things.",
            "First of all gives you mix this metric size of Tropic.",
            "So Lambda mean is 1 / D, which is the best you can hope and then you get because you've chosen a specific and orthogonal basis.",
            "The norm of your actions, you square the clear norm of your actions will be D. So now if you go and read out it bound, you get what I showed you before D2 to three apps.",
            "So this is a this gives idea.",
            "Of using a different ticket along along the same lines."
        ],
        [
            "So this is the last thing I'm showing here, so now a better basis for your exploration is given by the learner John Ellipsoid.",
            "So this is again your action set.",
            "These are not corners of the applicable, but of course it's nicer to draw."
        ],
        [
            "And you take the smallest volume enclosing ellipsoid of your action set, and now your new coordinate system will be given by the axis of this ellipsoid.",
            "OK, and your exploration is will be given by the contact points of the Ellipse ellipse ellipsoid with your action set.",
            "OK, now there is a very beautiful."
        ],
        [
            "Erm, result, John's theorem in convex geometry that tells you that if you do that, so if you do exploration on these points, but this position is not uniform over these points and just dinner and gives you the right coefficients, the right convex coefficients.",
            "So specify your desperation distribution, then the this term, which is the one that.",
            "Rules for the variance of estimates will be such that the smallest eigenvalues 1, /, D and.",
            "So the new system of coordinate is such that is very easy, so you re scale the axis so that this ellipsoid looks like the unit ball.",
            "OK, so in the new in the in this new in this news basis where the ellipsoid looks like the unit ball then this thing will be azeotropic.",
            "The action set will be the topic in the sense that this will hold the the minimum value of these metrics will be 1 / D. And since now the actions are in the unit ball, the length of every actions will action will be at most one.",
            "So this is smaller equal in one.",
            "And Furthermore category theorem will ensure that the contact points we will be at most this squared order of this squared, so you're at least your exploration distribution will be supported over a small number of points.",
            "OK, so computing the lower learner ellipsoid is not efficient in all cases, but in some in some cases it can be done efficiently in some other in most cases can be approximated efficiently and you will paint the regret the term due to the explosive nation.",
            "So now you see you've got an isotropic action set using this trick and the B squared.",
            "The Oriole action will have length bounded by one and you can prove that the game hasn't changed it.",
            "So this is this transformation is not changing the game if you do it properly.",
            "And now you're if you put this in your regret bound, you get the optimal regret.",
            "OK, so this is a very nice result.",
            "Basic result economist geometry that delivers very easily given the all the previous machinery.",
            "The optimal bound by giving you a way of optimally exploring the action set.",
            "So in order to keep all the variances controlled.",
            "Under control OK."
        ],
        [
            "OK, so this is the new picture, so you see now that there is a nice upper upper lower matching upper matching upper or lower bounds for all the feedback models.",
            "OK, however."
        ],
        [
            "I'm done.",
            "Still lots of interesting things to do, so let me just wrap up by saying that we look at that communal sequential prediction problems with linear loss in three feedback models.",
            "Full information, semi bandit and bandit.",
            "And we've seen two algorithms expanded edge and online mirror dissent.",
            "These are two fundamentally different algorithm.",
            "One is derived from the exponential weighting algorithm or the hedge algorithm for the experts model and the other one is really an interior method that does gradient descent in the comics.",
            "Correction set and keep problem, especially in the benefit back is estimating the loss in each coordinate while keeping the variance under control and we shown away with expanded edge.",
            "So to achieve this control of the variance by discovering the action set using the basis.",
            "This is provided by the lunar ellipsoid.",
            "So you re scale this ellipsoid then you get a good a good basis for your estimates.",
            "What's still open is.",
            "Really too.",
            "OK, first of all, this slow."
        ],
        [
            "We're bound.",
            "This lower bound is not is a general lower bound, so it might be very, and it is the case for specific action sets.",
            "You get better bounds better bounds than this, so this bound is not uniformly optimal.",
            "Overall action sets OK, so and you see this bound doesn't count is very poor in terms of doesn't contain information about the topology of the action set, so your lunar rescaling as wipe it away all the topology by making looking you know by making the actual set look like.",
            "The ball completely isotropic, so you are matching the you're matching the lower bound, but it might be the case that by retaining this topological information and then using a more clever way to control your variance, you may be able to get a bound which contains some information about the structure of the action sheet and can be better than this in for specific action sets and so."
        ],
        [
            "It could be.",
            "This could be a clever way of design potential function in using mirror dissent in order to prove optimal regret bounds that contain this more detailed information.",
            "And then of course there is still an open problem of finding general optimal efficient strategy that is allows you to run even in the bandit model.",
            "This algorithm and efficient way.",
            "OK, I think I'm done.",
            "Thank you for your attention.",
            "Definition goods rose in spanning trees, so I want you to being submitted ality without knowing it could be very well the case.",
            "Yes yes yes yes yes.",
            "I just have to be enlightened.",
            "No, no no.",
            "It's by I mean no.",
            "No kidding this this yeah I mean sitting here throughout the day.",
            "You know of course.",
            "I I should study these things better?",
            "I mean start OK, so you assume that the loss the losses were bounded.",
            "Yeah, on the full action.",
            "So if you if you do a different trick if let's say in the case of the path you bound the loss of each edge, yeah, then very often there's differences.",
            "Even the full information game between right the expanded hedge right?",
            "And the component head right extra extremely right?",
            "Yeah, yeah yeah.",
            "And it's very baffling because you do not know.",
            "Which algorithm is better?",
            "One always better.",
            "It's just yeah so.",
            "So Manfred Manfred is reminding me that there are different ways of.",
            "Scaling or making skinning assumption in your game.",
            "So you may assume instead of.",
            "I'm assuming that the inner product between the loss vector and any incident vector of your action set is at most one, but you may very well assume that you have a pair of dual norms and you say that the normal of the Los Vectores bus.",
            "This is bounded by this and the norm of the action.",
            "The dual norm of the action vectors is bounded by that.",
            "OK, so this is a different assumption and this gives you.",
            "Different results, I think Sebastian is the right person to really answer all the defined details to that.",
            "I'm focusing on this specific way of imposing a scaling assumption of the problem, but this is not the only way.",
            "Can you comment on the connection between this way of.",
            "Keeping the variance under control and what's called a Dickey Nordyke ellipsoid right are using it with the action spaces.",
            "There any connection you see me?",
            "Note, taking ellipsoid is local is a local basis, so this is this is a global basis that holds for the entire action set.",
            "The Dikin ellipsoid which is used in the online mirror.",
            "Dissent with self concordant potential functions is gives you a local coordinate system that helps you to control the variance.",
            "In the vicinity of your current point, inside of the convex Hull.",
            "Yeah, I was wondering whether.",
            "It's it's a different algorithm, so it's it's it's.",
            "Um?",
            "No, I know I don't again.",
            "You're saying OK, that diking knowledge side is the loner ellipsoid for is no, but it is not really becausw there.",
            "You're based on a different intuition.",
            "There you are.",
            "Really, are you really?",
            "I mean, I view this as a different thing, but maybe maybe I'm wrong.",
            "Maybe there is some connection, but at least it's not a superficial connection.",
            "Lois stackexchange.",
            "Thursdays I wouldn't dare to, you know, with this audience to say anything about speculate about these possibilities, but we can do it privately.",
            "Yes anymore questions.",
            "Thanks again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then to the organizers for inviting me here.",
                    "label": 0
                },
                {
                    "sent": "Thanks to everybody for sticking around until so late.",
                    "label": 0
                },
                {
                    "sent": "So this is not going to be a submodular talk.",
                    "label": 0
                },
                {
                    "sent": "And I don't know whether this is good or bad, but OK, this is a list of contributors that I was fortunate enough to work with this SMB back.",
                    "label": 0
                },
                {
                    "sent": "Shame Kakada Gabor logo.",
                    "label": 0
                },
                {
                    "sent": "She and I just realized I forgot to Geneva Diebert He's not here around so it's OK.",
                    "label": 0
                },
                {
                    "sent": "It's good though.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Forgot.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let me start with an example where of just so you understand what I'm talking about, so this is a combinatorial sequential prediction game, so you want to.",
                    "label": 0
                },
                {
                    "sent": "You want to predict certain combinatorial structure in a class in order to minimize a certain notion of costs across time.",
                    "label": 0
                },
                {
                    "sent": "So this is an example in which you want to keep a low cost spanning tree on a graph, and this games.",
                    "label": 0
                },
                {
                    "sent": "This prediction games are sequential, so they proceed in the steps and at every time step.",
                    "label": 1
                },
                {
                    "sent": "Given this graph which you know.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is some adversary that hidden from the player who is trying to predict the communal structures, puts some hidden costs on the edges of this graph.",
                    "label": 0
                },
                {
                    "sent": "So without seeing this car.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, the player chooses a tree.",
                    "label": 0
                },
                {
                    "sent": "Because his job is to maintain a good spanning tree or over this graph, and after the tree.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The trees chosen, the player suffer a loss which is linear in the edges that make up the tree.",
                    "label": 0
                },
                {
                    "sent": "So in this case, if you sum up the costs of the edges, you should get 22.",
                    "label": 0
                },
                {
                    "sent": "If I'm not mistaken, until yesterday was 17, but then I realized is that 22 OK, OK, and this game goes on and on.",
                    "label": 0
                },
                {
                    "sent": "So at the next step at the adversary will put the different edge costs on the edges again hidden from the player the player.",
                    "label": 1
                },
                {
                    "sent": "On the just by knowing what he lost on the previous trials will choose a different spanning tree and will suffer a new cost.",
                    "label": 0
                },
                {
                    "sent": "OK, in the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me make things more precise.",
                    "label": 0
                },
                {
                    "sent": "Now I want to specify what are, what is it?",
                    "label": 0
                },
                {
                    "sent": "Is that the player is observing and what is the the criterion that the player is trying to fulfill in this game.",
                    "label": 0
                },
                {
                    "sent": "So first of all, there are three nice feedback models that can be started in this sequential combinatorial games.",
                    "label": 1
                },
                {
                    "sent": "First of all is the full information criterion.",
                    "label": 1
                },
                {
                    "sent": "After choosing the tree, the player gets to see all the edge costs.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the second model is Semi Bandit which was also mentioned in Spotlight Talk before today is when the player only seats only gets to see the costs on the edges that belong to the city that he has chosen.",
                    "label": 0
                },
                {
                    "sent": "But he has information of the cost of every single edge that is part.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The tree and the third model is bandit model and this is the hardest case in which the the player has a feedback after each.",
                    "label": 0
                },
                {
                    "sent": "Each round of the game only gets to see the other cost of the tree as chosen so that the sum of the customer judges, but it doesn't see the cost.",
                    "label": 0
                },
                {
                    "sent": "An individual edges, just overall just the overall cost.",
                    "label": 0
                },
                {
                    "sent": "And so if you see the information is very different than the three models you see, the whole entire cost of all the edges, just the cost of the edges were chosen, or just the number which is the sum of those costs.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now to be a little little bit more formal.",
                    "label": 0
                },
                {
                    "sent": "In general you can play that.",
                    "label": 0
                },
                {
                    "sent": "You can play this game over trees, but over any other class of combinatorial structures.",
                    "label": 0
                },
                {
                    "sent": "And that's why we call these communitarian prediction games.",
                    "label": 0
                },
                {
                    "sent": "So in a more general setting you will have a Class S of combinatorial structures, for instance spanning trees of a given graph.",
                    "label": 0
                },
                {
                    "sent": "So again, the adversary chooses loss vector, which is the previous case was where the cost associated to all the edges of the graph.",
                    "label": 0
                },
                {
                    "sent": "So they were going to be the number of the edges in our graph.",
                    "label": 0
                },
                {
                    "sent": "In the previous example.",
                    "label": 0
                },
                {
                    "sent": "Now the player chooses an action which is an element, for instance spanning tree.",
                    "label": 1
                },
                {
                    "sent": "In the previous example.",
                    "label": 0
                },
                {
                    "sent": "And then gets to see the either the loss vector, entire loss vector and full information feedback model or the partial loss vector of those element that belongs to the.",
                    "label": 0
                },
                {
                    "sent": "Action has chosen in the semi bandit case or just the number which is the sum of the loss components associated to the agency's chosen.",
                    "label": 0
                },
                {
                    "sent": "So I should say here that it, but you should have guessed it by now, 'cause you've seen examples.",
                    "label": 0
                },
                {
                    "sent": "I'm representing actions with with incidence vectors so that it is natural to represent the cost associated to an action as a dinner product between the loss vector.",
                    "label": 0
                },
                {
                    "sent": "And the incidence vector that represents the action in the set.",
                    "label": 0
                },
                {
                    "sent": "So again, I represent I represent the spanning tree as the incident vector of the edges that belong to the spanning tree.",
                    "label": 0
                },
                {
                    "sent": "Given all the edges of the graph, OK.",
                    "label": 0
                },
                {
                    "sent": "So in the I can represent with incident vectors.",
                    "label": 0
                },
                {
                    "sent": "So we represent class of actions as classes of incident vectors over the Boolean hypercube.",
                    "label": 0
                },
                {
                    "sent": "You mentioned E OK and what's important here is that the loss of each action attende is linear over the incidence vector.",
                    "label": 1
                },
                {
                    "sent": "As I said before, so again, the loss of a tree.",
                    "label": 0
                },
                {
                    "sent": "This is not working and more the loss of a tree is the sum of its edge costs, and I'm assuming also a scaling scaling assumption such that the bees inner products are at most one.",
                    "label": 1
                },
                {
                    "sent": "For all the actions in my set, OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now what I'm supposed to do here?",
                    "label": 0
                },
                {
                    "sent": "What is the player?",
                    "label": 0
                },
                {
                    "sent": "What is the goal of the player in this game?",
                    "label": 0
                },
                {
                    "sent": "So in a if you play this game over and over, choosing again over and over sets.",
                    "label": 0
                },
                {
                    "sent": "Action from the action set and incurring losses, then the player over a sequence of choices in the action set will incur up to time TA certain cumulative loss, which is the sum of these inner products between the actually have chosen and the loss vectors.",
                    "label": 0
                },
                {
                    "sent": "This is irrespective to the feedback model in all the three feedback models.",
                    "label": 0
                },
                {
                    "sent": "And the goal, given a certain sequence of choices of the adversary, is to minimize the difference between the cumulative loss of the player and the cumulative loss of the best fixed action in the action set.",
                    "label": 0
                },
                {
                    "sent": "So this is the cumulative loss that a player would have suffered if he had chosen consistently the single best action isn't set.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the single best spanning tree.",
                    "label": 0
                },
                {
                    "sent": "On the graph for the specific sequence of losses.",
                    "label": 0
                },
                {
                    "sent": "So of course this is something this minimization cannot be done in this model, because the smaller is sequential, so the even in the full information model, the player doesn't get to see in advance all the sequence of loss vectors, But the sequence of loss vectors is revealed incrementally to the player, so I would like to control this quantity in all the three feedback models.",
                    "label": 0
                },
                {
                    "sent": "OK, and to bound it as a function of T.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because I have this scaling assumption.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that it's it's sensible to require that I would like this regret that to grow sublinearly overtime.",
                    "label": 0
                },
                {
                    "sent": "So that because it's trivial I mean, your instantaneous regret can be sort of a constant is at most one on each time step OK?",
                    "label": 0
                },
                {
                    "sent": "Because I have this assumption here.",
                    "label": 0
                },
                {
                    "sent": "So I'm I'm.",
                    "label": 0
                },
                {
                    "sent": "I'm happy if I'm able to prove that this difference will grow sublinearly overtime.",
                    "label": 0
                },
                {
                    "sent": "OK, so this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the Philippines, studied a lot, and so I'm going to serve a number of results for the different feedback models, and I'm going to sketch a couple of algorithms have been proposed, and then at the end of the talk I'm going to give you some more recent results that concerning the combinatorial bandit feedback model.",
                    "label": 0
                },
                {
                    "sent": "So this is a template for players that in this case we're going to be randomized because we are in an adversarial.",
                    "label": 1
                },
                {
                    "sent": "Prediction models so players should be randomized in order to have a chance to stand a chance against the versary.",
                    "label": 0
                },
                {
                    "sent": "So the player General Player will maintain a set of weights on the coordinates of the of the game.",
                    "label": 0
                },
                {
                    "sent": "OK, so for instance set of weights over the edges of the graph.",
                    "label": 0
                },
                {
                    "sent": "In the case of the spanning tree.",
                    "label": 0
                },
                {
                    "sent": "And then that will obtain it from this set of weights at distribution over the actions.",
                    "label": 0
                },
                {
                    "sent": "OK, we see ways of obtaining a distribution over the action set from weight assignment over the coordinates, and then will draw an action from the distribution of the action set.",
                    "label": 0
                },
                {
                    "sent": "And this will be the action played and then it will occur depending on the feedback model, it will have to come up with an estimate of the actual.",
                    "label": 0
                },
                {
                    "sent": "Loss vector, which in the semi bandit and bandit model is not observed, is only observed in the full information model and it will use this estimate of the loss vector in order to update the assignment of weights to the coordinates and this will allow him to come up with a different distribution for the next round of the game.",
                    "label": 0
                },
                {
                    "sent": "So this will go over and over.",
                    "label": 0
                },
                {
                    "sent": "So now in typically will have a different unbiased loss estimates for the weights.",
                    "label": 1
                },
                {
                    "sent": "So this game is completely deterministic.",
                    "label": 0
                },
                {
                    "sent": "The adversary we don't have any assumption on the way the adversary is generating the loss vectors, so the.",
                    "label": 0
                },
                {
                    "sent": "Player is using its own internal randomization to come up with the estimates that will be unbiased in the expectation with respect to the players on randomization.",
                    "label": 1
                },
                {
                    "sent": "OK, so in the case of full information, there's nothing to estimate.",
                    "label": 0
                },
                {
                    "sent": "We do observe the entire loss vector, so the estimate concise with the loss vector which we get to see.",
                    "label": 0
                },
                {
                    "sent": "So we get to see the cost of all the edges of our graphing cases spanning trees in the semi bandit case we only get to see the last component of the elements.",
                    "label": 0
                },
                {
                    "sent": "All the action sets of, let's say, of the edges that we make part of our spanning tree in the in the spanning tree case.",
                    "label": 0
                },
                {
                    "sent": "So the components of the incident vector that are one in general and we normalize this by the weighted succeeded with the coordinate.",
                    "label": 0
                },
                {
                    "sent": "So this this is a statistic because we can compute it in the sample space of the players.",
                    "label": 0
                },
                {
                    "sent": "Randomization OK and in the bandit remember them.",
                    "label": 0
                },
                {
                    "sent": "And if we just get to see this scalar quantity here, which is the projection of the last vector onto the action that we have chosen, and we use a sort of a least square estimate.",
                    "label": 1
                },
                {
                    "sent": "Where this PT is the pseudo inverse of the correlation matrix of the actions under the sampling distribution that we use the time T OK so this looks very much like a sort of at least square estimate, and this turns out to be a good thing to use in the bandit case.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have unbiased estimate for our losses and we have a general way of playing this game.",
                    "label": 0
                },
                {
                    "sent": "So now we want to see how to.",
                    "label": 0
                },
                {
                    "sent": "Go about computing these weights and doing the updates.",
                    "label": 0
                },
                {
                    "sent": "The updates OK, so please interrupt me if you have any questions.",
                    "label": 0
                },
                {
                    "sent": "Yes, is there.",
                    "label": 0
                },
                {
                    "sent": "If you go back to the last slide there is this estimation of the most vector you suddenly used PT which you can choose in many ways.",
                    "label": 0
                },
                {
                    "sent": "Would there be an advantage to choose meat in that particular way?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, I will give you 2 algorithms that specify both.",
                    "label": 0
                },
                {
                    "sent": "Joint way of choosing PT NWT and, of course, this estimate will make sense for specific choices of PT OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's see one way.",
                    "label": 0
                },
                {
                    "sent": "One way of playing this game and.",
                    "label": 0
                },
                {
                    "sent": "Is the so called expanded hedge algorithm, which is the.",
                    "label": 0
                },
                {
                    "sent": "Generalization to combinatorial sequential prediction problems of the hedge algorithms.",
                    "label": 1
                },
                {
                    "sent": "That, or the experts algorithm, that using exponential weights.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that it is very simple.",
                    "label": 0
                },
                {
                    "sent": "So you remember we had weights over coordinates.",
                    "label": 1
                },
                {
                    "sent": "This was the main element of our our playing strategy and this capital L had the cumulative loss estimates for each coordinate.",
                    "label": 0
                },
                {
                    "sent": "So essentially we penalize the coordinate exponentially according to the cumulative.",
                    "label": 0
                },
                {
                    "sent": "Estimated loss suffered by that coordinate.",
                    "label": 0
                },
                {
                    "sent": "So in the case of the full information feedback, we do know how much specific edge of our graph has the overall cost of a specific edge of our graph.",
                    "label": 0
                },
                {
                    "sent": "Because we can see it in the summer information model, we only we only saw the only so the cost of a single Ledger over all the times that we chose that edge in our as part of our action and in the bandit case we just have some.",
                    "label": 0
                },
                {
                    "sent": "We don't have a way of serving this directly.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we can use this to create a weight assignment over actions to watch for two actions of our action sets by again taking exponential weighting where the weight given to an action is the projection of our of the cumulative loss.",
                    "label": 0
                },
                {
                    "sent": "I estimate onto the action vector the incidence vector of the action OK and right.",
                    "label": 0
                },
                {
                    "sent": "So now in we can play this.",
                    "label": 0
                },
                {
                    "sent": "We can normalize this distribution and use this distribution to pick our actions and we can plug in our estimates for the let's say for the full information over the semi bandit information case in the bandit case we have to be a little bit.",
                    "label": 0
                },
                {
                    "sent": "More clever and that this algorithm spender data is known as dramatic hate hedge and in the since the variance of this loss estimates would be much higher in the bandit case because we get much less feedback.",
                    "label": 1
                },
                {
                    "sent": "You remember we only get to see this color information and which is the overall cost of our action.",
                    "label": 0
                },
                {
                    "sent": "So we have to be more careful.",
                    "label": 0
                },
                {
                    "sent": "And instead of playing the normalized playing.",
                    "label": 0
                },
                {
                    "sent": "Using the probability given by the normalized wait OK, we played we play a probability which is a mix of the normalized weight and then another distribution which is helpful in order to control the variance of the last estimator.",
                    "label": 0
                },
                {
                    "sent": "So we need some additional variance control in the feedback model where we get very little feedback OK.",
                    "label": 0
                },
                {
                    "sent": "So this essentially is a way of.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using this player because we know how to compute P and we have a way of expressing that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "View in terms of sums of lost estimates so we can run this player an gets results for using the previous last estimates for the Old Tree bandit models OK. Now the second algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It has been proposed is a variant of mirror dissent.",
                    "label": 0
                },
                {
                    "sent": "So this is a completely different idea and the idea is that you don't.",
                    "label": 0
                },
                {
                    "sent": "You don't compute a distribution over, you know directly, maintain a distribution over the actions, but what you maintain is a point inside the convex Hull of the action set, and then you do a gradient descent or a mirror dissent inside the convex Hull of the action set using a specific potential function.",
                    "label": 0
                },
                {
                    "sent": "So a potential function is a parameter of this of the mirror descent algorithm and helps you too.",
                    "label": 0
                },
                {
                    "sent": "Specially Peps who gives you a way to control the variance.",
                    "label": 0
                },
                {
                    "sent": "Again, it gives you a way to control the variance of the estimates and to any potential.",
                    "label": 0
                },
                {
                    "sent": "We can also set a Bregman divergences, which is a measure of distance, and for instance if the if the potential function which is always destructive potential strictly convex function is the squared loss is A squared Euclidean norm, then the Bregman divergences will be just squared Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "If you have any other.",
                    "label": 0
                },
                {
                    "sent": "Is strictly convex.",
                    "label": 0
                },
                {
                    "sent": "A potential function?",
                    "label": 0
                },
                {
                    "sent": "The Bregman divergences will be suitable generalization of the squared Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "So in general this this is a distance which doesn't obey the triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "A may not be even symmetric.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now this is a nice slide that I borrowed from Sebastian.",
                    "label": 0
                },
                {
                    "sent": "I'm not so good at drawing slicing later, so this is a sketch to explain how this online stochastic mirror dissent works.",
                    "label": 1
                },
                {
                    "sent": "So you have a wait.",
                    "label": 0
                },
                {
                    "sent": "And now this weight is is a point inside the convex Hull of your action set.",
                    "label": 0
                },
                {
                    "sent": "And here is the simplex over the action set.",
                    "label": 0
                },
                {
                    "sent": "So this is where your probability distribution over actions will be elements of this space and your weights will be elements of this space, and then you define the potential function over some domain which contains the convex Hull of your action set.",
                    "label": 0
                },
                {
                    "sent": "So now what you do?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's at any point that you have some probability distribution of their actions which you use to draw your action at time T. This chorus.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Points to a point in the convex Hull of your action set, and once you get your feedback.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, you will use your loss estimate depending on the feedback model to update during a mirror dissent gradient step to update your weight and this is the update rule for the mirror dissent, which is the gradient of the potential function.",
                    "label": 0
                },
                {
                    "sent": "I'm not going into details, so this is a standard mirror dissent strategies, which is the most common strategy for online convex optimization, so this update might take you out of the.",
                    "label": 0
                },
                {
                    "sent": "Comic sites, so you do.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Projection step using the Bregman divergences which takes you back inside the convex Hull, and then at this new weight.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will give you a new probabilities by decomposing this weight onto a sum onto.",
                    "label": 0
                },
                {
                    "sent": "By representing this weight as a convex combination of the sum of the point or some of the vertices of this polytope, you will get the point.",
                    "label": 0
                },
                {
                    "sent": "PT in the simplex, so this will be a distribution over your the corners of this polytope, which are your actions OK?",
                    "label": 0
                },
                {
                    "sent": "This is a convex Hull over the action set, yes.",
                    "label": 0
                },
                {
                    "sent": "This is not the unique way.",
                    "label": 0
                },
                {
                    "sent": "Now to define PT.",
                    "label": 0
                },
                {
                    "sent": "There is no unique way, but this is the way I choose for this algorithm, and so PT is chosen so to satisfy this condition here, which is what we need.",
                    "label": 0
                },
                {
                    "sent": "And there is a I mean.",
                    "label": 0
                },
                {
                    "sent": "There is no unique you may you may represent the PT in different way given this is what but I want PT to satisfy this equation.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now as Francis was observing, yes, we want to express a point inside the convex Hull of our action set as a distribution over the actions, and then we need also to draw an action according to this distribution.",
                    "label": 0
                },
                {
                    "sent": "So character dude category theorem ensures that we can find.",
                    "label": 0
                },
                {
                    "sent": "A point in the simplex which has a small support over which is supported by a small number of actions, and this would also solve the sampling problem because in that case it would be easy to sample an action which is supported by sample and action according to a distribution which is supported by a small number of actions.",
                    "label": 1
                },
                {
                    "sent": "However, computing this set is generally intractable, and this is the reason why this algorithm is in general.",
                    "label": 1
                },
                {
                    "sent": "Intractable.",
                    "label": 0
                },
                {
                    "sent": "And however, in many interesting, specific interesting cases, for instance, if the convex Hull of our action set as a polynomial number of faces, then we can find the in polynomial time distribution with that.",
                    "label": 0
                },
                {
                    "sent": "Satisfying that condition, which is supported by a small number of actions so we can draw easily from that one.",
                    "label": 0
                },
                {
                    "sent": "So this is for instance, a way with which you can use to.",
                    "label": 0
                },
                {
                    "sent": "That we have a way so that you can use this algorithm for.",
                    "label": 0
                },
                {
                    "sent": "For instance, for drawing paths in a graph.",
                    "label": 0
                },
                {
                    "sent": "OK, because you can represent using the Max flow decomposition, you can represent a path using a small number of.",
                    "label": 0
                },
                {
                    "sent": "Edges in your graph.",
                    "label": 0
                },
                {
                    "sent": "OK, sorry you can represent.",
                    "label": 0
                },
                {
                    "sent": "You can represent a distribution over.",
                    "label": 0
                },
                {
                    "sent": "You can represent an L.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I meant in your convex.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a distribution over a small number of paths in your graph, and this is how you can.",
                    "label": 0
                },
                {
                    "sent": "Draw a path in polynomial time.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so these two algorithms have various instances that very special cases and very instances that have been proposed in the past.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned, the hedge algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is for playing actions that are not.",
                    "label": 0
                },
                {
                    "sent": "Notorial elements of a combinatorial space, so by classical case I mean the case in which you are.",
                    "label": 0
                },
                {
                    "sent": "Your actions are the.",
                    "label": 0
                },
                {
                    "sent": "Orphans of the space.",
                    "label": 0
                },
                {
                    "sent": "So you just have a number of actions equal to the number of dimensions in your space.",
                    "label": 0
                },
                {
                    "sent": "And each action is there is a coordinating your space.",
                    "label": 0
                },
                {
                    "sent": "And so this is the you have the the hedge algorithm and then you have the X3 algorithm, the bandit case and the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Also in the bandit case that have been proposed and also in the past.",
                    "label": 0
                },
                {
                    "sent": "So these are all instances of the expanded edge algorithm.",
                    "label": 0
                },
                {
                    "sent": "That was mentioned before, OK. And then we also have special cases of the mirror descent algorithm.",
                    "label": 0
                },
                {
                    "sent": "For instance the algorithm that has been proposed three years ago that uses self concordant functions and a slightly different way of estimating losses loss vectors.",
                    "label": 0
                },
                {
                    "sent": "As in order to direct the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dissent, the gradient descent inside the convex Hull of actions.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "While the there's a wide range of special cases and.",
                    "label": 0
                },
                {
                    "sent": "This there are two main families of algorithms, so one is this.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mirror dissent, in which you make this gradient descent inside the convex Hull, and then you decompose and then this is a different exam.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is the expanded edge in which you more explicitly maintain a distribution over the corners of this action set, and the distribution is characterized by exponential weights.",
                    "label": 0
                },
                {
                    "sent": "This expanded edge algorithm can be viewed as a as a sort of in a strange way as an instance of mirror dissent.",
                    "label": 0
                },
                {
                    "sent": "But this is more convenient to view it as a completely separate strategy.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so this is a nice table summarizing the some known regret bounds.",
                    "label": 1
                },
                {
                    "sent": "OK, so you remember this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the difference between the cumulative cost to suffered by the player and the cumulative cost of the single best actions in your action set.",
                    "label": 0
                },
                {
                    "sent": "After T plays OK, so here you don't.",
                    "label": 0
                },
                {
                    "sent": "You don't see the size of the action set, which I'm assuming to be exponential in the number in the dimension of the problem.",
                    "label": 0
                },
                {
                    "sent": "For instance, typically have an exponential number of spanning trees in the graph, so this is not.",
                    "label": 0
                },
                {
                    "sent": "This is just going to affect the bounds by.",
                    "label": 0
                },
                {
                    "sent": "Constant factors becausw.",
                    "label": 0
                },
                {
                    "sent": "Just by constant factors.",
                    "label": 0
                },
                {
                    "sent": "So it's just important that it's exponential indeed.",
                    "label": 0
                },
                {
                    "sent": "So you see here that refeed that models so we have the full feedback models in which you fully observe the loss vector at each round, and then you see here that the two algorithms expanded edge and the online mirror dissent achieve the same regret bound.",
                    "label": 0
                },
                {
                    "sent": "You see, it's a sub linear in time is it grows with the square root of time and it depends.",
                    "label": 0
                },
                {
                    "sent": "Also, the square root of the number of dimensions in the problem.",
                    "label": 0
                },
                {
                    "sent": "So this is irrespective to the specific shape of the action set and irrespective to the way the adversary is giving the loss vectors OK and there is a matching lower bound that shows that this is essentially the best you can get.",
                    "label": 0
                },
                {
                    "sent": "OK, in the semi bandit case where you were the player observes the costs of the let's say the edges of the graph that belong to a spanning tree in the spanning tree case of, for instance, the edges of the graph that belong to the path in the graph.",
                    "label": 0
                },
                {
                    "sent": "If it is a routing problem then you then these arguments have different regrets.",
                    "label": 0
                },
                {
                    "sent": "So expanded edge depends linearly in the dimension.",
                    "label": 0
                },
                {
                    "sent": "And the square root of time and online online mirror dissent has for a specific choice of the of the potential function and has a better regret bound which is matching the lower bound and is also matching the full information cases.",
                    "label": 0
                },
                {
                    "sent": "So it's essentially this says that in the semi bandit the cost of estimating the missing information is.",
                    "label": 0
                },
                {
                    "sent": "Not affecting the regret.",
                    "label": 0
                },
                {
                    "sent": "OK, in this adversarial model.",
                    "label": 0
                },
                {
                    "sent": "So the interesting situation is in the in the bandit case, we just where the player just observes a scalar, which is the cost of the action is chosen, and here there is a lower bound of this form in the.",
                    "label": 0
                },
                {
                    "sent": "The two algorithms are not directly compareable.",
                    "label": 0
                },
                {
                    "sent": "Expanded Edge has bound which as this form here.",
                    "label": 0
                },
                {
                    "sent": "In the D online mirror dissent as abound of this form, where this bound is gotten using the self concordance at Concorde and potential function, which depends on a parameter Theta and in general Theta is can be order of the dimension of the problem, so that essentially in general these two bounds will have the same order of magnitude, so there's a.",
                    "label": 0
                },
                {
                    "sent": "So you see, there's a gap here, and I'm in the second part of this talk I'm going to.",
                    "label": 0
                },
                {
                    "sent": "Address a way to delete it to close this gap by showing a different different implementation of expanded hedge of this pended edge algorithm OK.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So right so let me now focus on the on the bandit feedback model and focus on the expanded add edge are going for bandits, which is called the geometric hedge.",
                    "label": 0
                },
                {
                    "sent": "So again, I'm maintaining.",
                    "label": 0
                },
                {
                    "sent": "I'm in a full band in the bandit model, so I just observed this scalar here and I'm taking the algorithm is maintaining this last estimate OK?",
                    "label": 0
                },
                {
                    "sent": "Where again at this PT is the.",
                    "label": 0
                },
                {
                    "sent": "Expectation is the correlation matrix of the actions under the sampling distribution PT.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm maintaining the weights and just repeating what I wrote in a previous light and maintaining the weights over the coordinates in my using composing these weights by making the product of the coordinates that belong to a specific action to come up with the distribution of actions.",
                    "label": 0
                },
                {
                    "sent": "And then I am a sampling from this distribution and I'm using this correction which is mixing, mixing exploracion distribution over the actions, which is good too.",
                    "label": 0
                },
                {
                    "sent": "Reduce the variance so I haven't told you how to choose this exploration distribution at Angle to address the question now, and I also would like to point out the fact that this algorithm is also not in general is not efficient.",
                    "label": 0
                },
                {
                    "sent": "Just like online mirror dissent, becausw, you have the problem of sampling and action over an exponential set of over an exponential set over S. So that you have the problem with somebody or something and action according to this distribution is potentially intractable, OK?",
                    "label": 0
                },
                {
                    "sent": "So we will discuss the special cases in which this is possible.",
                    "label": 0
                },
                {
                    "sent": "OK, so right.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is a general bound for this algorithm which is parameterized with respect to the exploration distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so mu here.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will be this exploration distribution which is the mixing term in the sampling distribution which is helps to control the variance of these estimates in every coordinate in every direction.",
                    "label": 0
                },
                {
                    "sent": "This will be the hard part to get a better bound for the bandit case.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now there is a general regret bound that depends.",
                    "label": 1
                },
                {
                    "sent": "You see, there's a square.",
                    "label": 0
                },
                {
                    "sent": "There is the number of dimension here.",
                    "label": 0
                },
                {
                    "sent": "There's this square root of time, and then there is this term here, which is the key term which depends on the size of the radius of your action set.",
                    "label": 0
                },
                {
                    "sent": "In the Euclid inclusion rate radius of your action set, and the smallest eigenvalue of the correlation matrix of your action set under the.",
                    "label": 1
                },
                {
                    "sent": "Exploration distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so this tells you somehow that you would like to.",
                    "label": 0
                },
                {
                    "sent": "You know you would like the spectrum of this matrix here to be as uniform as possible, so you don't want to have a skewered.",
                    "label": 0
                },
                {
                    "sent": "Metrics here.",
                    "label": 0
                },
                {
                    "sent": "OK, so in non uniform spectrum and the reason why you get this term here in the regret is because this term here this ratio is proportional to the variance of your last estimate.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is the term responsible.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or the variance in this loss estimate here?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the price that the price that you have to pay in order to in order to.",
                    "label": 0
                },
                {
                    "sent": "A compensate the fact that we are not observing enough information.",
                    "label": 0
                },
                {
                    "sent": "OK, so the amazing thing you can still get the square root of T dependence.",
                    "label": 0
                },
                {
                    "sent": "So now the question is I should come up with an exploration distribution that on my action set.",
                    "label": 0
                },
                {
                    "sent": "So this is the random variable that varies over the action set.",
                    "label": 0
                },
                {
                    "sent": "Give me a spectrum set of eigenvalues as uniform as possible.",
                    "label": 0
                },
                {
                    "sent": "OK, indeed, when we are, whenever we are able to prove this so that this ratio becomes a constant, then we get is bounded.",
                    "label": 0
                },
                {
                    "sent": "This square root of T which is the which matches the lower bound.",
                    "label": 0
                },
                {
                    "sent": "So whenever this is true, which means that the action space.",
                    "label": 0
                },
                {
                    "sent": "OK, so now what is an easy exploration distribution?",
                    "label": 0
                },
                {
                    "sent": "OK, let's try.",
                    "label": 0
                },
                {
                    "sent": "We can just explore it uniformly, so basically.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We now we our sampling distribution will have two parts.",
                    "label": 0
                },
                {
                    "sent": "This part which is the exploitation part which focuses on the actions on the actions that contains many coordinates that suffered suffered little loss in the past, at least estimated loss.",
                    "label": 0
                },
                {
                    "sent": "And then we add a mixing distribution exploration distribution which is uniform over the actual space.",
                    "label": 0
                },
                {
                    "sent": "OK, this is very simple, so we tried this.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what this means?",
                    "label": 0
                },
                {
                    "sent": "OK, since this distribution is uniform, it means that if the actual space is essentially isotropic.",
                    "label": 0
                },
                {
                    "sent": "So the distribution is uniform, and if the actual spaces isotropic, I hope that the eigenvalues will be of these metrics will be more or less all the same.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so there are lots of interesting cases of structures where this you get to this effect.",
                    "label": 0
                },
                {
                    "sent": "So these are this is for instance you see here.",
                    "label": 0
                },
                {
                    "sent": "Permutation spanning trees cuts of a click Hamiltonian, cycles, subsets of a set of the elements, so these are all classes of actions, tutorial actions, in which you can go and compute the minimum eigenvalue.",
                    "label": 1
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of this matrix for the uniform distribution.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this turns out to be just the right magnitude.",
                    "label": 0
                },
                {
                    "sent": "In order to achieve the.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optimal bound.",
                    "label": 0
                },
                {
                    "sent": "This bound here so becausw.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You when you plug in the quantities the relevant quantities here.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You get exactly this dependence.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in those cases we are lucky.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you can go about the sampling problem and right in some cases by using a darker algorithms developed in the literature, you can prove that you can efficiently sample actions from your sampling distribution PT in polynomial time by using.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Random Walk on the corners of the.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the corners of the hypercube.",
                    "label": 0
                },
                {
                    "sent": "OK, so in some cases this algorithm can be efficient and optimal.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so there is some bad cases, for instance that the routing problem is a bad case.",
                    "label": 0
                },
                {
                    "sent": "So you see here this is agreed that this is your graph and you want to route from this source to this destination.",
                    "label": 0
                },
                {
                    "sent": "So these are all possible paths you can take.",
                    "label": 0
                },
                {
                    "sent": "And of course some edges will be contained just in a very few paths, whereas some edges other edges will contain an exponential number of paths.",
                    "label": 1
                },
                {
                    "sent": "So now if you take a uniform exploration over this grid.",
                    "label": 0
                },
                {
                    "sent": "It means that those edges will be sampled much more often, so your estimate your last estimates will be bad.",
                    "label": 0
                },
                {
                    "sent": "We have a high variance on those edges, so this is a typical case in which uniform exploration doesn't work, there's an.",
                    "label": 1
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nice trick.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That you can use to control your variance and to address these specific.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problems, let's see what's the trick.",
                    "label": 0
                },
                {
                    "sent": "So remember, the actions are incidence vectors of the corners of the Boolean after cube, and you have a problem when the actual set is cured is not isotropic like in this case.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, because we have like a marginal actions over here, these paths are marginal.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now you want to get a little help by changing the way represent the problem.",
                    "label": 0
                },
                {
                    "sent": "So this is a linear.",
                    "label": 0
                },
                {
                    "sent": "This is a linear problem, so you can.",
                    "label": 0
                },
                {
                    "sent": "You can change the basis in which you represent your loss estimates.",
                    "label": 0
                },
                {
                    "sent": "OK, so you would like to choose a base of RDA under which the actual set looks more isotropic and then you would like to run this geometric algorithm with loss estimates, computed this basis, and then you hope that this.",
                    "label": 1
                },
                {
                    "sent": "Can help to cure the problem of skewered action sets.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one way which was proposed in the past and gets you so this gets you the.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And over here, which is not optimal, because there is a gap, is through the use of various.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Centric spanners this is a very nice idea so very centric.",
                    "label": 0
                },
                {
                    "sent": "Spanners are actually a subset.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of actions in your action set and if your action sets action, set span spans Rd, you will find the D actions and the actions will spend the whole action set, so will be a basis.",
                    "label": 0
                },
                {
                    "sent": "It won't be necessarily an orthogonal basis for your action set, but The thing is that in the in the coordinate of this basis all actions will have a small coordinates, so specifically.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And exploration barycentric spanners is such that all actions will have in the basis of debris centric spanners coordinates between minus one and one.",
                    "label": 1
                },
                {
                    "sent": "OK, so there is always existed very center spanners for any action set.",
                    "label": 0
                },
                {
                    "sent": "This is a very nice proof due to Kleinberg and the other book.",
                    "label": 1
                },
                {
                    "sent": "And now the trick is that you do exploration over uniformly over the spanners.",
                    "label": 0
                },
                {
                    "sent": "And this buys you two things.",
                    "label": 0
                },
                {
                    "sent": "First of all gives you mix this metric size of Tropic.",
                    "label": 0
                },
                {
                    "sent": "So Lambda mean is 1 / D, which is the best you can hope and then you get because you've chosen a specific and orthogonal basis.",
                    "label": 0
                },
                {
                    "sent": "The norm of your actions, you square the clear norm of your actions will be D. So now if you go and read out it bound, you get what I showed you before D2 to three apps.",
                    "label": 0
                },
                {
                    "sent": "So this is a this gives idea.",
                    "label": 0
                },
                {
                    "sent": "Of using a different ticket along along the same lines.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the last thing I'm showing here, so now a better basis for your exploration is given by the learner John Ellipsoid.",
                    "label": 0
                },
                {
                    "sent": "So this is again your action set.",
                    "label": 0
                },
                {
                    "sent": "These are not corners of the applicable, but of course it's nicer to draw.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you take the smallest volume enclosing ellipsoid of your action set, and now your new coordinate system will be given by the axis of this ellipsoid.",
                    "label": 0
                },
                {
                    "sent": "OK, and your exploration is will be given by the contact points of the Ellipse ellipse ellipsoid with your action set.",
                    "label": 0
                },
                {
                    "sent": "OK, now there is a very beautiful.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Erm, result, John's theorem in convex geometry that tells you that if you do that, so if you do exploration on these points, but this position is not uniform over these points and just dinner and gives you the right coefficients, the right convex coefficients.",
                    "label": 0
                },
                {
                    "sent": "So specify your desperation distribution, then the this term, which is the one that.",
                    "label": 0
                },
                {
                    "sent": "Rules for the variance of estimates will be such that the smallest eigenvalues 1, /, D and.",
                    "label": 0
                },
                {
                    "sent": "So the new system of coordinate is such that is very easy, so you re scale the axis so that this ellipsoid looks like the unit ball.",
                    "label": 0
                },
                {
                    "sent": "OK, so in the new in the in this new in this news basis where the ellipsoid looks like the unit ball then this thing will be azeotropic.",
                    "label": 0
                },
                {
                    "sent": "The action set will be the topic in the sense that this will hold the the minimum value of these metrics will be 1 / D. And since now the actions are in the unit ball, the length of every actions will action will be at most one.",
                    "label": 0
                },
                {
                    "sent": "So this is smaller equal in one.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore category theorem will ensure that the contact points we will be at most this squared order of this squared, so you're at least your exploration distribution will be supported over a small number of points.",
                    "label": 0
                },
                {
                    "sent": "OK, so computing the lower learner ellipsoid is not efficient in all cases, but in some in some cases it can be done efficiently in some other in most cases can be approximated efficiently and you will paint the regret the term due to the explosive nation.",
                    "label": 0
                },
                {
                    "sent": "So now you see you've got an isotropic action set using this trick and the B squared.",
                    "label": 0
                },
                {
                    "sent": "The Oriole action will have length bounded by one and you can prove that the game hasn't changed it.",
                    "label": 0
                },
                {
                    "sent": "So this is this transformation is not changing the game if you do it properly.",
                    "label": 0
                },
                {
                    "sent": "And now you're if you put this in your regret bound, you get the optimal regret.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a very nice result.",
                    "label": 0
                },
                {
                    "sent": "Basic result economist geometry that delivers very easily given the all the previous machinery.",
                    "label": 0
                },
                {
                    "sent": "The optimal bound by giving you a way of optimally exploring the action set.",
                    "label": 0
                },
                {
                    "sent": "So in order to keep all the variances controlled.",
                    "label": 0
                },
                {
                    "sent": "Under control OK.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is the new picture, so you see now that there is a nice upper upper lower matching upper matching upper or lower bounds for all the feedback models.",
                    "label": 0
                },
                {
                    "sent": "OK, however.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm done.",
                    "label": 0
                },
                {
                    "sent": "Still lots of interesting things to do, so let me just wrap up by saying that we look at that communal sequential prediction problems with linear loss in three feedback models.",
                    "label": 1
                },
                {
                    "sent": "Full information, semi bandit and bandit.",
                    "label": 0
                },
                {
                    "sent": "And we've seen two algorithms expanded edge and online mirror dissent.",
                    "label": 0
                },
                {
                    "sent": "These are two fundamentally different algorithm.",
                    "label": 0
                },
                {
                    "sent": "One is derived from the exponential weighting algorithm or the hedge algorithm for the experts model and the other one is really an interior method that does gradient descent in the comics.",
                    "label": 1
                },
                {
                    "sent": "Correction set and keep problem, especially in the benefit back is estimating the loss in each coordinate while keeping the variance under control and we shown away with expanded edge.",
                    "label": 1
                },
                {
                    "sent": "So to achieve this control of the variance by discovering the action set using the basis.",
                    "label": 0
                },
                {
                    "sent": "This is provided by the lunar ellipsoid.",
                    "label": 0
                },
                {
                    "sent": "So you re scale this ellipsoid then you get a good a good basis for your estimates.",
                    "label": 0
                },
                {
                    "sent": "What's still open is.",
                    "label": 0
                },
                {
                    "sent": "Really too.",
                    "label": 0
                },
                {
                    "sent": "OK, first of all, this slow.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're bound.",
                    "label": 0
                },
                {
                    "sent": "This lower bound is not is a general lower bound, so it might be very, and it is the case for specific action sets.",
                    "label": 0
                },
                {
                    "sent": "You get better bounds better bounds than this, so this bound is not uniformly optimal.",
                    "label": 0
                },
                {
                    "sent": "Overall action sets OK, so and you see this bound doesn't count is very poor in terms of doesn't contain information about the topology of the action set, so your lunar rescaling as wipe it away all the topology by making looking you know by making the actual set look like.",
                    "label": 0
                },
                {
                    "sent": "The ball completely isotropic, so you are matching the you're matching the lower bound, but it might be the case that by retaining this topological information and then using a more clever way to control your variance, you may be able to get a bound which contains some information about the structure of the action sheet and can be better than this in for specific action sets and so.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It could be.",
                    "label": 0
                },
                {
                    "sent": "This could be a clever way of design potential function in using mirror dissent in order to prove optimal regret bounds that contain this more detailed information.",
                    "label": 1
                },
                {
                    "sent": "And then of course there is still an open problem of finding general optimal efficient strategy that is allows you to run even in the bandit model.",
                    "label": 0
                },
                {
                    "sent": "This algorithm and efficient way.",
                    "label": 0
                },
                {
                    "sent": "OK, I think I'm done.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Definition goods rose in spanning trees, so I want you to being submitted ality without knowing it could be very well the case.",
                    "label": 0
                },
                {
                    "sent": "Yes yes yes yes yes.",
                    "label": 0
                },
                {
                    "sent": "I just have to be enlightened.",
                    "label": 0
                },
                {
                    "sent": "No, no no.",
                    "label": 0
                },
                {
                    "sent": "It's by I mean no.",
                    "label": 0
                },
                {
                    "sent": "No kidding this this yeah I mean sitting here throughout the day.",
                    "label": 0
                },
                {
                    "sent": "You know of course.",
                    "label": 0
                },
                {
                    "sent": "I I should study these things better?",
                    "label": 0
                },
                {
                    "sent": "I mean start OK, so you assume that the loss the losses were bounded.",
                    "label": 1
                },
                {
                    "sent": "Yeah, on the full action.",
                    "label": 0
                },
                {
                    "sent": "So if you if you do a different trick if let's say in the case of the path you bound the loss of each edge, yeah, then very often there's differences.",
                    "label": 0
                },
                {
                    "sent": "Even the full information game between right the expanded hedge right?",
                    "label": 0
                },
                {
                    "sent": "And the component head right extra extremely right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "And it's very baffling because you do not know.",
                    "label": 0
                },
                {
                    "sent": "Which algorithm is better?",
                    "label": 0
                },
                {
                    "sent": "One always better.",
                    "label": 0
                },
                {
                    "sent": "It's just yeah so.",
                    "label": 0
                },
                {
                    "sent": "So Manfred Manfred is reminding me that there are different ways of.",
                    "label": 0
                },
                {
                    "sent": "Scaling or making skinning assumption in your game.",
                    "label": 0
                },
                {
                    "sent": "So you may assume instead of.",
                    "label": 0
                },
                {
                    "sent": "I'm assuming that the inner product between the loss vector and any incident vector of your action set is at most one, but you may very well assume that you have a pair of dual norms and you say that the normal of the Los Vectores bus.",
                    "label": 1
                },
                {
                    "sent": "This is bounded by this and the norm of the action.",
                    "label": 0
                },
                {
                    "sent": "The dual norm of the action vectors is bounded by that.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a different assumption and this gives you.",
                    "label": 0
                },
                {
                    "sent": "Different results, I think Sebastian is the right person to really answer all the defined details to that.",
                    "label": 0
                },
                {
                    "sent": "I'm focusing on this specific way of imposing a scaling assumption of the problem, but this is not the only way.",
                    "label": 0
                },
                {
                    "sent": "Can you comment on the connection between this way of.",
                    "label": 1
                },
                {
                    "sent": "Keeping the variance under control and what's called a Dickey Nordyke ellipsoid right are using it with the action spaces.",
                    "label": 0
                },
                {
                    "sent": "There any connection you see me?",
                    "label": 0
                },
                {
                    "sent": "Note, taking ellipsoid is local is a local basis, so this is this is a global basis that holds for the entire action set.",
                    "label": 0
                },
                {
                    "sent": "The Dikin ellipsoid which is used in the online mirror.",
                    "label": 0
                },
                {
                    "sent": "Dissent with self concordant potential functions is gives you a local coordinate system that helps you to control the variance.",
                    "label": 0
                },
                {
                    "sent": "In the vicinity of your current point, inside of the convex Hull.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I was wondering whether.",
                    "label": 0
                },
                {
                    "sent": "It's it's a different algorithm, so it's it's it's.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "No, I know I don't again.",
                    "label": 0
                },
                {
                    "sent": "You're saying OK, that diking knowledge side is the loner ellipsoid for is no, but it is not really becausw there.",
                    "label": 0
                },
                {
                    "sent": "You're based on a different intuition.",
                    "label": 0
                },
                {
                    "sent": "There you are.",
                    "label": 0
                },
                {
                    "sent": "Really, are you really?",
                    "label": 0
                },
                {
                    "sent": "I mean, I view this as a different thing, but maybe maybe I'm wrong.",
                    "label": 0
                },
                {
                    "sent": "Maybe there is some connection, but at least it's not a superficial connection.",
                    "label": 0
                },
                {
                    "sent": "Lois stackexchange.",
                    "label": 0
                },
                {
                    "sent": "Thursdays I wouldn't dare to, you know, with this audience to say anything about speculate about these possibilities, but we can do it privately.",
                    "label": 0
                },
                {
                    "sent": "Yes anymore questions.",
                    "label": 0
                },
                {
                    "sent": "Thanks again.",
                    "label": 0
                }
            ]
        }
    }
}