{
    "id": "j5pyx4xfevdpd636ng4xhewnowbh6zk6",
    "title": "Listwise Approach to Learning to Rank - Theory and Algorithm",
    "info": {
        "author": [
            "Tie-Yan Liu, Microsoft Research"
        ],
        "published": "Aug. 4, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Learning to Rank"
        ]
    },
    "url": "http://videolectures.net/icml08_liu_lalr/",
    "segmentation": [
        [
            "Listwise approach to learning to rank theory and algorithm, and I'm tell you from Microsoft Research Asia.",
            "This is the Co work with my students in Phoenicia and some other colleagues.",
            "And as you can see, this paper is concerned with learning to rank.",
            "Actually, learning to rank has been very hot topic in the research community of machine learning and information retrieval.",
            "And we actually regard ranking as a new machine learning problem in addition to regression and classification differences that you know in ranking the eventual output of the model is ranked list of documents.",
            "So in this scenario, where it is more important to predict are accurate.",
            "Up a partial order or relative order of the documents rather than to predict accurate scores or class label single documents."
        ],
        [
            "So this is a typical framework for learning to rank.",
            "Really, we have two faces.",
            "One day the training phase and the other is a testing phase for the training phase.",
            "We are given a set of training queries.",
            "Each query will have his.",
            "Associated documents and the labels.",
            "For example, four kinds of levels are widely used in the application fields of learning to rank the binary label which specifies which document is relevant or not to the query.",
            "And multivalued descript.",
            "Which further specify the degree of relevance.",
            "For example, highly relevant, partially relevant, irrelevant and pairwise preference which specifies whether document is more relevant than the other.",
            "And it forces a partial order or even total out of the documents which actually specifies the permutation or permission group of a list of document.",
            "So with the training data with the documents and query and labels.",
            "The learning system actually want to learn the model of the ranking.",
            "The ranking model by minimizing a certain loss function.",
            "And for the testing phase, given a new query and all the documents associated with it.",
            "The model is applied to assign the score to each of the documents and then all the documents are sorted.",
            "According to descending order, this course and present it to the user, so this is a whole loop of learning to rank."
        ],
        [
            "And are in recent years actually many papers have been proposed published on this topic.",
            "So we roughly categorize all these words into three approaches.",
            "The pointwise approach, pairwise approach and listwise approach.",
            "I will not talk about details of these approaches, but will briefly mention their basic idea.",
            "For example, the pointwise approach actually want to solve the problem ranking by means of a regression classification or ordinal regression.",
            "And the pairwise approach actually regarded ranking problem as a problem of preference learning, so pairwise classification and some other tools are used here.",
            "And this was approach actually takes the entire set of documents associated associated with the query as a training as a learning instance.",
            "So this algorithm either directed optimized evaluation measure which is defined on the entire set of documents, or minimize a certain recall listwise loss also defined on the entire set of documents.",
            "So I also list some representative algorithms here for each of the approaches."
        ],
        [
            "So far, according to the empirical studies they listwise approach actually capture the ranking problem in a conceptual natural way and it has better performance than the pointwise and pairwise approach approaches on many benchmark datasets.",
            "However, you know the least wise approach actually lacks of the theoretical analysis.",
            "As you can see, some of the pointwise and pairwise approaches can be based on regression or classification.",
            "So a lot of tools can be used to analyze their theoretical properties.",
            "But the list was approach is a little bit different from regression classification, so the corresponding circular analysis is not sufficient."
        ],
        [
            "So this is actually the motivation of this paper.",
            "We want to take this was last minimization as an example to demonstrate how we can analyze the selected properties of this was approach to learning to rank.",
            "Will you are basically talk about the mathematical properties and statistical properties of the last function used in this list weather programs?"
        ],
        [
            "So in order to do that, we need to give a formal description of this ranking.",
            "So as you can see from this slide.",
            "The input is space of less wise ranking.",
            "R. Contains a lot of elements, which is a set of objects to be ranked.",
            "So you can see here the learning instance is a set of documents, but not single documents.",
            "And then the output space Y contains the permutations of these documents.",
            "Now we can define a joint probability distribution pxy of the input and output space.",
            "Also, we need a need to define the hypothesis space H where the ranking model will be selected from.",
            "And with all these definitions, we can easily define the expected loss of the ranking.",
            "Like this?",
            "And because you know Pcy is unknown in practice, so one usually minimize empirical loss which is defined like this."
        ],
        [
            "To make the expected, an empirical loss is really meaningful, one need to define the true loss.",
            "The air in this slide.",
            "So that rules should actually defines the difference between a given ranking list, which is outputted by the ranking model and the ground truth.",
            "Ideally, I think in the many applications, such as information retrieval, the true loss should be cost sensitive.",
            "But you know, for thick of a simplicity in this paper we will start with a 01 loss, which is very simple.",
            "The definition is just like this.",
            "If the model output is exactly.",
            "The same as the ground truth permutation.",
            "We say the loss is 0, otherwise losses one.",
            "So we will leave the discussion on the cost sensitive to loss to our future work, but we have to say it is non trivial because even in the area of information retrieval it is not.",
            "There is not a consensus what a cost sensitive evaluation measure should be.",
            "Many different kind of measures are there."
        ],
        [
            "OK, so here we would like to talk a little bit more about the hypothesis space age.",
            "This is a more related application.",
            "In the large scale, applications such as information retrieval.",
            "For the reason of efficiency, usually we will define the ranking model like this.",
            "As you can see, we will first have a scoring function G. To deal with each of the document and assigns a score to them.",
            "And then assaulting operation will be applied to really get a permutation or the rank list of the documents.",
            "So by considering this kind of.",
            "Ranking model we can rewrite the empirical risk in learning trend like this.",
            "Then it is clear due to the sorting function and 01 loss, this empirical risk is non differentiable, so usually it is very difficult to optimize.",
            "Then when you're level.",
            "Take another approach, apply a surrogate loss function here which is defined directly on the score of the documents and then actually the learning process.",
            "Minimize this circuit loss instead."
        ],
        [
            "So so far up to here we have introduced the framework for this wise ranking and let's see some example.",
            "At.",
            "I said showing in one of my previous previous slide.",
            "There have been some papers which can be characterized into their leastwise approach.",
            "For example the ranco sign paper and the net paper published in SM or last year.",
            "So we can after some very simple analysis we can come up with their surrogate loss function like this.",
            "As you can see, this is a loss function for the rank.",
            "Oh sorry, which we call Kosan Los Discos.",
            "Cosine losses basically defined based on the cosine similarity between the vector of the model output and the vector of the ground truth scores.",
            "Here the precise function is a mapping function which map the ground truth permutation to a set of real value scores.",
            "So this is a trick in that paper.",
            "And also for the list net paper.",
            "Are they also have a?",
            "Mapping function to map the permutation groundtruth permutation to scores.",
            "And in this last function, very famous probabilistic model named Lou's model is used to define the probability distribution over permutations.",
            "And this permutation distribution can be defined for both the model output and for the ground truth in the cross entropy between these two probability distributions is used as a loss function, so this is a basic idea of these two papers.",
            "And actually inspired by the cross entropy loss, it is very natural to define a new loss function, which we call the likelihood loss.",
            "Anne.",
            "The basic idea is once again we use the loose model to define a probability distribution based on the model output.",
            "Then we will check the probability of the given groundtruth permutation of this model.",
            "Then we define the likelihood of the ground truth and then we use the negative log likelihood as loss function.",
            "But minimizing this loss function.",
            "Actually we can maximize the likelihood of the ground truth.",
            "So get the model train.",
            "OK.",
            "So far here we have in total three loss functions which can be really categorized into the framework."
        ],
        [
            "This ranking the next step is that we want to analyze their theoretical properties.",
            "So the thing I want to mention here is that we just use these three loss functions as demonstrations as examples.",
            "The same methodology should be applied to any other this loss functions if there is any.",
            "So basically we will check for properties.",
            "The first one is a mathematical properties of continuity, differentiability and convexity.",
            "And then we will check about the computation efficiency of evaluating this loss function.",
            "The third one is a statistical consistency, and the last one is a soundness.",
            "We will talk about each of them in detail."
        ],
        [
            "For the mathematical properties and computation efficiency, it is quite straightforward to do the analysis.",
            "I just list some of the results here, and as you can see, the cosine loss function, the cross entropy loss, and likelihood.",
            "All these three loss functions are continuous and differentiable.",
            "And then the cost concerns is not convex while the other two that the other two are convex.",
            "And the time complexity of evaluating the cosine loss and the likelihood loss is quite low, which is in the order of a linear which is in the linear order of the number of documents.",
            "Sorry in the in the in the number of documents.",
            "And the value of the time complexity for evaluating the cross entropy loss is quite high, which is is is potentially larger than the other two."
        ],
        [
            "Any for the next property, which is a statistical consistency.",
            "The analysis is not so straightforward, so we need first to define what is statistical consistent is is in this scenario.",
            "Actually, the statistical consistent we want to investigate, we first do that when minimizing the expected circuit loss function.",
            "Is equivalent to minimizing the expected 01 loss.",
            "We say the circuit loss function is consistent.",
            "So in order to analyze this property, we actually prove theory in the paper which specifies two conditions for circuit loss function to be consistent.",
            "The first condition is that is called order preserving, which is simply basically shows that.",
            "It is a property of the permutation space, but not the loss function, so it shows that if the perfect ranking of an object is inherently determined by its own but not other objects, then the space we can see is order preserving.",
            "Actually this assumption has been widely assumed when we use our scoring function in the ranking model.",
            "And the second one is that the loss function should be order sensitive, and this probably actually shows that.",
            "If this condition holds, minimizing the loss loss function.",
            "The socket loss function will be minimized.",
            "Wendy is when sorting the GX, the score of the documents given by the scoring function will result in the same permutation with a given Y, so this is a property of the loss function itself.",
            "OK."
        ],
        [
            "So with this theory, the next step is how to evaluate these three loss functions, whether they they really satisfy the two conditions and with a lot of mathematical proof you can see them in the paper we actually come up with the conclusion that the cosine loss, the cross entropy loss and the likelihood loss all of them are order sensitive.",
            "So we can come to the conclusion all of them are actually statistically consistent.",
            "That is to say.",
            "If we minimize this target loss function, we will always get the optimal solution with respect to the 01 loss."
        ],
        [
            "The last property we want to investigate the soundness.",
            "Basically, this property is related to the reason whether the training process is reasonable.",
            "In other words, whether some of the.",
            "Whether the incorrectly ranking always receive.",
            "Are smaller penalty than the incorrect ranking.",
            "This is a basic idea of the soundness so we can see we will illustrate this property using an example.",
            "So suppose we have only two documents, D1 and D2, and according to the ground truth, D2 should be ranked before D1.",
            "Then suppose G1 and G2.",
            "Other scores of these two documents output it by the ranking model.",
            "Then we can come up with these two D2 dimensional space defined by G1G2.",
            "And here's the right point, is the ground truth.",
            "Please note that the position of the ground truth in the space depends on the mapping function, because the original ground truth is a permutation and we use the mapping function to map it to the scores, so different mapping function will lead to different position of the right point.",
            "And this is blue.",
            "Point is the model output.",
            "So having this setting we can get this curve.",
            "With the change of the model output, how the loss function will change.",
            "So based on this figure, we can easily see that.",
            "For causing loss some of the correct ranking actually here.",
            "Receive even larger penalty than the incorrect ranking key.",
            "Then we can do the based on this we can say that the cross entropy loss of the cause."
        ],
        [
            "Houses now, so some.",
            "And then we can do the same analysis for the cross entropy loss and once again the result is that some of the incorrect ranking will receive smaller penalties, incorrect ranking, so it is not so so."
        ],
        [
            "And this is for the likelihood loss.",
            "Very lucky this time we got a very good monotonically decreasing function, so always the correct ranking will receive smaller penalties and incorrect ranking."
        ],
        [
            "OK, based on this analysis we can have the discussion first or the three loss functions can be easily minimized because they are continuous and differentiable.",
            "And but the number of training samples is very large.",
            "The model learning can all be effective because all those three loss functions are actually statistical consistent.",
            "And then the cross entropy loss and causing loss are not as soon as the likelihood loss.",
            "And you know the cross entropy loss is a little bit sensitive to the initial sighting of the minimization process because it is non convex.",
            "So we will conduct experiments to validate all these."
        ],
        [
            "Conclusions.",
            "In here, so we use both synthetic data in real data to do."
        ],
        [
            "The analysis.",
            "An in photosynthetic synthetic data.",
            "We will check different kind of mapping functions because you know when we analysis the soundness of the cross entropy loss and consent laws, we have notice that the mapping function will play an important role.",
            "And also we have conducted experiments for 50 times and we compute the mean an average of the performance so we can see whether they will be sensitive to the initial setting.",
            "So you can see here.",
            "But listen that and run cosine will be very sensitive to the different manufacture used and the variance of the cross.",
            "Much larger than the other two.",
            "This is very important with."
        ],
        [
            "For analysis.",
            "And this is a experiment results on the real data and we can see the performance of the list Emery which correspond to the likelihood loss is significantly better than the other two algorithms, which is also included."
        ],
        [
            "OK, so this is a conclusion and future work of the paper.",
            "Basically, in this paper we analyzed the leastwise approach to learning to rank from the last function point of view.",
            "We've been investigated for different kind of properties of it.",
            "And the conclusion is that the likelihood loss seems to be the best among all the loss functions under investigation.",
            "And for the future work we will.",
            "Also investigate the rate of convergence and generalization ability of these.",
            "Learning to rank algorithms and also as I mentioned before, we use a 01 loss for very simple reason in this paper and we investigate the possibility of using a cost sensitive loss in the future."
        ],
        [
            "OK, thank you very much.",
            "That's it.",
            "Probably.",
            "So if you change your loss to something, that's still pretty simple.",
            "Something like precision at 10 from so for example I, I'm happy with what the results are in the top ten of the ranking.",
            "Do you think the view will change much from what you presented?",
            "OK, that's good.",
            "So first of all, I think Precision 10 might be might not be a very good true loss for ranking, but the point is you mentioned is very interesting.",
            "Maybe we can only look at the top ten of each permutation and find the true loss.",
            "We actually have done this this work.",
            "We define the true loss not as a 01 on based on the permutation, but 01 loss based on the top K permutation subgroup and all the conclusions hold for the same.",
            "OK, I love Alaska, rather rather vague question which I think I asked quite often in learning to rank problems so you didn't tell us much about how the ground truth ranking was obtained, and I'd be quite curious to hear.",
            "I mean, it's a very, you know.",
            "I'm not expecting a complete answer, but it would be quite interesting to hear what your perspectives are on how to obtain and what the aims are in terms of obtaining the ground truth.",
            "OK, so."
        ],
        [
            "Yeah, as I mentioned in this slide, you know you can see there are four types of ground truth, and usually we do the research.",
            "We can easily get the 1st two types of ground truth by asking human to label.",
            "And this is a common practice in the literature of IR.",
            "For example, we have a track conference in the track.",
            "The participants will be asked to label some of the results on auto contribute some result, and then we can ask some human labeler to label them.",
            "So by asking human laborers to help, we can easily get the binary and multivalued discrete ground truth.",
            "But it is of course much costly.",
            "And for the partial, the pairwise preferences and even partial order total order of the documents.",
            "Usually we can leverage the log data in the search engine, so user will really have a preference which document is more relevant than the other.",
            "So if I do the log money, we can come up with these kind of ground truth.",
            "But this is itself is also a research topic, so it is not determined which one.",
            "Which technology is the best to get these results?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Listwise approach to learning to rank theory and algorithm, and I'm tell you from Microsoft Research Asia.",
                    "label": 1
                },
                {
                    "sent": "This is the Co work with my students in Phoenicia and some other colleagues.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, this paper is concerned with learning to rank.",
                    "label": 0
                },
                {
                    "sent": "Actually, learning to rank has been very hot topic in the research community of machine learning and information retrieval.",
                    "label": 0
                },
                {
                    "sent": "And we actually regard ranking as a new machine learning problem in addition to regression and classification differences that you know in ranking the eventual output of the model is ranked list of documents.",
                    "label": 0
                },
                {
                    "sent": "So in this scenario, where it is more important to predict are accurate.",
                    "label": 0
                },
                {
                    "sent": "Up a partial order or relative order of the documents rather than to predict accurate scores or class label single documents.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a typical framework for learning to rank.",
                    "label": 1
                },
                {
                    "sent": "Really, we have two faces.",
                    "label": 0
                },
                {
                    "sent": "One day the training phase and the other is a testing phase for the training phase.",
                    "label": 0
                },
                {
                    "sent": "We are given a set of training queries.",
                    "label": 0
                },
                {
                    "sent": "Each query will have his.",
                    "label": 0
                },
                {
                    "sent": "Associated documents and the labels.",
                    "label": 0
                },
                {
                    "sent": "For example, four kinds of levels are widely used in the application fields of learning to rank the binary label which specifies which document is relevant or not to the query.",
                    "label": 0
                },
                {
                    "sent": "And multivalued descript.",
                    "label": 0
                },
                {
                    "sent": "Which further specify the degree of relevance.",
                    "label": 1
                },
                {
                    "sent": "For example, highly relevant, partially relevant, irrelevant and pairwise preference which specifies whether document is more relevant than the other.",
                    "label": 0
                },
                {
                    "sent": "And it forces a partial order or even total out of the documents which actually specifies the permutation or permission group of a list of document.",
                    "label": 1
                },
                {
                    "sent": "So with the training data with the documents and query and labels.",
                    "label": 0
                },
                {
                    "sent": "The learning system actually want to learn the model of the ranking.",
                    "label": 0
                },
                {
                    "sent": "The ranking model by minimizing a certain loss function.",
                    "label": 0
                },
                {
                    "sent": "And for the testing phase, given a new query and all the documents associated with it.",
                    "label": 0
                },
                {
                    "sent": "The model is applied to assign the score to each of the documents and then all the documents are sorted.",
                    "label": 0
                },
                {
                    "sent": "According to descending order, this course and present it to the user, so this is a whole loop of learning to rank.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And are in recent years actually many papers have been proposed published on this topic.",
                    "label": 0
                },
                {
                    "sent": "So we roughly categorize all these words into three approaches.",
                    "label": 0
                },
                {
                    "sent": "The pointwise approach, pairwise approach and listwise approach.",
                    "label": 0
                },
                {
                    "sent": "I will not talk about details of these approaches, but will briefly mention their basic idea.",
                    "label": 0
                },
                {
                    "sent": "For example, the pointwise approach actually want to solve the problem ranking by means of a regression classification or ordinal regression.",
                    "label": 0
                },
                {
                    "sent": "And the pairwise approach actually regarded ranking problem as a problem of preference learning, so pairwise classification and some other tools are used here.",
                    "label": 0
                },
                {
                    "sent": "And this was approach actually takes the entire set of documents associated associated with the query as a training as a learning instance.",
                    "label": 1
                },
                {
                    "sent": "So this algorithm either directed optimized evaluation measure which is defined on the entire set of documents, or minimize a certain recall listwise loss also defined on the entire set of documents.",
                    "label": 0
                },
                {
                    "sent": "So I also list some representative algorithms here for each of the approaches.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So far, according to the empirical studies they listwise approach actually capture the ranking problem in a conceptual natural way and it has better performance than the pointwise and pairwise approach approaches on many benchmark datasets.",
                    "label": 1
                },
                {
                    "sent": "However, you know the least wise approach actually lacks of the theoretical analysis.",
                    "label": 0
                },
                {
                    "sent": "As you can see, some of the pointwise and pairwise approaches can be based on regression or classification.",
                    "label": 0
                },
                {
                    "sent": "So a lot of tools can be used to analyze their theoretical properties.",
                    "label": 0
                },
                {
                    "sent": "But the list was approach is a little bit different from regression classification, so the corresponding circular analysis is not sufficient.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is actually the motivation of this paper.",
                    "label": 0
                },
                {
                    "sent": "We want to take this was last minimization as an example to demonstrate how we can analyze the selected properties of this was approach to learning to rank.",
                    "label": 1
                },
                {
                    "sent": "Will you are basically talk about the mathematical properties and statistical properties of the last function used in this list weather programs?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in order to do that, we need to give a formal description of this ranking.",
                    "label": 0
                },
                {
                    "sent": "So as you can see from this slide.",
                    "label": 0
                },
                {
                    "sent": "The input is space of less wise ranking.",
                    "label": 0
                },
                {
                    "sent": "R. Contains a lot of elements, which is a set of objects to be ranked.",
                    "label": 1
                },
                {
                    "sent": "So you can see here the learning instance is a set of documents, but not single documents.",
                    "label": 1
                },
                {
                    "sent": "And then the output space Y contains the permutations of these documents.",
                    "label": 1
                },
                {
                    "sent": "Now we can define a joint probability distribution pxy of the input and output space.",
                    "label": 0
                },
                {
                    "sent": "Also, we need a need to define the hypothesis space H where the ranking model will be selected from.",
                    "label": 0
                },
                {
                    "sent": "And with all these definitions, we can easily define the expected loss of the ranking.",
                    "label": 0
                },
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "And because you know Pcy is unknown in practice, so one usually minimize empirical loss which is defined like this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To make the expected, an empirical loss is really meaningful, one need to define the true loss.",
                    "label": 1
                },
                {
                    "sent": "The air in this slide.",
                    "label": 0
                },
                {
                    "sent": "So that rules should actually defines the difference between a given ranking list, which is outputted by the ranking model and the ground truth.",
                    "label": 1
                },
                {
                    "sent": "Ideally, I think in the many applications, such as information retrieval, the true loss should be cost sensitive.",
                    "label": 0
                },
                {
                    "sent": "But you know, for thick of a simplicity in this paper we will start with a 01 loss, which is very simple.",
                    "label": 0
                },
                {
                    "sent": "The definition is just like this.",
                    "label": 0
                },
                {
                    "sent": "If the model output is exactly.",
                    "label": 0
                },
                {
                    "sent": "The same as the ground truth permutation.",
                    "label": 0
                },
                {
                    "sent": "We say the loss is 0, otherwise losses one.",
                    "label": 0
                },
                {
                    "sent": "So we will leave the discussion on the cost sensitive to loss to our future work, but we have to say it is non trivial because even in the area of information retrieval it is not.",
                    "label": 0
                },
                {
                    "sent": "There is not a consensus what a cost sensitive evaluation measure should be.",
                    "label": 0
                },
                {
                    "sent": "Many different kind of measures are there.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here we would like to talk a little bit more about the hypothesis space age.",
                    "label": 0
                },
                {
                    "sent": "This is a more related application.",
                    "label": 0
                },
                {
                    "sent": "In the large scale, applications such as information retrieval.",
                    "label": 0
                },
                {
                    "sent": "For the reason of efficiency, usually we will define the ranking model like this.",
                    "label": 0
                },
                {
                    "sent": "As you can see, we will first have a scoring function G. To deal with each of the document and assigns a score to them.",
                    "label": 0
                },
                {
                    "sent": "And then assaulting operation will be applied to really get a permutation or the rank list of the documents.",
                    "label": 0
                },
                {
                    "sent": "So by considering this kind of.",
                    "label": 0
                },
                {
                    "sent": "Ranking model we can rewrite the empirical risk in learning trend like this.",
                    "label": 0
                },
                {
                    "sent": "Then it is clear due to the sorting function and 01 loss, this empirical risk is non differentiable, so usually it is very difficult to optimize.",
                    "label": 1
                },
                {
                    "sent": "Then when you're level.",
                    "label": 0
                },
                {
                    "sent": "Take another approach, apply a surrogate loss function here which is defined directly on the score of the documents and then actually the learning process.",
                    "label": 0
                },
                {
                    "sent": "Minimize this circuit loss instead.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so far up to here we have introduced the framework for this wise ranking and let's see some example.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "I said showing in one of my previous previous slide.",
                    "label": 0
                },
                {
                    "sent": "There have been some papers which can be characterized into their leastwise approach.",
                    "label": 0
                },
                {
                    "sent": "For example the ranco sign paper and the net paper published in SM or last year.",
                    "label": 0
                },
                {
                    "sent": "So we can after some very simple analysis we can come up with their surrogate loss function like this.",
                    "label": 0
                },
                {
                    "sent": "As you can see, this is a loss function for the rank.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, which we call Kosan Los Discos.",
                    "label": 0
                },
                {
                    "sent": "Cosine losses basically defined based on the cosine similarity between the vector of the model output and the vector of the ground truth scores.",
                    "label": 0
                },
                {
                    "sent": "Here the precise function is a mapping function which map the ground truth permutation to a set of real value scores.",
                    "label": 0
                },
                {
                    "sent": "So this is a trick in that paper.",
                    "label": 0
                },
                {
                    "sent": "And also for the list net paper.",
                    "label": 0
                },
                {
                    "sent": "Are they also have a?",
                    "label": 0
                },
                {
                    "sent": "Mapping function to map the permutation groundtruth permutation to scores.",
                    "label": 0
                },
                {
                    "sent": "And in this last function, very famous probabilistic model named Lou's model is used to define the probability distribution over permutations.",
                    "label": 0
                },
                {
                    "sent": "And this permutation distribution can be defined for both the model output and for the ground truth in the cross entropy between these two probability distributions is used as a loss function, so this is a basic idea of these two papers.",
                    "label": 0
                },
                {
                    "sent": "And actually inspired by the cross entropy loss, it is very natural to define a new loss function, which we call the likelihood loss.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is once again we use the loose model to define a probability distribution based on the model output.",
                    "label": 0
                },
                {
                    "sent": "Then we will check the probability of the given groundtruth permutation of this model.",
                    "label": 0
                },
                {
                    "sent": "Then we define the likelihood of the ground truth and then we use the negative log likelihood as loss function.",
                    "label": 0
                },
                {
                    "sent": "But minimizing this loss function.",
                    "label": 0
                },
                {
                    "sent": "Actually we can maximize the likelihood of the ground truth.",
                    "label": 0
                },
                {
                    "sent": "So get the model train.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 1
                },
                {
                    "sent": "So far here we have in total three loss functions which can be really categorized into the framework.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This ranking the next step is that we want to analyze their theoretical properties.",
                    "label": 0
                },
                {
                    "sent": "So the thing I want to mention here is that we just use these three loss functions as demonstrations as examples.",
                    "label": 0
                },
                {
                    "sent": "The same methodology should be applied to any other this loss functions if there is any.",
                    "label": 0
                },
                {
                    "sent": "So basically we will check for properties.",
                    "label": 0
                },
                {
                    "sent": "The first one is a mathematical properties of continuity, differentiability and convexity.",
                    "label": 1
                },
                {
                    "sent": "And then we will check about the computation efficiency of evaluating this loss function.",
                    "label": 0
                },
                {
                    "sent": "The third one is a statistical consistency, and the last one is a soundness.",
                    "label": 0
                },
                {
                    "sent": "We will talk about each of them in detail.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the mathematical properties and computation efficiency, it is quite straightforward to do the analysis.",
                    "label": 0
                },
                {
                    "sent": "I just list some of the results here, and as you can see, the cosine loss function, the cross entropy loss, and likelihood.",
                    "label": 0
                },
                {
                    "sent": "All these three loss functions are continuous and differentiable.",
                    "label": 0
                },
                {
                    "sent": "And then the cost concerns is not convex while the other two that the other two are convex.",
                    "label": 0
                },
                {
                    "sent": "And the time complexity of evaluating the cosine loss and the likelihood loss is quite low, which is in the order of a linear which is in the linear order of the number of documents.",
                    "label": 1
                },
                {
                    "sent": "Sorry in the in the in the number of documents.",
                    "label": 0
                },
                {
                    "sent": "And the value of the time complexity for evaluating the cross entropy loss is quite high, which is is is potentially larger than the other two.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Any for the next property, which is a statistical consistency.",
                    "label": 0
                },
                {
                    "sent": "The analysis is not so straightforward, so we need first to define what is statistical consistent is is in this scenario.",
                    "label": 0
                },
                {
                    "sent": "Actually, the statistical consistent we want to investigate, we first do that when minimizing the expected circuit loss function.",
                    "label": 0
                },
                {
                    "sent": "Is equivalent to minimizing the expected 01 loss.",
                    "label": 1
                },
                {
                    "sent": "We say the circuit loss function is consistent.",
                    "label": 0
                },
                {
                    "sent": "So in order to analyze this property, we actually prove theory in the paper which specifies two conditions for circuit loss function to be consistent.",
                    "label": 0
                },
                {
                    "sent": "The first condition is that is called order preserving, which is simply basically shows that.",
                    "label": 0
                },
                {
                    "sent": "It is a property of the permutation space, but not the loss function, so it shows that if the perfect ranking of an object is inherently determined by its own but not other objects, then the space we can see is order preserving.",
                    "label": 1
                },
                {
                    "sent": "Actually this assumption has been widely assumed when we use our scoring function in the ranking model.",
                    "label": 0
                },
                {
                    "sent": "And the second one is that the loss function should be order sensitive, and this probably actually shows that.",
                    "label": 1
                },
                {
                    "sent": "If this condition holds, minimizing the loss loss function.",
                    "label": 0
                },
                {
                    "sent": "The socket loss function will be minimized.",
                    "label": 0
                },
                {
                    "sent": "Wendy is when sorting the GX, the score of the documents given by the scoring function will result in the same permutation with a given Y, so this is a property of the loss function itself.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with this theory, the next step is how to evaluate these three loss functions, whether they they really satisfy the two conditions and with a lot of mathematical proof you can see them in the paper we actually come up with the conclusion that the cosine loss, the cross entropy loss and the likelihood loss all of them are order sensitive.",
                    "label": 1
                },
                {
                    "sent": "So we can come to the conclusion all of them are actually statistically consistent.",
                    "label": 0
                },
                {
                    "sent": "That is to say.",
                    "label": 0
                },
                {
                    "sent": "If we minimize this target loss function, we will always get the optimal solution with respect to the 01 loss.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The last property we want to investigate the soundness.",
                    "label": 0
                },
                {
                    "sent": "Basically, this property is related to the reason whether the training process is reasonable.",
                    "label": 0
                },
                {
                    "sent": "In other words, whether some of the.",
                    "label": 0
                },
                {
                    "sent": "Whether the incorrectly ranking always receive.",
                    "label": 0
                },
                {
                    "sent": "Are smaller penalty than the incorrect ranking.",
                    "label": 1
                },
                {
                    "sent": "This is a basic idea of the soundness so we can see we will illustrate this property using an example.",
                    "label": 0
                },
                {
                    "sent": "So suppose we have only two documents, D1 and D2, and according to the ground truth, D2 should be ranked before D1.",
                    "label": 1
                },
                {
                    "sent": "Then suppose G1 and G2.",
                    "label": 0
                },
                {
                    "sent": "Other scores of these two documents output it by the ranking model.",
                    "label": 0
                },
                {
                    "sent": "Then we can come up with these two D2 dimensional space defined by G1G2.",
                    "label": 0
                },
                {
                    "sent": "And here's the right point, is the ground truth.",
                    "label": 0
                },
                {
                    "sent": "Please note that the position of the ground truth in the space depends on the mapping function, because the original ground truth is a permutation and we use the mapping function to map it to the scores, so different mapping function will lead to different position of the right point.",
                    "label": 0
                },
                {
                    "sent": "And this is blue.",
                    "label": 0
                },
                {
                    "sent": "Point is the model output.",
                    "label": 0
                },
                {
                    "sent": "So having this setting we can get this curve.",
                    "label": 0
                },
                {
                    "sent": "With the change of the model output, how the loss function will change.",
                    "label": 1
                },
                {
                    "sent": "So based on this figure, we can easily see that.",
                    "label": 0
                },
                {
                    "sent": "For causing loss some of the correct ranking actually here.",
                    "label": 0
                },
                {
                    "sent": "Receive even larger penalty than the incorrect ranking key.",
                    "label": 0
                },
                {
                    "sent": "Then we can do the based on this we can say that the cross entropy loss of the cause.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Houses now, so some.",
                    "label": 0
                },
                {
                    "sent": "And then we can do the same analysis for the cross entropy loss and once again the result is that some of the incorrect ranking will receive smaller penalties, incorrect ranking, so it is not so so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is for the likelihood loss.",
                    "label": 0
                },
                {
                    "sent": "Very lucky this time we got a very good monotonically decreasing function, so always the correct ranking will receive smaller penalties and incorrect ranking.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, based on this analysis we can have the discussion first or the three loss functions can be easily minimized because they are continuous and differentiable.",
                    "label": 0
                },
                {
                    "sent": "And but the number of training samples is very large.",
                    "label": 1
                },
                {
                    "sent": "The model learning can all be effective because all those three loss functions are actually statistical consistent.",
                    "label": 1
                },
                {
                    "sent": "And then the cross entropy loss and causing loss are not as soon as the likelihood loss.",
                    "label": 0
                },
                {
                    "sent": "And you know the cross entropy loss is a little bit sensitive to the initial sighting of the minimization process because it is non convex.",
                    "label": 0
                },
                {
                    "sent": "So we will conduct experiments to validate all these.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conclusions.",
                    "label": 0
                },
                {
                    "sent": "In here, so we use both synthetic data in real data to do.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The analysis.",
                    "label": 0
                },
                {
                    "sent": "An in photosynthetic synthetic data.",
                    "label": 1
                },
                {
                    "sent": "We will check different kind of mapping functions because you know when we analysis the soundness of the cross entropy loss and consent laws, we have notice that the mapping function will play an important role.",
                    "label": 0
                },
                {
                    "sent": "And also we have conducted experiments for 50 times and we compute the mean an average of the performance so we can see whether they will be sensitive to the initial setting.",
                    "label": 0
                },
                {
                    "sent": "So you can see here.",
                    "label": 0
                },
                {
                    "sent": "But listen that and run cosine will be very sensitive to the different manufacture used and the variance of the cross.",
                    "label": 0
                },
                {
                    "sent": "Much larger than the other two.",
                    "label": 0
                },
                {
                    "sent": "This is very important with.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For analysis.",
                    "label": 0
                },
                {
                    "sent": "And this is a experiment results on the real data and we can see the performance of the list Emery which correspond to the likelihood loss is significantly better than the other two algorithms, which is also included.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is a conclusion and future work of the paper.",
                    "label": 1
                },
                {
                    "sent": "Basically, in this paper we analyzed the leastwise approach to learning to rank from the last function point of view.",
                    "label": 0
                },
                {
                    "sent": "We've been investigated for different kind of properties of it.",
                    "label": 0
                },
                {
                    "sent": "And the conclusion is that the likelihood loss seems to be the best among all the loss functions under investigation.",
                    "label": 1
                },
                {
                    "sent": "And for the future work we will.",
                    "label": 1
                },
                {
                    "sent": "Also investigate the rate of convergence and generalization ability of these.",
                    "label": 0
                },
                {
                    "sent": "Learning to rank algorithms and also as I mentioned before, we use a 01 loss for very simple reason in this paper and we investigate the possibility of using a cost sensitive loss in the future.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "Probably.",
                    "label": 0
                },
                {
                    "sent": "So if you change your loss to something, that's still pretty simple.",
                    "label": 0
                },
                {
                    "sent": "Something like precision at 10 from so for example I, I'm happy with what the results are in the top ten of the ranking.",
                    "label": 0
                },
                {
                    "sent": "Do you think the view will change much from what you presented?",
                    "label": 0
                },
                {
                    "sent": "OK, that's good.",
                    "label": 0
                },
                {
                    "sent": "So first of all, I think Precision 10 might be might not be a very good true loss for ranking, but the point is you mentioned is very interesting.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can only look at the top ten of each permutation and find the true loss.",
                    "label": 0
                },
                {
                    "sent": "We actually have done this this work.",
                    "label": 0
                },
                {
                    "sent": "We define the true loss not as a 01 on based on the permutation, but 01 loss based on the top K permutation subgroup and all the conclusions hold for the same.",
                    "label": 0
                },
                {
                    "sent": "OK, I love Alaska, rather rather vague question which I think I asked quite often in learning to rank problems so you didn't tell us much about how the ground truth ranking was obtained, and I'd be quite curious to hear.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's a very, you know.",
                    "label": 0
                },
                {
                    "sent": "I'm not expecting a complete answer, but it would be quite interesting to hear what your perspectives are on how to obtain and what the aims are in terms of obtaining the ground truth.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, as I mentioned in this slide, you know you can see there are four types of ground truth, and usually we do the research.",
                    "label": 0
                },
                {
                    "sent": "We can easily get the 1st two types of ground truth by asking human to label.",
                    "label": 0
                },
                {
                    "sent": "And this is a common practice in the literature of IR.",
                    "label": 0
                },
                {
                    "sent": "For example, we have a track conference in the track.",
                    "label": 0
                },
                {
                    "sent": "The participants will be asked to label some of the results on auto contribute some result, and then we can ask some human labeler to label them.",
                    "label": 0
                },
                {
                    "sent": "So by asking human laborers to help, we can easily get the binary and multivalued discrete ground truth.",
                    "label": 1
                },
                {
                    "sent": "But it is of course much costly.",
                    "label": 0
                },
                {
                    "sent": "And for the partial, the pairwise preferences and even partial order total order of the documents.",
                    "label": 1
                },
                {
                    "sent": "Usually we can leverage the log data in the search engine, so user will really have a preference which document is more relevant than the other.",
                    "label": 0
                },
                {
                    "sent": "So if I do the log money, we can come up with these kind of ground truth.",
                    "label": 0
                },
                {
                    "sent": "But this is itself is also a research topic, so it is not determined which one.",
                    "label": 0
                },
                {
                    "sent": "Which technology is the best to get these results?",
                    "label": 0
                }
            ]
        }
    }
}