{
    "id": "znst2hbvezyg7fsrit3bewycjl7fxts7",
    "title": "Tracking Multiple Simultaneous Speakers with Probabilistic Data Associaton Filters",
    "info": {
        "coauthor": [
            "Tobias Gehrig, University of Karlsruhe"
        ],
        "author": [
            "John McDonough, Interactive Systems Labs (ISL), University of Karlsruhe"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "May 2006",
        "category": [
            "Top->Computer Science->Speech Analysis"
        ]
    },
    "url": "http://videolectures.net/mlmi06_gehrig_tmssp/",
    "segmentation": [
        [
            "OK, so the next talk in this session is a provided by skate and drama, and it's called tracking multiple simultaneous speakers with probabilistic data Association filters and the talk is going to be given by Jonathan.",
            "OK, thank you Steve.",
            "OK so I was a little bit surprised to learn that up mark paper landed in the speaker diarization and tracking task.",
            "We do speaker tracking but not quite kind of tracking that.",
            "Perhaps most of you are thinking of because we track speakers in space and time, not just in time.",
            "OK.",
            "So I was struck by something that Xavier said.",
            "He said that typically when they do speaker diarization, they start with a signal.",
            "That sort of combined signal for all speakers.",
            "OK, that's the exact opposite of what we are striving towards.",
            "But this work in our estetik work, because what we'd like to come up with finally is multiple signals, and each signal corresponds to exactly 1 speaker.",
            "OK, and we want to run our STT engine's on each of those signals and transcribe exactly with that.",
            "One speaker says and attributes that speaker, OK.",
            "So today I'm going to talk about the first part of that problem, finding out when and where a speaker spoke."
        ],
        [
            "OK, so to go through some of the preliminaries here, we start with the time delay of arrival and time to leave.",
            "Arrival is very simple concept if you know where a speaker is.",
            "So here's my don't worry.",
            "Be happy speaker and where two microphones are for instance.",
            "You know the propagation delay between the speaker and the microphone, and you can figure out how long it takes with the time differences between the arrival of the signal from the speaker at the first microphone and at the second microphone.",
            "It's simply the different differences in distance is divided by the speed of sound.",
            "It's quite simple.",
            "OK."
        ],
        [
            "So a simple way of estimating the position of a speaker in three dimensional space is to say, I believe, that speaker is where at that position that minimizes this sort of difference between my expected time delay of arrival and my observed time delay of arrival.",
            "Overall microphone pairs.",
            "So if I have a microphone array, an array of several microphones, then I can easily form all a set of unique microphone pairs.",
            "So if I sum this weighted time delay of arrival.",
            "Overall, such microphone pairs.",
            "I simply say my speaker isn't that location that minimizes this.",
            "This weighted sum or this weighted squared sum?",
            "Yeah OK, how do I measure at the time delay of arrival between two pairs of microphones or two 2 microphones?",
            "Well, the most common technique is to use something called a generalized cross correlation.",
            "I won't go into a lot of details here because people who do source localization know exactly what this means.",
            "For those that don't do source localization is essentially a cross correlation between two signals, and as with the regular cross correlation, what you do is look for the maximum for the peak in this cross correlation, and you say, I believe that the time delay between those two signals.",
            "Corresponds to the peak in the cross correlation."
        ],
        [
            "OK.",
            "So as I mentioned, it's easy to formulate a criterion for source localization, namely this.",
            "OK, there's only one catch here.",
            "If you recall this expected time delay of arrival is nonlinear.",
            "OK, this is clearly nonlinear because you have these two norm terms alright, which involves squares and square roots.",
            "It's nonlinear, OK.",
            "So what you can do to get around that other part from that is very nice constrained problem because you have a least squares regression problem.",
            "OK, apart from this non linearity.",
            "So what you can do to get around that nonlinearity, of course linearize the the time, believe arrival function about the current operating points and if you do that you get this really simple term or this really simple criterion here OK?",
            "Now this is nice because again, this is a least squares criterion, and it's linearized.",
            "OK, so this sort of function is very amenable to solution, But the Kalman filter OK, so the reason that you would like to introduce Kalman filter is because prior estimates of where the speaker is are probably useful for finding out where the speaker is currently.",
            "OK, because you can't expect your speaker to move instantaneously from place to place, and you typically don't have speakers that are Chinese acrobats and so forth so.",
            "You can usually use the prior information to find out where he currently is.",
            "OK."
        ],
        [
            "Alright, so now we come to the little bit of math here.",
            "I tried to minimize the amount of math, but unfortunately it's difficult to talk about Kalman filters without speaking about the way common filters work.",
            "OK, so this is a little bit bewildering.",
            "Perhaps the first time you've seen it, but I'm going to try to make it as intuitive as possible, OK?",
            "So you start with the common filter cabin filter is governed by two equations.",
            "You have your process equation and your observation equation.",
            "OK, so first of all, the first part of the common filter is always the state.",
            "So in our case the state is of course the speaker's position in three space.",
            "The second part is the observation, and in our case the observations are going to be the setup time delays or a vector of time delays.",
            "Alright, so you have two equations.",
            "The first one governs how the state changes in time and the second governs how the state Maps to observations.",
            "OK, so at this point both all of these things are pretty much clear, because we've already specified how the state Maps to observations, namely, it's through this time delay of arrival equation.",
            "OK, so we need really need specifies how the state evolves in time, and you can make a number of simple, very simple assumptions about how the speaker moves.",
            "Namely, you can assume that the stationary that he moves with constant.",
            "LA city etc.",
            "So with each of these components there is a error term.",
            "Alright, so you don't expect your evolution of your state correspond exactly to the wet your model.",
            "So in order to compensate for that non correspondence, if you will, you introduce these error terms V1 and V2 and these are known as the state and processed noises alright, so typically in a common filter you don't assume you know the state and process noise, but you know their second order statistics, namely, you know their correlation values OK. Alright."
        ],
        [
            "And this is just a little bit of math.",
            "Hope we get through this point and then it should go clearer.",
            "Alright, so to form the update of a common filter, you have to have you first start off by defining two estimates of the state.",
            "OK, the first estimate is called the so-called predicted estimate.",
            "So the predicted estimate is based on all observations excluding the current observation.",
            "Alright, the filtered estimate, on the other hand, is based on all observations, including the current observations, so that seems like it's pretty small distinction, but it turns out to have consequences in the way you formulate the Kalman filter, right?",
            "So next thing you have to define is the so-called predicted observation.",
            "So if you know if you have your predicted state here.",
            "You should be able to guess because you know the form of CIE. You know the time delay of arrival function.",
            "You should be able to predict what time delay of arrival you are actually going to see.",
            "Alright, that's called the predicted observation.",
            "Then the innovation is the difference between what you predicted you would see and what you actually did see.",
            "OK, this is called the innovation, because it's exactly this term that contains the new information.",
            "OK, the new information that you got from the observation.",
            "Because most of the most of the information contained in the observation is not new, only this difference is new.",
            "OK. Alright, so then, in order to have extended Kalman filter as opposed to regular common filter, by the way, extended Kalman filter is useful for solving nonlinear problems like we have here.",
            "All we have to do is take our non linear time delay arrival equation and linearize it about the current operating point, namely about the current predicted state estimate."
        ],
        [
            "OK.",
            "Couple more things that we have to define.",
            "1st is called the correlation matrix.",
            "Of the innovation sequence, and this is simply a correlation matrix of our innovations.",
            "And again our innovations.",
            "Is this difference between predicted and true actual and predicted observation?",
            "It turns out we can.",
            "We can calculate this correlation matrix very simply.",
            "OK through this equation.",
            "And the terms that appear in the correlation match, or rather the correlation matrix of innovation sequence, are to first of all we have this QAT.",
            "Which is the error in our observations.",
            "The second thing we have is this so called predicted state error correlation matrix.",
            "OK, so this predicted state our correlation matrix tells us how uncertain we are about our estimate of the speakers position.",
            "So this correlation matrix is very large.",
            "We are very uncertain where the speaker is.",
            "If we get small and we're fairly certain where he is OK.",
            "So you notice that this equation is fairly intuitive.",
            "It says that the correlation matrix of the innovation sequence has two components.",
            "As I mentioned, the first is determined by our uncertainty from the state.",
            "The second is determined by our uncertainty of the actual observations.",
            "OK, the sum of those two is the innovation correlation matrix.",
            "Alright.",
            "The common gain, on the other hand.",
            "Is now defined as such.",
            "Right now at the moment, what?",
            "We'll see why it's useful to find this thing called the Kalman gain.",
            "But the common gain tells us how we're going to fold the information from the latest observation into our state estimate.",
            "OK. Alright."
        ],
        [
            "So this common gain is the most important quantity we have here, and you notice to calculate the common gain.",
            "We have to know this correlation matrix of the predicted state error.",
            "Alright, where do we get the correlation matrix of the dictator?",
            "Well, this comes from the so-called Ricotti equation.",
            "OK, the Ricardi equation tells us how to update the uncertainty matrix of our state estimate.",
            "OK, here it is right here.",
            "Alright, and you notice here in our mercati equation, we're looking at both the correlation matrix of the filtered state error.",
            "As well as the predicted state error.",
            "So this is the correlation matrix of the filter state error.",
            "This is the correlation matrix of the predicted state error.",
            "Notice the primary difference between those two.",
            "Is this Q one term and again this Q one term is the correlation matrix of our.",
            "Process noise.",
            "OK."
        ],
        [
            "So that was the math part.",
            "Let me now try to explain more intuitively what's happening here.",
            "Here is our state.",
            "Again, this is our position in three space of our speaker.",
            "This matrix here the smaller ellipse is meant to be is meant to represent the covariance matrix of our filtered state error.",
            "The slightly larger ellipse is the correlation matrix of our predicted state error.",
            "So why is it larger?",
            "Well, because you have this extra term here, you're essentially taking the.",
            "Filtered state error covariance matrix and adding it to adding to it this term here makes it larger.",
            "Now this up here, this little ellipse up here is meant to represent the covariance matrix of our innovation, OK?",
            "So you notice where we expect the innovation this dot here.",
            "Is we obtain from the state estimate through our non linearity through our functional or through our time to labor rival function?",
            "So this function tells us based on the value of the state.",
            "What's the observation should be?",
            "The difference between what we have as an observation.",
            "And what we thought the observation should be is our innovation.",
            "Alpha at see OK, and this ellipse is meant to represent the covariance matrix of the innovation."
        ],
        [
            "OK, alright, so given all those preliminaries.",
            "We're now ready to update our estimates of the.",
            "Position of the speaker.",
            "Alright, so in order to update the position of the speaker, we only need to do 2 steps.",
            "The first step is we take our prior estimate of speakers position.",
            "And we multiply it by our transition matrix and the transition matrix is our model of how the speaker is going to move, so this transition matrix could be, for instance the identity matrix.",
            "That would be the simplest assumption.",
            "OK, this gives us our new predicted state estimate.",
            "Once we have this predicted state estimate, we calculate the innovation.",
            "We linearize our observations functional about the current operating points, and we can calculate our common gain.",
            "Once we calculate the Kalman gain, we're ready to fold in the new information from the current observation and a fold in that new observation, we simply multiply the innovation.",
            "By the Kalman gain, we add that to our prior predicted estimates and we have the new filtered estimate.",
            "It's very, very simple.",
            "Right?",
            "So to look back here, you can think of the common gain as you take this common gaining multiplied by this innovation here and that tells us how much we have to have, how far we have to move this prior estimates of the speaker's position in order to have an optimal estimates given the current observation.",
            "OK."
        ],
        [
            "OK, so I just described the way we were doing source localization as of a year ago.",
            "Now in the past year, we've tried to enhance our source localization system so we can handle multiple speakers and that is in fact the primary point of this talk here.",
            "So the first generalization we made for our our speakers are source localization system.",
            "Our speaker tracking system is we want to first of all we want to make use of multiple multiple observations.",
            "OK, so we have not a single observation per time frame.",
            "Rather we have several OK. Why is this useful?",
            "Well, as I mentioned at the outset, we typically estimate our time to labor arrival based on the peaks in the cross correlation function.",
            "But this cross correlation function of course does not have a single peak.",
            "OK, you typically had peaks, not only due to the speaker or due to the direct source direct signal from the speaker's mouth, but also to do 2 reflections.",
            "Do two laptops due to people walking back and forth, etc etc.",
            "All of those things that we see in the speaker are in the RTO 6 tasks.",
            "So in order to generalize this, Kalman filter to use multiple observations, what we have to do is first of all we have to define a clutter model.",
            "OK, so you can think of all of what I've presented so far as defining a probabilistic probability density function for a speaker.",
            "We have a speaker position.",
            "Which is the position estimate that's the mean in our probability density model, probability density function and we have a covariance matrix, which is the covariance matrix in R PDF.",
            "So for the probabilistic data Association filter, what we do is add to that a background model for spurious acoustic events if you will.",
            "So we say we assume that we essentially have a uniform prior for background events, background events that are not associated with the current speaker, such as someone rapping on the table.",
            "OK.",
            "Someone walking across the room etc etc.",
            "So if we do that, we can really use pretty much a lot of the same formalism that we've developed already.",
            "We only have to extend it a little bit first, where we have to extend it is we have to define these so-called Association events so the Association events tell us which observation.",
            "Is in fact associated with the Speaker an which is associated with the clutter model.",
            "So once we do that, we have to define these conditional.",
            "These conditional innovations and such.",
            "And then we end up with the update formula.",
            "Looks much like the same, much the same as before.",
            "But now we have a combined innovation that's taken that's formed from the some of the conventional innovations weighted by these probabilities.",
            "Sorry, this posterior probabilities in the posterior probabilities offer each of the Association events.",
            "OK."
        ],
        [
            "So to extend our diagram, we now have something that looks like this.",
            "Again, we have here our state estimate.",
            "We have the uncertainty matrix for the state estimate.",
            "We have the.",
            "Equation that tells us how the state transforms into a predicted observation.",
            "But now we have several observations.",
            "So the question now becomes which of these observations is due to the target in which of these observations is due to the background, right?",
            "Once we know that once we've decided that in some probabilistic sense we can use the same update formula that we had before.",
            "OK."
        ],
        [
            "Now to go one step further.",
            "Running a little bit short on time so I won't go into this session."
        ],
        [
            "Such detail.",
            "In order to have a joint probabilistic data Association filter, what we do is in fact assume that we have in addition to multiple observations, we have multiple targets.",
            "Each of these targets has a current state estimate.",
            "OK, we have a predicted observation.",
            "We have the state error covariance matrices.",
            "We had the innovation covariance matrices and now we have this sort of cloud of possible observations, right?",
            "So in order to update our estimates of position for each of our targets, but we have to do is first of all we have to decide which of these observations map to which targets OK, or can in fact happen that all of the map to the background model.",
            "Alright.",
            "So once we've decided which targets rather which observations, which targets, then we can just use our prior update equations to update each of these separately.",
            "OK, so this is much like what we do when we're training a GMM or an HMM.",
            "We first of all, we have an observation, we decide which Gaussian observation came from, and then we update at the end each Gaussian separately.",
            "Alright, this is much the same.",
            "What we're doing here.",
            "All right, it only so happens that we in fact have to do this assignment.",
            "Of observations to targets we have to do that for all observations and all targets jointly.",
            "That's why it's called a joint probabilistic data Association filter."
        ],
        [
            "OK, getting onto the experiments.",
            "First of all, just some tracking results and I wanted to explain briefly our metrics.",
            "Alright.",
            "So first of all, we just defined a threshold of 50 centimeters from a ground truth position estimate.",
            "And we said that any instance where this 50 centimeter threshold was exceeded, okayyy there was no true target within 50 centimeters of a predicted or estimated position.",
            "We call that a false false positive.",
            "And we did not count those towards the multiply multi object tracking precision which is defined as the average world horizontal position error.",
            "If no estimate fell within 50 centimeters of the ground truth that was created asmis OK. Now, the most most important metric here for our tracking problem is the so called multi object tracking error which is defined as the number of false positives plus the number of misses divided by the number of ground truth labels.",
            "OK, so this is sort of like word error rates for for tracking and again it's based on this threshold of 50 centimeters."
        ],
        [
            "I'm not going to integrate the amount of detail about our sensor configuration.",
            "This is a little bit difficult to read anyway, but here is our room.",
            "It's roughly 6 centimeters by 7 centimeters.",
            "We have T arrays.",
            "6 centimeters 6 meters by 7 meters.",
            "OK, we're not tracking pygmies or mice.",
            "We have 6 meters by 7 meet 6 meters by 7 meters.",
            "We have T arrays on each of the walls and those are used for source localization.",
            "Here's the T array can see it's called AT array because it looks like an inverted T. And then we have a mark three array here in the background.",
            "Alright, so here the result."
        ],
        [
            "Yes.",
            "Most striking thing here is we compared our initial iek app design.",
            "The initial common filter which we used last year to the joint Probabilistic Data Association filter and we saw we got a nice production in the multi object tracking error and we compared that both for lecture and interactive seminars and got improvements across the board."
        ],
        [
            "The other thing we did was we used the position estimates to do beamforming based on our targets, and then we fed the output of the beamformer into our STT engine OK. And in that case we again saw improvements of the eye of the J PDF over the IE KF.",
            "Alright, so we get a decent improvement.",
            "Is significant improvement.",
            "The most striking thing here though was if you compare the beamforming signal to the STT result based on a single channel of the mark three array, and there we saw an absolute improvement of 13%.",
            "OK."
        ],
        [
            "Alright, last slide.",
            "So we've seen nice gains by extending our IE KFR iterated extended Kalman filter to a joint probabilistic data Association filter on the on the 2006 career development data.",
            "We saw a reduction in multi object tracking error from 20% roughly down to 14%.",
            "That's we also saw a nice reduction in Word error rate for beamforming.",
            "And in the future we like to use our system to develop a multi speaker tracking for multi stream STT.",
            "OK, So what we'd like to do as I said is we'd like to have separate tracks, one for each speaker, then will cluster those tracks together using the standard PC criteria such as Zavia spoke of.",
            "And then we'll take each of those sets of tracks which speaker dependent and run our STT engines on them, and then that way we hope to do speaker attributed STT.",
            "So will have a system that tells us who spoke what when.",
            "And where?",
            "Questions.",
            "So in applying your extended count filter, you make assumptions of a little video.",
            "Our president, do you think those are OK?",
            "Glad you asked that because it's an opportunity to make a plug for.",
            "Other work.",
            "I wanted to mention this at the outset.",
            "Anyway, here we investigate exactly that question.",
            "Here we look at only the extended County filter and we have some nice plots of.",
            "You have to do this local linearization.",
            "And we have some nice plots like this that show how well the linear model.",
            "In the green.",
            "It's not working.",
            "In the Green Match is the true model in the red, and by the way each hash mark here is 1 meter.",
            "So you see that within 1 meter it matches really pretty well, OK?",
            "Of one of the reviewers for this paper, actually said he wanted to see some comparisons with the standard technique.",
            "OK, we didn't talk about the comparisons with the standard techniques for source localization in the current work, because they're all written up and documented in this work.",
            "Which should appear in your ZIP Journal event signal processing in a few months.",
            "Refer you to that.",
            "OK. Yeah Steve.",
            "Using my time.",
            "So it appears that you need to fix the number of speakers that you're tracking the initial for the model and do you do that on priority number speakers, or do you overestimate so that you can track background or what?",
            "OK, I didn't speak about it, but there are a few heuristics that determine such things.",
            "OK, So what we do is we haven't used.",
            "It says if we have several observations in a row that cannot be associated to.",
            "Any speaker what we do is we form a new target.",
            "OK, so we say that a new target sort of beams into being right.",
            "And then we also have a criterion for saying if a given target has not been updated, if its position has not been updated in some time like 2 seconds, we delete it.",
            "Alright, so we have these heuristics tell us when new targets are created and one new targets are no longer active.",
            "So the number of active targets fluctuates.",
            "Alright, those mistakes are things that we're still tuning, so working with because obviously they have a large effect on the system performance.",
            "OK, so thanks once again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the next talk in this session is a provided by skate and drama, and it's called tracking multiple simultaneous speakers with probabilistic data Association filters and the talk is going to be given by Jonathan.",
                    "label": 1
                },
                {
                    "sent": "OK, thank you Steve.",
                    "label": 0
                },
                {
                    "sent": "OK so I was a little bit surprised to learn that up mark paper landed in the speaker diarization and tracking task.",
                    "label": 0
                },
                {
                    "sent": "We do speaker tracking but not quite kind of tracking that.",
                    "label": 0
                },
                {
                    "sent": "Perhaps most of you are thinking of because we track speakers in space and time, not just in time.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I was struck by something that Xavier said.",
                    "label": 0
                },
                {
                    "sent": "He said that typically when they do speaker diarization, they start with a signal.",
                    "label": 0
                },
                {
                    "sent": "That sort of combined signal for all speakers.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the exact opposite of what we are striving towards.",
                    "label": 0
                },
                {
                    "sent": "But this work in our estetik work, because what we'd like to come up with finally is multiple signals, and each signal corresponds to exactly 1 speaker.",
                    "label": 0
                },
                {
                    "sent": "OK, and we want to run our STT engine's on each of those signals and transcribe exactly with that.",
                    "label": 0
                },
                {
                    "sent": "One speaker says and attributes that speaker, OK.",
                    "label": 0
                },
                {
                    "sent": "So today I'm going to talk about the first part of that problem, finding out when and where a speaker spoke.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so to go through some of the preliminaries here, we start with the time delay of arrival and time to leave.",
                    "label": 1
                },
                {
                    "sent": "Arrival is very simple concept if you know where a speaker is.",
                    "label": 0
                },
                {
                    "sent": "So here's my don't worry.",
                    "label": 0
                },
                {
                    "sent": "Be happy speaker and where two microphones are for instance.",
                    "label": 0
                },
                {
                    "sent": "You know the propagation delay between the speaker and the microphone, and you can figure out how long it takes with the time differences between the arrival of the signal from the speaker at the first microphone and at the second microphone.",
                    "label": 1
                },
                {
                    "sent": "It's simply the different differences in distance is divided by the speed of sound.",
                    "label": 0
                },
                {
                    "sent": "It's quite simple.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a simple way of estimating the position of a speaker in three dimensional space is to say, I believe, that speaker is where at that position that minimizes this sort of difference between my expected time delay of arrival and my observed time delay of arrival.",
                    "label": 0
                },
                {
                    "sent": "Overall microphone pairs.",
                    "label": 0
                },
                {
                    "sent": "So if I have a microphone array, an array of several microphones, then I can easily form all a set of unique microphone pairs.",
                    "label": 0
                },
                {
                    "sent": "So if I sum this weighted time delay of arrival.",
                    "label": 0
                },
                {
                    "sent": "Overall, such microphone pairs.",
                    "label": 0
                },
                {
                    "sent": "I simply say my speaker isn't that location that minimizes this.",
                    "label": 0
                },
                {
                    "sent": "This weighted sum or this weighted squared sum?",
                    "label": 0
                },
                {
                    "sent": "Yeah OK, how do I measure at the time delay of arrival between two pairs of microphones or two 2 microphones?",
                    "label": 0
                },
                {
                    "sent": "Well, the most common technique is to use something called a generalized cross correlation.",
                    "label": 0
                },
                {
                    "sent": "I won't go into a lot of details here because people who do source localization know exactly what this means.",
                    "label": 0
                },
                {
                    "sent": "For those that don't do source localization is essentially a cross correlation between two signals, and as with the regular cross correlation, what you do is look for the maximum for the peak in this cross correlation, and you say, I believe that the time delay between those two signals.",
                    "label": 0
                },
                {
                    "sent": "Corresponds to the peak in the cross correlation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned, it's easy to formulate a criterion for source localization, namely this.",
                    "label": 1
                },
                {
                    "sent": "OK, there's only one catch here.",
                    "label": 0
                },
                {
                    "sent": "If you recall this expected time delay of arrival is nonlinear.",
                    "label": 0
                },
                {
                    "sent": "OK, this is clearly nonlinear because you have these two norm terms alright, which involves squares and square roots.",
                    "label": 0
                },
                {
                    "sent": "It's nonlinear, OK.",
                    "label": 0
                },
                {
                    "sent": "So what you can do to get around that other part from that is very nice constrained problem because you have a least squares regression problem.",
                    "label": 0
                },
                {
                    "sent": "OK, apart from this non linearity.",
                    "label": 1
                },
                {
                    "sent": "So what you can do to get around that nonlinearity, of course linearize the the time, believe arrival function about the current operating points and if you do that you get this really simple term or this really simple criterion here OK?",
                    "label": 0
                },
                {
                    "sent": "Now this is nice because again, this is a least squares criterion, and it's linearized.",
                    "label": 1
                },
                {
                    "sent": "OK, so this sort of function is very amenable to solution, But the Kalman filter OK, so the reason that you would like to introduce Kalman filter is because prior estimates of where the speaker is are probably useful for finding out where the speaker is currently.",
                    "label": 1
                },
                {
                    "sent": "OK, because you can't expect your speaker to move instantaneously from place to place, and you typically don't have speakers that are Chinese acrobats and so forth so.",
                    "label": 0
                },
                {
                    "sent": "You can usually use the prior information to find out where he currently is.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so now we come to the little bit of math here.",
                    "label": 0
                },
                {
                    "sent": "I tried to minimize the amount of math, but unfortunately it's difficult to talk about Kalman filters without speaking about the way common filters work.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a little bit bewildering.",
                    "label": 1
                },
                {
                    "sent": "Perhaps the first time you've seen it, but I'm going to try to make it as intuitive as possible, OK?",
                    "label": 0
                },
                {
                    "sent": "So you start with the common filter cabin filter is governed by two equations.",
                    "label": 1
                },
                {
                    "sent": "You have your process equation and your observation equation.",
                    "label": 0
                },
                {
                    "sent": "OK, so first of all, the first part of the common filter is always the state.",
                    "label": 0
                },
                {
                    "sent": "So in our case the state is of course the speaker's position in three space.",
                    "label": 0
                },
                {
                    "sent": "The second part is the observation, and in our case the observations are going to be the setup time delays or a vector of time delays.",
                    "label": 0
                },
                {
                    "sent": "Alright, so you have two equations.",
                    "label": 0
                },
                {
                    "sent": "The first one governs how the state changes in time and the second governs how the state Maps to observations.",
                    "label": 0
                },
                {
                    "sent": "OK, so at this point both all of these things are pretty much clear, because we've already specified how the state Maps to observations, namely, it's through this time delay of arrival equation.",
                    "label": 0
                },
                {
                    "sent": "OK, so we need really need specifies how the state evolves in time, and you can make a number of simple, very simple assumptions about how the speaker moves.",
                    "label": 0
                },
                {
                    "sent": "Namely, you can assume that the stationary that he moves with constant.",
                    "label": 1
                },
                {
                    "sent": "LA city etc.",
                    "label": 0
                },
                {
                    "sent": "So with each of these components there is a error term.",
                    "label": 0
                },
                {
                    "sent": "Alright, so you don't expect your evolution of your state correspond exactly to the wet your model.",
                    "label": 0
                },
                {
                    "sent": "So in order to compensate for that non correspondence, if you will, you introduce these error terms V1 and V2 and these are known as the state and processed noises alright, so typically in a common filter you don't assume you know the state and process noise, but you know their second order statistics, namely, you know their correlation values OK. Alright.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is just a little bit of math.",
                    "label": 0
                },
                {
                    "sent": "Hope we get through this point and then it should go clearer.",
                    "label": 0
                },
                {
                    "sent": "Alright, so to form the update of a common filter, you have to have you first start off by defining two estimates of the state.",
                    "label": 1
                },
                {
                    "sent": "OK, the first estimate is called the so-called predicted estimate.",
                    "label": 0
                },
                {
                    "sent": "So the predicted estimate is based on all observations excluding the current observation.",
                    "label": 1
                },
                {
                    "sent": "Alright, the filtered estimate, on the other hand, is based on all observations, including the current observations, so that seems like it's pretty small distinction, but it turns out to have consequences in the way you formulate the Kalman filter, right?",
                    "label": 0
                },
                {
                    "sent": "So next thing you have to define is the so-called predicted observation.",
                    "label": 0
                },
                {
                    "sent": "So if you know if you have your predicted state here.",
                    "label": 0
                },
                {
                    "sent": "You should be able to guess because you know the form of CIE. You know the time delay of arrival function.",
                    "label": 0
                },
                {
                    "sent": "You should be able to predict what time delay of arrival you are actually going to see.",
                    "label": 1
                },
                {
                    "sent": "Alright, that's called the predicted observation.",
                    "label": 1
                },
                {
                    "sent": "Then the innovation is the difference between what you predicted you would see and what you actually did see.",
                    "label": 0
                },
                {
                    "sent": "OK, this is called the innovation, because it's exactly this term that contains the new information.",
                    "label": 0
                },
                {
                    "sent": "OK, the new information that you got from the observation.",
                    "label": 0
                },
                {
                    "sent": "Because most of the most of the information contained in the observation is not new, only this difference is new.",
                    "label": 1
                },
                {
                    "sent": "OK. Alright, so then, in order to have extended Kalman filter as opposed to regular common filter, by the way, extended Kalman filter is useful for solving nonlinear problems like we have here.",
                    "label": 0
                },
                {
                    "sent": "All we have to do is take our non linear time delay arrival equation and linearize it about the current operating point, namely about the current predicted state estimate.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Couple more things that we have to define.",
                    "label": 0
                },
                {
                    "sent": "1st is called the correlation matrix.",
                    "label": 0
                },
                {
                    "sent": "Of the innovation sequence, and this is simply a correlation matrix of our innovations.",
                    "label": 1
                },
                {
                    "sent": "And again our innovations.",
                    "label": 0
                },
                {
                    "sent": "Is this difference between predicted and true actual and predicted observation?",
                    "label": 0
                },
                {
                    "sent": "It turns out we can.",
                    "label": 0
                },
                {
                    "sent": "We can calculate this correlation matrix very simply.",
                    "label": 0
                },
                {
                    "sent": "OK through this equation.",
                    "label": 0
                },
                {
                    "sent": "And the terms that appear in the correlation match, or rather the correlation matrix of innovation sequence, are to first of all we have this QAT.",
                    "label": 0
                },
                {
                    "sent": "Which is the error in our observations.",
                    "label": 1
                },
                {
                    "sent": "The second thing we have is this so called predicted state error correlation matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, so this predicted state our correlation matrix tells us how uncertain we are about our estimate of the speakers position.",
                    "label": 0
                },
                {
                    "sent": "So this correlation matrix is very large.",
                    "label": 0
                },
                {
                    "sent": "We are very uncertain where the speaker is.",
                    "label": 0
                },
                {
                    "sent": "If we get small and we're fairly certain where he is OK.",
                    "label": 0
                },
                {
                    "sent": "So you notice that this equation is fairly intuitive.",
                    "label": 0
                },
                {
                    "sent": "It says that the correlation matrix of the innovation sequence has two components.",
                    "label": 1
                },
                {
                    "sent": "As I mentioned, the first is determined by our uncertainty from the state.",
                    "label": 0
                },
                {
                    "sent": "The second is determined by our uncertainty of the actual observations.",
                    "label": 0
                },
                {
                    "sent": "OK, the sum of those two is the innovation correlation matrix.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "The common gain, on the other hand.",
                    "label": 0
                },
                {
                    "sent": "Is now defined as such.",
                    "label": 1
                },
                {
                    "sent": "Right now at the moment, what?",
                    "label": 0
                },
                {
                    "sent": "We'll see why it's useful to find this thing called the Kalman gain.",
                    "label": 0
                },
                {
                    "sent": "But the common gain tells us how we're going to fold the information from the latest observation into our state estimate.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this common gain is the most important quantity we have here, and you notice to calculate the common gain.",
                    "label": 1
                },
                {
                    "sent": "We have to know this correlation matrix of the predicted state error.",
                    "label": 0
                },
                {
                    "sent": "Alright, where do we get the correlation matrix of the dictator?",
                    "label": 0
                },
                {
                    "sent": "Well, this comes from the so-called Ricotti equation.",
                    "label": 1
                },
                {
                    "sent": "OK, the Ricardi equation tells us how to update the uncertainty matrix of our state estimate.",
                    "label": 0
                },
                {
                    "sent": "OK, here it is right here.",
                    "label": 0
                },
                {
                    "sent": "Alright, and you notice here in our mercati equation, we're looking at both the correlation matrix of the filtered state error.",
                    "label": 1
                },
                {
                    "sent": "As well as the predicted state error.",
                    "label": 0
                },
                {
                    "sent": "So this is the correlation matrix of the filter state error.",
                    "label": 0
                },
                {
                    "sent": "This is the correlation matrix of the predicted state error.",
                    "label": 0
                },
                {
                    "sent": "Notice the primary difference between those two.",
                    "label": 0
                },
                {
                    "sent": "Is this Q one term and again this Q one term is the correlation matrix of our.",
                    "label": 0
                },
                {
                    "sent": "Process noise.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that was the math part.",
                    "label": 0
                },
                {
                    "sent": "Let me now try to explain more intuitively what's happening here.",
                    "label": 0
                },
                {
                    "sent": "Here is our state.",
                    "label": 0
                },
                {
                    "sent": "Again, this is our position in three space of our speaker.",
                    "label": 0
                },
                {
                    "sent": "This matrix here the smaller ellipse is meant to be is meant to represent the covariance matrix of our filtered state error.",
                    "label": 0
                },
                {
                    "sent": "The slightly larger ellipse is the correlation matrix of our predicted state error.",
                    "label": 0
                },
                {
                    "sent": "So why is it larger?",
                    "label": 0
                },
                {
                    "sent": "Well, because you have this extra term here, you're essentially taking the.",
                    "label": 0
                },
                {
                    "sent": "Filtered state error covariance matrix and adding it to adding to it this term here makes it larger.",
                    "label": 0
                },
                {
                    "sent": "Now this up here, this little ellipse up here is meant to represent the covariance matrix of our innovation, OK?",
                    "label": 0
                },
                {
                    "sent": "So you notice where we expect the innovation this dot here.",
                    "label": 0
                },
                {
                    "sent": "Is we obtain from the state estimate through our non linearity through our functional or through our time to labor rival function?",
                    "label": 0
                },
                {
                    "sent": "So this function tells us based on the value of the state.",
                    "label": 1
                },
                {
                    "sent": "What's the observation should be?",
                    "label": 0
                },
                {
                    "sent": "The difference between what we have as an observation.",
                    "label": 0
                },
                {
                    "sent": "And what we thought the observation should be is our innovation.",
                    "label": 0
                },
                {
                    "sent": "Alpha at see OK, and this ellipse is meant to represent the covariance matrix of the innovation.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, alright, so given all those preliminaries.",
                    "label": 0
                },
                {
                    "sent": "We're now ready to update our estimates of the.",
                    "label": 1
                },
                {
                    "sent": "Position of the speaker.",
                    "label": 0
                },
                {
                    "sent": "Alright, so in order to update the position of the speaker, we only need to do 2 steps.",
                    "label": 0
                },
                {
                    "sent": "The first step is we take our prior estimate of speakers position.",
                    "label": 0
                },
                {
                    "sent": "And we multiply it by our transition matrix and the transition matrix is our model of how the speaker is going to move, so this transition matrix could be, for instance the identity matrix.",
                    "label": 0
                },
                {
                    "sent": "That would be the simplest assumption.",
                    "label": 1
                },
                {
                    "sent": "OK, this gives us our new predicted state estimate.",
                    "label": 1
                },
                {
                    "sent": "Once we have this predicted state estimate, we calculate the innovation.",
                    "label": 0
                },
                {
                    "sent": "We linearize our observations functional about the current operating points, and we can calculate our common gain.",
                    "label": 1
                },
                {
                    "sent": "Once we calculate the Kalman gain, we're ready to fold in the new information from the current observation and a fold in that new observation, we simply multiply the innovation.",
                    "label": 0
                },
                {
                    "sent": "By the Kalman gain, we add that to our prior predicted estimates and we have the new filtered estimate.",
                    "label": 0
                },
                {
                    "sent": "It's very, very simple.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So to look back here, you can think of the common gain as you take this common gaining multiplied by this innovation here and that tells us how much we have to have, how far we have to move this prior estimates of the speaker's position in order to have an optimal estimates given the current observation.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I just described the way we were doing source localization as of a year ago.",
                    "label": 0
                },
                {
                    "sent": "Now in the past year, we've tried to enhance our source localization system so we can handle multiple speakers and that is in fact the primary point of this talk here.",
                    "label": 0
                },
                {
                    "sent": "So the first generalization we made for our our speakers are source localization system.",
                    "label": 0
                },
                {
                    "sent": "Our speaker tracking system is we want to first of all we want to make use of multiple multiple observations.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have not a single observation per time frame.",
                    "label": 0
                },
                {
                    "sent": "Rather we have several OK. Why is this useful?",
                    "label": 0
                },
                {
                    "sent": "Well, as I mentioned at the outset, we typically estimate our time to labor arrival based on the peaks in the cross correlation function.",
                    "label": 0
                },
                {
                    "sent": "But this cross correlation function of course does not have a single peak.",
                    "label": 0
                },
                {
                    "sent": "OK, you typically had peaks, not only due to the speaker or due to the direct source direct signal from the speaker's mouth, but also to do 2 reflections.",
                    "label": 0
                },
                {
                    "sent": "Do two laptops due to people walking back and forth, etc etc.",
                    "label": 0
                },
                {
                    "sent": "All of those things that we see in the speaker are in the RTO 6 tasks.",
                    "label": 0
                },
                {
                    "sent": "So in order to generalize this, Kalman filter to use multiple observations, what we have to do is first of all we have to define a clutter model.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can think of all of what I've presented so far as defining a probabilistic probability density function for a speaker.",
                    "label": 0
                },
                {
                    "sent": "We have a speaker position.",
                    "label": 0
                },
                {
                    "sent": "Which is the position estimate that's the mean in our probability density model, probability density function and we have a covariance matrix, which is the covariance matrix in R PDF.",
                    "label": 0
                },
                {
                    "sent": "So for the probabilistic data Association filter, what we do is add to that a background model for spurious acoustic events if you will.",
                    "label": 1
                },
                {
                    "sent": "So we say we assume that we essentially have a uniform prior for background events, background events that are not associated with the current speaker, such as someone rapping on the table.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Someone walking across the room etc etc.",
                    "label": 0
                },
                {
                    "sent": "So if we do that, we can really use pretty much a lot of the same formalism that we've developed already.",
                    "label": 0
                },
                {
                    "sent": "We only have to extend it a little bit first, where we have to extend it is we have to define these so-called Association events so the Association events tell us which observation.",
                    "label": 0
                },
                {
                    "sent": "Is in fact associated with the Speaker an which is associated with the clutter model.",
                    "label": 0
                },
                {
                    "sent": "So once we do that, we have to define these conditional.",
                    "label": 0
                },
                {
                    "sent": "These conditional innovations and such.",
                    "label": 0
                },
                {
                    "sent": "And then we end up with the update formula.",
                    "label": 0
                },
                {
                    "sent": "Looks much like the same, much the same as before.",
                    "label": 0
                },
                {
                    "sent": "But now we have a combined innovation that's taken that's formed from the some of the conventional innovations weighted by these probabilities.",
                    "label": 1
                },
                {
                    "sent": "Sorry, this posterior probabilities in the posterior probabilities offer each of the Association events.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to extend our diagram, we now have something that looks like this.",
                    "label": 0
                },
                {
                    "sent": "Again, we have here our state estimate.",
                    "label": 0
                },
                {
                    "sent": "We have the uncertainty matrix for the state estimate.",
                    "label": 0
                },
                {
                    "sent": "We have the.",
                    "label": 0
                },
                {
                    "sent": "Equation that tells us how the state transforms into a predicted observation.",
                    "label": 0
                },
                {
                    "sent": "But now we have several observations.",
                    "label": 0
                },
                {
                    "sent": "So the question now becomes which of these observations is due to the target in which of these observations is due to the background, right?",
                    "label": 0
                },
                {
                    "sent": "Once we know that once we've decided that in some probabilistic sense we can use the same update formula that we had before.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now to go one step further.",
                    "label": 0
                },
                {
                    "sent": "Running a little bit short on time so I won't go into this session.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Such detail.",
                    "label": 0
                },
                {
                    "sent": "In order to have a joint probabilistic data Association filter, what we do is in fact assume that we have in addition to multiple observations, we have multiple targets.",
                    "label": 0
                },
                {
                    "sent": "Each of these targets has a current state estimate.",
                    "label": 0
                },
                {
                    "sent": "OK, we have a predicted observation.",
                    "label": 0
                },
                {
                    "sent": "We have the state error covariance matrices.",
                    "label": 0
                },
                {
                    "sent": "We had the innovation covariance matrices and now we have this sort of cloud of possible observations, right?",
                    "label": 0
                },
                {
                    "sent": "So in order to update our estimates of position for each of our targets, but we have to do is first of all we have to decide which of these observations map to which targets OK, or can in fact happen that all of the map to the background model.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So once we've decided which targets rather which observations, which targets, then we can just use our prior update equations to update each of these separately.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is much like what we do when we're training a GMM or an HMM.",
                    "label": 0
                },
                {
                    "sent": "We first of all, we have an observation, we decide which Gaussian observation came from, and then we update at the end each Gaussian separately.",
                    "label": 0
                },
                {
                    "sent": "Alright, this is much the same.",
                    "label": 0
                },
                {
                    "sent": "What we're doing here.",
                    "label": 0
                },
                {
                    "sent": "All right, it only so happens that we in fact have to do this assignment.",
                    "label": 0
                },
                {
                    "sent": "Of observations to targets we have to do that for all observations and all targets jointly.",
                    "label": 0
                },
                {
                    "sent": "That's why it's called a joint probabilistic data Association filter.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, getting onto the experiments.",
                    "label": 0
                },
                {
                    "sent": "First of all, just some tracking results and I wanted to explain briefly our metrics.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So first of all, we just defined a threshold of 50 centimeters from a ground truth position estimate.",
                    "label": 1
                },
                {
                    "sent": "And we said that any instance where this 50 centimeter threshold was exceeded, okayyy there was no true target within 50 centimeters of a predicted or estimated position.",
                    "label": 1
                },
                {
                    "sent": "We call that a false false positive.",
                    "label": 0
                },
                {
                    "sent": "And we did not count those towards the multiply multi object tracking precision which is defined as the average world horizontal position error.",
                    "label": 1
                },
                {
                    "sent": "If no estimate fell within 50 centimeters of the ground truth that was created asmis OK. Now, the most most important metric here for our tracking problem is the so called multi object tracking error which is defined as the number of false positives plus the number of misses divided by the number of ground truth labels.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is sort of like word error rates for for tracking and again it's based on this threshold of 50 centimeters.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm not going to integrate the amount of detail about our sensor configuration.",
                    "label": 1
                },
                {
                    "sent": "This is a little bit difficult to read anyway, but here is our room.",
                    "label": 0
                },
                {
                    "sent": "It's roughly 6 centimeters by 7 centimeters.",
                    "label": 0
                },
                {
                    "sent": "We have T arrays.",
                    "label": 0
                },
                {
                    "sent": "6 centimeters 6 meters by 7 meters.",
                    "label": 0
                },
                {
                    "sent": "OK, we're not tracking pygmies or mice.",
                    "label": 0
                },
                {
                    "sent": "We have 6 meters by 7 meet 6 meters by 7 meters.",
                    "label": 0
                },
                {
                    "sent": "We have T arrays on each of the walls and those are used for source localization.",
                    "label": 0
                },
                {
                    "sent": "Here's the T array can see it's called AT array because it looks like an inverted T. And then we have a mark three array here in the background.",
                    "label": 0
                },
                {
                    "sent": "Alright, so here the result.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Most striking thing here is we compared our initial iek app design.",
                    "label": 0
                },
                {
                    "sent": "The initial common filter which we used last year to the joint Probabilistic Data Association filter and we saw we got a nice production in the multi object tracking error and we compared that both for lecture and interactive seminars and got improvements across the board.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other thing we did was we used the position estimates to do beamforming based on our targets, and then we fed the output of the beamformer into our STT engine OK. And in that case we again saw improvements of the eye of the J PDF over the IE KF.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we get a decent improvement.",
                    "label": 0
                },
                {
                    "sent": "Is significant improvement.",
                    "label": 0
                },
                {
                    "sent": "The most striking thing here though was if you compare the beamforming signal to the STT result based on a single channel of the mark three array, and there we saw an absolute improvement of 13%.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, last slide.",
                    "label": 0
                },
                {
                    "sent": "So we've seen nice gains by extending our IE KFR iterated extended Kalman filter to a joint probabilistic data Association filter on the on the 2006 career development data.",
                    "label": 1
                },
                {
                    "sent": "We saw a reduction in multi object tracking error from 20% roughly down to 14%.",
                    "label": 0
                },
                {
                    "sent": "That's we also saw a nice reduction in Word error rate for beamforming.",
                    "label": 0
                },
                {
                    "sent": "And in the future we like to use our system to develop a multi speaker tracking for multi stream STT.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we'd like to do as I said is we'd like to have separate tracks, one for each speaker, then will cluster those tracks together using the standard PC criteria such as Zavia spoke of.",
                    "label": 0
                },
                {
                    "sent": "And then we'll take each of those sets of tracks which speaker dependent and run our STT engines on them, and then that way we hope to do speaker attributed STT.",
                    "label": 0
                },
                {
                    "sent": "So will have a system that tells us who spoke what when.",
                    "label": 0
                },
                {
                    "sent": "And where?",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "So in applying your extended count filter, you make assumptions of a little video.",
                    "label": 0
                },
                {
                    "sent": "Our president, do you think those are OK?",
                    "label": 0
                },
                {
                    "sent": "Glad you asked that because it's an opportunity to make a plug for.",
                    "label": 0
                },
                {
                    "sent": "Other work.",
                    "label": 0
                },
                {
                    "sent": "I wanted to mention this at the outset.",
                    "label": 0
                },
                {
                    "sent": "Anyway, here we investigate exactly that question.",
                    "label": 0
                },
                {
                    "sent": "Here we look at only the extended County filter and we have some nice plots of.",
                    "label": 0
                },
                {
                    "sent": "You have to do this local linearization.",
                    "label": 0
                },
                {
                    "sent": "And we have some nice plots like this that show how well the linear model.",
                    "label": 0
                },
                {
                    "sent": "In the green.",
                    "label": 0
                },
                {
                    "sent": "It's not working.",
                    "label": 0
                },
                {
                    "sent": "In the Green Match is the true model in the red, and by the way each hash mark here is 1 meter.",
                    "label": 0
                },
                {
                    "sent": "So you see that within 1 meter it matches really pretty well, OK?",
                    "label": 0
                },
                {
                    "sent": "Of one of the reviewers for this paper, actually said he wanted to see some comparisons with the standard technique.",
                    "label": 0
                },
                {
                    "sent": "OK, we didn't talk about the comparisons with the standard techniques for source localization in the current work, because they're all written up and documented in this work.",
                    "label": 0
                },
                {
                    "sent": "Which should appear in your ZIP Journal event signal processing in a few months.",
                    "label": 0
                },
                {
                    "sent": "Refer you to that.",
                    "label": 0
                },
                {
                    "sent": "OK. Yeah Steve.",
                    "label": 0
                },
                {
                    "sent": "Using my time.",
                    "label": 0
                },
                {
                    "sent": "So it appears that you need to fix the number of speakers that you're tracking the initial for the model and do you do that on priority number speakers, or do you overestimate so that you can track background or what?",
                    "label": 0
                },
                {
                    "sent": "OK, I didn't speak about it, but there are a few heuristics that determine such things.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we do is we haven't used.",
                    "label": 0
                },
                {
                    "sent": "It says if we have several observations in a row that cannot be associated to.",
                    "label": 0
                },
                {
                    "sent": "Any speaker what we do is we form a new target.",
                    "label": 0
                },
                {
                    "sent": "OK, so we say that a new target sort of beams into being right.",
                    "label": 0
                },
                {
                    "sent": "And then we also have a criterion for saying if a given target has not been updated, if its position has not been updated in some time like 2 seconds, we delete it.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we have these heuristics tell us when new targets are created and one new targets are no longer active.",
                    "label": 0
                },
                {
                    "sent": "So the number of active targets fluctuates.",
                    "label": 0
                },
                {
                    "sent": "Alright, those mistakes are things that we're still tuning, so working with because obviously they have a large effect on the system performance.",
                    "label": 0
                },
                {
                    "sent": "OK, so thanks once again.",
                    "label": 0
                }
            ]
        }
    }
}