{
    "id": "pmb6s72mqe25b3b523o32rxsmydhkmdo",
    "title": "A Graph-Based Approach to Learn Semantic Descriptions of Data Sources",
    "info": {
        "author": [
            "Craig A. Knoblock, Information Sciences Institute (ISI), University of Southern California"
        ],
        "published": "Nov. 28, 2013",
        "recorded": "October 2013",
        "category": [
            "Top->Computer Science->Information Design",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2013_knoblock_data_sources/",
    "segmentation": [
        [
            "So this is work on a graph based approach to learn semantic descriptions of data sources.",
            "This is work by one of my PhD students and listen to Harry and I'm Craig Knoblock.",
            "Alright, so."
        ],
        [
            "So, so the problem is how to actually learn semantic descriptions?",
            "That's what we want to solve here before I actually."
        ],
        [
            "We tell you how we do that.",
            "Let me first tell you what it is.",
            "I mean by a semantic description itself."
        ],
        [
            "Alright, So what we're trying to do is essentially take a source and describe that source in terms of the concepts and relationships defined in some domain ontology.",
            "So here's an example terminology, and this one is important to look at because we're going to use it as an example throughout my talk, so you see, you can see here that we have, you know.",
            "Set of classes.",
            "A person, organization, places, and then city and state which have this relationship, which is that the city has a state.",
            "This is a property in the US for every city has some state it's within and that these are both subclasses of place.",
            "OK, and then what we want to do is take ontology like this, which has a set of relationships and both object properties in data properties and then take the source.",
            "So I have an example source at the bottom here at which has a set of columns of information.",
            "What we want to do is take this data source and describe it in terms of this domain ontology.",
            "Alright, So what does that actually mean?"
        ],
        [
            "So if we take the columns of information, but that would mean is that we eventually find the classes and the properties that correspond to each column of information.",
            "So in the first column for example, we have a set of names.",
            "You know, these are essentially the names of some person, so it's the property name from the class person.",
            "Similarly, the second column is the birthdate of a person.",
            "We have the name of an organization with the name of a city in the name of the state.",
            "Now some people stop here and they say OK, Now I've got a description of all the columns of data, but in fact we're interested in actually something that's that's more complicated in this, which is we really want to understand the relationships between these different.",
            "Different colors."
        ],
        [
            "Information, so here what we really want to capture is an overall model of that this person.",
            "OK, in this data source has a name and this is the same person has a name and a birthday, so it's not just that it's that these two columns are person, it's actually the same person and this is the name and birthday of this person that they're born in this particular city, which in turn has some state.",
            "And then it works for some organization.",
            "So all of these things together are going to essentially compose the description of the source, and for those who are familiar with the latest standards.",
            "We can actually take this model and automatically convert this into a semantic description.",
            "In terms of this R2 RML language, which is now a W3C standard."
        ],
        [
            "OK, so we've done some previous work on actually learning the semantic descriptions, so in fact we built this system called Karma and the way Karma works today is on the left.",
            "We simply take in a domain ontology and a set of data sample data.",
            "The data source that we want to actually model and we go through a set of steps to actually build these semantic descriptions.",
            "So the system basically learns these semantic types overtime from data that is seen in the past.",
            "So we use a machine learning model to essentially automatically recognize this.",
            "Properties in classes.",
            "Then we go through an algorithm which is based on an efficient sort of Steiner tree algorithm for finding the really what we find.",
            "Is this the simplest explanation that we can generate from the combination of these learn semantic types in the actual ontology graph.",
            "The graph that's built from the Scientology and then automatically produce a semantic model.",
            "So our goal here is to actually automate this as much as possible, but with the idea that you see little people on the top here that the user can actually refine the model of this system doesn't get it right, because sometimes there's just not enough information out there.",
            "To completely automate this process of generating some exceptions, but our goal is to automate it as much as possible."
        ],
        [
            "So in the current Karma system you might generate for an existent existing data source that we want to show, you might enter generate an actual model that looks like this, where it says OK, this data sources and organization has a CEO, which is a person who has a name and birthday.",
            "Now this isn't actually quite right, and with system generated model like this, the user would then go in."
        ],
        [
            "Find this model and say no.",
            "It's actually up.",
            "The data source represents a person who works for an organization, is born in some city, and has some state.",
            "So that's the correct model and one of the problems in the existing systems are one of the limitations."
        ],
        [
            "Existing system is that the system doesn't actually get better at this overtime, right?",
            "It learn the semantic types and get better overtime, but the actual overall description of the relationships in data source every time it sees the same source, you're going to have to make the same refinement, so that's not great.",
            "So that's really what we were."
        ],
        [
            "So what I'm going to describe next to the approach that we now have developed to actually learn these semantic subscription so that we can refine these models and have the system get better overtime as it sees more examples."
        ],
        [
            "Alright, so the key idea then is that we're going to see sources in the same domain, and they're often going to have very similar kinds of data in similar kinds of properties in those data sources, and we want to exploit the knowledge of the existing source descriptions or source models so that when we see a new source were more likely to actually get the description correct.",
            "So in what we do this by leveraging these relationships in this known source models to essentially hypothesized relationship for the new source models, and as it sees more and more examples, it should be more likely to converge on the correct one.",
            "So."
        ],
        [
            "So the approach now looks like this.",
            "So instead of before, if you recall the input to the system was the domain ontology and some new source that is going to model.",
            "Now we've added an additional input here, which is now we actually passed to the system.",
            "The set of known source models.",
            "These are the ones that you've generated in the past, so the system knows about.",
            "Given that, we then construct this graph, which I'll describe how we do that in a minute.",
            "As before, we learn these semantic types for the property, and right now in the current system.",
            "The user would then refine these types, so if the system generates some initial set of types, the user could then go and refine them.",
            "Who doesn't get them correct.",
            "It then takes that, generates a set of candidate models, and then ranks the results, and I'll talk about each of these steps in turn.",
            "OK."
        ],
        [
            "So here's back to my example.",
            "So as before I have the ontology that I showed you earlier on in the talk and then I have 3 known sources in the corresponding models, so the sources are S1 here, which is a set of person info I have S2 which is get cities and then S3, which is business info in each of these has a related model, right?",
            "So they're not, they're not the same, but they each have essentially this source description or source models, shown here right above the source and you can see that one pattern I want to show you in this is that.",
            "For the S1 you can see that the person is born in this city, which has a state and for S2 essentially provides information about cities in the corresponding states.",
            "In S3 is a little different that has a city which is a part of a state, which is a possible relationship in the in the hierarchy and the ontology right?",
            "So you can see that these are city and state are subclasses of place.",
            "And so now I have a new source, so I'll come along with S4 and I want to actually build a model for Task Force.",
            "Quite simple 'cause it needs to fit on the slide.",
            "But As for essentially, is this Postal code look up which essentially given a Postal code looks up the city and state."
        ],
        [
            "And the algorithm now is.",
            "Essentially, we're going to build.",
            "First, we're going to build this graph from the known source models.",
            "So and what we do here is we're going to create a component in this graph for each of the known source models.",
            "So each of those three sources I saw you before we would create a component in G and we're only going to add it if the model is not already a subgraph from the existing component.",
            "I'll show you that example data 2nd, and then we're going to annotate each of the links with essentially the list of supporting models.",
            "So here's what that looks like.",
            "So first I start with S1, which is one of the existing known sources.",
            "How can I build my first part of my graph, which is basically I'm just going to plug in the graph that I have for S1 here.",
            "Next I'm going to take S2 and S2.",
            "Is this one where we essentially it is a sub graph of this other model, so it has this property that you know it has city related to state and has it has these properties of these and you can see that it's a sub graph of the existing component one so we don't have to add anything additional to the graph.",
            "Then we're going to add S 3 but S3 is actually constructed quite differently than.",
            "S1 and S2 because it has that that different relational structure.",
            "There we used part of instead of this state being directly connected to the city.",
            "So it's going to create a new component.",
            "Now the next step."
        ],
        [
            "Is to actually connect these components together, so I know this this picture starting a little busy but just bear with me here 'cause it's just to give you the general intuition about what we're doing here.",
            "So you see I have component one here and I have component to here and then what we do is we actually look at the ways we can actually connect these components together.",
            "Given the ontology and that creates all the green links here.",
            "They probably hard to read from where you are, but the idea is simply that it looks for ways to actually connect up these models in the ontology graph.",
            "We generate all of this.",
            "No."
        ],
        [
            "We do something which we need to score these and I'm going to save you the details of actually the detailed scoring algorithm.",
            "Just trying to give you the intuition here.",
            "The basic idea behind the scoring algorithm is that we essentially want to assign a very low weight between those links that are within a given component, because if it's within a given component, that means that it's very similar to an existing source that we've already modeled, right that those that that kind of relationship already.",
            "We have an example of that kind of relationship already existing in a known source.",
            "And then we're going to await the other links according to these sort of green links.",
            "Which are the things that allow us to connect up.",
            "So let's say that you have a new source that isn't contained with one of these other existing known sources, right?",
            "That it might actually span multiple sources, so that's why we need these green links to sort of connect up these different components, and then we essentially saw assigned weight.",
            "So within a given component you have this epsilon wait were very small and there across components, we're going to start with a weight which is essentially the size of the graph.",
            "In this case it's 18, which is why you see some eighteens here and then we're going to actually subtract for some things to actually prefer certain kinds of links over other links.",
            "In fact, what the kinds of things we prefer.",
            "Or if you see this relationship here works for between a person and organization.",
            "Well, that turns out that relationship also exists within a component, so that's the one that we want to actually increase the likelihood that we would choose that link, so that ends up with a lower score Hero 17.",
            "And then there's this other links here, which end up with these funny 17.94, which is really 18 -- 1 / 18.",
            "The way those links get generated, those are similar in that they are also using a link that occurs in the graphs, but the difference is they only occur in terms of the label, the domain and ranges don't match exactly, so that the details of the scoring algorithm actually in the paper.",
            "But I just want to give you an intuition that you're creating this sort of larger graph then building these links between it and then assigning weights on all this on all these links."
        ],
        [
            "OK, So what do we do with this graph?",
            "Well, first, before we actually run the graph at all, the first we do is the first step that we did before, which would be.",
            "Essentially, you're going to learn these semantic types and then try to recognize them in a new source.",
            "So given this new source as four here, we're going to recognize at least try to recognize what the set of what we call the semantic types are for each of these attributes, and the user can refine them.",
            "It doesn't get it right, but then now starts at his input to our algorithm."
        ],
        [
            "Then the next step is to actually generate the candidate models.",
            "So we go back to the graph I showed you in a minute ago, and we're going to take each of those nodes from the semantic type that I essentially just type each of these things in S4, which is the Postal zip code.",
            "The city name in this state name, and I'm going to look those up.",
            "Essentially, I'm going to find those in this larger graph, and in fact there may be multiple mappings of these things into the graph, and then I'm going to compute, given those, those nodes become were called the Steiner nodes, and we computed Steiner tree, which is.",
            "Simply inefficient polynomial algorithm and its approximation algorithm for essentially finding the lowest costs graph or sub graph in this.",
            "In this larger graph that connects all of those Steiner nodes, those are the things that were actually the nodes in our source.",
            "So what that looks?"
        ],
        [
            "Like them is, let's say we take our source.",
            "Our first mapping.",
            "OK, we're going to set up possible mappings here that the algorithm actually produced.",
            "So what's happening here is the source here is being color coded to yellow, blue and purple here, and it's showing the corresponding things.",
            "Actually finding the graph.",
            "So this is 1 possible."
        ],
        [
            "Mapping to this tree and here is just the relevant part or the relevant sub graph would actually be used to assign this source and what it's doing is essentially computing this sub graph, finding the overall weight of this graph, which it turns out this one is going to have the lowest cost 'cause it only has one green link in it you'll see."
        ],
        [
            "Other ones have more than one green link in it, 'cause what it's really trying to do is prefer those that are within some component."
        ],
        [
            "This mapping 2, which is the 2nd possible mapping.",
            "You can see this one is is highly sub optimal because it's sort of taking one piece from one component, one piece, another component connecting up with these very expensive green links so that one's not going to cry."
        ],
        [
            "The third."
        ],
        [
            "One is this one here, which is again it picks a different set pieces from each one, so it will also be outside."
        ],
        [
            "And the 4th one it also gets."
        ],
        [
            "Some pieces from here, but just the way the model that works here.",
            "This one ends up with two green links here connecting place which connects to Postal code.",
            "So what we've gone through then is is identified each of the different possible models that could generate for this, and it's essentially gone through and computed those and then scores them."
        ],
        [
            "And then generates this rank list of source models.",
            "So here are the set of candidates.",
            "As I mentioned this first one, here is the candidate, one is the one that sorry is the one that has the lowest set of weights on the graph.",
            "So this is the one that would actually prefer, and in fact is the best model of this particular source."
        ],
        [
            "We evaluated this by taking two different datasets.",
            "First we took a datasets of geospatial data of data sources that consisted of 17 different sources that contain a lot of overlapping data, and we created these semantic descriptions of each one of these, and then we use the second data set, which instead of museum sources and also created the semantic descriptions of them using sort of museum based ontologies or cultural heritage ontologies.",
            "Rather, we then learned a source model.",
            "Where we assumed all of the other sources input and then said OK. What would be involved in action modeling this source?",
            "And then for each one of those we compute this graph edit distance between the learn model in the correct one.",
            "So how close did actually get to that?",
            "And that's you know that's in terms of these different sort of graph operations.",
            "And then we compared the results with our previous work on, So how in the previous system as I mentioned, it would generate one of these automatically, wouldn't get better overtime, and then the user would have to go and refine these models.",
            "So the question is.",
            "You know how many steps does it actually take to refine one of these models so the."
        ],
        [
            "Our results look like this, so for the first data set, which as I mentioned, consists of all these geospace related sources you can see here we have each of the source of the signature of the source, the number of attributes, and then the previous work.",
            "So this is the previous work on Karma and the graph edit distance.",
            "How many steps does it actually take to get from the model the system actually produces to the correct model, and so you can see here had a total over all of these of 68 steps for this new approach I just described in 29 steps, so that's a significant reduction here.",
            "So 57% reduction in terms of how much user interaction or how much.",
            "Edits are going to be required to actually get to the result that you want."
        ],
        [
            "Can data set we did also did well, though not quite as much improvement, and that's partly because there were just fewer sources to learn from, so this only had six sources in it.",
            "And here we go from 29 to 20.",
            "So we get a 31% improvement."
        ],
        [
            "There is related work so you know currently a lot of what happens in terms of being able to describe sources is to actually write these things by hand, so there's a lot of work on our tomorrow our to our email models Swirl most of the systems out there require you to simply write down their solutions by hand, so this is rather tedious and time consuming and requires a fair amount of expertise.",
            "There's been working the past on what I mentioned before, which is essentially annotating the services and also annotating the columns of information and stuff.",
            "But very little work and actually learning relationships.",
            "We previously with another graduate student, I did some work previously on learning semantic descriptions from online sources where we did something very similar.",
            "We would learn equivalent kinds of semantic descriptions in terms of rules, but the assumption that work was that you could only learn source descriptions that had sort of complete overlap or some enough overlap with an existing source actually completely built the model.",
            "And so here we're trying to allow the case where I can bring in completely new sources that may have some overlap may have no overlap.",
            "And be able to quickly learn the models of these sources.",
            "OK, so."
        ],
        [
            "I've described then is an approach to automatically build these rich semantic descriptions of data sources and the idea here is to be able to exploit background knowledge from the domain ontology and the known source models, which is really the the advance here in terms of over the past work.",
            "You know the reason that you want these kind of semantic descriptions so there really key to automate many kinds of tasks that we want to do.",
            "So, for example, be able to do automated source discovery data integration if I want to combine information across various sources, service composition will put these things together and so on."
        ],
        [
            "And then one last slide which is, you know, just to give you an idea of where we're going with this next one is we're looking at methods to actually create a more compact graph.",
            "So as you saw, one of the problems we have now is that we create these components for each of the different sources, and this isn't going to scale well when you have very large numbers of sources, right?",
            "And we want the system get better overtime, and so we're looking now at methods where we can actually build a more compact graph representation of the combination of all the sources.",
            "Second thing, we're also working on is the fact that one of the assumptions I mentioned as input was the fact that we assumed that you did the semantic typing step first.",
            "But in fact, really why you want to do that as a combined step?",
            "And so we're working on a new version of the algorithm now where it will actually do both?",
            "The combination of the semantic typing and choose the relationships all at the same time.",
            "And we also want to essentially use the data in linked open data to help bias the graph algorithm stuff.",
            "So when you have limited sets of information, we think there's other sources of data that we can actually use.",
            "And then finally we want to integrate this into, So currently this isn't yet in the.",
            "There's an open source version of Karma you can download and play with.",
            "It doesn't yet have this algorithm in there, but that's in process, so thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is work on a graph based approach to learn semantic descriptions of data sources.",
                    "label": 1
                },
                {
                    "sent": "This is work by one of my PhD students and listen to Harry and I'm Craig Knoblock.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so the problem is how to actually learn semantic descriptions?",
                    "label": 0
                },
                {
                    "sent": "That's what we want to solve here before I actually.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We tell you how we do that.",
                    "label": 0
                },
                {
                    "sent": "Let me first tell you what it is.",
                    "label": 0
                },
                {
                    "sent": "I mean by a semantic description itself.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, So what we're trying to do is essentially take a source and describe that source in terms of the concepts and relationships defined in some domain ontology.",
                    "label": 1
                },
                {
                    "sent": "So here's an example terminology, and this one is important to look at because we're going to use it as an example throughout my talk, so you see, you can see here that we have, you know.",
                    "label": 0
                },
                {
                    "sent": "Set of classes.",
                    "label": 0
                },
                {
                    "sent": "A person, organization, places, and then city and state which have this relationship, which is that the city has a state.",
                    "label": 0
                },
                {
                    "sent": "This is a property in the US for every city has some state it's within and that these are both subclasses of place.",
                    "label": 0
                },
                {
                    "sent": "OK, and then what we want to do is take ontology like this, which has a set of relationships and both object properties in data properties and then take the source.",
                    "label": 0
                },
                {
                    "sent": "So I have an example source at the bottom here at which has a set of columns of information.",
                    "label": 0
                },
                {
                    "sent": "What we want to do is take this data source and describe it in terms of this domain ontology.",
                    "label": 0
                },
                {
                    "sent": "Alright, So what does that actually mean?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we take the columns of information, but that would mean is that we eventually find the classes and the properties that correspond to each column of information.",
                    "label": 0
                },
                {
                    "sent": "So in the first column for example, we have a set of names.",
                    "label": 0
                },
                {
                    "sent": "You know, these are essentially the names of some person, so it's the property name from the class person.",
                    "label": 0
                },
                {
                    "sent": "Similarly, the second column is the birthdate of a person.",
                    "label": 0
                },
                {
                    "sent": "We have the name of an organization with the name of a city in the name of the state.",
                    "label": 0
                },
                {
                    "sent": "Now some people stop here and they say OK, Now I've got a description of all the columns of data, but in fact we're interested in actually something that's that's more complicated in this, which is we really want to understand the relationships between these different.",
                    "label": 0
                },
                {
                    "sent": "Different colors.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Information, so here what we really want to capture is an overall model of that this person.",
                    "label": 0
                },
                {
                    "sent": "OK, in this data source has a name and this is the same person has a name and a birthday, so it's not just that it's that these two columns are person, it's actually the same person and this is the name and birthday of this person that they're born in this particular city, which in turn has some state.",
                    "label": 0
                },
                {
                    "sent": "And then it works for some organization.",
                    "label": 0
                },
                {
                    "sent": "So all of these things together are going to essentially compose the description of the source, and for those who are familiar with the latest standards.",
                    "label": 0
                },
                {
                    "sent": "We can actually take this model and automatically convert this into a semantic description.",
                    "label": 1
                },
                {
                    "sent": "In terms of this R2 RML language, which is now a W3C standard.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we've done some previous work on actually learning the semantic descriptions, so in fact we built this system called Karma and the way Karma works today is on the left.",
                    "label": 0
                },
                {
                    "sent": "We simply take in a domain ontology and a set of data sample data.",
                    "label": 0
                },
                {
                    "sent": "The data source that we want to actually model and we go through a set of steps to actually build these semantic descriptions.",
                    "label": 1
                },
                {
                    "sent": "So the system basically learns these semantic types overtime from data that is seen in the past.",
                    "label": 0
                },
                {
                    "sent": "So we use a machine learning model to essentially automatically recognize this.",
                    "label": 0
                },
                {
                    "sent": "Properties in classes.",
                    "label": 0
                },
                {
                    "sent": "Then we go through an algorithm which is based on an efficient sort of Steiner tree algorithm for finding the really what we find.",
                    "label": 0
                },
                {
                    "sent": "Is this the simplest explanation that we can generate from the combination of these learn semantic types in the actual ontology graph.",
                    "label": 1
                },
                {
                    "sent": "The graph that's built from the Scientology and then automatically produce a semantic model.",
                    "label": 0
                },
                {
                    "sent": "So our goal here is to actually automate this as much as possible, but with the idea that you see little people on the top here that the user can actually refine the model of this system doesn't get it right, because sometimes there's just not enough information out there.",
                    "label": 0
                },
                {
                    "sent": "To completely automate this process of generating some exceptions, but our goal is to automate it as much as possible.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the current Karma system you might generate for an existent existing data source that we want to show, you might enter generate an actual model that looks like this, where it says OK, this data sources and organization has a CEO, which is a person who has a name and birthday.",
                    "label": 0
                },
                {
                    "sent": "Now this isn't actually quite right, and with system generated model like this, the user would then go in.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find this model and say no.",
                    "label": 0
                },
                {
                    "sent": "It's actually up.",
                    "label": 0
                },
                {
                    "sent": "The data source represents a person who works for an organization, is born in some city, and has some state.",
                    "label": 0
                },
                {
                    "sent": "So that's the correct model and one of the problems in the existing systems are one of the limitations.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Existing system is that the system doesn't actually get better at this overtime, right?",
                    "label": 0
                },
                {
                    "sent": "It learn the semantic types and get better overtime, but the actual overall description of the relationships in data source every time it sees the same source, you're going to have to make the same refinement, so that's not great.",
                    "label": 0
                },
                {
                    "sent": "So that's really what we were.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I'm going to describe next to the approach that we now have developed to actually learn these semantic subscription so that we can refine these models and have the system get better overtime as it sees more examples.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so the key idea then is that we're going to see sources in the same domain, and they're often going to have very similar kinds of data in similar kinds of properties in those data sources, and we want to exploit the knowledge of the existing source descriptions or source models so that when we see a new source were more likely to actually get the description correct.",
                    "label": 0
                },
                {
                    "sent": "So in what we do this by leveraging these relationships in this known source models to essentially hypothesized relationship for the new source models, and as it sees more and more examples, it should be more likely to converge on the correct one.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the approach now looks like this.",
                    "label": 0
                },
                {
                    "sent": "So instead of before, if you recall the input to the system was the domain ontology and some new source that is going to model.",
                    "label": 0
                },
                {
                    "sent": "Now we've added an additional input here, which is now we actually passed to the system.",
                    "label": 0
                },
                {
                    "sent": "The set of known source models.",
                    "label": 1
                },
                {
                    "sent": "These are the ones that you've generated in the past, so the system knows about.",
                    "label": 0
                },
                {
                    "sent": "Given that, we then construct this graph, which I'll describe how we do that in a minute.",
                    "label": 0
                },
                {
                    "sent": "As before, we learn these semantic types for the property, and right now in the current system.",
                    "label": 0
                },
                {
                    "sent": "The user would then refine these types, so if the system generates some initial set of types, the user could then go and refine them.",
                    "label": 0
                },
                {
                    "sent": "Who doesn't get them correct.",
                    "label": 0
                },
                {
                    "sent": "It then takes that, generates a set of candidate models, and then ranks the results, and I'll talk about each of these steps in turn.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's back to my example.",
                    "label": 0
                },
                {
                    "sent": "So as before I have the ontology that I showed you earlier on in the talk and then I have 3 known sources in the corresponding models, so the sources are S1 here, which is a set of person info I have S2 which is get cities and then S3, which is business info in each of these has a related model, right?",
                    "label": 0
                },
                {
                    "sent": "So they're not, they're not the same, but they each have essentially this source description or source models, shown here right above the source and you can see that one pattern I want to show you in this is that.",
                    "label": 0
                },
                {
                    "sent": "For the S1 you can see that the person is born in this city, which has a state and for S2 essentially provides information about cities in the corresponding states.",
                    "label": 0
                },
                {
                    "sent": "In S3 is a little different that has a city which is a part of a state, which is a possible relationship in the in the hierarchy and the ontology right?",
                    "label": 0
                },
                {
                    "sent": "So you can see that these are city and state are subclasses of place.",
                    "label": 0
                },
                {
                    "sent": "And so now I have a new source, so I'll come along with S4 and I want to actually build a model for Task Force.",
                    "label": 0
                },
                {
                    "sent": "Quite simple 'cause it needs to fit on the slide.",
                    "label": 0
                },
                {
                    "sent": "But As for essentially, is this Postal code look up which essentially given a Postal code looks up the city and state.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the algorithm now is.",
                    "label": 0
                },
                {
                    "sent": "Essentially, we're going to build.",
                    "label": 0
                },
                {
                    "sent": "First, we're going to build this graph from the known source models.",
                    "label": 1
                },
                {
                    "sent": "So and what we do here is we're going to create a component in this graph for each of the known source models.",
                    "label": 0
                },
                {
                    "sent": "So each of those three sources I saw you before we would create a component in G and we're only going to add it if the model is not already a subgraph from the existing component.",
                    "label": 0
                },
                {
                    "sent": "I'll show you that example data 2nd, and then we're going to annotate each of the links with essentially the list of supporting models.",
                    "label": 0
                },
                {
                    "sent": "So here's what that looks like.",
                    "label": 0
                },
                {
                    "sent": "So first I start with S1, which is one of the existing known sources.",
                    "label": 0
                },
                {
                    "sent": "How can I build my first part of my graph, which is basically I'm just going to plug in the graph that I have for S1 here.",
                    "label": 0
                },
                {
                    "sent": "Next I'm going to take S2 and S2.",
                    "label": 0
                },
                {
                    "sent": "Is this one where we essentially it is a sub graph of this other model, so it has this property that you know it has city related to state and has it has these properties of these and you can see that it's a sub graph of the existing component one so we don't have to add anything additional to the graph.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to add S 3 but S3 is actually constructed quite differently than.",
                    "label": 0
                },
                {
                    "sent": "S1 and S2 because it has that that different relational structure.",
                    "label": 0
                },
                {
                    "sent": "There we used part of instead of this state being directly connected to the city.",
                    "label": 0
                },
                {
                    "sent": "So it's going to create a new component.",
                    "label": 0
                },
                {
                    "sent": "Now the next step.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is to actually connect these components together, so I know this this picture starting a little busy but just bear with me here 'cause it's just to give you the general intuition about what we're doing here.",
                    "label": 0
                },
                {
                    "sent": "So you see I have component one here and I have component to here and then what we do is we actually look at the ways we can actually connect these components together.",
                    "label": 0
                },
                {
                    "sent": "Given the ontology and that creates all the green links here.",
                    "label": 0
                },
                {
                    "sent": "They probably hard to read from where you are, but the idea is simply that it looks for ways to actually connect up these models in the ontology graph.",
                    "label": 0
                },
                {
                    "sent": "We generate all of this.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We do something which we need to score these and I'm going to save you the details of actually the detailed scoring algorithm.",
                    "label": 0
                },
                {
                    "sent": "Just trying to give you the intuition here.",
                    "label": 0
                },
                {
                    "sent": "The basic idea behind the scoring algorithm is that we essentially want to assign a very low weight between those links that are within a given component, because if it's within a given component, that means that it's very similar to an existing source that we've already modeled, right that those that that kind of relationship already.",
                    "label": 0
                },
                {
                    "sent": "We have an example of that kind of relationship already existing in a known source.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to await the other links according to these sort of green links.",
                    "label": 0
                },
                {
                    "sent": "Which are the things that allow us to connect up.",
                    "label": 0
                },
                {
                    "sent": "So let's say that you have a new source that isn't contained with one of these other existing known sources, right?",
                    "label": 0
                },
                {
                    "sent": "That it might actually span multiple sources, so that's why we need these green links to sort of connect up these different components, and then we essentially saw assigned weight.",
                    "label": 0
                },
                {
                    "sent": "So within a given component you have this epsilon wait were very small and there across components, we're going to start with a weight which is essentially the size of the graph.",
                    "label": 0
                },
                {
                    "sent": "In this case it's 18, which is why you see some eighteens here and then we're going to actually subtract for some things to actually prefer certain kinds of links over other links.",
                    "label": 0
                },
                {
                    "sent": "In fact, what the kinds of things we prefer.",
                    "label": 0
                },
                {
                    "sent": "Or if you see this relationship here works for between a person and organization.",
                    "label": 0
                },
                {
                    "sent": "Well, that turns out that relationship also exists within a component, so that's the one that we want to actually increase the likelihood that we would choose that link, so that ends up with a lower score Hero 17.",
                    "label": 0
                },
                {
                    "sent": "And then there's this other links here, which end up with these funny 17.94, which is really 18 -- 1 / 18.",
                    "label": 0
                },
                {
                    "sent": "The way those links get generated, those are similar in that they are also using a link that occurs in the graphs, but the difference is they only occur in terms of the label, the domain and ranges don't match exactly, so that the details of the scoring algorithm actually in the paper.",
                    "label": 0
                },
                {
                    "sent": "But I just want to give you an intuition that you're creating this sort of larger graph then building these links between it and then assigning weights on all this on all these links.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what do we do with this graph?",
                    "label": 0
                },
                {
                    "sent": "Well, first, before we actually run the graph at all, the first we do is the first step that we did before, which would be.",
                    "label": 0
                },
                {
                    "sent": "Essentially, you're going to learn these semantic types and then try to recognize them in a new source.",
                    "label": 0
                },
                {
                    "sent": "So given this new source as four here, we're going to recognize at least try to recognize what the set of what we call the semantic types are for each of these attributes, and the user can refine them.",
                    "label": 0
                },
                {
                    "sent": "It doesn't get it right, but then now starts at his input to our algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the next step is to actually generate the candidate models.",
                    "label": 0
                },
                {
                    "sent": "So we go back to the graph I showed you in a minute ago, and we're going to take each of those nodes from the semantic type that I essentially just type each of these things in S4, which is the Postal zip code.",
                    "label": 0
                },
                {
                    "sent": "The city name in this state name, and I'm going to look those up.",
                    "label": 0
                },
                {
                    "sent": "Essentially, I'm going to find those in this larger graph, and in fact there may be multiple mappings of these things into the graph, and then I'm going to compute, given those, those nodes become were called the Steiner nodes, and we computed Steiner tree, which is.",
                    "label": 0
                },
                {
                    "sent": "Simply inefficient polynomial algorithm and its approximation algorithm for essentially finding the lowest costs graph or sub graph in this.",
                    "label": 0
                },
                {
                    "sent": "In this larger graph that connects all of those Steiner nodes, those are the things that were actually the nodes in our source.",
                    "label": 0
                },
                {
                    "sent": "So what that looks?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like them is, let's say we take our source.",
                    "label": 0
                },
                {
                    "sent": "Our first mapping.",
                    "label": 0
                },
                {
                    "sent": "OK, we're going to set up possible mappings here that the algorithm actually produced.",
                    "label": 0
                },
                {
                    "sent": "So what's happening here is the source here is being color coded to yellow, blue and purple here, and it's showing the corresponding things.",
                    "label": 0
                },
                {
                    "sent": "Actually finding the graph.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 possible.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mapping to this tree and here is just the relevant part or the relevant sub graph would actually be used to assign this source and what it's doing is essentially computing this sub graph, finding the overall weight of this graph, which it turns out this one is going to have the lowest cost 'cause it only has one green link in it you'll see.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other ones have more than one green link in it, 'cause what it's really trying to do is prefer those that are within some component.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This mapping 2, which is the 2nd possible mapping.",
                    "label": 0
                },
                {
                    "sent": "You can see this one is is highly sub optimal because it's sort of taking one piece from one component, one piece, another component connecting up with these very expensive green links so that one's not going to cry.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The third.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One is this one here, which is again it picks a different set pieces from each one, so it will also be outside.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the 4th one it also gets.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some pieces from here, but just the way the model that works here.",
                    "label": 0
                },
                {
                    "sent": "This one ends up with two green links here connecting place which connects to Postal code.",
                    "label": 0
                },
                {
                    "sent": "So what we've gone through then is is identified each of the different possible models that could generate for this, and it's essentially gone through and computed those and then scores them.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then generates this rank list of source models.",
                    "label": 0
                },
                {
                    "sent": "So here are the set of candidates.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned this first one, here is the candidate, one is the one that sorry is the one that has the lowest set of weights on the graph.",
                    "label": 0
                },
                {
                    "sent": "So this is the one that would actually prefer, and in fact is the best model of this particular source.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We evaluated this by taking two different datasets.",
                    "label": 0
                },
                {
                    "sent": "First we took a datasets of geospatial data of data sources that consisted of 17 different sources that contain a lot of overlapping data, and we created these semantic descriptions of each one of these, and then we use the second data set, which instead of museum sources and also created the semantic descriptions of them using sort of museum based ontologies or cultural heritage ontologies.",
                    "label": 0
                },
                {
                    "sent": "Rather, we then learned a source model.",
                    "label": 0
                },
                {
                    "sent": "Where we assumed all of the other sources input and then said OK. What would be involved in action modeling this source?",
                    "label": 0
                },
                {
                    "sent": "And then for each one of those we compute this graph edit distance between the learn model in the correct one.",
                    "label": 0
                },
                {
                    "sent": "So how close did actually get to that?",
                    "label": 0
                },
                {
                    "sent": "And that's you know that's in terms of these different sort of graph operations.",
                    "label": 0
                },
                {
                    "sent": "And then we compared the results with our previous work on, So how in the previous system as I mentioned, it would generate one of these automatically, wouldn't get better overtime, and then the user would have to go and refine these models.",
                    "label": 0
                },
                {
                    "sent": "So the question is.",
                    "label": 0
                },
                {
                    "sent": "You know how many steps does it actually take to refine one of these models so the.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our results look like this, so for the first data set, which as I mentioned, consists of all these geospace related sources you can see here we have each of the source of the signature of the source, the number of attributes, and then the previous work.",
                    "label": 0
                },
                {
                    "sent": "So this is the previous work on Karma and the graph edit distance.",
                    "label": 1
                },
                {
                    "sent": "How many steps does it actually take to get from the model the system actually produces to the correct model, and so you can see here had a total over all of these of 68 steps for this new approach I just described in 29 steps, so that's a significant reduction here.",
                    "label": 0
                },
                {
                    "sent": "So 57% reduction in terms of how much user interaction or how much.",
                    "label": 0
                },
                {
                    "sent": "Edits are going to be required to actually get to the result that you want.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can data set we did also did well, though not quite as much improvement, and that's partly because there were just fewer sources to learn from, so this only had six sources in it.",
                    "label": 0
                },
                {
                    "sent": "And here we go from 29 to 20.",
                    "label": 0
                },
                {
                    "sent": "So we get a 31% improvement.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is related work so you know currently a lot of what happens in terms of being able to describe sources is to actually write these things by hand, so there's a lot of work on our tomorrow our to our email models Swirl most of the systems out there require you to simply write down their solutions by hand, so this is rather tedious and time consuming and requires a fair amount of expertise.",
                    "label": 0
                },
                {
                    "sent": "There's been working the past on what I mentioned before, which is essentially annotating the services and also annotating the columns of information and stuff.",
                    "label": 0
                },
                {
                    "sent": "But very little work and actually learning relationships.",
                    "label": 0
                },
                {
                    "sent": "We previously with another graduate student, I did some work previously on learning semantic descriptions from online sources where we did something very similar.",
                    "label": 0
                },
                {
                    "sent": "We would learn equivalent kinds of semantic descriptions in terms of rules, but the assumption that work was that you could only learn source descriptions that had sort of complete overlap or some enough overlap with an existing source actually completely built the model.",
                    "label": 0
                },
                {
                    "sent": "And so here we're trying to allow the case where I can bring in completely new sources that may have some overlap may have no overlap.",
                    "label": 0
                },
                {
                    "sent": "And be able to quickly learn the models of these sources.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've described then is an approach to automatically build these rich semantic descriptions of data sources and the idea here is to be able to exploit background knowledge from the domain ontology and the known source models, which is really the the advance here in terms of over the past work.",
                    "label": 0
                },
                {
                    "sent": "You know the reason that you want these kind of semantic descriptions so there really key to automate many kinds of tasks that we want to do.",
                    "label": 0
                },
                {
                    "sent": "So, for example, be able to do automated source discovery data integration if I want to combine information across various sources, service composition will put these things together and so on.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then one last slide which is, you know, just to give you an idea of where we're going with this next one is we're looking at methods to actually create a more compact graph.",
                    "label": 0
                },
                {
                    "sent": "So as you saw, one of the problems we have now is that we create these components for each of the different sources, and this isn't going to scale well when you have very large numbers of sources, right?",
                    "label": 0
                },
                {
                    "sent": "And we want the system get better overtime, and so we're looking now at methods where we can actually build a more compact graph representation of the combination of all the sources.",
                    "label": 0
                },
                {
                    "sent": "Second thing, we're also working on is the fact that one of the assumptions I mentioned as input was the fact that we assumed that you did the semantic typing step first.",
                    "label": 0
                },
                {
                    "sent": "But in fact, really why you want to do that as a combined step?",
                    "label": 0
                },
                {
                    "sent": "And so we're working on a new version of the algorithm now where it will actually do both?",
                    "label": 0
                },
                {
                    "sent": "The combination of the semantic typing and choose the relationships all at the same time.",
                    "label": 0
                },
                {
                    "sent": "And we also want to essentially use the data in linked open data to help bias the graph algorithm stuff.",
                    "label": 0
                },
                {
                    "sent": "So when you have limited sets of information, we think there's other sources of data that we can actually use.",
                    "label": 0
                },
                {
                    "sent": "And then finally we want to integrate this into, So currently this isn't yet in the.",
                    "label": 0
                },
                {
                    "sent": "There's an open source version of Karma you can download and play with.",
                    "label": 0
                },
                {
                    "sent": "It doesn't yet have this algorithm in there, but that's in process, so thank you.",
                    "label": 0
                }
            ]
        }
    }
}