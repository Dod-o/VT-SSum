{
    "id": "r7a6qzgzpvlvxp5sgx674c7iaokjyjp4",
    "title": "Semi-Supervised Graph Embedding Scheme with Active Learning (SSGEAL): Classifying High Dimensional Biomedical Data",
    "info": {
        "author": [
            "George Lee, Laboratory for Computational Imaging and Bioinformatics, Rutgers, The State University of New Jersey"
        ],
        "published": "Oct. 14, 2010",
        "recorded": "September 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/prib2010_lee_ssge/",
    "segmentation": [
        [
            "My name is George Lee.",
            "I'm the chair and I'll be the first presenter today.",
            "OK, so no further ado.",
            "Yeah, so I'm George Leon from Ruckers University.",
            "I'll be doing my work on semi supervised graph embedding with active learning as it pertains to biomedical gene expression data."
        ],
        [
            "So as an introduction, genics pression data is often difficult to try to analyze because the data is going to be high dimensional.",
            "You have your samples are described by a large feature vector containing of thousands of genes."
        ],
        [
            "So this results in the curse of dimensionality.",
            "So for those of you who aren't familiar with the curse of dimensionality is the case where you have too many features.",
            "Being used to describe your sample set.",
            "So why is this a problem?",
            "If you see here?",
            "We have a support vector machine classifier trying to discriminate between two samples in a 3 dimensional feature space.",
            "This is kind of exaggerated example, but."
        ],
        [
            "Going to be difficult to find.",
            "What is the best discriminant having too few samples describing your large feature space?"
        ],
        [
            "OK, so our goal is going to be to kind of reduce that high dimensional gene expression data into a low dimensional embedding 'cause consisting of a couple Jenny."
        ],
        [
            "American vectors.",
            "So currently there's a several strategies exist for trying to improve your classification.",
            "There is a semi supervised learning active learning dimension reduction to name a few."
        ],
        [
            "So my novel contribution is to try to use my methods deal to leverage all three of these methods into one new dimension reduction method to try to improve the classification of."
        ],
        [
            "Gene expression.",
            "So I'm going to start with some background on each of those ideas to measure reduction, semi supervised, dimension reduction and."
        ],
        [
            "Active learning.",
            "Dimensional reduction kind of assumes that if you have your high dimensional space that you can find a lower dimensional representation of that high dimensional space within it.",
            "So if there exists a low dimensional representation for your complex data that you can represent it instead of not using, you know hundreds of features, but using you know maybe 2 to 10 features.",
            "Either way, much smaller amount than you were using before, which helps with classification ultimately."
        ],
        [
            "So next time I go to semisupervised mention.",
            "So in typical dimension reduction applications, your data is going to be.",
            "Unlabeled data collection is as unsupervised.",
            "So, so this is just example with graph embedding where they try to use a kernel matrix to show the similarity between each pair of points.",
            "So I show a sample ziana sample XJ and they are similar based on the high dimensional feature space by something like the Gaussian distance here.",
            "Using the similarity matrix, you can use that information to find the low dimensional feature vectors for the new embedding space.",
            "This is often does not give you the best results, however, by using class labels we learn that we can improve the quality of the embedding.",
            "So imagine that this is a 2 dimensional embedding of the high dimensional space, so the originally you didn't have the label information for XI and XJ.",
            "However."
        ],
        [
            "If you include that information, we can assume that.",
            "XNXJ can be much closer than they are in this embedding, so you couldn't tell the difference between this sample and this sample the same distance apart, so this is opposite class sample.",
            "By using the class labels you can add a way to show that these are two samples are much more similar than they appear to be in the original fee."
        ],
        [
            "Airspace.",
            "So that's so that such that in the new embedding first, semi supervised case this samples are of the same labels that you know the same labels they're going to be mapped closer together in the embedding space."
        ],
        [
            "Yes.",
            "And you can see that in the results of our synthetic data set, showing that the unsupervised mention reduction method graph embedding does not perform as well as SSH.",
            "I semi supervised method where you have some labeled information being implemented."
        ],
        [
            "Data.",
            "This brings us to our third module, the active learning.",
            "So the idea behind active learning is to try to improve classification by querying samples that are going to improve your classification model.",
            "So in this case we have our data were given some initial training samples in green and red, and you try to build a classifier using their trade your training data.",
            "So yes, we build a decision boundary in this case support vector."
        ],
        [
            "And then we are able to find the samples that are going to be contributing most to classification improvement by selecting samples that are close to the boundary.",
            "These are samples that are going to be the most ambiguous, the most difficult to classify well for an SVM."
        ],
        [
            "By using that information and obtain the labels for that, you can build a new classifier using these active learning acquired labels to produce a better discriminate the second time around when you're trying to produce a new SVM model.",
            "So that's the idea for active learning, so that when you query additional labels that you get a better classify."
        ],
        [
            "In the end.",
            "And you can see just kind of ahead of myself here, but this is my method using the active learning where you can see that these samples that were difficult to classify for regular SSH with just random labeling by using active learning to improve your labeling, you can see a better result."
        ],
        [
            "So, just to recap of all the dimension reduction methods I just threw out, US age is a semi supervised embedding measure, semi supervised dimension reduction scheme.",
            "We showed that it was better than unsupervised dimension reduction and similarly for SCO is able to perform better in SSH because it's able to use the informative labels."
        ],
        [
            "Active learning.",
            "OK, so now a quick overview on my method, SSG."
        ],
        [
            "SQL is an iterative approach for leveraging active learning and semi supervised measure reduction to produce a better final."
        ],
        [
            "Bedding.",
            "We begin with the first active learning step where you embed the information into a low dimensional space using methods such as graph embedding.",
            "You have some initial training levels to kickstart the active learning method, so as we showed before, these labels help reduce ask decision boundary.",
            "We can obtain additional labels from that.",
            "And you can add that to your training samples."
        ],
        [
            "So keep an eye on these training samples.",
            "We can.",
            "We're going to use them to improve our semi supervised dimension reduction bedding.",
            "So I should remember before SDR is going to use some label information.",
            "So now we've acquired our initial kick starting training labels in addition to the labels that we require for active learning.",
            "We use that information as well as the similarity information from the original embedding.",
            "So basically the pairwise distances between each of the samples, and we can use that information to acquire.",
            "A new embedding using SDR.",
            "This new embedding actually provides a new, better feature space for two."
        ],
        [
            "Active learning again.",
            "So you see here on the new embedding we can continue finding active learning candidates to try to improve."
        ],
        [
            "Our scheme, so it's kind of like a positive feedback loop where you kind of have active learning instead of supervised dimension reduction.",
            "Iterating back and forth to try to boost each other.",
            "In the end, you can watch either run out of labels or show that you're in bed."
        ],
        [
            "It's good enough you finally will obtain your final embedding, which is able to show the unique clustering of the original data."
        ],
        [
            "OK, so here's an overview of."
        ],
        [
            "Experimental design we use seven binary class gene expression datasets, so these are publicly available datasets that have been looked over by many other papers.",
            "So the goal for for the embedding is to try to discriminate between the cancer classes.",
            "So for example, with the prostate cancer class you're trying to differentiate between 25 tumor samples and the nine normal samples, and this is a good data set because it's a high dimensional and you're trying to produce a low dimensional bending version of."
        ],
        [
            "We compare the three dimension reduction methods as we discussed before.",
            "We have graph embedding which is unsupervised method.",
            "We have all the way you have all the information but no labels for SAJ.",
            "You have some labels and for my method we also have the same number of labels as SAJ.",
            "By using the active learning to find those labels and all the methods are going to be mapped to two dimensions."
        ],
        [
            "To test the results, we use two qualitative embedding measures.",
            "One is measure of cluster overlap Tassilo End index which takes the mean within cluster within class distances and the main intercluster distances and tries to minimize the within class distances and maximized interclass distance is so the idea is that if you have a solid mix of 1 your sample circuit your classes samples are similar to each other.",
            "I'm going to be very close together and then just classes that are samples are the opposing classes are going to be very far apart in the embedding space.",
            "So this is kind of like a visualization method for trying to show that the embedding is going to be."
        ],
        [
            "There.",
            "Our second evaluation measure is just using random forest AUC.",
            "Random viruses simply just a bag of decision trees and Daisy is going to tell you definitively how well the data is going to be classified in a classic."
        ],
        [
            "Setting.",
            "So these are the results for the three methods for the cell index.",
            "You can see the SGL is able to show the better class separation compared to graph embedding an SSH three different datasets just shown here how to 7 similar classification AUC.",
            "You see similar trend as with the Silhouette index as can be expected."
        ],
        [
            "So some concluding remarks formalized a new dimension reduction scheme that tries to use the ideas of active learning and semi supervised method action to improve the embedding and improve the classification.",
            "And also we've shown that SGO is able to outperform essays using the same 7 gene expression datasets as well as two evaluation measures."
        ],
        [
            "These are funding sources.",
            "Thank you very much.",
            "So any questions?",
            "This animation is on auto sample.",
            "So we took a subset of the information and evaluation is based off.",
            "We have we have all the labels, so it's not really out of sample, but we're using a small subset of labels for producing the embedding.",
            "So.",
            "No.",
            "Miami Dade over withheld from so we have all those.",
            "And the classifier is real sensitive data intended results shows on consecutive data.",
            "No.",
            "There there.",
            "So the classification results that we use the two evaluation measures they.",
            "There's just cross validations of all the examples, so it's.",
            "So it's not exclusive to each other.",
            "Is cross validation so yeah, so it looks at all the sample points.",
            "English.",
            "Sure.",
            "And how do you know how many features should be?",
            "It's a good question.",
            "We've previously.",
            "Older features except one.",
            "We could.",
            "Could leave.",
            "Or maybe 3.",
            "Right, so we previously we've done studies.",
            "Feature index yeah, so we've done right.",
            "So we've done different experiments to try it on.",
            "You know, 234 all the way up to 20 dimensionality for data to see which.",
            "Responsible for selectivity level.",
            "From selectivity level.",
            "Maybe it's informal.",
            "Well, essentially we just use our invite.",
            "How many features should be removed?",
            "Well, the algorithm doesn't know.",
            "You have to look at, you know, explore the feature space essentially to see which you know.",
            "Embedding dimensionality to reduce it to.",
            "There's something we're definitely looking to explore, you know, try to figure out which combinations of parameter settings are going to give you solid results."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is George Lee.",
                    "label": 1
                },
                {
                    "sent": "I'm the chair and I'll be the first presenter today.",
                    "label": 0
                },
                {
                    "sent": "OK, so no further ado.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I'm George Leon from Ruckers University.",
                    "label": 0
                },
                {
                    "sent": "I'll be doing my work on semi supervised graph embedding with active learning as it pertains to biomedical gene expression data.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as an introduction, genics pression data is often difficult to try to analyze because the data is going to be high dimensional.",
                    "label": 0
                },
                {
                    "sent": "You have your samples are described by a large feature vector containing of thousands of genes.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this results in the curse of dimensionality.",
                    "label": 0
                },
                {
                    "sent": "So for those of you who aren't familiar with the curse of dimensionality is the case where you have too many features.",
                    "label": 1
                },
                {
                    "sent": "Being used to describe your sample set.",
                    "label": 0
                },
                {
                    "sent": "So why is this a problem?",
                    "label": 0
                },
                {
                    "sent": "If you see here?",
                    "label": 1
                },
                {
                    "sent": "We have a support vector machine classifier trying to discriminate between two samples in a 3 dimensional feature space.",
                    "label": 0
                },
                {
                    "sent": "This is kind of exaggerated example, but.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to be difficult to find.",
                    "label": 0
                },
                {
                    "sent": "What is the best discriminant having too few samples describing your large feature space?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so our goal is going to be to kind of reduce that high dimensional gene expression data into a low dimensional embedding 'cause consisting of a couple Jenny.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "American vectors.",
                    "label": 0
                },
                {
                    "sent": "So currently there's a several strategies exist for trying to improve your classification.",
                    "label": 1
                },
                {
                    "sent": "There is a semi supervised learning active learning dimension reduction to name a few.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So my novel contribution is to try to use my methods deal to leverage all three of these methods into one new dimension reduction method to try to improve the classification of.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gene expression.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to start with some background on each of those ideas to measure reduction, semi supervised, dimension reduction and.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Active learning.",
                    "label": 0
                },
                {
                    "sent": "Dimensional reduction kind of assumes that if you have your high dimensional space that you can find a lower dimensional representation of that high dimensional space within it.",
                    "label": 1
                },
                {
                    "sent": "So if there exists a low dimensional representation for your complex data that you can represent it instead of not using, you know hundreds of features, but using you know maybe 2 to 10 features.",
                    "label": 0
                },
                {
                    "sent": "Either way, much smaller amount than you were using before, which helps with classification ultimately.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So next time I go to semisupervised mention.",
                    "label": 0
                },
                {
                    "sent": "So in typical dimension reduction applications, your data is going to be.",
                    "label": 0
                },
                {
                    "sent": "Unlabeled data collection is as unsupervised.",
                    "label": 0
                },
                {
                    "sent": "So, so this is just example with graph embedding where they try to use a kernel matrix to show the similarity between each pair of points.",
                    "label": 0
                },
                {
                    "sent": "So I show a sample ziana sample XJ and they are similar based on the high dimensional feature space by something like the Gaussian distance here.",
                    "label": 0
                },
                {
                    "sent": "Using the similarity matrix, you can use that information to find the low dimensional feature vectors for the new embedding space.",
                    "label": 0
                },
                {
                    "sent": "This is often does not give you the best results, however, by using class labels we learn that we can improve the quality of the embedding.",
                    "label": 0
                },
                {
                    "sent": "So imagine that this is a 2 dimensional embedding of the high dimensional space, so the originally you didn't have the label information for XI and XJ.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you include that information, we can assume that.",
                    "label": 0
                },
                {
                    "sent": "XNXJ can be much closer than they are in this embedding, so you couldn't tell the difference between this sample and this sample the same distance apart, so this is opposite class sample.",
                    "label": 0
                },
                {
                    "sent": "By using the class labels you can add a way to show that these are two samples are much more similar than they appear to be in the original fee.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Airspace.",
                    "label": 0
                },
                {
                    "sent": "So that's so that such that in the new embedding first, semi supervised case this samples are of the same labels that you know the same labels they're going to be mapped closer together in the embedding space.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "And you can see that in the results of our synthetic data set, showing that the unsupervised mention reduction method graph embedding does not perform as well as SSH.",
                    "label": 0
                },
                {
                    "sent": "I semi supervised method where you have some labeled information being implemented.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Data.",
                    "label": 0
                },
                {
                    "sent": "This brings us to our third module, the active learning.",
                    "label": 1
                },
                {
                    "sent": "So the idea behind active learning is to try to improve classification by querying samples that are going to improve your classification model.",
                    "label": 0
                },
                {
                    "sent": "So in this case we have our data were given some initial training samples in green and red, and you try to build a classifier using their trade your training data.",
                    "label": 1
                },
                {
                    "sent": "So yes, we build a decision boundary in this case support vector.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we are able to find the samples that are going to be contributing most to classification improvement by selecting samples that are close to the boundary.",
                    "label": 0
                },
                {
                    "sent": "These are samples that are going to be the most ambiguous, the most difficult to classify well for an SVM.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By using that information and obtain the labels for that, you can build a new classifier using these active learning acquired labels to produce a better discriminate the second time around when you're trying to produce a new SVM model.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea for active learning, so that when you query additional labels that you get a better classify.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the end.",
                    "label": 0
                },
                {
                    "sent": "And you can see just kind of ahead of myself here, but this is my method using the active learning where you can see that these samples that were difficult to classify for regular SSH with just random labeling by using active learning to improve your labeling, you can see a better result.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, just to recap of all the dimension reduction methods I just threw out, US age is a semi supervised embedding measure, semi supervised dimension reduction scheme.",
                    "label": 0
                },
                {
                    "sent": "We showed that it was better than unsupervised dimension reduction and similarly for SCO is able to perform better in SSH because it's able to use the informative labels.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Active learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so now a quick overview on my method, SSG.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "SQL is an iterative approach for leveraging active learning and semi supervised measure reduction to produce a better final.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bedding.",
                    "label": 0
                },
                {
                    "sent": "We begin with the first active learning step where you embed the information into a low dimensional space using methods such as graph embedding.",
                    "label": 0
                },
                {
                    "sent": "You have some initial training levels to kickstart the active learning method, so as we showed before, these labels help reduce ask decision boundary.",
                    "label": 1
                },
                {
                    "sent": "We can obtain additional labels from that.",
                    "label": 0
                },
                {
                    "sent": "And you can add that to your training samples.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So keep an eye on these training samples.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "We're going to use them to improve our semi supervised dimension reduction bedding.",
                    "label": 0
                },
                {
                    "sent": "So I should remember before SDR is going to use some label information.",
                    "label": 0
                },
                {
                    "sent": "So now we've acquired our initial kick starting training labels in addition to the labels that we require for active learning.",
                    "label": 0
                },
                {
                    "sent": "We use that information as well as the similarity information from the original embedding.",
                    "label": 1
                },
                {
                    "sent": "So basically the pairwise distances between each of the samples, and we can use that information to acquire.",
                    "label": 1
                },
                {
                    "sent": "A new embedding using SDR.",
                    "label": 0
                },
                {
                    "sent": "This new embedding actually provides a new, better feature space for two.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Active learning again.",
                    "label": 0
                },
                {
                    "sent": "So you see here on the new embedding we can continue finding active learning candidates to try to improve.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our scheme, so it's kind of like a positive feedback loop where you kind of have active learning instead of supervised dimension reduction.",
                    "label": 0
                },
                {
                    "sent": "Iterating back and forth to try to boost each other.",
                    "label": 0
                },
                {
                    "sent": "In the end, you can watch either run out of labels or show that you're in bed.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's good enough you finally will obtain your final embedding, which is able to show the unique clustering of the original data.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's an overview of.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experimental design we use seven binary class gene expression datasets, so these are publicly available datasets that have been looked over by many other papers.",
                    "label": 1
                },
                {
                    "sent": "So the goal for for the embedding is to try to discriminate between the cancer classes.",
                    "label": 1
                },
                {
                    "sent": "So for example, with the prostate cancer class you're trying to differentiate between 25 tumor samples and the nine normal samples, and this is a good data set because it's a high dimensional and you're trying to produce a low dimensional bending version of.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We compare the three dimension reduction methods as we discussed before.",
                    "label": 0
                },
                {
                    "sent": "We have graph embedding which is unsupervised method.",
                    "label": 1
                },
                {
                    "sent": "We have all the way you have all the information but no labels for SAJ.",
                    "label": 0
                },
                {
                    "sent": "You have some labels and for my method we also have the same number of labels as SAJ.",
                    "label": 1
                },
                {
                    "sent": "By using the active learning to find those labels and all the methods are going to be mapped to two dimensions.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To test the results, we use two qualitative embedding measures.",
                    "label": 0
                },
                {
                    "sent": "One is measure of cluster overlap Tassilo End index which takes the mean within cluster within class distances and the main intercluster distances and tries to minimize the within class distances and maximized interclass distance is so the idea is that if you have a solid mix of 1 your sample circuit your classes samples are similar to each other.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be very close together and then just classes that are samples are the opposing classes are going to be very far apart in the embedding space.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of like a visualization method for trying to show that the embedding is going to be.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "Our second evaluation measure is just using random forest AUC.",
                    "label": 1
                },
                {
                    "sent": "Random viruses simply just a bag of decision trees and Daisy is going to tell you definitively how well the data is going to be classified in a classic.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Setting.",
                    "label": 0
                },
                {
                    "sent": "So these are the results for the three methods for the cell index.",
                    "label": 0
                },
                {
                    "sent": "You can see the SGL is able to show the better class separation compared to graph embedding an SSH three different datasets just shown here how to 7 similar classification AUC.",
                    "label": 0
                },
                {
                    "sent": "You see similar trend as with the Silhouette index as can be expected.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So some concluding remarks formalized a new dimension reduction scheme that tries to use the ideas of active learning and semi supervised method action to improve the embedding and improve the classification.",
                    "label": 0
                },
                {
                    "sent": "And also we've shown that SGO is able to outperform essays using the same 7 gene expression datasets as well as two evaluation measures.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are funding sources.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So any questions?",
                    "label": 0
                },
                {
                    "sent": "This animation is on auto sample.",
                    "label": 0
                },
                {
                    "sent": "So we took a subset of the information and evaluation is based off.",
                    "label": 0
                },
                {
                    "sent": "We have we have all the labels, so it's not really out of sample, but we're using a small subset of labels for producing the embedding.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Miami Dade over withheld from so we have all those.",
                    "label": 0
                },
                {
                    "sent": "And the classifier is real sensitive data intended results shows on consecutive data.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "There there.",
                    "label": 0
                },
                {
                    "sent": "So the classification results that we use the two evaluation measures they.",
                    "label": 0
                },
                {
                    "sent": "There's just cross validations of all the examples, so it's.",
                    "label": 0
                },
                {
                    "sent": "So it's not exclusive to each other.",
                    "label": 0
                },
                {
                    "sent": "Is cross validation so yeah, so it looks at all the sample points.",
                    "label": 0
                },
                {
                    "sent": "English.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "And how do you know how many features should be?",
                    "label": 0
                },
                {
                    "sent": "It's a good question.",
                    "label": 0
                },
                {
                    "sent": "We've previously.",
                    "label": 0
                },
                {
                    "sent": "Older features except one.",
                    "label": 0
                },
                {
                    "sent": "We could.",
                    "label": 0
                },
                {
                    "sent": "Could leave.",
                    "label": 0
                },
                {
                    "sent": "Or maybe 3.",
                    "label": 0
                },
                {
                    "sent": "Right, so we previously we've done studies.",
                    "label": 0
                },
                {
                    "sent": "Feature index yeah, so we've done right.",
                    "label": 0
                },
                {
                    "sent": "So we've done different experiments to try it on.",
                    "label": 0
                },
                {
                    "sent": "You know, 234 all the way up to 20 dimensionality for data to see which.",
                    "label": 0
                },
                {
                    "sent": "Responsible for selectivity level.",
                    "label": 0
                },
                {
                    "sent": "From selectivity level.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's informal.",
                    "label": 0
                },
                {
                    "sent": "Well, essentially we just use our invite.",
                    "label": 0
                },
                {
                    "sent": "How many features should be removed?",
                    "label": 0
                },
                {
                    "sent": "Well, the algorithm doesn't know.",
                    "label": 0
                },
                {
                    "sent": "You have to look at, you know, explore the feature space essentially to see which you know.",
                    "label": 0
                },
                {
                    "sent": "Embedding dimensionality to reduce it to.",
                    "label": 0
                },
                {
                    "sent": "There's something we're definitely looking to explore, you know, try to figure out which combinations of parameter settings are going to give you solid results.",
                    "label": 0
                }
            ]
        }
    }
}