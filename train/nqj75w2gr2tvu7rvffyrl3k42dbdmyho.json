{
    "id": "nqj75w2gr2tvu7rvffyrl3k42dbdmyho",
    "title": "An Efficient Projection for L1 Infinity Regularization",
    "info": {
        "author": [
            "Ariadna Quattoni, Universitat Polit\u00e8cnica de Catalunya"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_quattoni_epre/",
    "segmentation": [
        [
            "So I think most of the motivation.",
            "It's already been set up by the previous talk, but just to I will add a little bit more maybe so our goal is to be able to train join this sparse model sufficiently an why we want to do that.",
            "Well, there's different reasons which are pretty much the same reasons why we want sparsity in general, so one is that we might want to be able to learn from fewer label examples.",
            "The other reason is that by having a sparse model.",
            "We might be able to have a more efficient classifier at this time, and finally, the third reason is interpretability.",
            "So I think the previous application made that clear that sometimes you actually care in knowing what the relevant."
        ],
        [
            "Teachers are so this is to describe the idea of joint sparsity.",
            "So imagine a case where you want to be classifiers to classify images of different classes.",
            "So, for example, you might want to be like classifier that attacks whether an image is an image of a church, an imagine that in order to do that, the feature representation that we're going to use is very simple, which is going to represent every image by similarity to some set of unlabeled.",
            "Amy."
        ],
        [
            "So then you can imagine building a linear classifier in this space, and if you will single sparse linear classifier for example, you might choose these two features.",
            "But the point enjoying the sparsity as was described in the previous talk is."
        ],
        [
            "That you might actually want to build a set of classifiers that are together jointly as sparse in the sense that the parameter matrix would only have a few nonzeros, meaning only a few features will be used by any of your classification."
        ],
        [
            "Problems, so how do we do this?",
            "So we use that one Infinity norm.",
            "So this is essentially the same figure as in the previous talk.",
            "So how this known works again.",
            "Just to remind you so you have an Infinity norm on the rows of this matrix.",
            "So you take the maximum absolute value of arrow and then of that resulting vector you have an L1 norm.",
            "So the intuition here is that the Infinity norm on their rose is going to promote sharing of parameters.",
            "So if you think about this, I just think that you want to update one parameter of this matrix.",
            "Well, as long as you do not change the maximum absolute value of that row, you will pay no regularization penalty.",
            "So that's encouraging features to be reused.",
            "On the other hand, the L1 norm on the maximum absolute values has a role analogous to the standard one norm, in that it promotes sparsity at the role level.",
            "So what are the contributions of our work?",
            "So I'm going to."
        ],
        [
            "I sent an efficient projected method for a one Infinity regularization.",
            "I will show you that we can do the projection in analog and time, where N is the size of the matrix.",
            "Then I will present some experiments on multitask image classification problems, where will show that we can discover join this party solutions and in cases where you have a few samples you actually get some improvement in performance over the other two standard regularization.",
            "So this is just a set up."
        ],
        [
            "Some notation of the multitask problem, so we have a collection of training sets, so they're here there M does for each of them, we just have some labeled samples and the Golden multitask.",
            "Learning is that we're going to be minimizing some loss, so this is just the average average loss over all the training sets in our collection, and then will use that one Infinity regularization.",
            "So here, Q as usual, is measuring this tradeoff between fitting on the data.",
            "Anne."
        ],
        [
            "The sparsity of our model, but in fact what are we are going to use?",
            "Is that constraint convex formulation of this problem?",
            "So there's always two ways of setup parallelization problem.",
            "They want to show you before or this one.",
            "So here what we do is that we again have these comeback loss function that we're trying to minimize, but now the regularisation is going to be expressed as convex constraints, so here we're just saying find me the best parameter matrix.",
            "But this matrix has to be inside of final one.",
            "Infinity ball of radius C. In order to minimize this objective, we are going to use is projected to gradient method, so these are very simple methods.",
            "So how they work is that you take a step on the direction of the gradient or the subgradient and you ignore the constraints.",
            "And then if you are outside of your convex set, you project back to it.",
            "So the advantages is that, well, it's pretty simple to implement.",
            "I.",
            "And as long as you can compute the gradient of your function efficiently, which is true for most reasonable losses and the projection."
        ],
        [
            "So the convex set efficiently.",
            "Then you have a scalable algorithm and this is an algorithm in optimization theory, for which there are guaranteed convergence rates.",
            "So we have seen these methods applied to different regularization problems.",
            "So there is a war of shy, the Pegasos algorithm for L2 regularization, and more recently this war has already been mentioned for L1 regularization.",
            "Is the work of Gucci.",
            "So the question is going to be can we do efficient projections to that one Infinity ball?",
            "On"
        ],
        [
            "The answer is yes.",
            "So what I'm going to do now is map the original primal problem of the projection to a simple problem that can be solved efficiently.",
            "So here we have the primal formulation.",
            "So let's assume we have some matrix A and for simplicity we assume a is non negative.",
            "It generalizes to the other case too, but let's say a is non negative.",
            "So in the projection we want to find a matrix speed which is close enough.",
            "Read and Norm to a.",
            "That's just what the objective is saying.",
            "But we wanted to be on the convex set, so in order to do that, we incorporate these variables.",
            "Mu, there's going to be one knew for every row in the matrix, and this esentially will stand for the maximum absolute value of that role.",
            "So that's why the first constraint is saying and the second constraint is just saying that we want the maximal absolute values to sum to see."
        ],
        [
            "So by inspecting the Lagrangian we can find 2 properties.",
            "So the first property is simply telling us that if we know the optimal maximums, that will tell us how to create the matrix me an essentially what we're going to do is use the new maximums to truncate the original matrix.",
            "So what that means is that if I have a value whose sorry if if one parameter has a value above its corresponding UI.",
            "Then it's going to be truncated to that new I, otherwise it will remain the same."
        ],
        [
            "The second property is that we know that the optimal solution there has to be some constant Theta such that two things can happen either if the row is not going to go to zero, that is, system UI is larger than zero, then it must be the case that the amount of mass that I'm removing from that row by doing the truncation is this constant Theta, and this will have to be the same for all rows that do not become zero the second case.",
            "Is when the role that we can zero then it must be that the amount of mass in that role is less than this constant data.",
            "So the second condition is the one that is going to give us a sparse solution at the end.",
            "So this is actually pretty simple property an I'm trying to illustrate what it means in the graphs below.",
            "So imagine you have a matrix where it has four rows.",
            "So each of these graphs is a roll and six columns, so each graph.",
            "Um, so in each of these graphs they all have 6 bars.",
            "That's because there are six columns, so I'm putting the sorted values of that role.",
            "So what the property is telling us is that if I truncate by one particular muai, then the amount of mass that is in this box is has to be the same as set for them.",
            "UI that goes to zero, which in this case is the third row.",
            "In that case, the amount of mass in that box has to be less than in the others.",
            "I'm so put in."
        ],
        [
            "It all together.",
            "Now we can map the original problem to a simple problem which just involves finding you the muse and the constant Theta such that we satisfy the condition of the lemmas.",
            "So the last ingredient is that we need to make sure that this problem has a unique solution, because if this problem has a unique solution then we can just go ahead, solve this problem and get a solution for the regional problem and we show that in the paper."
        ],
        [
            "So here is how the algorithm works.",
            "So.",
            "For any particular value of Theta, we can compute its corresponding Meanwhile, so then once that we have all the new eyes, we can compute the norm of the projected matrix because that's just summing up the new eyes.",
            "That means that for every Theta value we can compute the norm of the projected matrix.",
            "This is in fact a piecewise linear function.",
            "So what the algorithm does is that we start with some value of Theta Theta 0.",
            "In that case, then you guys are set to be the maximum values of the original matrix and then we will keep increasing Theta and therefore decreasing the norm of the resulting matrix until we."
        ],
        [
            "Eat the norm that we're looking for."
        ],
        [
            "So I told you that the norm as a function of Theta is a piecewise linear function and the intervals of this function are related to the sorted entries of our matrix.",
            "So the main operation to do the projection is just to be able to sort this entries so it's in the order of DM log VMware.",
            "This number of Roseanne M is the number of columns.",
            "So now I'm."
        ],
        [
            "I was going to show you some experiments of so.",
            "The first experiment is a synthetic.",
            "It's on synthetic data, So what we do in this case is we actually create a matrix that is rose sparse and how we do that is we randomly select a set of a subset of the roles and assign random values to them.",
            "Then for every column which will correspond to a task, we generate training data.",
            "So we compare three different types of regularization, the L1 Infinity and their one and."
        ],
        [
            "The L2.",
            "And here are the results.",
            "So on the right, sorry on the left I have the test error as a function of the number of training samples per task and we can see that when there are few training samples, the L1 Infinity gives lower test error.",
            "But I think what is more interesting in the right hand side?",
            "I am looking at the feature selection performance of the different models.",
            "So here because I created the data I know actually what Rose.",
            "Are non 0 so I can see how well.",
            "How good are there different models are in predicting.",
            "One and zeros are so the solid line is showing the recall so we can see that actually in recall both L1 and L1 Infinity have similar meaning.",
            "If a row is non zero, both of them would suggest it as a relevant role.",
            "The main differences in the precision because essentially there one Infinity model.",
            "We propose much fewer rows, so as you can see with 160 examples, you get the exact set of relevant rows while their one model takes many more samples.",
            "So this."
        ],
        [
            "Had some experiments on an image classification task, so here the goal is given an image we want to predict whether a particular word is good annotation for that image.",
            "So, for example, we might take an image and want to say that's the word is the word President Edward annotation.",
            "So there were 40 words and we use a standard image representation which had 11,000 dimensions."
        ],
        [
            "I hear the results, so essentially we are again comparing L1 Infinity L1 and L2 and this is actually accuracy versus the number of training samples we can see.",
            "As I said at the beginning that when the number of training samples is small you get an improvement in performance and most of the differences are statistically significant."
        ],
        [
            "So here I'm showing you the ability of the model to retrieve jointly aspires solutions so the figure on top what we do is that we run the model for different values of C to obtain results with different levels of sparsity.",
            "So as you can see, with around 30% of the features, the L1 Infinity model gets pretty much the best that it can do where the L1 model is using near.",
            "Well, it's using 100% of the features to get it.",
            "Perform the expense performance.",
            "So the figures below are showing the actual parameter matrices for an L1 regularize model ananel one Infinity model where Gray corresponds to sorry, where black correspond to non 0 features.",
            "So it what happens is what we would expect the L1 model that give you sparse solutions, but they are not jointly as sparse where there one Infinity model we get there both sparsity."
        ],
        [
            "Operti this is some other set of experiments.",
            "It's also image classification.",
            "Here we classify images into different endorsing types, but here our representation is actually for every image we have similarities to answer to a set of unlabeled images, and this representation had 2000 dimensions."
        ],
        [
            "And what I'm plotting here again is the performance as a function of the percentage of non zeros and what we can see is that with around 30% of the features that one Infinity model.",
            "Give some reasonable performance where the L1 model takes at least three times that number of features to achieve similar performance."
        ],
        [
            "So in conclusion, so I show you an efficient global algorithm for L1 Infinity regularization.",
            "So this kind of completes the work on projected methods.",
            "So we knew how to do it for one and two.",
            "Now we know how to do it for one Infinity and soon I'm going to put the code so that you can try it.",
            "And I have shown you on real image classification problems that you can in fact recover join this sparse solutions.",
            "Thank you.",
            "Yes.",
            "Yeah, so essentially there's two ways two types of group lasso, so you can have an L1L2 and L1 an Infinity, so I think it's just depending on the problem.",
            "You have to see which one works better.",
            "The intuition is that the L1 Infinity is actually stronger assumption of relatedness that L1L2.",
            "So that's the main difference."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I think most of the motivation.",
                    "label": 0
                },
                {
                    "sent": "It's already been set up by the previous talk, but just to I will add a little bit more maybe so our goal is to be able to train join this sparse model sufficiently an why we want to do that.",
                    "label": 0
                },
                {
                    "sent": "Well, there's different reasons which are pretty much the same reasons why we want sparsity in general, so one is that we might want to be able to learn from fewer label examples.",
                    "label": 1
                },
                {
                    "sent": "The other reason is that by having a sparse model.",
                    "label": 1
                },
                {
                    "sent": "We might be able to have a more efficient classifier at this time, and finally, the third reason is interpretability.",
                    "label": 0
                },
                {
                    "sent": "So I think the previous application made that clear that sometimes you actually care in knowing what the relevant.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Teachers are so this is to describe the idea of joint sparsity.",
                    "label": 0
                },
                {
                    "sent": "So imagine a case where you want to be classifiers to classify images of different classes.",
                    "label": 0
                },
                {
                    "sent": "So, for example, you might want to be like classifier that attacks whether an image is an image of a church, an imagine that in order to do that, the feature representation that we're going to use is very simple, which is going to represent every image by similarity to some set of unlabeled.",
                    "label": 0
                },
                {
                    "sent": "Amy.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then you can imagine building a linear classifier in this space, and if you will single sparse linear classifier for example, you might choose these two features.",
                    "label": 0
                },
                {
                    "sent": "But the point enjoying the sparsity as was described in the previous talk is.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That you might actually want to build a set of classifiers that are together jointly as sparse in the sense that the parameter matrix would only have a few nonzeros, meaning only a few features will be used by any of your classification.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problems, so how do we do this?",
                    "label": 1
                },
                {
                    "sent": "So we use that one Infinity norm.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially the same figure as in the previous talk.",
                    "label": 0
                },
                {
                    "sent": "So how this known works again.",
                    "label": 0
                },
                {
                    "sent": "Just to remind you so you have an Infinity norm on the rows of this matrix.",
                    "label": 1
                },
                {
                    "sent": "So you take the maximum absolute value of arrow and then of that resulting vector you have an L1 norm.",
                    "label": 0
                },
                {
                    "sent": "So the intuition here is that the Infinity norm on their rose is going to promote sharing of parameters.",
                    "label": 0
                },
                {
                    "sent": "So if you think about this, I just think that you want to update one parameter of this matrix.",
                    "label": 0
                },
                {
                    "sent": "Well, as long as you do not change the maximum absolute value of that row, you will pay no regularization penalty.",
                    "label": 0
                },
                {
                    "sent": "So that's encouraging features to be reused.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, the L1 norm on the maximum absolute values has a role analogous to the standard one norm, in that it promotes sparsity at the role level.",
                    "label": 1
                },
                {
                    "sent": "So what are the contributions of our work?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I sent an efficient projected method for a one Infinity regularization.",
                    "label": 1
                },
                {
                    "sent": "I will show you that we can do the projection in analog and time, where N is the size of the matrix.",
                    "label": 0
                },
                {
                    "sent": "Then I will present some experiments on multitask image classification problems, where will show that we can discover join this party solutions and in cases where you have a few samples you actually get some improvement in performance over the other two standard regularization.",
                    "label": 1
                },
                {
                    "sent": "So this is just a set up.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some notation of the multitask problem, so we have a collection of training sets, so they're here there M does for each of them, we just have some labeled samples and the Golden multitask.",
                    "label": 0
                },
                {
                    "sent": "Learning is that we're going to be minimizing some loss, so this is just the average average loss over all the training sets in our collection, and then will use that one Infinity regularization.",
                    "label": 0
                },
                {
                    "sent": "So here, Q as usual, is measuring this tradeoff between fitting on the data.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The sparsity of our model, but in fact what are we are going to use?",
                    "label": 0
                },
                {
                    "sent": "Is that constraint convex formulation of this problem?",
                    "label": 0
                },
                {
                    "sent": "So there's always two ways of setup parallelization problem.",
                    "label": 0
                },
                {
                    "sent": "They want to show you before or this one.",
                    "label": 0
                },
                {
                    "sent": "So here what we do is that we again have these comeback loss function that we're trying to minimize, but now the regularisation is going to be expressed as convex constraints, so here we're just saying find me the best parameter matrix.",
                    "label": 0
                },
                {
                    "sent": "But this matrix has to be inside of final one.",
                    "label": 0
                },
                {
                    "sent": "Infinity ball of radius C. In order to minimize this objective, we are going to use is projected to gradient method, so these are very simple methods.",
                    "label": 0
                },
                {
                    "sent": "So how they work is that you take a step on the direction of the gradient or the subgradient and you ignore the constraints.",
                    "label": 0
                },
                {
                    "sent": "And then if you are outside of your convex set, you project back to it.",
                    "label": 0
                },
                {
                    "sent": "So the advantages is that, well, it's pretty simple to implement.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "And as long as you can compute the gradient of your function efficiently, which is true for most reasonable losses and the projection.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the convex set efficiently.",
                    "label": 0
                },
                {
                    "sent": "Then you have a scalable algorithm and this is an algorithm in optimization theory, for which there are guaranteed convergence rates.",
                    "label": 1
                },
                {
                    "sent": "So we have seen these methods applied to different regularization problems.",
                    "label": 1
                },
                {
                    "sent": "So there is a war of shy, the Pegasos algorithm for L2 regularization, and more recently this war has already been mentioned for L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "Is the work of Gucci.",
                    "label": 0
                },
                {
                    "sent": "So the question is going to be can we do efficient projections to that one Infinity ball?",
                    "label": 0
                },
                {
                    "sent": "On",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The answer is yes.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do now is map the original primal problem of the projection to a simple problem that can be solved efficiently.",
                    "label": 0
                },
                {
                    "sent": "So here we have the primal formulation.",
                    "label": 0
                },
                {
                    "sent": "So let's assume we have some matrix A and for simplicity we assume a is non negative.",
                    "label": 0
                },
                {
                    "sent": "It generalizes to the other case too, but let's say a is non negative.",
                    "label": 0
                },
                {
                    "sent": "So in the projection we want to find a matrix speed which is close enough.",
                    "label": 0
                },
                {
                    "sent": "Read and Norm to a.",
                    "label": 0
                },
                {
                    "sent": "That's just what the objective is saying.",
                    "label": 0
                },
                {
                    "sent": "But we wanted to be on the convex set, so in order to do that, we incorporate these variables.",
                    "label": 0
                },
                {
                    "sent": "Mu, there's going to be one knew for every row in the matrix, and this esentially will stand for the maximum absolute value of that role.",
                    "label": 0
                },
                {
                    "sent": "So that's why the first constraint is saying and the second constraint is just saying that we want the maximal absolute values to sum to see.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So by inspecting the Lagrangian we can find 2 properties.",
                    "label": 0
                },
                {
                    "sent": "So the first property is simply telling us that if we know the optimal maximums, that will tell us how to create the matrix me an essentially what we're going to do is use the new maximums to truncate the original matrix.",
                    "label": 0
                },
                {
                    "sent": "So what that means is that if I have a value whose sorry if if one parameter has a value above its corresponding UI.",
                    "label": 0
                },
                {
                    "sent": "Then it's going to be truncated to that new I, otherwise it will remain the same.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second property is that we know that the optimal solution there has to be some constant Theta such that two things can happen either if the row is not going to go to zero, that is, system UI is larger than zero, then it must be the case that the amount of mass that I'm removing from that row by doing the truncation is this constant Theta, and this will have to be the same for all rows that do not become zero the second case.",
                    "label": 0
                },
                {
                    "sent": "Is when the role that we can zero then it must be that the amount of mass in that role is less than this constant data.",
                    "label": 0
                },
                {
                    "sent": "So the second condition is the one that is going to give us a sparse solution at the end.",
                    "label": 0
                },
                {
                    "sent": "So this is actually pretty simple property an I'm trying to illustrate what it means in the graphs below.",
                    "label": 0
                },
                {
                    "sent": "So imagine you have a matrix where it has four rows.",
                    "label": 0
                },
                {
                    "sent": "So each of these graphs is a roll and six columns, so each graph.",
                    "label": 0
                },
                {
                    "sent": "Um, so in each of these graphs they all have 6 bars.",
                    "label": 0
                },
                {
                    "sent": "That's because there are six columns, so I'm putting the sorted values of that role.",
                    "label": 0
                },
                {
                    "sent": "So what the property is telling us is that if I truncate by one particular muai, then the amount of mass that is in this box is has to be the same as set for them.",
                    "label": 0
                },
                {
                    "sent": "UI that goes to zero, which in this case is the third row.",
                    "label": 0
                },
                {
                    "sent": "In that case, the amount of mass in that box has to be less than in the others.",
                    "label": 0
                },
                {
                    "sent": "I'm so put in.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It all together.",
                    "label": 0
                },
                {
                    "sent": "Now we can map the original problem to a simple problem which just involves finding you the muse and the constant Theta such that we satisfy the condition of the lemmas.",
                    "label": 1
                },
                {
                    "sent": "So the last ingredient is that we need to make sure that this problem has a unique solution, because if this problem has a unique solution then we can just go ahead, solve this problem and get a solution for the regional problem and we show that in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is how the algorithm works.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For any particular value of Theta, we can compute its corresponding Meanwhile, so then once that we have all the new eyes, we can compute the norm of the projected matrix because that's just summing up the new eyes.",
                    "label": 0
                },
                {
                    "sent": "That means that for every Theta value we can compute the norm of the projected matrix.",
                    "label": 0
                },
                {
                    "sent": "This is in fact a piecewise linear function.",
                    "label": 0
                },
                {
                    "sent": "So what the algorithm does is that we start with some value of Theta Theta 0.",
                    "label": 0
                },
                {
                    "sent": "In that case, then you guys are set to be the maximum values of the original matrix and then we will keep increasing Theta and therefore decreasing the norm of the resulting matrix until we.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eat the norm that we're looking for.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I told you that the norm as a function of Theta is a piecewise linear function and the intervals of this function are related to the sorted entries of our matrix.",
                    "label": 0
                },
                {
                    "sent": "So the main operation to do the projection is just to be able to sort this entries so it's in the order of DM log VMware.",
                    "label": 0
                },
                {
                    "sent": "This number of Roseanne M is the number of columns.",
                    "label": 0
                },
                {
                    "sent": "So now I'm.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I was going to show you some experiments of so.",
                    "label": 0
                },
                {
                    "sent": "The first experiment is a synthetic.",
                    "label": 0
                },
                {
                    "sent": "It's on synthetic data, So what we do in this case is we actually create a matrix that is rose sparse and how we do that is we randomly select a set of a subset of the roles and assign random values to them.",
                    "label": 0
                },
                {
                    "sent": "Then for every column which will correspond to a task, we generate training data.",
                    "label": 1
                },
                {
                    "sent": "So we compare three different types of regularization, the L1 Infinity and their one and.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The L2.",
                    "label": 0
                },
                {
                    "sent": "And here are the results.",
                    "label": 0
                },
                {
                    "sent": "So on the right, sorry on the left I have the test error as a function of the number of training samples per task and we can see that when there are few training samples, the L1 Infinity gives lower test error.",
                    "label": 0
                },
                {
                    "sent": "But I think what is more interesting in the right hand side?",
                    "label": 0
                },
                {
                    "sent": "I am looking at the feature selection performance of the different models.",
                    "label": 0
                },
                {
                    "sent": "So here because I created the data I know actually what Rose.",
                    "label": 0
                },
                {
                    "sent": "Are non 0 so I can see how well.",
                    "label": 0
                },
                {
                    "sent": "How good are there different models are in predicting.",
                    "label": 0
                },
                {
                    "sent": "One and zeros are so the solid line is showing the recall so we can see that actually in recall both L1 and L1 Infinity have similar meaning.",
                    "label": 0
                },
                {
                    "sent": "If a row is non zero, both of them would suggest it as a relevant role.",
                    "label": 0
                },
                {
                    "sent": "The main differences in the precision because essentially there one Infinity model.",
                    "label": 0
                },
                {
                    "sent": "We propose much fewer rows, so as you can see with 160 examples, you get the exact set of relevant rows while their one model takes many more samples.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Had some experiments on an image classification task, so here the goal is given an image we want to predict whether a particular word is good annotation for that image.",
                    "label": 0
                },
                {
                    "sent": "So, for example, we might take an image and want to say that's the word is the word President Edward annotation.",
                    "label": 0
                },
                {
                    "sent": "So there were 40 words and we use a standard image representation which had 11,000 dimensions.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I hear the results, so essentially we are again comparing L1 Infinity L1 and L2 and this is actually accuracy versus the number of training samples we can see.",
                    "label": 0
                },
                {
                    "sent": "As I said at the beginning that when the number of training samples is small you get an improvement in performance and most of the differences are statistically significant.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here I'm showing you the ability of the model to retrieve jointly aspires solutions so the figure on top what we do is that we run the model for different values of C to obtain results with different levels of sparsity.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, with around 30% of the features, the L1 Infinity model gets pretty much the best that it can do where the L1 model is using near.",
                    "label": 0
                },
                {
                    "sent": "Well, it's using 100% of the features to get it.",
                    "label": 0
                },
                {
                    "sent": "Perform the expense performance.",
                    "label": 0
                },
                {
                    "sent": "So the figures below are showing the actual parameter matrices for an L1 regularize model ananel one Infinity model where Gray corresponds to sorry, where black correspond to non 0 features.",
                    "label": 0
                },
                {
                    "sent": "So it what happens is what we would expect the L1 model that give you sparse solutions, but they are not jointly as sparse where there one Infinity model we get there both sparsity.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Operti this is some other set of experiments.",
                    "label": 0
                },
                {
                    "sent": "It's also image classification.",
                    "label": 0
                },
                {
                    "sent": "Here we classify images into different endorsing types, but here our representation is actually for every image we have similarities to answer to a set of unlabeled images, and this representation had 2000 dimensions.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what I'm plotting here again is the performance as a function of the percentage of non zeros and what we can see is that with around 30% of the features that one Infinity model.",
                    "label": 0
                },
                {
                    "sent": "Give some reasonable performance where the L1 model takes at least three times that number of features to achieve similar performance.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, so I show you an efficient global algorithm for L1 Infinity regularization.",
                    "label": 1
                },
                {
                    "sent": "So this kind of completes the work on projected methods.",
                    "label": 0
                },
                {
                    "sent": "So we knew how to do it for one and two.",
                    "label": 0
                },
                {
                    "sent": "Now we know how to do it for one Infinity and soon I'm going to put the code so that you can try it.",
                    "label": 0
                },
                {
                    "sent": "And I have shown you on real image classification problems that you can in fact recover join this sparse solutions.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so essentially there's two ways two types of group lasso, so you can have an L1L2 and L1 an Infinity, so I think it's just depending on the problem.",
                    "label": 0
                },
                {
                    "sent": "You have to see which one works better.",
                    "label": 0
                },
                {
                    "sent": "The intuition is that the L1 Infinity is actually stronger assumption of relatedness that L1L2.",
                    "label": 0
                },
                {
                    "sent": "So that's the main difference.",
                    "label": 0
                }
            ]
        }
    }
}