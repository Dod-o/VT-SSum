{
    "id": "wij3dt3j27m6gyvisqkklu22ifdhewn5",
    "title": "A metric notion of dimension and its applications to learning",
    "info": {
        "author": [
            "Robert Krauthgamer, Faculty of Mathematics and Computer Science, Weizmann Institute of Science"
        ],
        "published": "July 20, 2010",
        "recorded": "June 2010",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml2010_krauthgamer_amnda/",
    "segmentation": [
        [
            "Thanks for inviting me.",
            "I want to talk about some issues of the dimension and applications know.",
            "OK, so it applications to learning algorithms.",
            "This is based on a couple of joint works with Leah Gottlieb, James Lee and other Kantorovich.",
            "Where the adanali out here?",
            "OK.",
            "So first I love you."
        ],
        [
            "I think mostly about finite metric spaces.",
            "Things work also in the infinite case, but it's kind of easier to think of it this way.",
            "It's like a discrete metric space.",
            "So, so I just basically the distance is trying to get inequality in all the usual stuff, but not at the moment.",
            "I don't want to have any other additional assumption.",
            "So for example, like this example, you know Jerusalem is the excursion tomorrow, high phase where we are now and we have if you want to Tel Aviv then it's a bit longer but almost necessary.",
            "OK, so there are many examples for this metric spaces, either because the metric is completely unstructured.",
            "Sometimes I want to think about the distances becauses Euclidean, maybe data set.",
            "But it's a very high dimensional Euclidean space and therefore somehow the Euclidean structure is just not enough to work with.",
            "Because of this, high dimension is not.",
            "It's a big big problem.",
            "OK, so in both cases, well I want to use or leverage, and of course it's not going to be useful in all the cases, but hopefully if it works for a large percentage of the interesting applications then it's going to be useful is to capture this intrinsic dimension so either the data there's no notion of dimension is not Euclidean.",
            "It's not like a normal space, so it's not clear how to use dimension or how to define it, or because it's Euclidean data set dimension is very high, but how the points lie on a very like?",
            "Manifold or something?",
            "So if you could see it with, you know, inspected by by your eyes you could see that it's a low dimensional space, but you know how to formalize it."
        ],
        [
            "OK, so I want to do this definition to be abstract that depends only on the distance is so even if you give me something which is Euclidean, I'm going to ignore the fact that Euclidean essentially and it should have some power of analogy.",
            "So to generalize the usual definition in RN.",
            "Alright then OK, whatever your favorite letter and basically we take a notion that was.",
            "Use the well known in analysis in math and kind of my claim was I'll try to convince you today is that this notion, if you slightly turn it around in terms of what you care about it and then basically it controls the complexity of various algorithms or computational problems.",
            "And I'll of course focus on applications to learning.",
            "OK, so."
        ],
        [
            "It is the definition.",
            "It's I mean it doesn't really require the whole slide, but it's very simple little use.",
            "Be just to think about the ball.",
            "So what is the ball?",
            "You take a center point in your metric space and the radius and all the points that fall within this distance are.",
            "So here is an example of causing the plane.",
            "This is a ball.",
            "Now the definition is the following the doubling dimension.",
            "That's the way we're going to call it off a metric space XD.",
            "It's just the minimum number K, such as everybody in delayed in in the space can be covered by two to the cables of half the radius.",
            "So notice that I want to cover this ball by balls of half the radius.",
            "So for this example I can do something like this, so I managed to cover it by 7 bowls.",
            "So here I'm going to use the two to the case at most 7 so my case log to the base 2 seven.",
            "OK, I mean the fact that it's not natural number.",
            "It's not a big problem, but notice the key thing is I have to be able to do this.",
            "For every ball in space, so for every bowl of every radius, I should be able to cover it with seven bowls of half the radius.",
            "Just like in this picture.",
            "Notice also that there was this white space over here.",
            "What I mean to cover the ball and really need to cover the set of points in my metric space if I have like a data set in the plane, like in this example of it in the plane, I don't need to cover the data points.",
            "So if I have a datapoint datapoints in high dimensional space, I need to cover the points of my data and not necessarily the whole that you think about in you think about in high dimensional space.",
            "OK, so if I can do this with, you know these two to the cables every time then?",
            "The dimension case, like this dimension of the space.",
            "As I said, it's inspired the.",
            "Redefine it in this paper of with an open Gupta and James Lee, but still inspired over this notion for math by Aswad, and was used algorithmically.",
            "By Ken Clarkson.",
            "Before us just for the short and I just called the metric Dublin instead of just doubling dimension.",
            "If I think the dimension is constant independent of the number of points.",
            "So think about the number of points being either a large number or actually could be infinite depending on the context and the nice feature of it, which is very easy to verify.",
            "Is that it kind of basic, basically captures every Norman our case if you taken or monarcha Euclidean or L1 or any known, then the dimension is going to be the doubling dimension going to beat off K. So up to small constants, which of course theoretically there not important, theoretically speaking, then, then this good analogy to the usual definition of dimension.",
            "It's very robust notion.",
            "If you take a subset of the point that I mentioned currently go down.",
            "If you take a union of two sets of dimensional go up too much.",
            "If any number of sets, but it can go up, but only slowly.",
            "If you change distances a little bit because of noise or something, then.",
            "The dimension will not go up radically radically, so it's a very robust notion.",
            "I mean this this robustness is kind of like different from previous notions, and I won't go into the details.",
            "It's not that important, but most of them, I mean all of these previous notions.",
            "Look kind of similar about building stuff, but they look the difference is this cardinality sign here.",
            "OK, so you count the number of points when you count the number of points.",
            "If all of a sudden I change something like removing a point or all taking in union, things can go wrong."
        ],
        [
            "OK, so here is an example where this would be applicable.",
            "Of course there are many other examples.",
            "This is called the Earthmover distance has been used a lot in computer vision applications.",
            "So what is this example?",
            "In my data points and it's going to be a bit confusing because what does it mean?",
            "A point?",
            "So I data now a data point like an object and image or something is actually going to think of this set inside the unit square in the plane.",
            "OK, so it's A kind of like you take an image and you fix a few interesting points and that's your.",
            "These are your points and then somehow you normalize things so it's inside the unit square in the Euclidean plane.",
            "So I have two sets of features S one is S1.",
            "I still like to images and I want to compare them.",
            "I'm going to assume that the size of West in the size of TR equal here.",
            "And then the Earth mover distance between these sets is the basically measured by taking the best bijection between these sets.",
            "This projection Pi and for every bijection I measure the distance between every point in S comparing measure the distance to the corresponding point in T. So this is \u03c0 OS.",
            "I take the Euclidean distance and I take the average is basically like a perfect matching between the points in S the point in T for each edge in the matching I take the Euclidean distance and I average it out.",
            "This is the standard definition of earthmover distance.",
            "One way to think about it.",
            "Like you put some earth next to these points in S and you have to move them to T and that's what's the best way.",
            "This is like the perfect matching issue.",
            "The best way to move these.",
            "Piles of earth to the other configuration described by T. No, it's not difficult to see this is satisfied that I'm getting quality.",
            "It's a distance.",
            "For example, not very difficult.",
            "Calculation shows you that this earth mover distance.",
            "If you fix the size of the sets to be K, this is a parameter K and then the doubling dimension of this.",
            "All these sets out with like infinitely many actually write the doubling dimension is only order Cal OK. OK, so we can think of it as a low dimensional space even though you know it has nothing to do with Euclidean space.",
            "I mean there is something here, but it is definitely not a Euclidean space.",
            "It's very easy to see, so it's kind of like could be hard to work with it.",
            "What's the argument to outline the argument here?",
            "It's kind of simple.",
            "First you have to do some discretizations.",
            "If you query about balls of radius R and R / 2, like in the definition of doubling dimension we had before every ball of Radius RI want to cover with board of Reduce are over 2, right?",
            "So for this thing, for this purpose, let's fix some over 2 grid in the plane ride in this square.",
            "Miss calling the plane, you fix up some grid with granularity over 2 and now whenever you give me a set S, I'll just approximate it by moving every point to the nearby grid points, right?",
            "It's a very standard notion, and now the idea, so they just think about this approximation by discrete discreet isation of the sets.",
            "And now if I want to look at the ball of radius R. So what does it mean?",
            "I take one set T and I want to find all the I want to say something about all this so that we introduced our from this T at least the finishing.",
            "So, Fixity and everybody who's within distance out.",
            "So the old sets S which are like this.",
            "So for each such such set S I'm going to do this discretization, and that's very small.",
            "The distance because of the first bullet you have to prove this.",
            "And then the question is how many such S do I get because of the discretization?",
            "The number of this approximate S is is small and basically means if I take like small balls around these discrete SS versions of S. Basically, I can I can cover all the possible S is OK, so I mean it's some sort of counting argument based on top of this.",
            "Asian.",
            "OK, so it's a an example to have in mind.",
            "I hope it convinces you that it's The thing is is a generalization of Euclidean spaces or whatever.",
            "Non spaces which is not."
        ],
        [
            "Just normal spaces.",
            "OK, So what are the applications?",
            "Yeah so.",
            "Faster computation.",
            "Um?",
            "I don't think it would help, but haven't thought about it so I don't see how it could help immediately.",
            "OK, so here are some applications.",
            "One is approximate nearest neighbor search.",
            "I'm sure we all know the problem, but I'll do like I'll touch upon each one a little bit just to show you how it works.",
            "I thought it's going to work better than just delving into one of them in more detail.",
            "Second application is dimension reduction that also applications two embeddings which are related, which I won't discuss.",
            "The several known applications due to networking and distributed systems which are not relevant here, so of course will not discuss.",
            "Discuss them today and something about classification.",
            "It's going to be useful and I'll touch up on the last paper, which OK, so we will go."
        ],
        [
            "1 by 1.",
            "OK, so neighbor search.",
            "I have a set of points.",
            "I'm using the same picture and want to find the closest one in this data set to a query.",
            "I want to avoid doing like my main take of this is I want to avoid linear scan on the points.",
            "OK, so I mean depends on the context, but in my context that way I present till like computing the distance a one point X and the other point Y is like older one.",
            "It's very fast OK which of course depends on the context, might be false, but I'm assuming this is very fast while I'm trying to what is very expensive is like going over all the points which is like a huge data set.",
            "OK."
        ],
        [
            "OK, So what we know how to do is basically a very very simple algorithm that can solve not the exact version but one plus epsilon.",
            "Which means instead of finding the closest point to your query, I'm going to find something which is a little bit a little bit farther away by factor of 1 plus epsilon, where epsilon is an arbitrary parameter and hope you can see it from the front is somewhat small.",
            "The query time is roughly like one over epsilon to the dimension.",
            "Of the Space times log diameter.",
            "The preprocessing is essentially linear in ND.",
            "Assuming we don't assume that the dimension is constant, which sold it here for completeness, but you think of whatever you see this dimension of the space you should think of this as a constant 'cause that's the interesting scenario for these algorithms useful scenario.",
            "I mean, you can even do insertion of deletions, which means like to update your data set OK, but I'm not going to talk about it.",
            "So these are so.",
            "So this is basically query is in logarithmic time, right?",
            "That's the main point.",
            "It turns out to outperform previous schemes.",
            "Because of these differences in the definitions and I think one of the interesting aspect is that it's a very, very simple algorithm.",
            "It basically does almost nothing as I'll show in the next slide.",
            "And then I find it very interesting.",
            "It's not 'cause you're going to immediately go and implement this.",
            "To the contrary, a lot of people already implemented this.",
            "Maybe they didn't know the implementing this, so this gives a good explanation why things work.",
            "Basically even a very simple argument works and no matter how you do try to optimize it until you get a very good performance app immediately just because of this argument.",
            "OK, that looks subsequent enhancements like, you know, like optimizing the storage you have anytime something but then plus something.",
            "Which basically means order then and so on."
        ],
        [
            "OK, so So what is the ideal?",
            "Well, basically we're going to think about the case is just the points have low dimension in this abstract notion of dimension.",
            "But I'm going to think intuitively like everything lies in a Euclidean space of low dimension.",
            "Let's say even in the plane.",
            "OK, by analogy, that's going to be a picture, but we want to try to actually work with the general case by applying that motivation.",
            "So basically what we do in the plane will take this gridpoint.",
            "Usually that's a very simple discretization of space.",
            "We're going to do something very similar.",
            "Basically, we're going to take points that kind of like cover represent the space, but we're not going to sample them at random.",
            "I want whatever resolution or scale I'm going to scale.",
            "Resolution distances are.",
            "This is what I care about.",
            "I just take.",
            "A good representative for this scale.",
            "OK, and this is called basically in it, but in the Euclidean picture, of course we use these grids.",
            "OK, so this is the finish of Annette.",
            "Basically it means that every two points in this set of representatives there at least distance, all of which apart from each other away from each other.",
            "But every other point is closed, so they can kind of like cover the entire space of interesting points.",
            "So every other point is close to one of my representatives within distance L, and it's very easy to buy this."
        ],
        [
            "Build these sets and here is the algorithm, so I'm going to build this sets for different scales are you can think of it like different powers of two, so just finer and finer and finer representation of the space and the main point is just like kind of like navigation allows signposts.",
            "So whenever I get to some scale I need I need a pointer from the current scale to the next scale and this pointers.",
            "Well of course in general that would like have a large number of pointers from here to here, but I will need like pointers which are very local.",
            "So these are different copies of the same space.",
            "So if I have the query coming somewhere here, then right?",
            "So it's in represent different granularities.",
            "Then basically when my query will come I'll find the nearest representative in this scale.",
            "Then I want to find the nearest representative in the next scale.",
            "So only need pointers from this from this point to nearby representatives to the grid points that are nearby.",
            "This point 'cause I know I'm not going to go very far away.",
            "That's a very very simple principle, and therefore I only need this point to hold this pointer.",
            "So here and so on in the query time.",
            "And to do this in advance I only need to prepare the potential pointers from here to here, and there aren't too many.",
            "OK, that's the main argument if you just try to count how many points there are from a two to the inet to.",
            "Pointed at nearby and belong to the next net, which is like one scale below 2 to the minus one the number of pointers that I need to maintain here is very small, just two to the dimension which is the same behavior as you have in the Euclidean space.",
            "OK, so this is the number of local links that have this PowerPoint and per scale you do the calculation.",
            "You get the pounds that I showed you before.",
            "If you do a bit more carefully then you get these bounds.",
            "Essentially linear in N times this to the dimension, and this is like the very very simple argument.",
            "I haven't done much like this.",
            "It works basically.",
            "For example, you can try to optimize things here, like when you have this.",
            "All this outgoing pointers how you choose which one.",
            "How do you scan them?",
            "This kind of linearly.",
            "You can try to optimize the way you scan them and stuff like that.",
            "This could be important in different algorithms for better performance, but this would work anyway.",
            "I mean for this analysis."
        ],
        [
            "OK, so that's one application.",
            "I told you that a lot of work have to improve this further, but I wanted to talk about something else.",
            "So here is let's talk about dimension reduction.",
            "So let's recall the Johnson Lindenstrauss the dimension reduction lemma of Johnson and Strauss.",
            "So if you have given endpoints in a Euclidean space, OK, so I'm going to call the points at X. OK, so you have endpoints and it's in, let's say whatever very large dimension and a parameter epsilon.",
            "I can reduce the dimension, which means I can map this points X into a low dimensional space.",
            "RKULK is only depends only logarithmically on North.",
            "And the number of input points and preserves all the distances up to one plus epsilon, right?",
            "So the distance after the mapping is almost equal to the distance before the mapping.",
            "OK, and it can be realized by by very simple transformation in that part of the power of this thing, because you don't need to know much about it, just do something at random, it works.",
            "And so there's a lot of applications to this.",
            "When I asked, you know, can we do better?",
            "OK, so is this bound here.",
            "Optimal, because of course we can reduce the dimension farther.",
            "Then we'll get.",
            "You will gain more."
        ],
        [
            "So in the usual language this is called.",
            "This map is called an embedding.",
            "It is called distortion.",
            "So whenever you see in the next slide, you know distortion is in this.",
            "In this sense.",
            "So it turns out that is you cannot do much better.",
            "There's a matching lower bound of Noga alone.",
            "Again, it's a small font, but he says that if you take a uniform metric, we take endpoints where the distance between any pair is exactly 1.",
            "For example, if points on the regular simplex, that could be your data set.",
            "Then the dimension of this pointed by a simple argument is basically Logan.",
            "OK, so this is the dimension in my sister doubling dimension.",
            "It's not relevant for the time being, but you get is that the key that you need?",
            "That's his proof.",
            "You need at least Omega Tilda up to a log term epsilon minus 2 Logan, so it's basically matches the bandits written here in the jail.",
            "OK, so basically it says you cannot improve by the pound is like there's a tight case.",
            "Now.",
            "The point is that this tight cases for this uniform metric of endpoints, which exactly means that the dimension here is very very large and I care about cases where the dimension the abstract dimension the doubling dimension.",
            "Is constant.",
            "So then in this case that this lower bound is not applicable and hopefully I can do better.",
            "So it's an open question whether you can do an embedding like this.",
            "Johnson lindenstrauss.",
            "This embedding with dimension that depends on epsilon, another dimension of X. OK, independent of the number of points, this still has dementia dependent on the number of points right?",
            "So for example, if you take endpoints on the line, the theorem says, well, you know I can only produce dimension to log in, even though obviously it's a 1 dimensional space.",
            "So here is a more interesting example of this.",
            "So this was pulled by Kahan and by Alagona animator like different theorems, but I just didn't make a distinction here.",
            "We weren't cold, sometimes the Wilsons Helix so OK.",
            "This is a picture of something which is not really a Helix.",
            "OK think of it as being a Helix, which means like it goes in infinitely many dimensions, every time you OK, this is supposed to describe the following thing.",
            "You make one step every time you go away from the origin, but every time you go 90, turn 90 degrees in a new fresh dimension.",
            "Also everything else.",
            "OK, so this is of course not this picture.",
            "And here is the description just like a vector.",
            "This is the set.",
            "Tick vectors which has J number of ones and the rest is zeros where J goes from one to N. OK, so you start with the origin, then one and everything is 0 and then two ones and everything so and so on and the distance between the earth point and the Jeff point is.",
            "Basically I'm in SJ's quilt.",
            "OK, so that's why it's a Helix because this squirrel thing and for this data set it's easy to see when it leaves in an infinite dimensional space or unit 10 dimensions, but in general infinite dimensional space and it is doubling and they show how to map this into something which think like 3 dimensional Euclidean space.",
            "There is a good map into 3 dimensional space.",
            "Actually they show that you can get if you want distortion one plus epsilon like here.",
            "Then you can do it in dimension.",
            "I forget now either one of us in our website on squared.",
            "OK so it can get arbitrarily good embedding with very small distortion into a constant dimensional space.",
            "But this is only for this very specific metric.",
            "OK, very very specific set.",
            "And what I asked you know, can you do more generally whenever this dimension the doubling dimension is small, I want to have a good embedding.",
            "Now this question is open as I said here what we show is some partial result or resolution of this question.",
            "So we use the dimension of the metric squared.",
            "OK, so it's a little bit more than seems necessary, and but we have some partial guarantees, so either distortion is 1 plus epsilon only.",
            "What we call for a single scale and explain it in a minute so.",
            "A single scale of distances that you care about, which is that when the distance between X&Y are about our, let's say between Delta over 100, Delta is a parameter over 100 and this is the scale of this is you care the most.",
            "So for this I can get one plus epsilon distortion and some weaker guarantees for the rest.",
            "A second guarantee we can have a second theorem is a global embedding.",
            "I do something very good for all the pairs, but basically instead of maintaining the distance X -- Y like I had here.",
            "I'm going to maintain sqrt X -- Y so this is within the images is going to be a very very good approximation to sqrt X -- Y. OK so this is sometimes in some math literature.",
            "It's called snowflake embedding.",
            "OK, so that's the reason for this name, because this squared.",
            "And one way to phrase it is that if I know that the square is Euclidean.",
            "Then basically I think you know, I think the square root of the square, if I applied the feeling for the square of the metric, which is Euclidean, then I have all these things.",
            "So basically I get take the square root which begins in giving back my.",
            "Initial my input metric and it holds them OK.",
            "So one way to phrase this second theorem is to say that the conjecture over here is correct under some additional assumption, which is that the square of these distances it's Euclidean metric.",
            "OK, so that's a special case in which sold for example for altametrics.",
            "That's especially a special case of this thing, so we know it's true.",
            "Photometrics, I mean, we know that this conjecture is true."
        ],
        [
            "Altametrics OK so just to say a few words about what is the statement of this tool and say for one scale.",
            "OK, So what does it mean to maintain one scale then if you give me any finite subset of Euclidean space?",
            "And I am in a parameter.",
            "Then I can find a map into a low dimensional space.",
            "This is L2 to the K, so it's a K dimensional space or dimension of K is thing of like just the dimension of X squared, so this is the you know I match the doubling dimension, the abstract dimension necessary for this space for this point set actually matches by this my embedding and I have the following conditions.",
            "The Main 1 first, the first one is that the map is Lipschitz only contracts distances.",
            "But then at the scale that I care about whether this is about R. Well, they get that.",
            "I get the opposite direction.",
            "The distance between the images is Omega of the of X X -- Y. OK, so it has a good.",
            "It's very faithful at this scale and everyone else is just lipids.",
            "Here the distortion is constant because of Omega, right?",
            "But turns out with a little bit more work we can actually make it 1 plus epsilon here, OK, but I for simplification I left it out for because then things depends on epsilon.",
            "So if you compare this to the open question that I said before, it it only gives me the good guarantees at one scale, while I wanted to have things good, good guarantees for at every scale.",
            "So that's you know this is weaker, but it's also stronger than the conjecture because I get an absolute constant while then the conjecture what's open is, can you get a constant that depends also on the doubling damage?",
            "OK, let me not go into these details.",
            "Basically the approach to prove this is by divide and conquer approach so you.",
            "The Ticul points it you partition into like small sets or clusters or something and then you have to do something in each one and main idea is that each one is very small.",
            "You can basically apply the JL lemma, the Johnson Lindenstrauss lemma OK have a few points there, so you can apply this.",
            "So locali what you get is like a linear mapping.",
            "OK, because the JL lemma theorem basically is approved by a linear transformation, so every locality.",
            "Roughly speaking, is a linear map.",
            "Of course, then you have a problem it to glue the glowing in between, so you have to work a little bit harder so you average a few things.",
            "You smooth things out.",
            "That's the general idea.",
            "Now it's not like it's very, very close to it, but I find these similarities to things that this approach similar to things that are being done practically by using linear Maps separately on each locality."
        ],
        [
            "OK, so I'll skip the second theorem, which is the snowflake I told you about it before I'll skip that exactly."
        ],
        [
            "Statement.",
            "And the third application I want to tell you about these distance based classification.",
            "So OK starting afresh well you have to remember the doubling dimension, but now the new context menu application.",
            "So basically I have pointed like supervised learning.",
            "I have points with labels and I want to be able to label.",
            "I want to be able to label new points.",
            "So the usual thing, but the difference is that the spacer, everything lies is not Euclidean space, it's a metric space.",
            "Arbitrary metric space.",
            "Think about this CMD that we had example we had in the beginning.",
            "OK, so for Euclidean space we have a lot of algorithms.",
            "Now go to MD's.",
            "Oh my God, nobody has proved this.",
            "I have to prove it from scratch right?",
            "So the idea is that you don't have to do it from scratch.",
            "There's a general theory and there's actually a very nice framework of how to do this large margin classification.",
            "So you get OK, will talk about but five unlocks Bergen Busquet.",
            "I hope I pronounced it right.",
            "I apologize if not.",
            "OK, so here is roughly what they are able to show for summarized.",
            "So basically what are the natural hypothesis?",
            "The classifiers in this space you cannot use like a hyperplane 'cause there's no, it's not a vector space in the sense of this.",
            "So it turns out that the natural hyper hypothesis and from now on I'm going to call it the classifiers.",
            "Lipschitz functions, and you want this function to be as smooth as possible?",
            "You want the small lips constant for these functions.",
            "Let's say into the rails like you want to.",
            "Eventually you think of this function into actually plus 1 -- 1 or something, so it's two labels.",
            "Doing a classification what it mean basically what they show is how to reduce this problem into basically finding an F function F classifier hypothesis that is consistent with the labels you observe.",
            "OK, and.",
            "Turns out that.",
            "Like fixing such an F, it reduces to basically a classic problem in math, which is known as Lipschitz extension.",
            "You already have a function which is Lipschitz on the training set, but you have to be value be able to evaluate the function on points that are outside your training set, right?",
            "So this is exactly a well known problem essentially, but I mean you have to do this reduction.",
            "And then then they establish some generalization bounds.",
            "Now, in this generality, it's very difficult to say what are the bounds.",
            "I didn't write them here, but they depend on certain specific properties of the space, like Rademacher averages or something.",
            "And then they say OK.",
            "So now you have this classifier.",
            "You fixed a function.",
            "Now you have to compute this classification.",
            "You wanted to do it somehow organized given algorithm.",
            "How to compute F of X when you're given X.",
            "So basically said well they reduce it to the problem of 1 near neighbor search.",
            "So if you're given a point you want to give the label well, first you have to find the closest point.",
            "Roughly speaking, as you find the closest point of that point is labeled positively you answer positively.",
            "It's labeled negatively, negatively.",
            "OK so so basically doing exactly enable search and what they show is assumes zero training error.",
            "OK, so because you're giving the points, you basically try to match elliptic function to these points now and a very nice thing is that I find is that this one and this of course is a known holistic, so it kind of gives you explanation in very general terms.",
            "The 1S has good bounds, good good performance.",
            "What are you kind of like?",
            "Leave open I think is a few issues of computational efficiencies.",
            "So right now the number of samples that you need.",
            "But how to do it and is using an efficient algorithm in terms of runtime.",
            "So for example, they use one and S. And what happens if you cannot solve exactly?",
            "Then you enable such problem exactly.",
            "It's not clear that the proof would extend, and another issue is what happens with training or all this analysis assumes zero, 20, or part of the analysis assumes zero 20 euro."
        ],
        [
            "OK, So what we show in this work with Gottlieb and Control, which is basically that if you have the data line in low doubling dimension, OK, so that's one more assumption.",
            "Then you can have a classification which is both accurate and computationally efficient.",
            "OK, so it's kind of like you can.",
            "In other words, you can apply the framework, but also make sure that it is computationally efficient.",
            "OK, in terms of runtime.",
            "And I think this is the first relationship between this efficiency issues and that the abstract the metric dimension of your data.",
            "OK, there's a different work that relates something, but it's not the same thing, so I'll just point out I am aware of this work, but it's different.",
            "So here is like, you know, the two points, and that's basically like the end of what I want to tell you about this.",
            "I mean, the paper will be presented called.",
            "So if you really want to hear about it, you can come on Sunday morning.",
            "So basically, how do you choose the classifier quickly so they've seen?",
            "Show me how to do it without training goals and now I have.",
            "I have training goals.",
            "OK so if I have things I have to find those.",
            "Of course you can do it by exhaustive search tie.",
            "Say you know the training with 10%, you try to rule out every possible 10% of the data.",
            "Then you can match and you can use their algorithm.",
            "That's of course not very efficient.",
            "Exponential time so in another problem is how to optimize the bias variance tradeoff.",
            "Because if you change this 10% to different number then you know the tradeoff between these things.",
            "Changes between the bias in the violence.",
            "The question can you know in advance what's the right value?",
            "OK, because I mean they give some formula, but it's not clear you can optimize it.",
            "You know in runtime what's the right value to take if you don't know what the noise level.",
            "And the key step that we do analysis of course server summary of these things is if you show that you can do the following if you fix the target Lipschitz constant.",
            "Then I can find which are the best outliers to remove the smallest possible number and which are the points.",
            "Once you remove them.",
            "Basically the point reduces to zero training you and then you can do some sort of like binary search or something to find out.",
            "Figure out which one is the best and estimated all these quantities to optimize the bias variance tradeoff.",
            "And another issue is that I said we did exactly Ness and the analysis does not extend to approximately enable search OK and we show how you can do this.",
            "There's some problem with doing approximately.",
            "Google search, you only know that you get approximate new neighbors.",
            "So in principle if I give you the same query twice, you might give me different answers.",
            "So same code we think of like like you know, infinitesimally close queries or something.",
            "So just effectively the same query twice and I ask you again again you give me different answers about the nearest nearest neighbor because there might be like a tie.",
            "And then your function.",
            "My function defined might not be really function, so it turns out that it's not a real problem, but you have to work workout this analysis to see that really you can get around this."
        ],
        [
            "OK, so to summarize, I think the bigger picture is that we have good algorithms for low dimensional spaces, so this notion of dimension.",
            "I think in principle can be equated with.",
            "Computational efficiency, or, you know, complexity of the data in terms of computational efficiency, and I hope that this kind of approach of like saying, well, you know, I if I can.",
            "Have an assumption on my data which you know it's relatively reasonable.",
            "Relatively possible to verify this on the data.",
            "You can actually get good algorithms and you know you don't solve the problem.",
            "In the worst case completely, but only like your 80% of the cases in real life or something.",
            "And then keep going this way.",
            "I listed a few.",
            "Instances here and then.",
            "One thing that I like about this thing is also that somehow they give a good explanation of why you risztics Walton when I'm in good explanation is that there's a rigorous analysis.",
            "OK, that's the way I look at.",
            "You know now I understand why it works so well.",
            "So thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks for inviting me.",
                    "label": 0
                },
                {
                    "sent": "I want to talk about some issues of the dimension and applications know.",
                    "label": 0
                },
                {
                    "sent": "OK, so it applications to learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "This is based on a couple of joint works with Leah Gottlieb, James Lee and other Kantorovich.",
                    "label": 1
                },
                {
                    "sent": "Where the adanali out here?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So first I love you.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think mostly about finite metric spaces.",
                    "label": 1
                },
                {
                    "sent": "Things work also in the infinite case, but it's kind of easier to think of it this way.",
                    "label": 1
                },
                {
                    "sent": "It's like a discrete metric space.",
                    "label": 0
                },
                {
                    "sent": "So, so I just basically the distance is trying to get inequality in all the usual stuff, but not at the moment.",
                    "label": 0
                },
                {
                    "sent": "I don't want to have any other additional assumption.",
                    "label": 0
                },
                {
                    "sent": "So for example, like this example, you know Jerusalem is the excursion tomorrow, high phase where we are now and we have if you want to Tel Aviv then it's a bit longer but almost necessary.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are many examples for this metric spaces, either because the metric is completely unstructured.",
                    "label": 1
                },
                {
                    "sent": "Sometimes I want to think about the distances becauses Euclidean, maybe data set.",
                    "label": 0
                },
                {
                    "sent": "But it's a very high dimensional Euclidean space and therefore somehow the Euclidean structure is just not enough to work with.",
                    "label": 0
                },
                {
                    "sent": "Because of this, high dimension is not.",
                    "label": 0
                },
                {
                    "sent": "It's a big big problem.",
                    "label": 1
                },
                {
                    "sent": "OK, so in both cases, well I want to use or leverage, and of course it's not going to be useful in all the cases, but hopefully if it works for a large percentage of the interesting applications then it's going to be useful is to capture this intrinsic dimension so either the data there's no notion of dimension is not Euclidean.",
                    "label": 0
                },
                {
                    "sent": "It's not like a normal space, so it's not clear how to use dimension or how to define it, or because it's Euclidean data set dimension is very high, but how the points lie on a very like?",
                    "label": 0
                },
                {
                    "sent": "Manifold or something?",
                    "label": 0
                },
                {
                    "sent": "So if you could see it with, you know, inspected by by your eyes you could see that it's a low dimensional space, but you know how to formalize it.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I want to do this definition to be abstract that depends only on the distance is so even if you give me something which is Euclidean, I'm going to ignore the fact that Euclidean essentially and it should have some power of analogy.",
                    "label": 0
                },
                {
                    "sent": "So to generalize the usual definition in RN.",
                    "label": 0
                },
                {
                    "sent": "Alright then OK, whatever your favorite letter and basically we take a notion that was.",
                    "label": 0
                },
                {
                    "sent": "Use the well known in analysis in math and kind of my claim was I'll try to convince you today is that this notion, if you slightly turn it around in terms of what you care about it and then basically it controls the complexity of various algorithms or computational problems.",
                    "label": 1
                },
                {
                    "sent": "And I'll of course focus on applications to learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is the definition.",
                    "label": 0
                },
                {
                    "sent": "It's I mean it doesn't really require the whole slide, but it's very simple little use.",
                    "label": 0
                },
                {
                    "sent": "Be just to think about the ball.",
                    "label": 0
                },
                {
                    "sent": "So what is the ball?",
                    "label": 0
                },
                {
                    "sent": "You take a center point in your metric space and the radius and all the points that fall within this distance are.",
                    "label": 0
                },
                {
                    "sent": "So here is an example of causing the plane.",
                    "label": 0
                },
                {
                    "sent": "This is a ball.",
                    "label": 0
                },
                {
                    "sent": "Now the definition is the following the doubling dimension.",
                    "label": 1
                },
                {
                    "sent": "That's the way we're going to call it off a metric space XD.",
                    "label": 0
                },
                {
                    "sent": "It's just the minimum number K, such as everybody in delayed in in the space can be covered by two to the cables of half the radius.",
                    "label": 1
                },
                {
                    "sent": "So notice that I want to cover this ball by balls of half the radius.",
                    "label": 0
                },
                {
                    "sent": "So for this example I can do something like this, so I managed to cover it by 7 bowls.",
                    "label": 0
                },
                {
                    "sent": "So here I'm going to use the two to the case at most 7 so my case log to the base 2 seven.",
                    "label": 0
                },
                {
                    "sent": "OK, I mean the fact that it's not natural number.",
                    "label": 0
                },
                {
                    "sent": "It's not a big problem, but notice the key thing is I have to be able to do this.",
                    "label": 0
                },
                {
                    "sent": "For every ball in space, so for every bowl of every radius, I should be able to cover it with seven bowls of half the radius.",
                    "label": 0
                },
                {
                    "sent": "Just like in this picture.",
                    "label": 0
                },
                {
                    "sent": "Notice also that there was this white space over here.",
                    "label": 0
                },
                {
                    "sent": "What I mean to cover the ball and really need to cover the set of points in my metric space if I have like a data set in the plane, like in this example of it in the plane, I don't need to cover the data points.",
                    "label": 0
                },
                {
                    "sent": "So if I have a datapoint datapoints in high dimensional space, I need to cover the points of my data and not necessarily the whole that you think about in you think about in high dimensional space.",
                    "label": 1
                },
                {
                    "sent": "OK, so if I can do this with, you know these two to the cables every time then?",
                    "label": 0
                },
                {
                    "sent": "The dimension case, like this dimension of the space.",
                    "label": 0
                },
                {
                    "sent": "As I said, it's inspired the.",
                    "label": 0
                },
                {
                    "sent": "Redefine it in this paper of with an open Gupta and James Lee, but still inspired over this notion for math by Aswad, and was used algorithmically.",
                    "label": 0
                },
                {
                    "sent": "By Ken Clarkson.",
                    "label": 0
                },
                {
                    "sent": "Before us just for the short and I just called the metric Dublin instead of just doubling dimension.",
                    "label": 0
                },
                {
                    "sent": "If I think the dimension is constant independent of the number of points.",
                    "label": 0
                },
                {
                    "sent": "So think about the number of points being either a large number or actually could be infinite depending on the context and the nice feature of it, which is very easy to verify.",
                    "label": 0
                },
                {
                    "sent": "Is that it kind of basic, basically captures every Norman our case if you taken or monarcha Euclidean or L1 or any known, then the dimension is going to be the doubling dimension going to beat off K. So up to small constants, which of course theoretically there not important, theoretically speaking, then, then this good analogy to the usual definition of dimension.",
                    "label": 0
                },
                {
                    "sent": "It's very robust notion.",
                    "label": 0
                },
                {
                    "sent": "If you take a subset of the point that I mentioned currently go down.",
                    "label": 0
                },
                {
                    "sent": "If you take a union of two sets of dimensional go up too much.",
                    "label": 0
                },
                {
                    "sent": "If any number of sets, but it can go up, but only slowly.",
                    "label": 0
                },
                {
                    "sent": "If you change distances a little bit because of noise or something, then.",
                    "label": 0
                },
                {
                    "sent": "The dimension will not go up radically radically, so it's a very robust notion.",
                    "label": 0
                },
                {
                    "sent": "I mean this this robustness is kind of like different from previous notions, and I won't go into the details.",
                    "label": 0
                },
                {
                    "sent": "It's not that important, but most of them, I mean all of these previous notions.",
                    "label": 0
                },
                {
                    "sent": "Look kind of similar about building stuff, but they look the difference is this cardinality sign here.",
                    "label": 0
                },
                {
                    "sent": "OK, so you count the number of points when you count the number of points.",
                    "label": 0
                },
                {
                    "sent": "If all of a sudden I change something like removing a point or all taking in union, things can go wrong.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here is an example where this would be applicable.",
                    "label": 0
                },
                {
                    "sent": "Of course there are many other examples.",
                    "label": 0
                },
                {
                    "sent": "This is called the Earthmover distance has been used a lot in computer vision applications.",
                    "label": 1
                },
                {
                    "sent": "So what is this example?",
                    "label": 0
                },
                {
                    "sent": "In my data points and it's going to be a bit confusing because what does it mean?",
                    "label": 0
                },
                {
                    "sent": "A point?",
                    "label": 0
                },
                {
                    "sent": "So I data now a data point like an object and image or something is actually going to think of this set inside the unit square in the plane.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's A kind of like you take an image and you fix a few interesting points and that's your.",
                    "label": 0
                },
                {
                    "sent": "These are your points and then somehow you normalize things so it's inside the unit square in the Euclidean plane.",
                    "label": 0
                },
                {
                    "sent": "So I have two sets of features S one is S1.",
                    "label": 0
                },
                {
                    "sent": "I still like to images and I want to compare them.",
                    "label": 0
                },
                {
                    "sent": "I'm going to assume that the size of West in the size of TR equal here.",
                    "label": 0
                },
                {
                    "sent": "And then the Earth mover distance between these sets is the basically measured by taking the best bijection between these sets.",
                    "label": 0
                },
                {
                    "sent": "This projection Pi and for every bijection I measure the distance between every point in S comparing measure the distance to the corresponding point in T. So this is \u03c0 OS.",
                    "label": 0
                },
                {
                    "sent": "I take the Euclidean distance and I take the average is basically like a perfect matching between the points in S the point in T for each edge in the matching I take the Euclidean distance and I average it out.",
                    "label": 0
                },
                {
                    "sent": "This is the standard definition of earthmover distance.",
                    "label": 0
                },
                {
                    "sent": "One way to think about it.",
                    "label": 0
                },
                {
                    "sent": "Like you put some earth next to these points in S and you have to move them to T and that's what's the best way.",
                    "label": 0
                },
                {
                    "sent": "This is like the perfect matching issue.",
                    "label": 0
                },
                {
                    "sent": "The best way to move these.",
                    "label": 0
                },
                {
                    "sent": "Piles of earth to the other configuration described by T. No, it's not difficult to see this is satisfied that I'm getting quality.",
                    "label": 0
                },
                {
                    "sent": "It's a distance.",
                    "label": 0
                },
                {
                    "sent": "For example, not very difficult.",
                    "label": 0
                },
                {
                    "sent": "Calculation shows you that this earth mover distance.",
                    "label": 0
                },
                {
                    "sent": "If you fix the size of the sets to be K, this is a parameter K and then the doubling dimension of this.",
                    "label": 0
                },
                {
                    "sent": "All these sets out with like infinitely many actually write the doubling dimension is only order Cal OK. OK, so we can think of it as a low dimensional space even though you know it has nothing to do with Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "I mean there is something here, but it is definitely not a Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to see, so it's kind of like could be hard to work with it.",
                    "label": 0
                },
                {
                    "sent": "What's the argument to outline the argument here?",
                    "label": 0
                },
                {
                    "sent": "It's kind of simple.",
                    "label": 0
                },
                {
                    "sent": "First you have to do some discretizations.",
                    "label": 0
                },
                {
                    "sent": "If you query about balls of radius R and R / 2, like in the definition of doubling dimension we had before every ball of Radius RI want to cover with board of Reduce are over 2, right?",
                    "label": 0
                },
                {
                    "sent": "So for this thing, for this purpose, let's fix some over 2 grid in the plane ride in this square.",
                    "label": 0
                },
                {
                    "sent": "Miss calling the plane, you fix up some grid with granularity over 2 and now whenever you give me a set S, I'll just approximate it by moving every point to the nearby grid points, right?",
                    "label": 0
                },
                {
                    "sent": "It's a very standard notion, and now the idea, so they just think about this approximation by discrete discreet isation of the sets.",
                    "label": 0
                },
                {
                    "sent": "And now if I want to look at the ball of radius R. So what does it mean?",
                    "label": 0
                },
                {
                    "sent": "I take one set T and I want to find all the I want to say something about all this so that we introduced our from this T at least the finishing.",
                    "label": 0
                },
                {
                    "sent": "So, Fixity and everybody who's within distance out.",
                    "label": 0
                },
                {
                    "sent": "So the old sets S which are like this.",
                    "label": 0
                },
                {
                    "sent": "So for each such such set S I'm going to do this discretization, and that's very small.",
                    "label": 0
                },
                {
                    "sent": "The distance because of the first bullet you have to prove this.",
                    "label": 0
                },
                {
                    "sent": "And then the question is how many such S do I get because of the discretization?",
                    "label": 0
                },
                {
                    "sent": "The number of this approximate S is is small and basically means if I take like small balls around these discrete SS versions of S. Basically, I can I can cover all the possible S is OK, so I mean it's some sort of counting argument based on top of this.",
                    "label": 0
                },
                {
                    "sent": "Asian.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a an example to have in mind.",
                    "label": 0
                },
                {
                    "sent": "I hope it convinces you that it's The thing is is a generalization of Euclidean spaces or whatever.",
                    "label": 0
                },
                {
                    "sent": "Non spaces which is not.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just normal spaces.",
                    "label": 0
                },
                {
                    "sent": "OK, So what are the applications?",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "Faster computation.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I don't think it would help, but haven't thought about it so I don't see how it could help immediately.",
                    "label": 0
                },
                {
                    "sent": "OK, so here are some applications.",
                    "label": 0
                },
                {
                    "sent": "One is approximate nearest neighbor search.",
                    "label": 1
                },
                {
                    "sent": "I'm sure we all know the problem, but I'll do like I'll touch upon each one a little bit just to show you how it works.",
                    "label": 0
                },
                {
                    "sent": "I thought it's going to work better than just delving into one of them in more detail.",
                    "label": 0
                },
                {
                    "sent": "Second application is dimension reduction that also applications two embeddings which are related, which I won't discuss.",
                    "label": 1
                },
                {
                    "sent": "The several known applications due to networking and distributed systems which are not relevant here, so of course will not discuss.",
                    "label": 0
                },
                {
                    "sent": "Discuss them today and something about classification.",
                    "label": 0
                },
                {
                    "sent": "It's going to be useful and I'll touch up on the last paper, which OK, so we will go.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "1 by 1.",
                    "label": 0
                },
                {
                    "sent": "OK, so neighbor search.",
                    "label": 0
                },
                {
                    "sent": "I have a set of points.",
                    "label": 0
                },
                {
                    "sent": "I'm using the same picture and want to find the closest one in this data set to a query.",
                    "label": 1
                },
                {
                    "sent": "I want to avoid doing like my main take of this is I want to avoid linear scan on the points.",
                    "label": 0
                },
                {
                    "sent": "OK, so I mean depends on the context, but in my context that way I present till like computing the distance a one point X and the other point Y is like older one.",
                    "label": 0
                },
                {
                    "sent": "It's very fast OK which of course depends on the context, might be false, but I'm assuming this is very fast while I'm trying to what is very expensive is like going over all the points which is like a huge data set.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what we know how to do is basically a very very simple algorithm that can solve not the exact version but one plus epsilon.",
                    "label": 0
                },
                {
                    "sent": "Which means instead of finding the closest point to your query, I'm going to find something which is a little bit a little bit farther away by factor of 1 plus epsilon, where epsilon is an arbitrary parameter and hope you can see it from the front is somewhat small.",
                    "label": 0
                },
                {
                    "sent": "The query time is roughly like one over epsilon to the dimension.",
                    "label": 1
                },
                {
                    "sent": "Of the Space times log diameter.",
                    "label": 0
                },
                {
                    "sent": "The preprocessing is essentially linear in ND.",
                    "label": 0
                },
                {
                    "sent": "Assuming we don't assume that the dimension is constant, which sold it here for completeness, but you think of whatever you see this dimension of the space you should think of this as a constant 'cause that's the interesting scenario for these algorithms useful scenario.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can even do insertion of deletions, which means like to update your data set OK, but I'm not going to talk about it.",
                    "label": 0
                },
                {
                    "sent": "So these are so.",
                    "label": 0
                },
                {
                    "sent": "So this is basically query is in logarithmic time, right?",
                    "label": 0
                },
                {
                    "sent": "That's the main point.",
                    "label": 0
                },
                {
                    "sent": "It turns out to outperform previous schemes.",
                    "label": 1
                },
                {
                    "sent": "Because of these differences in the definitions and I think one of the interesting aspect is that it's a very, very simple algorithm.",
                    "label": 1
                },
                {
                    "sent": "It basically does almost nothing as I'll show in the next slide.",
                    "label": 0
                },
                {
                    "sent": "And then I find it very interesting.",
                    "label": 0
                },
                {
                    "sent": "It's not 'cause you're going to immediately go and implement this.",
                    "label": 0
                },
                {
                    "sent": "To the contrary, a lot of people already implemented this.",
                    "label": 0
                },
                {
                    "sent": "Maybe they didn't know the implementing this, so this gives a good explanation why things work.",
                    "label": 0
                },
                {
                    "sent": "Basically even a very simple argument works and no matter how you do try to optimize it until you get a very good performance app immediately just because of this argument.",
                    "label": 0
                },
                {
                    "sent": "OK, that looks subsequent enhancements like, you know, like optimizing the storage you have anytime something but then plus something.",
                    "label": 0
                },
                {
                    "sent": "Which basically means order then and so on.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so So what is the ideal?",
                    "label": 0
                },
                {
                    "sent": "Well, basically we're going to think about the case is just the points have low dimension in this abstract notion of dimension.",
                    "label": 1
                },
                {
                    "sent": "But I'm going to think intuitively like everything lies in a Euclidean space of low dimension.",
                    "label": 0
                },
                {
                    "sent": "Let's say even in the plane.",
                    "label": 0
                },
                {
                    "sent": "OK, by analogy, that's going to be a picture, but we want to try to actually work with the general case by applying that motivation.",
                    "label": 0
                },
                {
                    "sent": "So basically what we do in the plane will take this gridpoint.",
                    "label": 0
                },
                {
                    "sent": "Usually that's a very simple discretization of space.",
                    "label": 0
                },
                {
                    "sent": "We're going to do something very similar.",
                    "label": 0
                },
                {
                    "sent": "Basically, we're going to take points that kind of like cover represent the space, but we're not going to sample them at random.",
                    "label": 0
                },
                {
                    "sent": "I want whatever resolution or scale I'm going to scale.",
                    "label": 0
                },
                {
                    "sent": "Resolution distances are.",
                    "label": 0
                },
                {
                    "sent": "This is what I care about.",
                    "label": 0
                },
                {
                    "sent": "I just take.",
                    "label": 0
                },
                {
                    "sent": "A good representative for this scale.",
                    "label": 1
                },
                {
                    "sent": "OK, and this is called basically in it, but in the Euclidean picture, of course we use these grids.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the finish of Annette.",
                    "label": 0
                },
                {
                    "sent": "Basically it means that every two points in this set of representatives there at least distance, all of which apart from each other away from each other.",
                    "label": 1
                },
                {
                    "sent": "But every other point is closed, so they can kind of like cover the entire space of interesting points.",
                    "label": 0
                },
                {
                    "sent": "So every other point is close to one of my representatives within distance L, and it's very easy to buy this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Build these sets and here is the algorithm, so I'm going to build this sets for different scales are you can think of it like different powers of two, so just finer and finer and finer representation of the space and the main point is just like kind of like navigation allows signposts.",
                    "label": 0
                },
                {
                    "sent": "So whenever I get to some scale I need I need a pointer from the current scale to the next scale and this pointers.",
                    "label": 0
                },
                {
                    "sent": "Well of course in general that would like have a large number of pointers from here to here, but I will need like pointers which are very local.",
                    "label": 0
                },
                {
                    "sent": "So these are different copies of the same space.",
                    "label": 0
                },
                {
                    "sent": "So if I have the query coming somewhere here, then right?",
                    "label": 0
                },
                {
                    "sent": "So it's in represent different granularities.",
                    "label": 0
                },
                {
                    "sent": "Then basically when my query will come I'll find the nearest representative in this scale.",
                    "label": 0
                },
                {
                    "sent": "Then I want to find the nearest representative in the next scale.",
                    "label": 0
                },
                {
                    "sent": "So only need pointers from this from this point to nearby representatives to the grid points that are nearby.",
                    "label": 1
                },
                {
                    "sent": "This point 'cause I know I'm not going to go very far away.",
                    "label": 0
                },
                {
                    "sent": "That's a very very simple principle, and therefore I only need this point to hold this pointer.",
                    "label": 0
                },
                {
                    "sent": "So here and so on in the query time.",
                    "label": 0
                },
                {
                    "sent": "And to do this in advance I only need to prepare the potential pointers from here to here, and there aren't too many.",
                    "label": 1
                },
                {
                    "sent": "OK, that's the main argument if you just try to count how many points there are from a two to the inet to.",
                    "label": 0
                },
                {
                    "sent": "Pointed at nearby and belong to the next net, which is like one scale below 2 to the minus one the number of pointers that I need to maintain here is very small, just two to the dimension which is the same behavior as you have in the Euclidean space.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is the number of local links that have this PowerPoint and per scale you do the calculation.",
                    "label": 0
                },
                {
                    "sent": "You get the pounds that I showed you before.",
                    "label": 0
                },
                {
                    "sent": "If you do a bit more carefully then you get these bounds.",
                    "label": 1
                },
                {
                    "sent": "Essentially linear in N times this to the dimension, and this is like the very very simple argument.",
                    "label": 0
                },
                {
                    "sent": "I haven't done much like this.",
                    "label": 0
                },
                {
                    "sent": "It works basically.",
                    "label": 0
                },
                {
                    "sent": "For example, you can try to optimize things here, like when you have this.",
                    "label": 0
                },
                {
                    "sent": "All this outgoing pointers how you choose which one.",
                    "label": 0
                },
                {
                    "sent": "How do you scan them?",
                    "label": 0
                },
                {
                    "sent": "This kind of linearly.",
                    "label": 0
                },
                {
                    "sent": "You can try to optimize the way you scan them and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "This could be important in different algorithms for better performance, but this would work anyway.",
                    "label": 0
                },
                {
                    "sent": "I mean for this analysis.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's one application.",
                    "label": 0
                },
                {
                    "sent": "I told you that a lot of work have to improve this further, but I wanted to talk about something else.",
                    "label": 0
                },
                {
                    "sent": "So here is let's talk about dimension reduction.",
                    "label": 0
                },
                {
                    "sent": "So let's recall the Johnson Lindenstrauss the dimension reduction lemma of Johnson and Strauss.",
                    "label": 0
                },
                {
                    "sent": "So if you have given endpoints in a Euclidean space, OK, so I'm going to call the points at X. OK, so you have endpoints and it's in, let's say whatever very large dimension and a parameter epsilon.",
                    "label": 0
                },
                {
                    "sent": "I can reduce the dimension, which means I can map this points X into a low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "RKULK is only depends only logarithmically on North.",
                    "label": 0
                },
                {
                    "sent": "And the number of input points and preserves all the distances up to one plus epsilon, right?",
                    "label": 1
                },
                {
                    "sent": "So the distance after the mapping is almost equal to the distance before the mapping.",
                    "label": 0
                },
                {
                    "sent": "OK, and it can be realized by by very simple transformation in that part of the power of this thing, because you don't need to know much about it, just do something at random, it works.",
                    "label": 1
                },
                {
                    "sent": "And so there's a lot of applications to this.",
                    "label": 0
                },
                {
                    "sent": "When I asked, you know, can we do better?",
                    "label": 1
                },
                {
                    "sent": "OK, so is this bound here.",
                    "label": 0
                },
                {
                    "sent": "Optimal, because of course we can reduce the dimension farther.",
                    "label": 0
                },
                {
                    "sent": "Then we'll get.",
                    "label": 0
                },
                {
                    "sent": "You will gain more.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the usual language this is called.",
                    "label": 0
                },
                {
                    "sent": "This map is called an embedding.",
                    "label": 0
                },
                {
                    "sent": "It is called distortion.",
                    "label": 0
                },
                {
                    "sent": "So whenever you see in the next slide, you know distortion is in this.",
                    "label": 0
                },
                {
                    "sent": "In this sense.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that is you cannot do much better.",
                    "label": 0
                },
                {
                    "sent": "There's a matching lower bound of Noga alone.",
                    "label": 1
                },
                {
                    "sent": "Again, it's a small font, but he says that if you take a uniform metric, we take endpoints where the distance between any pair is exactly 1.",
                    "label": 0
                },
                {
                    "sent": "For example, if points on the regular simplex, that could be your data set.",
                    "label": 0
                },
                {
                    "sent": "Then the dimension of this pointed by a simple argument is basically Logan.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the dimension in my sister doubling dimension.",
                    "label": 0
                },
                {
                    "sent": "It's not relevant for the time being, but you get is that the key that you need?",
                    "label": 0
                },
                {
                    "sent": "That's his proof.",
                    "label": 0
                },
                {
                    "sent": "You need at least Omega Tilda up to a log term epsilon minus 2 Logan, so it's basically matches the bandits written here in the jail.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically it says you cannot improve by the pound is like there's a tight case.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The point is that this tight cases for this uniform metric of endpoints, which exactly means that the dimension here is very very large and I care about cases where the dimension the abstract dimension the doubling dimension.",
                    "label": 0
                },
                {
                    "sent": "Is constant.",
                    "label": 0
                },
                {
                    "sent": "So then in this case that this lower bound is not applicable and hopefully I can do better.",
                    "label": 0
                },
                {
                    "sent": "So it's an open question whether you can do an embedding like this.",
                    "label": 0
                },
                {
                    "sent": "Johnson lindenstrauss.",
                    "label": 0
                },
                {
                    "sent": "This embedding with dimension that depends on epsilon, another dimension of X. OK, independent of the number of points, this still has dementia dependent on the number of points right?",
                    "label": 0
                },
                {
                    "sent": "So for example, if you take endpoints on the line, the theorem says, well, you know I can only produce dimension to log in, even though obviously it's a 1 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So here is a more interesting example of this.",
                    "label": 0
                },
                {
                    "sent": "So this was pulled by Kahan and by Alagona animator like different theorems, but I just didn't make a distinction here.",
                    "label": 1
                },
                {
                    "sent": "We weren't cold, sometimes the Wilsons Helix so OK.",
                    "label": 0
                },
                {
                    "sent": "This is a picture of something which is not really a Helix.",
                    "label": 0
                },
                {
                    "sent": "OK think of it as being a Helix, which means like it goes in infinitely many dimensions, every time you OK, this is supposed to describe the following thing.",
                    "label": 0
                },
                {
                    "sent": "You make one step every time you go away from the origin, but every time you go 90, turn 90 degrees in a new fresh dimension.",
                    "label": 0
                },
                {
                    "sent": "Also everything else.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is of course not this picture.",
                    "label": 0
                },
                {
                    "sent": "And here is the description just like a vector.",
                    "label": 0
                },
                {
                    "sent": "This is the set.",
                    "label": 0
                },
                {
                    "sent": "Tick vectors which has J number of ones and the rest is zeros where J goes from one to N. OK, so you start with the origin, then one and everything is 0 and then two ones and everything so and so on and the distance between the earth point and the Jeff point is.",
                    "label": 0
                },
                {
                    "sent": "Basically I'm in SJ's quilt.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's why it's a Helix because this squirrel thing and for this data set it's easy to see when it leaves in an infinite dimensional space or unit 10 dimensions, but in general infinite dimensional space and it is doubling and they show how to map this into something which think like 3 dimensional Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "There is a good map into 3 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Actually they show that you can get if you want distortion one plus epsilon like here.",
                    "label": 0
                },
                {
                    "sent": "Then you can do it in dimension.",
                    "label": 0
                },
                {
                    "sent": "I forget now either one of us in our website on squared.",
                    "label": 0
                },
                {
                    "sent": "OK so it can get arbitrarily good embedding with very small distortion into a constant dimensional space.",
                    "label": 0
                },
                {
                    "sent": "But this is only for this very specific metric.",
                    "label": 0
                },
                {
                    "sent": "OK, very very specific set.",
                    "label": 0
                },
                {
                    "sent": "And what I asked you know, can you do more generally whenever this dimension the doubling dimension is small, I want to have a good embedding.",
                    "label": 0
                },
                {
                    "sent": "Now this question is open as I said here what we show is some partial result or resolution of this question.",
                    "label": 0
                },
                {
                    "sent": "So we use the dimension of the metric squared.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a little bit more than seems necessary, and but we have some partial guarantees, so either distortion is 1 plus epsilon only.",
                    "label": 1
                },
                {
                    "sent": "What we call for a single scale and explain it in a minute so.",
                    "label": 0
                },
                {
                    "sent": "A single scale of distances that you care about, which is that when the distance between X&Y are about our, let's say between Delta over 100, Delta is a parameter over 100 and this is the scale of this is you care the most.",
                    "label": 0
                },
                {
                    "sent": "So for this I can get one plus epsilon distortion and some weaker guarantees for the rest.",
                    "label": 0
                },
                {
                    "sent": "A second guarantee we can have a second theorem is a global embedding.",
                    "label": 0
                },
                {
                    "sent": "I do something very good for all the pairs, but basically instead of maintaining the distance X -- Y like I had here.",
                    "label": 0
                },
                {
                    "sent": "I'm going to maintain sqrt X -- Y so this is within the images is going to be a very very good approximation to sqrt X -- Y. OK so this is sometimes in some math literature.",
                    "label": 0
                },
                {
                    "sent": "It's called snowflake embedding.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the reason for this name, because this squared.",
                    "label": 0
                },
                {
                    "sent": "And one way to phrase it is that if I know that the square is Euclidean.",
                    "label": 0
                },
                {
                    "sent": "Then basically I think you know, I think the square root of the square, if I applied the feeling for the square of the metric, which is Euclidean, then I have all these things.",
                    "label": 0
                },
                {
                    "sent": "So basically I get take the square root which begins in giving back my.",
                    "label": 0
                },
                {
                    "sent": "Initial my input metric and it holds them OK.",
                    "label": 0
                },
                {
                    "sent": "So one way to phrase this second theorem is to say that the conjecture over here is correct under some additional assumption, which is that the square of these distances it's Euclidean metric.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a special case in which sold for example for altametrics.",
                    "label": 0
                },
                {
                    "sent": "That's especially a special case of this thing, so we know it's true.",
                    "label": 0
                },
                {
                    "sent": "Photometrics, I mean, we know that this conjecture is true.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Altametrics OK so just to say a few words about what is the statement of this tool and say for one scale.",
                    "label": 0
                },
                {
                    "sent": "OK, So what does it mean to maintain one scale then if you give me any finite subset of Euclidean space?",
                    "label": 1
                },
                {
                    "sent": "And I am in a parameter.",
                    "label": 0
                },
                {
                    "sent": "Then I can find a map into a low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "This is L2 to the K, so it's a K dimensional space or dimension of K is thing of like just the dimension of X squared, so this is the you know I match the doubling dimension, the abstract dimension necessary for this space for this point set actually matches by this my embedding and I have the following conditions.",
                    "label": 0
                },
                {
                    "sent": "The Main 1 first, the first one is that the map is Lipschitz only contracts distances.",
                    "label": 0
                },
                {
                    "sent": "But then at the scale that I care about whether this is about R. Well, they get that.",
                    "label": 0
                },
                {
                    "sent": "I get the opposite direction.",
                    "label": 0
                },
                {
                    "sent": "The distance between the images is Omega of the of X X -- Y. OK, so it has a good.",
                    "label": 0
                },
                {
                    "sent": "It's very faithful at this scale and everyone else is just lipids.",
                    "label": 0
                },
                {
                    "sent": "Here the distortion is constant because of Omega, right?",
                    "label": 0
                },
                {
                    "sent": "But turns out with a little bit more work we can actually make it 1 plus epsilon here, OK, but I for simplification I left it out for because then things depends on epsilon.",
                    "label": 0
                },
                {
                    "sent": "So if you compare this to the open question that I said before, it it only gives me the good guarantees at one scale, while I wanted to have things good, good guarantees for at every scale.",
                    "label": 1
                },
                {
                    "sent": "So that's you know this is weaker, but it's also stronger than the conjecture because I get an absolute constant while then the conjecture what's open is, can you get a constant that depends also on the doubling damage?",
                    "label": 0
                },
                {
                    "sent": "OK, let me not go into these details.",
                    "label": 0
                },
                {
                    "sent": "Basically the approach to prove this is by divide and conquer approach so you.",
                    "label": 1
                },
                {
                    "sent": "The Ticul points it you partition into like small sets or clusters or something and then you have to do something in each one and main idea is that each one is very small.",
                    "label": 0
                },
                {
                    "sent": "You can basically apply the JL lemma, the Johnson Lindenstrauss lemma OK have a few points there, so you can apply this.",
                    "label": 0
                },
                {
                    "sent": "So locali what you get is like a linear mapping.",
                    "label": 0
                },
                {
                    "sent": "OK, because the JL lemma theorem basically is approved by a linear transformation, so every locality.",
                    "label": 0
                },
                {
                    "sent": "Roughly speaking, is a linear map.",
                    "label": 0
                },
                {
                    "sent": "Of course, then you have a problem it to glue the glowing in between, so you have to work a little bit harder so you average a few things.",
                    "label": 0
                },
                {
                    "sent": "You smooth things out.",
                    "label": 0
                },
                {
                    "sent": "That's the general idea.",
                    "label": 0
                },
                {
                    "sent": "Now it's not like it's very, very close to it, but I find these similarities to things that this approach similar to things that are being done practically by using linear Maps separately on each locality.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'll skip the second theorem, which is the snowflake I told you about it before I'll skip that exactly.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Statement.",
                    "label": 0
                },
                {
                    "sent": "And the third application I want to tell you about these distance based classification.",
                    "label": 0
                },
                {
                    "sent": "So OK starting afresh well you have to remember the doubling dimension, but now the new context menu application.",
                    "label": 0
                },
                {
                    "sent": "So basically I have pointed like supervised learning.",
                    "label": 0
                },
                {
                    "sent": "I have points with labels and I want to be able to label.",
                    "label": 0
                },
                {
                    "sent": "I want to be able to label new points.",
                    "label": 0
                },
                {
                    "sent": "So the usual thing, but the difference is that the spacer, everything lies is not Euclidean space, it's a metric space.",
                    "label": 0
                },
                {
                    "sent": "Arbitrary metric space.",
                    "label": 0
                },
                {
                    "sent": "Think about this CMD that we had example we had in the beginning.",
                    "label": 0
                },
                {
                    "sent": "OK, so for Euclidean space we have a lot of algorithms.",
                    "label": 0
                },
                {
                    "sent": "Now go to MD's.",
                    "label": 0
                },
                {
                    "sent": "Oh my God, nobody has proved this.",
                    "label": 0
                },
                {
                    "sent": "I have to prove it from scratch right?",
                    "label": 0
                },
                {
                    "sent": "So the idea is that you don't have to do it from scratch.",
                    "label": 0
                },
                {
                    "sent": "There's a general theory and there's actually a very nice framework of how to do this large margin classification.",
                    "label": 0
                },
                {
                    "sent": "So you get OK, will talk about but five unlocks Bergen Busquet.",
                    "label": 0
                },
                {
                    "sent": "I hope I pronounced it right.",
                    "label": 0
                },
                {
                    "sent": "I apologize if not.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is roughly what they are able to show for summarized.",
                    "label": 0
                },
                {
                    "sent": "So basically what are the natural hypothesis?",
                    "label": 0
                },
                {
                    "sent": "The classifiers in this space you cannot use like a hyperplane 'cause there's no, it's not a vector space in the sense of this.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that the natural hyper hypothesis and from now on I'm going to call it the classifiers.",
                    "label": 0
                },
                {
                    "sent": "Lipschitz functions, and you want this function to be as smooth as possible?",
                    "label": 0
                },
                {
                    "sent": "You want the small lips constant for these functions.",
                    "label": 0
                },
                {
                    "sent": "Let's say into the rails like you want to.",
                    "label": 0
                },
                {
                    "sent": "Eventually you think of this function into actually plus 1 -- 1 or something, so it's two labels.",
                    "label": 0
                },
                {
                    "sent": "Doing a classification what it mean basically what they show is how to reduce this problem into basically finding an F function F classifier hypothesis that is consistent with the labels you observe.",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                },
                {
                    "sent": "Turns out that.",
                    "label": 0
                },
                {
                    "sent": "Like fixing such an F, it reduces to basically a classic problem in math, which is known as Lipschitz extension.",
                    "label": 1
                },
                {
                    "sent": "You already have a function which is Lipschitz on the training set, but you have to be value be able to evaluate the function on points that are outside your training set, right?",
                    "label": 0
                },
                {
                    "sent": "So this is exactly a well known problem essentially, but I mean you have to do this reduction.",
                    "label": 0
                },
                {
                    "sent": "And then then they establish some generalization bounds.",
                    "label": 0
                },
                {
                    "sent": "Now, in this generality, it's very difficult to say what are the bounds.",
                    "label": 0
                },
                {
                    "sent": "I didn't write them here, but they depend on certain specific properties of the space, like Rademacher averages or something.",
                    "label": 0
                },
                {
                    "sent": "And then they say OK.",
                    "label": 0
                },
                {
                    "sent": "So now you have this classifier.",
                    "label": 0
                },
                {
                    "sent": "You fixed a function.",
                    "label": 0
                },
                {
                    "sent": "Now you have to compute this classification.",
                    "label": 0
                },
                {
                    "sent": "You wanted to do it somehow organized given algorithm.",
                    "label": 0
                },
                {
                    "sent": "How to compute F of X when you're given X.",
                    "label": 0
                },
                {
                    "sent": "So basically said well they reduce it to the problem of 1 near neighbor search.",
                    "label": 0
                },
                {
                    "sent": "So if you're given a point you want to give the label well, first you have to find the closest point.",
                    "label": 0
                },
                {
                    "sent": "Roughly speaking, as you find the closest point of that point is labeled positively you answer positively.",
                    "label": 0
                },
                {
                    "sent": "It's labeled negatively, negatively.",
                    "label": 0
                },
                {
                    "sent": "OK so so basically doing exactly enable search and what they show is assumes zero training error.",
                    "label": 0
                },
                {
                    "sent": "OK, so because you're giving the points, you basically try to match elliptic function to these points now and a very nice thing is that I find is that this one and this of course is a known holistic, so it kind of gives you explanation in very general terms.",
                    "label": 0
                },
                {
                    "sent": "The 1S has good bounds, good good performance.",
                    "label": 0
                },
                {
                    "sent": "What are you kind of like?",
                    "label": 0
                },
                {
                    "sent": "Leave open I think is a few issues of computational efficiencies.",
                    "label": 0
                },
                {
                    "sent": "So right now the number of samples that you need.",
                    "label": 0
                },
                {
                    "sent": "But how to do it and is using an efficient algorithm in terms of runtime.",
                    "label": 0
                },
                {
                    "sent": "So for example, they use one and S. And what happens if you cannot solve exactly?",
                    "label": 0
                },
                {
                    "sent": "Then you enable such problem exactly.",
                    "label": 0
                },
                {
                    "sent": "It's not clear that the proof would extend, and another issue is what happens with training or all this analysis assumes zero, 20, or part of the analysis assumes zero 20 euro.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what we show in this work with Gottlieb and Control, which is basically that if you have the data line in low doubling dimension, OK, so that's one more assumption.",
                    "label": 0
                },
                {
                    "sent": "Then you can have a classification which is both accurate and computationally efficient.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's kind of like you can.",
                    "label": 0
                },
                {
                    "sent": "In other words, you can apply the framework, but also make sure that it is computationally efficient.",
                    "label": 0
                },
                {
                    "sent": "OK, in terms of runtime.",
                    "label": 0
                },
                {
                    "sent": "And I think this is the first relationship between this efficiency issues and that the abstract the metric dimension of your data.",
                    "label": 0
                },
                {
                    "sent": "OK, there's a different work that relates something, but it's not the same thing, so I'll just point out I am aware of this work, but it's different.",
                    "label": 0
                },
                {
                    "sent": "So here is like, you know, the two points, and that's basically like the end of what I want to tell you about this.",
                    "label": 0
                },
                {
                    "sent": "I mean, the paper will be presented called.",
                    "label": 0
                },
                {
                    "sent": "So if you really want to hear about it, you can come on Sunday morning.",
                    "label": 0
                },
                {
                    "sent": "So basically, how do you choose the classifier quickly so they've seen?",
                    "label": 0
                },
                {
                    "sent": "Show me how to do it without training goals and now I have.",
                    "label": 0
                },
                {
                    "sent": "I have training goals.",
                    "label": 0
                },
                {
                    "sent": "OK so if I have things I have to find those.",
                    "label": 0
                },
                {
                    "sent": "Of course you can do it by exhaustive search tie.",
                    "label": 0
                },
                {
                    "sent": "Say you know the training with 10%, you try to rule out every possible 10% of the data.",
                    "label": 0
                },
                {
                    "sent": "Then you can match and you can use their algorithm.",
                    "label": 0
                },
                {
                    "sent": "That's of course not very efficient.",
                    "label": 0
                },
                {
                    "sent": "Exponential time so in another problem is how to optimize the bias variance tradeoff.",
                    "label": 0
                },
                {
                    "sent": "Because if you change this 10% to different number then you know the tradeoff between these things.",
                    "label": 0
                },
                {
                    "sent": "Changes between the bias in the violence.",
                    "label": 0
                },
                {
                    "sent": "The question can you know in advance what's the right value?",
                    "label": 0
                },
                {
                    "sent": "OK, because I mean they give some formula, but it's not clear you can optimize it.",
                    "label": 0
                },
                {
                    "sent": "You know in runtime what's the right value to take if you don't know what the noise level.",
                    "label": 0
                },
                {
                    "sent": "And the key step that we do analysis of course server summary of these things is if you show that you can do the following if you fix the target Lipschitz constant.",
                    "label": 0
                },
                {
                    "sent": "Then I can find which are the best outliers to remove the smallest possible number and which are the points.",
                    "label": 0
                },
                {
                    "sent": "Once you remove them.",
                    "label": 0
                },
                {
                    "sent": "Basically the point reduces to zero training you and then you can do some sort of like binary search or something to find out.",
                    "label": 0
                },
                {
                    "sent": "Figure out which one is the best and estimated all these quantities to optimize the bias variance tradeoff.",
                    "label": 0
                },
                {
                    "sent": "And another issue is that I said we did exactly Ness and the analysis does not extend to approximately enable search OK and we show how you can do this.",
                    "label": 0
                },
                {
                    "sent": "There's some problem with doing approximately.",
                    "label": 0
                },
                {
                    "sent": "Google search, you only know that you get approximate new neighbors.",
                    "label": 0
                },
                {
                    "sent": "So in principle if I give you the same query twice, you might give me different answers.",
                    "label": 0
                },
                {
                    "sent": "So same code we think of like like you know, infinitesimally close queries or something.",
                    "label": 0
                },
                {
                    "sent": "So just effectively the same query twice and I ask you again again you give me different answers about the nearest nearest neighbor because there might be like a tie.",
                    "label": 0
                },
                {
                    "sent": "And then your function.",
                    "label": 0
                },
                {
                    "sent": "My function defined might not be really function, so it turns out that it's not a real problem, but you have to work workout this analysis to see that really you can get around this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so to summarize, I think the bigger picture is that we have good algorithms for low dimensional spaces, so this notion of dimension.",
                    "label": 1
                },
                {
                    "sent": "I think in principle can be equated with.",
                    "label": 0
                },
                {
                    "sent": "Computational efficiency, or, you know, complexity of the data in terms of computational efficiency, and I hope that this kind of approach of like saying, well, you know, I if I can.",
                    "label": 0
                },
                {
                    "sent": "Have an assumption on my data which you know it's relatively reasonable.",
                    "label": 0
                },
                {
                    "sent": "Relatively possible to verify this on the data.",
                    "label": 0
                },
                {
                    "sent": "You can actually get good algorithms and you know you don't solve the problem.",
                    "label": 0
                },
                {
                    "sent": "In the worst case completely, but only like your 80% of the cases in real life or something.",
                    "label": 1
                },
                {
                    "sent": "And then keep going this way.",
                    "label": 0
                },
                {
                    "sent": "I listed a few.",
                    "label": 0
                },
                {
                    "sent": "Instances here and then.",
                    "label": 0
                },
                {
                    "sent": "One thing that I like about this thing is also that somehow they give a good explanation of why you risztics Walton when I'm in good explanation is that there's a rigorous analysis.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the way I look at.",
                    "label": 0
                },
                {
                    "sent": "You know now I understand why it works so well.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}