{
    "id": "g7juyf7sjfwcq2k67saticcbbytccm34",
    "title": "Fast Query Execution for Retrieval Models Based on Path-Constrained Random Walks",
    "info": {
        "author": [
            "Ni Lao, Language Technologies Institute, Carnegie Mellon University"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Information Retrieval"
        ]
    },
    "url": "http://videolectures.net/kdd2010_lao_fer/",
    "segmentation": [
        [
            "I am Neal out.",
            "This will work with my advisor William Cohen.",
            "The topic is fast query execution for retrieval models based on past constraint random works."
        ],
        [
            "So here's the outline.",
            "Our first start with background.",
            "I will tell a little bit about our previous work retrieval models based on past constraint random walks, which is published in XML this year.",
            "And then I will go on to discuss about the efficiency of random walk.",
            "Basically we compare several sampling and truncation strategies when doing random walk."
        ],
        [
            "So.",
            "This is the problem we are considering right now.",
            "It's we call it relational retrieval problems.",
            "So the data of many retrieval tasks like recommendation or retrieval can be represented as a labeled directed graph for those type of graph we have notes like documents, users, words, or any other metadata.",
            "Any type of entities, and we have labeled edges like from author to its paper or from paper to the publication year.",
            "And with this representation, it's very general graph representation of data.",
            "We can support a family of type proximity queries.",
            "For example, if we want to do ad hoc search, we can treat terms for query and document as the target type we're trying to retrieval.",
            "For reference recommendation, we can treat a topic as the query and all the papers in the graph as our target and other tasks.",
            "Similarly, the main problem here is how to measure the proximity between our query nodes and the target nodes."
        ],
        [
            "So for example, this is the data we are using for this study.",
            "It's basically biology, literature data we collected from 2 databases.",
            "One is called East.",
            "It contains publication.",
            "And Journal user.",
            "Basically it's a pub later with publication model annotated with entities of biologic entities like genes and proteins and the second data set is called Fly and it's extracted from.",
            "Database called Fly Mine and the task so we are considering for this study.",
            "Three there's three of them.",
            "One is called gene recommendation.",
            "Basically we recommend to each author what are the possible gene there might be interested in the next year.",
            "So we include entity year, the current year as one of the query and query nails.",
            "Second task is called reference recommendation.",
            "This is a very common.",
            "Ask or which is given some type some topic or you want to recommend some papers that should be that's relevant and should be references referenced.",
            "The third task is called expert finding and this is also a typical task and I are in many other settings we find.",
            "People who are who have knowledge about some topic.",
            "Um?"
        ],
        [
            "So a random walk with restart has been commonly used as a similarity measure on those kind of labeled graphs, so there are several many previous work in this area and later recent.",
            "Trend was to train the random walk, restart model whereas supervised training.",
            "People have been optimizing the random walk restart model with different optimization procedure.",
            "So basically the model has one.",
            "Weight 1 weight on each type.",
            "Each type of edge.",
            "So the the optimization procedure, we can tune these edges so that the random worker can go to the relevant entities more often than irrelevant entities.",
            "So here I show a drunken man doing random walk.",
            "Around the light post."
        ],
        [
            "However, this popular similarity measure has its limitation.",
            "Here I show some example why it's limited.",
            "Limited so the main limitation is the parameterization.",
            "It has one parameter for each edge type and is, well essentially ignore the context in which this type of edge was used.",
            "So here's an example.",
            "Suppose we are trying to reply.",
            "Recommend new paper for an.",
            "For scientists to read.",
            "Based on our previous study, we found that.",
            "This edge sequence usually leads to irrelevant papers, but the second one usually lead the relevant entity irrelevant papers.",
            "I mark them as red and blue Blues are good weather player, so they both of them has the edge type.",
            "Read some moderate some paper, but it appears in both good and bad.",
            "Sequences similarly the the edge from paper to gene paper contains a gene appears both in bad and good.",
            "Seconds, so in order to distinguish different segments, we it's.",
            "Not natural, it's not a natural way to assign weight on each of the edges.",
            "It's more natural to assign weight to us whole sequence of edge.",
            "This is the basic idea in our study."
        ],
        [
            "Therefore, we introduce a new proxy measure called path constrained random walk.",
            "So in our previous work we introduce a model which can learn a weighted combination of simple pass experts, each of which correspond to a particular label path through the graph.",
            "Here I give an example how this model works.",
            "For example the there is a task in track, conference, track, chemical prior art search task.",
            "The task is to find relevant patent to site for particular patent.",
            "So researchers have found that it's more effective to 1st find patent about the topic and then aggregate their citations.",
            "Essentially the scheme scheme of the retrieval is like this one from the search word.",
            "We first find relevant paper.",
            "Or patent and then from this pattern find what other patent it was cited.",
            "This is people find what usually useful and with our model we can discover this scheme that is highly weighted and we can also discover other related schemes that can lead us to relevant documents.",
            "So basically our model will enumerate all possible passes up to some lens lens of path and find which one works best and how to combine them to do the retrieval task."
        ],
        [
            "So before diving into the detail of this.",
            "Model I give some definitions, so our data is represented as the entity relation graph, which is a type which is a set of entity types, a set of entities and a set of relations.",
            "Each entity has its type, each a relation path.",
            "It's a sequence of relations like this.",
            "This is Lynn Swann.",
            "Relation path is events 2.",
            "This relation is some paper publishing some year or some paper cites some.",
            "Other paper this is to relation.",
            "And we define a path constraint, random walk.",
            "As such, sorry.",
            "So given a query which is a set of seed entities and target entity type that we're trying to retrieve.",
            "Constraint one what would define.",
            "We will recursively define.",
            "Distributions for which.",
            "Distribution for Grandma Distribution will correspond to a random walk constraint hours.",
            "Particular passes, for example, of this distribution on this paper.",
            "Node will be a run walk starting from.",
            "Seat in this paper, type through this particular sequence."
        ],
        [
            "And the retrieval model later rank the target entities by linearly combined these distributions.",
            "So each edge P is a distribution for a particular path, and we combine these distributions by weight parameter Theta P. So in matrix form, it looks like S score vector A is a distribution matrix and state is our sorry.",
            "Very far away.",
            "Consider is our partner, and this model can be supervised Lee trained by maximizing the likelihood of observing the relevance provided by user.",
            "So y = 1.",
            "Missed means this entity is relevant or there always means it is irrelevant and this information is provided by user and we have shown that this past constrained random walk based model can significantly improve retrieval quality over the commonly used.",
            "Random Walk restart model."
        ],
        [
            "OK, until now we got the background about what model we're working on an now we move on to efficiency problem because you can see it."
        ],
        [
            "This model we have.",
            "Maybe hundreds of passes to combine for each path we need to do random walk.",
            "Which can be very expensive, especially when our graph is dense.",
            "Or when the path is very long.",
            "If the path is long, we have large number of passes to enumerate to execute.",
            "So therefore this motivate us to find efficient methods.",
            "Such strategies to do random walk.",
            "There has been.",
            "Are there have been several popular strategies that have been used by people before, like a sampling strategies?",
            "People usually called fingerprinting or truncation strategies and many other works has been down to analyze the graph itself to represent graphing at 2 levels.",
            "Into two level and doing random looking higher level and then to the lower level in this study will compare them pulling and truncation strategies which is easy simpler than the two level representation strategy."
        ],
        [
            "So what you have four strategies in total?",
            "The first one is called fingerprinting or sampling.",
            "It's quite straightforward.",
            "We simulate random workers on the graph starting from our query node many, many times, maybe thousands of times, and the probability of reaching one node is just a normalized count.",
            "This is very popular.",
            "Only used before the 2nd and the 3rd strategies are truncation strategies.",
            "The signal line is called fixed truncation because after we.",
            "We first calculate the exact distribution H and then we subtract.",
            "Smaller.",
            "A small amount for every entities.",
            "If 7.",
            "Therefore, if entity has very small probability, then it will be truncated talaro.",
            "Therefore our distribution is sparse and the handling world be efficient.",
            "The third strategy we call it Bing truncation.",
            "Here this small amount is called Ethan W, which is W's largest.",
            "Or list among probability among all the entities.",
            "Therefore, after this transition will get exactly W entities that have nonzero probability.",
            "Does the first strategy we called particle filtering, and it's basically a combination of exact inference and sampling.",
            "We will detail in the next slide."
        ],
        [
            "This is algorithms relatively simple, so when the particles are large we basically are doing that inference.",
            "We doing calculating exact distribution, but when the particles become smaller, smaller than a threshold, then we'll switch to sampling strategy will just pick several out links to follow instead of following all of them."
        ],
        [
            "Oh OK.",
            "This is our experiment.",
            "Set up again.",
            "We have the data, two data sources, Easton flying that basis and we we generate labeled data in the following way we take.",
            "Paper treat here like trick.",
            "It's also a year's query and trade the gene.",
            "The paper has mentioned as relevance judgment.",
            "In this way, in this way we can have thousands of quirion sound of judgment.",
            "Similarly, for reference, recommendation will choose the title words and the year of the paper's query, and trade the actual.",
            "OK. Actual paper the excited as relevant judgment.",
            "We randomly pick 2000 for training 2004 tuning into sample test.",
            "There is 1.",
            "Technology we used to track to create realistic training data which is we tag each edge in the graph with a timestamp.",
            "In here it's the year and we were we are doing to random work for training we only consider edges that earlier than the query well considering there working simulate the realistic situation for training."
        ],
        [
            "Here are some examples of features the model have learned similarly.",
            "This is finding paper on the topic and.",
            "Aggregate their citation.",
            "This is like finding.",
            "Co cited papers coincided with the Ontopic papers.",
            "#6 rassemble commonly use ad hoc retrieval system.",
            "You get put you give keyword and you find papers with this secure and you can say this is not the most important feature.",
            "The more important ones has more complex scheme.",
            "Yeah."
        ],
        [
            "Here it's a result on the East data and these two lines the these two lines are the truncation strategies and this more extended lines are the sampling strategies.",
            "So basically the truncation judges has limited ability to speed up the random walk, basically because you need to 1st do the exact inference and then do the truncation, so it limits his ability, but the number 2 sampling strategies the particle filtering is.",
            "Works better than the traditionally used lampooning started fingerprinting."
        ],
        [
            "This is result on the flight data on the train is very similar."
        ],
        [
            "This is the observations I just mentioned about sampling works better than truncation and particle filtering works better than fingerprinting and interesting.",
            "Sometimes the retrieval quality can be improved with.",
            "With these speedups strategies, like this one, when we get speedup, we also gain in the accuracy of our retrieval.",
            "Detailed analysis can be found in our paper.",
            "So in summer."
        ],
        [
            "We here we are considering retrieval and recommendation tasks.",
            "We have structured data with complex schema, but it's hard to manually design retrieval schemes.",
            "Therefore we discover retrieval schemes from users feedback and then with complex retrieval scheme this hard.",
            "It's very expensive to execute complex retrieval schemes, so we compare different approximate random walk strategies.",
            "And which can give us up to 100 vote speedup?",
            "So."
        ],
        [
            "That's it, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I am Neal out.",
                    "label": 0
                },
                {
                    "sent": "This will work with my advisor William Cohen.",
                    "label": 0
                },
                {
                    "sent": "The topic is fast query execution for retrieval models based on past constraint random works.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the outline.",
                    "label": 0
                },
                {
                    "sent": "Our first start with background.",
                    "label": 0
                },
                {
                    "sent": "I will tell a little bit about our previous work retrieval models based on past constraint random walks, which is published in XML this year.",
                    "label": 1
                },
                {
                    "sent": "And then I will go on to discuss about the efficiency of random walk.",
                    "label": 1
                },
                {
                    "sent": "Basically we compare several sampling and truncation strategies when doing random walk.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the problem we are considering right now.",
                    "label": 0
                },
                {
                    "sent": "It's we call it relational retrieval problems.",
                    "label": 1
                },
                {
                    "sent": "So the data of many retrieval tasks like recommendation or retrieval can be represented as a labeled directed graph for those type of graph we have notes like documents, users, words, or any other metadata.",
                    "label": 1
                },
                {
                    "sent": "Any type of entities, and we have labeled edges like from author to its paper or from paper to the publication year.",
                    "label": 0
                },
                {
                    "sent": "And with this representation, it's very general graph representation of data.",
                    "label": 0
                },
                {
                    "sent": "We can support a family of type proximity queries.",
                    "label": 1
                },
                {
                    "sent": "For example, if we want to do ad hoc search, we can treat terms for query and document as the target type we're trying to retrieval.",
                    "label": 0
                },
                {
                    "sent": "For reference recommendation, we can treat a topic as the query and all the papers in the graph as our target and other tasks.",
                    "label": 0
                },
                {
                    "sent": "Similarly, the main problem here is how to measure the proximity between our query nodes and the target nodes.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example, this is the data we are using for this study.",
                    "label": 1
                },
                {
                    "sent": "It's basically biology, literature data we collected from 2 databases.",
                    "label": 1
                },
                {
                    "sent": "One is called East.",
                    "label": 0
                },
                {
                    "sent": "It contains publication.",
                    "label": 0
                },
                {
                    "sent": "And Journal user.",
                    "label": 1
                },
                {
                    "sent": "Basically it's a pub later with publication model annotated with entities of biologic entities like genes and proteins and the second data set is called Fly and it's extracted from.",
                    "label": 0
                },
                {
                    "sent": "Database called Fly Mine and the task so we are considering for this study.",
                    "label": 0
                },
                {
                    "sent": "Three there's three of them.",
                    "label": 1
                },
                {
                    "sent": "One is called gene recommendation.",
                    "label": 0
                },
                {
                    "sent": "Basically we recommend to each author what are the possible gene there might be interested in the next year.",
                    "label": 1
                },
                {
                    "sent": "So we include entity year, the current year as one of the query and query nails.",
                    "label": 0
                },
                {
                    "sent": "Second task is called reference recommendation.",
                    "label": 0
                },
                {
                    "sent": "This is a very common.",
                    "label": 0
                },
                {
                    "sent": "Ask or which is given some type some topic or you want to recommend some papers that should be that's relevant and should be references referenced.",
                    "label": 0
                },
                {
                    "sent": "The third task is called expert finding and this is also a typical task and I are in many other settings we find.",
                    "label": 0
                },
                {
                    "sent": "People who are who have knowledge about some topic.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a random walk with restart has been commonly used as a similarity measure on those kind of labeled graphs, so there are several many previous work in this area and later recent.",
                    "label": 1
                },
                {
                    "sent": "Trend was to train the random walk, restart model whereas supervised training.",
                    "label": 0
                },
                {
                    "sent": "People have been optimizing the random walk restart model with different optimization procedure.",
                    "label": 0
                },
                {
                    "sent": "So basically the model has one.",
                    "label": 0
                },
                {
                    "sent": "Weight 1 weight on each type.",
                    "label": 0
                },
                {
                    "sent": "Each type of edge.",
                    "label": 0
                },
                {
                    "sent": "So the the optimization procedure, we can tune these edges so that the random worker can go to the relevant entities more often than irrelevant entities.",
                    "label": 0
                },
                {
                    "sent": "So here I show a drunken man doing random walk.",
                    "label": 0
                },
                {
                    "sent": "Around the light post.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, this popular similarity measure has its limitation.",
                    "label": 0
                },
                {
                    "sent": "Here I show some example why it's limited.",
                    "label": 0
                },
                {
                    "sent": "Limited so the main limitation is the parameterization.",
                    "label": 0
                },
                {
                    "sent": "It has one parameter for each edge type and is, well essentially ignore the context in which this type of edge was used.",
                    "label": 1
                },
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "Suppose we are trying to reply.",
                    "label": 0
                },
                {
                    "sent": "Recommend new paper for an.",
                    "label": 0
                },
                {
                    "sent": "For scientists to read.",
                    "label": 0
                },
                {
                    "sent": "Based on our previous study, we found that.",
                    "label": 0
                },
                {
                    "sent": "This edge sequence usually leads to irrelevant papers, but the second one usually lead the relevant entity irrelevant papers.",
                    "label": 0
                },
                {
                    "sent": "I mark them as red and blue Blues are good weather player, so they both of them has the edge type.",
                    "label": 0
                },
                {
                    "sent": "Read some moderate some paper, but it appears in both good and bad.",
                    "label": 1
                },
                {
                    "sent": "Sequences similarly the the edge from paper to gene paper contains a gene appears both in bad and good.",
                    "label": 0
                },
                {
                    "sent": "Seconds, so in order to distinguish different segments, we it's.",
                    "label": 0
                },
                {
                    "sent": "Not natural, it's not a natural way to assign weight on each of the edges.",
                    "label": 0
                },
                {
                    "sent": "It's more natural to assign weight to us whole sequence of edge.",
                    "label": 0
                },
                {
                    "sent": "This is the basic idea in our study.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Therefore, we introduce a new proxy measure called path constrained random walk.",
                    "label": 1
                },
                {
                    "sent": "So in our previous work we introduce a model which can learn a weighted combination of simple pass experts, each of which correspond to a particular label path through the graph.",
                    "label": 1
                },
                {
                    "sent": "Here I give an example how this model works.",
                    "label": 0
                },
                {
                    "sent": "For example the there is a task in track, conference, track, chemical prior art search task.",
                    "label": 1
                },
                {
                    "sent": "The task is to find relevant patent to site for particular patent.",
                    "label": 1
                },
                {
                    "sent": "So researchers have found that it's more effective to 1st find patent about the topic and then aggregate their citations.",
                    "label": 0
                },
                {
                    "sent": "Essentially the scheme scheme of the retrieval is like this one from the search word.",
                    "label": 0
                },
                {
                    "sent": "We first find relevant paper.",
                    "label": 0
                },
                {
                    "sent": "Or patent and then from this pattern find what other patent it was cited.",
                    "label": 0
                },
                {
                    "sent": "This is people find what usually useful and with our model we can discover this scheme that is highly weighted and we can also discover other related schemes that can lead us to relevant documents.",
                    "label": 0
                },
                {
                    "sent": "So basically our model will enumerate all possible passes up to some lens lens of path and find which one works best and how to combine them to do the retrieval task.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before diving into the detail of this.",
                    "label": 0
                },
                {
                    "sent": "Model I give some definitions, so our data is represented as the entity relation graph, which is a type which is a set of entity types, a set of entities and a set of relations.",
                    "label": 1
                },
                {
                    "sent": "Each entity has its type, each a relation path.",
                    "label": 1
                },
                {
                    "sent": "It's a sequence of relations like this.",
                    "label": 0
                },
                {
                    "sent": "This is Lynn Swann.",
                    "label": 0
                },
                {
                    "sent": "Relation path is events 2.",
                    "label": 1
                },
                {
                    "sent": "This relation is some paper publishing some year or some paper cites some.",
                    "label": 0
                },
                {
                    "sent": "Other paper this is to relation.",
                    "label": 1
                },
                {
                    "sent": "And we define a path constraint, random walk.",
                    "label": 0
                },
                {
                    "sent": "As such, sorry.",
                    "label": 0
                },
                {
                    "sent": "So given a query which is a set of seed entities and target entity type that we're trying to retrieve.",
                    "label": 0
                },
                {
                    "sent": "Constraint one what would define.",
                    "label": 0
                },
                {
                    "sent": "We will recursively define.",
                    "label": 0
                },
                {
                    "sent": "Distributions for which.",
                    "label": 0
                },
                {
                    "sent": "Distribution for Grandma Distribution will correspond to a random walk constraint hours.",
                    "label": 0
                },
                {
                    "sent": "Particular passes, for example, of this distribution on this paper.",
                    "label": 0
                },
                {
                    "sent": "Node will be a run walk starting from.",
                    "label": 0
                },
                {
                    "sent": "Seat in this paper, type through this particular sequence.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the retrieval model later rank the target entities by linearly combined these distributions.",
                    "label": 1
                },
                {
                    "sent": "So each edge P is a distribution for a particular path, and we combine these distributions by weight parameter Theta P. So in matrix form, it looks like S score vector A is a distribution matrix and state is our sorry.",
                    "label": 0
                },
                {
                    "sent": "Very far away.",
                    "label": 1
                },
                {
                    "sent": "Consider is our partner, and this model can be supervised Lee trained by maximizing the likelihood of observing the relevance provided by user.",
                    "label": 0
                },
                {
                    "sent": "So y = 1.",
                    "label": 0
                },
                {
                    "sent": "Missed means this entity is relevant or there always means it is irrelevant and this information is provided by user and we have shown that this past constrained random walk based model can significantly improve retrieval quality over the commonly used.",
                    "label": 1
                },
                {
                    "sent": "Random Walk restart model.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, until now we got the background about what model we're working on an now we move on to efficiency problem because you can see it.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This model we have.",
                    "label": 0
                },
                {
                    "sent": "Maybe hundreds of passes to combine for each path we need to do random walk.",
                    "label": 1
                },
                {
                    "sent": "Which can be very expensive, especially when our graph is dense.",
                    "label": 1
                },
                {
                    "sent": "Or when the path is very long.",
                    "label": 0
                },
                {
                    "sent": "If the path is long, we have large number of passes to enumerate to execute.",
                    "label": 0
                },
                {
                    "sent": "So therefore this motivate us to find efficient methods.",
                    "label": 0
                },
                {
                    "sent": "Such strategies to do random walk.",
                    "label": 1
                },
                {
                    "sent": "There has been.",
                    "label": 0
                },
                {
                    "sent": "Are there have been several popular strategies that have been used by people before, like a sampling strategies?",
                    "label": 0
                },
                {
                    "sent": "People usually called fingerprinting or truncation strategies and many other works has been down to analyze the graph itself to represent graphing at 2 levels.",
                    "label": 0
                },
                {
                    "sent": "Into two level and doing random looking higher level and then to the lower level in this study will compare them pulling and truncation strategies which is easy simpler than the two level representation strategy.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what you have four strategies in total?",
                    "label": 1
                },
                {
                    "sent": "The first one is called fingerprinting or sampling.",
                    "label": 0
                },
                {
                    "sent": "It's quite straightforward.",
                    "label": 0
                },
                {
                    "sent": "We simulate random workers on the graph starting from our query node many, many times, maybe thousands of times, and the probability of reaching one node is just a normalized count.",
                    "label": 0
                },
                {
                    "sent": "This is very popular.",
                    "label": 0
                },
                {
                    "sent": "Only used before the 2nd and the 3rd strategies are truncation strategies.",
                    "label": 0
                },
                {
                    "sent": "The signal line is called fixed truncation because after we.",
                    "label": 0
                },
                {
                    "sent": "We first calculate the exact distribution H and then we subtract.",
                    "label": 0
                },
                {
                    "sent": "Smaller.",
                    "label": 0
                },
                {
                    "sent": "A small amount for every entities.",
                    "label": 0
                },
                {
                    "sent": "If 7.",
                    "label": 0
                },
                {
                    "sent": "Therefore, if entity has very small probability, then it will be truncated talaro.",
                    "label": 0
                },
                {
                    "sent": "Therefore our distribution is sparse and the handling world be efficient.",
                    "label": 0
                },
                {
                    "sent": "The third strategy we call it Bing truncation.",
                    "label": 0
                },
                {
                    "sent": "Here this small amount is called Ethan W, which is W's largest.",
                    "label": 0
                },
                {
                    "sent": "Or list among probability among all the entities.",
                    "label": 0
                },
                {
                    "sent": "Therefore, after this transition will get exactly W entities that have nonzero probability.",
                    "label": 0
                },
                {
                    "sent": "Does the first strategy we called particle filtering, and it's basically a combination of exact inference and sampling.",
                    "label": 1
                },
                {
                    "sent": "We will detail in the next slide.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is algorithms relatively simple, so when the particles are large we basically are doing that inference.",
                    "label": 0
                },
                {
                    "sent": "We doing calculating exact distribution, but when the particles become smaller, smaller than a threshold, then we'll switch to sampling strategy will just pick several out links to follow instead of following all of them.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh OK.",
                    "label": 0
                },
                {
                    "sent": "This is our experiment.",
                    "label": 0
                },
                {
                    "sent": "Set up again.",
                    "label": 0
                },
                {
                    "sent": "We have the data, two data sources, Easton flying that basis and we we generate labeled data in the following way we take.",
                    "label": 0
                },
                {
                    "sent": "Paper treat here like trick.",
                    "label": 0
                },
                {
                    "sent": "It's also a year's query and trade the gene.",
                    "label": 0
                },
                {
                    "sent": "The paper has mentioned as relevance judgment.",
                    "label": 0
                },
                {
                    "sent": "In this way, in this way we can have thousands of quirion sound of judgment.",
                    "label": 0
                },
                {
                    "sent": "Similarly, for reference, recommendation will choose the title words and the year of the paper's query, and trade the actual.",
                    "label": 1
                },
                {
                    "sent": "OK. Actual paper the excited as relevant judgment.",
                    "label": 0
                },
                {
                    "sent": "We randomly pick 2000 for training 2004 tuning into sample test.",
                    "label": 0
                },
                {
                    "sent": "There is 1.",
                    "label": 1
                },
                {
                    "sent": "Technology we used to track to create realistic training data which is we tag each edge in the graph with a timestamp.",
                    "label": 0
                },
                {
                    "sent": "In here it's the year and we were we are doing to random work for training we only consider edges that earlier than the query well considering there working simulate the realistic situation for training.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are some examples of features the model have learned similarly.",
                    "label": 0
                },
                {
                    "sent": "This is finding paper on the topic and.",
                    "label": 0
                },
                {
                    "sent": "Aggregate their citation.",
                    "label": 0
                },
                {
                    "sent": "This is like finding.",
                    "label": 0
                },
                {
                    "sent": "Co cited papers coincided with the Ontopic papers.",
                    "label": 1
                },
                {
                    "sent": "#6 rassemble commonly use ad hoc retrieval system.",
                    "label": 0
                },
                {
                    "sent": "You get put you give keyword and you find papers with this secure and you can say this is not the most important feature.",
                    "label": 0
                },
                {
                    "sent": "The more important ones has more complex scheme.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here it's a result on the East data and these two lines the these two lines are the truncation strategies and this more extended lines are the sampling strategies.",
                    "label": 0
                },
                {
                    "sent": "So basically the truncation judges has limited ability to speed up the random walk, basically because you need to 1st do the exact inference and then do the truncation, so it limits his ability, but the number 2 sampling strategies the particle filtering is.",
                    "label": 0
                },
                {
                    "sent": "Works better than the traditionally used lampooning started fingerprinting.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is result on the flight data on the train is very similar.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the observations I just mentioned about sampling works better than truncation and particle filtering works better than fingerprinting and interesting.",
                    "label": 1
                },
                {
                    "sent": "Sometimes the retrieval quality can be improved with.",
                    "label": 0
                },
                {
                    "sent": "With these speedups strategies, like this one, when we get speedup, we also gain in the accuracy of our retrieval.",
                    "label": 0
                },
                {
                    "sent": "Detailed analysis can be found in our paper.",
                    "label": 0
                },
                {
                    "sent": "So in summer.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We here we are considering retrieval and recommendation tasks.",
                    "label": 0
                },
                {
                    "sent": "We have structured data with complex schema, but it's hard to manually design retrieval schemes.",
                    "label": 0
                },
                {
                    "sent": "Therefore we discover retrieval schemes from users feedback and then with complex retrieval scheme this hard.",
                    "label": 0
                },
                {
                    "sent": "It's very expensive to execute complex retrieval schemes, so we compare different approximate random walk strategies.",
                    "label": 0
                },
                {
                    "sent": "And which can give us up to 100 vote speedup?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's it, thanks.",
                    "label": 0
                }
            ]
        }
    }
}