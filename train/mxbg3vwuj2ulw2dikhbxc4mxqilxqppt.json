{
    "id": "mxbg3vwuj2ulw2dikhbxc4mxqilxqppt",
    "title": "Policy Search",
    "info": {
        "author": [
            "Sergey Levine, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "Oct. 11, 2018",
        "recorded": "August 2018",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Deep Learning"
        ]
    },
    "url": "http://videolectures.net/DLRLsummerschool2018_levine_policy_search/",
    "segmentation": [
        [
            "Thank you for the introduction Joel.",
            "Alright, so I'm going to start this lecture with a little kind of thought."
        ],
        [
            "Exercise.",
            "Let's imagine that you're playing a sport.",
            "Maybe if you're.",
            "If you like baseball, and if you're playing baseball, but any sport involves having to catch a ball, so there's a someone throws a ball to you.",
            "You have to run.",
            "You have to go and catch it.",
            "And now we're going to think about we're going to think about how you actually do this.",
            "Kind of going through your mind as you try to perform this task.",
            "We can ask some, you know, distinguished scientists what they think goes through your head.",
            "When you're doing this.",
            "So, for example, we can ask for sure.",
            "Dawkins, he is a paragraph in his book that discusses how he thinks this works.",
            "He says when a man throws a ball high in the air and catches it again, he behaves.",
            "If he had solved a set of differential equations in predicting the trajectory of the ball, and at some subconscious level something functionally equivalent to the mathematical calculations is going on.",
            "So this seems like a reasonable hypothesis about how this task works.",
            "You kind of understand something about the world you understand, maybe something about the force of gravity and air resistance, and so on.",
            "You're going to do this kind of mental calculation, estimate what's going to happen.",
            "It's going to land over there.",
            "You run over there and catch it.",
            "But in science we deal with hypothesis that are testable and we can conduct some experiments and we can say, well, is this the case or is it not and ensure it just so happens that we don't have to do that experiment ourselves, so why should did it for us in the 1990s?",
            "There was a.",
            "Nice paper that was published called.",
            "Do feel there's nowhere to go to catch the ball or only how to get there.",
            "Sounds a little bit strange, but what this really means is that when an actual person goes to catch a ball, they actually follow a very simple heuristic that doesn't require understanding anything about the force of gravity or air resistance, or the forces acting on the ball.",
            "It simply requires blindly following a simple rule, and the rule is the following.",
            "You look at the ball so it's a dark spot against a bright colored Sky.",
            "You remember where it is in your field of view?",
            "And then you start running and you modulate your running speed so the location of the ball in your field of view stays the same, which means that the angle between the ground and the Bolt remains constant.",
            "And if you follow this very simple heuristic, you can actually prove with a very simple kind of similar triangles argument that you will arrive at exactly the point where the ball lands and you will catch it.",
            "So the point is that you can actually for this task.",
            "You can acquire the skill of catching the ball very effective, very robust skill without necessarily going through the process of modeling the complexity of the physical phenomena.",
            "They give rise to the system that you're trying to interact with.",
            "So the skill is a policy we learn about policies in the previous lecture.",
            "It's a mapping from your observations to the actions that you should take in order to complete the task.",
            "So in today's lecture I'm going to talk about policy search.",
            "I'm going to talk about how we can find policies I'm going."
        ],
        [
            "Starts by discussing the problem set up, which will be basically the building on the problem set up from the previous lecture from Richard Lecture.",
            "I'll talk about model free policy, search algorithms, policy search algorithms that can learn to catch the ball without understanding all the nuances of air resistance and gravity.",
            "And then what I'm going to do is, I'm going to turn things around a little bit, and I'm going to explore the other facet of this.",
            "I'm going to say, well, what if Richard Dawkins is right?",
            "What if maybe not for that particular schedule for some other skill, we do actually want to model how the world works, and then use those models to understand how to how to act, how to discover policies.",
            "So I'll talk about how we can find policies if we know the model of the system, and then if we don't know the model but are willing to learn that model.",
            "So I'm going to discuss model based reinforcement learning and how it relates to policy search as well.",
            "So."
        ],
        [
            "So to begin with, let's set up the problem and this is going to look very similar to what you saw in the previous lecture.",
            "But I'm going to build up on it a little bit to build up a little more formalism for the policy gradients discussion.",
            "So let's start off with."
        ],
        [
            "A bit of terminology and I'm going to use as a running example, especially for the policy grading portion of the lecture.",
            "Kind of a deep reinforcement learning example where your goal is to learn a policy represented with some very expressive function class like a deep neural network.",
            "So we'll start by kind of building off of what many of you probably already know about supervised learning.",
            "So if you imagine you know, forget about RL for second, just imagine a standard supervised learning setting.",
            "Maybe you're doing image recognition.",
            "Very standard thing.",
            "You're recognizing that, say images and image net.",
            "Your goal is to take in the pixels of the image and to put a label.",
            "And now we're going to attach some symbols to these things, and the symbols are going to be a little unusual for supervised learning, but they're the ones that we're going to use for RLS, so we'll say that our input to our model is an observation which will denote with letter O, so.",
            "In the case of image net classification, observation consists of the pixels in the image.",
            "Your output we're going to notice a, so if you're doing image classification, it's a label in RL is going to be an action, but we don't really care so much, it's just something that you're going to output and the thing that you're learning this thing, called the policy, defines a distribution over the output.",
            "Given the input distribution over a given.",
            "Oh so in the same way that your classifier for classifying image net defines the distribution over labels given the image, your policy is going to find distribution over actions given the observation.",
            "So we're going to use the letter \u03c0 to denote this distribution.",
            "And very often you'll see that I'll write pie with a subscript Theta.",
            "Theta denotes the parameters of the policy, so if you're sort of represented, for example by big deep neural network, Theta is literally just the weights in that network.",
            "So those are the things that you actually want to learn.",
            "So this is the supervised learning setup.",
            "If we want to go and turn it into the reinforcement learning setup, we need to make a few changes.",
            "So the first change that we want might want to make is we might want to introduce the notion of time, so we'll just put this subscripti on everything to indicate the fact that you receive an observation.",
            "At a time step T and you output your action at a time step T and of course, in most interesting reinforcement learning setups, although not all of them, the action will affect the next observation you will see.",
            "So if you see this Tiger in your image and you successfully recognize that it's a Tiger, maybe the next observation will be something you're happy with, and if you fail to recognize the Tiger, then the next observation might be something that you're a little more concerned about.",
            "And of course, you can change the days if it doesn't have to be the label.",
            "It can be some action that you can actually take so you can choose to do different things when you see this Tiger, you can you know, run away, ignore Petit, whatever, and that will lead to different consequences.",
            "And of course the very same framework can be extended if you want to do this with continuous actions rather than discrete action, so you can just as well say well which way are you going to run.",
            "Maybe it's a continuous value that you're going to output, and that's all falls within this framework, so let's nail down the notation more concretely.",
            "We have our observations.",
            "Oh, so these are the things that your policy gets to see if your policy is controlling a robot.",
            "For example, those are literally the readings from the robot sensors.",
            "We have the action A.",
            "That's the thing that you want to output.",
            "So your the goal of this policy is to produce good days and we'll define good in a second.",
            "Another thing that you're learning is this policy.",
            "The distribution a given oh.",
            "Now I didn't mention something yet which actually came up in the previous lecture, which is very, very important, which is one more object that we really need in order to nail down the problem definition.",
            "That's the state you'll notice that the state didn't actually come up yet.",
            "In the most general case of policy search, we'd like to learn policies that operate on observations.",
            "Now, what's the difference difference between States and observations?",
            "And we can, by the way, also as a special case, train policies that depend on states.",
            "Let's discuss this distinction a little bit.",
            "If I show you this picture, this is a picture of a cheetah chasing a gazelle.",
            "This picture the way we represent in a computer is a big matrix of numbers.",
            "The numbers themselves don't really tell us anything about cheetah or gazelle apriori.",
            "There's you know if we actually interpret that image will realize that it contains a sheet in Excel, but the numbers just represent the pixels in the image.",
            "However, underlying these pixels, there's some kind of physical system, and that physical system has a state.",
            "The image pixels are not the physical system, that's just how we observe the physical system.",
            "The physical system has its own state.",
            "We can represent that state of different levels of abstraction.",
            "You know if you're if you're quantum physicists, maybe you care about you know all the uncertainty about the particles in the sheet in the gazelle.",
            "If you're operating at a higher level, maybe you care more about maybe the position of the sheet and the position of the gazelle.",
            "The state is a concise summary of the current configuration of the world.",
            "Formally, the state is everything that you need in order to predict the future about the world, and I'll define that on the next slide.",
            "But to make it clear what's the difference between state and observation?",
            "There is a state underlying this image.",
            "The position maybe of the cheetah MZL.",
            "The observation reflects the state.",
            "It's not necessary sufficient to fully reduce the state.",
            "So if, for example, an automobile drives in front of the cheetah, the pixels in the image of changed, perhaps you can't even see where the sheet is anymore.",
            "But it's still the same place.",
            "It hasn't vanished.",
            "So maybe the state hasn't really changed, but the observation has.",
            "So the observation is not always sufficient to do this state.",
            "Before we get into the more formal, I'm going to have a slightly less silly example.",
            "I'm going to say well, instead of chasing after Tigers and cheetahs, let's say that we're doing this little autonomous driving task just just for the sake of having a working example.",
            "So our observations will be images from a dashboard camera, and our actions are the way that you're going to turn the steering wheel so one scalar value for negative values for turning left and positive values for turning right."
        ],
        [
            "Now if we want to formally define the relationship between observations and states, we can draw a graphical model that we're both represents those relationships.",
            "So in this graphical model we have edges that relate observations to actions.",
            "That's our policy or policy gives us a given oh.",
            "We have edges that relate States and actions to next states.",
            "These are sometimes referred to as the dynamics or the transition function.",
            "And then we have this edge that goes from state observation.",
            "So this is are sometimes referred to as our observation function.",
            "So the thing to notice about the state and the main thing that distinguishes states from observations.",
            "Is that if you observe the state St. Then state S T -- 1 is conditionally independent of state S T + 1.",
            "What this implies is that if you want to make some kind of decision about what you're going to do in the future or in the present, and you know the state St. Knowing State S T -- 1 or S T -- 2 isn't going to tell you anything that you don't already have, so the future is conditionally independent of the past given the present, which means that if you're making decisions, you only need to know the present.",
            "That is true if we're talking about state.",
            "If we're talking about observations that is no longer the case.",
            "So if you remember the example with the car driving in front of the sheet on the previous slide, there maybe when you observe that car you don't know where the cheetah is.",
            "But if a second ago the car wasn't there yet, then that observation from a second collection tells you something that you wouldn't have known just from looking at the present image.",
            "So if you're just dealing with observations then past observations can actually tell you something that you don't already know.",
            "That can be helpful for making decisions, but if you're dealing with state then the present state.",
            "Tells you everything you need to know to make those decisions.",
            "So this conditional independence property is referred to as the Markov property.",
            "The Markov property says that condition on the present, the future is conditionally independent past.",
            "OK, so then we have our policy which in the most general case depends on observations and in some special cases we can also talk about policies that depend on state.",
            "OK, so now we've we've defined the variables that are play here.",
            "Can we think about some simple ways to learn policy's?",
            "There are some very simple ways to learn policies.",
            "For example, you can actually learn policies with supervised learning.",
            "That's called imitation learning.",
            "So if you want to learn policies with supervised learning, you do exactly the most obvious thing.",
            "For example, if you want."
        ],
        [
            "This driving thing get a person to drive the car, record the images of the car, sees record the actions of the person executed, put them in a big bucket of training data.",
            "Run supervised learning on that data and you will get a policy or whether it's a good policy or not is a very complicated question, and there's quite a bit of theory about when imitation learning works like this, and when it doesn't, and what you can do to fix it, I won't be discussing that in today's lecture because today's lecture is going for some learning, but do keep in mind that yes, supervised learning can be used to learn policies.",
            "OK, so in reinforcement learning we need to introduce one more object.",
            "That's going to tell us whether a given policy is good or bad, and we saw that in the previous lecture, and that's the reward function."
        ],
        [
            "So very simple.",
            "It's going to allow us to figure out whether particular actions or particular states are good or bad is a function of States and actions.",
            "It's a scalar value function in general to final States and actions.",
            "Sometimes you define it only on states, but in the most general case States and actions it tells us which States and actions are better than others, and crucially, the reward function is not something that we're trying to optimize greedily, so we're not going to take actions that simply give us the best reward.",
            "Right now, we're going to take the actions that give us the best total reward.",
            "Over our entire existence.",
            "So in the in the car driving example, maybe driving happily on the road at the desired speed gives you high reward.",
            "Being in a car accident gives you low reward, but of course if you want to avoid those low reward situations, you just take the actions now that will prevent getting to those bad states in the future.",
            "OK, so together the state actions, the reward and the transition dynamics.",
            "PFS Prime given essay define a Markov decision process if you also have observations, you can define something called partially observed Markov decision process, so I'll get into that in a second."
        ],
        [
            "So let's be concrete about these definitions.",
            "Before we define the Markov decision process, let me just as a brief aside, define the Markov chain.",
            "So who's this Markov guy anyway?",
            "Andrei Markov, he was a Russian mathematician at the turn of the 20th century, and he didn't actually invent Markov decision processes, but he did do a lot of work on something called Markov chains, which you can think of as a simplified setting that doesn't deal with decision making.",
            "So a Markov chain is just defined with two objects, a state space Anna transition operator.",
            "So the state space that we've already discussed, that's the configuration of the world and states can be discrete or continuous.",
            "So in the discrete case, you can just think of it as a set of things.",
            "The transition operator tells you what's your probability of going from one state to another.",
            "So it's just Pfc plus one given St, and the reason that we call it an operator is because you can write it as a linear operation.",
            "So if you have a vector of probabilities denoting your probability of being each state, then so that's the vector of probabilities muti, then you can express vector of probabilities being the next state as a matrix vector product.",
            "So you construct this matrix TJ which tells you the probability of going to state I from state J and then muti plus one is just given by T times.",
            "Muti so the graphical model for Markov chain.",
            "Very simple.",
            "It's just a chain of States and these states obey the Markov property, which means that in this picture S3 is independent of S1 given us too.",
            "OK.",
            "So."
        ],
        [
            "Now let's go to Markov decision processes.",
            "It's actually, you know, a little bit of debate about where and when they were exactly introduced, but to my knowledge, the first writing on the topic was, you know, appeared in some work by Richard Bellman in the middle of the 20th century.",
            "And the Markov decision process basically turns a Markov chain into a system that you can actually control, so it adds two things, an action space and a reward function.",
            "So we still have our state space from before we have an actual space, which is a set of actions which again can be discrete or continuous.",
            "The graphical model now includes this action.",
            "And the transition operator is conditioned on the action.",
            "The transition operators now tensor, so you have one one thing on the output that depends on two things, but you can still do all the linear stuff.",
            "I'm not going to go into that here, but you could if you wanted to.",
            "And you also have this reward."
        ],
        [
            "Auction the reward functions of scalar valued function of the state and the action.",
            "So it's a mapping from States and actions to real valued numbers.",
            "And now tomorrow precision process you can actually start talking about making decisions.",
            "Especially optimal decisions.",
            "The kinds of decisions that will maximize your reward."
        ],
        [
            "And then the last thing last, the mathematical object I'm going to find here is a partially observed Markov decision process, so this is where we're actually going to bring in those observations.",
            "So we can take a Markov decision process an introduce observations which will turn into a partially observed Markov decision process, and that adds two things.",
            "It has an observation space.",
            "Oh, and an observation function E. Sometimes referred to as an admission function.",
            "So you have a state space and action space like before, and now you have this observation space just like the state space and actually space.",
            "The observation space can be continuous or discrete.",
            "In our driving example from before, the observations might have been images and the state might have been maybe the position of the car on the road, maybe also the positions of the other cars.",
            "So the graphical model now includes these observations like this.",
            "And you still have a transition operator.",
            "It's the same transition operator that we had before for MPs, but you also have an admission probability.",
            "The admission probability tells you the probability of observing some observation OT given a state St. And this can also be stochastic, so there might be some kind of nuisance variables in the world that will affect what you see.",
            "Maybe there's some static in your camera, maybe sometimes something obscured the image.",
            "So in general the observations are also stochastic.",
            "And of course you have a reward function.",
            "In the conventional definition of partial observed Markov scission processes, the reward function actually depends on the state in the action, not on the observation.",
            "But you get only observe the observations.",
            "OK."
        ],
        [
            "So now we have these objects to work with and we can try to nail down an objective, a goal for reinforcement learning.",
            "So I alluded to this before when I said that you have this reward, you want to optimize the reward you want to maximize it.",
            "Not really, not for the immediate decision, but overall time.",
            "So let's try to write that down.",
            "Let's try to write down an equation for this reinforcement learning objective.",
            "So we have our policy as a working example.",
            "Let's pretend it's some kind of parametric function, maybe a deep neural network.",
            "We'll come back to the partially observed case later.",
            "I want to find this first for the fully observed case for the partial observed case, the objective is going to be the same, it's just some of the algorithms will change.",
            "Theta denotes the parameters of our policy, like the weights in the neural network.",
            "The policy takes in either the state or the observation.",
            "Produces actions, perhaps the cast eccle.",
            "The world takes the state in the action and produces a next state according to some unknown probability.",
            "So this is the the transition operator, the dynamics, whatever you want to call us and that defines distributions over trajectories.",
            "So when I use the word trajectory, what I mean is a sequence of States and actions.",
            "I'm dealing with a finite horizon setting.",
            "Here.",
            "You can extend this to the case of infinitely long trajectory as well, but just to make it easier to write down, let's deal with a finite horizon setting where you know that you're going to have to act 4.",
            "Capital T steps.",
            "So maybe you're driving to work.",
            "You know you're going to drive for one hour.",
            "Let's try to maximize reward for that one hour.",
            "But capital T can also be Infinity.",
            "So trajectory distribution is a probability distribution over sequence of States and actions, and because of how we've defined these Markov decision processes, we can factorize that distribution into product of conditionals.",
            "We can say that it's the probability of being some initial state S one times a big product over the probability at every time step of taking some action 80 and then transitioning to some next state S T + 1.",
            "So this is just using the chain rule from that Bayes net that we had on the previous slide.",
            "And once we've defined this trajectory distribution, now we can define an objective that we want to optimize.",
            "So the structure distribution for shorthand.",
            "If you see me right P Theta of touts, how just means the sequence of essays?",
            "And the objective we can define as trying to find the Theta Theta star that maximizes the expectation under the trajectory distribution of the total reward of that trajectory.",
            "So it's an expected value of a sum of rewards for the entire horizon, and the expectation is taken under the trajectory distribution that is induced by the product of those conditionals.",
            "And the only one of those conditions that depends on Theta forces pie.",
            "So as you change Theta Pi, Theta 80 given SD term changes, which in turn changes your future distribution, which in turn changes your total expected reward.",
            "So our goal is to maximize that thing at the bottom."
        ],
        [
            "Now I mentioned that was for the Fire Horizon case.",
            "We can define an objective for the Infinite Horizon case.",
            "Also we can introduce something called the discount factor if we want to have a sum that doesn't run off to Infinity.",
            "I'm not going to talk about discount factors in this lecture, but you'll see plenty of that in the other lectures.",
            "In the Infinite Horizon case, typically the way that you would define objective is that you would define it in expectation under the stationary distribution of the Markov chain induced by your policy.",
            "We won't talk too much about the enterprise in case, but just so you know you can define it and it's.",
            "Straightforward in terms of stationary distributions, but will deal mainly with the final horizon case for now."
        ],
        [
            "OK, so that's the problem set up.",
            "We have the mathematical objects are going to be dealing with MDP's and partially observed MPs.",
            "We have an objective function which is the expectation of the reward and now we need some algorithms.",
            "We need to figure out some algorithms that were going to be able to use to actually maximize that expected reward."
        ],
        [
            "Now before we, we can maximize our objective.",
            "Let's at least try to figure out how we can evaluate our objective.",
            "Does anybody have any ideas about how we can evaluate the expectation of the reward function for particular policy?",
            "Sampling, yeah, so you have an expectation of some function under some distribution.",
            "It's maybe really high dimensional, really simple default choice if you want to value in expectation like that is to do Monte Carlo estimation to sample an average together samples.",
            "So let's do that.",
            "Let's say that we have some kind of objective function will denoted J of Theta.",
            "To emphasize, that's a function of Theta.",
            "Its expectation under the structure distribution.",
            "And we can estimate it by generating some samples.",
            "And averaging those samples together so.",
            "How do we generate samples?",
            "Well, generating a sample for a policy basically just amounts to running that policy in the world.",
            "You can literally just run in the real world.",
            "You can let your car drive you know 50 times, see what it does, see which States and actions that sees, evaluate the reward function at those States and actions.",
            "Sum them together in an average over the samples so we have the summation over T inside and on the outside.",
            "You have some oversamples indexed by I and you have to divide by the number of samples yourself so you run your policy, get some trajectory's.",
            "Some of them are good, some are bad, some are somewhere in between.",
            "You average them altogether and you get a number, and that's an unbiased estimate of your objective value.",
            "That's the thing that you're trying to maximize in expectation."
        ],
        [
            "OK, so now we can evaluate our objective.",
            "Now let's think about how we can maximize.",
            "And to start with, we're going to maximize it.",
            "Kind of in the same way that we usually start thinking about maximizing things.",
            "If you're used to supervised learning techniques, very good way to minimize or maximize something.",
            "It's the computer's gradient and follow the gradient.",
            "So we're going to do exactly that.",
            "And then the trick just comes in.",
            "How do you compute the gradient affectively in a way that has little variance?",
            "Hopefully not too much bias and actually so practical algorithm.",
            "So just to throw the equation up there again, here is our objective and for shorthand I'm going to write RF Tau.",
            "We just means the sum of all of the reward for all the States and actions inside Tau.",
            "And.",
            "It's an expectation, so I can write out that expectation is an integral right?",
            "So we have an expectation of some function that's just the integral of the probability of the product of the probability of that variable times its function value.",
            "So the probability of the structure it out times the reward of town.",
            "So now we need this gradient.",
            "And this is where basically the only mathematically sophisticated part of today's lecture is going to come in.",
            "So in computing the gradient of this objective, what are we going to do?",
            "Well, we'll just do a bunch of algebra.",
            "OK, gradient operators are linear, which means that we can put it inside the integral.",
            "So we have this integral of grad Theta Pi Theta Tau times are Tau, and now we're going to introduce a little bit of a.",
            "Convenient identity that we're going to use to make it actually practical to compute the screening.",
            "The convenient entity is very, very simple.",
            "It just comes from the definition of the gradients of the log of some function, so I'm just going to write out the identity and then you'll see how to use it.",
            "So if you have the gradients of log Pi grading of log Pi is equal to 1 / \u03c0 times the gradients of \u03c0, so we can substitute that in there.",
            "We just literally, you know, look up a calculus textbook, substitute in the definition of the gradient.",
            "Algorithm and you get this equality and now notice that high appears on the top and bottom so we can cancel it out.",
            "So Pi of tile times grad log pile of cow is equal to grab pipe tile.",
            "So this is a convenient.",
            "It just comes from the definition of the gradient of a logarithm.",
            "You look it up in a calculus textbook substitute in there, do a little bit of algebra and you get this convenient entity.",
            "And now we can use this convenient identity to actually get a very nice and convenient form for our gradient.",
            "So notice that we have this grab pie.",
            "Which means that we can substitute in the left hand side of our convenient identity and we can convert this integral to the integral of Pi of Tau, grad log pipe.",
            "Tile times are of tile.",
            "And this thing now is again an expectation, right?",
            "Because it's Pi of Tau times something.",
            "So we can write it out as an expectation.",
            "Just like our objective was, an expectation is an expectation, but now it's an expectation of grad log.",
            "PY file times are of tile, so we took our objective, which is an expectation under the structure distribution computed as gradient and gradient itself is also an expectation under that same distribution, just of a slightly different function.",
            "And it turns out that this one is actually something we can evaluate without knowing the dynamics.",
            "The transition operator explicitly."
        ],
        [
            "So let's work through that.",
            "We have an expectation and just like before we're going to estimate that expectation by sampling, so we've agreed before the sampling is a reasonable way to estimate expectations.",
            "We're estimating that expectation under a distribution.",
            "That's the trajectory distribution.",
            "Again, the one that you saw before.",
            "And.",
            "And we want to evaluate that grad log Pi.",
            "Let's actually look at what the logarithm of structure distribution looks like.",
            "You take the logarithm of both sides and you get.",
            "I apologize, there's a little typo on this slide, but when you see log \u03c0 and log P, that's the same thing.",
            "Just pretend those are the same, so we take the logarithm.",
            "Both sides, the logarithm of product is just a sum of logarithms, so that's going to be.",
            "You have log PS1, plus some overall time steps of log \u03c0 plus log P of the transition.",
            "So this thing underlined in green.",
            "That's the thing that we want to take the gradient of.",
            "So there there it is right there, and we're taking the gradient of that thing.",
            "Now when you take a gradient of the sum of a bunch of functions.",
            "Remember that the only things that actually matter are the ones that depend on the parameters with respect to which you're taking the gradient.",
            "So the grading here is with respect to Theta.",
            "It's a gradient of a sum of a bunch of terms, and many of those terms do not depend on Theta, so your initial state distribution doesn't change if you change your policy parameters, the probability of a transition doesn't change if you change your policy parameters, which means that the gradient with respect to Theta of this term is just zero.",
            "And the greater respect to Theta of this term is just zero, which means that to compute the grain of the log probability of trajectory, you do not need to know the probability of the initial state nor the probability of a transition.",
            "So that's actually very convenient.",
            "That's going to allow us to derive a model free reinforcement learning algorithm from this math.",
            "So we substitute that in there and we get grad log.",
            "Pile of Tao is just the sum over all the time steps of the grad log probabilities of the individual."
        ],
        [
            "Actions.",
            "So just to make it clear what this equation is saying, it's saying that the.",
            "The gradient of your objective with respect to Theta can be written out as the expectation of the structure distribution of the sum of the gradients of the log probabilities of the actions that you took times the sum of the rewards that you got.",
            "Now and now we can.",
            "We can do the sampling.",
            "Now we can actually evaluate this expectation by taking samples.",
            "So just like before, we evaluated the objective by taking samples and average them together.",
            "Now we're going to evaluate the gradient by taking samples and averaging together these products of rewards and grad log pies.",
            "So once we compute this gradient, we can improve our policy just by doing gradient descent.",
            "We can take our old Theta and incremented by some step size Alpha times the gradient.",
            "Regular good old fashioned gradient descent.",
            "So the algorithm you can represent a you know visually like this.",
            "It has.",
            "It consists of basically three steps.",
            "Generate some samples by running your policy, estimate your return in gradient by adding up the rewards and adding up the grad log pies, and then improve the policy by taking a gradient sense step.",
            "And this is called the reinforce algorithm.",
            "So if you see reinforce also if you see likelihood ratio policy gradient, those all basically mean the same thing.",
            "It's referring to this thing.",
            "So the algorithm Step 1 sample your trajectory's by running the policy Step 2 compute your gradient step three, increment your parameters by the gradient and that will get you a slightly better policy.",
            "At least that's the idea.",
            "There are a few caveats and we will talk about the caveats shortly."
        ],
        [
            "But before we talk about the caveats, let's let's try to unpack a little bit.",
            "What is that we just did?",
            "So the sum of rewards you know.",
            "Hopefully that kind of makes sense to everybody, but these grad log pies.",
            "Those may be there a little bit mysterious like we have these funny grad log Pi terms.",
            "How does that relate?",
            "For example, the picture we saw before with this nice observation, observation, or state going into the deep net and actions coming out well, grad log pie.",
            "That's the gradient of the log probability of an action under your policy.",
            "When you're doing something like maximum likelihood with supervised learning, you're computing exactly the same grad log pies.",
            "So let's relate this to maximum like little more explicitly in policy gradient, this is the equation that I had before.",
            "In maximum likelihood, like supervised learning, you have actually very similar equation if you work through the math for doing regular supervised learning, the gradients actually just looks like this right?",
            "In supervised learning you're maximizing the probability of all of the observed input output pairs, so the gradients of your objective supervised learning is just the average over all of your data points of the gradients of the log probability of the output given the input.",
            "So in policy gradient with the same exact grad log pies except now they're multiplied by these rewards.",
            "So if you, if you're doing for example deep learning, you implement this thing in Tensorflow or something like that, you use a cross entropy loss that cross entropy loss.",
            "What you're actually asking Tensorflow to do is compute grad log \u03c0.",
            "In fact, I'll show you some pseudocode later on that will make this even more explicit, so this grad log pie.",
            "This is just a loss function.",
            "The gradient of loss function at the top of your function approximator."
        ],
        [
            "OK, so that explains the grad log pie.",
            "Now what about the rest of the album?",
            "Was this algorithm actually doing?",
            "Can we build up some intuition for how this is going to change the policy?"
        ],
        [
            "So I'll use a little bit of shorthand.",
            "I'll write out the sum of grad log Pi a given S, just grab pipe Tao and I'll write out the sum of the Rs as just R cow just to avoid cluttering notation.",
            "But just remember when you see this is really referring to some overtime steps.",
            "And remember that maximum likelihood regular supervised learning.",
            "It's just trying to maximize the probabilities of all of your input output pairs in policy gradients.",
            "It might not be trying to maximize all those probabilities because they're multiplied by the rewards.",
            "So if the reward is negative, then it's actually trying to minimize it.",
            "If there was positive trying to maximize it.",
            "If the reward is gigantic, it's trying to maximize it alot.",
            "If the reward is tiny, is trying to change it only a little bit.",
            "So what that means is that when you sample from your policy, you get these trajectories.",
            "You evaluate their rewards.",
            "Some of them are good, and some of them are bad and some of them are somewhere in between.",
            "What policy grading will try to do, is it will take the good trajectory's which have a big positive multiplier and will try to increase their probability.",
            "So the good trajectory's will have a big positive multiplier for RF Tao, which means that the gradient will try to make their log \u03c0 big, so grad log \u03c0 times a big positive number.",
            "The really bad stuff might have a really big negative multiplier, so those log probabilities are going to be decreased.",
            "And the stuff that's in between?",
            "Maybe it won't change very much.",
            "So policy gradients.",
            "After all that math and all that, algebra is actually doing something very, very simple.",
            "It's just saying make the good stuff more likely if you did something and you got a big reward, increase the probability of those actions.",
            "Make the bad stuff less likely if you did something you got a really bad reward.",
            "Decrease the probability of those actions.",
            "Don't do that again.",
            "So it's basically just a formalization of trial and error.",
            "So after we did all that algebra, derived gradients and so on, all we really get is trial and error.",
            "So that kind of makes makes sense."
        ],
        [
            "Now, as a quick aside, I talked a lot about partial observe ability and Palm DP's in the beginning.",
            "How does partial observe ability fit into all this?",
            "Well, the answer is that actually fits into all this very very easily.",
            "Nowhere in the derivation did I actually assume anything about the Markov property.",
            "So if you have a partially observed system and you want to use policy gradients with this kind of Monte Carlo estimator.",
            "Just go for it.",
            "There's actually nothing else you have to do.",
            "So you can just as well substitute O in place of S and everything will be fine.",
            "So the Markov property is not actually used and you can use policy grade impartial observer entities without any modification whatsoever.",
            "There's a little caveat here.",
            "In practice.",
            "You might want to use value functions together with policy gradient.",
            "So in the previous lecture you learned about value functions into functions, value function.",
            "Q functions can be combined with policy gradients to create work called actor critic algorithms.",
            "I won't cover it with today's lecture because I'm going to focus on policy policy, grading policy search, but if you do end up using a policy gradient algorithm, do keep in mind that even though the policy gradient does not require Markovian states, your value function might.",
            "So as a caveat, this definitely holds true when you're doing Monte Carlo returns.",
            "If you're estimating a value function, that value function might require Markovian State."
        ],
        [
            "OK, so so far we've painted kind of this rosy picture that you can just take your your MVP sample from it.",
            "Estimate returns us to make gradients with by taking samples and all that good stuff and get something really simple, and it seems to kind of make sense.",
            "So then of course we might ask, well, does it work?",
            "I think if you go home and you implement the algorithm that I just described so far, you will find that it does not work.",
            "You'll probably find that it doesn't work at all, probably not on anything now, probably not even on grid worlds.",
            "And that's because policy gradients require a little more care to use properly.",
            "There are a number of things wrong with the stuff that I described so far, but in in just a few words that can be summarized as the problem of variance.",
            "So the policy gradient estimator described so far is very high variance.",
            "What does that mean?",
            "That means that if you generate some number of samples, let's say generate 10 samples, decimal policy gradient, I estimate my gradients.",
            "I write it down and then I generate 10 more samples and I separately use those to estimate a different gradient and I compare these two things are going to be different.",
            "So you generate the, you know, a small number of samples.",
            "Take your gradients, generate some more samples to estimate a separated from that, and you do this a few times and you'll find there's a lot of variance between these estimates.",
            "Of course there results in very noisy gradients, which means that instead of going right uphill to maximize your order, kind of going on this really noisy path and maybe you're going to miss your optimum.",
            "So explaining why exactly policy great gradients have five variance is a little bit new involved, but I'll just give you an example of just one instance of a problem to policy grading has this is not the only explanation.",
            "There are many, many others, but this is something that makes sense to me and hopefully it will make sense to the rest of you as well.",
            "So I'm going to show you this plot where the vertical axis is the total reward of a trajectory and the horizontal axis represents the trajectory.",
            "So pretend that for some reason our trajectory's are 1 dimensional.",
            "I can project them on a number line just for visualization.",
            "So my policy you can do it as as kind of a function on this plot, so the trajectory is here at the peak of this little Gaussian shaped curve, the highest probability in this lower probability.",
            "So I can kind of visualize my policy like this.",
            "It's a distribution over structures.",
            "And let's say that I generated three samples from that policy and the height of these samples represents the reward, so that sample on the left has very negative reward and the two samples on the right have slightly positive reward.",
            "So I'm just visualizing three samples.",
            "And now imagine that I'm going to use those samples to estimate a policy gradient and modify my policy based on that policy gradient, so that really negative sample is going to push the distribution away from it.",
            "It wants to just reduce its probability, doesn't care which way you go, just so long as you go away and those two positive samples are going to slightly try to increase the probability at those locations.",
            "So maybe if I modify my policy by using those samples, it'll become this dotted line.",
            "So we'll try to put more mass on those positive samples and a lot less mass on that negative sample.",
            "But negative samples strongly repulsing the policy.",
            "And now what I'm going to do is I'm going to change my problem in a way that should not in any reasonable world affect the solution.",
            "I'm going to add a constant to my reward function.",
            "So I'm going to raise the floor.",
            "I'm going to add a constant everybody this can't change the solution because the optimum is still the same.",
            "If you add a constant function, its maximum remains in the same spot.",
            "But now that I've added a constant, all these rewards.",
            "Now, if you imagine what the policy rate is doing is going to increase the probabilities of all these samples is going to be increasing the probability on the right bit more, but it's trying to increase all of them.",
            "So now maybe if I modify the policy based on the gradient from these samples, maybe will try to spread out to cover those samples more.",
            "If I had a really giant constant, if I had a constant that is so big that it dwarfs the difference between the samples, then it will just look a lot like maximum likelihood.",
            "We'll just try to cover everything.",
            "So this is the problem.",
            "5 areas.",
            "You can imagine an even worse scenario.",
            "What if the good samples just happen to have a reward of 0?",
            "Now they'll be completely ignored in the gradient and all the training will do is run away from the negative sample.",
            "So the direction that the policy will go depends entirely on where it starts.",
            "It starts to the right it will run away to the right of it, starts.",
            "The left will run away to the left.",
            "So seemingly innocuous changes just completely changed my gradient.",
            "I should say in the limit of infinite samples I will get the right answer.",
            "So if I have the luxury to generate all possible samples, I will get the right answer no matter what I do to the reward.",
            "But for fire number of samples I have this problem of variance."
        ],
        [
            "So a lot of the games that we play in order to make policy gradients actually useful involve somehow reducing the variance and the good news is that there are many things that we can do to reduce the variance of policy gradients without actually changing the expectation.",
            "So in expectation you want to get the right answer.",
            "That means that your estimator is unbiased.",
            "If you hear unbiased, that just means in expectation get the right answer.",
            "But we can actually make changes that will keep us unbiased, but actually reduce variance, oftentimes reduce variance quite a lot.",
            "One very simple change we can make the policy gradient.",
            "Is to exploit our knowledge of the fact that our universe has causality.",
            "Causality means that things in the future don't cause things.",
            "In the past.",
            "This is not a property of any particular MDP, is just like a fact about the universe and we can use that fact.",
            "So causality means that what your policy does at time T prime cannot possibly affect the reward at time T if T is less than T prime.",
            "Write something I do in the future is not going to change my reward in the past and we can use this.",
            "We can use this fact to modify the policy gradient estimator.",
            "Here's how if you look at this product in the policy grade and you'll notice there's a product of a sum of grad log.",
            "Pi is the gradients of log probabilities of actions times a sum of rewards.",
            "So if I change the probability of an action at time step one or sorry at time step 10, that's actually multiplied by their wartime step one.",
            "So somehow rewarded time stuff one affects my actual time Step 10, which I know is a relationship that doesn't hold in our universe because of causality.",
            "So what I can do is I can actually make a very simple change the policy gradient 1st.",
            "I'm going to distribute the sum over grad log pies outside, so that's just the distributive property.",
            "So I can, equivalently, I haven't made any change yet.",
            "I've just applied the distributive property and then I get the policy gradient estimated some overtime of all these gridlock pies times the total reward of the trajectory.",
            "And now from here I can clearly see that the probability of the action at every time step is affected by rewards both in the past and in the future.",
            "So now I can apply my knowledge of the fact that causality holds to remove the dependence on past rewards.",
            "So the dependence on past rewards is removed simply by changing that one tuati right there, so I just said every time I'm going to decide which way to change the probability of an action, I'm only going to look at the rewards from the current time step until the end.",
            "I won't care bout past rewards because I know that changing the probability of action right now is not going to change rewards I've already received because of causality.",
            "And this thing is sometimes referred to, especially in controls, as the reward to go.",
            "The rewards that you have left to go in the future, current and future.",
            "And sometimes it's written as Q hat, and for those of you that are curious about actor critic algorithms, yes, this Q hat is exactly what the critic in the actor critic algorithm would change.",
            "They would estimate the skew had in other ways, not necessarily just by using Monte Carlo returns, but for now we're talking about policy gradients.",
            "So Q had is just a Monte Carlo estimate of the reward from now until the end of time, but not including the past.",
            "Making this change has reduced the variance of our policy gradients for a very very simple reason.",
            "The variance of some estimate depends on the magnitude of a function.",
            "If I take the function, I make it 10 times bigger.",
            "I have bigger variance if I make it smaller, I have smaller variance by removing terms from the sum, I have smaller coefficients, multiply my grad log pies.",
            "Therefore, I'm going to have lower variance.",
            "So in practice, there's basically no reason not to do this if you're implementing policy gradient.",
            "Just just do this.",
            "It helps.",
            "In fact, I can't think of any case where it would hurt.",
            "Just always do this."
        ],
        [
            "There is another thing that you should always do because it helps a lot and to my knowledge also doesn't hurt, although here you have to be a little bit careful because you can make some bad decisions.",
            "So I'm going to describe something called a baseline.",
            "Let's go back to this little intuitive explanation I talked about before I talk about home policy.",
            "Gradients are kind of a formalization of trial and error.",
            "You want to make the good stuff more likely in the bad stuff.",
            "Less likely that all made a lot of sense to us, right?",
            "You you generate these samples.",
            "The good ones get higher probability.",
            "The bad ones get lower probability.",
            "Unfortunately, that story was a little bit of a lie.",
            "It's a little bit of a lie because of that example I gave where you add a constant to the reward.",
            "So of course if you're bad reward is negative, you'll decrease the probability.",
            "What if your worst reward the worst toward you saw in that iteration?",
            "Is actually positive.",
            "You're not going to be necessarily decreasing the probability you might actually increase it.",
            "So adding this constant of the rewards actually changes behavior.",
            "It makes it not look quite so much like trial and error.",
            "So the intuition is that, well, maybe maybe the.",
            "Maybe the trial and error thing is what we actually want.",
            "Maybe we want to not just increase or decrease probabilities based on whether they got big or small reward.",
            "Maybe want to increase or decrease probabilities based on whether the reward was better or worse than average.",
            "So we compute our average reward and we can say well, if you saw something better than average, make it more likely.",
            "If you saw something worse than average, make it less likely that actually seems to make more sense than what policy gradient seems to be doing.",
            "Like somehow it really makes sense.",
            "You should make the stuff that's above average more likely, and the stuff below average less likely, regardless of its absolute value.",
            "But are we actually allowed to do that?",
            "Is that like, is that mathematically permitted like this is still a correct algorithm?",
            "And surprisingly, turns out the answer is yes.",
            "It turns out that if you take a constant and you subtract it from all of your rewards at a particular iteration, you still have an unbiased estimator.",
            "The policy gradient, and we can prove this.",
            "So the way that we prove it is we take that second term.",
            "So you distribute grad log Pi, you get grad log \u03c0 * R minus gridlock \u03a0 times B and you can actually show that the grad log \u03c0 * B.",
            "In expectation is zero, which means if you generate infinite samples, that baseline B makes no difference.",
            "But of course when you have a finite number of samples is going to reduce your variance.",
            "So here's how we can prove that the expectation of grad log \u03a0 times B is 0.",
            "Just like before, we're going to write out from the definition of expectation we're going to write out the integral, so it's an integral of Pi of Tau times grad log pile of tile times B.",
            "And we're going to go back to our convenient identity.",
            "This is the same convenient if we had before, except now we're going to play it in reverse.",
            "So before we use that identity, go from grad \u03c0 to \u03a0 times grad, log \u03c0, and now we're going to go backwards, we're going to go from Pi gridlock pie back to grad pie.",
            "So we apply our data in reverse.",
            "So now we have an integral of grad \u03c0 * B.",
            "By linearity we can take the and the gradient operator outside of the integral.",
            "So now we have B times the gradient of the integral of Pi of top.",
            "So probability distributions have very convenient property that if you integrate a probability distribution over the domain, you get one right because probability distributions have to be normalized so that integral it's actually one.",
            "And of course the gradient of 1.",
            "Is 0.",
            "So that's equal to B times the gradient of 1, which is equal to 0.",
            "So that means that in expectation.",
            "Any value of be.",
            "Is legitimate.",
            "You can pick any value of B.",
            "You want an expectation.",
            "You will still get the same answer.",
            "Of course, for a finite number of samples you will not get the same answer if a finite number of samples choosing be carefully can actually reduce your variance.",
            "So subtracting a baseline is unbiased and expectation.",
            "Average reward is a very good choice of baseline to use.",
            "It is not the optimal choice of baseline.",
            "You can figure out what is an optimal baseline by writing down the equation for the variance of an estimator.",
            "You can take the derivative of that equation with respect to be set that derivative zero and solve for B and you can actually find out what is the optimal baseline.",
            "The optimal baseline is actually different for every coordinate of your gradient.",
            "But average reward, while it's not the best baseline, is not the optimal baseline.",
            "It's actually a very good choice in practice.",
            "So if you're implementing policy gradient and you just want to code something up that works fairly quickly, average reward is a good choice.",
            "But there are better choices.",
            "OK, so if you have.",
            "An average toward baseline and you use the fact that causality holds in our universe to write out that reward to go thingy.",
            "You could actually get a previous unexpected implementation of policy gradient that will work on many problems, yes.",
            "I.",
            "The estimate the running average of your reward.",
            "Oh, I see.",
            "Oh, of course yeah.",
            "So any choice of B is is going to be unbiased here, so you can estimate be using samples from your current iteration.",
            "You can estimate be using samples from previous iterations any any way of estimating quantities?",
            "Find a very very simple default implementation.",
            "If you're doing kind of a batch mode policy gradient is to just average to go the rolls on that iteration.",
            "We can do many other things.",
            "In fact, one thing that I won't go into here, but that is worth keeping in mind, is that you can also make the baseline depend on state, so you can actually have a function of state.",
            "And that can give you an even lower variance.",
            "You can actually fit a function approximator to rewards as a function of state.",
            "If you make it a function of action, then things get really complicated.",
            "Then it's much harder to make it unbiased."
        ],
        [
            "OK, so now that I've introduced these pieces.",
            "We can actually try to put them together into something that looks like an algorithm that we can actually implement, and one thing that I think is maybe ills have to do at this point is to show how we can go from an implementation of a maximum likelihood.",
            "In this case.",
            "This is in tensor flow to an implementation of policy gradient, so for those of you who aren't familiar with tensor flow, I will walk through this.",
            "This is literally 4 lines of code we're going to.",
            "This is just setting up regular maximum like the regular supervised learning.",
            "I'm going to turn into policy gradient 2nd.",
            "So here I assume that I'm given two things, a tensor of actions and intensive states.",
            "So the tensor of actions is just going to be is actually just a matrix that has 2 dimensions.",
            "The first dimension is the number of samples times the number of time steps.",
            "So just I just took all of my trajectories and just concatenate into one really really big row and the second dimension is dimensionality of the actions, so N is the number of samples that is the length of my horizon D as the dimensionality action so actions.",
            "Is just a matrix, first dimension is N * T, second dimension is DA.",
            "States.",
            "Is a tensor of end time Steve ideas?",
            "Dimensionality of my state equivalent?",
            "This could be observations.",
            "It would make no difference.",
            "So if you want to implement maximum likelihood in Tensorflow, the first thing that you would do is you would ask your policy to actually make predictions for those states so that first line gives you the log probabilities from your function approximator.",
            "So policy some kind of neural net in this case, or some other kind of parameterized function, and you can call.",
            "This is just a made up thing dot predictions that basically says do a forward pass feed in the States and give me the outputs and the outputs.",
            "Here are going to be log probabilities of different.",
            "Labels.",
            "Then the second line.",
            "This negative likelihood is basically evaluates a cross entropy loss that basically amounts to saying, well, give me the probability of the right of the label.",
            "Given the what my network output is, so the network outputs a log probability for every possible label, that second line evaluates the probability the log probability of the actual actions and the actions tensor.",
            "So that's a correct cross entropy loss in Tensorflow.",
            "The third line just says average them together so it says OK, Now you have a probability for every single N * T. Average them altogether, get one scalar value and the fourth line says compute the gradients.",
            "So if you write this out, assuming that you know policy dot predictions a function you actually implemented, this will give you the gradient of the likelihood.",
            "This is regular supervised learning.",
            "If we want to turn this into policy gradients."
        ],
        [
            "We all we have to do is add this stuff in red so we have one more input that we need to know which is these rewards to go these Q values.",
            "I assume that those have already been precomputed, so that's just the sum of the reward from time step T until the end capital T and that can be expressed as an end times T by one tensor is just a bunch of values for every sample that I have.",
            "And all I'm going to do is after computing those negative likelihoods, I'm going to wait them by the rewards to go.",
            "I'm just going to point wise multiplication.",
            "And then when I do the reduced mean, I just plug in these weighted likelihood instead of the original ones.",
            "Now of course you have to actually compute those Q values, which is where you would put in the baseline and all that causality stuff and so on.",
            "But assuming they have computed the Q values, the only thing that you have to do in order to implement policy gradient is just add that one line and use the weighted likelihoods.",
            "That will give you a policy gradient implementation.",
            "So even though we have went through a lot of math, actually implementing this is fairly straightforward.",
            "So this is basically directly implementing this equation, so the loss is this Jay~ and when you called gradient on the loss then you get the basically puts the gradient in front of log pie and that gives you a policy gradient estimator.",
            "Does anybody have any questions about the pseudo code?",
            "This is a good time to ask some questions.",
            "Yes, back there.",
            "Sorry, what's the negative likelihood?",
            "So the negative likelihood it's just it's negative because typically use a minimizer rather than the maximizer and all it's doing is evaluating these log pies.",
            "So likelihood is log \u03c0 in this case.",
            "When you do weighted, when you waited, when you do the pointwise multiplication, you're taking each of those log pies and you're multiplying them by the reward to go by the Q hat.",
            "Question.",
            "For the rewards.",
            "Here are.",
            "Oh, you said it be a state dependent yeah so so if the state dependent then the proof has to be modified a little bit.",
            "You have to actually you can do it at the level of towels you have to actually put in the S as in the days.",
            "But if you do it at that level it still falls out of the integration because the probabilities are over a not over S, so that grad log paetow.",
            "It's actually a sum of log Pi a given S. So they only only ever have probabilities over a not over S in there.",
            "So if you actually put in the SNES, then you can repeat the same proof even when the baseline depends on state.",
            "Any other questions here?",
            "Yes.",
            "Yes, the result for that is a little bit more involved, but I have a citation in a few slides that you can take a look at if you want to see the proof of that.",
            "Yes, back there.",
            "Yes, I'm assuming that whatever baseline, whatever thing you're doing is already been done.",
            "That's right, yes.",
            "So this is.",
            "This is actually an important point.",
            "Is that if whatever you did to compute the you know any of these things actually involved the parameters their policy for some reason.",
            "Make sure that you're not actually back popping through it so you know if you want like a very simple tip about how to implement this, you can just.",
            "If you're doing this in Python, do all of your reward stuff in, like NUM py, it's just simple arithmetic and then just use the audited package for actually performing this stuff.",
            "So if you do all that you know just outside of your audit package, you don't need all the different about all those things.",
            "Then everything will be fine.",
            "OK, so that's that's basically the theory.",
            "Now are few practical tips."
        ],
        [
            "Unfortunately, the stuff that I described is only part of the story.",
            "It will allow you to implement a policy grain algorithm that works on simple problems, but there are a number of things that you should try to do if you want things that really work on more complicated problems, so policy gradients have high variance.",
            "The tricks that I described will mitigate that variance, but there are other things you can do, so using an actor critic algorithm, we have a critic in addition to the policy.",
            "Basically, a value function estimator can be used to reduce variance even further.",
            "In fact, quite a lot, and that can be really important for practical implementations.",
            "Choosing the step size for policy gradient is very difficult.",
            "It's usually much harder than choosing a step size for SGD.",
            "These kind of vanilla policy gradients, if you see that renewal policy, great, just referring to type of policy.",
            "Gradient described here can be very hard to use.",
            "So there are a few things that you can do to make them easier to use and to make a step size adjustment more automatic.",
            "One of the things that's really useful is to use natural gradients, trust regions, and so on, so those of you that are familiar with natural gradients is basically it's a way to incorporate some higher order information into your gradient estimator.",
            "We typically don't use natural gradients when we train, for example, deep Nets with supervised learning, but for policy gradient, natural gradients can make a big difference, so that essentially amounts of preconditioning your gradient by a matrix called the Fisher Information matrix.",
            "So it's a matrix that describes the local geometry of the probability distribution.",
            "So if you want to find out about this, look up natural policy gradient and you can also look up trust region policy optimization.",
            "These two things are very similar, just one one of them picks a step size, the other one picks a trust region in probability space.",
            "It's very useful to use automated step size adjustment techniques, so if you're used to doing everything with Judy and momentum and you're a real wizard at tuning your SGD learning rates, maybe you might consider using a more automated thing like Adam.",
            "If you're doing policy gradient, it's not required, but it will make your life a bit easier.",
            "Find other ways to reduce your variance so things like critics and actor critic algorithms, fancier baselines, maybe baselines that depend on state.",
            "These things can help.",
            "So use a baseline.",
            "You can also use just a huge batch size.",
            "Bigger batch sizes tend to make things more stable for policy gradient, so policy grading methods in general the way they stuck up to other reinforcement learning techniques, they can be very very stable.",
            "In fact, if set up correctly there can be some of the most stable reinforcement learning algorithms.",
            "They can also be some of the most inefficient ones, so good choice if you have, you know cheap simulator we can generate lots of data, maybe not such a great choice if you want something super efficient but also a good choice if you want something fairly stable provided that you do all the tricks.",
            "Properly.",
            "So."
        ],
        [
            "Here is an example just to just to show you some videos.",
            "Since you've all been listening patiently, patiently to me for about an hour time.",
            "For some animations, these are from a paper on a trust region policy gradient methods, so there's a policy grading method that uses some 2nd order information to impose a trust region, or how much the distribution changes.",
            "So it's a kind of a natural gradient with automatic step size adjustment.",
            "Handles discreet and continuous actions, and there's actual code available for this algorithm, so if you look up the benchmarking paper by dawn at all, so this is some work that we did with John Schulman in 2014, 2015, we, to my knowledge, is actually one of the earlier kind of from the recent incarnation of deep policy grading methods.",
            "We use it for Atari Simulator robotics tasks and so on, so if you want kind of a very reasonable starting point, just something to get started, get download the code for this method, try using it.",
            "It'll probably be a decent starting point."
        ],
        [
            "Some suggested readings, so first some classical papers.",
            "The reinforce algorithm that I described appeared to my knowledge for the first time, although maybe it appeared earlier too.",
            "But to my knowledge, for the first time it appeared in this paper from 1992 by Williams.",
            "The thing that I called causality this sometimes has been called the policy gradient theorem or the G Palm DP estimator appeared in several papers, this one by rich sudden actually and the subsequent paper by Backstrom, Bartlett.",
            "Those are good references if you want to find out about that and the question earlier about the proof for why this is correct, you can find that in those papers.",
            "If you want sort of a more recent paper kind of survey style paper to learn about policy gradients, especially natural policy gradient, I would highly recommend this paper by Peterson Shawl.",
            "It's not actually the 1st paper, introduce it, but it's a it's a good reference if you want something written in accessible style with all the math that you can find out about.",
            "For more recent papers, if you want to find out about deep reinforcement learning policy gradients, you can check out these two papers by John Schulman, the Trust region policy optimization and approximate policy optimization.",
            "There are many more papers on this topic, but these are kind of two fairly decent more recent papers.",
            "They can check out.",
            "And if you want to practice on your own, there is a homework assignment that you can try.",
            "Actually from my graduate class at UC Berkeley.",
            "So if you can go on GitHub, find the homework assignment there and you can try out implementing policy gradient and trying it out on some problems yourself."
        ],
        [
            "So that was the model free policy search story.",
            "Now in the time I have remaining, I want to tell you a little bit about what happens when you know the model.",
            "Basically, going back to the example of Richard Dawkins, if you know the differential equations that govern how the ball is going to fly through the air, and then say a few words about what happens if you don't know the model but are willing to learn it."
        ],
        [
            "So when we talked about reinforcement learning before we had this reinforced learning objective and we said that our objective is the expectation of our total reward under some distribution, the distribution depends on a few things.",
            "It depends of course on your policy, but also depends on your transition dynamics and the probability of the next state given the current state and action."
        ],
        [
            "And in model free reinforcement learning, we basically assume that we don't know this thing and not only do we assume that we don't know it, we also assume that we're kind of not willing to learn it.",
            "Which is OK in some cases, like sometimes it is really easier to figure out how to catch the ball then it is to figure out how gravity and wind resistance act on it, so it's OK to not want to learn that that quantity.",
            "But sometimes maybe you do want to learn it.",
            "Maybe you'll have a more flexible method if you can learn, maybe you can learn that model and use it for many different things and there will be more efficient."
        ],
        [
            "So what if we knew the transition dynamics?",
            "I'm going to talk about what happens if we know them, and then I'm going to talk about how we can learn them.",
            "So if we knew the transition dynamics, which we often do, we can use them to help us acquire policies and help us make decisions so we know the dynamics in many cases.",
            "For example, if you're playing a game, if you're playing the game of go or chess, or if you're playing a video game, you probably know the transition dynamics.",
            "Some systems are easy to model, so maybe you don't know exactly all the physical physical nuances that govern a car that's on the road, but the kinematics of that system sort of the high level behavior can be modeled pretty easily, and while it might miss some of the nuances, it might be good enough to make meaningful decisions.",
            "And of course, maybe you're dealing with simulated environment like a simulated robot or a video game where there's some piece of code that actually implements the model.",
            "So then you do actually know the model you have access to it.",
            "And oftentimes, when you don't know the model, you can learn it.",
            "So in robotics we have this thing called system identification which deals with a setting where you, basically, you know, the laws of physics.",
            "You know how forces amasses interact, but you might not know the particular properties of your system.",
            "You might not know the particular mass of your robot, but you have a physics textbook.",
            "So that's basically refers to a system identification.",
            "You can learn a model with a black box learning algorithm.",
            "You can take a big neural net and try to fit your model to that.",
            "That's fine too.",
            "So does knowing the dynamics make things easier?",
            "Oftentimes yes it does, but there are a few details that we have to get right as usual.",
            "So."
        ],
        [
            "I'll go through this part pretty quickly because I do want to kind of get to the learning part as well, but one important detail when it comes to using a known dynamics models.",
            "It comes down to whether you have a stochastic or deterministic model and whether you're doing open loop or closed loop planning.",
            "So let me explain this distinction because this distinction is important, I think it's often not not not appreciate it very much."
        ],
        [
            "So if you have a deterministic model, meaning that the world is, you know, is governed by the deterministic equation that you know USD plus one is equal to F of sdat.",
            "You can make decisions as following.",
            "Your agent asks the world which State am I am just tell me just my first state.",
            "Eurasian plans planning means you're going to select a sequence of actions that you believe will give you high reward under your current known dynamics.",
            "The agent sends that plan back to the world that says, here's what I want to do.",
            "This is what I'm going to do at time step one.",
            "This is what I'm going to test it.",
            "I'm stuck to this one.",
            "I'm going to do a time step 300.",
            "This is my plan.",
            "I'm going to commit to that plan and execute it.",
            "So that kind of problem can be solved with.",
            "You can write it out as a constraint optimization problem.",
            "You can write it out as an unconstrained optimization problem basically amounts to choosing a sequence of actions, A one through 80 that maximize your total reward over States and actions subject to the constraint that your states are governed by your dynamics, such as the constraint the plus one is equal to F of sdat.",
            "We sometimes refer to this as open loop control.",
            "Because you're going to commit to a sequence of actions and then execute them in the world without regard to what actually happens.",
            "If your dynamics were correct.",
            "If F, if F really is the true transition function for the world, this is going to work fine.",
            "However, if the world is stochastic or your F is incorrect, this can cause you a lot of problems.",
            "So."
        ],
        [
            "Let's think about the stochastic case in the stochastic case, you don't have just a deterministic function.",
            "If you have some distribution, so you can write down, for example, what's the distribution over states.",
            "You'll see conditional sequence of actions, and it's going to be random.",
            "And you can set up the same kind of optimization that I had in the previous slide.",
            "You can say, well, let's choose a sequence of actions that maximize my total reward in expectation given my known probability of states given given actions.",
            "This is the analogue of planning a stochastic environment.",
            "It's a reasonable thing to do, but now it might actually be a little bit suboptimal, because when you commit to a plan of action, you're not actually responding to the States you actually visit, so state S2, for example, is going to be a random consequences of the state S1.",
            "Any action A1?",
            "Once you actually execute a one, you get to observe as to we get to actually see what happened, and once you see us too.",
            "You might choose a different action A2.",
            "Then you would have chosen if you didn't know what it was.",
            "So open loop planning in the stochastic case can be extremely suboptimal because it basically amounts of saying I don't care what's going to happen next.",
            "Here's my plan of action.",
            "And you construct problems with arbitrarily bad."
        ],
        [
            "So I keep using this word loop.",
            "What does loop mean?",
            "Well when someone says that I have a closed loop controller, what they mean is they have a controller or a policy that's actually looking at the state and responding the state as it happens.",
            "So the world produces state St.",
            "The agent responds with an action 80 and then looks at the next status T + 1.",
            "So it's called closed loop because there's a closed loop between the agent and the world.",
            "When I say have an open loop policy or an open loop controller, that means that I'm not going to pay attention to what's actually happening around me.",
            "I'm just going to plan of action execute that plan.",
            "You can get pretty far with open loop control, but closed loop control can be better, especially for stochastic systems or systems where you haven't identified the dynamics perfectly."
        ],
        [
            "So if you want to do close loop control then that's where policy search comes in.",
            "So even though you could use your known model to commit to a plan of action directly without even having a policy, if you actually optimize the policy, then you can do close loop control so you can actually choose pie and then you can maximize the expected reward under PIE and that will actually model the fact that you get to change your mind about which actually take after you observe.",
            "The state actually resulted from your action.",
            "So This is why when we haven't known model.",
            "If it's the Catholic, or if it's or if it's not fully identified, we might actually choose to do policy search even though we don't have to.",
            "We could just do planning, but the policy search setting will help us by allowing us to do close loop control, and we can choose many different forms for Pi, we can choose, you know, deep neural networks, we can choose something similar like linear feedback policy, so there are a number of different choices here that I won't go through in detail."
        ],
        [
            "So I'll skip the open loop."
        ],
        [
            "Because that's kind of a little bit outside the scope of this lecture."
        ],
        [
            "I want to focus on closed loop control those with control of the model.",
            "So if you if you have a model you know the model and you want to figure out your policy, there are few choices that you have.",
            "So you can use your model to learn a policy.",
            "This means that you're not going to be planning a test time, you're just going to optimize your policy.",
            "Then just executed a test time and it's going to help with control.",
            "So."
        ],
        [
            "One very natural way that we can think about using a model to learn a policy is just do backpropagation.",
            "You know, if you have a model and you you have your policy, you basically have all the building blocks that you need to run a forward pass and evaluate your trajectory, and then you can imagine doing a backward pass through that to computer grading with spectrum policy parameters.",
            "So no more sampling, no more anything else.",
            "It's very easy to do if your dynamics are terministic, you can do it for stochastic dynamics too.",
            "It's a little bit more involved, so I'll only talk about the deterministic case, but for the cast, in case you can do things like relation trick moment matching or sampling, and they can all handle that.",
            "But let's just talk about the deterministic case because a bit simpler to deal with, so you have a computation graph that's defined by your known dynamics F and your policy Pi.",
            "If everything is deterministic, this computation graph is very straightforward.",
            "You can literally implement it in your favorite automatic differentiation software.",
            "And this computation graph consists of basically three objects, the policy Pi, the dynamics F and the reward R. And so it looks a lot like an RNN, right?",
            "It's just that in an RNN you have one transition operation is happening every step you have the DRM function.",
            "Here you have two things.",
            "You have a policy and dynamics F. So every single time step you have a state you ask \u03c0 to give you an action.",
            "Then you have a state, an action which you can use to evaluate the reward.",
            "Those are the blocks at the top and you can give them to your dynamics to get the next state SD plus one.",
            "So this is just the computation graph for that setting.",
            "And now you want to maximize R, so you could just do it by computing derivatives through this whole thing using back propagation.",
            "Just say OK, I want the derivative this whole thing with respect to some of the Rs.",
            "So compute those gradients, push them through the whole graph.",
            "Giant backpropagation through time, get a gradient and use that gradient approval policy.",
            "That's a very reasonable design for an algorithm.",
            "It's easy for deterministic policies.",
            "You can do it for suggested policies too.",
            "It's possible it can be really hard for the same reason that recurrent neural networks are hard.",
            "So part of why we use things like, you know, if you've heard of LCMS, all sorts of optimization tricks and so on.",
            "The train recurrent neural networks because backpropagation through time through along temporal sequence, can become extremely unstable here.",
            "Of course, you don't get to choose the form of FF is the dynamics of your actual dynamical system, so F can be some complicated thing with derivatives.",
            "They behave poorly when you multiply many of them together, so you backpropagation through time.",
            "It's a product of all these Jacobians.",
            "That's a product that scales with the number of time steps, so it can be extremely well behaved.",
            "You can get.",
            "The world's worst banishing or splitting gradients, and for that reason for complicated dynamical systems.",
            "This algorithm will often not work very well.",
            "And there are often better ways to do this.",
            "So."
        ],
        [
            "01 better way of doing it, which is maybe a little bit unsatisfying but worth mentioning is to say well, even though I know the model, I'm just going to use Model 3 RL anyway.",
            "It might seem a little bit silly because you have this additional knowledge in compute gradients for dynamics, but you're just not using them, but in many cases can actually work pretty well.",
            "So you can just use the same policy grading method outline before or another model free method, even though you have a model and then you generate your samples.",
            "But now you generate them in your own model.",
            "And in some cases can actually work well in some cases, substantially better than using the gradients.",
            "So I'm not going to talk about this in much more detail, but if you want an analysis of this, this is a very recent paper that I like a lot because it actually presents a pretty decent analysis of this problem and some tradeoffs for how to solve it.",
            "So this is a paper by permits at all in acnl this year.",
            "There are many other papers that deal with this problem as well, but this is a very recent one that could be a fun reference for those that want to learn more about this issue."
        ],
        [
            "Another thing you can do is you can learn policies with known models or with learned models, also without doing backpropagation through time by decomposing out the problem of selecting actions from the problem of training policy's so, this is basically the idea behind guided policy search.",
            "The idea is that in some cases it's actually easier to optimize trajectories to optimize plans then it is to learn a policy.",
            "So what you can do is you can optimize trajectory's or plans from different initial states.",
            "Use them as training data and user training policy.",
            "So if I have these differential states indicated in orange and I have some goal, maybe they're all trying to reach like Orange Star.",
            "I separately optimized trajectories for them using my favorite planning or optimal control algorithm and then just use that as training data training policy with supervised learning.",
            "And then the policy can be used to act in the world.",
            "There's a little nuanced which is that you can actually change the objective for your planning to say, well, construct plans that are consistent with what my policy wants to do, so you can add a little reward term for your trajectory is to say, well, maximize the reward, but also try to avoid actions that my policy doesn't like.",
            "Try to maximize the probability of the actions under the policy too, and the details of this method are provided in the paper, but it amounts to kind of iterative optimization where you alternate between optimizing your trajectory's to maximize reward and be consistent with your policy.",
            "And then updating your policy with supervised learning.",
            "Mathematically, the sense of being kind of a dual decomposition style algorithm.",
            "Alright."
        ],
        [
            "I'll skip the these parts.",
            "Because what I wanted."
        ],
        [
            "And in the remaining 15 minutes is talk about the case where you want to learn the model.",
            "So we discussed how if you have a model, if you know the dynamics of your system, there are a few things you can do.",
            "You can do open loop planning.",
            "You can learn your policy, you can learn a policy with different techniques like backpropagation through time, guided policy search, just plugging into Model 3 solver.",
            "So now let's talk about the case where you don't know the model, but you'd like to learn it."
        ],
        [
            "So.",
            "If we if we knew the model, we could of course do all these things that I discussed on the previous slides.",
            "So we can construct.",
            "A model based reinforcement learning kind of version 0.5.",
            "A very simple, logical and obvious way to do model based RL.",
            "Which is you have some initial base policy, maybe just a random policy.",
            "You run it, you collect data, so your data consists of transitions, S, A comma, S prime.",
            "You're going to learn a dynamics model on that data.",
            "It could be a terministic or stochastic model.",
            "I'm just going to use the terministic model for simplicity in these illustrative examples, so if you have a terministic model and maybe your states are continuous, you can just fit it by minimizing mean squared error.",
            "The difference between FSA and S prime.",
            "You have a stochastic model, you can maximize log probability.",
            "It's all good.",
            "And then you can use this model to make decisions.",
            "So step three.",
            "Let's just plan through this model, choose actions.",
            "So this is a simple kind of version 0.5 model based reinforcement learning algorithm."
        ],
        [
            "And now we can ask the question, does this algorithm work?",
            "Well, the practical answer is that sometimes yes, so this is essentially how system identification works in classical robotics.",
            "So in classical robotics, what you would do is you would choose your \u03c0 zero to be some random policy.",
            "That sort of that visits many different kind of modes of your physical system.",
            "You would use it to fit the parameters are physical system like the masses and lengths of different links, and then you would use it to control.",
            "So in many cases naive kind of version 0.5 algorithm can work well.",
            "Some care should be taken to design a good base policy, but it can work well, especially if you have some knowledge about your system.",
            "So if you have like your physics textbook, you can basically know how the world works and using it if you open parameters.",
            "Does this."
        ],
        [
            "Other work in general.",
            "In general, the answer is no, and we can illustrate this with a little example.",
            "So.",
            "Here, just reminding.",
            "Here's the algorithm.",
            "Step one is, you're going to run your base policy to collect data.",
            "And let's say that I'm a little, you know, and walking around in this Hill and my goal is to reach the highest point.",
            "So my reward is bigger the higher up I am.",
            "So I start off on the side of the Hill and I'm going to walk around so that Redpath represents my random walk and I'm going to see that while walking, kind of generally towards the right, seems to increase my height so that he'll have to speak on the right here.",
            "So I'm going to say further to the right is higher and therefore better.",
            "So my initial \u03c0 zero does things at random, and the dynamics that I learn tell me if I go to the right.",
            "I'm going to be higher up on the Hill.",
            "So when I fit those dynamics and then use them to make decisions.",
            "I'm going to say, well, going right is better, so I'm going to go to the right.",
            "And if I actually do that, I'm going to fall off the Hill.",
            "Now you might say you might look at us and say, well, OK. Of course that happened because that red that red thing didn't actually visit all the parts of the space, it just was constrained to particular region.",
            "But in large high dimensional state space this problem is actually a very general problems.",
            "It happens all the time.",
            "And mathematically, we can view this as a problem of distributional shift, because what happens when you run your initial policy \u03c0 zero, your random exploration policy?",
            "You observe states from the distribution P, \u03c0 zero.",
            "This is the state distribution of that policy.",
            "When you then plan according to your learning model, you're going to states from a different distribution.",
            "You're going to see states from distribution induced by that planning policy.",
            "If Pi, F and those distributions are not the same, which means that your model which was trained to minimize error on PI0 will not necessarily get low error on Pi F. So this is a distributional shift problem.",
            "And we can address this distributional shift problem simply by adjusting our model to the distribution of the currently have.",
            "So this distribution mismatch actually the trouble with this action becomes exacerbated as we use more and more expressive model classes.",
            "Essentially, you end up with your model overfitting to be \u03c0 zero.",
            "So it's a very unfortunate curse at the more powerful your model is, the more you suffer from this issue, and that's why in robotics when we do things like system identification and were fitting just three or four parameters, just the number of maybe masses in your robot, the symbol method works well.",
            "When you're fitting some big black box model like a neural net, this problem is going to really get you."
        ],
        [
            "So we can do better with a very simple fix.",
            "That's going to make our data distribution equal to our to state distribution of our policy, and the fix is basically exactly what you might expect us.",
            "So model based reinforcement learning version 1.0 run your base policy, collect some data, use the data to figure dynamics model.",
            "User dynamics model to act and then repeat, so execute those actions, collect the resulting data appended to your data set, and keep going.",
            "And if you do this, the intuition basically is that as you keep appending more and more data, eventually your buffer gets populated mainly by data that looks like the data you would see from your policy.",
            "So even if your initial policy \u03c0 zero is totally different as you do this more and more, eventually the larger and larger fraction of your buffer is populated by data that is close to your policy would actually see, and your model becomes good in exactly those regions that your possible visit.",
            "So this kind of the reason I call this 1.0 is because this is kind of the first practical algorithm that can probably implement.",
            "It will probably work decently well.",
            "There's a little detail, which is that you need to collect the data from PDF, and that's what this algorithm accomplishes."
        ],
        [
            "Now there are a few other things you can do that will make this work a lot better.",
            "One of them."
        ],
        [
            "You can do, of course is closed loop control, so if you make mistakes, if you're just planning according to F, those mistakes can add up.",
            "They can basically accumulate overtime if you do include control, you can correct mistakes as soon as they happen so."
        ],
        [
            "You can do better by doing closely control, either with planning.",
            "So if you close the control of planet, sometimes called moderated controller MPC, that means that you re plan every single time step.",
            "Or you can do close the control by learning a policy so both those things can actually be actually making more resistant to having an incorrect model.",
            "Positive control can mitigate the effects of slightly incorrect models."
        ],
        [
            "OK so I will skip through this part.",
            "Now when you learn your model, of course, yeah, you can back property that also.",
            "Just like I discussed before.",
            "And as I mentioned, back up into the policy is often not a great idea and there are better ways to optimize, so you can use your learn model and still run model free RL to learn your policy.",
            "You can take your learn model and use guided policy search and there are other things you can do as well."
        ],
        [
            "What about observations?",
            "So far when I talked about learning models, I only discussed systems when you have access to the full state, you can do model based reinforcement learning.",
            "We have access to observations as well.",
            "Then you end up basically learning this partially observed Markov decision process model.",
            "So you have to learn not just the transitions between the states, but also the observation function.",
            "How the states relate to observations.",
            "In many cases when you're doing model based IRL with observations you don't even know what the states are, so you might not even have access to a definition of the state space.",
            "So then you learn essentially a latent state model.",
            "Here are a few references to some recent papers that learn latent state models that you guys might want to check out.",
            "If you're interested in this topic.",
            "So this is a paper by water at all, called embed to control that learns a living space model based on the type of variational autoencoder and this is a visualization of a model that they learn for a little pendulum task.",
            "So the picture on the left shows of the visualization of their latent space and the pictures on the right show the generations that generated observations from the latent space model.",
            "So in this paper they train this model and then use it to make decisions via a type of planning.",
            "Here's a paper for my group on this.",
            "This is using something called the spatial autoencoder control robot.",
            "Very similar principle.",
            "Train an auto encoder style model to learn a latent space fit dynamics in that latent space, and then use those dynamics to do a model based RL procedure based.",
            "In this case, I'm guided policy search."
        ],
        [
            "So a summary for model based RLI talked about how we can have kind of version 0.5 where you collect some random samples using trainer dynamics and use that to plan.",
            "There's a simple.",
            "There's no iterative procedure, but it can have this distribution mismatch problem.",
            "I talked about a version 1.0 where we iteratively collect data replan and collect more data.",
            "This is simple.",
            "It solves distribution mismatch problem, but if you if you do, you know planning.",
            "Once you do open loop control, this might perform poorly.",
            "So what you can do is either model creative control where you plan every step or train a policy and both those will produce closed loop control which will allow you to get good results even before your model has fully fitted the dynamics accurately.",
            "So if you're interested in doing model based reinforcement learning, you definitely go at least with version 1.0.",
            "And if you want to close the control, maybe something like version 2.0 might be a good choice.",
            "OK so a few suggested readings for some classical papers on model based or Alan.",
            "I kind of used classical in a very loose way because this includes papers from as recently as 2012.",
            "I would recommend checking out these three papers.",
            "The first one by IRA Sutton introduced this thing called Dyna, which is kind of one of the best known frameworks for doing it for learning a model and then using model free algorithms within that model.",
            "So I would highly recommend that the 2nd paper by Mark Dozen Roth described this album called Pilko which is.",
            "Kind of a very good example of an algorithm that sort of back propagates through the model.",
            "They actually used acoustic model and they backpropagated through it using moment matching, but it's good kind of example of an algorithm in that category.",
            "And the third paper Rosson back, now that one deals with this distribution mismatch problem in model based reinforcement learning.",
            "So if you want to find out more about distribution mismatch, I highly recommend the 3rd paper.",
            "Some examples of more recent papers.",
            "These are not necessarily like particularly good papers, just examples of recent methods for model based Sorrell, especially in deep learning.",
            "This is the inventor control paper by water at all that I mentioned this paper by Hess.",
            "It all combines model free and model based learning in a different way by actually back propagate through the model to improve the model free algorithm.",
            "This guided policy search paper this is the this is a paper that actually does model based planning directly on images using video prediction.",
            "And this is a paper on learning neural net models and doing iteratively re planning via MPC.",
            "So if you want to find out version 1.5 algorithm, this is one of the papers to check out.",
            "OK, and you can also practice on your own.",
            "So if you want to do homework assignment model based RL again, this same GitHub links before homework for deals with model based reinforcement learning.",
            "If you wanna check that out.",
            "OK, so at this point I'm going to wrap up because I have about 4 minutes left.",
            "I do want to leave some time for questions, so thank you very much.",
            "I'll be happy to take questions.",
            "Thank you, can you go back to your direction of policy creating sure?",
            "Yes, yes.",
            "So here you have a capital T. Yep, the horizon.",
            "Yeah the horizon and this capital T is actually a random variable, which is which depends on policy \u03c0. Yeah, so let me clarify something about capital T. So for that I think it would be really helpful to bring up the slide with reward to go."
        ],
        [
            "So capital T the horizon of your problem, but it's not actually a function of your policy, it's a property of your problem statement.",
            "So when we define a reinforced learning problem, if it's a finite horizon problem, we basically choose a horizon capital T. So in some cases this makes a lot of sense.",
            "You know if you're playing a game and you have exactly 20 turns to win the game Capital T is 20.",
            "In some cases it doesn't entirely make sense, so in some cases maybe you want to achieve some kind of good behavior and then maintain that behavior for infinite time.",
            "In that case, it becomes a little tricky to, you know, do the thing at the top on the top line.",
            "Here we just some always from one capital T because he might be infinite.",
            "But in practice, what you would do in those cases that you would use what's called a discount factor.",
            "So you would say that rewards further in the future are worthless to me than rewards right now, which means that when you evaluate the reward to go, you will put a coefficient in front of this R, which is gamma to the T -- T prime.",
            "So rewards future further in the future and accounting for less.",
            "And when you do that, then even though you're summing from Tita Capital T. Things too far in the future, especially to get a multiplier of 0.",
            "Because if you have a discount less than one exponentiated by some large exponent, eventually goes to 0 for a big enough exponent.",
            "So in that case you can actually set capital T to Infinity, but not actually compute the whole sum up to Infinity, just computed up with some value.",
            "Once the multiplied it's small enough.",
            "In reality, if you're doing this where you would probably want to do is use an actor critic method and remove this whole issue altogether, in which case you get an estimated looks more like this and Q hat then as a function approximator.",
            "Thank you.",
            "Can you hear me know I can hear you OK?",
            "For a problem where he was sort of demonstrations and you could either use imitation learning or using virtual enforcement learning.",
            "Do you have any intuition for when you would choose one or the other?",
            "So the question is, if you have demonstrations, should you use imitation learning or reinforcement learning?",
            "If you are in problem setting where you have access to human demonstrations, using those demonstrations can be a really good idea.",
            "And there are actually a number of different ways to use them.",
            "The move, the simplest ways to just directly run supervised learning.",
            "But in some cases you might have both demonstrations and rewards, and if you know the reward and you have some demonstration that I should combine them pretty easily and there actually many techniques in the literature that combine both demonstrations and rewards, there are many ways to do the simplest as you could, for example, initialize your policy with demonstrations and then fine tune it with rewards.",
            "That can be a very good idea because the demonstrations.",
            "Here policy into good initial state that overcomes the challenge of exploration.",
            "So in the previous lecture you guys heard a little bit about explore exploration.",
            "Demonstrations could be really good way to overcome that.",
            "But you can also do something more interleaved.",
            "You can actually both imitate an maximize rewards at the same time, and that can be useful because the initialization you might forget.",
            "Forget that initialization as you try to maximize the rewards.",
            "If you keep both the demonstrations and the rewards around, then you can do even better.",
            "So yeah, if you're interested in this topic, there's quite a bit of literature on the subject.",
            "We are unfortunately out of time, so let me let us first.",
            "Thank Sergey for his very excellent talk."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for the introduction Joel.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'm going to start this lecture with a little kind of thought.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Exercise.",
                    "label": 0
                },
                {
                    "sent": "Let's imagine that you're playing a sport.",
                    "label": 0
                },
                {
                    "sent": "Maybe if you're.",
                    "label": 0
                },
                {
                    "sent": "If you like baseball, and if you're playing baseball, but any sport involves having to catch a ball, so there's a someone throws a ball to you.",
                    "label": 0
                },
                {
                    "sent": "You have to run.",
                    "label": 0
                },
                {
                    "sent": "You have to go and catch it.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to think about we're going to think about how you actually do this.",
                    "label": 0
                },
                {
                    "sent": "Kind of going through your mind as you try to perform this task.",
                    "label": 0
                },
                {
                    "sent": "We can ask some, you know, distinguished scientists what they think goes through your head.",
                    "label": 0
                },
                {
                    "sent": "When you're doing this.",
                    "label": 0
                },
                {
                    "sent": "So, for example, we can ask for sure.",
                    "label": 0
                },
                {
                    "sent": "Dawkins, he is a paragraph in his book that discusses how he thinks this works.",
                    "label": 0
                },
                {
                    "sent": "He says when a man throws a ball high in the air and catches it again, he behaves.",
                    "label": 1
                },
                {
                    "sent": "If he had solved a set of differential equations in predicting the trajectory of the ball, and at some subconscious level something functionally equivalent to the mathematical calculations is going on.",
                    "label": 1
                },
                {
                    "sent": "So this seems like a reasonable hypothesis about how this task works.",
                    "label": 0
                },
                {
                    "sent": "You kind of understand something about the world you understand, maybe something about the force of gravity and air resistance, and so on.",
                    "label": 0
                },
                {
                    "sent": "You're going to do this kind of mental calculation, estimate what's going to happen.",
                    "label": 0
                },
                {
                    "sent": "It's going to land over there.",
                    "label": 0
                },
                {
                    "sent": "You run over there and catch it.",
                    "label": 0
                },
                {
                    "sent": "But in science we deal with hypothesis that are testable and we can conduct some experiments and we can say, well, is this the case or is it not and ensure it just so happens that we don't have to do that experiment ourselves, so why should did it for us in the 1990s?",
                    "label": 0
                },
                {
                    "sent": "There was a.",
                    "label": 1
                },
                {
                    "sent": "Nice paper that was published called.",
                    "label": 0
                },
                {
                    "sent": "Do feel there's nowhere to go to catch the ball or only how to get there.",
                    "label": 0
                },
                {
                    "sent": "Sounds a little bit strange, but what this really means is that when an actual person goes to catch a ball, they actually follow a very simple heuristic that doesn't require understanding anything about the force of gravity or air resistance, or the forces acting on the ball.",
                    "label": 0
                },
                {
                    "sent": "It simply requires blindly following a simple rule, and the rule is the following.",
                    "label": 0
                },
                {
                    "sent": "You look at the ball so it's a dark spot against a bright colored Sky.",
                    "label": 0
                },
                {
                    "sent": "You remember where it is in your field of view?",
                    "label": 0
                },
                {
                    "sent": "And then you start running and you modulate your running speed so the location of the ball in your field of view stays the same, which means that the angle between the ground and the Bolt remains constant.",
                    "label": 0
                },
                {
                    "sent": "And if you follow this very simple heuristic, you can actually prove with a very simple kind of similar triangles argument that you will arrive at exactly the point where the ball lands and you will catch it.",
                    "label": 0
                },
                {
                    "sent": "So the point is that you can actually for this task.",
                    "label": 0
                },
                {
                    "sent": "You can acquire the skill of catching the ball very effective, very robust skill without necessarily going through the process of modeling the complexity of the physical phenomena.",
                    "label": 0
                },
                {
                    "sent": "They give rise to the system that you're trying to interact with.",
                    "label": 0
                },
                {
                    "sent": "So the skill is a policy we learn about policies in the previous lecture.",
                    "label": 0
                },
                {
                    "sent": "It's a mapping from your observations to the actions that you should take in order to complete the task.",
                    "label": 0
                },
                {
                    "sent": "So in today's lecture I'm going to talk about policy search.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about how we can find policies I'm going.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Starts by discussing the problem set up, which will be basically the building on the problem set up from the previous lecture from Richard Lecture.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about model free policy, search algorithms, policy search algorithms that can learn to catch the ball without understanding all the nuances of air resistance and gravity.",
                    "label": 0
                },
                {
                    "sent": "And then what I'm going to do is, I'm going to turn things around a little bit, and I'm going to explore the other facet of this.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say, well, what if Richard Dawkins is right?",
                    "label": 0
                },
                {
                    "sent": "What if maybe not for that particular schedule for some other skill, we do actually want to model how the world works, and then use those models to understand how to how to act, how to discover policies.",
                    "label": 0
                },
                {
                    "sent": "So I'll talk about how we can find policies if we know the model of the system, and then if we don't know the model but are willing to learn that model.",
                    "label": 1
                },
                {
                    "sent": "So I'm going to discuss model based reinforcement learning and how it relates to policy search as well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to begin with, let's set up the problem and this is going to look very similar to what you saw in the previous lecture.",
                    "label": 1
                },
                {
                    "sent": "But I'm going to build up on it a little bit to build up a little more formalism for the policy gradients discussion.",
                    "label": 1
                },
                {
                    "sent": "So let's start off with.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A bit of terminology and I'm going to use as a running example, especially for the policy grading portion of the lecture.",
                    "label": 0
                },
                {
                    "sent": "Kind of a deep reinforcement learning example where your goal is to learn a policy represented with some very expressive function class like a deep neural network.",
                    "label": 0
                },
                {
                    "sent": "So we'll start by kind of building off of what many of you probably already know about supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So if you imagine you know, forget about RL for second, just imagine a standard supervised learning setting.",
                    "label": 0
                },
                {
                    "sent": "Maybe you're doing image recognition.",
                    "label": 0
                },
                {
                    "sent": "Very standard thing.",
                    "label": 0
                },
                {
                    "sent": "You're recognizing that, say images and image net.",
                    "label": 0
                },
                {
                    "sent": "Your goal is to take in the pixels of the image and to put a label.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to attach some symbols to these things, and the symbols are going to be a little unusual for supervised learning, but they're the ones that we're going to use for RLS, so we'll say that our input to our model is an observation which will denote with letter O, so.",
                    "label": 0
                },
                {
                    "sent": "In the case of image net classification, observation consists of the pixels in the image.",
                    "label": 0
                },
                {
                    "sent": "Your output we're going to notice a, so if you're doing image classification, it's a label in RL is going to be an action, but we don't really care so much, it's just something that you're going to output and the thing that you're learning this thing, called the policy, defines a distribution over the output.",
                    "label": 0
                },
                {
                    "sent": "Given the input distribution over a given.",
                    "label": 0
                },
                {
                    "sent": "Oh so in the same way that your classifier for classifying image net defines the distribution over labels given the image, your policy is going to find distribution over actions given the observation.",
                    "label": 0
                },
                {
                    "sent": "So we're going to use the letter \u03c0 to denote this distribution.",
                    "label": 0
                },
                {
                    "sent": "And very often you'll see that I'll write pie with a subscript Theta.",
                    "label": 0
                },
                {
                    "sent": "Theta denotes the parameters of the policy, so if you're sort of represented, for example by big deep neural network, Theta is literally just the weights in that network.",
                    "label": 0
                },
                {
                    "sent": "So those are the things that you actually want to learn.",
                    "label": 0
                },
                {
                    "sent": "So this is the supervised learning setup.",
                    "label": 0
                },
                {
                    "sent": "If we want to go and turn it into the reinforcement learning setup, we need to make a few changes.",
                    "label": 0
                },
                {
                    "sent": "So the first change that we want might want to make is we might want to introduce the notion of time, so we'll just put this subscripti on everything to indicate the fact that you receive an observation.",
                    "label": 0
                },
                {
                    "sent": "At a time step T and you output your action at a time step T and of course, in most interesting reinforcement learning setups, although not all of them, the action will affect the next observation you will see.",
                    "label": 0
                },
                {
                    "sent": "So if you see this Tiger in your image and you successfully recognize that it's a Tiger, maybe the next observation will be something you're happy with, and if you fail to recognize the Tiger, then the next observation might be something that you're a little more concerned about.",
                    "label": 0
                },
                {
                    "sent": "And of course, you can change the days if it doesn't have to be the label.",
                    "label": 0
                },
                {
                    "sent": "It can be some action that you can actually take so you can choose to do different things when you see this Tiger, you can you know, run away, ignore Petit, whatever, and that will lead to different consequences.",
                    "label": 0
                },
                {
                    "sent": "And of course the very same framework can be extended if you want to do this with continuous actions rather than discrete action, so you can just as well say well which way are you going to run.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's a continuous value that you're going to output, and that's all falls within this framework, so let's nail down the notation more concretely.",
                    "label": 0
                },
                {
                    "sent": "We have our observations.",
                    "label": 0
                },
                {
                    "sent": "Oh, so these are the things that your policy gets to see if your policy is controlling a robot.",
                    "label": 0
                },
                {
                    "sent": "For example, those are literally the readings from the robot sensors.",
                    "label": 0
                },
                {
                    "sent": "We have the action A.",
                    "label": 0
                },
                {
                    "sent": "That's the thing that you want to output.",
                    "label": 0
                },
                {
                    "sent": "So your the goal of this policy is to produce good days and we'll define good in a second.",
                    "label": 0
                },
                {
                    "sent": "Another thing that you're learning is this policy.",
                    "label": 0
                },
                {
                    "sent": "The distribution a given oh.",
                    "label": 0
                },
                {
                    "sent": "Now I didn't mention something yet which actually came up in the previous lecture, which is very, very important, which is one more object that we really need in order to nail down the problem definition.",
                    "label": 0
                },
                {
                    "sent": "That's the state you'll notice that the state didn't actually come up yet.",
                    "label": 0
                },
                {
                    "sent": "In the most general case of policy search, we'd like to learn policies that operate on observations.",
                    "label": 0
                },
                {
                    "sent": "Now, what's the difference difference between States and observations?",
                    "label": 0
                },
                {
                    "sent": "And we can, by the way, also as a special case, train policies that depend on states.",
                    "label": 0
                },
                {
                    "sent": "Let's discuss this distinction a little bit.",
                    "label": 0
                },
                {
                    "sent": "If I show you this picture, this is a picture of a cheetah chasing a gazelle.",
                    "label": 0
                },
                {
                    "sent": "This picture the way we represent in a computer is a big matrix of numbers.",
                    "label": 0
                },
                {
                    "sent": "The numbers themselves don't really tell us anything about cheetah or gazelle apriori.",
                    "label": 0
                },
                {
                    "sent": "There's you know if we actually interpret that image will realize that it contains a sheet in Excel, but the numbers just represent the pixels in the image.",
                    "label": 0
                },
                {
                    "sent": "However, underlying these pixels, there's some kind of physical system, and that physical system has a state.",
                    "label": 0
                },
                {
                    "sent": "The image pixels are not the physical system, that's just how we observe the physical system.",
                    "label": 0
                },
                {
                    "sent": "The physical system has its own state.",
                    "label": 0
                },
                {
                    "sent": "We can represent that state of different levels of abstraction.",
                    "label": 0
                },
                {
                    "sent": "You know if you're if you're quantum physicists, maybe you care about you know all the uncertainty about the particles in the sheet in the gazelle.",
                    "label": 0
                },
                {
                    "sent": "If you're operating at a higher level, maybe you care more about maybe the position of the sheet and the position of the gazelle.",
                    "label": 0
                },
                {
                    "sent": "The state is a concise summary of the current configuration of the world.",
                    "label": 0
                },
                {
                    "sent": "Formally, the state is everything that you need in order to predict the future about the world, and I'll define that on the next slide.",
                    "label": 0
                },
                {
                    "sent": "But to make it clear what's the difference between state and observation?",
                    "label": 0
                },
                {
                    "sent": "There is a state underlying this image.",
                    "label": 0
                },
                {
                    "sent": "The position maybe of the cheetah MZL.",
                    "label": 0
                },
                {
                    "sent": "The observation reflects the state.",
                    "label": 0
                },
                {
                    "sent": "It's not necessary sufficient to fully reduce the state.",
                    "label": 0
                },
                {
                    "sent": "So if, for example, an automobile drives in front of the cheetah, the pixels in the image of changed, perhaps you can't even see where the sheet is anymore.",
                    "label": 0
                },
                {
                    "sent": "But it's still the same place.",
                    "label": 0
                },
                {
                    "sent": "It hasn't vanished.",
                    "label": 0
                },
                {
                    "sent": "So maybe the state hasn't really changed, but the observation has.",
                    "label": 0
                },
                {
                    "sent": "So the observation is not always sufficient to do this state.",
                    "label": 0
                },
                {
                    "sent": "Before we get into the more formal, I'm going to have a slightly less silly example.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say well, instead of chasing after Tigers and cheetahs, let's say that we're doing this little autonomous driving task just just for the sake of having a working example.",
                    "label": 0
                },
                {
                    "sent": "So our observations will be images from a dashboard camera, and our actions are the way that you're going to turn the steering wheel so one scalar value for negative values for turning left and positive values for turning right.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now if we want to formally define the relationship between observations and states, we can draw a graphical model that we're both represents those relationships.",
                    "label": 0
                },
                {
                    "sent": "So in this graphical model we have edges that relate observations to actions.",
                    "label": 0
                },
                {
                    "sent": "That's our policy or policy gives us a given oh.",
                    "label": 0
                },
                {
                    "sent": "We have edges that relate States and actions to next states.",
                    "label": 0
                },
                {
                    "sent": "These are sometimes referred to as the dynamics or the transition function.",
                    "label": 0
                },
                {
                    "sent": "And then we have this edge that goes from state observation.",
                    "label": 0
                },
                {
                    "sent": "So this is are sometimes referred to as our observation function.",
                    "label": 0
                },
                {
                    "sent": "So the thing to notice about the state and the main thing that distinguishes states from observations.",
                    "label": 0
                },
                {
                    "sent": "Is that if you observe the state St. Then state S T -- 1 is conditionally independent of state S T + 1.",
                    "label": 0
                },
                {
                    "sent": "What this implies is that if you want to make some kind of decision about what you're going to do in the future or in the present, and you know the state St. Knowing State S T -- 1 or S T -- 2 isn't going to tell you anything that you don't already have, so the future is conditionally independent of the past given the present, which means that if you're making decisions, you only need to know the present.",
                    "label": 0
                },
                {
                    "sent": "That is true if we're talking about state.",
                    "label": 0
                },
                {
                    "sent": "If we're talking about observations that is no longer the case.",
                    "label": 0
                },
                {
                    "sent": "So if you remember the example with the car driving in front of the sheet on the previous slide, there maybe when you observe that car you don't know where the cheetah is.",
                    "label": 0
                },
                {
                    "sent": "But if a second ago the car wasn't there yet, then that observation from a second collection tells you something that you wouldn't have known just from looking at the present image.",
                    "label": 0
                },
                {
                    "sent": "So if you're just dealing with observations then past observations can actually tell you something that you don't already know.",
                    "label": 0
                },
                {
                    "sent": "That can be helpful for making decisions, but if you're dealing with state then the present state.",
                    "label": 0
                },
                {
                    "sent": "Tells you everything you need to know to make those decisions.",
                    "label": 0
                },
                {
                    "sent": "So this conditional independence property is referred to as the Markov property.",
                    "label": 0
                },
                {
                    "sent": "The Markov property says that condition on the present, the future is conditionally independent past.",
                    "label": 0
                },
                {
                    "sent": "OK, so then we have our policy which in the most general case depends on observations and in some special cases we can also talk about policies that depend on state.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we've we've defined the variables that are play here.",
                    "label": 0
                },
                {
                    "sent": "Can we think about some simple ways to learn policy's?",
                    "label": 0
                },
                {
                    "sent": "There are some very simple ways to learn policies.",
                    "label": 0
                },
                {
                    "sent": "For example, you can actually learn policies with supervised learning.",
                    "label": 0
                },
                {
                    "sent": "That's called imitation learning.",
                    "label": 0
                },
                {
                    "sent": "So if you want to learn policies with supervised learning, you do exactly the most obvious thing.",
                    "label": 0
                },
                {
                    "sent": "For example, if you want.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This driving thing get a person to drive the car, record the images of the car, sees record the actions of the person executed, put them in a big bucket of training data.",
                    "label": 0
                },
                {
                    "sent": "Run supervised learning on that data and you will get a policy or whether it's a good policy or not is a very complicated question, and there's quite a bit of theory about when imitation learning works like this, and when it doesn't, and what you can do to fix it, I won't be discussing that in today's lecture because today's lecture is going for some learning, but do keep in mind that yes, supervised learning can be used to learn policies.",
                    "label": 0
                },
                {
                    "sent": "OK, so in reinforcement learning we need to introduce one more object.",
                    "label": 0
                },
                {
                    "sent": "That's going to tell us whether a given policy is good or bad, and we saw that in the previous lecture, and that's the reward function.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So very simple.",
                    "label": 0
                },
                {
                    "sent": "It's going to allow us to figure out whether particular actions or particular states are good or bad is a function of States and actions.",
                    "label": 0
                },
                {
                    "sent": "It's a scalar value function in general to final States and actions.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you define it only on states, but in the most general case States and actions it tells us which States and actions are better than others, and crucially, the reward function is not something that we're trying to optimize greedily, so we're not going to take actions that simply give us the best reward.",
                    "label": 0
                },
                {
                    "sent": "Right now, we're going to take the actions that give us the best total reward.",
                    "label": 0
                },
                {
                    "sent": "Over our entire existence.",
                    "label": 0
                },
                {
                    "sent": "So in the in the car driving example, maybe driving happily on the road at the desired speed gives you high reward.",
                    "label": 0
                },
                {
                    "sent": "Being in a car accident gives you low reward, but of course if you want to avoid those low reward situations, you just take the actions now that will prevent getting to those bad states in the future.",
                    "label": 0
                },
                {
                    "sent": "OK, so together the state actions, the reward and the transition dynamics.",
                    "label": 0
                },
                {
                    "sent": "PFS Prime given essay define a Markov decision process if you also have observations, you can define something called partially observed Markov decision process, so I'll get into that in a second.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's be concrete about these definitions.",
                    "label": 0
                },
                {
                    "sent": "Before we define the Markov decision process, let me just as a brief aside, define the Markov chain.",
                    "label": 0
                },
                {
                    "sent": "So who's this Markov guy anyway?",
                    "label": 0
                },
                {
                    "sent": "Andrei Markov, he was a Russian mathematician at the turn of the 20th century, and he didn't actually invent Markov decision processes, but he did do a lot of work on something called Markov chains, which you can think of as a simplified setting that doesn't deal with decision making.",
                    "label": 0
                },
                {
                    "sent": "So a Markov chain is just defined with two objects, a state space Anna transition operator.",
                    "label": 0
                },
                {
                    "sent": "So the state space that we've already discussed, that's the configuration of the world and states can be discrete or continuous.",
                    "label": 0
                },
                {
                    "sent": "So in the discrete case, you can just think of it as a set of things.",
                    "label": 0
                },
                {
                    "sent": "The transition operator tells you what's your probability of going from one state to another.",
                    "label": 0
                },
                {
                    "sent": "So it's just Pfc plus one given St, and the reason that we call it an operator is because you can write it as a linear operation.",
                    "label": 0
                },
                {
                    "sent": "So if you have a vector of probabilities denoting your probability of being each state, then so that's the vector of probabilities muti, then you can express vector of probabilities being the next state as a matrix vector product.",
                    "label": 0
                },
                {
                    "sent": "So you construct this matrix TJ which tells you the probability of going to state I from state J and then muti plus one is just given by T times.",
                    "label": 0
                },
                {
                    "sent": "Muti so the graphical model for Markov chain.",
                    "label": 0
                },
                {
                    "sent": "Very simple.",
                    "label": 0
                },
                {
                    "sent": "It's just a chain of States and these states obey the Markov property, which means that in this picture S3 is independent of S1 given us too.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's go to Markov decision processes.",
                    "label": 0
                },
                {
                    "sent": "It's actually, you know, a little bit of debate about where and when they were exactly introduced, but to my knowledge, the first writing on the topic was, you know, appeared in some work by Richard Bellman in the middle of the 20th century.",
                    "label": 0
                },
                {
                    "sent": "And the Markov decision process basically turns a Markov chain into a system that you can actually control, so it adds two things, an action space and a reward function.",
                    "label": 0
                },
                {
                    "sent": "So we still have our state space from before we have an actual space, which is a set of actions which again can be discrete or continuous.",
                    "label": 0
                },
                {
                    "sent": "The graphical model now includes this action.",
                    "label": 0
                },
                {
                    "sent": "And the transition operator is conditioned on the action.",
                    "label": 0
                },
                {
                    "sent": "The transition operators now tensor, so you have one one thing on the output that depends on two things, but you can still do all the linear stuff.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into that here, but you could if you wanted to.",
                    "label": 0
                },
                {
                    "sent": "And you also have this reward.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Auction the reward functions of scalar valued function of the state and the action.",
                    "label": 0
                },
                {
                    "sent": "So it's a mapping from States and actions to real valued numbers.",
                    "label": 0
                },
                {
                    "sent": "And now tomorrow precision process you can actually start talking about making decisions.",
                    "label": 0
                },
                {
                    "sent": "Especially optimal decisions.",
                    "label": 0
                },
                {
                    "sent": "The kinds of decisions that will maximize your reward.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the last thing last, the mathematical object I'm going to find here is a partially observed Markov decision process, so this is where we're actually going to bring in those observations.",
                    "label": 0
                },
                {
                    "sent": "So we can take a Markov decision process an introduce observations which will turn into a partially observed Markov decision process, and that adds two things.",
                    "label": 0
                },
                {
                    "sent": "It has an observation space.",
                    "label": 0
                },
                {
                    "sent": "Oh, and an observation function E. Sometimes referred to as an admission function.",
                    "label": 0
                },
                {
                    "sent": "So you have a state space and action space like before, and now you have this observation space just like the state space and actually space.",
                    "label": 0
                },
                {
                    "sent": "The observation space can be continuous or discrete.",
                    "label": 0
                },
                {
                    "sent": "In our driving example from before, the observations might have been images and the state might have been maybe the position of the car on the road, maybe also the positions of the other cars.",
                    "label": 0
                },
                {
                    "sent": "So the graphical model now includes these observations like this.",
                    "label": 0
                },
                {
                    "sent": "And you still have a transition operator.",
                    "label": 0
                },
                {
                    "sent": "It's the same transition operator that we had before for MPs, but you also have an admission probability.",
                    "label": 0
                },
                {
                    "sent": "The admission probability tells you the probability of observing some observation OT given a state St. And this can also be stochastic, so there might be some kind of nuisance variables in the world that will affect what you see.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's some static in your camera, maybe sometimes something obscured the image.",
                    "label": 0
                },
                {
                    "sent": "So in general the observations are also stochastic.",
                    "label": 0
                },
                {
                    "sent": "And of course you have a reward function.",
                    "label": 0
                },
                {
                    "sent": "In the conventional definition of partial observed Markov scission processes, the reward function actually depends on the state in the action, not on the observation.",
                    "label": 0
                },
                {
                    "sent": "But you get only observe the observations.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we have these objects to work with and we can try to nail down an objective, a goal for reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So I alluded to this before when I said that you have this reward, you want to optimize the reward you want to maximize it.",
                    "label": 0
                },
                {
                    "sent": "Not really, not for the immediate decision, but overall time.",
                    "label": 0
                },
                {
                    "sent": "So let's try to write that down.",
                    "label": 0
                },
                {
                    "sent": "Let's try to write down an equation for this reinforcement learning objective.",
                    "label": 0
                },
                {
                    "sent": "So we have our policy as a working example.",
                    "label": 0
                },
                {
                    "sent": "Let's pretend it's some kind of parametric function, maybe a deep neural network.",
                    "label": 0
                },
                {
                    "sent": "We'll come back to the partially observed case later.",
                    "label": 1
                },
                {
                    "sent": "I want to find this first for the fully observed case for the partial observed case, the objective is going to be the same, it's just some of the algorithms will change.",
                    "label": 0
                },
                {
                    "sent": "Theta denotes the parameters of our policy, like the weights in the neural network.",
                    "label": 0
                },
                {
                    "sent": "The policy takes in either the state or the observation.",
                    "label": 0
                },
                {
                    "sent": "Produces actions, perhaps the cast eccle.",
                    "label": 0
                },
                {
                    "sent": "The world takes the state in the action and produces a next state according to some unknown probability.",
                    "label": 0
                },
                {
                    "sent": "So this is the the transition operator, the dynamics, whatever you want to call us and that defines distributions over trajectories.",
                    "label": 0
                },
                {
                    "sent": "So when I use the word trajectory, what I mean is a sequence of States and actions.",
                    "label": 0
                },
                {
                    "sent": "I'm dealing with a finite horizon setting.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "You can extend this to the case of infinitely long trajectory as well, but just to make it easier to write down, let's deal with a finite horizon setting where you know that you're going to have to act 4.",
                    "label": 0
                },
                {
                    "sent": "Capital T steps.",
                    "label": 0
                },
                {
                    "sent": "So maybe you're driving to work.",
                    "label": 0
                },
                {
                    "sent": "You know you're going to drive for one hour.",
                    "label": 0
                },
                {
                    "sent": "Let's try to maximize reward for that one hour.",
                    "label": 0
                },
                {
                    "sent": "But capital T can also be Infinity.",
                    "label": 0
                },
                {
                    "sent": "So trajectory distribution is a probability distribution over sequence of States and actions, and because of how we've defined these Markov decision processes, we can factorize that distribution into product of conditionals.",
                    "label": 0
                },
                {
                    "sent": "We can say that it's the probability of being some initial state S one times a big product over the probability at every time step of taking some action 80 and then transitioning to some next state S T + 1.",
                    "label": 0
                },
                {
                    "sent": "So this is just using the chain rule from that Bayes net that we had on the previous slide.",
                    "label": 0
                },
                {
                    "sent": "And once we've defined this trajectory distribution, now we can define an objective that we want to optimize.",
                    "label": 0
                },
                {
                    "sent": "So the structure distribution for shorthand.",
                    "label": 0
                },
                {
                    "sent": "If you see me right P Theta of touts, how just means the sequence of essays?",
                    "label": 0
                },
                {
                    "sent": "And the objective we can define as trying to find the Theta Theta star that maximizes the expectation under the trajectory distribution of the total reward of that trajectory.",
                    "label": 0
                },
                {
                    "sent": "So it's an expected value of a sum of rewards for the entire horizon, and the expectation is taken under the trajectory distribution that is induced by the product of those conditionals.",
                    "label": 0
                },
                {
                    "sent": "And the only one of those conditions that depends on Theta forces pie.",
                    "label": 0
                },
                {
                    "sent": "So as you change Theta Pi, Theta 80 given SD term changes, which in turn changes your future distribution, which in turn changes your total expected reward.",
                    "label": 0
                },
                {
                    "sent": "So our goal is to maximize that thing at the bottom.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I mentioned that was for the Fire Horizon case.",
                    "label": 0
                },
                {
                    "sent": "We can define an objective for the Infinite Horizon case.",
                    "label": 1
                },
                {
                    "sent": "Also we can introduce something called the discount factor if we want to have a sum that doesn't run off to Infinity.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about discount factors in this lecture, but you'll see plenty of that in the other lectures.",
                    "label": 0
                },
                {
                    "sent": "In the Infinite Horizon case, typically the way that you would define objective is that you would define it in expectation under the stationary distribution of the Markov chain induced by your policy.",
                    "label": 0
                },
                {
                    "sent": "We won't talk too much about the enterprise in case, but just so you know you can define it and it's.",
                    "label": 0
                },
                {
                    "sent": "Straightforward in terms of stationary distributions, but will deal mainly with the final horizon case for now.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's the problem set up.",
                    "label": 1
                },
                {
                    "sent": "We have the mathematical objects are going to be dealing with MDP's and partially observed MPs.",
                    "label": 0
                },
                {
                    "sent": "We have an objective function which is the expectation of the reward and now we need some algorithms.",
                    "label": 0
                },
                {
                    "sent": "We need to figure out some algorithms that were going to be able to use to actually maximize that expected reward.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now before we, we can maximize our objective.",
                    "label": 0
                },
                {
                    "sent": "Let's at least try to figure out how we can evaluate our objective.",
                    "label": 0
                },
                {
                    "sent": "Does anybody have any ideas about how we can evaluate the expectation of the reward function for particular policy?",
                    "label": 0
                },
                {
                    "sent": "Sampling, yeah, so you have an expectation of some function under some distribution.",
                    "label": 0
                },
                {
                    "sent": "It's maybe really high dimensional, really simple default choice if you want to value in expectation like that is to do Monte Carlo estimation to sample an average together samples.",
                    "label": 0
                },
                {
                    "sent": "So let's do that.",
                    "label": 0
                },
                {
                    "sent": "Let's say that we have some kind of objective function will denoted J of Theta.",
                    "label": 0
                },
                {
                    "sent": "To emphasize, that's a function of Theta.",
                    "label": 0
                },
                {
                    "sent": "Its expectation under the structure distribution.",
                    "label": 0
                },
                {
                    "sent": "And we can estimate it by generating some samples.",
                    "label": 0
                },
                {
                    "sent": "And averaging those samples together so.",
                    "label": 0
                },
                {
                    "sent": "How do we generate samples?",
                    "label": 0
                },
                {
                    "sent": "Well, generating a sample for a policy basically just amounts to running that policy in the world.",
                    "label": 0
                },
                {
                    "sent": "You can literally just run in the real world.",
                    "label": 0
                },
                {
                    "sent": "You can let your car drive you know 50 times, see what it does, see which States and actions that sees, evaluate the reward function at those States and actions.",
                    "label": 0
                },
                {
                    "sent": "Sum them together in an average over the samples so we have the summation over T inside and on the outside.",
                    "label": 0
                },
                {
                    "sent": "You have some oversamples indexed by I and you have to divide by the number of samples yourself so you run your policy, get some trajectory's.",
                    "label": 0
                },
                {
                    "sent": "Some of them are good, some are bad, some are somewhere in between.",
                    "label": 0
                },
                {
                    "sent": "You average them altogether and you get a number, and that's an unbiased estimate of your objective value.",
                    "label": 0
                },
                {
                    "sent": "That's the thing that you're trying to maximize in expectation.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we can evaluate our objective.",
                    "label": 0
                },
                {
                    "sent": "Now let's think about how we can maximize.",
                    "label": 0
                },
                {
                    "sent": "And to start with, we're going to maximize it.",
                    "label": 0
                },
                {
                    "sent": "Kind of in the same way that we usually start thinking about maximizing things.",
                    "label": 0
                },
                {
                    "sent": "If you're used to supervised learning techniques, very good way to minimize or maximize something.",
                    "label": 0
                },
                {
                    "sent": "It's the computer's gradient and follow the gradient.",
                    "label": 0
                },
                {
                    "sent": "So we're going to do exactly that.",
                    "label": 0
                },
                {
                    "sent": "And then the trick just comes in.",
                    "label": 0
                },
                {
                    "sent": "How do you compute the gradient affectively in a way that has little variance?",
                    "label": 0
                },
                {
                    "sent": "Hopefully not too much bias and actually so practical algorithm.",
                    "label": 0
                },
                {
                    "sent": "So just to throw the equation up there again, here is our objective and for shorthand I'm going to write RF Tau.",
                    "label": 0
                },
                {
                    "sent": "We just means the sum of all of the reward for all the States and actions inside Tau.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It's an expectation, so I can write out that expectation is an integral right?",
                    "label": 0
                },
                {
                    "sent": "So we have an expectation of some function that's just the integral of the probability of the product of the probability of that variable times its function value.",
                    "label": 0
                },
                {
                    "sent": "So the probability of the structure it out times the reward of town.",
                    "label": 0
                },
                {
                    "sent": "So now we need this gradient.",
                    "label": 0
                },
                {
                    "sent": "And this is where basically the only mathematically sophisticated part of today's lecture is going to come in.",
                    "label": 0
                },
                {
                    "sent": "So in computing the gradient of this objective, what are we going to do?",
                    "label": 0
                },
                {
                    "sent": "Well, we'll just do a bunch of algebra.",
                    "label": 0
                },
                {
                    "sent": "OK, gradient operators are linear, which means that we can put it inside the integral.",
                    "label": 0
                },
                {
                    "sent": "So we have this integral of grad Theta Pi Theta Tau times are Tau, and now we're going to introduce a little bit of a.",
                    "label": 0
                },
                {
                    "sent": "Convenient identity that we're going to use to make it actually practical to compute the screening.",
                    "label": 0
                },
                {
                    "sent": "The convenient entity is very, very simple.",
                    "label": 0
                },
                {
                    "sent": "It just comes from the definition of the gradients of the log of some function, so I'm just going to write out the identity and then you'll see how to use it.",
                    "label": 0
                },
                {
                    "sent": "So if you have the gradients of log Pi grading of log Pi is equal to 1 / \u03c0 times the gradients of \u03c0, so we can substitute that in there.",
                    "label": 0
                },
                {
                    "sent": "We just literally, you know, look up a calculus textbook, substitute in the definition of the gradient.",
                    "label": 0
                },
                {
                    "sent": "Algorithm and you get this equality and now notice that high appears on the top and bottom so we can cancel it out.",
                    "label": 0
                },
                {
                    "sent": "So Pi of tile times grad log pile of cow is equal to grab pipe tile.",
                    "label": 0
                },
                {
                    "sent": "So this is a convenient.",
                    "label": 1
                },
                {
                    "sent": "It just comes from the definition of the gradient of a logarithm.",
                    "label": 0
                },
                {
                    "sent": "You look it up in a calculus textbook substitute in there, do a little bit of algebra and you get this convenient entity.",
                    "label": 0
                },
                {
                    "sent": "And now we can use this convenient identity to actually get a very nice and convenient form for our gradient.",
                    "label": 0
                },
                {
                    "sent": "So notice that we have this grab pie.",
                    "label": 0
                },
                {
                    "sent": "Which means that we can substitute in the left hand side of our convenient identity and we can convert this integral to the integral of Pi of Tau, grad log pipe.",
                    "label": 0
                },
                {
                    "sent": "Tile times are of tile.",
                    "label": 0
                },
                {
                    "sent": "And this thing now is again an expectation, right?",
                    "label": 0
                },
                {
                    "sent": "Because it's Pi of Tau times something.",
                    "label": 0
                },
                {
                    "sent": "So we can write it out as an expectation.",
                    "label": 0
                },
                {
                    "sent": "Just like our objective was, an expectation is an expectation, but now it's an expectation of grad log.",
                    "label": 0
                },
                {
                    "sent": "PY file times are of tile, so we took our objective, which is an expectation under the structure distribution computed as gradient and gradient itself is also an expectation under that same distribution, just of a slightly different function.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that this one is actually something we can evaluate without knowing the dynamics.",
                    "label": 0
                },
                {
                    "sent": "The transition operator explicitly.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's work through that.",
                    "label": 0
                },
                {
                    "sent": "We have an expectation and just like before we're going to estimate that expectation by sampling, so we've agreed before the sampling is a reasonable way to estimate expectations.",
                    "label": 0
                },
                {
                    "sent": "We're estimating that expectation under a distribution.",
                    "label": 0
                },
                {
                    "sent": "That's the trajectory distribution.",
                    "label": 0
                },
                {
                    "sent": "Again, the one that you saw before.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And we want to evaluate that grad log Pi.",
                    "label": 0
                },
                {
                    "sent": "Let's actually look at what the logarithm of structure distribution looks like.",
                    "label": 0
                },
                {
                    "sent": "You take the logarithm of both sides and you get.",
                    "label": 0
                },
                {
                    "sent": "I apologize, there's a little typo on this slide, but when you see log \u03c0 and log P, that's the same thing.",
                    "label": 0
                },
                {
                    "sent": "Just pretend those are the same, so we take the logarithm.",
                    "label": 0
                },
                {
                    "sent": "Both sides, the logarithm of product is just a sum of logarithms, so that's going to be.",
                    "label": 0
                },
                {
                    "sent": "You have log PS1, plus some overall time steps of log \u03c0 plus log P of the transition.",
                    "label": 0
                },
                {
                    "sent": "So this thing underlined in green.",
                    "label": 0
                },
                {
                    "sent": "That's the thing that we want to take the gradient of.",
                    "label": 0
                },
                {
                    "sent": "So there there it is right there, and we're taking the gradient of that thing.",
                    "label": 0
                },
                {
                    "sent": "Now when you take a gradient of the sum of a bunch of functions.",
                    "label": 0
                },
                {
                    "sent": "Remember that the only things that actually matter are the ones that depend on the parameters with respect to which you're taking the gradient.",
                    "label": 0
                },
                {
                    "sent": "So the grading here is with respect to Theta.",
                    "label": 0
                },
                {
                    "sent": "It's a gradient of a sum of a bunch of terms, and many of those terms do not depend on Theta, so your initial state distribution doesn't change if you change your policy parameters, the probability of a transition doesn't change if you change your policy parameters, which means that the gradient with respect to Theta of this term is just zero.",
                    "label": 0
                },
                {
                    "sent": "And the greater respect to Theta of this term is just zero, which means that to compute the grain of the log probability of trajectory, you do not need to know the probability of the initial state nor the probability of a transition.",
                    "label": 0
                },
                {
                    "sent": "So that's actually very convenient.",
                    "label": 0
                },
                {
                    "sent": "That's going to allow us to derive a model free reinforcement learning algorithm from this math.",
                    "label": 0
                },
                {
                    "sent": "So we substitute that in there and we get grad log.",
                    "label": 0
                },
                {
                    "sent": "Pile of Tao is just the sum over all the time steps of the grad log probabilities of the individual.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actions.",
                    "label": 0
                },
                {
                    "sent": "So just to make it clear what this equation is saying, it's saying that the.",
                    "label": 0
                },
                {
                    "sent": "The gradient of your objective with respect to Theta can be written out as the expectation of the structure distribution of the sum of the gradients of the log probabilities of the actions that you took times the sum of the rewards that you got.",
                    "label": 0
                },
                {
                    "sent": "Now and now we can.",
                    "label": 0
                },
                {
                    "sent": "We can do the sampling.",
                    "label": 0
                },
                {
                    "sent": "Now we can actually evaluate this expectation by taking samples.",
                    "label": 0
                },
                {
                    "sent": "So just like before, we evaluated the objective by taking samples and average them together.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to evaluate the gradient by taking samples and averaging together these products of rewards and grad log pies.",
                    "label": 0
                },
                {
                    "sent": "So once we compute this gradient, we can improve our policy just by doing gradient descent.",
                    "label": 0
                },
                {
                    "sent": "We can take our old Theta and incremented by some step size Alpha times the gradient.",
                    "label": 0
                },
                {
                    "sent": "Regular good old fashioned gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm you can represent a you know visually like this.",
                    "label": 0
                },
                {
                    "sent": "It has.",
                    "label": 0
                },
                {
                    "sent": "It consists of basically three steps.",
                    "label": 0
                },
                {
                    "sent": "Generate some samples by running your policy, estimate your return in gradient by adding up the rewards and adding up the grad log pies, and then improve the policy by taking a gradient sense step.",
                    "label": 1
                },
                {
                    "sent": "And this is called the reinforce algorithm.",
                    "label": 0
                },
                {
                    "sent": "So if you see reinforce also if you see likelihood ratio policy gradient, those all basically mean the same thing.",
                    "label": 0
                },
                {
                    "sent": "It's referring to this thing.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm Step 1 sample your trajectory's by running the policy Step 2 compute your gradient step three, increment your parameters by the gradient and that will get you a slightly better policy.",
                    "label": 0
                },
                {
                    "sent": "At least that's the idea.",
                    "label": 0
                },
                {
                    "sent": "There are a few caveats and we will talk about the caveats shortly.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But before we talk about the caveats, let's let's try to unpack a little bit.",
                    "label": 0
                },
                {
                    "sent": "What is that we just did?",
                    "label": 0
                },
                {
                    "sent": "So the sum of rewards you know.",
                    "label": 0
                },
                {
                    "sent": "Hopefully that kind of makes sense to everybody, but these grad log pies.",
                    "label": 0
                },
                {
                    "sent": "Those may be there a little bit mysterious like we have these funny grad log Pi terms.",
                    "label": 0
                },
                {
                    "sent": "How does that relate?",
                    "label": 0
                },
                {
                    "sent": "For example, the picture we saw before with this nice observation, observation, or state going into the deep net and actions coming out well, grad log pie.",
                    "label": 0
                },
                {
                    "sent": "That's the gradient of the log probability of an action under your policy.",
                    "label": 0
                },
                {
                    "sent": "When you're doing something like maximum likelihood with supervised learning, you're computing exactly the same grad log pies.",
                    "label": 0
                },
                {
                    "sent": "So let's relate this to maximum like little more explicitly in policy gradient, this is the equation that I had before.",
                    "label": 0
                },
                {
                    "sent": "In maximum likelihood, like supervised learning, you have actually very similar equation if you work through the math for doing regular supervised learning, the gradients actually just looks like this right?",
                    "label": 0
                },
                {
                    "sent": "In supervised learning you're maximizing the probability of all of the observed input output pairs, so the gradients of your objective supervised learning is just the average over all of your data points of the gradients of the log probability of the output given the input.",
                    "label": 0
                },
                {
                    "sent": "So in policy gradient with the same exact grad log pies except now they're multiplied by these rewards.",
                    "label": 0
                },
                {
                    "sent": "So if you, if you're doing for example deep learning, you implement this thing in Tensorflow or something like that, you use a cross entropy loss that cross entropy loss.",
                    "label": 0
                },
                {
                    "sent": "What you're actually asking Tensorflow to do is compute grad log \u03c0.",
                    "label": 0
                },
                {
                    "sent": "In fact, I'll show you some pseudocode later on that will make this even more explicit, so this grad log pie.",
                    "label": 0
                },
                {
                    "sent": "This is just a loss function.",
                    "label": 0
                },
                {
                    "sent": "The gradient of loss function at the top of your function approximator.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that explains the grad log pie.",
                    "label": 0
                },
                {
                    "sent": "Now what about the rest of the album?",
                    "label": 0
                },
                {
                    "sent": "Was this algorithm actually doing?",
                    "label": 0
                },
                {
                    "sent": "Can we build up some intuition for how this is going to change the policy?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll use a little bit of shorthand.",
                    "label": 0
                },
                {
                    "sent": "I'll write out the sum of grad log Pi a given S, just grab pipe Tao and I'll write out the sum of the Rs as just R cow just to avoid cluttering notation.",
                    "label": 0
                },
                {
                    "sent": "But just remember when you see this is really referring to some overtime steps.",
                    "label": 0
                },
                {
                    "sent": "And remember that maximum likelihood regular supervised learning.",
                    "label": 0
                },
                {
                    "sent": "It's just trying to maximize the probabilities of all of your input output pairs in policy gradients.",
                    "label": 0
                },
                {
                    "sent": "It might not be trying to maximize all those probabilities because they're multiplied by the rewards.",
                    "label": 0
                },
                {
                    "sent": "So if the reward is negative, then it's actually trying to minimize it.",
                    "label": 0
                },
                {
                    "sent": "If there was positive trying to maximize it.",
                    "label": 0
                },
                {
                    "sent": "If the reward is gigantic, it's trying to maximize it alot.",
                    "label": 0
                },
                {
                    "sent": "If the reward is tiny, is trying to change it only a little bit.",
                    "label": 0
                },
                {
                    "sent": "So what that means is that when you sample from your policy, you get these trajectories.",
                    "label": 0
                },
                {
                    "sent": "You evaluate their rewards.",
                    "label": 0
                },
                {
                    "sent": "Some of them are good, and some of them are bad and some of them are somewhere in between.",
                    "label": 0
                },
                {
                    "sent": "What policy grading will try to do, is it will take the good trajectory's which have a big positive multiplier and will try to increase their probability.",
                    "label": 0
                },
                {
                    "sent": "So the good trajectory's will have a big positive multiplier for RF Tao, which means that the gradient will try to make their log \u03c0 big, so grad log \u03c0 times a big positive number.",
                    "label": 0
                },
                {
                    "sent": "The really bad stuff might have a really big negative multiplier, so those log probabilities are going to be decreased.",
                    "label": 0
                },
                {
                    "sent": "And the stuff that's in between?",
                    "label": 0
                },
                {
                    "sent": "Maybe it won't change very much.",
                    "label": 0
                },
                {
                    "sent": "So policy gradients.",
                    "label": 0
                },
                {
                    "sent": "After all that math and all that, algebra is actually doing something very, very simple.",
                    "label": 0
                },
                {
                    "sent": "It's just saying make the good stuff more likely if you did something and you got a big reward, increase the probability of those actions.",
                    "label": 0
                },
                {
                    "sent": "Make the bad stuff less likely if you did something you got a really bad reward.",
                    "label": 1
                },
                {
                    "sent": "Decrease the probability of those actions.",
                    "label": 0
                },
                {
                    "sent": "Don't do that again.",
                    "label": 0
                },
                {
                    "sent": "So it's basically just a formalization of trial and error.",
                    "label": 1
                },
                {
                    "sent": "So after we did all that algebra, derived gradients and so on, all we really get is trial and error.",
                    "label": 0
                },
                {
                    "sent": "So that kind of makes makes sense.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, as a quick aside, I talked a lot about partial observe ability and Palm DP's in the beginning.",
                    "label": 0
                },
                {
                    "sent": "How does partial observe ability fit into all this?",
                    "label": 0
                },
                {
                    "sent": "Well, the answer is that actually fits into all this very very easily.",
                    "label": 0
                },
                {
                    "sent": "Nowhere in the derivation did I actually assume anything about the Markov property.",
                    "label": 0
                },
                {
                    "sent": "So if you have a partially observed system and you want to use policy gradients with this kind of Monte Carlo estimator.",
                    "label": 0
                },
                {
                    "sent": "Just go for it.",
                    "label": 0
                },
                {
                    "sent": "There's actually nothing else you have to do.",
                    "label": 0
                },
                {
                    "sent": "So you can just as well substitute O in place of S and everything will be fine.",
                    "label": 0
                },
                {
                    "sent": "So the Markov property is not actually used and you can use policy grade impartial observer entities without any modification whatsoever.",
                    "label": 0
                },
                {
                    "sent": "There's a little caveat here.",
                    "label": 0
                },
                {
                    "sent": "In practice.",
                    "label": 0
                },
                {
                    "sent": "You might want to use value functions together with policy gradient.",
                    "label": 0
                },
                {
                    "sent": "So in the previous lecture you learned about value functions into functions, value function.",
                    "label": 0
                },
                {
                    "sent": "Q functions can be combined with policy gradients to create work called actor critic algorithms.",
                    "label": 0
                },
                {
                    "sent": "I won't cover it with today's lecture because I'm going to focus on policy policy, grading policy search, but if you do end up using a policy gradient algorithm, do keep in mind that even though the policy gradient does not require Markovian states, your value function might.",
                    "label": 0
                },
                {
                    "sent": "So as a caveat, this definitely holds true when you're doing Monte Carlo returns.",
                    "label": 0
                },
                {
                    "sent": "If you're estimating a value function, that value function might require Markovian State.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so far we've painted kind of this rosy picture that you can just take your your MVP sample from it.",
                    "label": 0
                },
                {
                    "sent": "Estimate returns us to make gradients with by taking samples and all that good stuff and get something really simple, and it seems to kind of make sense.",
                    "label": 0
                },
                {
                    "sent": "So then of course we might ask, well, does it work?",
                    "label": 0
                },
                {
                    "sent": "I think if you go home and you implement the algorithm that I just described so far, you will find that it does not work.",
                    "label": 0
                },
                {
                    "sent": "You'll probably find that it doesn't work at all, probably not on anything now, probably not even on grid worlds.",
                    "label": 0
                },
                {
                    "sent": "And that's because policy gradients require a little more care to use properly.",
                    "label": 0
                },
                {
                    "sent": "There are a number of things wrong with the stuff that I described so far, but in in just a few words that can be summarized as the problem of variance.",
                    "label": 0
                },
                {
                    "sent": "So the policy gradient estimator described so far is very high variance.",
                    "label": 1
                },
                {
                    "sent": "What does that mean?",
                    "label": 0
                },
                {
                    "sent": "That means that if you generate some number of samples, let's say generate 10 samples, decimal policy gradient, I estimate my gradients.",
                    "label": 0
                },
                {
                    "sent": "I write it down and then I generate 10 more samples and I separately use those to estimate a different gradient and I compare these two things are going to be different.",
                    "label": 0
                },
                {
                    "sent": "So you generate the, you know, a small number of samples.",
                    "label": 0
                },
                {
                    "sent": "Take your gradients, generate some more samples to estimate a separated from that, and you do this a few times and you'll find there's a lot of variance between these estimates.",
                    "label": 0
                },
                {
                    "sent": "Of course there results in very noisy gradients, which means that instead of going right uphill to maximize your order, kind of going on this really noisy path and maybe you're going to miss your optimum.",
                    "label": 0
                },
                {
                    "sent": "So explaining why exactly policy great gradients have five variance is a little bit new involved, but I'll just give you an example of just one instance of a problem to policy grading has this is not the only explanation.",
                    "label": 0
                },
                {
                    "sent": "There are many, many others, but this is something that makes sense to me and hopefully it will make sense to the rest of you as well.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to show you this plot where the vertical axis is the total reward of a trajectory and the horizontal axis represents the trajectory.",
                    "label": 0
                },
                {
                    "sent": "So pretend that for some reason our trajectory's are 1 dimensional.",
                    "label": 0
                },
                {
                    "sent": "I can project them on a number line just for visualization.",
                    "label": 0
                },
                {
                    "sent": "So my policy you can do it as as kind of a function on this plot, so the trajectory is here at the peak of this little Gaussian shaped curve, the highest probability in this lower probability.",
                    "label": 0
                },
                {
                    "sent": "So I can kind of visualize my policy like this.",
                    "label": 0
                },
                {
                    "sent": "It's a distribution over structures.",
                    "label": 0
                },
                {
                    "sent": "And let's say that I generated three samples from that policy and the height of these samples represents the reward, so that sample on the left has very negative reward and the two samples on the right have slightly positive reward.",
                    "label": 0
                },
                {
                    "sent": "So I'm just visualizing three samples.",
                    "label": 0
                },
                {
                    "sent": "And now imagine that I'm going to use those samples to estimate a policy gradient and modify my policy based on that policy gradient, so that really negative sample is going to push the distribution away from it.",
                    "label": 0
                },
                {
                    "sent": "It wants to just reduce its probability, doesn't care which way you go, just so long as you go away and those two positive samples are going to slightly try to increase the probability at those locations.",
                    "label": 0
                },
                {
                    "sent": "So maybe if I modify my policy by using those samples, it'll become this dotted line.",
                    "label": 0
                },
                {
                    "sent": "So we'll try to put more mass on those positive samples and a lot less mass on that negative sample.",
                    "label": 0
                },
                {
                    "sent": "But negative samples strongly repulsing the policy.",
                    "label": 0
                },
                {
                    "sent": "And now what I'm going to do is I'm going to change my problem in a way that should not in any reasonable world affect the solution.",
                    "label": 0
                },
                {
                    "sent": "I'm going to add a constant to my reward function.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to raise the floor.",
                    "label": 0
                },
                {
                    "sent": "I'm going to add a constant everybody this can't change the solution because the optimum is still the same.",
                    "label": 0
                },
                {
                    "sent": "If you add a constant function, its maximum remains in the same spot.",
                    "label": 0
                },
                {
                    "sent": "But now that I've added a constant, all these rewards.",
                    "label": 0
                },
                {
                    "sent": "Now, if you imagine what the policy rate is doing is going to increase the probabilities of all these samples is going to be increasing the probability on the right bit more, but it's trying to increase all of them.",
                    "label": 0
                },
                {
                    "sent": "So now maybe if I modify the policy based on the gradient from these samples, maybe will try to spread out to cover those samples more.",
                    "label": 0
                },
                {
                    "sent": "If I had a really giant constant, if I had a constant that is so big that it dwarfs the difference between the samples, then it will just look a lot like maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "We'll just try to cover everything.",
                    "label": 0
                },
                {
                    "sent": "So this is the problem.",
                    "label": 0
                },
                {
                    "sent": "5 areas.",
                    "label": 0
                },
                {
                    "sent": "You can imagine an even worse scenario.",
                    "label": 0
                },
                {
                    "sent": "What if the good samples just happen to have a reward of 0?",
                    "label": 0
                },
                {
                    "sent": "Now they'll be completely ignored in the gradient and all the training will do is run away from the negative sample.",
                    "label": 0
                },
                {
                    "sent": "So the direction that the policy will go depends entirely on where it starts.",
                    "label": 0
                },
                {
                    "sent": "It starts to the right it will run away to the right of it, starts.",
                    "label": 0
                },
                {
                    "sent": "The left will run away to the left.",
                    "label": 0
                },
                {
                    "sent": "So seemingly innocuous changes just completely changed my gradient.",
                    "label": 0
                },
                {
                    "sent": "I should say in the limit of infinite samples I will get the right answer.",
                    "label": 0
                },
                {
                    "sent": "So if I have the luxury to generate all possible samples, I will get the right answer no matter what I do to the reward.",
                    "label": 0
                },
                {
                    "sent": "But for fire number of samples I have this problem of variance.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a lot of the games that we play in order to make policy gradients actually useful involve somehow reducing the variance and the good news is that there are many things that we can do to reduce the variance of policy gradients without actually changing the expectation.",
                    "label": 0
                },
                {
                    "sent": "So in expectation you want to get the right answer.",
                    "label": 0
                },
                {
                    "sent": "That means that your estimator is unbiased.",
                    "label": 0
                },
                {
                    "sent": "If you hear unbiased, that just means in expectation get the right answer.",
                    "label": 0
                },
                {
                    "sent": "But we can actually make changes that will keep us unbiased, but actually reduce variance, oftentimes reduce variance quite a lot.",
                    "label": 0
                },
                {
                    "sent": "One very simple change we can make the policy gradient.",
                    "label": 0
                },
                {
                    "sent": "Is to exploit our knowledge of the fact that our universe has causality.",
                    "label": 0
                },
                {
                    "sent": "Causality means that things in the future don't cause things.",
                    "label": 0
                },
                {
                    "sent": "In the past.",
                    "label": 0
                },
                {
                    "sent": "This is not a property of any particular MDP, is just like a fact about the universe and we can use that fact.",
                    "label": 0
                },
                {
                    "sent": "So causality means that what your policy does at time T prime cannot possibly affect the reward at time T if T is less than T prime.",
                    "label": 0
                },
                {
                    "sent": "Write something I do in the future is not going to change my reward in the past and we can use this.",
                    "label": 0
                },
                {
                    "sent": "We can use this fact to modify the policy gradient estimator.",
                    "label": 0
                },
                {
                    "sent": "Here's how if you look at this product in the policy grade and you'll notice there's a product of a sum of grad log.",
                    "label": 0
                },
                {
                    "sent": "Pi is the gradients of log probabilities of actions times a sum of rewards.",
                    "label": 0
                },
                {
                    "sent": "So if I change the probability of an action at time step one or sorry at time step 10, that's actually multiplied by their wartime step one.",
                    "label": 0
                },
                {
                    "sent": "So somehow rewarded time stuff one affects my actual time Step 10, which I know is a relationship that doesn't hold in our universe because of causality.",
                    "label": 0
                },
                {
                    "sent": "So what I can do is I can actually make a very simple change the policy gradient 1st.",
                    "label": 0
                },
                {
                    "sent": "I'm going to distribute the sum over grad log pies outside, so that's just the distributive property.",
                    "label": 0
                },
                {
                    "sent": "So I can, equivalently, I haven't made any change yet.",
                    "label": 0
                },
                {
                    "sent": "I've just applied the distributive property and then I get the policy gradient estimated some overtime of all these gridlock pies times the total reward of the trajectory.",
                    "label": 0
                },
                {
                    "sent": "And now from here I can clearly see that the probability of the action at every time step is affected by rewards both in the past and in the future.",
                    "label": 0
                },
                {
                    "sent": "So now I can apply my knowledge of the fact that causality holds to remove the dependence on past rewards.",
                    "label": 0
                },
                {
                    "sent": "So the dependence on past rewards is removed simply by changing that one tuati right there, so I just said every time I'm going to decide which way to change the probability of an action, I'm only going to look at the rewards from the current time step until the end.",
                    "label": 0
                },
                {
                    "sent": "I won't care bout past rewards because I know that changing the probability of action right now is not going to change rewards I've already received because of causality.",
                    "label": 0
                },
                {
                    "sent": "And this thing is sometimes referred to, especially in controls, as the reward to go.",
                    "label": 1
                },
                {
                    "sent": "The rewards that you have left to go in the future, current and future.",
                    "label": 0
                },
                {
                    "sent": "And sometimes it's written as Q hat, and for those of you that are curious about actor critic algorithms, yes, this Q hat is exactly what the critic in the actor critic algorithm would change.",
                    "label": 0
                },
                {
                    "sent": "They would estimate the skew had in other ways, not necessarily just by using Monte Carlo returns, but for now we're talking about policy gradients.",
                    "label": 0
                },
                {
                    "sent": "So Q had is just a Monte Carlo estimate of the reward from now until the end of time, but not including the past.",
                    "label": 0
                },
                {
                    "sent": "Making this change has reduced the variance of our policy gradients for a very very simple reason.",
                    "label": 0
                },
                {
                    "sent": "The variance of some estimate depends on the magnitude of a function.",
                    "label": 0
                },
                {
                    "sent": "If I take the function, I make it 10 times bigger.",
                    "label": 0
                },
                {
                    "sent": "I have bigger variance if I make it smaller, I have smaller variance by removing terms from the sum, I have smaller coefficients, multiply my grad log pies.",
                    "label": 0
                },
                {
                    "sent": "Therefore, I'm going to have lower variance.",
                    "label": 0
                },
                {
                    "sent": "So in practice, there's basically no reason not to do this if you're implementing policy gradient.",
                    "label": 0
                },
                {
                    "sent": "Just just do this.",
                    "label": 0
                },
                {
                    "sent": "It helps.",
                    "label": 0
                },
                {
                    "sent": "In fact, I can't think of any case where it would hurt.",
                    "label": 0
                },
                {
                    "sent": "Just always do this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is another thing that you should always do because it helps a lot and to my knowledge also doesn't hurt, although here you have to be a little bit careful because you can make some bad decisions.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to describe something called a baseline.",
                    "label": 0
                },
                {
                    "sent": "Let's go back to this little intuitive explanation I talked about before I talk about home policy.",
                    "label": 0
                },
                {
                    "sent": "Gradients are kind of a formalization of trial and error.",
                    "label": 0
                },
                {
                    "sent": "You want to make the good stuff more likely in the bad stuff.",
                    "label": 0
                },
                {
                    "sent": "Less likely that all made a lot of sense to us, right?",
                    "label": 0
                },
                {
                    "sent": "You you generate these samples.",
                    "label": 0
                },
                {
                    "sent": "The good ones get higher probability.",
                    "label": 0
                },
                {
                    "sent": "The bad ones get lower probability.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, that story was a little bit of a lie.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit of a lie because of that example I gave where you add a constant to the reward.",
                    "label": 0
                },
                {
                    "sent": "So of course if you're bad reward is negative, you'll decrease the probability.",
                    "label": 0
                },
                {
                    "sent": "What if your worst reward the worst toward you saw in that iteration?",
                    "label": 0
                },
                {
                    "sent": "Is actually positive.",
                    "label": 0
                },
                {
                    "sent": "You're not going to be necessarily decreasing the probability you might actually increase it.",
                    "label": 0
                },
                {
                    "sent": "So adding this constant of the rewards actually changes behavior.",
                    "label": 0
                },
                {
                    "sent": "It makes it not look quite so much like trial and error.",
                    "label": 0
                },
                {
                    "sent": "So the intuition is that, well, maybe maybe the.",
                    "label": 0
                },
                {
                    "sent": "Maybe the trial and error thing is what we actually want.",
                    "label": 0
                },
                {
                    "sent": "Maybe we want to not just increase or decrease probabilities based on whether they got big or small reward.",
                    "label": 0
                },
                {
                    "sent": "Maybe want to increase or decrease probabilities based on whether the reward was better or worse than average.",
                    "label": 0
                },
                {
                    "sent": "So we compute our average reward and we can say well, if you saw something better than average, make it more likely.",
                    "label": 0
                },
                {
                    "sent": "If you saw something worse than average, make it less likely that actually seems to make more sense than what policy gradient seems to be doing.",
                    "label": 0
                },
                {
                    "sent": "Like somehow it really makes sense.",
                    "label": 0
                },
                {
                    "sent": "You should make the stuff that's above average more likely, and the stuff below average less likely, regardless of its absolute value.",
                    "label": 0
                },
                {
                    "sent": "But are we actually allowed to do that?",
                    "label": 1
                },
                {
                    "sent": "Is that like, is that mathematically permitted like this is still a correct algorithm?",
                    "label": 0
                },
                {
                    "sent": "And surprisingly, turns out the answer is yes.",
                    "label": 0
                },
                {
                    "sent": "It turns out that if you take a constant and you subtract it from all of your rewards at a particular iteration, you still have an unbiased estimator.",
                    "label": 0
                },
                {
                    "sent": "The policy gradient, and we can prove this.",
                    "label": 0
                },
                {
                    "sent": "So the way that we prove it is we take that second term.",
                    "label": 0
                },
                {
                    "sent": "So you distribute grad log Pi, you get grad log \u03c0 * R minus gridlock \u03a0 times B and you can actually show that the grad log \u03c0 * B.",
                    "label": 0
                },
                {
                    "sent": "In expectation is zero, which means if you generate infinite samples, that baseline B makes no difference.",
                    "label": 0
                },
                {
                    "sent": "But of course when you have a finite number of samples is going to reduce your variance.",
                    "label": 0
                },
                {
                    "sent": "So here's how we can prove that the expectation of grad log \u03a0 times B is 0.",
                    "label": 0
                },
                {
                    "sent": "Just like before, we're going to write out from the definition of expectation we're going to write out the integral, so it's an integral of Pi of Tau times grad log pile of tile times B.",
                    "label": 0
                },
                {
                    "sent": "And we're going to go back to our convenient identity.",
                    "label": 0
                },
                {
                    "sent": "This is the same convenient if we had before, except now we're going to play it in reverse.",
                    "label": 0
                },
                {
                    "sent": "So before we use that identity, go from grad \u03c0 to \u03a0 times grad, log \u03c0, and now we're going to go backwards, we're going to go from Pi gridlock pie back to grad pie.",
                    "label": 0
                },
                {
                    "sent": "So we apply our data in reverse.",
                    "label": 0
                },
                {
                    "sent": "So now we have an integral of grad \u03c0 * B.",
                    "label": 0
                },
                {
                    "sent": "By linearity we can take the and the gradient operator outside of the integral.",
                    "label": 0
                },
                {
                    "sent": "So now we have B times the gradient of the integral of Pi of top.",
                    "label": 0
                },
                {
                    "sent": "So probability distributions have very convenient property that if you integrate a probability distribution over the domain, you get one right because probability distributions have to be normalized so that integral it's actually one.",
                    "label": 0
                },
                {
                    "sent": "And of course the gradient of 1.",
                    "label": 0
                },
                {
                    "sent": "Is 0.",
                    "label": 0
                },
                {
                    "sent": "So that's equal to B times the gradient of 1, which is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So that means that in expectation.",
                    "label": 0
                },
                {
                    "sent": "Any value of be.",
                    "label": 0
                },
                {
                    "sent": "Is legitimate.",
                    "label": 0
                },
                {
                    "sent": "You can pick any value of B.",
                    "label": 0
                },
                {
                    "sent": "You want an expectation.",
                    "label": 0
                },
                {
                    "sent": "You will still get the same answer.",
                    "label": 0
                },
                {
                    "sent": "Of course, for a finite number of samples you will not get the same answer if a finite number of samples choosing be carefully can actually reduce your variance.",
                    "label": 0
                },
                {
                    "sent": "So subtracting a baseline is unbiased and expectation.",
                    "label": 0
                },
                {
                    "sent": "Average reward is a very good choice of baseline to use.",
                    "label": 0
                },
                {
                    "sent": "It is not the optimal choice of baseline.",
                    "label": 0
                },
                {
                    "sent": "You can figure out what is an optimal baseline by writing down the equation for the variance of an estimator.",
                    "label": 0
                },
                {
                    "sent": "You can take the derivative of that equation with respect to be set that derivative zero and solve for B and you can actually find out what is the optimal baseline.",
                    "label": 0
                },
                {
                    "sent": "The optimal baseline is actually different for every coordinate of your gradient.",
                    "label": 0
                },
                {
                    "sent": "But average reward, while it's not the best baseline, is not the optimal baseline.",
                    "label": 1
                },
                {
                    "sent": "It's actually a very good choice in practice.",
                    "label": 0
                },
                {
                    "sent": "So if you're implementing policy gradient and you just want to code something up that works fairly quickly, average reward is a good choice.",
                    "label": 0
                },
                {
                    "sent": "But there are better choices.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have.",
                    "label": 0
                },
                {
                    "sent": "An average toward baseline and you use the fact that causality holds in our universe to write out that reward to go thingy.",
                    "label": 0
                },
                {
                    "sent": "You could actually get a previous unexpected implementation of policy gradient that will work on many problems, yes.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "The estimate the running average of your reward.",
                    "label": 0
                },
                {
                    "sent": "Oh, I see.",
                    "label": 0
                },
                {
                    "sent": "Oh, of course yeah.",
                    "label": 0
                },
                {
                    "sent": "So any choice of B is is going to be unbiased here, so you can estimate be using samples from your current iteration.",
                    "label": 0
                },
                {
                    "sent": "You can estimate be using samples from previous iterations any any way of estimating quantities?",
                    "label": 0
                },
                {
                    "sent": "Find a very very simple default implementation.",
                    "label": 0
                },
                {
                    "sent": "If you're doing kind of a batch mode policy gradient is to just average to go the rolls on that iteration.",
                    "label": 0
                },
                {
                    "sent": "We can do many other things.",
                    "label": 0
                },
                {
                    "sent": "In fact, one thing that I won't go into here, but that is worth keeping in mind, is that you can also make the baseline depend on state, so you can actually have a function of state.",
                    "label": 0
                },
                {
                    "sent": "And that can give you an even lower variance.",
                    "label": 0
                },
                {
                    "sent": "You can actually fit a function approximator to rewards as a function of state.",
                    "label": 0
                },
                {
                    "sent": "If you make it a function of action, then things get really complicated.",
                    "label": 0
                },
                {
                    "sent": "Then it's much harder to make it unbiased.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now that I've introduced these pieces.",
                    "label": 0
                },
                {
                    "sent": "We can actually try to put them together into something that looks like an algorithm that we can actually implement, and one thing that I think is maybe ills have to do at this point is to show how we can go from an implementation of a maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "This is in tensor flow to an implementation of policy gradient, so for those of you who aren't familiar with tensor flow, I will walk through this.",
                    "label": 0
                },
                {
                    "sent": "This is literally 4 lines of code we're going to.",
                    "label": 0
                },
                {
                    "sent": "This is just setting up regular maximum like the regular supervised learning.",
                    "label": 0
                },
                {
                    "sent": "I'm going to turn into policy gradient 2nd.",
                    "label": 1
                },
                {
                    "sent": "So here I assume that I'm given two things, a tensor of actions and intensive states.",
                    "label": 1
                },
                {
                    "sent": "So the tensor of actions is just going to be is actually just a matrix that has 2 dimensions.",
                    "label": 0
                },
                {
                    "sent": "The first dimension is the number of samples times the number of time steps.",
                    "label": 0
                },
                {
                    "sent": "So just I just took all of my trajectories and just concatenate into one really really big row and the second dimension is dimensionality of the actions, so N is the number of samples that is the length of my horizon D as the dimensionality action so actions.",
                    "label": 0
                },
                {
                    "sent": "Is just a matrix, first dimension is N * T, second dimension is DA.",
                    "label": 1
                },
                {
                    "sent": "States.",
                    "label": 0
                },
                {
                    "sent": "Is a tensor of end time Steve ideas?",
                    "label": 0
                },
                {
                    "sent": "Dimensionality of my state equivalent?",
                    "label": 0
                },
                {
                    "sent": "This could be observations.",
                    "label": 0
                },
                {
                    "sent": "It would make no difference.",
                    "label": 0
                },
                {
                    "sent": "So if you want to implement maximum likelihood in Tensorflow, the first thing that you would do is you would ask your policy to actually make predictions for those states so that first line gives you the log probabilities from your function approximator.",
                    "label": 0
                },
                {
                    "sent": "So policy some kind of neural net in this case, or some other kind of parameterized function, and you can call.",
                    "label": 0
                },
                {
                    "sent": "This is just a made up thing dot predictions that basically says do a forward pass feed in the States and give me the outputs and the outputs.",
                    "label": 0
                },
                {
                    "sent": "Here are going to be log probabilities of different.",
                    "label": 0
                },
                {
                    "sent": "Labels.",
                    "label": 0
                },
                {
                    "sent": "Then the second line.",
                    "label": 0
                },
                {
                    "sent": "This negative likelihood is basically evaluates a cross entropy loss that basically amounts to saying, well, give me the probability of the right of the label.",
                    "label": 0
                },
                {
                    "sent": "Given the what my network output is, so the network outputs a log probability for every possible label, that second line evaluates the probability the log probability of the actual actions and the actions tensor.",
                    "label": 0
                },
                {
                    "sent": "So that's a correct cross entropy loss in Tensorflow.",
                    "label": 0
                },
                {
                    "sent": "The third line just says average them together so it says OK, Now you have a probability for every single N * T. Average them altogether, get one scalar value and the fourth line says compute the gradients.",
                    "label": 0
                },
                {
                    "sent": "So if you write this out, assuming that you know policy dot predictions a function you actually implemented, this will give you the gradient of the likelihood.",
                    "label": 0
                },
                {
                    "sent": "This is regular supervised learning.",
                    "label": 0
                },
                {
                    "sent": "If we want to turn this into policy gradients.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We all we have to do is add this stuff in red so we have one more input that we need to know which is these rewards to go these Q values.",
                    "label": 0
                },
                {
                    "sent": "I assume that those have already been precomputed, so that's just the sum of the reward from time step T until the end capital T and that can be expressed as an end times T by one tensor is just a bunch of values for every sample that I have.",
                    "label": 0
                },
                {
                    "sent": "And all I'm going to do is after computing those negative likelihoods, I'm going to wait them by the rewards to go.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to point wise multiplication.",
                    "label": 0
                },
                {
                    "sent": "And then when I do the reduced mean, I just plug in these weighted likelihood instead of the original ones.",
                    "label": 0
                },
                {
                    "sent": "Now of course you have to actually compute those Q values, which is where you would put in the baseline and all that causality stuff and so on.",
                    "label": 0
                },
                {
                    "sent": "But assuming they have computed the Q values, the only thing that you have to do in order to implement policy gradient is just add that one line and use the weighted likelihoods.",
                    "label": 0
                },
                {
                    "sent": "That will give you a policy gradient implementation.",
                    "label": 0
                },
                {
                    "sent": "So even though we have went through a lot of math, actually implementing this is fairly straightforward.",
                    "label": 0
                },
                {
                    "sent": "So this is basically directly implementing this equation, so the loss is this Jay~ and when you called gradient on the loss then you get the basically puts the gradient in front of log pie and that gives you a policy gradient estimator.",
                    "label": 0
                },
                {
                    "sent": "Does anybody have any questions about the pseudo code?",
                    "label": 0
                },
                {
                    "sent": "This is a good time to ask some questions.",
                    "label": 0
                },
                {
                    "sent": "Yes, back there.",
                    "label": 0
                },
                {
                    "sent": "Sorry, what's the negative likelihood?",
                    "label": 0
                },
                {
                    "sent": "So the negative likelihood it's just it's negative because typically use a minimizer rather than the maximizer and all it's doing is evaluating these log pies.",
                    "label": 0
                },
                {
                    "sent": "So likelihood is log \u03c0 in this case.",
                    "label": 0
                },
                {
                    "sent": "When you do weighted, when you waited, when you do the pointwise multiplication, you're taking each of those log pies and you're multiplying them by the reward to go by the Q hat.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "For the rewards.",
                    "label": 0
                },
                {
                    "sent": "Here are.",
                    "label": 0
                },
                {
                    "sent": "Oh, you said it be a state dependent yeah so so if the state dependent then the proof has to be modified a little bit.",
                    "label": 0
                },
                {
                    "sent": "You have to actually you can do it at the level of towels you have to actually put in the S as in the days.",
                    "label": 0
                },
                {
                    "sent": "But if you do it at that level it still falls out of the integration because the probabilities are over a not over S, so that grad log paetow.",
                    "label": 0
                },
                {
                    "sent": "It's actually a sum of log Pi a given S. So they only only ever have probabilities over a not over S in there.",
                    "label": 0
                },
                {
                    "sent": "So if you actually put in the SNES, then you can repeat the same proof even when the baseline depends on state.",
                    "label": 0
                },
                {
                    "sent": "Any other questions here?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, the result for that is a little bit more involved, but I have a citation in a few slides that you can take a look at if you want to see the proof of that.",
                    "label": 0
                },
                {
                    "sent": "Yes, back there.",
                    "label": 0
                },
                {
                    "sent": "Yes, I'm assuming that whatever baseline, whatever thing you're doing is already been done.",
                    "label": 0
                },
                {
                    "sent": "That's right, yes.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "This is actually an important point.",
                    "label": 0
                },
                {
                    "sent": "Is that if whatever you did to compute the you know any of these things actually involved the parameters their policy for some reason.",
                    "label": 0
                },
                {
                    "sent": "Make sure that you're not actually back popping through it so you know if you want like a very simple tip about how to implement this, you can just.",
                    "label": 0
                },
                {
                    "sent": "If you're doing this in Python, do all of your reward stuff in, like NUM py, it's just simple arithmetic and then just use the audited package for actually performing this stuff.",
                    "label": 0
                },
                {
                    "sent": "So if you do all that you know just outside of your audit package, you don't need all the different about all those things.",
                    "label": 0
                },
                {
                    "sent": "Then everything will be fine.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that's basically the theory.",
                    "label": 0
                },
                {
                    "sent": "Now are few practical tips.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Unfortunately, the stuff that I described is only part of the story.",
                    "label": 1
                },
                {
                    "sent": "It will allow you to implement a policy grain algorithm that works on simple problems, but there are a number of things that you should try to do if you want things that really work on more complicated problems, so policy gradients have high variance.",
                    "label": 0
                },
                {
                    "sent": "The tricks that I described will mitigate that variance, but there are other things you can do, so using an actor critic algorithm, we have a critic in addition to the policy.",
                    "label": 0
                },
                {
                    "sent": "Basically, a value function estimator can be used to reduce variance even further.",
                    "label": 0
                },
                {
                    "sent": "In fact, quite a lot, and that can be really important for practical implementations.",
                    "label": 1
                },
                {
                    "sent": "Choosing the step size for policy gradient is very difficult.",
                    "label": 1
                },
                {
                    "sent": "It's usually much harder than choosing a step size for SGD.",
                    "label": 1
                },
                {
                    "sent": "These kind of vanilla policy gradients, if you see that renewal policy, great, just referring to type of policy.",
                    "label": 0
                },
                {
                    "sent": "Gradient described here can be very hard to use.",
                    "label": 0
                },
                {
                    "sent": "So there are a few things that you can do to make them easier to use and to make a step size adjustment more automatic.",
                    "label": 0
                },
                {
                    "sent": "One of the things that's really useful is to use natural gradients, trust regions, and so on, so those of you that are familiar with natural gradients is basically it's a way to incorporate some higher order information into your gradient estimator.",
                    "label": 0
                },
                {
                    "sent": "We typically don't use natural gradients when we train, for example, deep Nets with supervised learning, but for policy gradient, natural gradients can make a big difference, so that essentially amounts of preconditioning your gradient by a matrix called the Fisher Information matrix.",
                    "label": 0
                },
                {
                    "sent": "So it's a matrix that describes the local geometry of the probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So if you want to find out about this, look up natural policy gradient and you can also look up trust region policy optimization.",
                    "label": 1
                },
                {
                    "sent": "These two things are very similar, just one one of them picks a step size, the other one picks a trust region in probability space.",
                    "label": 0
                },
                {
                    "sent": "It's very useful to use automated step size adjustment techniques, so if you're used to doing everything with Judy and momentum and you're a real wizard at tuning your SGD learning rates, maybe you might consider using a more automated thing like Adam.",
                    "label": 0
                },
                {
                    "sent": "If you're doing policy gradient, it's not required, but it will make your life a bit easier.",
                    "label": 0
                },
                {
                    "sent": "Find other ways to reduce your variance so things like critics and actor critic algorithms, fancier baselines, maybe baselines that depend on state.",
                    "label": 0
                },
                {
                    "sent": "These things can help.",
                    "label": 1
                },
                {
                    "sent": "So use a baseline.",
                    "label": 0
                },
                {
                    "sent": "You can also use just a huge batch size.",
                    "label": 0
                },
                {
                    "sent": "Bigger batch sizes tend to make things more stable for policy gradient, so policy grading methods in general the way they stuck up to other reinforcement learning techniques, they can be very very stable.",
                    "label": 0
                },
                {
                    "sent": "In fact, if set up correctly there can be some of the most stable reinforcement learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "They can also be some of the most inefficient ones, so good choice if you have, you know cheap simulator we can generate lots of data, maybe not such a great choice if you want something super efficient but also a good choice if you want something fairly stable provided that you do all the tricks.",
                    "label": 0
                },
                {
                    "sent": "Properly.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is an example just to just to show you some videos.",
                    "label": 0
                },
                {
                    "sent": "Since you've all been listening patiently, patiently to me for about an hour time.",
                    "label": 0
                },
                {
                    "sent": "For some animations, these are from a paper on a trust region policy gradient methods, so there's a policy grading method that uses some 2nd order information to impose a trust region, or how much the distribution changes.",
                    "label": 0
                },
                {
                    "sent": "So it's a kind of a natural gradient with automatic step size adjustment.",
                    "label": 1
                },
                {
                    "sent": "Handles discreet and continuous actions, and there's actual code available for this algorithm, so if you look up the benchmarking paper by dawn at all, so this is some work that we did with John Schulman in 2014, 2015, we, to my knowledge, is actually one of the earlier kind of from the recent incarnation of deep policy grading methods.",
                    "label": 0
                },
                {
                    "sent": "We use it for Atari Simulator robotics tasks and so on, so if you want kind of a very reasonable starting point, just something to get started, get download the code for this method, try using it.",
                    "label": 0
                },
                {
                    "sent": "It'll probably be a decent starting point.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some suggested readings, so first some classical papers.",
                    "label": 1
                },
                {
                    "sent": "The reinforce algorithm that I described appeared to my knowledge for the first time, although maybe it appeared earlier too.",
                    "label": 0
                },
                {
                    "sent": "But to my knowledge, for the first time it appeared in this paper from 1992 by Williams.",
                    "label": 0
                },
                {
                    "sent": "The thing that I called causality this sometimes has been called the policy gradient theorem or the G Palm DP estimator appeared in several papers, this one by rich sudden actually and the subsequent paper by Backstrom, Bartlett.",
                    "label": 0
                },
                {
                    "sent": "Those are good references if you want to find out about that and the question earlier about the proof for why this is correct, you can find that in those papers.",
                    "label": 0
                },
                {
                    "sent": "If you want sort of a more recent paper kind of survey style paper to learn about policy gradients, especially natural policy gradient, I would highly recommend this paper by Peterson Shawl.",
                    "label": 1
                },
                {
                    "sent": "It's not actually the 1st paper, introduce it, but it's a it's a good reference if you want something written in accessible style with all the math that you can find out about.",
                    "label": 0
                },
                {
                    "sent": "For more recent papers, if you want to find out about deep reinforcement learning policy gradients, you can check out these two papers by John Schulman, the Trust region policy optimization and approximate policy optimization.",
                    "label": 1
                },
                {
                    "sent": "There are many more papers on this topic, but these are kind of two fairly decent more recent papers.",
                    "label": 1
                },
                {
                    "sent": "They can check out.",
                    "label": 1
                },
                {
                    "sent": "And if you want to practice on your own, there is a homework assignment that you can try.",
                    "label": 0
                },
                {
                    "sent": "Actually from my graduate class at UC Berkeley.",
                    "label": 0
                },
                {
                    "sent": "So if you can go on GitHub, find the homework assignment there and you can try out implementing policy gradient and trying it out on some problems yourself.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that was the model free policy search story.",
                    "label": 1
                },
                {
                    "sent": "Now in the time I have remaining, I want to tell you a little bit about what happens when you know the model.",
                    "label": 0
                },
                {
                    "sent": "Basically, going back to the example of Richard Dawkins, if you know the differential equations that govern how the ball is going to fly through the air, and then say a few words about what happens if you don't know the model but are willing to learn it.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when we talked about reinforcement learning before we had this reinforced learning objective and we said that our objective is the expectation of our total reward under some distribution, the distribution depends on a few things.",
                    "label": 0
                },
                {
                    "sent": "It depends of course on your policy, but also depends on your transition dynamics and the probability of the next state given the current state and action.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in model free reinforcement learning, we basically assume that we don't know this thing and not only do we assume that we don't know it, we also assume that we're kind of not willing to learn it.",
                    "label": 0
                },
                {
                    "sent": "Which is OK in some cases, like sometimes it is really easier to figure out how to catch the ball then it is to figure out how gravity and wind resistance act on it, so it's OK to not want to learn that that quantity.",
                    "label": 0
                },
                {
                    "sent": "But sometimes maybe you do want to learn it.",
                    "label": 1
                },
                {
                    "sent": "Maybe you'll have a more flexible method if you can learn, maybe you can learn that model and use it for many different things and there will be more efficient.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what if we knew the transition dynamics?",
                    "label": 1
                },
                {
                    "sent": "I'm going to talk about what happens if we know them, and then I'm going to talk about how we can learn them.",
                    "label": 0
                },
                {
                    "sent": "So if we knew the transition dynamics, which we often do, we can use them to help us acquire policies and help us make decisions so we know the dynamics in many cases.",
                    "label": 0
                },
                {
                    "sent": "For example, if you're playing a game, if you're playing the game of go or chess, or if you're playing a video game, you probably know the transition dynamics.",
                    "label": 0
                },
                {
                    "sent": "Some systems are easy to model, so maybe you don't know exactly all the physical physical nuances that govern a car that's on the road, but the kinematics of that system sort of the high level behavior can be modeled pretty easily, and while it might miss some of the nuances, it might be good enough to make meaningful decisions.",
                    "label": 0
                },
                {
                    "sent": "And of course, maybe you're dealing with simulated environment like a simulated robot or a video game where there's some piece of code that actually implements the model.",
                    "label": 0
                },
                {
                    "sent": "So then you do actually know the model you have access to it.",
                    "label": 1
                },
                {
                    "sent": "And oftentimes, when you don't know the model, you can learn it.",
                    "label": 0
                },
                {
                    "sent": "So in robotics we have this thing called system identification which deals with a setting where you, basically, you know, the laws of physics.",
                    "label": 0
                },
                {
                    "sent": "You know how forces amasses interact, but you might not know the particular properties of your system.",
                    "label": 0
                },
                {
                    "sent": "You might not know the particular mass of your robot, but you have a physics textbook.",
                    "label": 0
                },
                {
                    "sent": "So that's basically refers to a system identification.",
                    "label": 0
                },
                {
                    "sent": "You can learn a model with a black box learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "You can take a big neural net and try to fit your model to that.",
                    "label": 0
                },
                {
                    "sent": "That's fine too.",
                    "label": 1
                },
                {
                    "sent": "So does knowing the dynamics make things easier?",
                    "label": 0
                },
                {
                    "sent": "Oftentimes yes it does, but there are a few details that we have to get right as usual.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll go through this part pretty quickly because I do want to kind of get to the learning part as well, but one important detail when it comes to using a known dynamics models.",
                    "label": 0
                },
                {
                    "sent": "It comes down to whether you have a stochastic or deterministic model and whether you're doing open loop or closed loop planning.",
                    "label": 0
                },
                {
                    "sent": "So let me explain this distinction because this distinction is important, I think it's often not not not appreciate it very much.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you have a deterministic model, meaning that the world is, you know, is governed by the deterministic equation that you know USD plus one is equal to F of sdat.",
                    "label": 0
                },
                {
                    "sent": "You can make decisions as following.",
                    "label": 0
                },
                {
                    "sent": "Your agent asks the world which State am I am just tell me just my first state.",
                    "label": 0
                },
                {
                    "sent": "Eurasian plans planning means you're going to select a sequence of actions that you believe will give you high reward under your current known dynamics.",
                    "label": 0
                },
                {
                    "sent": "The agent sends that plan back to the world that says, here's what I want to do.",
                    "label": 0
                },
                {
                    "sent": "This is what I'm going to do at time step one.",
                    "label": 0
                },
                {
                    "sent": "This is what I'm going to test it.",
                    "label": 0
                },
                {
                    "sent": "I'm stuck to this one.",
                    "label": 0
                },
                {
                    "sent": "I'm going to do a time step 300.",
                    "label": 0
                },
                {
                    "sent": "This is my plan.",
                    "label": 0
                },
                {
                    "sent": "I'm going to commit to that plan and execute it.",
                    "label": 0
                },
                {
                    "sent": "So that kind of problem can be solved with.",
                    "label": 0
                },
                {
                    "sent": "You can write it out as a constraint optimization problem.",
                    "label": 0
                },
                {
                    "sent": "You can write it out as an unconstrained optimization problem basically amounts to choosing a sequence of actions, A one through 80 that maximize your total reward over States and actions subject to the constraint that your states are governed by your dynamics, such as the constraint the plus one is equal to F of sdat.",
                    "label": 0
                },
                {
                    "sent": "We sometimes refer to this as open loop control.",
                    "label": 0
                },
                {
                    "sent": "Because you're going to commit to a sequence of actions and then execute them in the world without regard to what actually happens.",
                    "label": 0
                },
                {
                    "sent": "If your dynamics were correct.",
                    "label": 0
                },
                {
                    "sent": "If F, if F really is the true transition function for the world, this is going to work fine.",
                    "label": 0
                },
                {
                    "sent": "However, if the world is stochastic or your F is incorrect, this can cause you a lot of problems.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's think about the stochastic case in the stochastic case, you don't have just a deterministic function.",
                    "label": 1
                },
                {
                    "sent": "If you have some distribution, so you can write down, for example, what's the distribution over states.",
                    "label": 0
                },
                {
                    "sent": "You'll see conditional sequence of actions, and it's going to be random.",
                    "label": 0
                },
                {
                    "sent": "And you can set up the same kind of optimization that I had in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "You can say, well, let's choose a sequence of actions that maximize my total reward in expectation given my known probability of states given given actions.",
                    "label": 0
                },
                {
                    "sent": "This is the analogue of planning a stochastic environment.",
                    "label": 0
                },
                {
                    "sent": "It's a reasonable thing to do, but now it might actually be a little bit suboptimal, because when you commit to a plan of action, you're not actually responding to the States you actually visit, so state S2, for example, is going to be a random consequences of the state S1.",
                    "label": 0
                },
                {
                    "sent": "Any action A1?",
                    "label": 0
                },
                {
                    "sent": "Once you actually execute a one, you get to observe as to we get to actually see what happened, and once you see us too.",
                    "label": 0
                },
                {
                    "sent": "You might choose a different action A2.",
                    "label": 0
                },
                {
                    "sent": "Then you would have chosen if you didn't know what it was.",
                    "label": 0
                },
                {
                    "sent": "So open loop planning in the stochastic case can be extremely suboptimal because it basically amounts of saying I don't care what's going to happen next.",
                    "label": 0
                },
                {
                    "sent": "Here's my plan of action.",
                    "label": 0
                },
                {
                    "sent": "And you construct problems with arbitrarily bad.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I keep using this word loop.",
                    "label": 0
                },
                {
                    "sent": "What does loop mean?",
                    "label": 0
                },
                {
                    "sent": "Well when someone says that I have a closed loop controller, what they mean is they have a controller or a policy that's actually looking at the state and responding the state as it happens.",
                    "label": 0
                },
                {
                    "sent": "So the world produces state St.",
                    "label": 0
                },
                {
                    "sent": "The agent responds with an action 80 and then looks at the next status T + 1.",
                    "label": 1
                },
                {
                    "sent": "So it's called closed loop because there's a closed loop between the agent and the world.",
                    "label": 0
                },
                {
                    "sent": "When I say have an open loop policy or an open loop controller, that means that I'm not going to pay attention to what's actually happening around me.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to plan of action execute that plan.",
                    "label": 0
                },
                {
                    "sent": "You can get pretty far with open loop control, but closed loop control can be better, especially for stochastic systems or systems where you haven't identified the dynamics perfectly.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you want to do close loop control then that's where policy search comes in.",
                    "label": 0
                },
                {
                    "sent": "So even though you could use your known model to commit to a plan of action directly without even having a policy, if you actually optimize the policy, then you can do close loop control so you can actually choose pie and then you can maximize the expected reward under PIE and that will actually model the fact that you get to change your mind about which actually take after you observe.",
                    "label": 0
                },
                {
                    "sent": "The state actually resulted from your action.",
                    "label": 0
                },
                {
                    "sent": "So This is why when we haven't known model.",
                    "label": 0
                },
                {
                    "sent": "If it's the Catholic, or if it's or if it's not fully identified, we might actually choose to do policy search even though we don't have to.",
                    "label": 0
                },
                {
                    "sent": "We could just do planning, but the policy search setting will help us by allowing us to do close loop control, and we can choose many different forms for Pi, we can choose, you know, deep neural networks, we can choose something similar like linear feedback policy, so there are a number of different choices here that I won't go through in detail.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll skip the open loop.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because that's kind of a little bit outside the scope of this lecture.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to focus on closed loop control those with control of the model.",
                    "label": 1
                },
                {
                    "sent": "So if you if you have a model you know the model and you want to figure out your policy, there are few choices that you have.",
                    "label": 0
                },
                {
                    "sent": "So you can use your model to learn a policy.",
                    "label": 1
                },
                {
                    "sent": "This means that you're not going to be planning a test time, you're just going to optimize your policy.",
                    "label": 0
                },
                {
                    "sent": "Then just executed a test time and it's going to help with control.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One very natural way that we can think about using a model to learn a policy is just do backpropagation.",
                    "label": 0
                },
                {
                    "sent": "You know, if you have a model and you you have your policy, you basically have all the building blocks that you need to run a forward pass and evaluate your trajectory, and then you can imagine doing a backward pass through that to computer grading with spectrum policy parameters.",
                    "label": 0
                },
                {
                    "sent": "So no more sampling, no more anything else.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to do if your dynamics are terministic, you can do it for stochastic dynamics too.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit more involved, so I'll only talk about the deterministic case, but for the cast, in case you can do things like relation trick moment matching or sampling, and they can all handle that.",
                    "label": 0
                },
                {
                    "sent": "But let's just talk about the deterministic case because a bit simpler to deal with, so you have a computation graph that's defined by your known dynamics F and your policy Pi.",
                    "label": 0
                },
                {
                    "sent": "If everything is deterministic, this computation graph is very straightforward.",
                    "label": 0
                },
                {
                    "sent": "You can literally implement it in your favorite automatic differentiation software.",
                    "label": 0
                },
                {
                    "sent": "And this computation graph consists of basically three objects, the policy Pi, the dynamics F and the reward R. And so it looks a lot like an RNN, right?",
                    "label": 0
                },
                {
                    "sent": "It's just that in an RNN you have one transition operation is happening every step you have the DRM function.",
                    "label": 0
                },
                {
                    "sent": "Here you have two things.",
                    "label": 0
                },
                {
                    "sent": "You have a policy and dynamics F. So every single time step you have a state you ask \u03c0 to give you an action.",
                    "label": 0
                },
                {
                    "sent": "Then you have a state, an action which you can use to evaluate the reward.",
                    "label": 0
                },
                {
                    "sent": "Those are the blocks at the top and you can give them to your dynamics to get the next state SD plus one.",
                    "label": 0
                },
                {
                    "sent": "So this is just the computation graph for that setting.",
                    "label": 0
                },
                {
                    "sent": "And now you want to maximize R, so you could just do it by computing derivatives through this whole thing using back propagation.",
                    "label": 0
                },
                {
                    "sent": "Just say OK, I want the derivative this whole thing with respect to some of the Rs.",
                    "label": 0
                },
                {
                    "sent": "So compute those gradients, push them through the whole graph.",
                    "label": 0
                },
                {
                    "sent": "Giant backpropagation through time, get a gradient and use that gradient approval policy.",
                    "label": 0
                },
                {
                    "sent": "That's a very reasonable design for an algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's easy for deterministic policies.",
                    "label": 1
                },
                {
                    "sent": "You can do it for suggested policies too.",
                    "label": 0
                },
                {
                    "sent": "It's possible it can be really hard for the same reason that recurrent neural networks are hard.",
                    "label": 0
                },
                {
                    "sent": "So part of why we use things like, you know, if you've heard of LCMS, all sorts of optimization tricks and so on.",
                    "label": 0
                },
                {
                    "sent": "The train recurrent neural networks because backpropagation through time through along temporal sequence, can become extremely unstable here.",
                    "label": 0
                },
                {
                    "sent": "Of course, you don't get to choose the form of FF is the dynamics of your actual dynamical system, so F can be some complicated thing with derivatives.",
                    "label": 0
                },
                {
                    "sent": "They behave poorly when you multiply many of them together, so you backpropagation through time.",
                    "label": 0
                },
                {
                    "sent": "It's a product of all these Jacobians.",
                    "label": 0
                },
                {
                    "sent": "That's a product that scales with the number of time steps, so it can be extremely well behaved.",
                    "label": 0
                },
                {
                    "sent": "You can get.",
                    "label": 0
                },
                {
                    "sent": "The world's worst banishing or splitting gradients, and for that reason for complicated dynamical systems.",
                    "label": 1
                },
                {
                    "sent": "This algorithm will often not work very well.",
                    "label": 1
                },
                {
                    "sent": "And there are often better ways to do this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "01 better way of doing it, which is maybe a little bit unsatisfying but worth mentioning is to say well, even though I know the model, I'm just going to use Model 3 RL anyway.",
                    "label": 0
                },
                {
                    "sent": "It might seem a little bit silly because you have this additional knowledge in compute gradients for dynamics, but you're just not using them, but in many cases can actually work pretty well.",
                    "label": 0
                },
                {
                    "sent": "So you can just use the same policy grading method outline before or another model free method, even though you have a model and then you generate your samples.",
                    "label": 1
                },
                {
                    "sent": "But now you generate them in your own model.",
                    "label": 1
                },
                {
                    "sent": "And in some cases can actually work well in some cases, substantially better than using the gradients.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to talk about this in much more detail, but if you want an analysis of this, this is a very recent paper that I like a lot because it actually presents a pretty decent analysis of this problem and some tradeoffs for how to solve it.",
                    "label": 0
                },
                {
                    "sent": "So this is a paper by permits at all in acnl this year.",
                    "label": 0
                },
                {
                    "sent": "There are many other papers that deal with this problem as well, but this is a very recent one that could be a fun reference for those that want to learn more about this issue.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another thing you can do is you can learn policies with known models or with learned models, also without doing backpropagation through time by decomposing out the problem of selecting actions from the problem of training policy's so, this is basically the idea behind guided policy search.",
                    "label": 1
                },
                {
                    "sent": "The idea is that in some cases it's actually easier to optimize trajectories to optimize plans then it is to learn a policy.",
                    "label": 0
                },
                {
                    "sent": "So what you can do is you can optimize trajectory's or plans from different initial states.",
                    "label": 0
                },
                {
                    "sent": "Use them as training data and user training policy.",
                    "label": 0
                },
                {
                    "sent": "So if I have these differential states indicated in orange and I have some goal, maybe they're all trying to reach like Orange Star.",
                    "label": 0
                },
                {
                    "sent": "I separately optimized trajectories for them using my favorite planning or optimal control algorithm and then just use that as training data training policy with supervised learning.",
                    "label": 0
                },
                {
                    "sent": "And then the policy can be used to act in the world.",
                    "label": 0
                },
                {
                    "sent": "There's a little nuanced which is that you can actually change the objective for your planning to say, well, construct plans that are consistent with what my policy wants to do, so you can add a little reward term for your trajectory is to say, well, maximize the reward, but also try to avoid actions that my policy doesn't like.",
                    "label": 0
                },
                {
                    "sent": "Try to maximize the probability of the actions under the policy too, and the details of this method are provided in the paper, but it amounts to kind of iterative optimization where you alternate between optimizing your trajectory's to maximize reward and be consistent with your policy.",
                    "label": 0
                },
                {
                    "sent": "And then updating your policy with supervised learning.",
                    "label": 1
                },
                {
                    "sent": "Mathematically, the sense of being kind of a dual decomposition style algorithm.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll skip the these parts.",
                    "label": 0
                },
                {
                    "sent": "Because what I wanted.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in the remaining 15 minutes is talk about the case where you want to learn the model.",
                    "label": 0
                },
                {
                    "sent": "So we discussed how if you have a model, if you know the dynamics of your system, there are a few things you can do.",
                    "label": 0
                },
                {
                    "sent": "You can do open loop planning.",
                    "label": 0
                },
                {
                    "sent": "You can learn your policy, you can learn a policy with different techniques like backpropagation through time, guided policy search, just plugging into Model 3 solver.",
                    "label": 0
                },
                {
                    "sent": "So now let's talk about the case where you don't know the model, but you'd like to learn it.",
                    "label": 1
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we if we knew the model, we could of course do all these things that I discussed on the previous slides.",
                    "label": 1
                },
                {
                    "sent": "So we can construct.",
                    "label": 0
                },
                {
                    "sent": "A model based reinforcement learning kind of version 0.5.",
                    "label": 0
                },
                {
                    "sent": "A very simple, logical and obvious way to do model based RL.",
                    "label": 0
                },
                {
                    "sent": "Which is you have some initial base policy, maybe just a random policy.",
                    "label": 0
                },
                {
                    "sent": "You run it, you collect data, so your data consists of transitions, S, A comma, S prime.",
                    "label": 0
                },
                {
                    "sent": "You're going to learn a dynamics model on that data.",
                    "label": 0
                },
                {
                    "sent": "It could be a terministic or stochastic model.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to use the terministic model for simplicity in these illustrative examples, so if you have a terministic model and maybe your states are continuous, you can just fit it by minimizing mean squared error.",
                    "label": 0
                },
                {
                    "sent": "The difference between FSA and S prime.",
                    "label": 0
                },
                {
                    "sent": "You have a stochastic model, you can maximize log probability.",
                    "label": 0
                },
                {
                    "sent": "It's all good.",
                    "label": 0
                },
                {
                    "sent": "And then you can use this model to make decisions.",
                    "label": 0
                },
                {
                    "sent": "So step three.",
                    "label": 0
                },
                {
                    "sent": "Let's just plan through this model, choose actions.",
                    "label": 0
                },
                {
                    "sent": "So this is a simple kind of version 0.5 model based reinforcement learning algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now we can ask the question, does this algorithm work?",
                    "label": 0
                },
                {
                    "sent": "Well, the practical answer is that sometimes yes, so this is essentially how system identification works in classical robotics.",
                    "label": 1
                },
                {
                    "sent": "So in classical robotics, what you would do is you would choose your \u03c0 zero to be some random policy.",
                    "label": 0
                },
                {
                    "sent": "That sort of that visits many different kind of modes of your physical system.",
                    "label": 0
                },
                {
                    "sent": "You would use it to fit the parameters are physical system like the masses and lengths of different links, and then you would use it to control.",
                    "label": 0
                },
                {
                    "sent": "So in many cases naive kind of version 0.5 algorithm can work well.",
                    "label": 0
                },
                {
                    "sent": "Some care should be taken to design a good base policy, but it can work well, especially if you have some knowledge about your system.",
                    "label": 1
                },
                {
                    "sent": "So if you have like your physics textbook, you can basically know how the world works and using it if you open parameters.",
                    "label": 0
                },
                {
                    "sent": "Does this.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other work in general.",
                    "label": 0
                },
                {
                    "sent": "In general, the answer is no, and we can illustrate this with a little example.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here, just reminding.",
                    "label": 0
                },
                {
                    "sent": "Here's the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Step one is, you're going to run your base policy to collect data.",
                    "label": 0
                },
                {
                    "sent": "And let's say that I'm a little, you know, and walking around in this Hill and my goal is to reach the highest point.",
                    "label": 0
                },
                {
                    "sent": "So my reward is bigger the higher up I am.",
                    "label": 0
                },
                {
                    "sent": "So I start off on the side of the Hill and I'm going to walk around so that Redpath represents my random walk and I'm going to see that while walking, kind of generally towards the right, seems to increase my height so that he'll have to speak on the right here.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to say further to the right is higher and therefore better.",
                    "label": 0
                },
                {
                    "sent": "So my initial \u03c0 zero does things at random, and the dynamics that I learn tell me if I go to the right.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be higher up on the Hill.",
                    "label": 0
                },
                {
                    "sent": "So when I fit those dynamics and then use them to make decisions.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say, well, going right is better, so I'm going to go to the right.",
                    "label": 0
                },
                {
                    "sent": "And if I actually do that, I'm going to fall off the Hill.",
                    "label": 0
                },
                {
                    "sent": "Now you might say you might look at us and say, well, OK. Of course that happened because that red that red thing didn't actually visit all the parts of the space, it just was constrained to particular region.",
                    "label": 0
                },
                {
                    "sent": "But in large high dimensional state space this problem is actually a very general problems.",
                    "label": 0
                },
                {
                    "sent": "It happens all the time.",
                    "label": 0
                },
                {
                    "sent": "And mathematically, we can view this as a problem of distributional shift, because what happens when you run your initial policy \u03c0 zero, your random exploration policy?",
                    "label": 0
                },
                {
                    "sent": "You observe states from the distribution P, \u03c0 zero.",
                    "label": 0
                },
                {
                    "sent": "This is the state distribution of that policy.",
                    "label": 0
                },
                {
                    "sent": "When you then plan according to your learning model, you're going to states from a different distribution.",
                    "label": 0
                },
                {
                    "sent": "You're going to see states from distribution induced by that planning policy.",
                    "label": 0
                },
                {
                    "sent": "If Pi, F and those distributions are not the same, which means that your model which was trained to minimize error on PI0 will not necessarily get low error on Pi F. So this is a distributional shift problem.",
                    "label": 0
                },
                {
                    "sent": "And we can address this distributional shift problem simply by adjusting our model to the distribution of the currently have.",
                    "label": 0
                },
                {
                    "sent": "So this distribution mismatch actually the trouble with this action becomes exacerbated as we use more and more expressive model classes.",
                    "label": 1
                },
                {
                    "sent": "Essentially, you end up with your model overfitting to be \u03c0 zero.",
                    "label": 0
                },
                {
                    "sent": "So it's a very unfortunate curse at the more powerful your model is, the more you suffer from this issue, and that's why in robotics when we do things like system identification and were fitting just three or four parameters, just the number of maybe masses in your robot, the symbol method works well.",
                    "label": 0
                },
                {
                    "sent": "When you're fitting some big black box model like a neural net, this problem is going to really get you.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can do better with a very simple fix.",
                    "label": 0
                },
                {
                    "sent": "That's going to make our data distribution equal to our to state distribution of our policy, and the fix is basically exactly what you might expect us.",
                    "label": 0
                },
                {
                    "sent": "So model based reinforcement learning version 1.0 run your base policy, collect some data, use the data to figure dynamics model.",
                    "label": 0
                },
                {
                    "sent": "User dynamics model to act and then repeat, so execute those actions, collect the resulting data appended to your data set, and keep going.",
                    "label": 0
                },
                {
                    "sent": "And if you do this, the intuition basically is that as you keep appending more and more data, eventually your buffer gets populated mainly by data that looks like the data you would see from your policy.",
                    "label": 0
                },
                {
                    "sent": "So even if your initial policy \u03c0 zero is totally different as you do this more and more, eventually the larger and larger fraction of your buffer is populated by data that is close to your policy would actually see, and your model becomes good in exactly those regions that your possible visit.",
                    "label": 0
                },
                {
                    "sent": "So this kind of the reason I call this 1.0 is because this is kind of the first practical algorithm that can probably implement.",
                    "label": 0
                },
                {
                    "sent": "It will probably work decently well.",
                    "label": 0
                },
                {
                    "sent": "There's a little detail, which is that you need to collect the data from PDF, and that's what this algorithm accomplishes.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now there are a few other things you can do that will make this work a lot better.",
                    "label": 0
                },
                {
                    "sent": "One of them.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can do, of course is closed loop control, so if you make mistakes, if you're just planning according to F, those mistakes can add up.",
                    "label": 0
                },
                {
                    "sent": "They can basically accumulate overtime if you do include control, you can correct mistakes as soon as they happen so.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can do better by doing closely control, either with planning.",
                    "label": 0
                },
                {
                    "sent": "So if you close the control of planet, sometimes called moderated controller MPC, that means that you re plan every single time step.",
                    "label": 0
                },
                {
                    "sent": "Or you can do close the control by learning a policy so both those things can actually be actually making more resistant to having an incorrect model.",
                    "label": 0
                },
                {
                    "sent": "Positive control can mitigate the effects of slightly incorrect models.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I will skip through this part.",
                    "label": 0
                },
                {
                    "sent": "Now when you learn your model, of course, yeah, you can back property that also.",
                    "label": 0
                },
                {
                    "sent": "Just like I discussed before.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned, back up into the policy is often not a great idea and there are better ways to optimize, so you can use your learn model and still run model free RL to learn your policy.",
                    "label": 1
                },
                {
                    "sent": "You can take your learn model and use guided policy search and there are other things you can do as well.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What about observations?",
                    "label": 0
                },
                {
                    "sent": "So far when I talked about learning models, I only discussed systems when you have access to the full state, you can do model based reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "We have access to observations as well.",
                    "label": 0
                },
                {
                    "sent": "Then you end up basically learning this partially observed Markov decision process model.",
                    "label": 0
                },
                {
                    "sent": "So you have to learn not just the transitions between the states, but also the observation function.",
                    "label": 0
                },
                {
                    "sent": "How the states relate to observations.",
                    "label": 0
                },
                {
                    "sent": "In many cases when you're doing model based IRL with observations you don't even know what the states are, so you might not even have access to a definition of the state space.",
                    "label": 0
                },
                {
                    "sent": "So then you learn essentially a latent state model.",
                    "label": 0
                },
                {
                    "sent": "Here are a few references to some recent papers that learn latent state models that you guys might want to check out.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in this topic.",
                    "label": 0
                },
                {
                    "sent": "So this is a paper by water at all, called embed to control that learns a living space model based on the type of variational autoencoder and this is a visualization of a model that they learn for a little pendulum task.",
                    "label": 1
                },
                {
                    "sent": "So the picture on the left shows of the visualization of their latent space and the pictures on the right show the generations that generated observations from the latent space model.",
                    "label": 0
                },
                {
                    "sent": "So in this paper they train this model and then use it to make decisions via a type of planning.",
                    "label": 0
                },
                {
                    "sent": "Here's a paper for my group on this.",
                    "label": 0
                },
                {
                    "sent": "This is using something called the spatial autoencoder control robot.",
                    "label": 1
                },
                {
                    "sent": "Very similar principle.",
                    "label": 0
                },
                {
                    "sent": "Train an auto encoder style model to learn a latent space fit dynamics in that latent space, and then use those dynamics to do a model based RL procedure based.",
                    "label": 0
                },
                {
                    "sent": "In this case, I'm guided policy search.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a summary for model based RLI talked about how we can have kind of version 0.5 where you collect some random samples using trainer dynamics and use that to plan.",
                    "label": 0
                },
                {
                    "sent": "There's a simple.",
                    "label": 0
                },
                {
                    "sent": "There's no iterative procedure, but it can have this distribution mismatch problem.",
                    "label": 1
                },
                {
                    "sent": "I talked about a version 1.0 where we iteratively collect data replan and collect more data.",
                    "label": 1
                },
                {
                    "sent": "This is simple.",
                    "label": 1
                },
                {
                    "sent": "It solves distribution mismatch problem, but if you if you do, you know planning.",
                    "label": 0
                },
                {
                    "sent": "Once you do open loop control, this might perform poorly.",
                    "label": 0
                },
                {
                    "sent": "So what you can do is either model creative control where you plan every step or train a policy and both those will produce closed loop control which will allow you to get good results even before your model has fully fitted the dynamics accurately.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in doing model based reinforcement learning, you definitely go at least with version 1.0.",
                    "label": 0
                },
                {
                    "sent": "And if you want to close the control, maybe something like version 2.0 might be a good choice.",
                    "label": 0
                },
                {
                    "sent": "OK so a few suggested readings for some classical papers on model based or Alan.",
                    "label": 0
                },
                {
                    "sent": "I kind of used classical in a very loose way because this includes papers from as recently as 2012.",
                    "label": 0
                },
                {
                    "sent": "I would recommend checking out these three papers.",
                    "label": 0
                },
                {
                    "sent": "The first one by IRA Sutton introduced this thing called Dyna, which is kind of one of the best known frameworks for doing it for learning a model and then using model free algorithms within that model.",
                    "label": 0
                },
                {
                    "sent": "So I would highly recommend that the 2nd paper by Mark Dozen Roth described this album called Pilko which is.",
                    "label": 0
                },
                {
                    "sent": "Kind of a very good example of an algorithm that sort of back propagates through the model.",
                    "label": 0
                },
                {
                    "sent": "They actually used acoustic model and they backpropagated through it using moment matching, but it's good kind of example of an algorithm in that category.",
                    "label": 0
                },
                {
                    "sent": "And the third paper Rosson back, now that one deals with this distribution mismatch problem in model based reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So if you want to find out more about distribution mismatch, I highly recommend the 3rd paper.",
                    "label": 0
                },
                {
                    "sent": "Some examples of more recent papers.",
                    "label": 0
                },
                {
                    "sent": "These are not necessarily like particularly good papers, just examples of recent methods for model based Sorrell, especially in deep learning.",
                    "label": 0
                },
                {
                    "sent": "This is the inventor control paper by water at all that I mentioned this paper by Hess.",
                    "label": 0
                },
                {
                    "sent": "It all combines model free and model based learning in a different way by actually back propagate through the model to improve the model free algorithm.",
                    "label": 0
                },
                {
                    "sent": "This guided policy search paper this is the this is a paper that actually does model based planning directly on images using video prediction.",
                    "label": 0
                },
                {
                    "sent": "And this is a paper on learning neural net models and doing iteratively re planning via MPC.",
                    "label": 0
                },
                {
                    "sent": "So if you want to find out version 1.5 algorithm, this is one of the papers to check out.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can also practice on your own.",
                    "label": 0
                },
                {
                    "sent": "So if you want to do homework assignment model based RL again, this same GitHub links before homework for deals with model based reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "If you wanna check that out.",
                    "label": 0
                },
                {
                    "sent": "OK, so at this point I'm going to wrap up because I have about 4 minutes left.",
                    "label": 0
                },
                {
                    "sent": "I do want to leave some time for questions, so thank you very much.",
                    "label": 1
                },
                {
                    "sent": "I'll be happy to take questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you, can you go back to your direction of policy creating sure?",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "So here you have a capital T. Yep, the horizon.",
                    "label": 0
                },
                {
                    "sent": "Yeah the horizon and this capital T is actually a random variable, which is which depends on policy \u03c0. Yeah, so let me clarify something about capital T. So for that I think it would be really helpful to bring up the slide with reward to go.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So capital T the horizon of your problem, but it's not actually a function of your policy, it's a property of your problem statement.",
                    "label": 0
                },
                {
                    "sent": "So when we define a reinforced learning problem, if it's a finite horizon problem, we basically choose a horizon capital T. So in some cases this makes a lot of sense.",
                    "label": 0
                },
                {
                    "sent": "You know if you're playing a game and you have exactly 20 turns to win the game Capital T is 20.",
                    "label": 0
                },
                {
                    "sent": "In some cases it doesn't entirely make sense, so in some cases maybe you want to achieve some kind of good behavior and then maintain that behavior for infinite time.",
                    "label": 0
                },
                {
                    "sent": "In that case, it becomes a little tricky to, you know, do the thing at the top on the top line.",
                    "label": 0
                },
                {
                    "sent": "Here we just some always from one capital T because he might be infinite.",
                    "label": 0
                },
                {
                    "sent": "But in practice, what you would do in those cases that you would use what's called a discount factor.",
                    "label": 0
                },
                {
                    "sent": "So you would say that rewards further in the future are worthless to me than rewards right now, which means that when you evaluate the reward to go, you will put a coefficient in front of this R, which is gamma to the T -- T prime.",
                    "label": 1
                },
                {
                    "sent": "So rewards future further in the future and accounting for less.",
                    "label": 0
                },
                {
                    "sent": "And when you do that, then even though you're summing from Tita Capital T. Things too far in the future, especially to get a multiplier of 0.",
                    "label": 0
                },
                {
                    "sent": "Because if you have a discount less than one exponentiated by some large exponent, eventually goes to 0 for a big enough exponent.",
                    "label": 0
                },
                {
                    "sent": "So in that case you can actually set capital T to Infinity, but not actually compute the whole sum up to Infinity, just computed up with some value.",
                    "label": 0
                },
                {
                    "sent": "Once the multiplied it's small enough.",
                    "label": 0
                },
                {
                    "sent": "In reality, if you're doing this where you would probably want to do is use an actor critic method and remove this whole issue altogether, in which case you get an estimated looks more like this and Q hat then as a function approximator.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Can you hear me know I can hear you OK?",
                    "label": 0
                },
                {
                    "sent": "For a problem where he was sort of demonstrations and you could either use imitation learning or using virtual enforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Do you have any intuition for when you would choose one or the other?",
                    "label": 0
                },
                {
                    "sent": "So the question is, if you have demonstrations, should you use imitation learning or reinforcement learning?",
                    "label": 0
                },
                {
                    "sent": "If you are in problem setting where you have access to human demonstrations, using those demonstrations can be a really good idea.",
                    "label": 0
                },
                {
                    "sent": "And there are actually a number of different ways to use them.",
                    "label": 0
                },
                {
                    "sent": "The move, the simplest ways to just directly run supervised learning.",
                    "label": 0
                },
                {
                    "sent": "But in some cases you might have both demonstrations and rewards, and if you know the reward and you have some demonstration that I should combine them pretty easily and there actually many techniques in the literature that combine both demonstrations and rewards, there are many ways to do the simplest as you could, for example, initialize your policy with demonstrations and then fine tune it with rewards.",
                    "label": 0
                },
                {
                    "sent": "That can be a very good idea because the demonstrations.",
                    "label": 0
                },
                {
                    "sent": "Here policy into good initial state that overcomes the challenge of exploration.",
                    "label": 0
                },
                {
                    "sent": "So in the previous lecture you guys heard a little bit about explore exploration.",
                    "label": 0
                },
                {
                    "sent": "Demonstrations could be really good way to overcome that.",
                    "label": 0
                },
                {
                    "sent": "But you can also do something more interleaved.",
                    "label": 0
                },
                {
                    "sent": "You can actually both imitate an maximize rewards at the same time, and that can be useful because the initialization you might forget.",
                    "label": 0
                },
                {
                    "sent": "Forget that initialization as you try to maximize the rewards.",
                    "label": 0
                },
                {
                    "sent": "If you keep both the demonstrations and the rewards around, then you can do even better.",
                    "label": 0
                },
                {
                    "sent": "So yeah, if you're interested in this topic, there's quite a bit of literature on the subject.",
                    "label": 0
                },
                {
                    "sent": "We are unfortunately out of time, so let me let us first.",
                    "label": 0
                },
                {
                    "sent": "Thank Sergey for his very excellent talk.",
                    "label": 0
                }
            ]
        }
    }
}