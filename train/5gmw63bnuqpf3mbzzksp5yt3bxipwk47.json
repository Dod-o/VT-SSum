{
    "id": "5gmw63bnuqpf3mbzzksp5yt3bxipwk47",
    "title": "State estimation and prediction based on dynamic spike train decoding: noise, adaptation, and multisensory integration",
    "info": {
        "author": [
            "Ron Meir, Technion - Israel Institute of Technology"
        ],
        "published": "Aug. 5, 2008",
        "recorded": "May 2008",
        "category": [
            "Top->Computer Science->Bioinformatics",
            "Top->Computer Science->Artificial Intelligence",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/aispds08_meir_sep/",
    "segmentation": [
        [
            "OK thanks so this is joint work with mobile skin you nailed at the Technion.",
            "So the motivation for this work is essentially coming from computational neuroscience, where people are interested in question of how to decode neural spike trains.",
            "This is something that the brain itself has to do, and there's also a lot of interest in this issue of spike train decoding for brain computer interfaces."
        ],
        [
            "The problem is essentially I'll go over it very quickly because this is something which essentially is shared amongst many of the talks.",
            "Here you have an environment which we describe as a world state.",
            "We have a sensory system with which observes this environment and we have some kind of system which is supposed to estimate the state or predict the state of the environment.",
            "So the objective here is essentially based on this partial noisy sensor input to estimate the world state.",
            "Now, of course noise is ubiquitous, it's everywhere in this system and this is something which has to be which has to be."
        ],
        [
            "Rest, So what we'd like ideally as similarly to many talks that have been presented here, is to compute the posterior distribution, which is the distribution of the current state given the sensory input, or if you were interested in prediction, the probability of the next state given the sensory input.",
            "We'd like to do this in continuous time because neural neural data neural processing is taking place in continuous time, we don't want to lose any information.",
            "Related to binning and discretization affects, we'd like to do this online so that each spike can be processed the PON arrival and we like to do this in real time so we have a fixed computational load which does not increase as the temporal window increases an since we're trying to propose a mechanism by which brain can can do spike train decoding, we'd like to think of a way how to implement this.",
            "This online real time decoding by a neural network."
        ],
        [
            "So the main results are the following, so I'll present the precise model in a moment, but the main assumption we have is that the state is described by some kind of kind of Markov process.",
            "The mathematical foundations that to this field have been around since the 1970s, although unfortunately I think many people within the computational neuroscience literature or not aware of this long history of of people dealing with this issue of what I would call now point process filtering.",
            "We suggest an implementation of this exact filtering by a simple bilinear neural network, and we extend the original framework that was developed during the 70s.",
            "Too noisy inputs to history dependence spike trains and to multiple multiple multimodal inputs and to prediction that skip the line there.",
            "We can demonstrate some well known results from the static case, which have been widely studied within the computational neuroscience literature so well."
        ],
        [
            "Comparing looking at various algorithms and approaches people have developed over the years, there are many, many dimensions across which you could compare these models.",
            "In particular, our work will deal with the as is fit for this workshop.",
            "We have this dynamic stimulus continuous time Markov process.",
            "The space we will consider here is finite.",
            "The solution we propose is exact.",
            "It's neurally implementable an.",
            "OK, the receptive fields I'll get to in a moment and we include environmental noise and we and we deal with some issue which is very important in an in a neural context which is adaptation.",
            "So the main restrictions of our work it's exactly the result.",
            "We present this exact, except that we're restricting ourselves to finite state and Markovian dynamics."
        ],
        [
            "Well, it's obviously you know.",
            "Obviously sensors have a finite resolution, but this is just a, you know, that's a week a week answer the.",
            "Ideally we would like to be able to address continuous spaces.",
            "Unfortunately, much more complicated systems of equations, but this is something that we do want to do.",
            "We just haven't done it at this point.",
            "So the problem setup is the following.",
            "We have XT which is the process representing the world state.",
            "We have a set of Spike Trains N 1 N 22 NM representing the responses of sensory cells, and these these.",
            "These responses can be, you know partial, noisy adelaid, redundant, what have you and we'd like oops.",
            "And we'd like to have a network here, which is a decoding network which is able to decode the state of the world given the sensory input.",
            "So the."
        ],
        [
            "And assumptions are that we have the world state is given by a finite state continuous time Markov process.",
            "We are characterized by a generator matrix Q.",
            "The sensory activity is given by.",
            "For now I will assume them to be person processes N 1 to NM where the rates of each person process the rate of each person processes Lambda one of XT to Lambda M of XT.",
            "So and these are generating the generating the spike trains.",
            "So essentially what we have what is called in the in the stochastic process literature, doubly stochastic process and process.",
            "And again, what we'd like to do is compute this posterior probabilities.",
            "And again, we'd like to do this in an online way, an online real time computation and demonstrate how this can be implemented by noon."
        ],
        [
            "Network.",
            "So historically, as I said, the as a people here know very well these ideas within this field were formulated in the 60s by people like Kushners Archive on Ham and many others, and their works for essentially diffusion Gaussian processes and later extended in the 70s to point processes by people like Snyder Sehgal and Cal ETH, which I get.",
            "Some of them have already been mentioned previously here.",
            "So this work really relies on this rigorous theory and demonstrates how it can be implemented in real time, and we provide various extensions and generalizations of these of these ideas."
        ],
        [
            "So the key concept is what is called in the stochastic filtering in the optimal filtering literature, these Akai equations and izakaya's equations essentially provide a non a simple filter for the non normalized probabilities.",
            "So the general problem of filtering is is not a non optimal solution is nonlinear.",
            "However there's a key equations are very neat trick too.",
            "To transform the problem into a linear problem.",
            "So what can be shown and I won't show it here, but this is by now a standard approach that it showed that there is a special set of functions row I = 1 to N, where N is the number of states such that the posterior probability is given.",
            "Just buy a re normalized version of these rohais so this row eyes are non negative variables which when normalized provide the posterior distribution.",
            "Now the equations that the differential equations Obaid by this realize are much simpler than the differential equations Obaid by the posterior distributions.",
            "So the computing probabilities is hard.",
            "However, computing an unnormalized probabilities is easy, and this was demonstrated in the 60s for the diffusion processes and later in the 70s for point process filled."
        ],
        [
            "During.",
            "So going through Ascentia Lee, a set of by now standard mathematical manipulations if based on their son of theorem and various various change of measure techniques, one comes up with the following set of equations.",
            "For this for Ro again row is, this row is this non normalized probability vector.",
            "Normalized probabilities Q is the generator matrix for the underlying Markov process.",
            "New M essentially is the spike trainers.",
            "This is a sum of Delta function.",
            "This is essentially the spike train corresponding to the NTH sensory cell.",
            "So this contains the spiking activity of this M sensory cell.",
            "This matrix, Lambda M is this matrix.",
            "Of so called tuning curves of these functions, which are which are the intensity functions for the person processes.",
            "Lambda is just this sum of all these lambdas of Lambda, M's and so if we look at this equation we see quite neatly here a breakup of this equation into a part which contains information about the prior through this matrix Q which contains the information about the underlying dynamics of the Markov process.",
            "We have a term.",
            "Let's look at this term first.",
            "This is a term which is just a global decay term which has kind of you can interpret is in a Bayesian way as a bias towards having no spikes.",
            "And then this term is the only part which depends on the data through these spike trains, new M of T. So this is an exact filtering equation for the you see it's linear in row for the non normalized probabilities.",
            "In between, the data is just the prior yeah, and I'll present the exact solution in a moment, so this."
        ],
        [
            "Actually, equation already is essentially describes just a simple neural network which which is composed as following you have you have these this high quality posterior network.",
            "This network has lateral connections queue.",
            "We have these connections from the sensory layer to this posterior network here.",
            "With these weights, Lambda, MSI and these are so, so the activity the their input here to this posterior network and they multiply the variable row and then we have this global decay term Lambda.",
            "So this this simple filtering equation Maps very simply unto a simple.",
            "I would call it a BI linear network, not a linear network, cause the variable row is the date.",
            "It appears multiplicatively with the input here, so this is some simple yeah.",
            "The environment is stationary in this case.",
            "Well, the environment.",
            "I mean this is the environment affects the system through the spike trades norm of T and it's at this point it's totally arbitrary.",
            "I mean, I've assumed that the underlying for the in order to Darrow to derive this equation and show that it's optimal one assumes a Markovian dynamics of the environment, but but this except for that this is totally general.",
            "So, but this is computing posterior distribution.",
            "This is computing the unnormalized posterior distribution as you go along in real time.",
            "So you mean if this Q matrix is changing?",
            "Or yeah, so this assumes right?",
            "So in that sense it assumes a stationary environment, yeah?",
            "But I think it wouldn't be there.",
            "Wouldn't be any problem here.",
            "Having a time dependent queue, but I'm not sure I have to think about that.",
            "So you have to roll out to 0 is the solution then.",
            "Rodat is no because you you know if if it's a solution only if Q = 0 It's you know it's.",
            "You know, as long as you get spiking activity, as long as you're getting inputs then then row will be non 0 because every time you get an input Rd changes.",
            "So every spike, any input spike changes rule.",
            "OK, you have this term.",
            "So deep, but I, I thought that it is in the environment stationary the posterior network.",
            "The posterior distribution should be stationary and here's a time changing so I don't understand.",
            "Well, I mean, you know the posterior distribution changes based on the spikes.",
            "I mean, if you look at if you look at the."
        ],
        [
            "Yeah, if you look at this or is it?",
            "If you look at the posterior distribution, I mean this is essentially this depends on the particular spike train.",
            "So this is all conditioned.",
            "So this is the posterior distribution given the data you've observed.",
            "So this depends on the specific spike train you've seen, so it depends on time explicitly.",
            "OK."
        ],
        [
            "OK, just a simple example of some particle moving and this is the trajectory and the blue dots represent the spiking activity of the cells.",
            "And here you see the posterior probability of Aleutian coming out of that network and you can see essentially that when you have a small number of spikes you how you see an increased uncertainty here as would be expected.",
            "OK, so so."
        ],
        [
            "The behavior of this system is very simple.",
            "Between spikes, you just have this exponential evolution and upon the arrival of the NTH sensory spike, this function row is just multiplied by the tuning curve.",
            "So if this is the row prior to the NTH spike and this is just the tuning curve, then an input came from the region S. Then it's just multiplied and the peak of row is shifted a little bit to the right depending on the tuning curve.",
            "There's a closed form solution available here, because it's a very simple system of."
        ],
        [
            "Now, just as a simple example, look at the static case where you have a constant input selected from a finite set and if we take a Gaussian now it's a Gaussian star prior because it's a Gaussian, but these are we're assuming discrete variables, so so you have this kind of Gaussian an assuming Gaussian tuning curves, so the posterior distribution in this case, again, is Gaussian with the mean and variance, and when the.",
            "So Sigma X is the prior distribution of the X variable.",
            "When this, when the prior is flat, we get this.",
            "We find that the posterior mean is an average of the weighted average of the centers of these receptive fields.",
            "So CMR the centers of these receptive fields an all this NM is the spiking activity of the MSIL and we see a weighted average of these activities.",
            "Now this is a well known.",
            "Proposal proposed in 1982 by Georgopoulos called the population Vector is used widely in in computational neuroscience.",
            "An in motor control.",
            "Then in many brain computer interfaces.",
            "This is a basic element and it just pops out of this solution."
        ],
        [
            "So this is basically well known stuff what we have."
        ],
        [
            "Looked at here is tried to extend it in various directions, so up to now we've assumed that we have some kind of hidden state which is observed through a sensory system and a posterior network is trying to decode it.",
            "Now.",
            "The sensory response here is a direct function of the state.",
            "Now in reality, the world is a noisy place and we never observe the world.",
            "I mean there may be fog, there may be noise.",
            "We never observe the world directly, so so there is a noisy channel between the world.",
            "And the sensory system which we will denote by WT.",
            "So what we'd like to do is we'd like to extend this framework to be able to deal with with environmental noise.",
            "So in this case the rate functions here Lambda are functions of the noisy state F of XT&WT and not of the actual state itself."
        ],
        [
            "So in order to model this, we've assumed that the noise is again a finite state Markov process, and we will make some assumption later, which about the time scale of this of this of this noise.",
            "As you know, one cannot have white noise in continuous time.",
            "However, XD and WT are independent, and we assume that the tuning curves OK, so the tuning curves are functions of the observed state through the noise.",
            "For instance, you could have.",
            "Additive noise Now the key observation in this derivation of the result here, is to observe that this augmented state X TWT is a Markov process of dimension N * L, where L is the dimension of this Markov process.",
            "So what one can do here essentially is right recursive equations for computing the joint distribution of the state and the noise given a spike trains and then marginalized over the over these non normalized probability distributions.",
            "To get the posterior distribution where it's averaged over the noise."
        ],
        [
            "So going through the analysis here, what one comes up with is the following filtering equation, which where we have these variables fi M here, which are just the expectation values over these tuning curves.",
            "Lambda M. Remember that Lambda M is a function of the noise through these tuning curves, so these are just the expectation over the noise distribution of the tuning curves.",
            "They're just the average average.",
            "Provide just the average."
        ],
        [
            "Sensory response, so just to demonstrate what this looks like here, we have.",
            "If we have the clean dynamics of the of the state of the world and we add some noise to it.",
            "This is the posterior network activity.",
            "And if we try to filter to estimate the state based on the and this is the MSE, the error in the filtering error, the reconstruction error if we try to do that directly with the original equations, which did not take into account noise.",
            "We get this result and obviously as expected, if we add if we add this if we replace the tuning curves by their average value according to our previous equation, we see that we indeed get improved performance.",
            "Bye bye bye.",
            "Taking account of the noise."
        ],
        [
            "One interesting question here is how does the?",
            "How does the?",
            "How do the properties of the system depend on the level of noise?",
            "So one could expect if you have tuning curves which are very narrow and.",
            "They give you whenever they whenever they.",
            "Whenever an object occurs within these receptive fields, we get very high precision.",
            "However, we have rather poor coverage because their regions which are not covered by these receptive fields.",
            "So we essentially have few spikes.",
            "On the other hand, if we have very broad receptive fields, we can have low precision precision but good coverage and we have many spikes, but we cannot really resolve them, so one would expect naively that there is an optimal.",
            "Tuning curve width, which I'm which which covers the domain in an optimal way and I'm assuming here that the number of tuning curves is fixed, because if I let them allow, if I allow them increase without limit, obviously I can.",
            "There is no, there is no problem here.",
            "So for a fixed by the fixed budget of a number of tuning curves, the question is whether there is an optimal value of the width here.",
            "So we."
        ],
        [
            "Make the same assumptions as previously and we look at the mercy of the optimal estimator so we look what we have here is as a function of Alpha, which is the width of the tuning curve.",
            "We look at the MSE for different values of environmental noise.",
            "So what we see here quite clearly is that for each value of the environmental noise we have a clear minimum in the MSE as a function of the width of the tuning curve, so.",
            "If we plot this, if we plot the width of the optimal tuning curve as a function of the external noise, we see that this increases and ultimately one can even show analytically that this is a linear dependence.",
            "So ultimately the optimal tuning curve width.",
            "Is is the proportional essentially to the variance of the external noise, which makes sense and actually leads to an experimental prediction for adaptation by neural systems, which is something that we are planning to test on.",
            "On the retina, working with some person is.",
            "Does experimental work on the right now?",
            "This is a specific experimental prediction about adaptation to increase noise."
        ],
        [
            "OK, so that is 1 one specific prediction for this system.",
            "Now another interesting application is to the case of history dependence.",
            "Spike trains and adaptation.",
            "So many people use person types of processes for dealing with spike trains.",
            "However, they are limited in at least two respects.",
            "First, obviously they lack memory and there are physiological phenomena like the refractory.",
            "You know that after.",
            "A neuron fires.",
            "There is a certain.",
            "Where it cannot fire, so obviously this cannot be described by person process and there is a very important phenomenon termed adaptation, where neurons essentially get bored.",
            "So if initially they start spiking at a fast rate, their spiking rate decreases as time goes by and they and they they fire more slowly.",
            "So these obviously cannot be modeled by Poisson processes."
        ],
        [
            "So what we try to do here is to extend this framework to what are called self exciting point processes where the rates Lambda depend not only on the state of the world, but they can depend on the history of the spiking activity of the sensory cell.",
            "So these include posson renewal processes and many other processes.",
            "So without going through the derivation, essentially it's not hard to show that the derivation goes through.",
            "The only difference is that now these tuning curves are essentially history dependent and they depends specifically on the history of the spiking activity of this of this cell.",
            "The assumptions that this this is process is not markup anymore does not enter this.",
            "No no, I mean it.",
            "It's right, right?",
            "Well, yeah, as long as it can be characterized as long as the as long as you have these are processes which can be described by a rate function, not all stochastic.",
            "But if you can describe it by a rate function with essentially an up sensually an arbitrary history dependency framework still goes through.",
            "Yeah so.",
            "Is there any?",
            "Maybe later, yeah, so so I mean.",
            "OK, so."
        ],
        [
            "OK so the So what we were trying to do here is to try to model.",
            "The phenomenon refers to as adaptation, so we have the now.",
            "So a specific model we considered here is this tuning curve which depends on time in the following way it's given by multiplication of mu times Phi, where Phi is a standard tuning curve.",
            "Now Mu has the following behavior between spikes, it just relaxes exponentially towards one.",
            "Now upon the arrival of a spike.",
            "It gets pushed down so so the you know, physical idea here is that whenever a spike whenever a neuron emits a spikes, its resources are depleted and it gets tired.",
            "It cannot fire for a few milliseconds, so we model this by just by just pushing the tuning curve value down, and therefore the probability of emitting spike is lowered upon the emission of a spike.",
            "So this is essentially models the phenomenon of adaptation.",
            "And what we see here, essentially what we plot here as a function of Tau for various values of Delta, we plot the spike count ratio.",
            "So this is just the number of spikes we have in the system compared to the number of spikes without adaptation.",
            "And here we plot the MSE ratio as a function of this parameter Tau.",
            "So what we see if we look for instance at a value of Tau equals one and Delta equals .2.",
            "We see that we have only about.",
            "We have 70% of the spikes we had without adaptation.",
            "However, the performance has improved.",
            "Now the MSE is .8 of it what it was before, so this adaptation process essentially allows you to reduce the number of spikes quite significantly and at the same time improve performance.",
            "Which is kind of interesting from an optimization point of view now."
        ],
        [
            "Explanation for that is is that if you look at the.",
            "This remember the equation had this self inhibition term minus Lambda times row.",
            "So if you look if without it without adaptation we have the this variable Lambda is more or less constant, except that it goes to a decreases at the boundaries of the domain.",
            "What happens is that cells near the true state in the case of the when I allow adaptation, their value of Lambda decreases because of this adaptation and therefore the self inhibition term.",
            "Is reduced and these cells are less inhibited so so we can actually have a pretty mechanistic explanation of why adaptation is efficacious in neural system.",
            "It actually improves the signal to noise ratio and and reduces at a reduced number of spikes."
        ],
        [
            "How much time do I have?",
            "It's about out of time, so I'll skip this issue of multi sensory integration essentially.",
            "And I'll just move towards the end of the talk.",
            "Um?",
            "OK.",
            "So."
        ],
        [
            "OK, one other thing.",
            "I should point out in passing that this framework, as you may imagine, can be extended quite simply to the case of prediction where we are trying to predict the state not at time T but at time T plus talk given the spiking activity up to time T, and it pretty straightforward modification of the filtering equation leads to the same type of equation for the prediction, except that these matrices Lambda here are multiplied on the left and right by these.",
            "Transition probabilities P. But essentially it's the same kind of by linear equation, which can describe a neural network which can implement this system.",
            "This prediction posterior."
        ],
        [
            "Prediction.",
            "So let me just summarize so.",
            "So we presented a spike based filtering of continuous time Markov process.",
            "We've shown how to implement it online using bilinear neural networks provides a mathematically rigorous treatment of continuous time in this neural context.",
            "No information temporal information has been lost.",
            "We've been able to deal with multimodal effects easily, and I haven't described that, and there are many possible extensions we've discussed, and again, something which I haven't shown many.",
            "A lot of the work in the literature has involved the static case and all the results from the static case that we came across in the literature actually turn out to be special cases of this general dynamic formalism.",
            "So there are many extensions possible."
        ],
        [
            "So you know, learning these matrices Q and Lambda using more distributed and robust representations, continuous state spaces, and ultimately we'd like to have a good physiological interpretation.",
            "An implementation of these models and make predictions for specific biologic experiments, which is something we are currently doing with regard to.",
            "A retinal preparation."
        ],
        [
            "OK, thank you.",
            "Time for one.",
            "Well.",
            "There is biological evidence for this type of.",
            "I mean for the behavior and this adaptation model, for instance, was modeled after some kind of biological adaptation, and this specific structure of the network we consider there is the specific nature of the multiplicative interaction between the input and the state.",
            "There is evidence for that in certain neural systems, but this is not something that at this point you know I would like to talk too much about because there was something we only recently looking into, but.",
            "But there is definitely.",
            "There is a recent paper by some Polinsky and Meister which proposed a very similar model and they provide quite extensive evidence.",
            "Experimental evidence for the.",
            "For the existence of such types of networks.",
            "When you have this history dependent race because it seems to me that this generative model is no longer kind of Markovian, why is it still possible to like the filtering in such a stinky playing on my missing something?",
            "Well, because I mean, you know, because we're assuming.",
            "Essentially I mean, all these equations are all conditioned on the spiking activity.",
            "So basically the.",
            "We're not averaging over the spikes, so this doesn't.",
            "I think this is a bit of a technical point.",
            "Maybe we should go over it if you have to really look at the derivation to understand this, but it goes through pretty clearly.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK thanks so this is joint work with mobile skin you nailed at the Technion.",
                    "label": 0
                },
                {
                    "sent": "So the motivation for this work is essentially coming from computational neuroscience, where people are interested in question of how to decode neural spike trains.",
                    "label": 0
                },
                {
                    "sent": "This is something that the brain itself has to do, and there's also a lot of interest in this issue of spike train decoding for brain computer interfaces.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem is essentially I'll go over it very quickly because this is something which essentially is shared amongst many of the talks.",
                    "label": 0
                },
                {
                    "sent": "Here you have an environment which we describe as a world state.",
                    "label": 1
                },
                {
                    "sent": "We have a sensory system with which observes this environment and we have some kind of system which is supposed to estimate the state or predict the state of the environment.",
                    "label": 0
                },
                {
                    "sent": "So the objective here is essentially based on this partial noisy sensor input to estimate the world state.",
                    "label": 1
                },
                {
                    "sent": "Now, of course noise is ubiquitous, it's everywhere in this system and this is something which has to be which has to be.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rest, So what we'd like ideally as similarly to many talks that have been presented here, is to compute the posterior distribution, which is the distribution of the current state given the sensory input, or if you were interested in prediction, the probability of the next state given the sensory input.",
                    "label": 0
                },
                {
                    "sent": "We'd like to do this in continuous time because neural neural data neural processing is taking place in continuous time, we don't want to lose any information.",
                    "label": 0
                },
                {
                    "sent": "Related to binning and discretization affects, we'd like to do this online so that each spike can be processed the PON arrival and we like to do this in real time so we have a fixed computational load which does not increase as the temporal window increases an since we're trying to propose a mechanism by which brain can can do spike train decoding, we'd like to think of a way how to implement this.",
                    "label": 0
                },
                {
                    "sent": "This online real time decoding by a neural network.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main results are the following, so I'll present the precise model in a moment, but the main assumption we have is that the state is described by some kind of kind of Markov process.",
                    "label": 1
                },
                {
                    "sent": "The mathematical foundations that to this field have been around since the 1970s, although unfortunately I think many people within the computational neuroscience literature or not aware of this long history of of people dealing with this issue of what I would call now point process filtering.",
                    "label": 0
                },
                {
                    "sent": "We suggest an implementation of this exact filtering by a simple bilinear neural network, and we extend the original framework that was developed during the 70s.",
                    "label": 1
                },
                {
                    "sent": "Too noisy inputs to history dependence spike trains and to multiple multiple multimodal inputs and to prediction that skip the line there.",
                    "label": 0
                },
                {
                    "sent": "We can demonstrate some well known results from the static case, which have been widely studied within the computational neuroscience literature so well.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Comparing looking at various algorithms and approaches people have developed over the years, there are many, many dimensions across which you could compare these models.",
                    "label": 0
                },
                {
                    "sent": "In particular, our work will deal with the as is fit for this workshop.",
                    "label": 0
                },
                {
                    "sent": "We have this dynamic stimulus continuous time Markov process.",
                    "label": 0
                },
                {
                    "sent": "The space we will consider here is finite.",
                    "label": 0
                },
                {
                    "sent": "The solution we propose is exact.",
                    "label": 0
                },
                {
                    "sent": "It's neurally implementable an.",
                    "label": 0
                },
                {
                    "sent": "OK, the receptive fields I'll get to in a moment and we include environmental noise and we and we deal with some issue which is very important in an in a neural context which is adaptation.",
                    "label": 1
                },
                {
                    "sent": "So the main restrictions of our work it's exactly the result.",
                    "label": 1
                },
                {
                    "sent": "We present this exact, except that we're restricting ourselves to finite state and Markovian dynamics.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, it's obviously you know.",
                    "label": 0
                },
                {
                    "sent": "Obviously sensors have a finite resolution, but this is just a, you know, that's a week a week answer the.",
                    "label": 0
                },
                {
                    "sent": "Ideally we would like to be able to address continuous spaces.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, much more complicated systems of equations, but this is something that we do want to do.",
                    "label": 0
                },
                {
                    "sent": "We just haven't done it at this point.",
                    "label": 0
                },
                {
                    "sent": "So the problem setup is the following.",
                    "label": 0
                },
                {
                    "sent": "We have XT which is the process representing the world state.",
                    "label": 0
                },
                {
                    "sent": "We have a set of Spike Trains N 1 N 22 NM representing the responses of sensory cells, and these these.",
                    "label": 1
                },
                {
                    "sent": "These responses can be, you know partial, noisy adelaid, redundant, what have you and we'd like oops.",
                    "label": 0
                },
                {
                    "sent": "And we'd like to have a network here, which is a decoding network which is able to decode the state of the world given the sensory input.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And assumptions are that we have the world state is given by a finite state continuous time Markov process.",
                    "label": 1
                },
                {
                    "sent": "We are characterized by a generator matrix Q.",
                    "label": 1
                },
                {
                    "sent": "The sensory activity is given by.",
                    "label": 0
                },
                {
                    "sent": "For now I will assume them to be person processes N 1 to NM where the rates of each person process the rate of each person processes Lambda one of XT to Lambda M of XT.",
                    "label": 0
                },
                {
                    "sent": "So and these are generating the generating the spike trains.",
                    "label": 0
                },
                {
                    "sent": "So essentially what we have what is called in the in the stochastic process literature, doubly stochastic process and process.",
                    "label": 0
                },
                {
                    "sent": "And again, what we'd like to do is compute this posterior probabilities.",
                    "label": 0
                },
                {
                    "sent": "And again, we'd like to do this in an online way, an online real time computation and demonstrate how this can be implemented by noon.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Network.",
                    "label": 0
                },
                {
                    "sent": "So historically, as I said, the as a people here know very well these ideas within this field were formulated in the 60s by people like Kushners Archive on Ham and many others, and their works for essentially diffusion Gaussian processes and later extended in the 70s to point processes by people like Snyder Sehgal and Cal ETH, which I get.",
                    "label": 0
                },
                {
                    "sent": "Some of them have already been mentioned previously here.",
                    "label": 0
                },
                {
                    "sent": "So this work really relies on this rigorous theory and demonstrates how it can be implemented in real time, and we provide various extensions and generalizations of these of these ideas.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the key concept is what is called in the stochastic filtering in the optimal filtering literature, these Akai equations and izakaya's equations essentially provide a non a simple filter for the non normalized probabilities.",
                    "label": 1
                },
                {
                    "sent": "So the general problem of filtering is is not a non optimal solution is nonlinear.",
                    "label": 0
                },
                {
                    "sent": "However there's a key equations are very neat trick too.",
                    "label": 0
                },
                {
                    "sent": "To transform the problem into a linear problem.",
                    "label": 0
                },
                {
                    "sent": "So what can be shown and I won't show it here, but this is by now a standard approach that it showed that there is a special set of functions row I = 1 to N, where N is the number of states such that the posterior probability is given.",
                    "label": 1
                },
                {
                    "sent": "Just buy a re normalized version of these rohais so this row eyes are non negative variables which when normalized provide the posterior distribution.",
                    "label": 1
                },
                {
                    "sent": "Now the equations that the differential equations Obaid by this realize are much simpler than the differential equations Obaid by the posterior distributions.",
                    "label": 0
                },
                {
                    "sent": "So the computing probabilities is hard.",
                    "label": 0
                },
                {
                    "sent": "However, computing an unnormalized probabilities is easy, and this was demonstrated in the 60s for the diffusion processes and later in the 70s for point process filled.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "During.",
                    "label": 0
                },
                {
                    "sent": "So going through Ascentia Lee, a set of by now standard mathematical manipulations if based on their son of theorem and various various change of measure techniques, one comes up with the following set of equations.",
                    "label": 0
                },
                {
                    "sent": "For this for Ro again row is, this row is this non normalized probability vector.",
                    "label": 0
                },
                {
                    "sent": "Normalized probabilities Q is the generator matrix for the underlying Markov process.",
                    "label": 0
                },
                {
                    "sent": "New M essentially is the spike trainers.",
                    "label": 0
                },
                {
                    "sent": "This is a sum of Delta function.",
                    "label": 0
                },
                {
                    "sent": "This is essentially the spike train corresponding to the NTH sensory cell.",
                    "label": 1
                },
                {
                    "sent": "So this contains the spiking activity of this M sensory cell.",
                    "label": 0
                },
                {
                    "sent": "This matrix, Lambda M is this matrix.",
                    "label": 0
                },
                {
                    "sent": "Of so called tuning curves of these functions, which are which are the intensity functions for the person processes.",
                    "label": 0
                },
                {
                    "sent": "Lambda is just this sum of all these lambdas of Lambda, M's and so if we look at this equation we see quite neatly here a breakup of this equation into a part which contains information about the prior through this matrix Q which contains the information about the underlying dynamics of the Markov process.",
                    "label": 0
                },
                {
                    "sent": "We have a term.",
                    "label": 0
                },
                {
                    "sent": "Let's look at this term first.",
                    "label": 1
                },
                {
                    "sent": "This is a term which is just a global decay term which has kind of you can interpret is in a Bayesian way as a bias towards having no spikes.",
                    "label": 0
                },
                {
                    "sent": "And then this term is the only part which depends on the data through these spike trains, new M of T. So this is an exact filtering equation for the you see it's linear in row for the non normalized probabilities.",
                    "label": 0
                },
                {
                    "sent": "In between, the data is just the prior yeah, and I'll present the exact solution in a moment, so this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, equation already is essentially describes just a simple neural network which which is composed as following you have you have these this high quality posterior network.",
                    "label": 0
                },
                {
                    "sent": "This network has lateral connections queue.",
                    "label": 0
                },
                {
                    "sent": "We have these connections from the sensory layer to this posterior network here.",
                    "label": 1
                },
                {
                    "sent": "With these weights, Lambda, MSI and these are so, so the activity the their input here to this posterior network and they multiply the variable row and then we have this global decay term Lambda.",
                    "label": 0
                },
                {
                    "sent": "So this this simple filtering equation Maps very simply unto a simple.",
                    "label": 0
                },
                {
                    "sent": "I would call it a BI linear network, not a linear network, cause the variable row is the date.",
                    "label": 0
                },
                {
                    "sent": "It appears multiplicatively with the input here, so this is some simple yeah.",
                    "label": 0
                },
                {
                    "sent": "The environment is stationary in this case.",
                    "label": 0
                },
                {
                    "sent": "Well, the environment.",
                    "label": 0
                },
                {
                    "sent": "I mean this is the environment affects the system through the spike trades norm of T and it's at this point it's totally arbitrary.",
                    "label": 0
                },
                {
                    "sent": "I mean, I've assumed that the underlying for the in order to Darrow to derive this equation and show that it's optimal one assumes a Markovian dynamics of the environment, but but this except for that this is totally general.",
                    "label": 0
                },
                {
                    "sent": "So, but this is computing posterior distribution.",
                    "label": 1
                },
                {
                    "sent": "This is computing the unnormalized posterior distribution as you go along in real time.",
                    "label": 0
                },
                {
                    "sent": "So you mean if this Q matrix is changing?",
                    "label": 0
                },
                {
                    "sent": "Or yeah, so this assumes right?",
                    "label": 0
                },
                {
                    "sent": "So in that sense it assumes a stationary environment, yeah?",
                    "label": 0
                },
                {
                    "sent": "But I think it wouldn't be there.",
                    "label": 0
                },
                {
                    "sent": "Wouldn't be any problem here.",
                    "label": 0
                },
                {
                    "sent": "Having a time dependent queue, but I'm not sure I have to think about that.",
                    "label": 0
                },
                {
                    "sent": "So you have to roll out to 0 is the solution then.",
                    "label": 0
                },
                {
                    "sent": "Rodat is no because you you know if if it's a solution only if Q = 0 It's you know it's.",
                    "label": 0
                },
                {
                    "sent": "You know, as long as you get spiking activity, as long as you're getting inputs then then row will be non 0 because every time you get an input Rd changes.",
                    "label": 0
                },
                {
                    "sent": "So every spike, any input spike changes rule.",
                    "label": 0
                },
                {
                    "sent": "OK, you have this term.",
                    "label": 0
                },
                {
                    "sent": "So deep, but I, I thought that it is in the environment stationary the posterior network.",
                    "label": 0
                },
                {
                    "sent": "The posterior distribution should be stationary and here's a time changing so I don't understand.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean, you know the posterior distribution changes based on the spikes.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you look at if you look at the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, if you look at this or is it?",
                    "label": 0
                },
                {
                    "sent": "If you look at the posterior distribution, I mean this is essentially this depends on the particular spike train.",
                    "label": 0
                },
                {
                    "sent": "So this is all conditioned.",
                    "label": 0
                },
                {
                    "sent": "So this is the posterior distribution given the data you've observed.",
                    "label": 0
                },
                {
                    "sent": "So this depends on the specific spike train you've seen, so it depends on time explicitly.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, just a simple example of some particle moving and this is the trajectory and the blue dots represent the spiking activity of the cells.",
                    "label": 0
                },
                {
                    "sent": "And here you see the posterior probability of Aleutian coming out of that network and you can see essentially that when you have a small number of spikes you how you see an increased uncertainty here as would be expected.",
                    "label": 0
                },
                {
                    "sent": "OK, so so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The behavior of this system is very simple.",
                    "label": 0
                },
                {
                    "sent": "Between spikes, you just have this exponential evolution and upon the arrival of the NTH sensory spike, this function row is just multiplied by the tuning curve.",
                    "label": 0
                },
                {
                    "sent": "So if this is the row prior to the NTH spike and this is just the tuning curve, then an input came from the region S. Then it's just multiplied and the peak of row is shifted a little bit to the right depending on the tuning curve.",
                    "label": 0
                },
                {
                    "sent": "There's a closed form solution available here, because it's a very simple system of.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, just as a simple example, look at the static case where you have a constant input selected from a finite set and if we take a Gaussian now it's a Gaussian star prior because it's a Gaussian, but these are we're assuming discrete variables, so so you have this kind of Gaussian an assuming Gaussian tuning curves, so the posterior distribution in this case, again, is Gaussian with the mean and variance, and when the.",
                    "label": 1
                },
                {
                    "sent": "So Sigma X is the prior distribution of the X variable.",
                    "label": 0
                },
                {
                    "sent": "When this, when the prior is flat, we get this.",
                    "label": 0
                },
                {
                    "sent": "We find that the posterior mean is an average of the weighted average of the centers of these receptive fields.",
                    "label": 0
                },
                {
                    "sent": "So CMR the centers of these receptive fields an all this NM is the spiking activity of the MSIL and we see a weighted average of these activities.",
                    "label": 0
                },
                {
                    "sent": "Now this is a well known.",
                    "label": 1
                },
                {
                    "sent": "Proposal proposed in 1982 by Georgopoulos called the population Vector is used widely in in computational neuroscience.",
                    "label": 0
                },
                {
                    "sent": "An in motor control.",
                    "label": 0
                },
                {
                    "sent": "Then in many brain computer interfaces.",
                    "label": 0
                },
                {
                    "sent": "This is a basic element and it just pops out of this solution.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is basically well known stuff what we have.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looked at here is tried to extend it in various directions, so up to now we've assumed that we have some kind of hidden state which is observed through a sensory system and a posterior network is trying to decode it.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The sensory response here is a direct function of the state.",
                    "label": 1
                },
                {
                    "sent": "Now in reality, the world is a noisy place and we never observe the world.",
                    "label": 0
                },
                {
                    "sent": "I mean there may be fog, there may be noise.",
                    "label": 0
                },
                {
                    "sent": "We never observe the world directly, so so there is a noisy channel between the world.",
                    "label": 0
                },
                {
                    "sent": "And the sensory system which we will denote by WT.",
                    "label": 0
                },
                {
                    "sent": "So what we'd like to do is we'd like to extend this framework to be able to deal with with environmental noise.",
                    "label": 1
                },
                {
                    "sent": "So in this case the rate functions here Lambda are functions of the noisy state F of XT&WT and not of the actual state itself.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in order to model this, we've assumed that the noise is again a finite state Markov process, and we will make some assumption later, which about the time scale of this of this of this noise.",
                    "label": 0
                },
                {
                    "sent": "As you know, one cannot have white noise in continuous time.",
                    "label": 0
                },
                {
                    "sent": "However, XD and WT are independent, and we assume that the tuning curves OK, so the tuning curves are functions of the observed state through the noise.",
                    "label": 0
                },
                {
                    "sent": "For instance, you could have.",
                    "label": 0
                },
                {
                    "sent": "Additive noise Now the key observation in this derivation of the result here, is to observe that this augmented state X TWT is a Markov process of dimension N * L, where L is the dimension of this Markov process.",
                    "label": 1
                },
                {
                    "sent": "So what one can do here essentially is right recursive equations for computing the joint distribution of the state and the noise given a spike trains and then marginalized over the over these non normalized probability distributions.",
                    "label": 0
                },
                {
                    "sent": "To get the posterior distribution where it's averaged over the noise.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So going through the analysis here, what one comes up with is the following filtering equation, which where we have these variables fi M here, which are just the expectation values over these tuning curves.",
                    "label": 0
                },
                {
                    "sent": "Lambda M. Remember that Lambda M is a function of the noise through these tuning curves, so these are just the expectation over the noise distribution of the tuning curves.",
                    "label": 0
                },
                {
                    "sent": "They're just the average average.",
                    "label": 0
                },
                {
                    "sent": "Provide just the average.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sensory response, so just to demonstrate what this looks like here, we have.",
                    "label": 0
                },
                {
                    "sent": "If we have the clean dynamics of the of the state of the world and we add some noise to it.",
                    "label": 0
                },
                {
                    "sent": "This is the posterior network activity.",
                    "label": 0
                },
                {
                    "sent": "And if we try to filter to estimate the state based on the and this is the MSE, the error in the filtering error, the reconstruction error if we try to do that directly with the original equations, which did not take into account noise.",
                    "label": 0
                },
                {
                    "sent": "We get this result and obviously as expected, if we add if we add this if we replace the tuning curves by their average value according to our previous equation, we see that we indeed get improved performance.",
                    "label": 0
                },
                {
                    "sent": "Bye bye bye.",
                    "label": 0
                },
                {
                    "sent": "Taking account of the noise.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One interesting question here is how does the?",
                    "label": 0
                },
                {
                    "sent": "How does the?",
                    "label": 0
                },
                {
                    "sent": "How do the properties of the system depend on the level of noise?",
                    "label": 0
                },
                {
                    "sent": "So one could expect if you have tuning curves which are very narrow and.",
                    "label": 0
                },
                {
                    "sent": "They give you whenever they whenever they.",
                    "label": 0
                },
                {
                    "sent": "Whenever an object occurs within these receptive fields, we get very high precision.",
                    "label": 0
                },
                {
                    "sent": "However, we have rather poor coverage because their regions which are not covered by these receptive fields.",
                    "label": 0
                },
                {
                    "sent": "So we essentially have few spikes.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if we have very broad receptive fields, we can have low precision precision but good coverage and we have many spikes, but we cannot really resolve them, so one would expect naively that there is an optimal.",
                    "label": 1
                },
                {
                    "sent": "Tuning curve width, which I'm which which covers the domain in an optimal way and I'm assuming here that the number of tuning curves is fixed, because if I let them allow, if I allow them increase without limit, obviously I can.",
                    "label": 0
                },
                {
                    "sent": "There is no, there is no problem here.",
                    "label": 0
                },
                {
                    "sent": "So for a fixed by the fixed budget of a number of tuning curves, the question is whether there is an optimal value of the width here.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Make the same assumptions as previously and we look at the mercy of the optimal estimator so we look what we have here is as a function of Alpha, which is the width of the tuning curve.",
                    "label": 0
                },
                {
                    "sent": "We look at the MSE for different values of environmental noise.",
                    "label": 0
                },
                {
                    "sent": "So what we see here quite clearly is that for each value of the environmental noise we have a clear minimum in the MSE as a function of the width of the tuning curve, so.",
                    "label": 0
                },
                {
                    "sent": "If we plot this, if we plot the width of the optimal tuning curve as a function of the external noise, we see that this increases and ultimately one can even show analytically that this is a linear dependence.",
                    "label": 0
                },
                {
                    "sent": "So ultimately the optimal tuning curve width.",
                    "label": 1
                },
                {
                    "sent": "Is is the proportional essentially to the variance of the external noise, which makes sense and actually leads to an experimental prediction for adaptation by neural systems, which is something that we are planning to test on.",
                    "label": 0
                },
                {
                    "sent": "On the retina, working with some person is.",
                    "label": 0
                },
                {
                    "sent": "Does experimental work on the right now?",
                    "label": 0
                },
                {
                    "sent": "This is a specific experimental prediction about adaptation to increase noise.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that is 1 one specific prediction for this system.",
                    "label": 0
                },
                {
                    "sent": "Now another interesting application is to the case of history dependence.",
                    "label": 0
                },
                {
                    "sent": "Spike trains and adaptation.",
                    "label": 0
                },
                {
                    "sent": "So many people use person types of processes for dealing with spike trains.",
                    "label": 1
                },
                {
                    "sent": "However, they are limited in at least two respects.",
                    "label": 0
                },
                {
                    "sent": "First, obviously they lack memory and there are physiological phenomena like the refractory.",
                    "label": 1
                },
                {
                    "sent": "You know that after.",
                    "label": 0
                },
                {
                    "sent": "A neuron fires.",
                    "label": 0
                },
                {
                    "sent": "There is a certain.",
                    "label": 0
                },
                {
                    "sent": "Where it cannot fire, so obviously this cannot be described by person process and there is a very important phenomenon termed adaptation, where neurons essentially get bored.",
                    "label": 0
                },
                {
                    "sent": "So if initially they start spiking at a fast rate, their spiking rate decreases as time goes by and they and they they fire more slowly.",
                    "label": 0
                },
                {
                    "sent": "So these obviously cannot be modeled by Poisson processes.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we try to do here is to extend this framework to what are called self exciting point processes where the rates Lambda depend not only on the state of the world, but they can depend on the history of the spiking activity of the sensory cell.",
                    "label": 0
                },
                {
                    "sent": "So these include posson renewal processes and many other processes.",
                    "label": 0
                },
                {
                    "sent": "So without going through the derivation, essentially it's not hard to show that the derivation goes through.",
                    "label": 0
                },
                {
                    "sent": "The only difference is that now these tuning curves are essentially history dependent and they depends specifically on the history of the spiking activity of this of this cell.",
                    "label": 0
                },
                {
                    "sent": "The assumptions that this this is process is not markup anymore does not enter this.",
                    "label": 0
                },
                {
                    "sent": "No no, I mean it.",
                    "label": 0
                },
                {
                    "sent": "It's right, right?",
                    "label": 0
                },
                {
                    "sent": "Well, yeah, as long as it can be characterized as long as the as long as you have these are processes which can be described by a rate function, not all stochastic.",
                    "label": 0
                },
                {
                    "sent": "But if you can describe it by a rate function with essentially an up sensually an arbitrary history dependency framework still goes through.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "Is there any?",
                    "label": 0
                },
                {
                    "sent": "Maybe later, yeah, so so I mean.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so the So what we were trying to do here is to try to model.",
                    "label": 0
                },
                {
                    "sent": "The phenomenon refers to as adaptation, so we have the now.",
                    "label": 0
                },
                {
                    "sent": "So a specific model we considered here is this tuning curve which depends on time in the following way it's given by multiplication of mu times Phi, where Phi is a standard tuning curve.",
                    "label": 0
                },
                {
                    "sent": "Now Mu has the following behavior between spikes, it just relaxes exponentially towards one.",
                    "label": 0
                },
                {
                    "sent": "Now upon the arrival of a spike.",
                    "label": 0
                },
                {
                    "sent": "It gets pushed down so so the you know, physical idea here is that whenever a spike whenever a neuron emits a spikes, its resources are depleted and it gets tired.",
                    "label": 0
                },
                {
                    "sent": "It cannot fire for a few milliseconds, so we model this by just by just pushing the tuning curve value down, and therefore the probability of emitting spike is lowered upon the emission of a spike.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially models the phenomenon of adaptation.",
                    "label": 0
                },
                {
                    "sent": "And what we see here, essentially what we plot here as a function of Tau for various values of Delta, we plot the spike count ratio.",
                    "label": 0
                },
                {
                    "sent": "So this is just the number of spikes we have in the system compared to the number of spikes without adaptation.",
                    "label": 0
                },
                {
                    "sent": "And here we plot the MSE ratio as a function of this parameter Tau.",
                    "label": 0
                },
                {
                    "sent": "So what we see if we look for instance at a value of Tau equals one and Delta equals .2.",
                    "label": 0
                },
                {
                    "sent": "We see that we have only about.",
                    "label": 0
                },
                {
                    "sent": "We have 70% of the spikes we had without adaptation.",
                    "label": 0
                },
                {
                    "sent": "However, the performance has improved.",
                    "label": 0
                },
                {
                    "sent": "Now the MSE is .8 of it what it was before, so this adaptation process essentially allows you to reduce the number of spikes quite significantly and at the same time improve performance.",
                    "label": 0
                },
                {
                    "sent": "Which is kind of interesting from an optimization point of view now.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Explanation for that is is that if you look at the.",
                    "label": 0
                },
                {
                    "sent": "This remember the equation had this self inhibition term minus Lambda times row.",
                    "label": 0
                },
                {
                    "sent": "So if you look if without it without adaptation we have the this variable Lambda is more or less constant, except that it goes to a decreases at the boundaries of the domain.",
                    "label": 0
                },
                {
                    "sent": "What happens is that cells near the true state in the case of the when I allow adaptation, their value of Lambda decreases because of this adaptation and therefore the self inhibition term.",
                    "label": 1
                },
                {
                    "sent": "Is reduced and these cells are less inhibited so so we can actually have a pretty mechanistic explanation of why adaptation is efficacious in neural system.",
                    "label": 0
                },
                {
                    "sent": "It actually improves the signal to noise ratio and and reduces at a reduced number of spikes.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How much time do I have?",
                    "label": 0
                },
                {
                    "sent": "It's about out of time, so I'll skip this issue of multi sensory integration essentially.",
                    "label": 0
                },
                {
                    "sent": "And I'll just move towards the end of the talk.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, one other thing.",
                    "label": 0
                },
                {
                    "sent": "I should point out in passing that this framework, as you may imagine, can be extended quite simply to the case of prediction where we are trying to predict the state not at time T but at time T plus talk given the spiking activity up to time T, and it pretty straightforward modification of the filtering equation leads to the same type of equation for the prediction, except that these matrices Lambda here are multiplied on the left and right by these.",
                    "label": 0
                },
                {
                    "sent": "Transition probabilities P. But essentially it's the same kind of by linear equation, which can describe a neural network which can implement this system.",
                    "label": 0
                },
                {
                    "sent": "This prediction posterior.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Prediction.",
                    "label": 0
                },
                {
                    "sent": "So let me just summarize so.",
                    "label": 0
                },
                {
                    "sent": "So we presented a spike based filtering of continuous time Markov process.",
                    "label": 1
                },
                {
                    "sent": "We've shown how to implement it online using bilinear neural networks provides a mathematically rigorous treatment of continuous time in this neural context.",
                    "label": 1
                },
                {
                    "sent": "No information temporal information has been lost.",
                    "label": 0
                },
                {
                    "sent": "We've been able to deal with multimodal effects easily, and I haven't described that, and there are many possible extensions we've discussed, and again, something which I haven't shown many.",
                    "label": 0
                },
                {
                    "sent": "A lot of the work in the literature has involved the static case and all the results from the static case that we came across in the literature actually turn out to be special cases of this general dynamic formalism.",
                    "label": 0
                },
                {
                    "sent": "So there are many extensions possible.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you know, learning these matrices Q and Lambda using more distributed and robust representations, continuous state spaces, and ultimately we'd like to have a good physiological interpretation.",
                    "label": 1
                },
                {
                    "sent": "An implementation of these models and make predictions for specific biologic experiments, which is something we are currently doing with regard to.",
                    "label": 0
                },
                {
                    "sent": "A retinal preparation.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Time for one.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "There is biological evidence for this type of.",
                    "label": 0
                },
                {
                    "sent": "I mean for the behavior and this adaptation model, for instance, was modeled after some kind of biological adaptation, and this specific structure of the network we consider there is the specific nature of the multiplicative interaction between the input and the state.",
                    "label": 0
                },
                {
                    "sent": "There is evidence for that in certain neural systems, but this is not something that at this point you know I would like to talk too much about because there was something we only recently looking into, but.",
                    "label": 0
                },
                {
                    "sent": "But there is definitely.",
                    "label": 0
                },
                {
                    "sent": "There is a recent paper by some Polinsky and Meister which proposed a very similar model and they provide quite extensive evidence.",
                    "label": 0
                },
                {
                    "sent": "Experimental evidence for the.",
                    "label": 0
                },
                {
                    "sent": "For the existence of such types of networks.",
                    "label": 0
                },
                {
                    "sent": "When you have this history dependent race because it seems to me that this generative model is no longer kind of Markovian, why is it still possible to like the filtering in such a stinky playing on my missing something?",
                    "label": 0
                },
                {
                    "sent": "Well, because I mean, you know, because we're assuming.",
                    "label": 0
                },
                {
                    "sent": "Essentially I mean, all these equations are all conditioned on the spiking activity.",
                    "label": 0
                },
                {
                    "sent": "So basically the.",
                    "label": 0
                },
                {
                    "sent": "We're not averaging over the spikes, so this doesn't.",
                    "label": 0
                },
                {
                    "sent": "I think this is a bit of a technical point.",
                    "label": 0
                },
                {
                    "sent": "Maybe we should go over it if you have to really look at the derivation to understand this, but it goes through pretty clearly.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}