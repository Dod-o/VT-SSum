{
    "id": "dfprqouflcapa3vhpwsgqkigg3tfpbt4",
    "title": "Density Ratio Estimation in Machine Learning",
    "info": {
        "author": [
            "Masashi Sugiyama, Tokyo Institute of Technology"
        ],
        "published": "Dec. 3, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Technology->Neurotechnology",
            "Top->Computer Science->Machine Learning",
            "Top->Medicine->Neuroscience"
        ]
    },
    "url": "http://videolectures.net/bbci2012_sugiyama_machine_learning/",
    "segmentation": [
        [
            "OK, hello, so my name is massages Yama.",
            "I'm from talking shit about technology so I'm actually a machine learning person on my talk will be really like pure machine learning.",
            "So today I would like to introduce a new framework of machine learning and we have already developed several algorithms that can be easily implemented and we have nice theory behind and also we have many applications, but unfortunately I didn't have a lot of applications in like neuroscience, PCI X signal processing, so actually.",
            "It's my great pleasure to give a talk here today 'cause we have so many nice PCI people here, but it's very nice to have applications in then and also I'd like to thank organizers for inviting me to give a talk here.",
            "OK, so then my talk is about density ratio estimation, so this is the."
        ],
        [
            "You want all of the talk.",
            "OK, a general goal of machine learning is to learn some information hidden behind data forgiven data.",
            "We want to learn something from the data.",
            "And as a machine learning researcher, we are always looking at many different machine learning tasks and I just listed a lot of learning tasks that I have in my mind.",
            "You don't really have to see carefully but just know there are many machine learning tasks here."
        ],
        [
            "And one of the most universal and general approach to solving these machine learning tasks would be to learn data generating probability distribution.",
            "'cause once we can estimate the data generating probability distribution, basically we can know everything about the data.",
            "We can reproduce the data and we can whatever properties of the data.",
            "So in a sense, this is the most general approach.",
            "Most University.",
            "This is a universal approach and we can basically solve all machine learning tasks tasks if we can estimate the probability distribution behind the data.",
            "For example, in the simplest case of pattern recognition, so we have two classes of samples and if we estimate the probability distribution of class one and class minus one, then so we have some densities like this.",
            "And OK, then compare the densities and from that we can know the decision boundary between two classes.",
            "So part time recognition can be solved by estimating data generating probability distribution.",
            "OK, so this is the most general approach, so in a sense the best way to do machine learning resources to study density estimation."
        ],
        [
            "However, estimating probability density probability distribution is known to be difficult, so this card means without having any good prior knowledge, we can't really have accurate estimate.",
            "Like if we can easily suffer from the curse of dimensionality, for example.",
            "So in a sense, avoiding probability distribution is really important.",
            "But still we want to achieve.",
            "We want to solve a certain task.",
            "So support vector machine is the electrical example following this line that I call task specific approach.",
            "It doesn't involve density estimation, but it directly solved the target problem.",
            "So in the case of support vector machine, we don't really estimate that data generating distribution of class one and class minus one.",
            "But we directly learn the decision boundary from data.",
            "This was shown to work quite well in practice.",
            "OK."
        ],
        [
            "Then so.",
            "And in principle, this task specific approaches can be more accurate than the universal approach of density estimation, 'cause we are really directly solving the machine down target machine learning tasks.",
            "So ideally we want to we had many machine learning tasks, so we want to ideally develop algorithms for each of the tasks and design nice algorithms.",
            "But the discourse, research and development for each machine learning task is actually very high that we have so many different machine learning tasks, and we only have a small number of machine learning resources.",
            "So it's not really possible to develop nice algorithms developed, nice machine learning theory, and to have very fast implementation for each of the tasks.",
            "So still we want to have a kind of universal approach to cope with this problem."
        ],
        [
            "So the target of my talk today is kind of an intermediate approach.",
            "So we want to develop machine learning algorithms for a group of tasks, so I don't say the entire task, but we can start a group subset of tasks.",
            "So this is an image, so we have many machine learning tasks and if we do density estimation then basically we can cover everything.",
            "So this is the most general approach.",
            "But as I said before, this mission is a difficult problem.",
            "So on the other hand, like support vector machine, if we develop an algorithm for specific tasks, then OK, it's the most powerful approach.",
            "But then we need to develop many different algorithms for different tasks.",
            "This is quite costly.",
            "So we want to take on kind of intermediate intermediate approach that covers a certain class of tasks, but hopefully some useful interesting machine learning tasks are still included in this class.",
            "So that's that's my target today.",
            "And more specifically, here we use the notion of density ratio so that the keyboard.",
            "So I listed many tasks here, but actually this tasks have."
        ],
        [
            "Come on property that is, they have they include multiple probability distribution.",
            "So let's say two distribution and say densities PMQ.",
            "And of course, if we estimate P&Q ball, then we can solve the target stuff.",
            "But actually we can show that for solving these tasks we don't really have to know P&Q both separately, but we just need to know their ratio.",
            "So P / Q.",
            "So I define it as a ratio of.",
            "So we don't really have to know P&Q, but we only have to know R. So why don't we directly estimate the ratio function or without really estimating P&Q?",
            "So then we can avoid doing dense decimation of P&Q.",
            "But still we can solve a class of tasks.",
            "Of course, it's not all, but I argue that for this density ratio framework includes a lot of interesting and practical tasks.",
            "So once we develop a very nice algorithm to estimate the density ratio, we can we can give a solution to a class of tasks at the same time."
        ],
        [
            "And here is a simple, intuitive justification of this density ratio approach.",
            "So let me about Nick is a founder of front of support Vector Machine and in his book he said this kind of thing.",
            "So when solving a problem of interest, one should not solve a more general problem as an intermediate step.",
            "So this is so I call it principle, so it's not a theorem, so we can't really prove this this fact, but it's kind of nice.",
            "You know, guiding principle for machine learning researchers to develop practically useful algorithms.",
            "So in our context, so we don't really have to know two densities, but we only have to know the ratio of them.",
            "So if we know the densities, then of course we can compute the ratio by this dividing P by Q.",
            "But if we know the ratio, we cannot decompose this one into P&Q uniquely.",
            "So this roughly means the left problem is more general than the right problem.",
            "So if we follow this public Sprint store then so we shouldn't estimate two densities, but we just have to estimate the ratio so that the best way to solve the problem.",
            "So whether there was a public 75th birthday symposium last year, December tubing and I had the chance to present distance eraser work in photo in front of that near and he said, OK, he liked actually this idea, so now I'm happy to say that I can use public Springfield with his support.",
            "So anyway, so now we want to estimate the density ratio, so without really going through density estimation of PM PM here so that the machine learning part of my talk."
        ],
        [
            "OK, and let me quickly conclude my talk now 'cause its been longer later.",
            "So first of all, density ratios can be accurately on computationally absently, estimated by just like a simple least squares.",
            "So no complex method is necessary, just the squares you know.",
            "So this means, and I'm saying that once we know the density ratio, we can solve a class of tasks.",
            "So this means this class of tasks can be solved just by this square.",
            "We don't really need the complex knowledge.",
            "And the tasks roughly includes important sampling techniques or some divergent estimation techniques or mutual mutual information estimation techniques and conditional probability estimation techniques.",
            "So all these tasks can be solved just by estimating the installation.",
            "Is it clear?",
            "OK."
        ],
        [
            "So then from now.",
            "So let's go into the technical parts.",
            "So in the next part I explain 2 methods of density ratio."
        ],
        [
            "So basically my images like we have many bars here, browser tasks and Stone is a dense racist method method and I want to kill all bars just by throwing barnstorm.",
            "So that's an ideal goal."
        ],
        [
            "And I want to explain the ultimate storm in this section.",
            "But before that I introduce a bit simpler method.",
            "The first method and then I introduce second method and the second method is a kind of ultimate storm to kill all bars.",
            "And then after that I explain how we can really use these denser estimators to solve various machine learning tasks.",
            "And if you have some time, so I want to say a little bit more on density restoration.",
            "Alright, so let's start from the methods of dense."
        ],
        [
            "Richest nation?",
            "The first let me formulate the problem, so as I said, the goal is to estimate the density ratio function.",
            "So from now two densities are denoted by P numerator and denominator.",
            "So two density function and we want to estimate this ratio function from two sets of samples.",
            "The first step is coming from the numerator and the second set is coming from P denominator.",
            "So we have two sets of samples and from these two sets of samples we want to learn the ratio function."
        ],
        [
            "And of course, the simplest way would be to just perform dense destination so we have two sets of samples, so we just estimate each density facpe numerator.",
            "Hutton P Denominator had from each sample, and then compute their ratio.",
            "But I am arguing that this is not the valid abroad becausw the first step of estimating densities.",
            "Is performed without regards to the second step of taking the ratio.",
            "Of course we can perform density estimation in optimal way, but by using some nice algorithms.",
            "But still we always have estimation error in density in densities.",
            "So then by taking their ratio the error small error included in the first step can be magnified in the second step.",
            "So in the simplest case, if the denominator has a very small value at 0.001, so then the small error included in the P numerator is magnified by acting then to the power four or five or something like that.",
            "So this doesn't work at all if the dimension is Lau.",
            "So we want to have a single single sub procedure that estimates the density ratio directly.",
            "So that."
        ],
        [
            "Target.",
            "OK, so I introduced two methods now.",
            "The first method is called density fitting approach."
        ],
        [
            "OK. OK, we want to directly estimate the density ratio function, so let's.",
            "Use all hop as our density ratio model, so this is a function we want to learn.",
            "I will give a specific form later, but for now let's just use our heart as a model.",
            "And the basic idea of this density fitting approach called callback library important estimation procedure is to minimize the kullback Leibler divergent from P numerator to estimate given by this form.",
            "So this form means the ratio is P numerator over P denominator and if we if we multiply our model with the integration model with the denominator, we can estimate the numerator.",
            "So now we minimize these two divergent.",
            "These too clever clever diversions.",
            "So we minimize this one with respect to our heart.",
            "So this is the formulation of the algorithm called creep.",
            "And it's.",
            "Easy to see that we can decompose this log part into the constant part and a term that includes our model our hub.",
            "So then constant can be of course ignored, so minimizing kullback Leibler divergent."
        ],
        [
            "Is now equivalent to maximizing the second term, so this down.",
            "So this is our objective function to maximize.",
            "But we need a little more constraint becausw this P numerator had should be a probability density function.",
            "So this means the integral of this function is 1 and also the model should be non negative.",
            "So by that we can guarantee that this P numerator is a density function.",
            "And here we introduce a specific form of a model we can use.",
            "We can actually use many different types of models, but for simplicity let's focus on a linear in parameter model.",
            "So we just consider a fixed basis function and we take the linear combination and learn this coefficient Alpha from data.",
            "So that the simplest of laws in practice we use Gaussian kernels were sent out on sample points or something.",
            "Any models can be used here?",
            "OK, that's the creep formulation.",
            "And.",
            "OK, now we have like integration to integration here and this is actually an expectation of local hop over P numerator.",
            "And this is an integration of our heart over P denominator.",
            "But these two are basically expect expectations, so it's an expectation of our look of our model over P numerator and this is an expectation of our model over P denominator and expectations can be approximated by sample operating.",
            "So we just introduced."
        ],
        [
            "Sample operates here, so then we end up in having this problem like this.",
            "So it looks a bit complicated at first sight, but this is a convex optimization problem.",
            "So this means just doing gradient descent and projection onto feasible, feasible reason, repeating this one several times gives you the global solution, so it's very simple.",
            "And also because of this non negativity constraint, actually we can show that the global solution is sparse and sometimes this sparseness is quite useful, but not always.",
            "Anyway, so this is an algorithm of creep.",
            "So by that we can directly estimate the density ratio function without really doing against estimation of being numerator and denominator.",
            "So the basic point was, so we just have expectation of our model of up enumerate."
        ],
        [
            "Therapy denominator.",
            "So then we don't really have to estimate densities, but we can just replace the expectation by sample rate.",
            "So this is a simple tree tree, but we can avoid destination by that."
        ],
        [
            "OK, and.",
            "I want to show a little theoretical properties of this method.",
            "The only approximation we introduced as the empirical approximation expectations while based by sample averages.",
            "And now we can start two scenarios and in the first novel it's a parametric case where our model is already fixed, so we have a fixed number of basis functions and we have fixed basis function and we just learn their question.",
            "So this is a problematic situation.",
            "And in this parameter situation, learn parameter by the clip algorithm converts to the optimal value in the model with order of N minus half.",
            "Here into the power of minus half, where N is the minimum of a numerator and denominator.",
            "So these two are the number of samples from the numerator densities and they don't make the testing.",
            "And this N minus half is actually the optimal convergence rate in the parametric setup.",
            "So we can simply say that no method can outperform this approach.",
            "So this is a very simple approach, but this is already optimal.",
            "OK, the Prometric case is rather simple, but now so we can start a nonparametric case where the number of parameters that grows with the number of samples.",
            "But it's a typical situation, model.",
            "If we put the kernel function on the samples, then the number of basis functions number of primary objective grows.",
            "Don't like this form?",
            "And in this case, so we can have a similar result that the Lambda function by by this model still converge the optimal function with order of disorder.",
            "So end to the power of 1 / -- 2 plus, and gamma is a kind of complexity measure of the function graph rated to the covering number of bracketing entropy.",
            "So I don't really have to introduce a very technical details here, but we have like this order and disorder was already shown to be the optimal rate.",
            "But none of them is can go beyond this order, and our simple method achieves this order.",
            "So this means this is already an optimal method.",
            "Right, so the idea was quite simple.",
            "We just fitted density and density estimated by that interracial forfeited under collaborative options, but this was shown to be optimal here with theoretically and algorithm is quite simple."
        ],
        [
            "Here is just illustrative toy example, so we have two densities, P denominator, it's blue Gaussian like this and the numerator black Gaussian like this and we took 200 samples from each densities.",
            "So then the true density, it true density ratio is described by the Red Line red line here.",
            "And estimated density by the clip algorithm was shown here by green line.",
            "So in this case the clip algorithm gave a very nice approximation to the density to the density ratio function.",
            "So this direction, this ratio estimation is kind of learning real valued function, so it looks similar to regulation 'cause we are learning function like this.",
            "But the difference from the regulation is that we don't really have samples from the ratio function or directly like if we have samples in this space then it's a regression problem.",
            "But in the case of distress estimation, we have samples in this space, so we don't really know the value of our directly.",
            "But still we can estimate the ratio function in a simple way like this.",
            "And in this clip procedure, there is a tuning parameter that is the Gaussian with if he is a Gaussian kernel function.",
            "So this experiment was done with the Gaussian corner.",
            "And of course if you change the girls song with the solution changes and discussion with it too too large, we have to flap function and if they got some of it is too small, we have highly fluctuated function so it should be tuned carefully.",
            "And for this purpose we can just use a simple cross validation 'cause we have an objective function, so it's a second term of the clever clever diversions.",
            "So we have here.",
            "So we have this decomposition at this time.",
            "So we are Mac."
        ],
        [
            "Amazing this time so we can use this criterion."
        ],
        [
            "Cross validation.",
            "If you have a set of samples, let's say use 80% for estimating a function density ratio function and use the remaining 20% to validate this error.",
            "This score so this is maximized.",
            "And for each location with, we prepare a set of Gaussian within the beginning candidate values, and then for each of the values we train, we learn offensive racial function and we estimate the score global score and then we choose the the Gaussian width that maximizes this score.",
            "And this solution was obtained like that.",
            "So in the end, so there's no tuning parameter here.",
            "So given data, we just do cross validation under we obtain so everybody can obtain the same density ratio function.",
            "So later after this this this property turns out to be quite important 'cause sometimes we want to solve unsupervised learning problem.",
            "So in the case of supervised learning problem, maybe we can somehow tune parameters easily.",
            "But in the case of unsupervised learning, basically there's no way to tune parameters and we just change the parameter and observe the result whether it looks nice or not.",
            "But if you have, if we use the clip algorithm for for some superb for unsupervised learning tasks, still we can do cross validation and the objective solution can be obtained in the easy way.",
            "OK, so this is the."
        ],
        [
            "Area of the clip algorithm.",
            "So it doesn't involve density estimation, so that's the first important point, and hopefully this improved estimation quality.",
            "And as I said, cross validation is available for Connor parameter for example.",
            "So this is quite helpful, helpful in practice.",
            "And I explained an algorithm for a linear model linear in parameter model, including candle models.",
            "But this can be extended to other types of models like log linear model or Gaussian mixture or PCM itself.",
            "We can develop a similar algorithm.",
            "And also this clip algorithm is actually useful to estimate the kullback Leibler divergent between two things, two distribution.",
            "This is partially becausw unconstrained variant of this clip algorithm corresponds to maximizing our lower bound of quiver quiver divergent.",
            "So through that so we can estimate the claim that I got this so I will go back to this this point later, but this clip algorithm is suitable for estimating the critical about I was there.",
            "OK, so that was the first method and is it?"
        ],
        [
            "Do you have any questions so far?",
            "Alright, so then, so that's the first method and then, so let's move to the second method that I think is the most powerful approach.",
            "So idea is actually quite similar, it's."
        ],
        [
            "Call the density retrofitting.",
            "So the method is called least squares important fitting, at least IF.",
            "And now we use the squared loss.",
            "So I'll have this again.",
            "Our density ratio model and let's consider the squared error between our model and through ratio function squared.",
            "And we have a weight function P denominator here, so this is actually important.",
            "And then now let's just decompose.",
            "This are hot minus R-squared into our hot square, and two times are hot and all and ask where they just said competition.",
            "And then if you see the first term, but this is the, this is the expectation of our hot squared over P denominator.",
            "So again, extension can be approximated by sample operates like this.",
            "And for the second term.",
            "So this was originally our heart, our PDE.",
            "But how is the numerator over P denominator?",
            "So PDE canceled out here and we only have P numerical here.",
            "So it's just an expectation of our heart over P numerator, so again this can be approximated by some problems simply like this.",
            "And the last time ask here is a constant, so it can be ignored.",
            "So this is our training criteria.",
            "So now we didn't do anything special, but we just introduce squared loss and just compute it."
        ],
        [
            "And again, let's introduce linear in parameter model like this.",
            "So in practice this could be a Gaussian linear combination of Gaussian kernel function.",
            "So then the based on this list I formulation we can consider two algorithms.",
            "The first one is called constraint SIF.",
            "So in the first formulation we add.",
            "And negativity constraint of the parameters.",
            "And we include L1 regularizer.",
            "So this is actually one regular, it's just some of parameters, but we have no negativity constraint.",
            "So then this is actually the L1 penalty.",
            "And OK, so on each half an 8 update have matrix is defined by this and each had Victor is defined like this.",
            "So these are just computed from samples and also our basis function.",
            "Maybe got some kind of function?",
            "And this is actually a standard convex quadratic program, so we can solve it just by using a standard QP solver.",
            "And we know that with L1 regularizer the solution becomes sparse.",
            "So this is the first formulation.",
            "But actually this."
        ],
        [
            "Maybe even more computationally efficient because we can actually track the focal regularization path.",
            "So this is the formulation and actually we can show that the solution path over Lambda if we change Lambda we want to see how the solution changes.",
            "If you consider this solution path with respect to Lambda, this is actually piecewise linear.",
            "So let's see if Lambda is very large, like 1,000,000.",
            "So then basically these two terms are ignored and we only care the L one term.",
            "So then minimizing vantomme means 0.",
            "So if Lambda is very large, so the solution is at the origin in the beginning.",
            "Then from this large Lambda we reduced the value of Lambda and then at some point the solution starts to depart from the origin and go straight to some point.",
            "And then at some point Lambda one.",
            "So actually the direction changes and again the solution calls straight to another point here.",
            "And at Lambda, two again goes like this and #3 and like that.",
            "So the solution path is actually piecewise linear like this.",
            "And the important point is that we can compute this change points in advance without really solving a QP problem.",
            "Like in the beginning, in the first step we know the solution is at the origin, so we don't have to solve any optimization problem.",
            "So then the second, the second point, the first change point can be computed from this solution and this part we need to solve a system of linear equations, but by that we can obtain this point analytically.",
            "And also like the second part, second point can be computed from the first point by solving a system of linear equation like that.",
            "So just by solving linear equations we can know all the change points and all the solution path.",
            "And this is actually quite important because we don't really need QP solvers anymore.",
            "We just have to solve a system of linear equations.",
            "And also so I already talked about cross validation, but we also need to tune Lambda impact in practice.",
            "That means that we prepare different values of Lambda and compute the solution and compare the cross validation score.",
            "But once we take this regularization path approach, we know the solution for all Lambda.",
            "Actually, the computational complexity for obtaining the entire solution path is the same as solving a single QP problem in the computation order, so it's quite different, and it's quite efficient, including model selection of Lambda.",
            "So that's quite a convenient approach."
        ],
        [
            "OK, that was a constraint formulation.",
            "And finally, I want to introduce even simpler approach is its unconstrained alist LSI formulation.",
            "I call it Elsif, unconstrained LSI F. And in us, if we forget the negativity constraint.",
            "To make it simple.",
            "And then so we replaced the L1 constraint to to complain, but without.",
            "Without non negativity constraint this this term cannot be a regularizer anymore.",
            "So we just use a simple penalty.",
            "So then if you are familiar with Richard Grayson, so this is similar to a form of Revelation.",
            "So as a function of Alpha, it's just a quadratic function, so without any constraint.",
            "So if you take the derivative and equate it to zero, you can obtain the solution analytically like this.",
            "So this H hot matrix on a chart vector are the same as the previous one.",
            "So in the end, so that the ratio can be just estimated by rhythm, these squares regularize this clear.",
            "So it's an unconstrained formulation.",
            "And another important computational advantage of this unconstrained LSI F is that leave one out cross Validation Square."
        ],
        [
            "Can be computed negatively.",
            "So this is basically the same same line of regression.",
            "But if we perform lever not cross validation, so we compute a solution with any minus one samples and we validate its error by the remaining single single point and this is repeated for all in samples.",
            "And if N is large, usually this is not tractable.",
            "But in the case of us, if we can compute this liver note score analytically, we don't really have to repeat the computation validation process for anytime, but only a single shot procedure.",
            "And we can compute analytically the River, not score.",
            "So this means the final computation time, including model selection, can be significantly reduced.",
            "Each solution can be computed analytically by by this way, just by solving a system of linear equations and also the model section score, liver not square can also be computed analytically.",
            "So everything is so simple but we can achieve the best performance, right?"
        ],
        [
            "OK, and we have similar theoretical properties and for parametric case and nonparametric case basically.",
            "So it's the same as the previous screen.",
            "So both in both cases the simple of this approach can achieve optimal convergence, right?",
            "So it's a very simple approach, but this is the optimal approach.",
            "And also one more addition is that so about numerical stability when we solve an optimization problem by some iterative algorithm.",
            "So sometimes numerical stability is quite important.",
            "And we can prove that actually this massive optimization problem has the smallest condition number among a class of the general class of density ratio estimator.",
            "So smallest condition number means it has the.",
            "Highest stability, so that means from a numerical viewpoint, the algorithm, the algorithm becomes quite stable, so this is useful if you have a large number of samples and we have a huge matrix and you need to invert it.",
            "So the smallest condition number is very stable and fast optimization.",
            "So anyway, so this simple ulses approach has very desirable theoretical properties."
        ],
        [
            "OK, one more simple numerical example here.",
            "So OK, now let's P numerator is the Gaussian centered at origin and covariance matrix is identity.",
            "NP denominator is again Gaussian, but center is 100 and covariance matrix is identity.",
            "And we increase the dimension of the space.",
            "And horizontal axis is the dimension of the input space and the vertical axis is the mean squared error of the estimated density ratio values function.",
            "And if you just perform confidence estimation for two for these two densities, I'm taking their ratio, then actually the error grows like this.",
            "So this is a log scale, so it's basically the cost of dimensionality in this case.",
            "But at least for this toy data.",
            "So this algorithm for this algorithm, the error, doesn't really increase.",
            "Of course it's not a general result, but at least for this simple case, directly estimating the ratio is really helpful.",
            "We don't really suffer from the cost of dimensionality for this toy data set.",
            "And in many cases, directory is making the ratio is much better than taking the ratio of kernel density estimator.",
            "Yep.",
            "Yeah.",
            "So OK, so we didn't really increase the dimension more than 20 'cause we only have a small number of samples on.",
            "Otherwise it has some numerical problems, but still OK, maybe this decrease here is a little bit artifact, but still I'm just saying that the error doesn't really grow exponentially in this case, so this is like minimum message here.",
            "Any other questions?",
            "Kate."
        ],
        [
            "So OK, this is somebody summary of this algorithm, so just simple least squares formulation is quite computationally efficient for density estimation, and if we have a cross train, then that regularization path tracking is available.",
            "And if we forget the negativity constraint, then analytic solution is available and also leave are not square can be expressed analytically.",
            "So this is a computer very efficient and also it's very easy to implement this sometimes.",
            "This easy implementation is quite important, 'cause like I am, I am living in Tokyo and if you're in Tokyo it's quite nice that we have many companies around the University so we can have many partners industry, but usually they don't really want to implement difficult algorithms, even support vector machine is quite difficult for them.",
            "Of course they can download the software, but they can't use downloaded software in their product.",
            "But in the end they need to implement it by themselves.",
            "It's quite time consuming and sometimes impossible for engineers there.",
            "But maybe they know least squares and they have some software for solving a system of linear equations.",
            "So then actually solving many machine learning tasks just by these squares is really important for this industry people.",
            "And in the case of a clip algorithm, I say that it is suitable for estimating the kullback Leibler divergent.",
            "But for this Alice algorithm, actually it is suitable for approximating a so-called Pearson diversion.",
            "Appear some divergences defined like this?",
            "Maybe you are not really familiar with this kind of function, But this is so clever.",
            "Clever divergences is a member of a class code.",
            "F diversions.",
            "And this person divergences is also a member of diversions and divergences included in this.",
            "If divergent class share kind of similar theoretical properties.",
            "So this can be a kind of squared loss bazhenov callback right about this.",
            "Maybe you can regard it as so.",
            "And.",
            "By using the active algorithm we can approximate this person divergent analytically.",
            "So that's quite nice, and later I will show that we use this Pearson diversions approximator to solve some machine learning algorithm.",
            "And there we have some additional parameters to be learned, like in the case of a set dimension reduction.",
            "So we want to find like subspace the projector samples.",
            "So then basically we want to such a subspace.",
            "And we may want to minimize or maximize something.",
            "Pearson diversions as a function of this subspace.",
            "And if we have a analytic form approximator of this divergent, then we can compute its gradient.",
            "So then we can basically solve the subspace easy.",
            "But if we use a creep algorithm, we can approximate the clever clever diversions very well, but the solution is defined as a solution over from newmaker optimization problem.",
            "So then we can't really compute the derivative.",
            "So then we can't really use it for like dimension reduction or some other purposes.",
            "So having another having an analytic solution is really essential in solving many machine learning task and by using the Pearson diversions we can really achieve that."
        ],
        [
            "OK so so far I introduced to basically basically two methods of machine or two methods of density ratio estimators and I'm saying that the Elsif algorithm.",
            "It is a kind of kind of regression type algorithm is the simplest and most useful algorithm for density ratio estimation.",
            "Any questions so far?",
            "OK, so then from now let's move on to the next part on.",
            "I want to show possible usages of density ratio function.",
            "And already I thought in the beginning we have roughly four types of applications and let's go 1 by 1.",
            "The first one is important sampling.",
            "OK."
        ],
        [
            "I started this work from this important sampling part and it's called learning under Covariate Shift.",
            "And the covariate shift is a supervised learning situation where training and test input distributions are different.",
            "But still the target function to belong remains unchanged.",
            "Decisions like that.",
            "Let's consider simple regression problem.",
            "And we have training input density active.",
            "And test input density here platform.",
            "So they are different.",
            "And then so it's a regression problem.",
            "We have a common function to estimate the red one.",
            "But the training points are coming from this blue Gaussian.",
            "That means we have training points in the left hand side of the graph.",
            "But for this testing phase we have test points not from the green blue distribution but black distribution.",
            "So we have test point in the right hand side so we don't really have test samples in reality.",
            "But I just showed up for the sake of visualization.",
            "So basically we want to predict these black crosses from blue samples.",
            "It's a kind of extrapolation problem, but I call it weak separation 'cause we need to have overlap between two densities.",
            "If training P training and P testing are completely disjoint, then theoretically it is impossible to make prediction for the test distribution.",
            "So we need to have some overlap, so overlap means we don't have many samples here, but in principle in principle we have some samples here.",
            "So that's the situation.",
            "So covariate is, by the way, the name of input in statistics.",
            "So covariate shift means the input distribution changes.",
            "But the target function P of Y given XY distribution doesn't change."
        ],
        [
            "OK, let's do this one dimensional example to explain what is happening.",
            "Actually, in the covariate situation.",
            "And let's go to the simplest straight line fitting by these squares so we have A1 plus A2 X so simple straight line model and we learn A1 and Alpha to buy standard B ^2.",
            "So then if there's no covariate shift, then.",
            "This squares was shown to be consistent.",
            "Consistent means if you have infinitely many samples, you can recover the optimal function.",
            "For N goes to Infinity, you have the best function.",
            "This is consistency.",
            "But under coverage has already, you can see see here.",
            "Ordinary squares is not consistent anymore.",
            "So we want to make prediction of these black crosses, so we shouldn't have function like this, But we should have a function like this positive slope.",
            "And we have already many samples here and this function is almost converged already, so this already illustrates the inconsistency of these squares under covariate shift."
        ],
        [
            "And the proof of consistency is actually quite simple.",
            "Usually we just use the law of large numbers.",
            "Loss function, so in the case of least squares, we use a squared loss, so those function averaged over in samples.",
            "Converge the expectation over the last function of RP train.",
            "So we are training in points are coming from P train, so this is a simple law of large numbers.",
            "So if training, distribution, training, input, distribution and test input distribution are the same, then so this simple law guarantees that consistency.",
            "But under covariate shift actually training points are coming from P train.",
            "But what we want to minimize in them is loss function expected over P test.",
            "So this still converts to this one.",
            "But we want to minimize this one.",
            "So by that we have inconsistency."
        ],
        [
            "But this inconsistency can be easily avoided by using the important weighting technique.",
            "But the important is defined as the ratio of test and training important things like this.",
            "And then instead of just computing the."
        ],
        [
            "Sample rate"
        ],
        [
            "We compute the importance weighted sample average like this.",
            "The last function is weighted by this important weight.",
            "Everest overall samples.",
            "Then again, let's just apply the simple law of large numbers to this function this entire function.",
            "So then this entire function converts to the expectation of PPP train.",
            "And then soapy train here on P train here cancelled out each other and in the end we have loss expected over pee test.",
            "So by that we can at least asymptotically minimize the loss function for the test point.",
            "Is it clear?",
            "OK.",
            "So then OK, let's use this idea in this square."
        ],
        [
            "So just we have important slated this squares we have here important weight and of course we can use the density ratio estimation to obtain this important way.",
            "And this important weighted risk is consistent even under covariate is already we have seen in the previous slide.",
            "And I just use the least squares as an example, but this simple important waiting idea can be applicable.",
            "Can be applied to any likelihood based method.",
            "If you have a log likelihood or any loss function then just you need to wait it by importance lately.",
            "Like support vector machine, logistic regression, conditional random field, maybe almost all machine learning techniques can be used in recovery rates scenario if we have density ratio function here important, wait here.",
            "OK, so this is the method on."
        ],
        [
            "One more thing we need to consider a little bit is actually the bias variance tradeoff.",
            "So I just said that if we have important weight then OK, we can recover consistency.",
            "So if you have infinitely many points, we can have the best function that the theoretical guarantee I gave so far.",
            "But of course, in reality we don't have infinitely many samples, only a finite number of samples, maybe small number of samples.",
            "So then so bias variance tradeoff is an important issue.",
            "If we don't have any weight, then after we can so that it has lower variance but high by us.",
            "And on the other hand, if we use importance weight then we can show that it has low bias but high variance.",
            "But in practice, we want to minimize the kind of sum of bias and variance because the squared error can be decomposed decomposed into squared bias plus variance.",
            "So we want to minimize the sum.",
            "So one of the practical approaches to flatten this important weight.",
            "Like we have a power parameter Lambda here.",
            "And if Lambda is 0, then wait, it's zero.",
            "Wait, wait is 1 so it's a flat weight.",
            "So this corresponds to ordinary ordinary least squares.",
            "If Lambda is 1, it's important.",
            "Waited this square so it is consistent, but it has a high variance.",
            "So in this simple artificial example, if Lambda is set to 0.5, then actually we can have a very nice function that optimally controlled the tradeoff between bias and variance.",
            "But of course 0.5 is just an example in this case, and this Lambda should be chosen carefully depending on the data in the problem.",
            "Depending on the noise level, something like that.",
            "So it's basically a model selection problem."
        ],
        [
            "And in this model selection problem again importance weight plays an important role, 'cause if he used standard model selection techniques like Akaike information criterion across validation, then it's biased.",
            "Both training and test points are points follow different distribution.",
            "So if you just naively do cross validation, then OK about cross validation score expire.",
            "And already kind of importance weighted version of model selection criteria well investigated in these papers, and the simplest one is actually to extend the cross validation.",
            "So in cross validation, so we divide our data into K groups and use K -- 1 groups for training and use the remaining group for validation.",
            "And in user cross validation we just use squared loss for validation.",
            "But under covariate shift we again use important late here.",
            "So then, but by this important weighting the this validation data looks like a test point.",
            "The validation data is coming from P train, but by applying this important weight it looks as a test point.",
            "By that we can kind of adjust the cross validation score.",
            "200 covariate assistance.",
            "OK, we're together, we can."
        ],
        [
            "Perform the adaptation undercover, it's scenario.",
            "And here are some some examples.",
            "The first example is a speaker identification task.",
            "So I I'm from Japan, so I used Japanese data set.",
            "It's actually quite famous data set in Japan.",
            "And it's called text independent speaker identification.",
            "So Speaker speaks something, but text is not fixed.",
            "But still we want to identify who is speaking.",
            "And we used kernel secret mission for classification and we use a sequence corner.",
            "And the setup here is that the training data and test data are coming from different time periods like this data is now, but we try to use training data gathered in the past.",
            "But if the training data is gathered nine months before.",
            "So then so usually actually the distribution of voice is actually quite different.",
            "After three months it is said.",
            "So then so we need some kind of adjustment.",
            "And I'm OK.",
            "This part is just ordinary carnal carnal relations plus cross validation, so no other adaptation.",
            "Then the recognition rate is not so high.",
            "But if we use important sweded version of kernel circulation and also important rated bars on the cross validation and importers, weights are estimated by clip in this case.",
            "So then the performances at the uniformly improved in this case.",
            "It's a very simple adaptation procedure and I didn't use any knowledge in like signal processing just by looking at the statistical properties.",
            "But still the performance can be improved not too much, but reasonably well.",
            "And of course, on top.",
            "On top of that, we can incorporate more prior knowledge in the target domain, signal processing, speech recognition."
        ],
        [
            "And one more example is against the Japanese text analysis.",
            "So actually English English is nice, or German is nice because you have blank between 2 walls.",
            "But so this is actually a Japanese sentence, so it means something like such a failure is cute or something like that.",
            "Charming or something, but this is a sentence and there's no blank between walls.",
            "So actually this preprocessing is really important task in Japanese language analysis it's called text segmentation.",
            "So given the sentence we want to chop it into words.",
            "So that's quite important preprocessing step so the solution is actually like this.",
            "I'm.",
            "Now the task is actually adaptation from daily conversation to medical domain.",
            "So we have actually a lot of corpus already segmented data for daily conversation, so maybe some companies spend some money to prepare labeled data.",
            "But now we want to apply this training data to more specific domains such as medical domain.",
            "And we used a bit complex algorithm called conditional random field that can handle this kind of sequence data in a very nice way, formalized way.",
            "And again, we compare importance weighted methods and no important weight method and the accuracy is measured by F measure.",
            "For larger it's simply better.",
            "So we got 94 point but this is 92 points so we had a little improvement here.",
            "And the third part.",
            "So this is interesting.",
            "'cause we spend some additional cost to have labels from the test domain medical domain.",
            "Untrain CLF on cross validation again.",
            "So then we achieve 94.43.",
            "So these two are quite quite equivalent, comparable.",
            "So this means that so we don't really have to spend like additional cost.",
            "In this case, it seems supervised.",
            "We need unlabeled points from the test domain, but we don't need level points.",
            "Material we can achieve a similar performance.",
            "To the case where we had additional test labels for this domain.",
            "So this is quite quite nice."
        ],
        [
            "There are many more interesting applications like age prediction from face images by looking at the face we want to predict like gender and age over person.",
            "This is actually quite useful for like targeting advertisement like this is joint work with actually and we see they have a system like that should be installed in front of the station or in front of the supermarket and they want to know who is coming, but they try not to really identify a problem but just gender and age.",
            "And then, depending on the gender and age, they changed the advertisement like 20s man OK, their computer game or we thought these women OK cosmetic.",
            "And in this scenario, the training data is gathered in my studio where the lighting condition is perfectly adjusted.",
            "But if you like install the system, let's say in front of the supermarket, then like in the morning you have sunlight from this direction I'm going today.",
            "You have sunlight from this.",
            "An individual here and at night may be quite dark, so lighting condition changes a lot and we need some kind of adaptation.",
            "And I must say that maybe it's not covariate shift.",
            "Every everything has been changed, but still we assume that it's close to covariate shift and then the performance is improved nicely.",
            "But at the same time, this is the weakness of this coverage adaptation approach, 'cause we don't really know whether we are saying that.",
            "Distributions of exchanges input changes OK, this is fine, so we can easily confirm that OK distributions are different, but we need to assume that the conditional distribution of output P of Y given X doesn't change.",
            "The function doesn't change.",
            "So this is something we need to assume in this framework.",
            "And if you don't have any samples from test distribution, then there's no way to really validate this assumption.",
            "So perhaps you need some prior knowledge, or you need a small number of label test samples to see with the coverage shift assumption is, well, reasonable or not.",
            "And also we did some experiments with brain computer interface and because of the mental condition change again the distribution of samples really changes because you are all experts on PCI.",
            "So maybe I shouldn't really speak a lot about PCI today, so.",
            "OK, so again VCI problem.",
            "Maybe it's not really covariate shift but still sometimes this kind of coverage just out of this and technique can really help improving the performance.",
            "And those are finally in robot control, so we may want to robot learns how to move and getting data and update his moving policy and up again get the data and update his moving policy.",
            "So then actually data gathering distribution changes overtime.",
            "But still we may want to reuse previously collected data, so gathering data is actually costly in many robotics program.",
            "So in this case we again want to have a important weighting technique to adjust the distribution difference between the previous policy, previous control policy and the current control policy."
        ],
        [
            "OK, so this was the important.",
            "Something part and any questions.",
            "Yeah.",
            "OK, so then let's completely change the story now and go through the 2nd application distribution comparison."
        ],
        [
            "Let's consider an outlier detection problem.",
            "So the original outlet next on problem is given a set of samples you want to find irregular points in your data set.",
            "But I want to consider the ratio.",
            "So we need two sets of samples, so let me slightly change the problem.",
            "So now we have two sets of samples and one set consists of only inliers regular points.",
            "And the other set is attested from which we want to identify outliers if exists.",
            "So we have one more data set that consists only of inliers.",
            "So this is like slight change of the problem.",
            "Like in the simplest case, OK, we have P regular distribution like this.",
            "It's just the caption.",
            "And P test is blue Gaussian but we have outlier here.",
            "And in the third part, outlier Dixon.",
            "We try to find this small bump by dense destination and say that OK, here's an outlier at X = 5.",
            "But as you can see, estimating this smaller boundary bump bump is extremely difficult.",
            "But once we have P regular distribution density then the taking their ratio we have a function like this.",
            "For inlier point entire region, so we have almost the same density both calcium.",
            "So then the ratio value is almost one.",
            "But for this outlier part we have small increase of the value here.",
            "But the original value here is very small.",
            "So then once you take the ratio, the ratio is far from 1.",
            "So finding this small bump is extremely difficult, but finding this big bottom is quite easy.",
            "So this means by concerning density ratio function in this outlier detection scenario, we can somehow convert us extremely difficult problem into a very simple one.",
            "And of course, our assumption is that we have P regular data points, but maybe this is not a big problem in many cases.",
            "Like next page I show some application but in let's say in some quality control in factory we already know that yesterday or products are fine.",
            "So then we just use that data as P regular and try to find outliers from today's web points.",
            "So of course it's an assumption, but maybe it's not so practical, so restrictive in practice.",
            "And also another important point, I want to emphasize here is that so outlier detection is an unsupervised learning problem and already we have many nice outlier detection algorithms like one class support vector machine.",
            "It works quite well given that hyperparameters are chosen optimally.",
            "But in this outlier detection problem, basically there's no way to really feel like Gaussian with regulation parameter 'cause we don't know which points are outliers.",
            "The in completely unsupervised setup.",
            "So then it's more like an art.",
            "So if you do outlet Dixon then this point on this point, well regarded as I'd like.",
            "But if you do outlet Jackson maybe other points are regarded as well.",
            "But once we use this density ratio approach, so as I said before, we can do cross validation.",
            "Of course, if this one is done in terms of the density ratio approximation error, so it's a bit different from outlier detection error, but still it's objective, so everybody can do the same procedure and obtain the same solution.",
            "So this kind of object objectivity is actually quite important in real world application.",
            "Otherwise none of the outlier detection techniques is really useful in practice.",
            "And here's a toy."
        ],
        [
            "Example, so machine learning researchers like USPS data set and maybe we have seen this kind of data set for hundreds of times, but actually it is known that USPS.",
            "Test data set contains some outliers.",
            "Some points that are very difficult to classify, so it's almost impossible to achieve 100% accuracy because of outliers in the test set.",
            "So we tried to identify outliers in the test set based on the training data set.",
            "And assume that training data set contains on the entire.",
            "And then so these 10 digits are top ten outliers in the USPS test data set.",
            "And this is actually labeled as five.",
            "And this is actually David at 0, but it looks sick and like that, but most of them are even difficult to read by human, so they are maybe outliers in the test set.",
            "So this roughly illustrates the validity of this density ratio approach."
        ],
        [
            "Another application is failure prediction in hard disk drive.",
            "So recently we don't really use how describe but SSD, but previous hard disk drives included a system called self monitoring and Reporting Technology smart, so it records the behavior of the hard disk drive and if it crashes then that record us will be shown and and the companies can really analyze.",
            "But what happened behind?",
            "And we applied this hard disk drive data.",
            "Use this art described data and compared against the ratio method and one class SVM and also another method called local outlier factor.",
            "So this was proposed in a data database area and this is actually quite popular method in that area.",
            "So they use.",
            "Nearest neighbors course to identify local outlier point.",
            "And OK, so we computed area under the curve or seek off.",
            "So larger is better.",
            "One is the best.",
            "And then OK overall.",
            "So this method is not bad, but this local outlier factor is the best performing method in this case.",
            "But the problem is that in this method we need to tune the number of nearest neighbors.",
            "And if it is set to 30, it performs quite well, but if it's set to five, it doesn't perform well.",
            "And in this benchmark I know the answer so we can tune the solution.",
            "But in practice there's no way to choose the number of nearest neighbors, so this is a nice method potentially, but in practice it's not so useful.",
            "But in our case we use cross validation so everybody can obtain the same solution by procedure by the cross validation procedure and also it's computationally quite efficient 'cause nearest neighbor thoughts in a big database is very time consuming.",
            "Of course there are several recognized fast approximation algorithm, but still the nearest neighbor search is quite time consuming."
        ],
        [
            "And more applications in like steel plant analysis or like printer roller analysis and things like that.",
            "Again, it's nice to be in Tokyo, but we have many companies and they have many outliers in their data set.",
            "And in the outlier detection case, we care, so we're comparing two distributions, but we care kind of pointwise difference difference at the outlier point.",
            "But now let's consider the difference of entire distribution.",
            "So this is basically an estimation of a divergent between two probability distribution.",
            "So now the task is, given two sets of data points, we want to estimate, let's say the clever clever diversions.",
            "But this is a very simple setup and if we use the creep algorithm, we may estimate equivocal but Oh well."
        ],
        [
            "But if he uses the House, if algorithm these squares algorithm, then we may want to estimate this fearsome divergent.",
            "But anyway, in both cases we just have the simple density ratio, P /, P prime.",
            "So we can just apply the denser estimator and approximate the divergences.",
            "And we can actually theoretically prove that this density ratio based diversions of approximator is again optimal.",
            "So none of the method can go beyond this approach.",
            "It's a very simple approach, but theoretically it achieves the best performance."
        ],
        [
            "So this divergent estimation can also be used for many applications, like if we want to find region of interest in images then so we can start a 2 reasons to surrounding regions and computer like inner region after region distribution of samples in an outdoor reason.",
            "And by comparing this exhaustively in the image, then we can find the object in the net.",
            "So this kind of thing is quite important.",
            "If you would give up.",
            "Let's say this is our camera.",
            "If you want to find an object and have a forecast automatically to the target object.",
            "Or if you have movies we want to find events.",
            "Of course, the definition of event is on Vegas here, but we just take windows of two consecutive time Times Sigma Times segment and take samples like in a sliding window way like this.",
            "And compare distribution of these points on these points.",
            "But if nothing happens or the movie is kind of stationary, then distribution of these two intervals are quite the same.",
            "The two distributions are the same, but once we have some event, like in this case, it's a tennis movie and actually a ballboy entered in the code and it can be like predicted because the distribution of samples really changes if a boy comes.",
            "But here we just estimate the divergent between two distributions.",
            "So the method is really the same.",
            "A similar technique can also be applied to Twitter data.",
            "Like we have.",
            "So, so we have frequency of rod overtime.",
            "Like if he just gather Twitter data and see how many times this word is pronounced here on there and by looking at the change of the frequency of words we can dig event like in this case it is actually a BP oil spill and we took we preachers chosen several keywords in advance and then just monitor the frequency difference of these words.",
            "And from that we computed.",
            "This is actually basically diversions between two distributions.",
            "I again taken this sliding window way, so then at this first stop at the oil which is the mainland of the United States in the second peak, Obama visited Louisiana or something like that.",
            "So maybe so this is done afterwards, so maybe we just find try to find an event that happened here and here.",
            "So maybe it's not a fair evaluation, but it is a kind of fun application.",
            "You may do some kind of exploratory data analysis.",
            "OK, that was a."
        ],
        [
            "Distribution comparison, so we estimated the divergent between two distributions.",
            "Any questions about that?",
            "OK, so try to think about any possible applications in PC or neuroscience.",
            "So that's the purpose of this vector.",
            "I don't have any like neuroscience application here.",
            "OK, the third one is mutual information estimation, so this is technique."
        ],
        [
            "Quite simple, so mutual information is an important quantity in information theory, and it's defined by this form.",
            "And pxy and log PXY over PXPY.",
            "And basically this is the clever clever diversions from P. Of XY joint distribution to PXPY the product of marginal.",
            "And if this mutual information is 0, so this is always nonnegative and this is zero if and only if X&Y are statistically independent.",
            "Or PX y = P XPY.",
            "So by looking at the value of new termination, we can know the dependency of two random variables.",
            "And this is actually quite important to know the dependencies between 2 random variables, and we can again use the clip algorithm to estimate distance D ratio.",
            "So we just have to slightly modify the code 200 this Jason, but it just straight for one or two lines change.",
            "So let's just wait how this is useful.",
            "So already I mentioned that the Kleiber estimation based on density ratio is optimal.",
            "So then this mutual emission estimator is also optimal.",
            "Actually, the study of estimating nutrition has long history, information theory.",
            "People developed many different nonparametric methods and they tried to show consistency of the algorithms.",
            "But so far none of the methods can really achieve the best convergence rate.",
            "But distance ratio approach achieved the best convergence rate, so it's a simple approach, but it's the best approach."
        ],
        [
            "So when we compare the density ratio method with confidence estimators and the nearest neighbour estimator, another Edgeworth expansion.",
            "So this is I will mention that later.",
            "That we used for."
        ],
        [
            "Dataset so one so X is one dimension and one is also one dimension, and in this case every UU.",
            "So it's independent and this is linear dependency between X&Y.",
            "This has quadratic dependency, so in this case X&Y are dependent, but correlation is 0, so it's not easy to detect by just looking at the correlation.",
            "And also electric car dependency like it is again creation is 0, so it's not easy to detect the dependency.",
            "So that means we need to see higher order."
        ],
        [
            "Distichs and here's the result.",
            "The horizontal axis is the number of samples used for estimating middle nation and vertical axis is the approximation error of mutual information.",
            "MI Hot is an estimated meter.",
            "Imagine an MRI is the true one 'cause we can compute the true one for this toilet or set.",
            "And we compared several algorithms and red one is the density ratio based method and overall we can say that the density ratio method method is always one of the best solution for all the datasets.",
            "And again, Ken.",
            "Yes, never based method works, sometimes quite well, but it has tuning parameter the number of nearest neighbors, and actually there's no way to again just the number of nearest neighbors.",
            "So once it is chosen optimally then it works very well for all cases.",
            "But once it is chosen in the wrong way, we have quite cross solution K nearest neighbor with K = 15 works very well in this case, but it doesn't work really well for this case.",
            "So choice of K is quite important.",
            "But in our case, of course we also have like Gaussian bit in the similar way, but we can choose that by cross validation.",
            "So no tuning parameter remains."
        ],
        [
            "OK, that was mutual information, and theoretically that's quite nice.",
            "It's a nice paper information theory society, but from the machine learning viewpoint.",
            "Still mutilation is not so easy to use because as I said before, so it's solution is computed by some of migration problem and we don't really have like explicit expression of the solution.",
            "So here, so we change the definition of metabolism and we use another information measure called squared loss mitigation, a semi.",
            "It's just the Pearson diversion spots on the liver.",
            "There was a version of the mutilation, so it's got some diversions from P. Of XY2P XPY.",
            "So as I said before, Jason diversions also belongs to the F diversions class, and this is not always non negative and zero if and only if X&Y are independent.",
            "So basically the same as materialism, but easier to estimate by using the office algorithm so we can have an analytic form approximator.",
            "And this is actually very useful."
        ],
        [
            "In solving various machine learning tasks, so I just listed several possible applications.",
            "And you know, so X is input and Y is output.",
            "And if we consider mutual information between or SMI between input X and output Y, then basically we can do feature selection.",
            "Like we have, X is a set of genes.",
            "Why is whether the patient is cancer or not?",
            "We want to know which Gene is responsible for predicting the sickness of the patient.",
            "For example, the feature ranking can be done just by looking at the SMI values.",
            "Or dimension reduction is a kind of feature extraction so we don't choose the subset of features.",
            "But we can start combination of features, lower dimensional expression of features.",
            "So I will explain the details in the next slide.",
            "And also clustering can be done in the same way.",
            "So in the case of clustering, it's unsupervised learning, so we don't have Y, but only X.",
            "But we can predict why based on mutual Amazon, it's more like a data compression.",
            "So given X we want to have one dimensional expression Y that contains the most information of X.",
            "By doing that, we can like the clustering and also it's important to say that.",
            "Again, we can do cross validation.",
            "Many nonlinear clustering algorithms, like spectral clustering is very nice, but it contains like tuning parameters, Gaussian width for example.",
            "Then by changing the Gaussian with, the solution changes really allowed.",
            "But based on this density ratio approach we can again tune the calcium with based on cross validation even in the clustering scenario.",
            "So it's quite quite useful in practice.",
            "And also if we can start metering SMI between two inputs or multiple inputs, then we can do ICA.",
            "So given like vector of X.",
            "So in this case it doesn't have to be 2 elements, but it could be more than two, but we can define SMI in the same way.",
            "And we just optimize the separation matrix in a way that SMI is minimized to be independent.",
            "Again, close by this and can be used in ICA, so it's quite useful.",
            "And also like object method, these are two to specifics.",
            "I will skip them but we have other interesting applications.",
            "And finally, so between SMI between input and residual we predict Y from X and consider its residual epsilon.",
            "So by some modeling assumption, actually by looking at the independence between X and epsilon we can do cause our direction in frame.",
            "Because our direction means whether X effects YX causes Y or Y causes XX&Y are created to each other.",
            "But we want to know its direction.",
            "This is actually quite fundamental question in science and it's very difficult problem, but under some modeling assumption we can prove that it is possible and it can be achieved by looking at the independence between X and it so.",
            "OK, so there are many applications of SMI in machine learning."
        ],
        [
            "And here's a small detail of sufficient dimension reduction.",
            "So it's a supervised dimension reduction method, so we have input X and output by output.",
            "Why can be anything?",
            "It can be either real value in the case of regression class label in the classification problem, or a vector of class labels in the case of multi label classification.",
            "And we want to project Project X onto Z, so this is a lower dimensional expression of X and we can start linear prediction W here.",
            "So W is assumed to be also going out like this.",
            "So then the goal of this sentencing reduction is to find a projection matrix W so that this projected vector Z contains all information about Y.",
            "So this means given ZY&X are conditionally independent, this is what we want to achieve for optimal W. And in terms of SMI, this can be simply expressed as like maximizing SMI over WX&Y.",
            "And this maximizer achieves this one.",
            "And already we have a analytic estimator of SMI.",
            "So then we can just maximize this one with respect to the projection matrix W."
        ],
        [
            "I leave it more detail if SMI approximately.",
            "It's simply shown in this way analytic form.",
            "And we may use a natural gradient method or just gradient descent gradient descent."
        ],
        [
            "As in gradient descent or natural gradient, awesome to find the optimal W. Because W belongs to so called a Grassmann manifold.",
            "It's all except of also gonna matrices if we can exploit this structure, we have a better gradient algorithm.",
            "And also we have some better heuristic to update W matrix and it can scale quite well."
        ],
        [
            "And here's a simple experiment for multi label data multi label classification problem like it's a like in the Left Hand case.",
            "It's a image classification problem.",
            "So given that image we want to know whether a person exists or not.",
            "So this is the first label or a dog exists or not.",
            "So this second level account exist or not Sky you could just turn up.",
            "We had around I think 200.",
            "No.",
            "So in this case around like 20 or 30 labels, and for each image we have like 20 dimensional label vector that contains 0 and Y.",
            "Either this object exists or not.",
            "This is a multi label classification problem.",
            "And for this multi label classification problem we we did dimension reduction and the original dimension was quite quite high.",
            "But now it's reduced to 20 dimensions or like 60 dimension 140 dimensions like that.",
            "The classification accuracy misclassification rate.",
            "Of no dimension reduction is somewhere here.",
            "But the dimension reduction based on density ratio goes lower than this one, so it improves the performance by reducing the time zone.",
            "So we can simply get rid of the noise components.",
            "But with other methods.",
            "Unfortunately it doesn't go beyond the nordmende reduction baseline.",
            "So performance is not bad, but method works much better than this method.",
            "But we are really investigating the conditional independence, so this is what we really wanted.",
            "The independent reduction, but other methods are kind of heuristic to find reasonable projection matrix, so maybe that's the difference why we had better performance here.",
            "And the similar thing goes for like audio tagging problem.",
            "So it's again attacking problems given the audio sound, we want to know whether it's a broad exist or some engine sound exists or something like that.",
            "So we have in this case 200 diamonds.",
            "Now label vector can be obtained quite similar results likely."
        ],
        [
            "Yeah, so that was a mutual information, but it was a bit longer, but estimation of mutual information is really.",
            "Yes.",
            "So.",
            "Yeah, OK."
        ],
        [
            "That's a very good point.",
            "So in principle, of course we can use mutual information, but in this case we want to like.",
            "This also holds for MI.",
            "But the problem is in my approximator is not analytic.",
            "So then we."
        ],
        [
            "Can't really compute its derivative with respect to W. So then there's no way to optimize W in a competition recently, if you play with the equation, you may somehow update W, But it's not straightforward if it's lower.",
            "So from the computational viewpoint, using SMI is not much more convenient in this case.",
            "Any other questions for SMI is quite useful in solving many machine learning tasks and it's great.",
            "There will be some cost in the afternoon.",
            "Applications were made actually useful information that you would like to defective.",
            "OK, that's a very good point.",
            "So OK SMI and am I the same independent so like here dependences actually no direction or directions are dependent and in the case of SMI, maybe we prefer to have this dependency and in the case of them why we have this dependency and like maximizing MI and maximizing SMI may have different solutions.",
            "You are right and it's not so far we didn't have any characterization for this part.",
            "So we can't really say anything, but at least we can say that we are maximizing some kind of dependency, so error metric is slightly different in a log case and squared loss case, but roughly, so we are anyway maximizing dependency, and mathematically it's no problem.",
            "But in practice we may have different solutions and we can't really say whether one is better than the other.",
            "It depends on the situation perhaps.",
            "It's something like a choice of loss function.",
            "We can't simply say one is always better than the other.",
            "But on the other hand, another practical advantage of using SMI is that it's more robust against outliers.",
            "Because the clever clever diversions contains log function, and it's really bad if you have a single outlier.",
            "But in the case of SMI we are using the squared loss and you may think squared losses.",
            "Sensitive to outliers in the case of regulation, yes.",
            "But in the case of like density ratio estimation or density estimation at this squared loss, it's quite robust against outliers.",
            "Like not spit out on the log function, so that's another practical advantage of using a semi.",
            "But this is like the pros and cons, so it is robust against outliers.",
            "But on the other hand this means for outlier detection, maybe using critical libraries data 'cause SMI is robust against outliers.",
            "So then the outlier score is not sensitive to outlier.",
            "So in the case of authorized techs and perhaps we should use clever clever diversions.",
            "But in other cases maybe SMI Pearson diversions would be more useful.",
            "OK, any other questions?",
            "OK then let's go to the 4th part conditional."
        ],
        [
            "Probability estimation this is quite simple unsolved."
        ],
        [
            "Or a conditional probability P of Y given X is by definition given by P of XY over PX.",
            "That's it, it's density ratio.",
            "So we can directly estimate it without really changing the program.",
            "And so for now, let's consider the situation where X&Y are both continuous.",
            "So then it's called conditional density estimation.",
            "Like we have X here and why here and we have blue points like this.",
            "And conditional density is something like this red line.",
            "Given X, we want to know the density of life, so this is a kind of data visualization problem.",
            "And regulation is rated problem, but it estimates conditional mean the given X.",
            "We want to know if meaning of life.",
            "Then so this green line is a regulation covering this problem.",
            "And for a data set like this, like having two modes in this case, then actually the regression graph goes between two mountains from the viewpoint of prediction.",
            "Maybe this is fine, but from the viewpoint of data visualization interpretation, maybe regression is not so useful in this case.",
            "Another system would be in addition to the multi modality.",
            "We may have a symmetric noise or heteroscedastic noise, so noise variance is dependent on the position X.",
            "So in that kind of situation, we may want to know the conditional density itself.",
            "But I must agree, I must say that conditional density estimation is a difficult problem.",
            "It's against estimation problem even harder problem.",
            "So it's not maybe possible to.",
            "We can't really have a good solution if the dimension is larger than five or something, but as long as the dimension is less than five or four, maybe directly estimating the conditional density is sometimes useful.",
            "And here we can just use our density ratio estimator and directly obtained conditional density estimator.",
            "Is it clear?",
            "So technically it's nothing new, but this is all.",
            "This condition of this destination.",
            "And."
        ],
        [
            "In the same way?",
            "Or OK on the experiment here.",
            "So it's a robot transition probability estimation, so here's a simple toy robot and it at their size of this side, and it has two values, and once two wheels are moved forward, it moves forward and backward, backward and in a different way it can rotate.",
            "And also the robot has two infrared sensors and by that it can measure the distance to the nearest obstacle.",
            "So V is the velocity of two wheels and that's an action that we can recommend to the robot.",
            "And the value of the infrared sensors is called state.",
            "And by that we can know the surrounding environment of the robot.",
            "The task of transition probability estimation is to estimate P of X plane given SA.",
            "So at the current position, so we give a command to the robot.",
            "OK, like it goes forward, then it moves and we want to know in what kind of space on the robot will exist in the next step.",
            "It's S prime.",
            "And this is actually quite multimodal, cause like if the robot is somewhere here, an infrared sensor detects this corner, but once the robot turns a little bit then we can't find this corner anymore.",
            "So then the infrared value sensor value changes a lot just by slightly rotating the robot.",
            "So it's not a simple unimodal problem, but it can be a highly complex multimodal problem.",
            "So we have like density restoration method and from KDE Extended method and also neural network method and the performance.",
            "So this is accuracy.",
            "So this is smaller is better error.",
            "This is an error.",
            "Small writes better density ratio based method and neural network method both work equally well.",
            "But the competition time is like 1000 times faster, in this case.",
            "Of course, you know neural network training is quite slow, but in the case of the integration estimation, we just have to solve system of linear equation on the ones and solution is analytic.",
            "No local Optima."
        ],
        [
            "But there was a conditional density estimation and.",
            "Just change the definition of Y.",
            "So in the previous case, why was a continuous variable like state vector?",
            "But let's assume that Y is a categorical value classification problem.",
            "So then actually estimating this P of Y given X is actually estimating the class conditional probability the same as logistic regression.",
            "So I'm saying that so these squares density racist matter provides a clear analytics analytic estimator of this class conditional probability.",
            "So this means it can be a nice alternative to kernel logistical regression.",
            "So I know Carnivalistic listen is quite useful method and it is used widely, but its training is quite time consuming.",
            "Because it has a log function and it doesn't have a nice property to play with, so we just have to use a simple cross Newton method or Newton method or something like that to obtain the solution.",
            "And recently study of Cross Newton is really advanced and we have a very nice implementation nowadays, but still it's very slow so it doesn't scale well.",
            "But if we use us, if we just have to solve a linear equation.",
            "Then we can obtain this, get the same value as logistically lesson.",
            "Of course it's not the same method, but it uses different loss function, either log loss or squared loss.",
            "But maybe the performance is comparable, but the computational complexity is much much lower.",
            "And in particular, no normalization term is included in our safe computation of the normalization time is quite difficult, quite computationally.",
            "Heavy in logistic regression, but we don't have that in our system.",
            "And also classified training is actually possible.",
            "So this is actually a big advantage, like if you have like 1000 classes like you have many samples from super Mini classes then Colonel stipulation needs to be trained with all samples.",
            "But in the case of this assist method, we just need to train a classifier for each class.",
            "So this is actually computationally much much more efficient.",
            "Undersea"
        ],
        [
            "Uncle here.",
            "So it's just a toy benchmark data set.",
            "Let our data set.",
            "It contains 26 alphabets.",
            "Horizontal axis is the number of samples and vertical axis is a misclassification error.",
            "The Red one is the density ratio based method and green ones carnivalistic relation.",
            "So in this case carnivalistic regression is slightly better.",
            "Maybe they are similar.",
            "But if you see the training time as a function of the number of samples, so it's actually log scale.",
            "So 10 to minus two here for 1:50 here.",
            "10,000 times faster.",
            "But we don't need any training procedure, but just we need to solve a system of linear equations super fast.",
            "And more experiments."
        ],
        [
            "Unlike image and sound data set and they are not so large scale.",
            "So again, in computation time is not so large, but still the density ratio method is much faster than the performance is compatible.",
            "Or in this case actually slightly better.",
            "We don't really argue that distance ratio method works better than eternal separation, but they should be at least comparable.",
            "But computationally it's much more efficient."
        ],
        [
            "OK, and we have more applications in action recognition from accelerometer.",
            "If you have iPhone so it has accelerometer and from this information we want to know whether a person is walking or like going up the stairs or taking elevators or something like that and there's actually a serious medical application behind 'cause we want to know what patients patients are really doing in their daily life.",
            "So they come to the doctor, come to see the doctor once a month and they say.",
            "I'm always using the stair and I don't use any elevator but it is recorded so we can identify that.",
            "And that's actually quite important in diabetic diagnosis, so we have many diabetes patients in Japan and this is a big problem.",
            "And also again, it's prediction from this image is so we have face images from many different tribes.",
            "Then the number of classes are now increasing and it's nice to have a computationally efficient approach.",
            "OK."
        ],
        [
            "So that was the conditional probability estimation part.",
            "Do you have any questions?",
            "Yep.",
            "Yeah, I thought OK, OK, so I originally I was not really planning to go to this part and so we have more like development, recent development on density ratio estimation and I just."
        ],
        [
            "One idea that so I'm saying that then seriously estimation is better than a separate density estimation, and maybe this is true in many cases.",
            "But still I must admit that if the dimension is high density ratio estimation, it's still difficult.",
            "So then it's not.",
            "It's natural to combine the installation with dimension reduction.",
            "And."
        ],
        [
            "The point is that we have to density.",
            "So if you play with a single density, it's not easy to the dimension reduction, but we have two densities then, so we can actually try to find a common part or different part.",
            "So in our case we try to find a different part.",
            "Given two densities, we want to find, so we assume that they are different only in some subspace that we call Hitler distribution subspace.",
            "Like in the simplest 2D example 2 dimensional example.",
            "So we have like red points from denominator and blue points from numerator and if we see this direction then written blue has the same distribution.",
            "But if you see this direction, red and blue has different distribution.",
            "And suppose different direction is U and same direction is V. So then this ratio can be without any assumption.",
            "We can decompose it in this way.",
            "And if we pop is the same, we can cancel this part and in the end we only have density ratio over you.",
            "And suppose X is 10.",
            "So don't dimensions, but maybe they only share are small.",
            "They are different only in small dimensional subspace and this kind of thing often happens in outlier detection problem.",
            "For example, the original data is quite high, high dimensional but change is not, it's confined in some lower dimensional subspace.",
            "Then basically we need to have to estimate the density ratio only in this subset.",
            "And then the question is how to find this sort of space and we have several algorithms, but I need to just keep it.",
            "We have some procedure already to find it a redistribution subspace."
        ],
        [
            "And.",
            "Well done so conclusion.",
            "So the beginning of this work was for estimating data generating probability distribution is the universal approach.",
            "So this is the most general approach that we want to solve.",
            "But on the other hand, if it's not so great without having good prior prior knowledge.",
            "On the other hand, solving each task like support vector machine for panic when is an ideal approach, but it's not.",
            "It's costly for solving many different tasks.",
            "So density ratio estimation is a kind of realistic compromise between these two approaches.",
            "And I don't say that density ratio covers all interesting tasks, but still this set contains many interesting tasks that we want to solve in practice.",
            "And it can then stress test mission can systematically avoid density estimation and and also it was shown to be useful in many real world problems.",
            "But maybe it's nice to explore more PCI directions in the near future."
        ],
        [
            "And we have already published a book on density ratios.",
            "This is just published a couple months ago.",
            "And also for the covariate shift adaptation part, we have another book that was published from MIT Press.",
            "So if you have some interest, you may take a look at these books."
        ],
        [
            "And also like we have OK many colleagues to this project and we have funding as it says here and all the papers articles on software of density restaurant mission is available from from this web page.",
            "Software is written in Matlab and some are on someone else, but mostly it's not love.",
            "So there I think ready to use on some industry.",
            "People are like this testing from over the algorithm.",
            "So if you have nice data to analyze then just try some of the density ratio estimation methods and if you let me know the result I'm quite happy.",
            "OK, that's it.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, hello, so my name is massages Yama.",
                    "label": 0
                },
                {
                    "sent": "I'm from talking shit about technology so I'm actually a machine learning person on my talk will be really like pure machine learning.",
                    "label": 1
                },
                {
                    "sent": "So today I would like to introduce a new framework of machine learning and we have already developed several algorithms that can be easily implemented and we have nice theory behind and also we have many applications, but unfortunately I didn't have a lot of applications in like neuroscience, PCI X signal processing, so actually.",
                    "label": 0
                },
                {
                    "sent": "It's my great pleasure to give a talk here today 'cause we have so many nice PCI people here, but it's very nice to have applications in then and also I'd like to thank organizers for inviting me to give a talk here.",
                    "label": 0
                },
                {
                    "sent": "OK, so then my talk is about density ratio estimation, so this is the.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You want all of the talk.",
                    "label": 0
                },
                {
                    "sent": "OK, a general goal of machine learning is to learn some information hidden behind data forgiven data.",
                    "label": 1
                },
                {
                    "sent": "We want to learn something from the data.",
                    "label": 0
                },
                {
                    "sent": "And as a machine learning researcher, we are always looking at many different machine learning tasks and I just listed a lot of learning tasks that I have in my mind.",
                    "label": 1
                },
                {
                    "sent": "You don't really have to see carefully but just know there are many machine learning tasks here.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one of the most universal and general approach to solving these machine learning tasks would be to learn data generating probability distribution.",
                    "label": 0
                },
                {
                    "sent": "'cause once we can estimate the data generating probability distribution, basically we can know everything about the data.",
                    "label": 0
                },
                {
                    "sent": "We can reproduce the data and we can whatever properties of the data.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, this is the most general approach.",
                    "label": 0
                },
                {
                    "sent": "Most University.",
                    "label": 0
                },
                {
                    "sent": "This is a universal approach and we can basically solve all machine learning tasks tasks if we can estimate the probability distribution behind the data.",
                    "label": 1
                },
                {
                    "sent": "For example, in the simplest case of pattern recognition, so we have two classes of samples and if we estimate the probability distribution of class one and class minus one, then so we have some densities like this.",
                    "label": 1
                },
                {
                    "sent": "And OK, then compare the densities and from that we can know the decision boundary between two classes.",
                    "label": 1
                },
                {
                    "sent": "So part time recognition can be solved by estimating data generating probability distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the most general approach, so in a sense the best way to do machine learning resources to study density estimation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, estimating probability density probability distribution is known to be difficult, so this card means without having any good prior knowledge, we can't really have accurate estimate.",
                    "label": 1
                },
                {
                    "sent": "Like if we can easily suffer from the curse of dimensionality, for example.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, avoiding probability distribution is really important.",
                    "label": 0
                },
                {
                    "sent": "But still we want to achieve.",
                    "label": 0
                },
                {
                    "sent": "We want to solve a certain task.",
                    "label": 1
                },
                {
                    "sent": "So support vector machine is the electrical example following this line that I call task specific approach.",
                    "label": 0
                },
                {
                    "sent": "It doesn't involve density estimation, but it directly solved the target problem.",
                    "label": 0
                },
                {
                    "sent": "So in the case of support vector machine, we don't really estimate that data generating distribution of class one and class minus one.",
                    "label": 1
                },
                {
                    "sent": "But we directly learn the decision boundary from data.",
                    "label": 0
                },
                {
                    "sent": "This was shown to work quite well in practice.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then so.",
                    "label": 0
                },
                {
                    "sent": "And in principle, this task specific approaches can be more accurate than the universal approach of density estimation, 'cause we are really directly solving the machine down target machine learning tasks.",
                    "label": 1
                },
                {
                    "sent": "So ideally we want to we had many machine learning tasks, so we want to ideally develop algorithms for each of the tasks and design nice algorithms.",
                    "label": 1
                },
                {
                    "sent": "But the discourse, research and development for each machine learning task is actually very high that we have so many different machine learning tasks, and we only have a small number of machine learning resources.",
                    "label": 0
                },
                {
                    "sent": "So it's not really possible to develop nice algorithms developed, nice machine learning theory, and to have very fast implementation for each of the tasks.",
                    "label": 0
                },
                {
                    "sent": "So still we want to have a kind of universal approach to cope with this problem.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the target of my talk today is kind of an intermediate approach.",
                    "label": 1
                },
                {
                    "sent": "So we want to develop machine learning algorithms for a group of tasks, so I don't say the entire task, but we can start a group subset of tasks.",
                    "label": 1
                },
                {
                    "sent": "So this is an image, so we have many machine learning tasks and if we do density estimation then basically we can cover everything.",
                    "label": 0
                },
                {
                    "sent": "So this is the most general approach.",
                    "label": 0
                },
                {
                    "sent": "But as I said before, this mission is a difficult problem.",
                    "label": 0
                },
                {
                    "sent": "So on the other hand, like support vector machine, if we develop an algorithm for specific tasks, then OK, it's the most powerful approach.",
                    "label": 0
                },
                {
                    "sent": "But then we need to develop many different algorithms for different tasks.",
                    "label": 0
                },
                {
                    "sent": "This is quite costly.",
                    "label": 0
                },
                {
                    "sent": "So we want to take on kind of intermediate intermediate approach that covers a certain class of tasks, but hopefully some useful interesting machine learning tasks are still included in this class.",
                    "label": 0
                },
                {
                    "sent": "So that's that's my target today.",
                    "label": 0
                },
                {
                    "sent": "And more specifically, here we use the notion of density ratio so that the keyboard.",
                    "label": 0
                },
                {
                    "sent": "So I listed many tasks here, but actually this tasks have.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Come on property that is, they have they include multiple probability distribution.",
                    "label": 1
                },
                {
                    "sent": "So let's say two distribution and say densities PMQ.",
                    "label": 0
                },
                {
                    "sent": "And of course, if we estimate P&Q ball, then we can solve the target stuff.",
                    "label": 1
                },
                {
                    "sent": "But actually we can show that for solving these tasks we don't really have to know P&Q both separately, but we just need to know their ratio.",
                    "label": 0
                },
                {
                    "sent": "So P / Q.",
                    "label": 0
                },
                {
                    "sent": "So I define it as a ratio of.",
                    "label": 0
                },
                {
                    "sent": "So we don't really have to know P&Q, but we only have to know R. So why don't we directly estimate the ratio function or without really estimating P&Q?",
                    "label": 1
                },
                {
                    "sent": "So then we can avoid doing dense decimation of P&Q.",
                    "label": 1
                },
                {
                    "sent": "But still we can solve a class of tasks.",
                    "label": 0
                },
                {
                    "sent": "Of course, it's not all, but I argue that for this density ratio framework includes a lot of interesting and practical tasks.",
                    "label": 0
                },
                {
                    "sent": "So once we develop a very nice algorithm to estimate the density ratio, we can we can give a solution to a class of tasks at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is a simple, intuitive justification of this density ratio approach.",
                    "label": 0
                },
                {
                    "sent": "So let me about Nick is a founder of front of support Vector Machine and in his book he said this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "So when solving a problem of interest, one should not solve a more general problem as an intermediate step.",
                    "label": 1
                },
                {
                    "sent": "So this is so I call it principle, so it's not a theorem, so we can't really prove this this fact, but it's kind of nice.",
                    "label": 0
                },
                {
                    "sent": "You know, guiding principle for machine learning researchers to develop practically useful algorithms.",
                    "label": 0
                },
                {
                    "sent": "So in our context, so we don't really have to know two densities, but we only have to know the ratio of them.",
                    "label": 0
                },
                {
                    "sent": "So if we know the densities, then of course we can compute the ratio by this dividing P by Q.",
                    "label": 0
                },
                {
                    "sent": "But if we know the ratio, we cannot decompose this one into P&Q uniquely.",
                    "label": 0
                },
                {
                    "sent": "So this roughly means the left problem is more general than the right problem.",
                    "label": 0
                },
                {
                    "sent": "So if we follow this public Sprint store then so we shouldn't estimate two densities, but we just have to estimate the ratio so that the best way to solve the problem.",
                    "label": 0
                },
                {
                    "sent": "So whether there was a public 75th birthday symposium last year, December tubing and I had the chance to present distance eraser work in photo in front of that near and he said, OK, he liked actually this idea, so now I'm happy to say that I can use public Springfield with his support.",
                    "label": 0
                },
                {
                    "sent": "So anyway, so now we want to estimate the density ratio, so without really going through density estimation of PM PM here so that the machine learning part of my talk.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and let me quickly conclude my talk now 'cause its been longer later.",
                    "label": 0
                },
                {
                    "sent": "So first of all, density ratios can be accurately on computationally absently, estimated by just like a simple least squares.",
                    "label": 1
                },
                {
                    "sent": "So no complex method is necessary, just the squares you know.",
                    "label": 0
                },
                {
                    "sent": "So this means, and I'm saying that once we know the density ratio, we can solve a class of tasks.",
                    "label": 0
                },
                {
                    "sent": "So this means this class of tasks can be solved just by this square.",
                    "label": 0
                },
                {
                    "sent": "We don't really need the complex knowledge.",
                    "label": 1
                },
                {
                    "sent": "And the tasks roughly includes important sampling techniques or some divergent estimation techniques or mutual mutual information estimation techniques and conditional probability estimation techniques.",
                    "label": 0
                },
                {
                    "sent": "So all these tasks can be solved just by estimating the installation.",
                    "label": 1
                },
                {
                    "sent": "Is it clear?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then from now.",
                    "label": 0
                },
                {
                    "sent": "So let's go into the technical parts.",
                    "label": 0
                },
                {
                    "sent": "So in the next part I explain 2 methods of density ratio.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically my images like we have many bars here, browser tasks and Stone is a dense racist method method and I want to kill all bars just by throwing barnstorm.",
                    "label": 0
                },
                {
                    "sent": "So that's an ideal goal.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I want to explain the ultimate storm in this section.",
                    "label": 0
                },
                {
                    "sent": "But before that I introduce a bit simpler method.",
                    "label": 0
                },
                {
                    "sent": "The first method and then I introduce second method and the second method is a kind of ultimate storm to kill all bars.",
                    "label": 0
                },
                {
                    "sent": "And then after that I explain how we can really use these denser estimators to solve various machine learning tasks.",
                    "label": 0
                },
                {
                    "sent": "And if you have some time, so I want to say a little bit more on density restoration.",
                    "label": 1
                },
                {
                    "sent": "Alright, so let's start from the methods of dense.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Richest nation?",
                    "label": 0
                },
                {
                    "sent": "The first let me formulate the problem, so as I said, the goal is to estimate the density ratio function.",
                    "label": 1
                },
                {
                    "sent": "So from now two densities are denoted by P numerator and denominator.",
                    "label": 0
                },
                {
                    "sent": "So two density function and we want to estimate this ratio function from two sets of samples.",
                    "label": 0
                },
                {
                    "sent": "The first step is coming from the numerator and the second set is coming from P denominator.",
                    "label": 0
                },
                {
                    "sent": "So we have two sets of samples and from these two sets of samples we want to learn the ratio function.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of course, the simplest way would be to just perform dense destination so we have two sets of samples, so we just estimate each density facpe numerator.",
                    "label": 0
                },
                {
                    "sent": "Hutton P Denominator had from each sample, and then compute their ratio.",
                    "label": 0
                },
                {
                    "sent": "But I am arguing that this is not the valid abroad becausw the first step of estimating densities.",
                    "label": 0
                },
                {
                    "sent": "Is performed without regards to the second step of taking the ratio.",
                    "label": 1
                },
                {
                    "sent": "Of course we can perform density estimation in optimal way, but by using some nice algorithms.",
                    "label": 0
                },
                {
                    "sent": "But still we always have estimation error in density in densities.",
                    "label": 0
                },
                {
                    "sent": "So then by taking their ratio the error small error included in the first step can be magnified in the second step.",
                    "label": 0
                },
                {
                    "sent": "So in the simplest case, if the denominator has a very small value at 0.001, so then the small error included in the P numerator is magnified by acting then to the power four or five or something like that.",
                    "label": 0
                },
                {
                    "sent": "So this doesn't work at all if the dimension is Lau.",
                    "label": 0
                },
                {
                    "sent": "So we want to have a single single sub procedure that estimates the density ratio directly.",
                    "label": 0
                },
                {
                    "sent": "So that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Target.",
                    "label": 0
                },
                {
                    "sent": "OK, so I introduced two methods now.",
                    "label": 0
                },
                {
                    "sent": "The first method is called density fitting approach.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. OK, we want to directly estimate the density ratio function, so let's.",
                    "label": 0
                },
                {
                    "sent": "Use all hop as our density ratio model, so this is a function we want to learn.",
                    "label": 0
                },
                {
                    "sent": "I will give a specific form later, but for now let's just use our heart as a model.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea of this density fitting approach called callback library important estimation procedure is to minimize the kullback Leibler divergent from P numerator to estimate given by this form.",
                    "label": 0
                },
                {
                    "sent": "So this form means the ratio is P numerator over P denominator and if we if we multiply our model with the integration model with the denominator, we can estimate the numerator.",
                    "label": 0
                },
                {
                    "sent": "So now we minimize these two divergent.",
                    "label": 0
                },
                {
                    "sent": "These too clever clever diversions.",
                    "label": 0
                },
                {
                    "sent": "So we minimize this one with respect to our heart.",
                    "label": 0
                },
                {
                    "sent": "So this is the formulation of the algorithm called creep.",
                    "label": 0
                },
                {
                    "sent": "And it's.",
                    "label": 0
                },
                {
                    "sent": "Easy to see that we can decompose this log part into the constant part and a term that includes our model our hub.",
                    "label": 0
                },
                {
                    "sent": "So then constant can be of course ignored, so minimizing kullback Leibler divergent.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is now equivalent to maximizing the second term, so this down.",
                    "label": 0
                },
                {
                    "sent": "So this is our objective function to maximize.",
                    "label": 1
                },
                {
                    "sent": "But we need a little more constraint becausw this P numerator had should be a probability density function.",
                    "label": 1
                },
                {
                    "sent": "So this means the integral of this function is 1 and also the model should be non negative.",
                    "label": 0
                },
                {
                    "sent": "So by that we can guarantee that this P numerator is a density function.",
                    "label": 0
                },
                {
                    "sent": "And here we introduce a specific form of a model we can use.",
                    "label": 0
                },
                {
                    "sent": "We can actually use many different types of models, but for simplicity let's focus on a linear in parameter model.",
                    "label": 0
                },
                {
                    "sent": "So we just consider a fixed basis function and we take the linear combination and learn this coefficient Alpha from data.",
                    "label": 0
                },
                {
                    "sent": "So that the simplest of laws in practice we use Gaussian kernels were sent out on sample points or something.",
                    "label": 0
                },
                {
                    "sent": "Any models can be used here?",
                    "label": 0
                },
                {
                    "sent": "OK, that's the creep formulation.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK, now we have like integration to integration here and this is actually an expectation of local hop over P numerator.",
                    "label": 0
                },
                {
                    "sent": "And this is an integration of our heart over P denominator.",
                    "label": 0
                },
                {
                    "sent": "But these two are basically expect expectations, so it's an expectation of our look of our model over P numerator and this is an expectation of our model over P denominator and expectations can be approximated by sample operating.",
                    "label": 0
                },
                {
                    "sent": "So we just introduced.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sample operates here, so then we end up in having this problem like this.",
                    "label": 0
                },
                {
                    "sent": "So it looks a bit complicated at first sight, but this is a convex optimization problem.",
                    "label": 1
                },
                {
                    "sent": "So this means just doing gradient descent and projection onto feasible, feasible reason, repeating this one several times gives you the global solution, so it's very simple.",
                    "label": 1
                },
                {
                    "sent": "And also because of this non negativity constraint, actually we can show that the global solution is sparse and sometimes this sparseness is quite useful, but not always.",
                    "label": 1
                },
                {
                    "sent": "Anyway, so this is an algorithm of creep.",
                    "label": 0
                },
                {
                    "sent": "So by that we can directly estimate the density ratio function without really doing against estimation of being numerator and denominator.",
                    "label": 0
                },
                {
                    "sent": "So the basic point was, so we just have expectation of our model of up enumerate.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Therapy denominator.",
                    "label": 0
                },
                {
                    "sent": "So then we don't really have to estimate densities, but we can just replace the expectation by sample rate.",
                    "label": 0
                },
                {
                    "sent": "So this is a simple tree tree, but we can avoid destination by that.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and.",
                    "label": 0
                },
                {
                    "sent": "I want to show a little theoretical properties of this method.",
                    "label": 0
                },
                {
                    "sent": "The only approximation we introduced as the empirical approximation expectations while based by sample averages.",
                    "label": 0
                },
                {
                    "sent": "And now we can start two scenarios and in the first novel it's a parametric case where our model is already fixed, so we have a fixed number of basis functions and we have fixed basis function and we just learn their question.",
                    "label": 0
                },
                {
                    "sent": "So this is a problematic situation.",
                    "label": 0
                },
                {
                    "sent": "And in this parameter situation, learn parameter by the clip algorithm converts to the optimal value in the model with order of N minus half.",
                    "label": 1
                },
                {
                    "sent": "Here into the power of minus half, where N is the minimum of a numerator and denominator.",
                    "label": 0
                },
                {
                    "sent": "So these two are the number of samples from the numerator densities and they don't make the testing.",
                    "label": 0
                },
                {
                    "sent": "And this N minus half is actually the optimal convergence rate in the parametric setup.",
                    "label": 0
                },
                {
                    "sent": "So we can simply say that no method can outperform this approach.",
                    "label": 0
                },
                {
                    "sent": "So this is a very simple approach, but this is already optimal.",
                    "label": 0
                },
                {
                    "sent": "OK, the Prometric case is rather simple, but now so we can start a nonparametric case where the number of parameters that grows with the number of samples.",
                    "label": 0
                },
                {
                    "sent": "But it's a typical situation, model.",
                    "label": 0
                },
                {
                    "sent": "If we put the kernel function on the samples, then the number of basis functions number of primary objective grows.",
                    "label": 0
                },
                {
                    "sent": "Don't like this form?",
                    "label": 1
                },
                {
                    "sent": "And in this case, so we can have a similar result that the Lambda function by by this model still converge the optimal function with order of disorder.",
                    "label": 0
                },
                {
                    "sent": "So end to the power of 1 / -- 2 plus, and gamma is a kind of complexity measure of the function graph rated to the covering number of bracketing entropy.",
                    "label": 1
                },
                {
                    "sent": "So I don't really have to introduce a very technical details here, but we have like this order and disorder was already shown to be the optimal rate.",
                    "label": 0
                },
                {
                    "sent": "But none of them is can go beyond this order, and our simple method achieves this order.",
                    "label": 0
                },
                {
                    "sent": "So this means this is already an optimal method.",
                    "label": 0
                },
                {
                    "sent": "Right, so the idea was quite simple.",
                    "label": 0
                },
                {
                    "sent": "We just fitted density and density estimated by that interracial forfeited under collaborative options, but this was shown to be optimal here with theoretically and algorithm is quite simple.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is just illustrative toy example, so we have two densities, P denominator, it's blue Gaussian like this and the numerator black Gaussian like this and we took 200 samples from each densities.",
                    "label": 0
                },
                {
                    "sent": "So then the true density, it true density ratio is described by the Red Line red line here.",
                    "label": 0
                },
                {
                    "sent": "And estimated density by the clip algorithm was shown here by green line.",
                    "label": 0
                },
                {
                    "sent": "So in this case the clip algorithm gave a very nice approximation to the density to the density ratio function.",
                    "label": 0
                },
                {
                    "sent": "So this direction, this ratio estimation is kind of learning real valued function, so it looks similar to regulation 'cause we are learning function like this.",
                    "label": 0
                },
                {
                    "sent": "But the difference from the regulation is that we don't really have samples from the ratio function or directly like if we have samples in this space then it's a regression problem.",
                    "label": 0
                },
                {
                    "sent": "But in the case of distress estimation, we have samples in this space, so we don't really know the value of our directly.",
                    "label": 0
                },
                {
                    "sent": "But still we can estimate the ratio function in a simple way like this.",
                    "label": 0
                },
                {
                    "sent": "And in this clip procedure, there is a tuning parameter that is the Gaussian with if he is a Gaussian kernel function.",
                    "label": 0
                },
                {
                    "sent": "So this experiment was done with the Gaussian corner.",
                    "label": 0
                },
                {
                    "sent": "And of course if you change the girls song with the solution changes and discussion with it too too large, we have to flap function and if they got some of it is too small, we have highly fluctuated function so it should be tuned carefully.",
                    "label": 0
                },
                {
                    "sent": "And for this purpose we can just use a simple cross validation 'cause we have an objective function, so it's a second term of the clever clever diversions.",
                    "label": 0
                },
                {
                    "sent": "So we have here.",
                    "label": 0
                },
                {
                    "sent": "So we have this decomposition at this time.",
                    "label": 0
                },
                {
                    "sent": "So we are Mac.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Amazing this time so we can use this criterion.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cross validation.",
                    "label": 0
                },
                {
                    "sent": "If you have a set of samples, let's say use 80% for estimating a function density ratio function and use the remaining 20% to validate this error.",
                    "label": 0
                },
                {
                    "sent": "This score so this is maximized.",
                    "label": 0
                },
                {
                    "sent": "And for each location with, we prepare a set of Gaussian within the beginning candidate values, and then for each of the values we train, we learn offensive racial function and we estimate the score global score and then we choose the the Gaussian width that maximizes this score.",
                    "label": 0
                },
                {
                    "sent": "And this solution was obtained like that.",
                    "label": 0
                },
                {
                    "sent": "So in the end, so there's no tuning parameter here.",
                    "label": 0
                },
                {
                    "sent": "So given data, we just do cross validation under we obtain so everybody can obtain the same density ratio function.",
                    "label": 0
                },
                {
                    "sent": "So later after this this this property turns out to be quite important 'cause sometimes we want to solve unsupervised learning problem.",
                    "label": 0
                },
                {
                    "sent": "So in the case of supervised learning problem, maybe we can somehow tune parameters easily.",
                    "label": 0
                },
                {
                    "sent": "But in the case of unsupervised learning, basically there's no way to tune parameters and we just change the parameter and observe the result whether it looks nice or not.",
                    "label": 0
                },
                {
                    "sent": "But if you have, if we use the clip algorithm for for some superb for unsupervised learning tasks, still we can do cross validation and the objective solution can be obtained in the easy way.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Area of the clip algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't involve density estimation, so that's the first important point, and hopefully this improved estimation quality.",
                    "label": 0
                },
                {
                    "sent": "And as I said, cross validation is available for Connor parameter for example.",
                    "label": 1
                },
                {
                    "sent": "So this is quite helpful, helpful in practice.",
                    "label": 0
                },
                {
                    "sent": "And I explained an algorithm for a linear model linear in parameter model, including candle models.",
                    "label": 0
                },
                {
                    "sent": "But this can be extended to other types of models like log linear model or Gaussian mixture or PCM itself.",
                    "label": 0
                },
                {
                    "sent": "We can develop a similar algorithm.",
                    "label": 0
                },
                {
                    "sent": "And also this clip algorithm is actually useful to estimate the kullback Leibler divergent between two things, two distribution.",
                    "label": 0
                },
                {
                    "sent": "This is partially becausw unconstrained variant of this clip algorithm corresponds to maximizing our lower bound of quiver quiver divergent.",
                    "label": 1
                },
                {
                    "sent": "So through that so we can estimate the claim that I got this so I will go back to this this point later, but this clip algorithm is suitable for estimating the critical about I was there.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was the first method and is it?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do you have any questions so far?",
                    "label": 0
                },
                {
                    "sent": "Alright, so then, so that's the first method and then, so let's move to the second method that I think is the most powerful approach.",
                    "label": 0
                },
                {
                    "sent": "So idea is actually quite similar, it's.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Call the density retrofitting.",
                    "label": 0
                },
                {
                    "sent": "So the method is called least squares important fitting, at least IF.",
                    "label": 0
                },
                {
                    "sent": "And now we use the squared loss.",
                    "label": 0
                },
                {
                    "sent": "So I'll have this again.",
                    "label": 0
                },
                {
                    "sent": "Our density ratio model and let's consider the squared error between our model and through ratio function squared.",
                    "label": 0
                },
                {
                    "sent": "And we have a weight function P denominator here, so this is actually important.",
                    "label": 0
                },
                {
                    "sent": "And then now let's just decompose.",
                    "label": 0
                },
                {
                    "sent": "This are hot minus R-squared into our hot square, and two times are hot and all and ask where they just said competition.",
                    "label": 0
                },
                {
                    "sent": "And then if you see the first term, but this is the, this is the expectation of our hot squared over P denominator.",
                    "label": 0
                },
                {
                    "sent": "So again, extension can be approximated by sample operates like this.",
                    "label": 0
                },
                {
                    "sent": "And for the second term.",
                    "label": 0
                },
                {
                    "sent": "So this was originally our heart, our PDE.",
                    "label": 0
                },
                {
                    "sent": "But how is the numerator over P denominator?",
                    "label": 0
                },
                {
                    "sent": "So PDE canceled out here and we only have P numerical here.",
                    "label": 0
                },
                {
                    "sent": "So it's just an expectation of our heart over P numerator, so again this can be approximated by some problems simply like this.",
                    "label": 0
                },
                {
                    "sent": "And the last time ask here is a constant, so it can be ignored.",
                    "label": 0
                },
                {
                    "sent": "So this is our training criteria.",
                    "label": 0
                },
                {
                    "sent": "So now we didn't do anything special, but we just introduce squared loss and just compute it.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And again, let's introduce linear in parameter model like this.",
                    "label": 0
                },
                {
                    "sent": "So in practice this could be a Gaussian linear combination of Gaussian kernel function.",
                    "label": 0
                },
                {
                    "sent": "So then the based on this list I formulation we can consider two algorithms.",
                    "label": 0
                },
                {
                    "sent": "The first one is called constraint SIF.",
                    "label": 0
                },
                {
                    "sent": "So in the first formulation we add.",
                    "label": 0
                },
                {
                    "sent": "And negativity constraint of the parameters.",
                    "label": 0
                },
                {
                    "sent": "And we include L1 regularizer.",
                    "label": 0
                },
                {
                    "sent": "So this is actually one regular, it's just some of parameters, but we have no negativity constraint.",
                    "label": 0
                },
                {
                    "sent": "So then this is actually the L1 penalty.",
                    "label": 0
                },
                {
                    "sent": "And OK, so on each half an 8 update have matrix is defined by this and each had Victor is defined like this.",
                    "label": 0
                },
                {
                    "sent": "So these are just computed from samples and also our basis function.",
                    "label": 0
                },
                {
                    "sent": "Maybe got some kind of function?",
                    "label": 0
                },
                {
                    "sent": "And this is actually a standard convex quadratic program, so we can solve it just by using a standard QP solver.",
                    "label": 1
                },
                {
                    "sent": "And we know that with L1 regularizer the solution becomes sparse.",
                    "label": 0
                },
                {
                    "sent": "So this is the first formulation.",
                    "label": 0
                },
                {
                    "sent": "But actually this.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maybe even more computationally efficient because we can actually track the focal regularization path.",
                    "label": 0
                },
                {
                    "sent": "So this is the formulation and actually we can show that the solution path over Lambda if we change Lambda we want to see how the solution changes.",
                    "label": 0
                },
                {
                    "sent": "If you consider this solution path with respect to Lambda, this is actually piecewise linear.",
                    "label": 1
                },
                {
                    "sent": "So let's see if Lambda is very large, like 1,000,000.",
                    "label": 0
                },
                {
                    "sent": "So then basically these two terms are ignored and we only care the L one term.",
                    "label": 0
                },
                {
                    "sent": "So then minimizing vantomme means 0.",
                    "label": 0
                },
                {
                    "sent": "So if Lambda is very large, so the solution is at the origin in the beginning.",
                    "label": 0
                },
                {
                    "sent": "Then from this large Lambda we reduced the value of Lambda and then at some point the solution starts to depart from the origin and go straight to some point.",
                    "label": 0
                },
                {
                    "sent": "And then at some point Lambda one.",
                    "label": 0
                },
                {
                    "sent": "So actually the direction changes and again the solution calls straight to another point here.",
                    "label": 0
                },
                {
                    "sent": "And at Lambda, two again goes like this and #3 and like that.",
                    "label": 0
                },
                {
                    "sent": "So the solution path is actually piecewise linear like this.",
                    "label": 1
                },
                {
                    "sent": "And the important point is that we can compute this change points in advance without really solving a QP problem.",
                    "label": 0
                },
                {
                    "sent": "Like in the beginning, in the first step we know the solution is at the origin, so we don't have to solve any optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So then the second, the second point, the first change point can be computed from this solution and this part we need to solve a system of linear equations, but by that we can obtain this point analytically.",
                    "label": 0
                },
                {
                    "sent": "And also like the second part, second point can be computed from the first point by solving a system of linear equation like that.",
                    "label": 0
                },
                {
                    "sent": "So just by solving linear equations we can know all the change points and all the solution path.",
                    "label": 0
                },
                {
                    "sent": "And this is actually quite important because we don't really need QP solvers anymore.",
                    "label": 0
                },
                {
                    "sent": "We just have to solve a system of linear equations.",
                    "label": 0
                },
                {
                    "sent": "And also so I already talked about cross validation, but we also need to tune Lambda impact in practice.",
                    "label": 1
                },
                {
                    "sent": "That means that we prepare different values of Lambda and compute the solution and compare the cross validation score.",
                    "label": 0
                },
                {
                    "sent": "But once we take this regularization path approach, we know the solution for all Lambda.",
                    "label": 0
                },
                {
                    "sent": "Actually, the computational complexity for obtaining the entire solution path is the same as solving a single QP problem in the computation order, so it's quite different, and it's quite efficient, including model selection of Lambda.",
                    "label": 0
                },
                {
                    "sent": "So that's quite a convenient approach.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, that was a constraint formulation.",
                    "label": 0
                },
                {
                    "sent": "And finally, I want to introduce even simpler approach is its unconstrained alist LSI formulation.",
                    "label": 0
                },
                {
                    "sent": "I call it Elsif, unconstrained LSI F. And in us, if we forget the negativity constraint.",
                    "label": 0
                },
                {
                    "sent": "To make it simple.",
                    "label": 0
                },
                {
                    "sent": "And then so we replaced the L1 constraint to to complain, but without.",
                    "label": 0
                },
                {
                    "sent": "Without non negativity constraint this this term cannot be a regularizer anymore.",
                    "label": 0
                },
                {
                    "sent": "So we just use a simple penalty.",
                    "label": 0
                },
                {
                    "sent": "So then if you are familiar with Richard Grayson, so this is similar to a form of Revelation.",
                    "label": 0
                },
                {
                    "sent": "So as a function of Alpha, it's just a quadratic function, so without any constraint.",
                    "label": 0
                },
                {
                    "sent": "So if you take the derivative and equate it to zero, you can obtain the solution analytically like this.",
                    "label": 0
                },
                {
                    "sent": "So this H hot matrix on a chart vector are the same as the previous one.",
                    "label": 0
                },
                {
                    "sent": "So in the end, so that the ratio can be just estimated by rhythm, these squares regularize this clear.",
                    "label": 0
                },
                {
                    "sent": "So it's an unconstrained formulation.",
                    "label": 0
                },
                {
                    "sent": "And another important computational advantage of this unconstrained LSI F is that leave one out cross Validation Square.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can be computed negatively.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the same same line of regression.",
                    "label": 0
                },
                {
                    "sent": "But if we perform lever not cross validation, so we compute a solution with any minus one samples and we validate its error by the remaining single single point and this is repeated for all in samples.",
                    "label": 0
                },
                {
                    "sent": "And if N is large, usually this is not tractable.",
                    "label": 0
                },
                {
                    "sent": "But in the case of us, if we can compute this liver note score analytically, we don't really have to repeat the computation validation process for anytime, but only a single shot procedure.",
                    "label": 0
                },
                {
                    "sent": "And we can compute analytically the River, not score.",
                    "label": 0
                },
                {
                    "sent": "So this means the final computation time, including model selection, can be significantly reduced.",
                    "label": 1
                },
                {
                    "sent": "Each solution can be computed analytically by by this way, just by solving a system of linear equations and also the model section score, liver not square can also be computed analytically.",
                    "label": 0
                },
                {
                    "sent": "So everything is so simple but we can achieve the best performance, right?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and we have similar theoretical properties and for parametric case and nonparametric case basically.",
                    "label": 0
                },
                {
                    "sent": "So it's the same as the previous screen.",
                    "label": 0
                },
                {
                    "sent": "So both in both cases the simple of this approach can achieve optimal convergence, right?",
                    "label": 0
                },
                {
                    "sent": "So it's a very simple approach, but this is the optimal approach.",
                    "label": 1
                },
                {
                    "sent": "And also one more addition is that so about numerical stability when we solve an optimization problem by some iterative algorithm.",
                    "label": 1
                },
                {
                    "sent": "So sometimes numerical stability is quite important.",
                    "label": 0
                },
                {
                    "sent": "And we can prove that actually this massive optimization problem has the smallest condition number among a class of the general class of density ratio estimator.",
                    "label": 1
                },
                {
                    "sent": "So smallest condition number means it has the.",
                    "label": 0
                },
                {
                    "sent": "Highest stability, so that means from a numerical viewpoint, the algorithm, the algorithm becomes quite stable, so this is useful if you have a large number of samples and we have a huge matrix and you need to invert it.",
                    "label": 0
                },
                {
                    "sent": "So the smallest condition number is very stable and fast optimization.",
                    "label": 0
                },
                {
                    "sent": "So anyway, so this simple ulses approach has very desirable theoretical properties.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, one more simple numerical example here.",
                    "label": 1
                },
                {
                    "sent": "So OK, now let's P numerator is the Gaussian centered at origin and covariance matrix is identity.",
                    "label": 0
                },
                {
                    "sent": "NP denominator is again Gaussian, but center is 100 and covariance matrix is identity.",
                    "label": 0
                },
                {
                    "sent": "And we increase the dimension of the space.",
                    "label": 0
                },
                {
                    "sent": "And horizontal axis is the dimension of the input space and the vertical axis is the mean squared error of the estimated density ratio values function.",
                    "label": 0
                },
                {
                    "sent": "And if you just perform confidence estimation for two for these two densities, I'm taking their ratio, then actually the error grows like this.",
                    "label": 0
                },
                {
                    "sent": "So this is a log scale, so it's basically the cost of dimensionality in this case.",
                    "label": 0
                },
                {
                    "sent": "But at least for this toy data.",
                    "label": 0
                },
                {
                    "sent": "So this algorithm for this algorithm, the error, doesn't really increase.",
                    "label": 0
                },
                {
                    "sent": "Of course it's not a general result, but at least for this simple case, directly estimating the ratio is really helpful.",
                    "label": 0
                },
                {
                    "sent": "We don't really suffer from the cost of dimensionality for this toy data set.",
                    "label": 0
                },
                {
                    "sent": "And in many cases, directory is making the ratio is much better than taking the ratio of kernel density estimator.",
                    "label": 1
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So OK, so we didn't really increase the dimension more than 20 'cause we only have a small number of samples on.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it has some numerical problems, but still OK, maybe this decrease here is a little bit artifact, but still I'm just saying that the error doesn't really grow exponentially in this case, so this is like minimum message here.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Kate.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK, this is somebody summary of this algorithm, so just simple least squares formulation is quite computationally efficient for density estimation, and if we have a cross train, then that regularization path tracking is available.",
                    "label": 1
                },
                {
                    "sent": "And if we forget the negativity constraint, then analytic solution is available and also leave are not square can be expressed analytically.",
                    "label": 0
                },
                {
                    "sent": "So this is a computer very efficient and also it's very easy to implement this sometimes.",
                    "label": 0
                },
                {
                    "sent": "This easy implementation is quite important, 'cause like I am, I am living in Tokyo and if you're in Tokyo it's quite nice that we have many companies around the University so we can have many partners industry, but usually they don't really want to implement difficult algorithms, even support vector machine is quite difficult for them.",
                    "label": 0
                },
                {
                    "sent": "Of course they can download the software, but they can't use downloaded software in their product.",
                    "label": 0
                },
                {
                    "sent": "But in the end they need to implement it by themselves.",
                    "label": 0
                },
                {
                    "sent": "It's quite time consuming and sometimes impossible for engineers there.",
                    "label": 0
                },
                {
                    "sent": "But maybe they know least squares and they have some software for solving a system of linear equations.",
                    "label": 0
                },
                {
                    "sent": "So then actually solving many machine learning tasks just by these squares is really important for this industry people.",
                    "label": 0
                },
                {
                    "sent": "And in the case of a clip algorithm, I say that it is suitable for estimating the kullback Leibler divergent.",
                    "label": 0
                },
                {
                    "sent": "But for this Alice algorithm, actually it is suitable for approximating a so-called Pearson diversion.",
                    "label": 0
                },
                {
                    "sent": "Appear some divergences defined like this?",
                    "label": 0
                },
                {
                    "sent": "Maybe you are not really familiar with this kind of function, But this is so clever.",
                    "label": 0
                },
                {
                    "sent": "Clever divergences is a member of a class code.",
                    "label": 0
                },
                {
                    "sent": "F diversions.",
                    "label": 0
                },
                {
                    "sent": "And this person divergences is also a member of diversions and divergences included in this.",
                    "label": 0
                },
                {
                    "sent": "If divergent class share kind of similar theoretical properties.",
                    "label": 0
                },
                {
                    "sent": "So this can be a kind of squared loss bazhenov callback right about this.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can regard it as so.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "By using the active algorithm we can approximate this person divergent analytically.",
                    "label": 0
                },
                {
                    "sent": "So that's quite nice, and later I will show that we use this Pearson diversions approximator to solve some machine learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "And there we have some additional parameters to be learned, like in the case of a set dimension reduction.",
                    "label": 0
                },
                {
                    "sent": "So we want to find like subspace the projector samples.",
                    "label": 0
                },
                {
                    "sent": "So then basically we want to such a subspace.",
                    "label": 0
                },
                {
                    "sent": "And we may want to minimize or maximize something.",
                    "label": 0
                },
                {
                    "sent": "Pearson diversions as a function of this subspace.",
                    "label": 0
                },
                {
                    "sent": "And if we have a analytic form approximator of this divergent, then we can compute its gradient.",
                    "label": 0
                },
                {
                    "sent": "So then we can basically solve the subspace easy.",
                    "label": 0
                },
                {
                    "sent": "But if we use a creep algorithm, we can approximate the clever clever diversions very well, but the solution is defined as a solution over from newmaker optimization problem.",
                    "label": 1
                },
                {
                    "sent": "So then we can't really compute the derivative.",
                    "label": 0
                },
                {
                    "sent": "So then we can't really use it for like dimension reduction or some other purposes.",
                    "label": 0
                },
                {
                    "sent": "So having another having an analytic solution is really essential in solving many machine learning task and by using the Pearson diversions we can really achieve that.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so so far I introduced to basically basically two methods of machine or two methods of density ratio estimators and I'm saying that the Elsif algorithm.",
                    "label": 1
                },
                {
                    "sent": "It is a kind of kind of regression type algorithm is the simplest and most useful algorithm for density ratio estimation.",
                    "label": 0
                },
                {
                    "sent": "Any questions so far?",
                    "label": 0
                },
                {
                    "sent": "OK, so then from now let's move on to the next part on.",
                    "label": 1
                },
                {
                    "sent": "I want to show possible usages of density ratio function.",
                    "label": 0
                },
                {
                    "sent": "And already I thought in the beginning we have roughly four types of applications and let's go 1 by 1.",
                    "label": 0
                },
                {
                    "sent": "The first one is important sampling.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I started this work from this important sampling part and it's called learning under Covariate Shift.",
                    "label": 0
                },
                {
                    "sent": "And the covariate shift is a supervised learning situation where training and test input distributions are different.",
                    "label": 1
                },
                {
                    "sent": "But still the target function to belong remains unchanged.",
                    "label": 0
                },
                {
                    "sent": "Decisions like that.",
                    "label": 0
                },
                {
                    "sent": "Let's consider simple regression problem.",
                    "label": 0
                },
                {
                    "sent": "And we have training input density active.",
                    "label": 0
                },
                {
                    "sent": "And test input density here platform.",
                    "label": 0
                },
                {
                    "sent": "So they are different.",
                    "label": 0
                },
                {
                    "sent": "And then so it's a regression problem.",
                    "label": 0
                },
                {
                    "sent": "We have a common function to estimate the red one.",
                    "label": 0
                },
                {
                    "sent": "But the training points are coming from this blue Gaussian.",
                    "label": 0
                },
                {
                    "sent": "That means we have training points in the left hand side of the graph.",
                    "label": 0
                },
                {
                    "sent": "But for this testing phase we have test points not from the green blue distribution but black distribution.",
                    "label": 0
                },
                {
                    "sent": "So we have test point in the right hand side so we don't really have test samples in reality.",
                    "label": 0
                },
                {
                    "sent": "But I just showed up for the sake of visualization.",
                    "label": 0
                },
                {
                    "sent": "So basically we want to predict these black crosses from blue samples.",
                    "label": 0
                },
                {
                    "sent": "It's a kind of extrapolation problem, but I call it weak separation 'cause we need to have overlap between two densities.",
                    "label": 0
                },
                {
                    "sent": "If training P training and P testing are completely disjoint, then theoretically it is impossible to make prediction for the test distribution.",
                    "label": 0
                },
                {
                    "sent": "So we need to have some overlap, so overlap means we don't have many samples here, but in principle in principle we have some samples here.",
                    "label": 0
                },
                {
                    "sent": "So that's the situation.",
                    "label": 0
                },
                {
                    "sent": "So covariate is, by the way, the name of input in statistics.",
                    "label": 0
                },
                {
                    "sent": "So covariate shift means the input distribution changes.",
                    "label": 0
                },
                {
                    "sent": "But the target function P of Y given XY distribution doesn't change.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's do this one dimensional example to explain what is happening.",
                    "label": 0
                },
                {
                    "sent": "Actually, in the covariate situation.",
                    "label": 0
                },
                {
                    "sent": "And let's go to the simplest straight line fitting by these squares so we have A1 plus A2 X so simple straight line model and we learn A1 and Alpha to buy standard B ^2.",
                    "label": 0
                },
                {
                    "sent": "So then if there's no covariate shift, then.",
                    "label": 0
                },
                {
                    "sent": "This squares was shown to be consistent.",
                    "label": 0
                },
                {
                    "sent": "Consistent means if you have infinitely many samples, you can recover the optimal function.",
                    "label": 0
                },
                {
                    "sent": "For N goes to Infinity, you have the best function.",
                    "label": 1
                },
                {
                    "sent": "This is consistency.",
                    "label": 0
                },
                {
                    "sent": "But under coverage has already, you can see see here.",
                    "label": 0
                },
                {
                    "sent": "Ordinary squares is not consistent anymore.",
                    "label": 0
                },
                {
                    "sent": "So we want to make prediction of these black crosses, so we shouldn't have function like this, But we should have a function like this positive slope.",
                    "label": 0
                },
                {
                    "sent": "And we have already many samples here and this function is almost converged already, so this already illustrates the inconsistency of these squares under covariate shift.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the proof of consistency is actually quite simple.",
                    "label": 0
                },
                {
                    "sent": "Usually we just use the law of large numbers.",
                    "label": 1
                },
                {
                    "sent": "Loss function, so in the case of least squares, we use a squared loss, so those function averaged over in samples.",
                    "label": 1
                },
                {
                    "sent": "Converge the expectation over the last function of RP train.",
                    "label": 0
                },
                {
                    "sent": "So we are training in points are coming from P train, so this is a simple law of large numbers.",
                    "label": 0
                },
                {
                    "sent": "So if training, distribution, training, input, distribution and test input distribution are the same, then so this simple law guarantees that consistency.",
                    "label": 0
                },
                {
                    "sent": "But under covariate shift actually training points are coming from P train.",
                    "label": 0
                },
                {
                    "sent": "But what we want to minimize in them is loss function expected over P test.",
                    "label": 0
                },
                {
                    "sent": "So this still converts to this one.",
                    "label": 1
                },
                {
                    "sent": "But we want to minimize this one.",
                    "label": 0
                },
                {
                    "sent": "So by that we have inconsistency.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But this inconsistency can be easily avoided by using the important weighting technique.",
                    "label": 0
                },
                {
                    "sent": "But the important is defined as the ratio of test and training important things like this.",
                    "label": 1
                },
                {
                    "sent": "And then instead of just computing the.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample rate",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We compute the importance weighted sample average like this.",
                    "label": 0
                },
                {
                    "sent": "The last function is weighted by this important weight.",
                    "label": 0
                },
                {
                    "sent": "Everest overall samples.",
                    "label": 0
                },
                {
                    "sent": "Then again, let's just apply the simple law of large numbers to this function this entire function.",
                    "label": 0
                },
                {
                    "sent": "So then this entire function converts to the expectation of PPP train.",
                    "label": 0
                },
                {
                    "sent": "And then soapy train here on P train here cancelled out each other and in the end we have loss expected over pee test.",
                    "label": 0
                },
                {
                    "sent": "So by that we can at least asymptotically minimize the loss function for the test point.",
                    "label": 0
                },
                {
                    "sent": "Is it clear?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So then OK, let's use this idea in this square.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just we have important slated this squares we have here important weight and of course we can use the density ratio estimation to obtain this important way.",
                    "label": 0
                },
                {
                    "sent": "And this important weighted risk is consistent even under covariate is already we have seen in the previous slide.",
                    "label": 1
                },
                {
                    "sent": "And I just use the least squares as an example, but this simple important waiting idea can be applicable.",
                    "label": 0
                },
                {
                    "sent": "Can be applied to any likelihood based method.",
                    "label": 0
                },
                {
                    "sent": "If you have a log likelihood or any loss function then just you need to wait it by importance lately.",
                    "label": 0
                },
                {
                    "sent": "Like support vector machine, logistic regression, conditional random field, maybe almost all machine learning techniques can be used in recovery rates scenario if we have density ratio function here important, wait here.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is the method on.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One more thing we need to consider a little bit is actually the bias variance tradeoff.",
                    "label": 0
                },
                {
                    "sent": "So I just said that if we have important weight then OK, we can recover consistency.",
                    "label": 0
                },
                {
                    "sent": "So if you have infinitely many points, we can have the best function that the theoretical guarantee I gave so far.",
                    "label": 0
                },
                {
                    "sent": "But of course, in reality we don't have infinitely many samples, only a finite number of samples, maybe small number of samples.",
                    "label": 0
                },
                {
                    "sent": "So then so bias variance tradeoff is an important issue.",
                    "label": 1
                },
                {
                    "sent": "If we don't have any weight, then after we can so that it has lower variance but high by us.",
                    "label": 0
                },
                {
                    "sent": "And on the other hand, if we use importance weight then we can show that it has low bias but high variance.",
                    "label": 0
                },
                {
                    "sent": "But in practice, we want to minimize the kind of sum of bias and variance because the squared error can be decomposed decomposed into squared bias plus variance.",
                    "label": 0
                },
                {
                    "sent": "So we want to minimize the sum.",
                    "label": 0
                },
                {
                    "sent": "So one of the practical approaches to flatten this important weight.",
                    "label": 0
                },
                {
                    "sent": "Like we have a power parameter Lambda here.",
                    "label": 0
                },
                {
                    "sent": "And if Lambda is 0, then wait, it's zero.",
                    "label": 0
                },
                {
                    "sent": "Wait, wait is 1 so it's a flat weight.",
                    "label": 0
                },
                {
                    "sent": "So this corresponds to ordinary ordinary least squares.",
                    "label": 0
                },
                {
                    "sent": "If Lambda is 1, it's important.",
                    "label": 0
                },
                {
                    "sent": "Waited this square so it is consistent, but it has a high variance.",
                    "label": 0
                },
                {
                    "sent": "So in this simple artificial example, if Lambda is set to 0.5, then actually we can have a very nice function that optimally controlled the tradeoff between bias and variance.",
                    "label": 0
                },
                {
                    "sent": "But of course 0.5 is just an example in this case, and this Lambda should be chosen carefully depending on the data in the problem.",
                    "label": 0
                },
                {
                    "sent": "Depending on the noise level, something like that.",
                    "label": 0
                },
                {
                    "sent": "So it's basically a model selection problem.",
                    "label": 1
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in this model selection problem again importance weight plays an important role, 'cause if he used standard model selection techniques like Akaike information criterion across validation, then it's biased.",
                    "label": 1
                },
                {
                    "sent": "Both training and test points are points follow different distribution.",
                    "label": 0
                },
                {
                    "sent": "So if you just naively do cross validation, then OK about cross validation score expire.",
                    "label": 0
                },
                {
                    "sent": "And already kind of importance weighted version of model selection criteria well investigated in these papers, and the simplest one is actually to extend the cross validation.",
                    "label": 0
                },
                {
                    "sent": "So in cross validation, so we divide our data into K groups and use K -- 1 groups for training and use the remaining group for validation.",
                    "label": 0
                },
                {
                    "sent": "And in user cross validation we just use squared loss for validation.",
                    "label": 0
                },
                {
                    "sent": "But under covariate shift we again use important late here.",
                    "label": 0
                },
                {
                    "sent": "So then, but by this important weighting the this validation data looks like a test point.",
                    "label": 0
                },
                {
                    "sent": "The validation data is coming from P train, but by applying this important weight it looks as a test point.",
                    "label": 0
                },
                {
                    "sent": "By that we can kind of adjust the cross validation score.",
                    "label": 0
                },
                {
                    "sent": "200 covariate assistance.",
                    "label": 0
                },
                {
                    "sent": "OK, we're together, we can.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Perform the adaptation undercover, it's scenario.",
                    "label": 0
                },
                {
                    "sent": "And here are some some examples.",
                    "label": 0
                },
                {
                    "sent": "The first example is a speaker identification task.",
                    "label": 1
                },
                {
                    "sent": "So I I'm from Japan, so I used Japanese data set.",
                    "label": 0
                },
                {
                    "sent": "It's actually quite famous data set in Japan.",
                    "label": 0
                },
                {
                    "sent": "And it's called text independent speaker identification.",
                    "label": 1
                },
                {
                    "sent": "So Speaker speaks something, but text is not fixed.",
                    "label": 0
                },
                {
                    "sent": "But still we want to identify who is speaking.",
                    "label": 0
                },
                {
                    "sent": "And we used kernel secret mission for classification and we use a sequence corner.",
                    "label": 0
                },
                {
                    "sent": "And the setup here is that the training data and test data are coming from different time periods like this data is now, but we try to use training data gathered in the past.",
                    "label": 0
                },
                {
                    "sent": "But if the training data is gathered nine months before.",
                    "label": 1
                },
                {
                    "sent": "So then so usually actually the distribution of voice is actually quite different.",
                    "label": 0
                },
                {
                    "sent": "After three months it is said.",
                    "label": 0
                },
                {
                    "sent": "So then so we need some kind of adjustment.",
                    "label": 0
                },
                {
                    "sent": "And I'm OK.",
                    "label": 0
                },
                {
                    "sent": "This part is just ordinary carnal carnal relations plus cross validation, so no other adaptation.",
                    "label": 0
                },
                {
                    "sent": "Then the recognition rate is not so high.",
                    "label": 0
                },
                {
                    "sent": "But if we use important sweded version of kernel circulation and also important rated bars on the cross validation and importers, weights are estimated by clip in this case.",
                    "label": 0
                },
                {
                    "sent": "So then the performances at the uniformly improved in this case.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple adaptation procedure and I didn't use any knowledge in like signal processing just by looking at the statistical properties.",
                    "label": 0
                },
                {
                    "sent": "But still the performance can be improved not too much, but reasonably well.",
                    "label": 0
                },
                {
                    "sent": "And of course, on top.",
                    "label": 0
                },
                {
                    "sent": "On top of that, we can incorporate more prior knowledge in the target domain, signal processing, speech recognition.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one more example is against the Japanese text analysis.",
                    "label": 0
                },
                {
                    "sent": "So actually English English is nice, or German is nice because you have blank between 2 walls.",
                    "label": 0
                },
                {
                    "sent": "But so this is actually a Japanese sentence, so it means something like such a failure is cute or something like that.",
                    "label": 0
                },
                {
                    "sent": "Charming or something, but this is a sentence and there's no blank between walls.",
                    "label": 0
                },
                {
                    "sent": "So actually this preprocessing is really important task in Japanese language analysis it's called text segmentation.",
                    "label": 0
                },
                {
                    "sent": "So given the sentence we want to chop it into words.",
                    "label": 0
                },
                {
                    "sent": "So that's quite important preprocessing step so the solution is actually like this.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Now the task is actually adaptation from daily conversation to medical domain.",
                    "label": 1
                },
                {
                    "sent": "So we have actually a lot of corpus already segmented data for daily conversation, so maybe some companies spend some money to prepare labeled data.",
                    "label": 0
                },
                {
                    "sent": "But now we want to apply this training data to more specific domains such as medical domain.",
                    "label": 0
                },
                {
                    "sent": "And we used a bit complex algorithm called conditional random field that can handle this kind of sequence data in a very nice way, formalized way.",
                    "label": 0
                },
                {
                    "sent": "And again, we compare importance weighted methods and no important weight method and the accuracy is measured by F measure.",
                    "label": 0
                },
                {
                    "sent": "For larger it's simply better.",
                    "label": 0
                },
                {
                    "sent": "So we got 94 point but this is 92 points so we had a little improvement here.",
                    "label": 0
                },
                {
                    "sent": "And the third part.",
                    "label": 0
                },
                {
                    "sent": "So this is interesting.",
                    "label": 0
                },
                {
                    "sent": "'cause we spend some additional cost to have labels from the test domain medical domain.",
                    "label": 0
                },
                {
                    "sent": "Untrain CLF on cross validation again.",
                    "label": 0
                },
                {
                    "sent": "So then we achieve 94.43.",
                    "label": 0
                },
                {
                    "sent": "So these two are quite quite equivalent, comparable.",
                    "label": 0
                },
                {
                    "sent": "So this means that so we don't really have to spend like additional cost.",
                    "label": 0
                },
                {
                    "sent": "In this case, it seems supervised.",
                    "label": 0
                },
                {
                    "sent": "We need unlabeled points from the test domain, but we don't need level points.",
                    "label": 0
                },
                {
                    "sent": "Material we can achieve a similar performance.",
                    "label": 1
                },
                {
                    "sent": "To the case where we had additional test labels for this domain.",
                    "label": 0
                },
                {
                    "sent": "So this is quite quite nice.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are many more interesting applications like age prediction from face images by looking at the face we want to predict like gender and age over person.",
                    "label": 1
                },
                {
                    "sent": "This is actually quite useful for like targeting advertisement like this is joint work with actually and we see they have a system like that should be installed in front of the station or in front of the supermarket and they want to know who is coming, but they try not to really identify a problem but just gender and age.",
                    "label": 0
                },
                {
                    "sent": "And then, depending on the gender and age, they changed the advertisement like 20s man OK, their computer game or we thought these women OK cosmetic.",
                    "label": 0
                },
                {
                    "sent": "And in this scenario, the training data is gathered in my studio where the lighting condition is perfectly adjusted.",
                    "label": 0
                },
                {
                    "sent": "But if you like install the system, let's say in front of the supermarket, then like in the morning you have sunlight from this direction I'm going today.",
                    "label": 0
                },
                {
                    "sent": "You have sunlight from this.",
                    "label": 0
                },
                {
                    "sent": "An individual here and at night may be quite dark, so lighting condition changes a lot and we need some kind of adaptation.",
                    "label": 0
                },
                {
                    "sent": "And I must say that maybe it's not covariate shift.",
                    "label": 0
                },
                {
                    "sent": "Every everything has been changed, but still we assume that it's close to covariate shift and then the performance is improved nicely.",
                    "label": 0
                },
                {
                    "sent": "But at the same time, this is the weakness of this coverage adaptation approach, 'cause we don't really know whether we are saying that.",
                    "label": 0
                },
                {
                    "sent": "Distributions of exchanges input changes OK, this is fine, so we can easily confirm that OK distributions are different, but we need to assume that the conditional distribution of output P of Y given X doesn't change.",
                    "label": 0
                },
                {
                    "sent": "The function doesn't change.",
                    "label": 0
                },
                {
                    "sent": "So this is something we need to assume in this framework.",
                    "label": 0
                },
                {
                    "sent": "And if you don't have any samples from test distribution, then there's no way to really validate this assumption.",
                    "label": 0
                },
                {
                    "sent": "So perhaps you need some prior knowledge, or you need a small number of label test samples to see with the coverage shift assumption is, well, reasonable or not.",
                    "label": 0
                },
                {
                    "sent": "And also we did some experiments with brain computer interface and because of the mental condition change again the distribution of samples really changes because you are all experts on PCI.",
                    "label": 0
                },
                {
                    "sent": "So maybe I shouldn't really speak a lot about PCI today, so.",
                    "label": 0
                },
                {
                    "sent": "OK, so again VCI problem.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's not really covariate shift but still sometimes this kind of coverage just out of this and technique can really help improving the performance.",
                    "label": 0
                },
                {
                    "sent": "And those are finally in robot control, so we may want to robot learns how to move and getting data and update his moving policy and up again get the data and update his moving policy.",
                    "label": 0
                },
                {
                    "sent": "So then actually data gathering distribution changes overtime.",
                    "label": 0
                },
                {
                    "sent": "But still we may want to reuse previously collected data, so gathering data is actually costly in many robotics program.",
                    "label": 0
                },
                {
                    "sent": "So in this case we again want to have a important weighting technique to adjust the distribution difference between the previous policy, previous control policy and the current control policy.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this was the important.",
                    "label": 0
                },
                {
                    "sent": "Something part and any questions.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so then let's completely change the story now and go through the 2nd application distribution comparison.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's consider an outlier detection problem.",
                    "label": 0
                },
                {
                    "sent": "So the original outlet next on problem is given a set of samples you want to find irregular points in your data set.",
                    "label": 1
                },
                {
                    "sent": "But I want to consider the ratio.",
                    "label": 0
                },
                {
                    "sent": "So we need two sets of samples, so let me slightly change the problem.",
                    "label": 0
                },
                {
                    "sent": "So now we have two sets of samples and one set consists of only inliers regular points.",
                    "label": 0
                },
                {
                    "sent": "And the other set is attested from which we want to identify outliers if exists.",
                    "label": 0
                },
                {
                    "sent": "So we have one more data set that consists only of inliers.",
                    "label": 0
                },
                {
                    "sent": "So this is like slight change of the problem.",
                    "label": 0
                },
                {
                    "sent": "Like in the simplest case, OK, we have P regular distribution like this.",
                    "label": 0
                },
                {
                    "sent": "It's just the caption.",
                    "label": 0
                },
                {
                    "sent": "And P test is blue Gaussian but we have outlier here.",
                    "label": 0
                },
                {
                    "sent": "And in the third part, outlier Dixon.",
                    "label": 0
                },
                {
                    "sent": "We try to find this small bump by dense destination and say that OK, here's an outlier at X = 5.",
                    "label": 0
                },
                {
                    "sent": "But as you can see, estimating this smaller boundary bump bump is extremely difficult.",
                    "label": 0
                },
                {
                    "sent": "But once we have P regular distribution density then the taking their ratio we have a function like this.",
                    "label": 0
                },
                {
                    "sent": "For inlier point entire region, so we have almost the same density both calcium.",
                    "label": 0
                },
                {
                    "sent": "So then the ratio value is almost one.",
                    "label": 0
                },
                {
                    "sent": "But for this outlier part we have small increase of the value here.",
                    "label": 0
                },
                {
                    "sent": "But the original value here is very small.",
                    "label": 0
                },
                {
                    "sent": "So then once you take the ratio, the ratio is far from 1.",
                    "label": 0
                },
                {
                    "sent": "So finding this small bump is extremely difficult, but finding this big bottom is quite easy.",
                    "label": 0
                },
                {
                    "sent": "So this means by concerning density ratio function in this outlier detection scenario, we can somehow convert us extremely difficult problem into a very simple one.",
                    "label": 0
                },
                {
                    "sent": "And of course, our assumption is that we have P regular data points, but maybe this is not a big problem in many cases.",
                    "label": 0
                },
                {
                    "sent": "Like next page I show some application but in let's say in some quality control in factory we already know that yesterday or products are fine.",
                    "label": 0
                },
                {
                    "sent": "So then we just use that data as P regular and try to find outliers from today's web points.",
                    "label": 0
                },
                {
                    "sent": "So of course it's an assumption, but maybe it's not so practical, so restrictive in practice.",
                    "label": 0
                },
                {
                    "sent": "And also another important point, I want to emphasize here is that so outlier detection is an unsupervised learning problem and already we have many nice outlier detection algorithms like one class support vector machine.",
                    "label": 0
                },
                {
                    "sent": "It works quite well given that hyperparameters are chosen optimally.",
                    "label": 0
                },
                {
                    "sent": "But in this outlier detection problem, basically there's no way to really feel like Gaussian with regulation parameter 'cause we don't know which points are outliers.",
                    "label": 0
                },
                {
                    "sent": "The in completely unsupervised setup.",
                    "label": 0
                },
                {
                    "sent": "So then it's more like an art.",
                    "label": 0
                },
                {
                    "sent": "So if you do outlet Dixon then this point on this point, well regarded as I'd like.",
                    "label": 0
                },
                {
                    "sent": "But if you do outlet Jackson maybe other points are regarded as well.",
                    "label": 0
                },
                {
                    "sent": "But once we use this density ratio approach, so as I said before, we can do cross validation.",
                    "label": 0
                },
                {
                    "sent": "Of course, if this one is done in terms of the density ratio approximation error, so it's a bit different from outlier detection error, but still it's objective, so everybody can do the same procedure and obtain the same solution.",
                    "label": 1
                },
                {
                    "sent": "So this kind of object objectivity is actually quite important in real world application.",
                    "label": 0
                },
                {
                    "sent": "Otherwise none of the outlier detection techniques is really useful in practice.",
                    "label": 0
                },
                {
                    "sent": "And here's a toy.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example, so machine learning researchers like USPS data set and maybe we have seen this kind of data set for hundreds of times, but actually it is known that USPS.",
                    "label": 0
                },
                {
                    "sent": "Test data set contains some outliers.",
                    "label": 0
                },
                {
                    "sent": "Some points that are very difficult to classify, so it's almost impossible to achieve 100% accuracy because of outliers in the test set.",
                    "label": 0
                },
                {
                    "sent": "So we tried to identify outliers in the test set based on the training data set.",
                    "label": 0
                },
                {
                    "sent": "And assume that training data set contains on the entire.",
                    "label": 0
                },
                {
                    "sent": "And then so these 10 digits are top ten outliers in the USPS test data set.",
                    "label": 0
                },
                {
                    "sent": "And this is actually labeled as five.",
                    "label": 0
                },
                {
                    "sent": "And this is actually David at 0, but it looks sick and like that, but most of them are even difficult to read by human, so they are maybe outliers in the test set.",
                    "label": 0
                },
                {
                    "sent": "So this roughly illustrates the validity of this density ratio approach.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another application is failure prediction in hard disk drive.",
                    "label": 1
                },
                {
                    "sent": "So recently we don't really use how describe but SSD, but previous hard disk drives included a system called self monitoring and Reporting Technology smart, so it records the behavior of the hard disk drive and if it crashes then that record us will be shown and and the companies can really analyze.",
                    "label": 0
                },
                {
                    "sent": "But what happened behind?",
                    "label": 0
                },
                {
                    "sent": "And we applied this hard disk drive data.",
                    "label": 0
                },
                {
                    "sent": "Use this art described data and compared against the ratio method and one class SVM and also another method called local outlier factor.",
                    "label": 1
                },
                {
                    "sent": "So this was proposed in a data database area and this is actually quite popular method in that area.",
                    "label": 0
                },
                {
                    "sent": "So they use.",
                    "label": 0
                },
                {
                    "sent": "Nearest neighbors course to identify local outlier point.",
                    "label": 0
                },
                {
                    "sent": "And OK, so we computed area under the curve or seek off.",
                    "label": 0
                },
                {
                    "sent": "So larger is better.",
                    "label": 0
                },
                {
                    "sent": "One is the best.",
                    "label": 0
                },
                {
                    "sent": "And then OK overall.",
                    "label": 0
                },
                {
                    "sent": "So this method is not bad, but this local outlier factor is the best performing method in this case.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that in this method we need to tune the number of nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "And if it is set to 30, it performs quite well, but if it's set to five, it doesn't perform well.",
                    "label": 0
                },
                {
                    "sent": "And in this benchmark I know the answer so we can tune the solution.",
                    "label": 0
                },
                {
                    "sent": "But in practice there's no way to choose the number of nearest neighbors, so this is a nice method potentially, but in practice it's not so useful.",
                    "label": 0
                },
                {
                    "sent": "But in our case we use cross validation so everybody can obtain the same solution by procedure by the cross validation procedure and also it's computationally quite efficient 'cause nearest neighbor thoughts in a big database is very time consuming.",
                    "label": 0
                },
                {
                    "sent": "Of course there are several recognized fast approximation algorithm, but still the nearest neighbor search is quite time consuming.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And more applications in like steel plant analysis or like printer roller analysis and things like that.",
                    "label": 0
                },
                {
                    "sent": "Again, it's nice to be in Tokyo, but we have many companies and they have many outliers in their data set.",
                    "label": 0
                },
                {
                    "sent": "And in the outlier detection case, we care, so we're comparing two distributions, but we care kind of pointwise difference difference at the outlier point.",
                    "label": 0
                },
                {
                    "sent": "But now let's consider the difference of entire distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is basically an estimation of a divergent between two probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So now the task is, given two sets of data points, we want to estimate, let's say the clever clever diversions.",
                    "label": 0
                },
                {
                    "sent": "But this is a very simple setup and if we use the creep algorithm, we may estimate equivocal but Oh well.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if he uses the House, if algorithm these squares algorithm, then we may want to estimate this fearsome divergent.",
                    "label": 0
                },
                {
                    "sent": "But anyway, in both cases we just have the simple density ratio, P /, P prime.",
                    "label": 0
                },
                {
                    "sent": "So we can just apply the denser estimator and approximate the divergences.",
                    "label": 0
                },
                {
                    "sent": "And we can actually theoretically prove that this density ratio based diversions of approximator is again optimal.",
                    "label": 0
                },
                {
                    "sent": "So none of the method can go beyond this approach.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple approach, but theoretically it achieves the best performance.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this divergent estimation can also be used for many applications, like if we want to find region of interest in images then so we can start a 2 reasons to surrounding regions and computer like inner region after region distribution of samples in an outdoor reason.",
                    "label": 0
                },
                {
                    "sent": "And by comparing this exhaustively in the image, then we can find the object in the net.",
                    "label": 0
                },
                {
                    "sent": "So this kind of thing is quite important.",
                    "label": 0
                },
                {
                    "sent": "If you would give up.",
                    "label": 0
                },
                {
                    "sent": "Let's say this is our camera.",
                    "label": 0
                },
                {
                    "sent": "If you want to find an object and have a forecast automatically to the target object.",
                    "label": 0
                },
                {
                    "sent": "Or if you have movies we want to find events.",
                    "label": 0
                },
                {
                    "sent": "Of course, the definition of event is on Vegas here, but we just take windows of two consecutive time Times Sigma Times segment and take samples like in a sliding window way like this.",
                    "label": 0
                },
                {
                    "sent": "And compare distribution of these points on these points.",
                    "label": 0
                },
                {
                    "sent": "But if nothing happens or the movie is kind of stationary, then distribution of these two intervals are quite the same.",
                    "label": 0
                },
                {
                    "sent": "The two distributions are the same, but once we have some event, like in this case, it's a tennis movie and actually a ballboy entered in the code and it can be like predicted because the distribution of samples really changes if a boy comes.",
                    "label": 0
                },
                {
                    "sent": "But here we just estimate the divergent between two distributions.",
                    "label": 0
                },
                {
                    "sent": "So the method is really the same.",
                    "label": 0
                },
                {
                    "sent": "A similar technique can also be applied to Twitter data.",
                    "label": 1
                },
                {
                    "sent": "Like we have.",
                    "label": 0
                },
                {
                    "sent": "So, so we have frequency of rod overtime.",
                    "label": 0
                },
                {
                    "sent": "Like if he just gather Twitter data and see how many times this word is pronounced here on there and by looking at the change of the frequency of words we can dig event like in this case it is actually a BP oil spill and we took we preachers chosen several keywords in advance and then just monitor the frequency difference of these words.",
                    "label": 0
                },
                {
                    "sent": "And from that we computed.",
                    "label": 0
                },
                {
                    "sent": "This is actually basically diversions between two distributions.",
                    "label": 0
                },
                {
                    "sent": "I again taken this sliding window way, so then at this first stop at the oil which is the mainland of the United States in the second peak, Obama visited Louisiana or something like that.",
                    "label": 0
                },
                {
                    "sent": "So maybe so this is done afterwards, so maybe we just find try to find an event that happened here and here.",
                    "label": 0
                },
                {
                    "sent": "So maybe it's not a fair evaluation, but it is a kind of fun application.",
                    "label": 0
                },
                {
                    "sent": "You may do some kind of exploratory data analysis.",
                    "label": 0
                },
                {
                    "sent": "OK, that was a.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distribution comparison, so we estimated the divergent between two distributions.",
                    "label": 1
                },
                {
                    "sent": "Any questions about that?",
                    "label": 0
                },
                {
                    "sent": "OK, so try to think about any possible applications in PC or neuroscience.",
                    "label": 0
                },
                {
                    "sent": "So that's the purpose of this vector.",
                    "label": 1
                },
                {
                    "sent": "I don't have any like neuroscience application here.",
                    "label": 0
                },
                {
                    "sent": "OK, the third one is mutual information estimation, so this is technique.",
                    "label": 1
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quite simple, so mutual information is an important quantity in information theory, and it's defined by this form.",
                    "label": 0
                },
                {
                    "sent": "And pxy and log PXY over PXPY.",
                    "label": 0
                },
                {
                    "sent": "And basically this is the clever clever diversions from P. Of XY joint distribution to PXPY the product of marginal.",
                    "label": 0
                },
                {
                    "sent": "And if this mutual information is 0, so this is always nonnegative and this is zero if and only if X&Y are statistically independent.",
                    "label": 1
                },
                {
                    "sent": "Or PX y = P XPY.",
                    "label": 0
                },
                {
                    "sent": "So by looking at the value of new termination, we can know the dependency of two random variables.",
                    "label": 0
                },
                {
                    "sent": "And this is actually quite important to know the dependencies between 2 random variables, and we can again use the clip algorithm to estimate distance D ratio.",
                    "label": 0
                },
                {
                    "sent": "So we just have to slightly modify the code 200 this Jason, but it just straight for one or two lines change.",
                    "label": 0
                },
                {
                    "sent": "So let's just wait how this is useful.",
                    "label": 1
                },
                {
                    "sent": "So already I mentioned that the Kleiber estimation based on density ratio is optimal.",
                    "label": 0
                },
                {
                    "sent": "So then this mutual emission estimator is also optimal.",
                    "label": 0
                },
                {
                    "sent": "Actually, the study of estimating nutrition has long history, information theory.",
                    "label": 0
                },
                {
                    "sent": "People developed many different nonparametric methods and they tried to show consistency of the algorithms.",
                    "label": 0
                },
                {
                    "sent": "But so far none of the methods can really achieve the best convergence rate.",
                    "label": 0
                },
                {
                    "sent": "But distance ratio approach achieved the best convergence rate, so it's a simple approach, but it's the best approach.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when we compare the density ratio method with confidence estimators and the nearest neighbour estimator, another Edgeworth expansion.",
                    "label": 1
                },
                {
                    "sent": "So this is I will mention that later.",
                    "label": 0
                },
                {
                    "sent": "That we used for.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dataset so one so X is one dimension and one is also one dimension, and in this case every UU.",
                    "label": 0
                },
                {
                    "sent": "So it's independent and this is linear dependency between X&Y.",
                    "label": 1
                },
                {
                    "sent": "This has quadratic dependency, so in this case X&Y are dependent, but correlation is 0, so it's not easy to detect by just looking at the correlation.",
                    "label": 0
                },
                {
                    "sent": "And also electric car dependency like it is again creation is 0, so it's not easy to detect the dependency.",
                    "label": 0
                },
                {
                    "sent": "So that means we need to see higher order.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distichs and here's the result.",
                    "label": 0
                },
                {
                    "sent": "The horizontal axis is the number of samples used for estimating middle nation and vertical axis is the approximation error of mutual information.",
                    "label": 1
                },
                {
                    "sent": "MI Hot is an estimated meter.",
                    "label": 0
                },
                {
                    "sent": "Imagine an MRI is the true one 'cause we can compute the true one for this toilet or set.",
                    "label": 0
                },
                {
                    "sent": "And we compared several algorithms and red one is the density ratio based method and overall we can say that the density ratio method method is always one of the best solution for all the datasets.",
                    "label": 0
                },
                {
                    "sent": "And again, Ken.",
                    "label": 0
                },
                {
                    "sent": "Yes, never based method works, sometimes quite well, but it has tuning parameter the number of nearest neighbors, and actually there's no way to again just the number of nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "So once it is chosen optimally then it works very well for all cases.",
                    "label": 0
                },
                {
                    "sent": "But once it is chosen in the wrong way, we have quite cross solution K nearest neighbor with K = 15 works very well in this case, but it doesn't work really well for this case.",
                    "label": 0
                },
                {
                    "sent": "So choice of K is quite important.",
                    "label": 0
                },
                {
                    "sent": "But in our case, of course we also have like Gaussian bit in the similar way, but we can choose that by cross validation.",
                    "label": 0
                },
                {
                    "sent": "So no tuning parameter remains.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, that was mutual information, and theoretically that's quite nice.",
                    "label": 0
                },
                {
                    "sent": "It's a nice paper information theory society, but from the machine learning viewpoint.",
                    "label": 0
                },
                {
                    "sent": "Still mutilation is not so easy to use because as I said before, so it's solution is computed by some of migration problem and we don't really have like explicit expression of the solution.",
                    "label": 0
                },
                {
                    "sent": "So here, so we change the definition of metabolism and we use another information measure called squared loss mitigation, a semi.",
                    "label": 0
                },
                {
                    "sent": "It's just the Pearson diversion spots on the liver.",
                    "label": 1
                },
                {
                    "sent": "There was a version of the mutilation, so it's got some diversions from P. Of XY2P XPY.",
                    "label": 0
                },
                {
                    "sent": "So as I said before, Jason diversions also belongs to the F diversions class, and this is not always non negative and zero if and only if X&Y are independent.",
                    "label": 0
                },
                {
                    "sent": "So basically the same as materialism, but easier to estimate by using the office algorithm so we can have an analytic form approximator.",
                    "label": 0
                },
                {
                    "sent": "And this is actually very useful.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In solving various machine learning tasks, so I just listed several possible applications.",
                    "label": 0
                },
                {
                    "sent": "And you know, so X is input and Y is output.",
                    "label": 1
                },
                {
                    "sent": "And if we consider mutual information between or SMI between input X and output Y, then basically we can do feature selection.",
                    "label": 1
                },
                {
                    "sent": "Like we have, X is a set of genes.",
                    "label": 0
                },
                {
                    "sent": "Why is whether the patient is cancer or not?",
                    "label": 0
                },
                {
                    "sent": "We want to know which Gene is responsible for predicting the sickness of the patient.",
                    "label": 0
                },
                {
                    "sent": "For example, the feature ranking can be done just by looking at the SMI values.",
                    "label": 0
                },
                {
                    "sent": "Or dimension reduction is a kind of feature extraction so we don't choose the subset of features.",
                    "label": 0
                },
                {
                    "sent": "But we can start combination of features, lower dimensional expression of features.",
                    "label": 0
                },
                {
                    "sent": "So I will explain the details in the next slide.",
                    "label": 0
                },
                {
                    "sent": "And also clustering can be done in the same way.",
                    "label": 0
                },
                {
                    "sent": "So in the case of clustering, it's unsupervised learning, so we don't have Y, but only X.",
                    "label": 0
                },
                {
                    "sent": "But we can predict why based on mutual Amazon, it's more like a data compression.",
                    "label": 0
                },
                {
                    "sent": "So given X we want to have one dimensional expression Y that contains the most information of X.",
                    "label": 0
                },
                {
                    "sent": "By doing that, we can like the clustering and also it's important to say that.",
                    "label": 0
                },
                {
                    "sent": "Again, we can do cross validation.",
                    "label": 0
                },
                {
                    "sent": "Many nonlinear clustering algorithms, like spectral clustering is very nice, but it contains like tuning parameters, Gaussian width for example.",
                    "label": 0
                },
                {
                    "sent": "Then by changing the Gaussian with, the solution changes really allowed.",
                    "label": 0
                },
                {
                    "sent": "But based on this density ratio approach we can again tune the calcium with based on cross validation even in the clustering scenario.",
                    "label": 0
                },
                {
                    "sent": "So it's quite quite useful in practice.",
                    "label": 0
                },
                {
                    "sent": "And also if we can start metering SMI between two inputs or multiple inputs, then we can do ICA.",
                    "label": 0
                },
                {
                    "sent": "So given like vector of X.",
                    "label": 0
                },
                {
                    "sent": "So in this case it doesn't have to be 2 elements, but it could be more than two, but we can define SMI in the same way.",
                    "label": 0
                },
                {
                    "sent": "And we just optimize the separation matrix in a way that SMI is minimized to be independent.",
                    "label": 0
                },
                {
                    "sent": "Again, close by this and can be used in ICA, so it's quite useful.",
                    "label": 0
                },
                {
                    "sent": "And also like object method, these are two to specifics.",
                    "label": 0
                },
                {
                    "sent": "I will skip them but we have other interesting applications.",
                    "label": 0
                },
                {
                    "sent": "And finally, so between SMI between input and residual we predict Y from X and consider its residual epsilon.",
                    "label": 1
                },
                {
                    "sent": "So by some modeling assumption, actually by looking at the independence between X and epsilon we can do cause our direction in frame.",
                    "label": 0
                },
                {
                    "sent": "Because our direction means whether X effects YX causes Y or Y causes XX&Y are created to each other.",
                    "label": 1
                },
                {
                    "sent": "But we want to know its direction.",
                    "label": 0
                },
                {
                    "sent": "This is actually quite fundamental question in science and it's very difficult problem, but under some modeling assumption we can prove that it is possible and it can be achieved by looking at the independence between X and it so.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are many applications of SMI in machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here's a small detail of sufficient dimension reduction.",
                    "label": 1
                },
                {
                    "sent": "So it's a supervised dimension reduction method, so we have input X and output by output.",
                    "label": 0
                },
                {
                    "sent": "Why can be anything?",
                    "label": 0
                },
                {
                    "sent": "It can be either real value in the case of regression class label in the classification problem, or a vector of class labels in the case of multi label classification.",
                    "label": 0
                },
                {
                    "sent": "And we want to project Project X onto Z, so this is a lower dimensional expression of X and we can start linear prediction W here.",
                    "label": 0
                },
                {
                    "sent": "So W is assumed to be also going out like this.",
                    "label": 1
                },
                {
                    "sent": "So then the goal of this sentencing reduction is to find a projection matrix W so that this projected vector Z contains all information about Y.",
                    "label": 0
                },
                {
                    "sent": "So this means given ZY&X are conditionally independent, this is what we want to achieve for optimal W. And in terms of SMI, this can be simply expressed as like maximizing SMI over WX&Y.",
                    "label": 1
                },
                {
                    "sent": "And this maximizer achieves this one.",
                    "label": 0
                },
                {
                    "sent": "And already we have a analytic estimator of SMI.",
                    "label": 0
                },
                {
                    "sent": "So then we can just maximize this one with respect to the projection matrix W.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I leave it more detail if SMI approximately.",
                    "label": 0
                },
                {
                    "sent": "It's simply shown in this way analytic form.",
                    "label": 0
                },
                {
                    "sent": "And we may use a natural gradient method or just gradient descent gradient descent.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As in gradient descent or natural gradient, awesome to find the optimal W. Because W belongs to so called a Grassmann manifold.",
                    "label": 1
                },
                {
                    "sent": "It's all except of also gonna matrices if we can exploit this structure, we have a better gradient algorithm.",
                    "label": 0
                },
                {
                    "sent": "And also we have some better heuristic to update W matrix and it can scale quite well.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here's a simple experiment for multi label data multi label classification problem like it's a like in the Left Hand case.",
                    "label": 0
                },
                {
                    "sent": "It's a image classification problem.",
                    "label": 1
                },
                {
                    "sent": "So given that image we want to know whether a person exists or not.",
                    "label": 0
                },
                {
                    "sent": "So this is the first label or a dog exists or not.",
                    "label": 0
                },
                {
                    "sent": "So this second level account exist or not Sky you could just turn up.",
                    "label": 0
                },
                {
                    "sent": "We had around I think 200.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So in this case around like 20 or 30 labels, and for each image we have like 20 dimensional label vector that contains 0 and Y.",
                    "label": 0
                },
                {
                    "sent": "Either this object exists or not.",
                    "label": 0
                },
                {
                    "sent": "This is a multi label classification problem.",
                    "label": 0
                },
                {
                    "sent": "And for this multi label classification problem we we did dimension reduction and the original dimension was quite quite high.",
                    "label": 0
                },
                {
                    "sent": "But now it's reduced to 20 dimensions or like 60 dimension 140 dimensions like that.",
                    "label": 0
                },
                {
                    "sent": "The classification accuracy misclassification rate.",
                    "label": 1
                },
                {
                    "sent": "Of no dimension reduction is somewhere here.",
                    "label": 0
                },
                {
                    "sent": "But the dimension reduction based on density ratio goes lower than this one, so it improves the performance by reducing the time zone.",
                    "label": 0
                },
                {
                    "sent": "So we can simply get rid of the noise components.",
                    "label": 0
                },
                {
                    "sent": "But with other methods.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately it doesn't go beyond the nordmende reduction baseline.",
                    "label": 0
                },
                {
                    "sent": "So performance is not bad, but method works much better than this method.",
                    "label": 0
                },
                {
                    "sent": "But we are really investigating the conditional independence, so this is what we really wanted.",
                    "label": 1
                },
                {
                    "sent": "The independent reduction, but other methods are kind of heuristic to find reasonable projection matrix, so maybe that's the difference why we had better performance here.",
                    "label": 0
                },
                {
                    "sent": "And the similar thing goes for like audio tagging problem.",
                    "label": 0
                },
                {
                    "sent": "So it's again attacking problems given the audio sound, we want to know whether it's a broad exist or some engine sound exists or something like that.",
                    "label": 0
                },
                {
                    "sent": "So we have in this case 200 diamonds.",
                    "label": 0
                },
                {
                    "sent": "Now label vector can be obtained quite similar results likely.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so that was a mutual information, but it was a bit longer, but estimation of mutual information is really.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's a very good point.",
                    "label": 0
                },
                {
                    "sent": "So in principle, of course we can use mutual information, but in this case we want to like.",
                    "label": 0
                },
                {
                    "sent": "This also holds for MI.",
                    "label": 0
                },
                {
                    "sent": "But the problem is in my approximator is not analytic.",
                    "label": 0
                },
                {
                    "sent": "So then we.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can't really compute its derivative with respect to W. So then there's no way to optimize W in a competition recently, if you play with the equation, you may somehow update W, But it's not straightforward if it's lower.",
                    "label": 0
                },
                {
                    "sent": "So from the computational viewpoint, using SMI is not much more convenient in this case.",
                    "label": 0
                },
                {
                    "sent": "Any other questions for SMI is quite useful in solving many machine learning tasks and it's great.",
                    "label": 0
                },
                {
                    "sent": "There will be some cost in the afternoon.",
                    "label": 0
                },
                {
                    "sent": "Applications were made actually useful information that you would like to defective.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a very good point.",
                    "label": 0
                },
                {
                    "sent": "So OK SMI and am I the same independent so like here dependences actually no direction or directions are dependent and in the case of SMI, maybe we prefer to have this dependency and in the case of them why we have this dependency and like maximizing MI and maximizing SMI may have different solutions.",
                    "label": 0
                },
                {
                    "sent": "You are right and it's not so far we didn't have any characterization for this part.",
                    "label": 0
                },
                {
                    "sent": "So we can't really say anything, but at least we can say that we are maximizing some kind of dependency, so error metric is slightly different in a log case and squared loss case, but roughly, so we are anyway maximizing dependency, and mathematically it's no problem.",
                    "label": 0
                },
                {
                    "sent": "But in practice we may have different solutions and we can't really say whether one is better than the other.",
                    "label": 0
                },
                {
                    "sent": "It depends on the situation perhaps.",
                    "label": 0
                },
                {
                    "sent": "It's something like a choice of loss function.",
                    "label": 0
                },
                {
                    "sent": "We can't simply say one is always better than the other.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, another practical advantage of using SMI is that it's more robust against outliers.",
                    "label": 0
                },
                {
                    "sent": "Because the clever clever diversions contains log function, and it's really bad if you have a single outlier.",
                    "label": 0
                },
                {
                    "sent": "But in the case of SMI we are using the squared loss and you may think squared losses.",
                    "label": 0
                },
                {
                    "sent": "Sensitive to outliers in the case of regulation, yes.",
                    "label": 0
                },
                {
                    "sent": "But in the case of like density ratio estimation or density estimation at this squared loss, it's quite robust against outliers.",
                    "label": 0
                },
                {
                    "sent": "Like not spit out on the log function, so that's another practical advantage of using a semi.",
                    "label": 0
                },
                {
                    "sent": "But this is like the pros and cons, so it is robust against outliers.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand this means for outlier detection, maybe using critical libraries data 'cause SMI is robust against outliers.",
                    "label": 0
                },
                {
                    "sent": "So then the outlier score is not sensitive to outlier.",
                    "label": 0
                },
                {
                    "sent": "So in the case of authorized techs and perhaps we should use clever clever diversions.",
                    "label": 0
                },
                {
                    "sent": "But in other cases maybe SMI Pearson diversions would be more useful.",
                    "label": 0
                },
                {
                    "sent": "OK, any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK then let's go to the 4th part conditional.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probability estimation this is quite simple unsolved.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or a conditional probability P of Y given X is by definition given by P of XY over PX.",
                    "label": 0
                },
                {
                    "sent": "That's it, it's density ratio.",
                    "label": 0
                },
                {
                    "sent": "So we can directly estimate it without really changing the program.",
                    "label": 0
                },
                {
                    "sent": "And so for now, let's consider the situation where X&Y are both continuous.",
                    "label": 0
                },
                {
                    "sent": "So then it's called conditional density estimation.",
                    "label": 1
                },
                {
                    "sent": "Like we have X here and why here and we have blue points like this.",
                    "label": 1
                },
                {
                    "sent": "And conditional density is something like this red line.",
                    "label": 1
                },
                {
                    "sent": "Given X, we want to know the density of life, so this is a kind of data visualization problem.",
                    "label": 0
                },
                {
                    "sent": "And regulation is rated problem, but it estimates conditional mean the given X.",
                    "label": 0
                },
                {
                    "sent": "We want to know if meaning of life.",
                    "label": 0
                },
                {
                    "sent": "Then so this green line is a regulation covering this problem.",
                    "label": 0
                },
                {
                    "sent": "And for a data set like this, like having two modes in this case, then actually the regression graph goes between two mountains from the viewpoint of prediction.",
                    "label": 1
                },
                {
                    "sent": "Maybe this is fine, but from the viewpoint of data visualization interpretation, maybe regression is not so useful in this case.",
                    "label": 0
                },
                {
                    "sent": "Another system would be in addition to the multi modality.",
                    "label": 0
                },
                {
                    "sent": "We may have a symmetric noise or heteroscedastic noise, so noise variance is dependent on the position X.",
                    "label": 0
                },
                {
                    "sent": "So in that kind of situation, we may want to know the conditional density itself.",
                    "label": 0
                },
                {
                    "sent": "But I must agree, I must say that conditional density estimation is a difficult problem.",
                    "label": 0
                },
                {
                    "sent": "It's against estimation problem even harder problem.",
                    "label": 0
                },
                {
                    "sent": "So it's not maybe possible to.",
                    "label": 0
                },
                {
                    "sent": "We can't really have a good solution if the dimension is larger than five or something, but as long as the dimension is less than five or four, maybe directly estimating the conditional density is sometimes useful.",
                    "label": 0
                },
                {
                    "sent": "And here we can just use our density ratio estimator and directly obtained conditional density estimator.",
                    "label": 0
                },
                {
                    "sent": "Is it clear?",
                    "label": 0
                },
                {
                    "sent": "So technically it's nothing new, but this is all.",
                    "label": 0
                },
                {
                    "sent": "This condition of this destination.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the same way?",
                    "label": 0
                },
                {
                    "sent": "Or OK on the experiment here.",
                    "label": 0
                },
                {
                    "sent": "So it's a robot transition probability estimation, so here's a simple toy robot and it at their size of this side, and it has two values, and once two wheels are moved forward, it moves forward and backward, backward and in a different way it can rotate.",
                    "label": 0
                },
                {
                    "sent": "And also the robot has two infrared sensors and by that it can measure the distance to the nearest obstacle.",
                    "label": 0
                },
                {
                    "sent": "So V is the velocity of two wheels and that's an action that we can recommend to the robot.",
                    "label": 0
                },
                {
                    "sent": "And the value of the infrared sensors is called state.",
                    "label": 1
                },
                {
                    "sent": "And by that we can know the surrounding environment of the robot.",
                    "label": 1
                },
                {
                    "sent": "The task of transition probability estimation is to estimate P of X plane given SA.",
                    "label": 0
                },
                {
                    "sent": "So at the current position, so we give a command to the robot.",
                    "label": 0
                },
                {
                    "sent": "OK, like it goes forward, then it moves and we want to know in what kind of space on the robot will exist in the next step.",
                    "label": 0
                },
                {
                    "sent": "It's S prime.",
                    "label": 0
                },
                {
                    "sent": "And this is actually quite multimodal, cause like if the robot is somewhere here, an infrared sensor detects this corner, but once the robot turns a little bit then we can't find this corner anymore.",
                    "label": 0
                },
                {
                    "sent": "So then the infrared value sensor value changes a lot just by slightly rotating the robot.",
                    "label": 0
                },
                {
                    "sent": "So it's not a simple unimodal problem, but it can be a highly complex multimodal problem.",
                    "label": 0
                },
                {
                    "sent": "So we have like density restoration method and from KDE Extended method and also neural network method and the performance.",
                    "label": 0
                },
                {
                    "sent": "So this is accuracy.",
                    "label": 0
                },
                {
                    "sent": "So this is smaller is better error.",
                    "label": 1
                },
                {
                    "sent": "This is an error.",
                    "label": 0
                },
                {
                    "sent": "Small writes better density ratio based method and neural network method both work equally well.",
                    "label": 0
                },
                {
                    "sent": "But the competition time is like 1000 times faster, in this case.",
                    "label": 0
                },
                {
                    "sent": "Of course, you know neural network training is quite slow, but in the case of the integration estimation, we just have to solve system of linear equation on the ones and solution is analytic.",
                    "label": 0
                },
                {
                    "sent": "No local Optima.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But there was a conditional density estimation and.",
                    "label": 0
                },
                {
                    "sent": "Just change the definition of Y.",
                    "label": 0
                },
                {
                    "sent": "So in the previous case, why was a continuous variable like state vector?",
                    "label": 0
                },
                {
                    "sent": "But let's assume that Y is a categorical value classification problem.",
                    "label": 0
                },
                {
                    "sent": "So then actually estimating this P of Y given X is actually estimating the class conditional probability the same as logistic regression.",
                    "label": 0
                },
                {
                    "sent": "So I'm saying that so these squares density racist matter provides a clear analytics analytic estimator of this class conditional probability.",
                    "label": 1
                },
                {
                    "sent": "So this means it can be a nice alternative to kernel logistical regression.",
                    "label": 1
                },
                {
                    "sent": "So I know Carnivalistic listen is quite useful method and it is used widely, but its training is quite time consuming.",
                    "label": 0
                },
                {
                    "sent": "Because it has a log function and it doesn't have a nice property to play with, so we just have to use a simple cross Newton method or Newton method or something like that to obtain the solution.",
                    "label": 0
                },
                {
                    "sent": "And recently study of Cross Newton is really advanced and we have a very nice implementation nowadays, but still it's very slow so it doesn't scale well.",
                    "label": 0
                },
                {
                    "sent": "But if we use us, if we just have to solve a linear equation.",
                    "label": 0
                },
                {
                    "sent": "Then we can obtain this, get the same value as logistically lesson.",
                    "label": 0
                },
                {
                    "sent": "Of course it's not the same method, but it uses different loss function, either log loss or squared loss.",
                    "label": 1
                },
                {
                    "sent": "But maybe the performance is comparable, but the computational complexity is much much lower.",
                    "label": 1
                },
                {
                    "sent": "And in particular, no normalization term is included in our safe computation of the normalization time is quite difficult, quite computationally.",
                    "label": 0
                },
                {
                    "sent": "Heavy in logistic regression, but we don't have that in our system.",
                    "label": 0
                },
                {
                    "sent": "And also classified training is actually possible.",
                    "label": 0
                },
                {
                    "sent": "So this is actually a big advantage, like if you have like 1000 classes like you have many samples from super Mini classes then Colonel stipulation needs to be trained with all samples.",
                    "label": 0
                },
                {
                    "sent": "But in the case of this assist method, we just need to train a classifier for each class.",
                    "label": 0
                },
                {
                    "sent": "So this is actually computationally much much more efficient.",
                    "label": 0
                },
                {
                    "sent": "Undersea",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uncle here.",
                    "label": 0
                },
                {
                    "sent": "So it's just a toy benchmark data set.",
                    "label": 0
                },
                {
                    "sent": "Let our data set.",
                    "label": 0
                },
                {
                    "sent": "It contains 26 alphabets.",
                    "label": 0
                },
                {
                    "sent": "Horizontal axis is the number of samples and vertical axis is a misclassification error.",
                    "label": 0
                },
                {
                    "sent": "The Red one is the density ratio based method and green ones carnivalistic relation.",
                    "label": 0
                },
                {
                    "sent": "So in this case carnivalistic regression is slightly better.",
                    "label": 0
                },
                {
                    "sent": "Maybe they are similar.",
                    "label": 0
                },
                {
                    "sent": "But if you see the training time as a function of the number of samples, so it's actually log scale.",
                    "label": 0
                },
                {
                    "sent": "So 10 to minus two here for 1:50 here.",
                    "label": 0
                },
                {
                    "sent": "10,000 times faster.",
                    "label": 0
                },
                {
                    "sent": "But we don't need any training procedure, but just we need to solve a system of linear equations super fast.",
                    "label": 0
                },
                {
                    "sent": "And more experiments.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unlike image and sound data set and they are not so large scale.",
                    "label": 0
                },
                {
                    "sent": "So again, in computation time is not so large, but still the density ratio method is much faster than the performance is compatible.",
                    "label": 0
                },
                {
                    "sent": "Or in this case actually slightly better.",
                    "label": 0
                },
                {
                    "sent": "We don't really argue that distance ratio method works better than eternal separation, but they should be at least comparable.",
                    "label": 0
                },
                {
                    "sent": "But computationally it's much more efficient.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and we have more applications in action recognition from accelerometer.",
                    "label": 1
                },
                {
                    "sent": "If you have iPhone so it has accelerometer and from this information we want to know whether a person is walking or like going up the stairs or taking elevators or something like that and there's actually a serious medical application behind 'cause we want to know what patients patients are really doing in their daily life.",
                    "label": 0
                },
                {
                    "sent": "So they come to the doctor, come to see the doctor once a month and they say.",
                    "label": 0
                },
                {
                    "sent": "I'm always using the stair and I don't use any elevator but it is recorded so we can identify that.",
                    "label": 0
                },
                {
                    "sent": "And that's actually quite important in diabetic diagnosis, so we have many diabetes patients in Japan and this is a big problem.",
                    "label": 0
                },
                {
                    "sent": "And also again, it's prediction from this image is so we have face images from many different tribes.",
                    "label": 0
                },
                {
                    "sent": "Then the number of classes are now increasing and it's nice to have a computationally efficient approach.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that was the conditional probability estimation part.",
                    "label": 0
                },
                {
                    "sent": "Do you have any questions?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I thought OK, OK, so I originally I was not really planning to go to this part and so we have more like development, recent development on density ratio estimation and I just.",
                    "label": 1
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One idea that so I'm saying that then seriously estimation is better than a separate density estimation, and maybe this is true in many cases.",
                    "label": 1
                },
                {
                    "sent": "But still I must admit that if the dimension is high density ratio estimation, it's still difficult.",
                    "label": 0
                },
                {
                    "sent": "So then it's not.",
                    "label": 0
                },
                {
                    "sent": "It's natural to combine the installation with dimension reduction.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The point is that we have to density.",
                    "label": 0
                },
                {
                    "sent": "So if you play with a single density, it's not easy to the dimension reduction, but we have two densities then, so we can actually try to find a common part or different part.",
                    "label": 0
                },
                {
                    "sent": "So in our case we try to find a different part.",
                    "label": 0
                },
                {
                    "sent": "Given two densities, we want to find, so we assume that they are different only in some subspace that we call Hitler distribution subspace.",
                    "label": 0
                },
                {
                    "sent": "Like in the simplest 2D example 2 dimensional example.",
                    "label": 0
                },
                {
                    "sent": "So we have like red points from denominator and blue points from numerator and if we see this direction then written blue has the same distribution.",
                    "label": 0
                },
                {
                    "sent": "But if you see this direction, red and blue has different distribution.",
                    "label": 0
                },
                {
                    "sent": "And suppose different direction is U and same direction is V. So then this ratio can be without any assumption.",
                    "label": 0
                },
                {
                    "sent": "We can decompose it in this way.",
                    "label": 0
                },
                {
                    "sent": "And if we pop is the same, we can cancel this part and in the end we only have density ratio over you.",
                    "label": 0
                },
                {
                    "sent": "And suppose X is 10.",
                    "label": 0
                },
                {
                    "sent": "So don't dimensions, but maybe they only share are small.",
                    "label": 0
                },
                {
                    "sent": "They are different only in small dimensional subspace and this kind of thing often happens in outlier detection problem.",
                    "label": 0
                },
                {
                    "sent": "For example, the original data is quite high, high dimensional but change is not, it's confined in some lower dimensional subspace.",
                    "label": 0
                },
                {
                    "sent": "Then basically we need to have to estimate the density ratio only in this subset.",
                    "label": 1
                },
                {
                    "sent": "And then the question is how to find this sort of space and we have several algorithms, but I need to just keep it.",
                    "label": 0
                },
                {
                    "sent": "We have some procedure already to find it a redistribution subspace.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Well done so conclusion.",
                    "label": 0
                },
                {
                    "sent": "So the beginning of this work was for estimating data generating probability distribution is the universal approach.",
                    "label": 0
                },
                {
                    "sent": "So this is the most general approach that we want to solve.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, if it's not so great without having good prior prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, solving each task like support vector machine for panic when is an ideal approach, but it's not.",
                    "label": 1
                },
                {
                    "sent": "It's costly for solving many different tasks.",
                    "label": 0
                },
                {
                    "sent": "So density ratio estimation is a kind of realistic compromise between these two approaches.",
                    "label": 1
                },
                {
                    "sent": "And I don't say that density ratio covers all interesting tasks, but still this set contains many interesting tasks that we want to solve in practice.",
                    "label": 1
                },
                {
                    "sent": "And it can then stress test mission can systematically avoid density estimation and and also it was shown to be useful in many real world problems.",
                    "label": 0
                },
                {
                    "sent": "But maybe it's nice to explore more PCI directions in the near future.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we have already published a book on density ratios.",
                    "label": 1
                },
                {
                    "sent": "This is just published a couple months ago.",
                    "label": 0
                },
                {
                    "sent": "And also for the covariate shift adaptation part, we have another book that was published from MIT Press.",
                    "label": 0
                },
                {
                    "sent": "So if you have some interest, you may take a look at these books.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And also like we have OK many colleagues to this project and we have funding as it says here and all the papers articles on software of density restaurant mission is available from from this web page.",
                    "label": 1
                },
                {
                    "sent": "Software is written in Matlab and some are on someone else, but mostly it's not love.",
                    "label": 0
                },
                {
                    "sent": "So there I think ready to use on some industry.",
                    "label": 0
                },
                {
                    "sent": "People are like this testing from over the algorithm.",
                    "label": 1
                },
                {
                    "sent": "So if you have nice data to analyze then just try some of the density ratio estimation methods and if you let me know the result I'm quite happy.",
                    "label": 0
                },
                {
                    "sent": "OK, that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}