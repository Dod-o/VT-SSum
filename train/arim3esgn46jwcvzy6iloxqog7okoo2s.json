{
    "id": "arim3esgn46jwcvzy6iloxqog7okoo2s",
    "title": "An HDP-HMM for Systems with State Persistence",
    "info": {
        "author": [
            "Emily Fox, Department of Statistics, University of Washington"
        ],
        "published": "Aug. 29, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_fox_ahh/",
    "segmentation": [
        [
            "OK for many HMM applications there's this sense of temporal state persistence.",
            "That is, the probability of a self transition is typically higher than the probability of a transition to another state."
        ],
        [
            "This does not work.",
            "So take for example the problem of speaker diarization, where we're trying to parse an audio file into labels of who is speaking one.",
            "Let's start by assuming that we know there are K people speaking an.",
            "If we use an HMM to model this process, the Markov structure on the state sequence dictates that the distribution over who's speaking at the next time step solely depends on who is currently speaking.",
            "So for this application there's a high probability of self transition, because if you're currently speaking.",
            "You're likely to continue speaking another key aspect of this application is the fact that this speaker specific emission distributions are very complex multimodal distributions.",
            "Does anybody know if there's a clicker?",
            "OK.",
            "So often, however, 1 cannot assume that there are K people speaking and would like to be able to infer that from the data or for or to allow new speakers to appear with time.",
            "There currently exists nonparametric methods for learning Hmm's with an unknown number of states, or in this case, an unknown number of speakers.",
            "However, what will show during this talk is that they have a hard time dealing with processes that have this high probability of self transition or these multi modal emissions."
        ],
        [
            "So maneuvering target tracking is another application where there's an unknown number of states that persist over continuous periods of time.",
            "Here, and you know, capturing this temporal state persistence is really key in performing inference here.",
            "We modeled the maneuvering target as some dynamical system.",
            "That's driven by this exogenous input, which we take to be the output of an HMM.",
            "And here each hmm state is a different maneuver mode for the target."
        ],
        [
            "So we mentioned that there currently exists nonparametric methods for learning Hmm's, specifically, the hierarchical tier sleep process can be used as a prior on Hmm's with unknown state space.",
            "These models are referred to as the Infinite HMM or the HTP HMM and were studied by Bielen tea at all.",
            "So the HTP, HMM, has a number of nice properties.",
            "First, it encourages the use of a sparse subset of this unbounded state space because it reinforces transitions that you've already seen in the data.",
            "Another thing is that it allows for new states to be generated as you get more observations, and so thus the model complexity complexity adapts to that of the observations.",
            "However, what will show is that the HTP, hmm, inanimately captures this temporal state persistence.",
            "And.",
            "Here's a set of observations generated from a three state hmm.",
            "And here are those observations, colored by the true state sequence.",
            "We now show a plot of the same observations colored by the inferred state sequence from the HTP HMM after 1000 iterations of Gibbs sampling and what you see is that it's taken one true state and in Ferd two states which it then rapidly switches between."
        ],
        [
            "OK, so during this talk will first just run through a very quick overview of Hmm's.",
            "In order to solidify some notation and then we'll discuss this HTP HMM formulation and how we augment the model with a learned bias towards self transitions.",
            "We call this is sticky HTP.",
            "Hmm, and once we've developed this model, we can start looking at capturing multimodal emission distributions.",
            "We conclude the talk by presenting results on the NIST Speaker Diarization database and show that our performance is comparable to that of the current state of the art."
        ],
        [
            "OK, so and hmm is just a class of doubly stochastic processes consisting of an underlying discrete state sequence represented by the C random variables, and it has this Markov structure with transition distributions PIE.",
            "Condition on the state sequence.",
            "You just have a set of conditionally independent observations Y from emission distributions parameterized by Theta.",
            "This box notation here just simply means to replicate that portion of the graph K times.",
            "So one can view sample path of the state sequence as a walk through this state versus time lattice here."
        ],
        [
            "Let's assume that.",
            "We start in state two and we have the following possible transitions from time step, one to time Step 2.",
            "And here these arrows are shaded by their respective probabilities which are captured in the transition distribution for that state."
        ],
        [
            "Let's then assume we transition to state one and have the following set of possible transitions."
        ],
        [
            "And so on."
        ],
        [
            "What the HCP hmm allows is for an unbounded state space.",
            "That is, we let K go to Infinity.",
            "And so now we have these countably infinite transition distributions.",
            "So specifically, the richley process portion of the HTP is what allows for this unbounded state space, and also encourages using a sparse subset of this infinite state space.",
            "The hierarchical layering of these Dirichlet processes ties together each of these state specific transition distributions so that they share this sparse subset of possible states visited.",
            "So just a little bit more formally, we start with an average transition distribution beta, which is drawn according to the stick breaking construction, which goes as follows.",
            "We start with a stick of unit probability mass."
        ],
        [
            "And we draw a number from a beta one gamma distribution and we break off that portion of the stick and define it as the first mixture.",
            "Wait.",
            "We then draw another number from a beta distribution break off that portion of the remainder of the stick and define it as the second mixture, wait.",
            "And we keep doing this infinitely many times.",
            "OK, so now that we've defined this average tree."
        ],
        [
            "Addition distribution.",
            "We can use it to define a set of state specific transition distributions.",
            "Each of which is distributed according to Adir Ishly process with beta as the base measure.",
            "And because of this, the expected weights for each one of these transition distributions is equivalent to beta, regardless of which state we're in.",
            "And we can also see that the sparsity of beta is the shared between the state specific transition distributions."
        ],
        [
            "But let's return to this example from those three states.",
            "Hmm, here we use slightly different way to show the true state labels and the estimated state labels with errors shown in red.",
            "So there we see that very rapid switching between two redundant states.",
            "And what we see is that the Dirichlet processes bias towards simpler models is really inadequate for preventing this very unrealistic rapid switching between the states.",
            "And there are a few things to note about this issue.",
            "The first is that your predictive performance can actually be hurt because you're assigning fewer observations to each state from which to infer parameters that are used for prediction.",
            "In addition, in applications like the speaker diarization problem, you actually care about the learn segmentation."
        ],
        [
            "So in order to address this issue, what we do is we add this parameter Kappa, which allows for a bias towards self transitions.",
            "We call this is sticky parameter.",
            "Our average transition distribution is defined as before, but now each of the state specific distributions has an added weight on the base measure in the component corresponding to self transition.",
            "So if we look at the expected weights of each of the transition distributions, we see it's a convex combination of the global set of weights from the average transition distribution plus a set of state specific weights.",
            "And we can qualitatively compare what these distributions look like to their original model, and we see that there's this bias towards self transitions now."
        ],
        [
            "OK, so now that we've developed the model, we can start talking about inference.",
            "So the basic inference algorithm for the HTP HMM, is the direct assignment Gibbs sampler.",
            "Which marginalises over the infinite set of transition distributions and parameters and sequentially samples the state condition on all other state assignments.",
            "OK, so let's just let's say that the previous state took a value J and the next state took a value L. Well, the prior probability of assigning the current state to value K is a variant of the Chinese restaurant prior, which determines how many Jada K&K to L transitions we've already seen.",
            "And then that's weighted by a likelihood term based on all other observations currently assigned to that state.",
            "Although rather straightforward, the sampler suffers from extremely slow mixing rates.",
            "Be close because global assignment changes to the entire state sequence or forced to occur component by component.",
            "And this problem is exacerbated when we add this sticky parameter, because let's assume we have the following true state sequence.",
            "If the sampler splits one state into two, the self transition bias reinforces that assignment and it's very hard to then merge those states back together."
        ],
        [
            "So far, our sampler has not taken advantage of the fact that we actually have this very nice Markov chain structure to our graph.",
            "Um?",
            "An there exists very efficient algorithms for Hmm's using dynamic programming techniques like the forward backward algorithm which can be used to jointly sample the entire state sequence conditioned on a set of observations.",
            "However, this algorithm relies on actually instantiating the set of transition distributions and parameters so they can no longer be integrated out.",
            "However, because we're using an HTP HMM, these distributions are countably infinite, so we need to approximate them.",
            "Using some finite approximation.",
            "And what we choose to use here?",
            "There are many different options.",
            "You'll see a talk on this later.",
            "Is the week limit approximation, which simply places a symmetric Dirichlet distribution prior on our average transition distribution, which then induces a finite dearsley distribution on each of the state specific transition distributions.",
            "We then iterate between sampling the transition distributions and parameters.",
            "And the entire state sequence.",
            "And to sample the entire state sequence, we simply pass messages backwards and then sequentially sample sample forwards."
        ],
        [
            "In addition to sampling the transition distributions, parameters and state sequence, we also place weakly informative priors on our hyperparameters and learn these from the data as well.",
            "And for all the experiments, we're going to show, we use the same settings for these hyper priors.",
            "So, just to reiterate, we actually place a vague prior on this bias towards self transition and learn it from the data.",
            "So a related self transition parameter was presented in the infinite HMM paper by bill at all, but they didn't develop a globally consistent generative model, which meant that they relied on some heuristics in their Gibbs sampling."
        ],
        [
            "OK, so let's return again to this data set and look at some of the performance of the sticky HTP hmm versus the original model.",
            "First, we're going to show using the block sampler, and So what we do is we run hundred initializations of the sampler for 1000 Gibbs iterations, and that each iteration we compute the Hamming distance between the true state sequence and the estimated state sequence, and here we plot in read the 10th.",
            "And 90th quantile.",
            "And here the median performance.",
            "And so we see.",
            "An improvement both in mixing rate and overall performance by going to the sticking model.",
            "When we look at the sequential sampler, we see that much lower mixing rate, especially in the case when we have that sticky parameter."
        ],
        [
            "We then tested the model on data that actually had very fast state switching.",
            "Here's an observation sequence and the observations colored by the true state sequence, and we see this very rapid switching between four different States and what we see is that the sticky HTP HMM is still able to learn these fast dynamics, and in this case the performance is comparable to that of the original HTP, HMM?"
        ],
        [
            "OK, so now that we've developed this sticky HTP.",
            "Hmm, we can start looking at learning multimodal emission distributions."
        ],
        [
            "Because for a lot of HMM applications like the speaker diarization problem, the state specific emission distributions are multimodal and can't be well captured by a single parametric distribution.",
            "Instead, one can approximate these distributions as a mixture of Gaussians and here we show an observation sequence from a two state hmm with these mixture of Gaussian emissions and if we pass this data to either the sticky or non sticky HTP, HMM.",
            "When it's constrained to single Gaussian admissions.",
            "What it typically infers is it assigns a new state for each one of the mixture components and then rapidly switches between them."
        ],
        [
            "However, now that we've modeled this bias towards self transition, we can further augment the model to Start learning these multi modal missions and to do this.",
            "We approximate these multi modal missions as an infinite Gaussian mixture.",
            "And we place a dearsley process prior.",
            "So now every state has its own stick breaking density over the mixture weights of the Gaussians to finding that states emission distribution.",
            "And the generative model then says we choose a state dependent upon the previous state as before.",
            "Then, given that state we choose a mixture component from its stick breaking density.",
            "And then.",
            "Given that mixture component, you generate an observation.",
            "So it allows us to.",
            "Discriminate between the underlying hmm states is both this nice Markov structure on the state sequence as well as the bias towards self transitions, 'cause if the model was free to both rapidly switch between hmm States and associate multiple Gaussians per state, there would be quite a bit of posterior uncertainty."
        ],
        [
            "OK, so let's return to this observation sequence from a two state hmm with multimodal emissions.",
            "If we look at the sticky HTP HMM and the nonsticky model, both with this dearsley process admissions, this infinite mixture of Gaussian admissions, we see that the nonsticky model places very little weight on good segmentations, and that's because it's choosing between creating more hmm states, each with fewer mixture components or fewer hmm states, each with more mixture components, and it's very hard to discriminate between those two things.",
            "When either of the models is constrained to single Gaussians, of course we get much worse performance."
        ],
        [
            "OK, so now we present some results on this NIST speaker Diarization database which consists of 21 recorded meetings with ground truth labels.",
            "So this is run this competition for the past six years and.",
            "Using this database of meetings and the Berkeley ICSI team has won by a large margin, and here, just to reiterate, the goal is both in for the number of speakers and labels of who is speaking when.",
            "So the ICSI teams algorithm is this agglomerative clustering approach that starts with a large number of states, large number of speakers and then slowly merges them when certain criteria are met.",
            "Um?",
            "So this algorithm is like very highly engineered to this task, and it's been developed over many years by a large team of researchers."
        ],
        [
            "OK, so in order to compare our performance to the ICSI teams, we used the same set of features, specifically the 1st 19 dimensions of the Mel frequency capsule coefficients.",
            "And here we show a movie.",
            "Of what these features look like, colored by the true state speaker and it's a movie over every dimension of that feature vector.",
            "And what you see is that the features look very similar between speakers, which makes it a very challenging problem.",
            "And in addition along some components of the feature vector, a speaker will look different overtime, which leaves the possibility of inferring multiple speakers from a given true speaker.",
            "And just to note, although there are true speaker labels available here, we don't use any training data that we do this fully unsupervised, just passed the raw observations, whereas the ICSI teams algorithm relies on a lot of training data."
        ],
        [
            "OK, so.",
            "We compare the performance of the sticky HTP HMM with these seriously process admissions to the nonsticky model, and we use the standard NIST speaker diarization error rate metric, which is just basically a calculation of the percentage of wrong speaker labels and what we do is we run 10 initializations of the Gibbs sampler for 1000 iterations for each of the 21 meetings, and then at the thousandth iteration we choose the initialization that has the highest likelihood of the observations given the inferred parameters.",
            "So we plot those.",
            "21 most likely points here and what we see is that since a majority of those points lie significantly above this diagonal line, the sticky model dominates in performance.",
            "We can then compare the performance to that of the ICSI team.",
            "We see that overall, our performance is pretty comprable and in addition, if we look at our best and worst performance over the 21 meeting database, our performance is slightly better than their best and worst performance.",
            "In addition, one advantage of art model is the fact that we get multiple speaker segmentations because the Gibbs sampler gives us a sense of this posterior uncertainty, whereas the collaborative clustering approach just outputs one speaker label.",
            "What one speaker label sequence?",
            "So if you look at the top five, most likely initializations are performance drops to 15% Der.",
            "We can then look at plotting the estimated number of speakers versus the true number of speakers for each of the meetings.",
            "Each of the initializations that we ran.",
            "And here we scale each one of these points by their respective likelihoods, and we see since there's this strong correlation with the diagonal, it shows our model's ability to learn very number of speakers."
        ],
        [
            "Just to quickly show some qualitative performance, here's one meeting where we did incredibly well here.",
            "The true speaker labels.",
            "Here's the first set of speaker labels for the most likely sequence, and.",
            "We show errors in red.",
            "Kind of hard to see here."
        ],
        [
            "But if you look at a meeting where we did much worse, you see that it's taken one true speaker and split into two inferred speakers.",
            "But if you look at a different initialization, not the most likely one, we have a much better segmentation, much lower Der, and that it's important to note that this likelihood metric that we used is very biased towards segmentations that infer more states that are more finely tuned to the observations, but."
        ],
        [
            "Once again, we can look at multiple possible segmentations.",
            "So in conclusion, we looked at some of the limitations of the original HTP HMM formulation and showed how we can augment the model to have this learned bias towards self transitions, which then allows us to look at multi modal missions.",
            "In terms of computational costs, the additional cost of sampling the sticky parameter is really negligible compared to the other costs of the Gibbs sampler, and in addition, in practice it often runs much faster because it will typically instantiate fewer state components.",
            "We've also shown that the sticky HTP HMM is able to learn a wide range of dynamics even when the state persistence is not present in the data.",
            "So in conclusion, is a very simple and effective addition to the model.",
            "Thank you.",
            "So you said the state of the art system you are comparing with this 60 system was using supervised segmented data.",
            "Have you thought about how you could make your things semi supervised and use some of that segmented data and hopefully even outperforming yeah?",
            "I mean it is possible, at least in setting those hyper priors to use some of.",
            "The other like they use a development set and then test on an evaluation set which changes every year and it would be possible to use that development set to either fix those hyperparameters or better adjust the priors that we place on them so that they're not like weakly informative.",
            "We haven't really tried doing that yet though, but you know, it's something that we're thinking about looking into.",
            "So it was more just a verification of this model rather than trying to implement a new speaker diarization application, but something interesting to look into.",
            "Question.",
            "Just one more.",
            "So I'm just wondering, you know, in a sense the this new hyperparameter is telling us the probability of some transition, right?",
            "And so now what's the the shared prior learning?",
            "I mean, it's like the probability of making transitions.",
            "You mean the prior on this self transition parameter the whole?",
            "Oh, that's the average transition distribution, right?",
            "I mean, it gives you it still gives you a sense of the possible states that you're visiting it.",
            "There's still a sparse subset of states visited, and it does give you the sense of transitions between states, but.",
            "It's basically that's shifted down just slightly with an added weight on a self transition for each state.",
            "But what that still gives you is that sparsity.",
            "That shared between states, you know, maybe states one and two are very likely to be transitioned to but not other States and that's what we still get from using that."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK for many HMM applications there's this sense of temporal state persistence.",
                    "label": 0
                },
                {
                    "sent": "That is, the probability of a self transition is typically higher than the probability of a transition to another state.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This does not work.",
                    "label": 0
                },
                {
                    "sent": "So take for example the problem of speaker diarization, where we're trying to parse an audio file into labels of who is speaking one.",
                    "label": 1
                },
                {
                    "sent": "Let's start by assuming that we know there are K people speaking an.",
                    "label": 0
                },
                {
                    "sent": "If we use an HMM to model this process, the Markov structure on the state sequence dictates that the distribution over who's speaking at the next time step solely depends on who is currently speaking.",
                    "label": 0
                },
                {
                    "sent": "So for this application there's a high probability of self transition, because if you're currently speaking.",
                    "label": 1
                },
                {
                    "sent": "You're likely to continue speaking another key aspect of this application is the fact that this speaker specific emission distributions are very complex multimodal distributions.",
                    "label": 0
                },
                {
                    "sent": "Does anybody know if there's a clicker?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So often, however, 1 cannot assume that there are K people speaking and would like to be able to infer that from the data or for or to allow new speakers to appear with time.",
                    "label": 0
                },
                {
                    "sent": "There currently exists nonparametric methods for learning Hmm's with an unknown number of states, or in this case, an unknown number of speakers.",
                    "label": 0
                },
                {
                    "sent": "However, what will show during this talk is that they have a hard time dealing with processes that have this high probability of self transition or these multi modal emissions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So maneuvering target tracking is another application where there's an unknown number of states that persist over continuous periods of time.",
                    "label": 1
                },
                {
                    "sent": "Here, and you know, capturing this temporal state persistence is really key in performing inference here.",
                    "label": 1
                },
                {
                    "sent": "We modeled the maneuvering target as some dynamical system.",
                    "label": 0
                },
                {
                    "sent": "That's driven by this exogenous input, which we take to be the output of an HMM.",
                    "label": 0
                },
                {
                    "sent": "And here each hmm state is a different maneuver mode for the target.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we mentioned that there currently exists nonparametric methods for learning Hmm's, specifically, the hierarchical tier sleep process can be used as a prior on Hmm's with unknown state space.",
                    "label": 1
                },
                {
                    "sent": "These models are referred to as the Infinite HMM or the HTP HMM and were studied by Bielen tea at all.",
                    "label": 0
                },
                {
                    "sent": "So the HTP, HMM, has a number of nice properties.",
                    "label": 1
                },
                {
                    "sent": "First, it encourages the use of a sparse subset of this unbounded state space because it reinforces transitions that you've already seen in the data.",
                    "label": 1
                },
                {
                    "sent": "Another thing is that it allows for new states to be generated as you get more observations, and so thus the model complexity complexity adapts to that of the observations.",
                    "label": 0
                },
                {
                    "sent": "However, what will show is that the HTP, hmm, inanimately captures this temporal state persistence.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "Here's a set of observations generated from a three state hmm.",
                    "label": 0
                },
                {
                    "sent": "And here are those observations, colored by the true state sequence.",
                    "label": 0
                },
                {
                    "sent": "We now show a plot of the same observations colored by the inferred state sequence from the HTP HMM after 1000 iterations of Gibbs sampling and what you see is that it's taken one true state and in Ferd two states which it then rapidly switches between.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so during this talk will first just run through a very quick overview of Hmm's.",
                    "label": 0
                },
                {
                    "sent": "In order to solidify some notation and then we'll discuss this HTP HMM formulation and how we augment the model with a learned bias towards self transitions.",
                    "label": 0
                },
                {
                    "sent": "We call this is sticky HTP.",
                    "label": 0
                },
                {
                    "sent": "Hmm, and once we've developed this model, we can start looking at capturing multimodal emission distributions.",
                    "label": 1
                },
                {
                    "sent": "We conclude the talk by presenting results on the NIST Speaker Diarization database and show that our performance is comparable to that of the current state of the art.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so and hmm is just a class of doubly stochastic processes consisting of an underlying discrete state sequence represented by the C random variables, and it has this Markov structure with transition distributions PIE.",
                    "label": 0
                },
                {
                    "sent": "Condition on the state sequence.",
                    "label": 0
                },
                {
                    "sent": "You just have a set of conditionally independent observations Y from emission distributions parameterized by Theta.",
                    "label": 0
                },
                {
                    "sent": "This box notation here just simply means to replicate that portion of the graph K times.",
                    "label": 0
                },
                {
                    "sent": "So one can view sample path of the state sequence as a walk through this state versus time lattice here.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's assume that.",
                    "label": 0
                },
                {
                    "sent": "We start in state two and we have the following possible transitions from time step, one to time Step 2.",
                    "label": 0
                },
                {
                    "sent": "And here these arrows are shaded by their respective probabilities which are captured in the transition distribution for that state.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's then assume we transition to state one and have the following set of possible transitions.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so on.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What the HCP hmm allows is for an unbounded state space.",
                    "label": 1
                },
                {
                    "sent": "That is, we let K go to Infinity.",
                    "label": 0
                },
                {
                    "sent": "And so now we have these countably infinite transition distributions.",
                    "label": 1
                },
                {
                    "sent": "So specifically, the richley process portion of the HTP is what allows for this unbounded state space, and also encourages using a sparse subset of this infinite state space.",
                    "label": 0
                },
                {
                    "sent": "The hierarchical layering of these Dirichlet processes ties together each of these state specific transition distributions so that they share this sparse subset of possible states visited.",
                    "label": 0
                },
                {
                    "sent": "So just a little bit more formally, we start with an average transition distribution beta, which is drawn according to the stick breaking construction, which goes as follows.",
                    "label": 0
                },
                {
                    "sent": "We start with a stick of unit probability mass.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we draw a number from a beta one gamma distribution and we break off that portion of the stick and define it as the first mixture.",
                    "label": 0
                },
                {
                    "sent": "Wait.",
                    "label": 0
                },
                {
                    "sent": "We then draw another number from a beta distribution break off that portion of the remainder of the stick and define it as the second mixture, wait.",
                    "label": 0
                },
                {
                    "sent": "And we keep doing this infinitely many times.",
                    "label": 0
                },
                {
                    "sent": "OK, so now that we've defined this average tree.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Addition distribution.",
                    "label": 0
                },
                {
                    "sent": "We can use it to define a set of state specific transition distributions.",
                    "label": 0
                },
                {
                    "sent": "Each of which is distributed according to Adir Ishly process with beta as the base measure.",
                    "label": 0
                },
                {
                    "sent": "And because of this, the expected weights for each one of these transition distributions is equivalent to beta, regardless of which state we're in.",
                    "label": 0
                },
                {
                    "sent": "And we can also see that the sparsity of beta is the shared between the state specific transition distributions.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But let's return to this example from those three states.",
                    "label": 0
                },
                {
                    "sent": "Hmm, here we use slightly different way to show the true state labels and the estimated state labels with errors shown in red.",
                    "label": 0
                },
                {
                    "sent": "So there we see that very rapid switching between two redundant states.",
                    "label": 0
                },
                {
                    "sent": "And what we see is that the Dirichlet processes bias towards simpler models is really inadequate for preventing this very unrealistic rapid switching between the states.",
                    "label": 0
                },
                {
                    "sent": "And there are a few things to note about this issue.",
                    "label": 0
                },
                {
                    "sent": "The first is that your predictive performance can actually be hurt because you're assigning fewer observations to each state from which to infer parameters that are used for prediction.",
                    "label": 0
                },
                {
                    "sent": "In addition, in applications like the speaker diarization problem, you actually care about the learn segmentation.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in order to address this issue, what we do is we add this parameter Kappa, which allows for a bias towards self transitions.",
                    "label": 0
                },
                {
                    "sent": "We call this is sticky parameter.",
                    "label": 0
                },
                {
                    "sent": "Our average transition distribution is defined as before, but now each of the state specific distributions has an added weight on the base measure in the component corresponding to self transition.",
                    "label": 0
                },
                {
                    "sent": "So if we look at the expected weights of each of the transition distributions, we see it's a convex combination of the global set of weights from the average transition distribution plus a set of state specific weights.",
                    "label": 0
                },
                {
                    "sent": "And we can qualitatively compare what these distributions look like to their original model, and we see that there's this bias towards self transitions now.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now that we've developed the model, we can start talking about inference.",
                    "label": 0
                },
                {
                    "sent": "So the basic inference algorithm for the HTP HMM, is the direct assignment Gibbs sampler.",
                    "label": 1
                },
                {
                    "sent": "Which marginalises over the infinite set of transition distributions and parameters and sequentially samples the state condition on all other state assignments.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's just let's say that the previous state took a value J and the next state took a value L. Well, the prior probability of assigning the current state to value K is a variant of the Chinese restaurant prior, which determines how many Jada K&K to L transitions we've already seen.",
                    "label": 0
                },
                {
                    "sent": "And then that's weighted by a likelihood term based on all other observations currently assigned to that state.",
                    "label": 0
                },
                {
                    "sent": "Although rather straightforward, the sampler suffers from extremely slow mixing rates.",
                    "label": 0
                },
                {
                    "sent": "Be close because global assignment changes to the entire state sequence or forced to occur component by component.",
                    "label": 0
                },
                {
                    "sent": "And this problem is exacerbated when we add this sticky parameter, because let's assume we have the following true state sequence.",
                    "label": 0
                },
                {
                    "sent": "If the sampler splits one state into two, the self transition bias reinforces that assignment and it's very hard to then merge those states back together.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So far, our sampler has not taken advantage of the fact that we actually have this very nice Markov chain structure to our graph.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "An there exists very efficient algorithms for Hmm's using dynamic programming techniques like the forward backward algorithm which can be used to jointly sample the entire state sequence conditioned on a set of observations.",
                    "label": 0
                },
                {
                    "sent": "However, this algorithm relies on actually instantiating the set of transition distributions and parameters so they can no longer be integrated out.",
                    "label": 0
                },
                {
                    "sent": "However, because we're using an HTP HMM, these distributions are countably infinite, so we need to approximate them.",
                    "label": 0
                },
                {
                    "sent": "Using some finite approximation.",
                    "label": 0
                },
                {
                    "sent": "And what we choose to use here?",
                    "label": 0
                },
                {
                    "sent": "There are many different options.",
                    "label": 0
                },
                {
                    "sent": "You'll see a talk on this later.",
                    "label": 0
                },
                {
                    "sent": "Is the week limit approximation, which simply places a symmetric Dirichlet distribution prior on our average transition distribution, which then induces a finite dearsley distribution on each of the state specific transition distributions.",
                    "label": 1
                },
                {
                    "sent": "We then iterate between sampling the transition distributions and parameters.",
                    "label": 0
                },
                {
                    "sent": "And the entire state sequence.",
                    "label": 0
                },
                {
                    "sent": "And to sample the entire state sequence, we simply pass messages backwards and then sequentially sample sample forwards.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In addition to sampling the transition distributions, parameters and state sequence, we also place weakly informative priors on our hyperparameters and learn these from the data as well.",
                    "label": 1
                },
                {
                    "sent": "And for all the experiments, we're going to show, we use the same settings for these hyper priors.",
                    "label": 0
                },
                {
                    "sent": "So, just to reiterate, we actually place a vague prior on this bias towards self transition and learn it from the data.",
                    "label": 0
                },
                {
                    "sent": "So a related self transition parameter was presented in the infinite HMM paper by bill at all, but they didn't develop a globally consistent generative model, which meant that they relied on some heuristics in their Gibbs sampling.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's return again to this data set and look at some of the performance of the sticky HTP hmm versus the original model.",
                    "label": 0
                },
                {
                    "sent": "First, we're going to show using the block sampler, and So what we do is we run hundred initializations of the sampler for 1000 Gibbs iterations, and that each iteration we compute the Hamming distance between the true state sequence and the estimated state sequence, and here we plot in read the 10th.",
                    "label": 0
                },
                {
                    "sent": "And 90th quantile.",
                    "label": 0
                },
                {
                    "sent": "And here the median performance.",
                    "label": 0
                },
                {
                    "sent": "And so we see.",
                    "label": 0
                },
                {
                    "sent": "An improvement both in mixing rate and overall performance by going to the sticking model.",
                    "label": 0
                },
                {
                    "sent": "When we look at the sequential sampler, we see that much lower mixing rate, especially in the case when we have that sticky parameter.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We then tested the model on data that actually had very fast state switching.",
                    "label": 0
                },
                {
                    "sent": "Here's an observation sequence and the observations colored by the true state sequence, and we see this very rapid switching between four different States and what we see is that the sticky HTP HMM is still able to learn these fast dynamics, and in this case the performance is comparable to that of the original HTP, HMM?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now that we've developed this sticky HTP.",
                    "label": 0
                },
                {
                    "sent": "Hmm, we can start looking at learning multimodal emission distributions.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because for a lot of HMM applications like the speaker diarization problem, the state specific emission distributions are multimodal and can't be well captured by a single parametric distribution.",
                    "label": 0
                },
                {
                    "sent": "Instead, one can approximate these distributions as a mixture of Gaussians and here we show an observation sequence from a two state hmm with these mixture of Gaussian emissions and if we pass this data to either the sticky or non sticky HTP, HMM.",
                    "label": 0
                },
                {
                    "sent": "When it's constrained to single Gaussian admissions.",
                    "label": 0
                },
                {
                    "sent": "What it typically infers is it assigns a new state for each one of the mixture components and then rapidly switches between them.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, now that we've modeled this bias towards self transition, we can further augment the model to Start learning these multi modal missions and to do this.",
                    "label": 0
                },
                {
                    "sent": "We approximate these multi modal missions as an infinite Gaussian mixture.",
                    "label": 1
                },
                {
                    "sent": "And we place a dearsley process prior.",
                    "label": 0
                },
                {
                    "sent": "So now every state has its own stick breaking density over the mixture weights of the Gaussians to finding that states emission distribution.",
                    "label": 0
                },
                {
                    "sent": "And the generative model then says we choose a state dependent upon the previous state as before.",
                    "label": 0
                },
                {
                    "sent": "Then, given that state we choose a mixture component from its stick breaking density.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Given that mixture component, you generate an observation.",
                    "label": 0
                },
                {
                    "sent": "So it allows us to.",
                    "label": 0
                },
                {
                    "sent": "Discriminate between the underlying hmm states is both this nice Markov structure on the state sequence as well as the bias towards self transitions, 'cause if the model was free to both rapidly switch between hmm States and associate multiple Gaussians per state, there would be quite a bit of posterior uncertainty.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's return to this observation sequence from a two state hmm with multimodal emissions.",
                    "label": 0
                },
                {
                    "sent": "If we look at the sticky HTP HMM and the nonsticky model, both with this dearsley process admissions, this infinite mixture of Gaussian admissions, we see that the nonsticky model places very little weight on good segmentations, and that's because it's choosing between creating more hmm states, each with fewer mixture components or fewer hmm states, each with more mixture components, and it's very hard to discriminate between those two things.",
                    "label": 0
                },
                {
                    "sent": "When either of the models is constrained to single Gaussians, of course we get much worse performance.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we present some results on this NIST speaker Diarization database which consists of 21 recorded meetings with ground truth labels.",
                    "label": 0
                },
                {
                    "sent": "So this is run this competition for the past six years and.",
                    "label": 0
                },
                {
                    "sent": "Using this database of meetings and the Berkeley ICSI team has won by a large margin, and here, just to reiterate, the goal is both in for the number of speakers and labels of who is speaking when.",
                    "label": 0
                },
                {
                    "sent": "So the ICSI teams algorithm is this agglomerative clustering approach that starts with a large number of states, large number of speakers and then slowly merges them when certain criteria are met.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this algorithm is like very highly engineered to this task, and it's been developed over many years by a large team of researchers.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in order to compare our performance to the ICSI teams, we used the same set of features, specifically the 1st 19 dimensions of the Mel frequency capsule coefficients.",
                    "label": 0
                },
                {
                    "sent": "And here we show a movie.",
                    "label": 0
                },
                {
                    "sent": "Of what these features look like, colored by the true state speaker and it's a movie over every dimension of that feature vector.",
                    "label": 0
                },
                {
                    "sent": "And what you see is that the features look very similar between speakers, which makes it a very challenging problem.",
                    "label": 1
                },
                {
                    "sent": "And in addition along some components of the feature vector, a speaker will look different overtime, which leaves the possibility of inferring multiple speakers from a given true speaker.",
                    "label": 0
                },
                {
                    "sent": "And just to note, although there are true speaker labels available here, we don't use any training data that we do this fully unsupervised, just passed the raw observations, whereas the ICSI teams algorithm relies on a lot of training data.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We compare the performance of the sticky HTP HMM with these seriously process admissions to the nonsticky model, and we use the standard NIST speaker diarization error rate metric, which is just basically a calculation of the percentage of wrong speaker labels and what we do is we run 10 initializations of the Gibbs sampler for 1000 iterations for each of the 21 meetings, and then at the thousandth iteration we choose the initialization that has the highest likelihood of the observations given the inferred parameters.",
                    "label": 0
                },
                {
                    "sent": "So we plot those.",
                    "label": 0
                },
                {
                    "sent": "21 most likely points here and what we see is that since a majority of those points lie significantly above this diagonal line, the sticky model dominates in performance.",
                    "label": 0
                },
                {
                    "sent": "We can then compare the performance to that of the ICSI team.",
                    "label": 0
                },
                {
                    "sent": "We see that overall, our performance is pretty comprable and in addition, if we look at our best and worst performance over the 21 meeting database, our performance is slightly better than their best and worst performance.",
                    "label": 0
                },
                {
                    "sent": "In addition, one advantage of art model is the fact that we get multiple speaker segmentations because the Gibbs sampler gives us a sense of this posterior uncertainty, whereas the collaborative clustering approach just outputs one speaker label.",
                    "label": 0
                },
                {
                    "sent": "What one speaker label sequence?",
                    "label": 0
                },
                {
                    "sent": "So if you look at the top five, most likely initializations are performance drops to 15% Der.",
                    "label": 0
                },
                {
                    "sent": "We can then look at plotting the estimated number of speakers versus the true number of speakers for each of the meetings.",
                    "label": 1
                },
                {
                    "sent": "Each of the initializations that we ran.",
                    "label": 0
                },
                {
                    "sent": "And here we scale each one of these points by their respective likelihoods, and we see since there's this strong correlation with the diagonal, it shows our model's ability to learn very number of speakers.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to quickly show some qualitative performance, here's one meeting where we did incredibly well here.",
                    "label": 0
                },
                {
                    "sent": "The true speaker labels.",
                    "label": 0
                },
                {
                    "sent": "Here's the first set of speaker labels for the most likely sequence, and.",
                    "label": 0
                },
                {
                    "sent": "We show errors in red.",
                    "label": 0
                },
                {
                    "sent": "Kind of hard to see here.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if you look at a meeting where we did much worse, you see that it's taken one true speaker and split into two inferred speakers.",
                    "label": 0
                },
                {
                    "sent": "But if you look at a different initialization, not the most likely one, we have a much better segmentation, much lower Der, and that it's important to note that this likelihood metric that we used is very biased towards segmentations that infer more states that are more finely tuned to the observations, but.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Once again, we can look at multiple possible segmentations.",
                    "label": 0
                },
                {
                    "sent": "So in conclusion, we looked at some of the limitations of the original HTP HMM formulation and showed how we can augment the model to have this learned bias towards self transitions, which then allows us to look at multi modal missions.",
                    "label": 0
                },
                {
                    "sent": "In terms of computational costs, the additional cost of sampling the sticky parameter is really negligible compared to the other costs of the Gibbs sampler, and in addition, in practice it often runs much faster because it will typically instantiate fewer state components.",
                    "label": 0
                },
                {
                    "sent": "We've also shown that the sticky HTP HMM is able to learn a wide range of dynamics even when the state persistence is not present in the data.",
                    "label": 1
                },
                {
                    "sent": "So in conclusion, is a very simple and effective addition to the model.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So you said the state of the art system you are comparing with this 60 system was using supervised segmented data.",
                    "label": 0
                },
                {
                    "sent": "Have you thought about how you could make your things semi supervised and use some of that segmented data and hopefully even outperforming yeah?",
                    "label": 0
                },
                {
                    "sent": "I mean it is possible, at least in setting those hyper priors to use some of.",
                    "label": 0
                },
                {
                    "sent": "The other like they use a development set and then test on an evaluation set which changes every year and it would be possible to use that development set to either fix those hyperparameters or better adjust the priors that we place on them so that they're not like weakly informative.",
                    "label": 0
                },
                {
                    "sent": "We haven't really tried doing that yet though, but you know, it's something that we're thinking about looking into.",
                    "label": 0
                },
                {
                    "sent": "So it was more just a verification of this model rather than trying to implement a new speaker diarization application, but something interesting to look into.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Just one more.",
                    "label": 0
                },
                {
                    "sent": "So I'm just wondering, you know, in a sense the this new hyperparameter is telling us the probability of some transition, right?",
                    "label": 0
                },
                {
                    "sent": "And so now what's the the shared prior learning?",
                    "label": 0
                },
                {
                    "sent": "I mean, it's like the probability of making transitions.",
                    "label": 0
                },
                {
                    "sent": "You mean the prior on this self transition parameter the whole?",
                    "label": 0
                },
                {
                    "sent": "Oh, that's the average transition distribution, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, it gives you it still gives you a sense of the possible states that you're visiting it.",
                    "label": 0
                },
                {
                    "sent": "There's still a sparse subset of states visited, and it does give you the sense of transitions between states, but.",
                    "label": 0
                },
                {
                    "sent": "It's basically that's shifted down just slightly with an added weight on a self transition for each state.",
                    "label": 0
                },
                {
                    "sent": "But what that still gives you is that sparsity.",
                    "label": 0
                },
                {
                    "sent": "That shared between states, you know, maybe states one and two are very likely to be transitioned to but not other States and that's what we still get from using that.",
                    "label": 0
                }
            ]
        }
    }
}