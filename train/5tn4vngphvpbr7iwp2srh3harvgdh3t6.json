{
    "id": "5tn4vngphvpbr7iwp2srh3harvgdh3t6",
    "title": "Introduction to Support Vector Machines",
    "info": {
        "author": [
            "Colin Campbell, University of Bristol"
        ],
        "published": "Feb. 5, 2008",
        "recorded": "January 2008",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/epsrcws08_campbell_isvm/",
    "segmentation": [
        [
            "Press on yeah.",
            "A license she's calling Campbell, who's and spends the day.",
            "Support vector machines right?",
            "OK, so I'm calling Campbell from Bristol University.",
            "This my first talks really the introduction support vector machine, so I'm just doing this sort of classical story started with binary classification, then multiclass classification, soft margins, that sort of stuff.",
            "My second talk after a break.",
            "I'll have to sort out what how long the break is, and so when we get to it, but would be on more general kernel methods.",
            "In fact the idea kernel methods as it can define many.",
            "Types of learning machines.",
            "There are many types of kernels.",
            "Many types of things you can do with these kernel based methods.",
            "So this second talk after break I'm trying to make the subject much more general.",
            "OK, this afternoon is opportunity to actually try out support vector machines.",
            "Now that's a really sort of four levels.",
            "I'll give a little talk when we come to it.",
            "Simplest level is a nice little graphical thing done by Steve Gunn.",
            "We click points on the screen and you see the actual support vector machine setting up a boundary to separate data.",
            "And then there's various other options.",
            "As a packager wrote myself, where you can use UCI repository data and download data and do classification type problems, and then finally, if you're familiar with MATLAB, you can try the quad prog program in the optimization toolbox and actually write your own SVM learner so we will come to that this afternoon, but this is support vector machines and my own interest.",
            "I researched quite a lot on support vector machines in the past.",
            "In the second talk, there will be quite a few bits which run my own research.",
            "Seems like novelty detection.",
            "I worked in a few things like that may mention a Bayes point machine I worked on, but my main interest at the moment is probabilistic graphical methods and also work a lot in applications to cancer.",
            "In fact, which is a new subject called Cancer informatics right at the end I'll be using these support vector machines, for example to decide relapse versus non relapse, predicting the future whether cancer patient will have a relapse or not.",
            "And it's quite successful problem I did with the staff and Institute for Cancer Research up in London so but will get on to applications that."
        ],
        [
            "End of the second talk.",
            "So if you like this first talk, I want to first of all just do the simplest story with a support vector machine for binary classification and then this topic of soft margins.",
            "I'll come on to that.",
            "Then of course many real life applications are multi class so then elaborate.",
            "This story of binary class to multiclass classification.",
            "The schema uses by Nella Christy knees.",
            "In my Department you did it with the John Platt.",
            "And then briefly I'll also talk about regression so."
        ],
        [
            "That's the plan for the first talk, the second talk.",
            "In the first talk, I really use quadratic programming to learn the problem.",
            "In the second talk, I want to really show that there's a bigger subject out there and it's not just quadratic programming.",
            "You can do kernel methods with linear programming.",
            "Indeed, sort of nonlinear optimization, and many other things.",
            "I'll be talking about how to train SVM's.",
            "Model selection there is potentially one parameter you would have in your SVM which is the kernel parameter.",
            "How do you find it?",
            "Well, you might find the data so the kernel parameter from the data using cross validation, but there are other ways of doing it from various theorems which had been established.",
            "So this is the topic of model selection.",
            "I'll come onto it then different types of kernels I'll come on to that and then I talked just now about actually using.",
            "SPMS and practice insane medical applications."
        ],
        [
            "So let's first of all start.",
            "What are the advantages of SVM's?",
            "Well, it's a principle approach to machine learning, in particular to all the various different types of machine learning, classification, regression, novelty detection, etc.",
            "SVM, that's a bit good generalization and practice.",
            "When I first encountered this subject which had been about New Year 2000, knowing that 98, I tried out.",
            "I've been doing your networks at that point.",
            "I tried out the multilayer perceptrons against an SVM.",
            "Across a whole lot of UCI datasets, and my impression was for classification.",
            "SVM was better than a multilayered perceptron, though.",
            "For the regression they were fairly similar.",
            "But in any case exhibits good generalization to new data, and the hypothesis has an explicit dependence on the data.",
            "These of course are support vectors, which I come onto.",
            "What I'm saying here is if I take something on your network model, its dependence on the data is not so clear, but.",
            "I can interpret the dependence on the data quite simply from the model I generate, hence it can be readily interpreted so."
        ],
        [
            "First advantage is when I said principle approach to the subject.",
            "Natural motivation for the subject to support vector machines really came from statistical learning theory.",
            "Various bounds, which approved in that subject, and so unlike many other approaches to machine learning like neural networks, rather sort of ad hoc sort of approach.",
            "Here you've got learning theory.",
            "Learning theory motivates your approach, and your approach is successful.",
            "Relatively few, if any, parameters you have to adjust so the operator doesn't come into it.",
            "You simply percent of data an you're learning machine goes and learns it.",
            "Also, another really big plus of this subject is the optimization task is convex.",
            "Again, in contrast to a new network, the new network, typically the function I wish to optimize, has false minimum.",
            "There has local minima.",
            "OK, have a problem finding some to the global minimum will actually the task I wish to do with a support vector machine is a quadratic problem, so it's convex.",
            "One solution, no local minima unlike a neural network, so that's a big plus for this approach.",
            "Relatively few world very few parameters.",
            "To adjust, potentially, there's a soft margin Anna kernel parameter.",
            "If I'll do medical data like gene expression data are used later on, I don't use anything more than a linear kernel.",
            "I don't have a kernel parameter, I don't have anything to adjust, unlike, say, a new network.",
            "Again where I have questions about how many hidden nodes are there, how many hidden layers?",
            "What do I do about some of the parameters in the updating functions, etc.",
            "OK also can implement a.",
            "Confidence measure very important for classifier.",
            "It's not just as it says relapse or non relapse.",
            "For my medical application, but it should be able to assign a probability to those two categories.",
            "OK so these are all pluses of this holup."
        ],
        [
            "Roche, so I want to first of all just recount the story for binary classification, OK?",
            "And first of all, just some things in my notation.",
            "My inputs will be this big X OK, potentially a vector.",
            "The attributes have number attributes or components.",
            "I will label the particular sample I'm considering an associated with this particular input.",
            "I have a corresponding output.",
            "Will should be denoted Yi OK and I'm just going to do binary classification to start with, so my outputs my labels.",
            "I call them will be plus or minus one.",
            "OK, my medical application and talk about much further on will be a binary classification type task where plus will be the patient relapse minus we they won't relapse.",
            "Now call these the labels.",
            "It could also perhaps call them targets and these eyes are at range one to M where M is the number of samples or pairings of X&Y's.",
            "OK, so these XI define a space which are called.",
            "Input space OK, you've probably seen input spaces in the previous talks.",
            "And the input space consists of labeled points."
        ],
        [
            "Label plus or minus one.",
            "Now I did say at the beginning that's what's nice about the subject of support vector machines is it was motivated from statistical learning theory.",
            "OK, an indeed social learning theory are just a little digression about learning theory.",
            "There's various different approaches to learning theory.",
            "One is sort of like a Bayesian type approach.",
            "Another one is not so well known, but it's a physics you type approach.",
            "It sort of decayed away in the literature.",
            "But the physics or system mechanics type approach you have to you can establish learning curves and.",
            "It involves complex calculations called replica calculations.",
            "A third approach is just a school learning theory, where you devise bounds upper and lower bounds on the generalization error, and I won't go into this subject in great detail, but also you can drive these theoretical bounds on the generalization error OK, and I just I'm not going to state the bound mathematically, I'm just going to have a look at its implications.",
            "OK, so if I'm doing it some sort of learning task.",
            "From handling real life data, I would evaluate it on test data and then give me the test error and the corresponding theoretical test error.",
            "I call the generalization error OK, so that generalization error from statistical learning theory I can draw."
        ],
        [
            "My bounds on it and the bound I'm interested in.",
            "It has two properties.",
            "OK, the upper bound on the generalization error does not depend on the dimension of the space.",
            "OK, that's one observation.",
            "I'm going to make the bound is independent and actual size of the input space I'm considering.",
            "Observation #2 is that the bound is minimized by maximizing an object called the margin, which are the notice gamma, which is a minimal distance between the hyperplane separating two classes and a two and the closest data points belonging to each class.",
            "All that's best illustrated."
        ],
        [
            "With a picture OK and here it is.",
            "So here I've just got binary classification, two types of labeled points, plus or minus one, and in this case everything is quite nice because the two types of data are well separated.",
            "Here they are OK and my classifier is a line in two dimensions or in N dimensions and dimensional input space is a hyperplane OK?",
            "Appointment and the 2nd but the the classifier is a hyperplane, it's a directed hyperplane, in which anything lying on this side of the hyperplane given in Brown here will be labeled as plus one positive and everything on this side of my hyperplane will be labeled as negative.",
            "OK, so it's a directed hyperplane.",
            "Now what the?",
            "So all the classifiers?",
            "Well, we can be viewed as such.",
            "And what the theorem therefore says is that my best classifier is that hyperplane or classifier which is maximally distant from the closest points on both sides, well, actually, intuitively that seems pretty clear, because suppose my hyperplane was like so OK separates the data correctly, but is actually quite close to some points and doesn't seem to be intuitively quite as nice as this thing.",
            "Then again, if I came along with the classifier.",
            "Like so we'll obviously it's going to get some points wrong.",
            "OK, so there will be a lot of classifiers I can devise.",
            "Some will just simply be wrong.",
            "They'll give me an error on my training data.",
            "There will be a number which will correctly separate the data and it seems intuitive that in fact the best hyperplane I could find is this hyperplane which is maximally distant from the closest points on both sides.",
            "For this very nicely separated data, OK indeed.",
            "These closest points here are the support vectors.",
            "They're called support vectors.",
            "Will affect, of course, 'cause really I'm in a N dimensional input space, so their vector they are called support vectors.",
            "'cause if you like they support where the hyperplane should be Huawei, because if I start to removing these guys here then my optimally separating hyperplane would shift OK.",
            "Whereas if I remove these ones non support vectors, well they were making a difference.",
            "I can remove them and my maximally separating hyperplane will just sit where it is.",
            "OK, yeah?",
            "I'll come on to that.",
            "I come onto it so my story at the moment is quite simple, nicely separated data, OK, and I'm going to mess the picture up shortly, right?",
            "So these are support vectors.",
            "Remove them.",
            "My separating hyperplane would shift remove these doesn't, so that's why I called them support vectors.",
            "And finally I call it a support vector machine because it's a learning machine.",
            "OK, so that's sort.",
            "Magical, so that's."
        ],
        [
            "So the geometric picture.",
            "Now I actually want to formally drive the story of the support vector machine and first of all we note that that separating hyperplane is represented like so.",
            "So if you go back to your first year mathematics which did universal or school, then you would know that hyperplane can be written like so.",
            "Generally you would have it in a textbook as N dot X = C. The equation one equation.",
            "For hyperplane or playing OK, and I've taken the see over to the side, but this is an equation of a plane and X is my day to be as written solely because of the bias.",
            "OK could be if I move it to other side threshold W if I normalize it will actually be the normal to the hyperplane.",
            "OK, so I haven't got a pen here, but I'm just using the formal definition of a hyperplane N dot X = C. Where N is the normal type of pain, so call this the bias.",
            "OK, I'll call these the weights OK and this thing here goes into my decision function like so looks just like any sort of argument of something like a new network decision function and hence I put my data point in here.",
            "Perhaps a new data point work this thing out.",
            "If it's positive I get a plus one out, it was negative again minus one out.",
            "So that's my decision function.",
            "Like so OK.",
            "Right so."
        ],
        [
            "I."
        ],
        [
            "I know one thing about this.",
            "Decision function here.",
            "This sign function is influenced by the sign of the subject here, but not by its magnitude.",
            "In other words, this decision function is left invariant if I scale the WS and the bees, but any positive quantity.",
            "OK, we observe that any multiply this and this by two.",
            "Then nothing happens.",
            "So there's an invariant."
        ],
        [
            "In my sort of domain.",
            "OK, so decision function is left invariant under any positive should say under positive rescaling of my W Zombies.",
            "Now we want to implicitly fix a scale.",
            "In fact, why are we actually wanting to implicitly fix the scale?",
            "Well, let's just go back to our little geometric picture."
        ],
        [
            "Which is here I said according to my theorem from Cisco learning theory.",
            "My just best generalization is achieved by maximizing the margin or the distance between that separating hyperplane and the closest points.",
            "So this margin is actually a distance measure.",
            "OK, because it's a distance measuring this input space, I must actually fix the scale of my input space, sort of my space."
        ],
        [
            "And."
        ],
        [
            "Invariants causes me a sort of problem.",
            "Indeed, I'm going to actually implicitly fix a scale so."
        ],
        [
            "So I'm going wrong way.",
            "I know there's an in variance in here, OK?",
            "And I'm actually going to implicitly fix a Lambda value in order to make this distance."
        ],
        [
            "Here one OK here.",
            "I'm gonna actually those invariants.",
            "I'm actually going to dictate that this distance from here to here is a value one.",
            "Or if you like from here to here, that distance is of value one.",
            "OK, how do I do that well?"
        ],
        [
            "So say I'm going to for those support vectors on each side on one side of my directed hyperplane.",
            "This object here will be equal to plus one if I'm a support vector on one side and a -- 1 if I'm a support vector on the other side that dictating this condition here defines what is called a Canonical hyperplane and what I'm doing is implicitly fixing a Lambda value.",
            "OK, so.",
            "So just going back again."
        ],
        [
            "Picture here I've now dictated that this guy here W X + B will be minus one.",
            "This will be greater than minus one WX plus B we plus one and this will be more than plus one.",
            "OK and implicitly I've fix."
        ],
        [
            "At scale.",
            "OK, so we have these two equations for support vectors on both sides of my hyperplane.",
            "OK and that define."
        ],
        [
            "I want to be by Canonical playing now."
        ],
        [
            "One thing I notice, of course, is I can subtract this from this.",
            "It would get rid of the bees and give me a 2 on the other side."
        ],
        [
            "So just subtracting one equation from the other, I get this equation for support vectors on both sides OK.",
            "Right, so we know that.",
            "Now."
        ],
        [
            "Also note the following.",
            "The margin is given by the projection of a vector X1 X 1 -- X two onto the normal to the hyperplane OK?"
        ],
        [
            "Let's just show this is a.",
            "Sort of picture.",
            "This is my margin.",
            "I want to maximize the margin I want to find a hyperplane maximum distance for the closest points on both sides.",
            "OK, now let me choose two support vectors.",
            "Let this be X1 and this guy here be X2.",
            "OK, well obviously that to 9 drawn in red is a vector connecting those two points.",
            "OK, I'm saying the margin.",
            "01 further observation I have this separating hyperplane that's given by W dot X + B = 0, While the normal that hyperplane is W hat.",
            "OK, just that's definitely from planes.",
            "OK, I'm saying that the margin the singing black is given by the projection of this vector onto the onto model W hat.",
            "That's a normal to hyperplane if I project this vector onto the green normal, then I'll get the margin OK. Just plane geometry projections OK, nothing mysterious."
        ],
        [
            "So that's what I do now.",
            "I will project W 1 -- W two onto the normal normal OK, which is just given by W over MoD W. That's a normal to hyperplane.",
            "I observed therefore the projection just from plane geometry will be X 1 -- X two times this W. Sorry, dotted with W MoD W. But just coming back to my previous account."
        ],
        [
            "Vision OK, I had already established this quite separately, so the projection of this onto W over MoD W will be equal to two over MoD W. OK, so just divide both sides by W and I'm really looking at the projection of this vector onto the normal hyperplane and hints that the margin is equal to two on MoD W OK?"
        ],
        [
            "Sweeping the following what I've been natural gone about, then I've actually argued that the margin is given well.",
            "Of what I've argued.",
            "Axe"
        ],
        [
            "Really.",
            "Is that this?",
            "Is 2 or more W?",
            "Actually, if I define the margin, I should really say the margins between here and here is one or more W OK."
        ],
        [
            "So I'm saying."
        ],
        [
            "In the day that the distance."
        ],
        [
            "Losing myself here.",
            "The distance between separating hyperplane and the support vectors is actually given by one or MoD W, OK?",
            "So if I want to maximize the margin.",
            "Then I must maximize this object here, OK?",
            "Make it as big as possible.",
            "Now maximizing one or more W is the same thing as minimizing MoD W OK.",
            "Maximize this is equivalent to minimizing just this OK.",
            "So we've."
        ],
        [
            "Got to the following.",
            "Maximization of the of the margin OK is equivalent, and minimization of MoD W well minimizing MoD W is going to be the same sort of thing as minimizing WW.",
            "OK sort of forgot the square root somewhere, but you can therefore imagine if I'm trying to maximize my margin is equivalent to minimizing.",
            "Therefore this thing OK, but subject to constraints and the constraints I already had.",
            "I had if I'm.",
            "This object here was a plus.",
            "One OK when y = + 1.",
            "This was a plus.",
            "One efforts or support vector, and if it wasn't support vector and on that same side of the hyperplane is greater than plus one.",
            "OK so if this is a plus one this thing will be greater than or equal to plus one if I'm on the other side of the hyperplane from the support vectors minus one.",
            "And if it's a non support vector it will be greater than minus one for the case of Y = -- 1.",
            "On that side of the hyperplane, the end of the day that dictation of my Canonical hyperplanes can be written down as this constraint equation here OK.",
            "So at the end of the day, it's anybody losses how I got either of these two.",
            "OK, I defined Canonical High Plains that gave me this defined maximize my margin, which gave me this.",
            "OK, I now have a constraint optimization problem because I'm trying to minimize subjects.",
            "Something subject to constraints.",
            "OK, that immediately sends up a little flag in my brain constrained optimization."
        ],
        [
            "Indeed, I hope most people are familiar with optimization.",
            "If not, doesn't really matter, but.",
            "I'll just.",
            "I'll just say, therefore I now approach it from the viewpoint of optimization I set down was called objective function, and that objective function has this object which I'm trying to minimize, and this is my constraint condition OK, which I had just now.",
            "And these alphas are LaGrange multipliers which are introduced because it's constrained optimization.",
            "So you've either heard a little branch multipliers you sell.",
            "This is totally obvious, or you happen, in which case you lossed right?",
            "So these are LaGrange multipliers, so this object is my primal objective function OK?",
            "Now again, if you're familiar optimization, you'll say, Oh well, there's a a primal formulation of a problem and an equivalent dual formulation of the problem, and it's going to be the dual formulation.",
            "I'm going to go for, and, but if you don't know anything about that doesn't matter, 'cause I'll show you just some derivatives to find solution, which will take me there.",
            "So this is my object.",
            "I've arrived at my primal objective function.",
            "I'm going to do 2 operations, which is.",
            "I wish to find an optimum.",
            "Well actually minimum of this L here at the optimum.",
            "What I would expect is DL by DB equals zero and the other variable I have which is the Elbe IDW to be 0.",
            "Those would be conditions or my gradient had expected the optimum.",
            "Let's have a look at those, so the optimum I want DL by DB equals zero.",
            "Well that's the only be here that gives me this constraint here OK?",
            "Secondly,"
        ],
        [
            "I do deal by DW deal by DW.",
            "Can quickly check that gives me W is equal to this thing here OK?",
            "My next thing is to substitute back this W into my previous equation.",
            "OK, if you're totally fine."
        ],
        [
            "Optimization this is all strikingly obvious because it's working out a Wolf jewel.",
            "If you're not, well, just is OK, so I put back the WI just arrived in here, here and here.",
            "OK, and that would actually give me."
        ],
        [
            "Uh."
        ],
        [
            "Following it gives me at the end of the day this expression here, which is the jewels brush.",
            "And why am I doing this jewel?",
            "Well one thing notice is it's quadratic programming.",
            "OK my thing which which to optimize is going to be this Alpha.",
            "This quadratic looks like this.",
            "OK the prime loss started from was also actually quadratic in W. OK look like this and there's a. Furman optimization theory.",
            "For every primal formulation, there's a dual formulation.",
            "If you solve the, obtain a solution of the primal, it's the same solution as a solution of the jewel.",
            "OK, this is actually the jewel.",
            "OK, now for getting all the story that led up to this point.",
            "OK, I've never arrived at point.",
            "I wanted to arrive at.",
            "This is the object I optimize to find solution for a support vector machine.",
            "Some comments.",
            "First of all, what you notice is the data.",
            "Appears in this block here.",
            "OK, this would be my input vector.",
            "My X and number of components in it.",
            "And why would be associated labels?",
            "But that's the data lump OK.",
            "The task I wish to do this is my previous task with them in task.",
            "The dual task is actually a Max task.",
            "I wish to maximize this object subject to constraints I come onto in a second, but a maximizing the Alpha.",
            "After all, it's the only thing I can maximize in because the rest of it's just data.",
            "OK, so I maximize this quadratic in Alpha.",
            "OK, now it is constrained optimization.",
            "First of all, these alphas were actually LaGrange multipliers and requirement on growth multipliers.",
            "They must be positive, OK?"
        ],
        [
            "Secondly, there was this condition from DL by DB that this equals 0, so it is constrained optimization OK."
        ],
        [
            "Now there's comments.",
            "Data comes in here and it's quadratic an there's a several convex problem.",
            "Therefore it looks a bit like this.",
            "There is one solution, it's not like a new network or whole approaches to machine learning, where unfortunately different starting point gets you a different solution.",
            "Different starting points will only get you the one solution, OK, which it will be this maximally separating hyperplane.",
            "OK, yeah.",
            "Why this is?",
            "Well, roughly, roughly this is Alpha squared, and that's linear in Alpha, so this is a quadratic yes.",
            "Is a quadratic type expression.",
            "So multi dimensional quadratic.",
            "Oh, quadratic optimization.",
            "Well, you've got within optimization theory.",
            "You got linear programming, which would be linear in Alpha.",
            "I'll come on to that later.",
            "OK, you got quadratic optimization, which means roughly Alpha squared times expressions.",
            "You could have something horrible root Alpha and things like that.",
            "That would be some sort of nonlinear optimization.",
            "Now, for quadratic programming, there's been a lot of work on trying to find very efficient routines with quadratic programming.",
            "You possibly have things like conjugate gradient method and so on.",
            "There actually first developed for quadratic programming, so another big plus is that there's pretty rapid routines to find the optimum of this expression W OK indeed.",
            "Such as the case if I had well, I've actually done this with 60,000 data points in there personally, but I know people have gone way beyond that.",
            "So that means I can have EM is 60,000.",
            "I've done it with Postal data.",
            "OK, now that's because I can use very fast routines from quadratic programming, and if you use something many other approaches to machine learning and you'll network, try anul network with a million data points some like.",
            "Back propagation, it will take you some time.",
            "I can tell you so it's one big pluses."
        ],
        [
            "Play rapid.",
            "So."
        ],
        [
            "Might ask for support vector machines.",
            "You could do this this afternoon if you wish.",
            "Actually program this thing up using quad prog in MATLAB if you're familiar, but there is to actually optimize this with respect to Alpha."
        ],
        [
            "Subject to the constraints, having found those alphas, I plant them in here OK, and then I may have now a new test data point.",
            "I wanted to know what the answer is that test data point comes in here said I got it with my data and my ex is.",
            "I have my Alpha so just found for my constrained quadratic programming I have Y which are my labels be our total greatness.",
            "Second but I therefore have set up my decision function and it will make a decision.",
            "On this new datapoint, said OK, so this is my decision function.",
            "I drive now some comments.",
            "If I actually do this in practice.",
            "I will find somebody alphas are non zero and some of our zero Ashley the ones which are non zero alphas are the support vectors.",
            "If the office is 0 then there are non support vectors.",
            "In fact that's sort of clear because if I go back to my."
        ],
        [
            "Virtual picture OK, here it is.",
            "When I've done the learning task these non support vectors.",
            "If I remove them from the data set well it makes a difference.",
            "'cause that's separating hyperplane will be just where it is.",
            "If I remove a support vector then my separating hyperplane would shift OK.",
            "So as I expect when I actually do that optimization tasks some of the alphas are zero non support vectors, summer non zero.",
            "There are support vectors.",
            "Into"
        ],
        [
            "It is a little bit more interpretation.",
            "You can actually get out of a support out of this hypothesis than just are they support vectors or not.",
            "Often worth actually having a print out of these alphas, you will find some of 0 not support vectors.",
            "Some alphas are non 0 support vectors sometimes alphas can be large and value OK if it's large and value.",
            "What it means is that particular data point is having a big influence on where that separating hyperplane should be and that can be for two reasons.",
            "One is it's correct but just unusual data point.",
            "The other is it can be an outlier.",
            "OK I'll give you.",
            "An actual example.",
            "This used in practice.",
            "Some of my collaborators, MIT in Tagalog's Group uses support vector machine to distinguish two types of leukemia.",
            "Lymphoblastic and myeloid leukemia is OK, sometimes mixed together.",
            "Now, when they actually ran the support vector machine, they did find one patient with a large Alpha value, and as I said, could be an outlier or could be just correct, but unusual point.",
            "They went back to the medics and queried this particular sample and it had been a patient, have been wrongly classified in terms of leukemia.",
            "OK, so you can spot what's going on from the alphas this."
        ],
        [
            "Pops back to a comment I made at the beginning that we have much greater interpretability out of this model than.",
            "Then something new network OK now OK half eleven.",
            "I think I'll try and get the end at least soft margins.",
            "Then we might have a break right?",
            "So my story so far.",
            "It's been a nice simple story of data which is very separable.",
            "OK, somebody asked in the audience what happens when my data is not separable.",
            "The answers problem is to exploit a second point in that bound which I mentioned above.",
            "I said the theoretical generalization bound does not depend on the eventuality of the space.",
            "OK. And in fact there."
        ],
        [
            "All.",
            "This means I can map my data from input space into a high dimensional space and the reason why I want to do that.",
            "There are many reasons why do it, but one of them is that by mapping from a low dimensional space to high dimensional space, if my data is horribly intermeshed in a low dimensional space, then in a high dimensional space it'll be separable OK.",
            "Indeed, that's indeed what I'll be doing now.",
            "OK, is that always the case?",
            "If I map from non separated data into high dimensional space, it will separate.",
            "Well.",
            "I've always found it's always the case.",
            "Indeed, I think it's actually clear it would be achievable.",
            "Some of you may know the X or Oren parity problem.",
            "Don't have pens, quite sketch on the board but.",
            "If I consider a cube in which I move plus minus plus minus on all the edges, OK, that data is maximally intermeshed.",
            "OK now the simplest one is actually the X or problem, don't ever.",
            "Pen here I will just mention plus minus minus plus OK. Now I can't draw a line which separates the pluses and minus is OK.",
            "However if I go from that 2 dimensional space, two or three dimensional space and move the pluses into the board, the two minus is out.",
            "Then I can easy draw hyperplane that separates the two OK in three dimensions.",
            "I can do it easily in three, but I can't do plus, minus, minus.",
            "Plus I can't separate with them online in two.",
            "OK, so indeed if I go to this high dimensional space.",
            "I can separate the data and that's my trick to handle intermesh data.",
            "Indeed, I've never found a problem I can't handle using a support vector machine Anna, right kernel?",
            "Yes, a question.",
            "Yeah.",
            "If I typically use on come onto it a Gaussian kernel, which I'm bout to use a Gaussian kernels strictly an infinite dimensional space, so I've never I've done in Paraty two spirals problem, all manner, different, horrible problems with a Gaussian kernel.",
            "I've never found an instance where there's some sort of problem non zero training error.",
            "It may be the case, yes, yeah, and parity excellent would be the case.",
            "Yes, there are some problems where everything is a support vector.",
            "Typical real life datasets can be up down to 1/4 of the data or support vectors.",
            "Just depends on your data.",
            "OK, now OK, probably convince you it's a good idea to map to high dimensional space.",
            "Now one point of course is that high dimensional space.",
            "It's going to have to be an inner product space.",
            "OK, now let's just go back to."
        ],
        [
            "The object I had.",
            "Here we notice that my data was here as are marked.",
            "In particular, I could re define the exit.",
            "I could let X be X prime and X prime does yx OK.",
            "So if you like really all my data is in the form of a dot product X dot X. OK, so that's where my data appears."
        ],
        [
            "And hence the end of the day if I mapped to high dimensional space, I must map that inner product X dot X goes to Phi X Phi X and Phi will be my mapping function for input space to this higher dimensional space, which are called feature space.",
            "Now one of the old things about kernel methods.",
            "Is that I do not need to know what that mapping function is by choosing a kernel, which I'll do shortly, I implicitly define a mapping function, but I do not need to bother.",
            "I never know what that mapping function is.",
            "I don't care about it.",
            "OK, it implicitly does it.",
            "OK, so I'm saying yeah, better representation of data data can be achieved, particularly for not separable by mapping across the high dimensional space, and that's my mapping function and what I must do is map a dot Pro."
        ],
        [
            "OK, yeah."
        ],
        [
            "Multiplication.",
            "Uh, no, it would not be not necessarily 5.",
            "There is a.",
            "There's a little assumption we talking about.",
            "Yeah, I'm going to do it this way.",
            "I'm going to X goes to 5X.",
            "There is a requirement for me to be able to do that.",
            "One requirement or automatically is a space.",
            "IMAP into must have a dot product, so it must be an inner product space.",
            "It must actually be a pre Hilbert space, or indeed 'cause of connectivity.",
            "A Hilbert space which was defined earlier.",
            "OK, so there are restrictions on the space.",
            "I can map into, so I can't have total liberality in doing that.",
            "But I have a high degree of liberality, as I point out shortly."
        ],
        [
            "OK, but probably convince you I can do this mapping.",
            "The mapping function is Fi and I've just said that this high dimensional spaces in generality called a feature space and must be a Hilbert space because there must be a concept on inner product OK?",
            "In some places they actually.",
            "It could be a pre Hilbert space.",
            "OK, in the product space but."
        ],
        [
            "I would just say Hilbert space now.",
            "What is the mapping function?",
            "I've just said you don't need to know what it is because this map dot product this object here I'll set equal to the kernel kernel will just be the mapped data mapped.",
            "X is OK. Now I can define various kernels and by defining those kernels I implicitly define a mapping function, but I do not need to know what that mapping function is in certain simple cases, like if I do a polynomial type mapping.",
            "I can actually workout what, five years been generality?",
            "I don't know what it is and I don't care either, because provided I have certain restrictions on that choice of kernel then."
        ],
        [
            "Then it's all OK.",
            "So what is a mapping relation?",
            "I say here.",
            "In fact we do not need to know the formulas mapping because it's implicitly defined by the choice of kernel."
        ],
        [
            "Yeah.",
            "Yes, but like that.",
            "Um?",
            "Yes, it's some sort of well Pfizer function.",
            "It's a functional.",
            "A function which Maps my data like 1, one to some very high dimensional vector in some sort of unique way.",
            "OK, I don't.",
            "Yes, data, my data is mapped according to a function 5X to this high dimensional space.",
            "OK, now a lot of people get disturbed.",
            "Don't I really need to know what that Phi is?",
            "I'm just saying that no because.",
            "I don't have total freedom of choice about my kernel out there right now.",
            "Restrictions I'm coming into shortly, but let me suppose I define my kernel speed.",
            "This object here.",
            "OK, KX1X2 is equal to Gaussian.",
            "OK, it has a parameter in it talk about later, but if I choose that as my kernel, then implicitly defines what the Phi X is, but I don't need to care what that is.",
            "OK, now there are various choices for.",
            "Kernels OK, I'll give you typical examples of kernels and also the restrictions that apply to kernels.",
            "If I choose this kernel, this is a good choice.",
            "Usually often used as a coolant Gaussian kernel might call in RBF type kernel.",
            "This is one choice.",
            "Popular choice.",
            "OK. Another type of kernel I might choose might be a polynomial.",
            "OK like so there are popular choices.",
            "Now what you notice?",
            "If I choose a kernel?",
            "And it's a it's a RBF type kernel.",
            "Indeed, what I'm doing is implicitly giving you an RBF network.",
            "You probably heard of RBF networks.",
            "If I choose a polynomial kernel I'm giving you supplies from statistics.",
            "Another choice I can go for.",
            "Rarely used actually, don't even give it is attention choice for the kernel.",
            "If I choose attention, I generate a new network.",
            "OK, type of neural network.",
            "Indeed through my choices are kernels.",
            "I can actually give you a spline, some statistics, RBF networks.",
            "Neural networks arrange a different models which you would be familiar to with from previous."
        ],
        [
            "Previous interest in machine learning.",
            "OK now indeed.",
            "Legitimate kernels do not need to be defined by functions like a Gaussian OK.",
            "It's about kernels can also be defined by algorithms.",
            "OK, I'll give you the structure and curl in a little while, but let's consider the following situation.",
            "String kernels.",
            "Now let's take the following text strings Karkat cart chart you notice is a degree of similarity with these strings.",
            "OK, they've always got CAE in them and they got an R and Doug is of equal length the car.",
            "But it's really quite different.",
            "OK, indeed I can define things called edit codes.",
            "Levenstein codes used a lot for genetic information, OK to actually give me a score or number which tells me how similar text strings are to each other OK?",
            "That little algorithm for edit code actually satisfies the characteristics of a kernel, so it's a legitimate kernel to use in an SVM.",
            "OK, I'm saying therefore that there's a big class of kernels I can give you, and they will give you RBF networks, neural networks and so on, but they'll generate totally new types of learning machines that can handle text strings.",
            "If I consider a neural network, there have been introduced multilayer perceptron.",
            "One thing you notice about it has a fixed number of inputs, 40 inputs.",
            "Goes to the hidden layer and some outputs.",
            "Well, what do you do when you're handling text strings?",
            "When Ashley, the number of inputs would appear to be variable 5433, etc.",
            "What do I do about that?",
            "So I can't use the neural network very easily?",
            "Or if at all, when I'm handling text strings with text strings are hugely important and maybe reading a document and using my learning machine to do that, or I may be looking at genetic codes, CGH, etc trying to spot introns, exons or something?",
            "Then I must be able to handle strings of an equal length and I can use a string kernel."
        ],
        [
            "To do that?",
            "OK so I say here text processing by informatics you can use string."
        ],
        [
            "Reynolds now you can have other very interesting types of kernels and this makes this whole subject much more general.",
            "In particular, I can actually have a concept of similarity of graphs.",
            "OK, I've drawn a graph here, and again, I don't have a pen on the ball, but imagine I had two graphs.",
            "OK, I have certain number of nodes and I draw the links, put me out of my misery.",
            "Great so.",
            "If I have.",
            "Just a set of nodes.",
            "OK like so OK. Then obviously these two graphs this this, this these are quite similar there just one link difference between them.",
            "But like an easy draw another graph which is really very different.",
            "OK, I'm just saying intuitively you can easily see the must be some sort of distance measure between graph.",
            "Some graphs are similar, some not OK. Well, you could also derive a kernel.",
            "Start show called Diffusion Kernel Condor Lafferty, which will actually give you a measure of similarity between graphs and you can put that measure into your SVM.",
            "OK, so you can have a SVM that handles graph so you can have a big wide range of learning machines.",
            "Huawei might consider grass well again by informatics applications you often interested in say which genes are functionally connected to each other or not connected and you may wish to classify information.",
            "Based on pathways or networks in by informatics, so graphs are important.",
            "I don't say anything more."
        ],
        [
            "About graph kernels.",
            "So if you're interested, I can give you references, but there must be some sort of restrictions, and kernels are very broad spaces, such things and what are they?"
        ],
        [
            "Structions well, you'll find.",
            "Text books in the past, the destruction that on a legitimate kernel is it must satisfy Mercer's conditions.",
            "OK, here's the kernel OK, and if I choose any GX which is put it in here then this must be the case to define a legitimate kernel and that was Mercer's conditions.",
            "Indeed, actually better statement of it is what you notice is.",
            "This looks like the condition for positive definite, and in fact they can straighten.",
            "Curl is actually very simple one.",
            "The kernel must be a positive must be a positive semidefinite.",
            "OK, so if I have a kernel for which this is true, then I establish its positive semidefinite, then then it's a legitimate kernel.",
            "I shall not.",
            "I'm sure that it might not be positive definite.",
            "About that rap."
        ],
        [
            "Right, but that is a requirement.",
            "OK, so I've got here.",
            "A condition of positive semidefinite guess many will come across it CTK see if these Caesar reels and this is true then I have positive semidefinite and my kernel is OK. OK so that's the only requirement I must have on my kernel and all those previous kernels I gave you Gaussian kernel diffusion kernel."
        ],
        [
            "I'm able to establish that.",
            "Right, so I want to summarize the main steps, then this is how I approach the problem.",
            "I'm given some data and I wish to do binary classification.",
            "I choose my kernel.",
            "Now that may be an awkward point.",
            "I'm going to come back to later.",
            "Actually, what is my choice of kernel?",
            "Very high dimensional data.",
            "Often I just go for linear kernel.",
            "You haven't all stuck, might go for Gaussian kernel OK, but let me suppose I have chosen my chosen my kernel.",
            "Here it is here are introduced the data into the kernel and the wise.",
            "I maximize this object with respect to the alphas and these constraints having found the alphas I put them."
        ],
        [
            "Into my."
        ],
        [
            "That"
        ],
        [
            "Put them into my decision function, having also found the B which is my next slide.",
            "the B is my bias.",
            "I haven't really said anything about it, but it's relatively straightforward to argument to get what the B should be, but it is actually defined by this.",
            "OK, so I find the bias haven't got the bias, the alphas, the date."
        ],
        [
            "Sure, my choice of Colonel.",
            "They all go into my decision function.",
            "I made a choice of kernel.",
            "I've worked out the bias.",
            "I know this.",
            "I worked out the Alphaform optimization.",
            "I've done everything that is now my decision function, which I use.",
            "OK now I think it's 2211.",
            "More time was supposed to end.",
            "Coffee break.",
            "Oh to 10:30.",
            "If you wish you want a break now."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Press on yeah.",
                    "label": 0
                },
                {
                    "sent": "A license she's calling Campbell, who's and spends the day.",
                    "label": 0
                },
                {
                    "sent": "Support vector machines right?",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm calling Campbell from Bristol University.",
                    "label": 1
                },
                {
                    "sent": "This my first talks really the introduction support vector machine, so I'm just doing this sort of classical story started with binary classification, then multiclass classification, soft margins, that sort of stuff.",
                    "label": 0
                },
                {
                    "sent": "My second talk after a break.",
                    "label": 0
                },
                {
                    "sent": "I'll have to sort out what how long the break is, and so when we get to it, but would be on more general kernel methods.",
                    "label": 0
                },
                {
                    "sent": "In fact the idea kernel methods as it can define many.",
                    "label": 0
                },
                {
                    "sent": "Types of learning machines.",
                    "label": 0
                },
                {
                    "sent": "There are many types of kernels.",
                    "label": 0
                },
                {
                    "sent": "Many types of things you can do with these kernel based methods.",
                    "label": 0
                },
                {
                    "sent": "So this second talk after break I'm trying to make the subject much more general.",
                    "label": 0
                },
                {
                    "sent": "OK, this afternoon is opportunity to actually try out support vector machines.",
                    "label": 1
                },
                {
                    "sent": "Now that's a really sort of four levels.",
                    "label": 0
                },
                {
                    "sent": "I'll give a little talk when we come to it.",
                    "label": 0
                },
                {
                    "sent": "Simplest level is a nice little graphical thing done by Steve Gunn.",
                    "label": 0
                },
                {
                    "sent": "We click points on the screen and you see the actual support vector machine setting up a boundary to separate data.",
                    "label": 0
                },
                {
                    "sent": "And then there's various other options.",
                    "label": 0
                },
                {
                    "sent": "As a packager wrote myself, where you can use UCI repository data and download data and do classification type problems, and then finally, if you're familiar with MATLAB, you can try the quad prog program in the optimization toolbox and actually write your own SVM learner so we will come to that this afternoon, but this is support vector machines and my own interest.",
                    "label": 0
                },
                {
                    "sent": "I researched quite a lot on support vector machines in the past.",
                    "label": 0
                },
                {
                    "sent": "In the second talk, there will be quite a few bits which run my own research.",
                    "label": 0
                },
                {
                    "sent": "Seems like novelty detection.",
                    "label": 0
                },
                {
                    "sent": "I worked in a few things like that may mention a Bayes point machine I worked on, but my main interest at the moment is probabilistic graphical methods and also work a lot in applications to cancer.",
                    "label": 0
                },
                {
                    "sent": "In fact, which is a new subject called Cancer informatics right at the end I'll be using these support vector machines, for example to decide relapse versus non relapse, predicting the future whether cancer patient will have a relapse or not.",
                    "label": 0
                },
                {
                    "sent": "And it's quite successful problem I did with the staff and Institute for Cancer Research up in London so but will get on to applications that.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "End of the second talk.",
                    "label": 0
                },
                {
                    "sent": "So if you like this first talk, I want to first of all just do the simplest story with a support vector machine for binary classification and then this topic of soft margins.",
                    "label": 1
                },
                {
                    "sent": "I'll come on to that.",
                    "label": 0
                },
                {
                    "sent": "Then of course many real life applications are multi class so then elaborate.",
                    "label": 1
                },
                {
                    "sent": "This story of binary class to multiclass classification.",
                    "label": 0
                },
                {
                    "sent": "The schema uses by Nella Christy knees.",
                    "label": 0
                },
                {
                    "sent": "In my Department you did it with the John Platt.",
                    "label": 0
                },
                {
                    "sent": "And then briefly I'll also talk about regression so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's the plan for the first talk, the second talk.",
                    "label": 0
                },
                {
                    "sent": "In the first talk, I really use quadratic programming to learn the problem.",
                    "label": 0
                },
                {
                    "sent": "In the second talk, I want to really show that there's a bigger subject out there and it's not just quadratic programming.",
                    "label": 0
                },
                {
                    "sent": "You can do kernel methods with linear programming.",
                    "label": 1
                },
                {
                    "sent": "Indeed, sort of nonlinear optimization, and many other things.",
                    "label": 0
                },
                {
                    "sent": "I'll be talking about how to train SVM's.",
                    "label": 0
                },
                {
                    "sent": "Model selection there is potentially one parameter you would have in your SVM which is the kernel parameter.",
                    "label": 0
                },
                {
                    "sent": "How do you find it?",
                    "label": 0
                },
                {
                    "sent": "Well, you might find the data so the kernel parameter from the data using cross validation, but there are other ways of doing it from various theorems which had been established.",
                    "label": 1
                },
                {
                    "sent": "So this is the topic of model selection.",
                    "label": 0
                },
                {
                    "sent": "I'll come onto it then different types of kernels I'll come on to that and then I talked just now about actually using.",
                    "label": 1
                },
                {
                    "sent": "SPMS and practice insane medical applications.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's first of all start.",
                    "label": 0
                },
                {
                    "sent": "What are the advantages of SVM's?",
                    "label": 1
                },
                {
                    "sent": "Well, it's a principle approach to machine learning, in particular to all the various different types of machine learning, classification, regression, novelty detection, etc.",
                    "label": 0
                },
                {
                    "sent": "SVM, that's a bit good generalization and practice.",
                    "label": 0
                },
                {
                    "sent": "When I first encountered this subject which had been about New Year 2000, knowing that 98, I tried out.",
                    "label": 0
                },
                {
                    "sent": "I've been doing your networks at that point.",
                    "label": 0
                },
                {
                    "sent": "I tried out the multilayer perceptrons against an SVM.",
                    "label": 0
                },
                {
                    "sent": "Across a whole lot of UCI datasets, and my impression was for classification.",
                    "label": 0
                },
                {
                    "sent": "SVM was better than a multilayered perceptron, though.",
                    "label": 0
                },
                {
                    "sent": "For the regression they were fairly similar.",
                    "label": 0
                },
                {
                    "sent": "But in any case exhibits good generalization to new data, and the hypothesis has an explicit dependence on the data.",
                    "label": 1
                },
                {
                    "sent": "These of course are support vectors, which I come onto.",
                    "label": 0
                },
                {
                    "sent": "What I'm saying here is if I take something on your network model, its dependence on the data is not so clear, but.",
                    "label": 0
                },
                {
                    "sent": "I can interpret the dependence on the data quite simply from the model I generate, hence it can be readily interpreted so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First advantage is when I said principle approach to the subject.",
                    "label": 0
                },
                {
                    "sent": "Natural motivation for the subject to support vector machines really came from statistical learning theory.",
                    "label": 0
                },
                {
                    "sent": "Various bounds, which approved in that subject, and so unlike many other approaches to machine learning like neural networks, rather sort of ad hoc sort of approach.",
                    "label": 0
                },
                {
                    "sent": "Here you've got learning theory.",
                    "label": 0
                },
                {
                    "sent": "Learning theory motivates your approach, and your approach is successful.",
                    "label": 0
                },
                {
                    "sent": "Relatively few, if any, parameters you have to adjust so the operator doesn't come into it.",
                    "label": 0
                },
                {
                    "sent": "You simply percent of data an you're learning machine goes and learns it.",
                    "label": 0
                },
                {
                    "sent": "Also, another really big plus of this subject is the optimization task is convex.",
                    "label": 0
                },
                {
                    "sent": "Again, in contrast to a new network, the new network, typically the function I wish to optimize, has false minimum.",
                    "label": 0
                },
                {
                    "sent": "There has local minima.",
                    "label": 0
                },
                {
                    "sent": "OK, have a problem finding some to the global minimum will actually the task I wish to do with a support vector machine is a quadratic problem, so it's convex.",
                    "label": 0
                },
                {
                    "sent": "One solution, no local minima unlike a neural network, so that's a big plus for this approach.",
                    "label": 1
                },
                {
                    "sent": "Relatively few world very few parameters.",
                    "label": 0
                },
                {
                    "sent": "To adjust, potentially, there's a soft margin Anna kernel parameter.",
                    "label": 0
                },
                {
                    "sent": "If I'll do medical data like gene expression data are used later on, I don't use anything more than a linear kernel.",
                    "label": 0
                },
                {
                    "sent": "I don't have a kernel parameter, I don't have anything to adjust, unlike, say, a new network.",
                    "label": 0
                },
                {
                    "sent": "Again where I have questions about how many hidden nodes are there, how many hidden layers?",
                    "label": 0
                },
                {
                    "sent": "What do I do about some of the parameters in the updating functions, etc.",
                    "label": 1
                },
                {
                    "sent": "OK also can implement a.",
                    "label": 0
                },
                {
                    "sent": "Confidence measure very important for classifier.",
                    "label": 0
                },
                {
                    "sent": "It's not just as it says relapse or non relapse.",
                    "label": 0
                },
                {
                    "sent": "For my medical application, but it should be able to assign a probability to those two categories.",
                    "label": 0
                },
                {
                    "sent": "OK so these are all pluses of this holup.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Roche, so I want to first of all just recount the story for binary classification, OK?",
                    "label": 0
                },
                {
                    "sent": "And first of all, just some things in my notation.",
                    "label": 0
                },
                {
                    "sent": "My inputs will be this big X OK, potentially a vector.",
                    "label": 0
                },
                {
                    "sent": "The attributes have number attributes or components.",
                    "label": 0
                },
                {
                    "sent": "I will label the particular sample I'm considering an associated with this particular input.",
                    "label": 0
                },
                {
                    "sent": "I have a corresponding output.",
                    "label": 0
                },
                {
                    "sent": "Will should be denoted Yi OK and I'm just going to do binary classification to start with, so my outputs my labels.",
                    "label": 0
                },
                {
                    "sent": "I call them will be plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "OK, my medical application and talk about much further on will be a binary classification type task where plus will be the patient relapse minus we they won't relapse.",
                    "label": 0
                },
                {
                    "sent": "Now call these the labels.",
                    "label": 0
                },
                {
                    "sent": "It could also perhaps call them targets and these eyes are at range one to M where M is the number of samples or pairings of X&Y's.",
                    "label": 0
                },
                {
                    "sent": "OK, so these XI define a space which are called.",
                    "label": 1
                },
                {
                    "sent": "Input space OK, you've probably seen input spaces in the previous talks.",
                    "label": 0
                },
                {
                    "sent": "And the input space consists of labeled points.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Label plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "Now I did say at the beginning that's what's nice about the subject of support vector machines is it was motivated from statistical learning theory.",
                    "label": 0
                },
                {
                    "sent": "OK, an indeed social learning theory are just a little digression about learning theory.",
                    "label": 0
                },
                {
                    "sent": "There's various different approaches to learning theory.",
                    "label": 0
                },
                {
                    "sent": "One is sort of like a Bayesian type approach.",
                    "label": 0
                },
                {
                    "sent": "Another one is not so well known, but it's a physics you type approach.",
                    "label": 0
                },
                {
                    "sent": "It sort of decayed away in the literature.",
                    "label": 0
                },
                {
                    "sent": "But the physics or system mechanics type approach you have to you can establish learning curves and.",
                    "label": 0
                },
                {
                    "sent": "It involves complex calculations called replica calculations.",
                    "label": 0
                },
                {
                    "sent": "A third approach is just a school learning theory, where you devise bounds upper and lower bounds on the generalization error, and I won't go into this subject in great detail, but also you can drive these theoretical bounds on the generalization error OK, and I just I'm not going to state the bound mathematically, I'm just going to have a look at its implications.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I'm doing it some sort of learning task.",
                    "label": 0
                },
                {
                    "sent": "From handling real life data, I would evaluate it on test data and then give me the test error and the corresponding theoretical test error.",
                    "label": 0
                },
                {
                    "sent": "I call the generalization error OK, so that generalization error from statistical learning theory I can draw.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My bounds on it and the bound I'm interested in.",
                    "label": 0
                },
                {
                    "sent": "It has two properties.",
                    "label": 0
                },
                {
                    "sent": "OK, the upper bound on the generalization error does not depend on the dimension of the space.",
                    "label": 1
                },
                {
                    "sent": "OK, that's one observation.",
                    "label": 0
                },
                {
                    "sent": "I'm going to make the bound is independent and actual size of the input space I'm considering.",
                    "label": 1
                },
                {
                    "sent": "Observation #2 is that the bound is minimized by maximizing an object called the margin, which are the notice gamma, which is a minimal distance between the hyperplane separating two classes and a two and the closest data points belonging to each class.",
                    "label": 0
                },
                {
                    "sent": "All that's best illustrated.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With a picture OK and here it is.",
                    "label": 0
                },
                {
                    "sent": "So here I've just got binary classification, two types of labeled points, plus or minus one, and in this case everything is quite nice because the two types of data are well separated.",
                    "label": 0
                },
                {
                    "sent": "Here they are OK and my classifier is a line in two dimensions or in N dimensions and dimensional input space is a hyperplane OK?",
                    "label": 0
                },
                {
                    "sent": "Appointment and the 2nd but the the classifier is a hyperplane, it's a directed hyperplane, in which anything lying on this side of the hyperplane given in Brown here will be labeled as plus one positive and everything on this side of my hyperplane will be labeled as negative.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a directed hyperplane.",
                    "label": 0
                },
                {
                    "sent": "Now what the?",
                    "label": 0
                },
                {
                    "sent": "So all the classifiers?",
                    "label": 0
                },
                {
                    "sent": "Well, we can be viewed as such.",
                    "label": 0
                },
                {
                    "sent": "And what the theorem therefore says is that my best classifier is that hyperplane or classifier which is maximally distant from the closest points on both sides, well, actually, intuitively that seems pretty clear, because suppose my hyperplane was like so OK separates the data correctly, but is actually quite close to some points and doesn't seem to be intuitively quite as nice as this thing.",
                    "label": 0
                },
                {
                    "sent": "Then again, if I came along with the classifier.",
                    "label": 0
                },
                {
                    "sent": "Like so we'll obviously it's going to get some points wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, so there will be a lot of classifiers I can devise.",
                    "label": 0
                },
                {
                    "sent": "Some will just simply be wrong.",
                    "label": 0
                },
                {
                    "sent": "They'll give me an error on my training data.",
                    "label": 0
                },
                {
                    "sent": "There will be a number which will correctly separate the data and it seems intuitive that in fact the best hyperplane I could find is this hyperplane which is maximally distant from the closest points on both sides.",
                    "label": 0
                },
                {
                    "sent": "For this very nicely separated data, OK indeed.",
                    "label": 0
                },
                {
                    "sent": "These closest points here are the support vectors.",
                    "label": 0
                },
                {
                    "sent": "They're called support vectors.",
                    "label": 0
                },
                {
                    "sent": "Will affect, of course, 'cause really I'm in a N dimensional input space, so their vector they are called support vectors.",
                    "label": 0
                },
                {
                    "sent": "'cause if you like they support where the hyperplane should be Huawei, because if I start to removing these guys here then my optimally separating hyperplane would shift OK.",
                    "label": 0
                },
                {
                    "sent": "Whereas if I remove these ones non support vectors, well they were making a difference.",
                    "label": 0
                },
                {
                    "sent": "I can remove them and my maximally separating hyperplane will just sit where it is.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah?",
                    "label": 0
                },
                {
                    "sent": "I'll come on to that.",
                    "label": 0
                },
                {
                    "sent": "I come onto it so my story at the moment is quite simple, nicely separated data, OK, and I'm going to mess the picture up shortly, right?",
                    "label": 0
                },
                {
                    "sent": "So these are support vectors.",
                    "label": 0
                },
                {
                    "sent": "Remove them.",
                    "label": 0
                },
                {
                    "sent": "My separating hyperplane would shift remove these doesn't, so that's why I called them support vectors.",
                    "label": 0
                },
                {
                    "sent": "And finally I call it a support vector machine because it's a learning machine.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's sort.",
                    "label": 0
                },
                {
                    "sent": "Magical, so that's.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the geometric picture.",
                    "label": 0
                },
                {
                    "sent": "Now I actually want to formally drive the story of the support vector machine and first of all we note that that separating hyperplane is represented like so.",
                    "label": 1
                },
                {
                    "sent": "So if you go back to your first year mathematics which did universal or school, then you would know that hyperplane can be written like so.",
                    "label": 1
                },
                {
                    "sent": "Generally you would have it in a textbook as N dot X = C. The equation one equation.",
                    "label": 0
                },
                {
                    "sent": "For hyperplane or playing OK, and I've taken the see over to the side, but this is an equation of a plane and X is my day to be as written solely because of the bias.",
                    "label": 0
                },
                {
                    "sent": "OK could be if I move it to other side threshold W if I normalize it will actually be the normal to the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "OK, so I haven't got a pen here, but I'm just using the formal definition of a hyperplane N dot X = C. Where N is the normal type of pain, so call this the bias.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll call these the weights OK and this thing here goes into my decision function like so looks just like any sort of argument of something like a new network decision function and hence I put my data point in here.",
                    "label": 0
                },
                {
                    "sent": "Perhaps a new data point work this thing out.",
                    "label": 0
                },
                {
                    "sent": "If it's positive I get a plus one out, it was negative again minus one out.",
                    "label": 1
                },
                {
                    "sent": "So that's my decision function.",
                    "label": 0
                },
                {
                    "sent": "Like so OK.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I know one thing about this.",
                    "label": 0
                },
                {
                    "sent": "Decision function here.",
                    "label": 0
                },
                {
                    "sent": "This sign function is influenced by the sign of the subject here, but not by its magnitude.",
                    "label": 1
                },
                {
                    "sent": "In other words, this decision function is left invariant if I scale the WS and the bees, but any positive quantity.",
                    "label": 0
                },
                {
                    "sent": "OK, we observe that any multiply this and this by two.",
                    "label": 0
                },
                {
                    "sent": "Then nothing happens.",
                    "label": 0
                },
                {
                    "sent": "So there's an invariant.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In my sort of domain.",
                    "label": 0
                },
                {
                    "sent": "OK, so decision function is left invariant under any positive should say under positive rescaling of my W Zombies.",
                    "label": 0
                },
                {
                    "sent": "Now we want to implicitly fix a scale.",
                    "label": 1
                },
                {
                    "sent": "In fact, why are we actually wanting to implicitly fix the scale?",
                    "label": 0
                },
                {
                    "sent": "Well, let's just go back to our little geometric picture.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is here I said according to my theorem from Cisco learning theory.",
                    "label": 0
                },
                {
                    "sent": "My just best generalization is achieved by maximizing the margin or the distance between that separating hyperplane and the closest points.",
                    "label": 0
                },
                {
                    "sent": "So this margin is actually a distance measure.",
                    "label": 0
                },
                {
                    "sent": "OK, because it's a distance measuring this input space, I must actually fix the scale of my input space, sort of my space.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Invariants causes me a sort of problem.",
                    "label": 0
                },
                {
                    "sent": "Indeed, I'm going to actually implicitly fix a scale so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going wrong way.",
                    "label": 0
                },
                {
                    "sent": "I know there's an in variance in here, OK?",
                    "label": 0
                },
                {
                    "sent": "And I'm actually going to implicitly fix a Lambda value in order to make this distance.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here one OK here.",
                    "label": 0
                },
                {
                    "sent": "I'm gonna actually those invariants.",
                    "label": 0
                },
                {
                    "sent": "I'm actually going to dictate that this distance from here to here is a value one.",
                    "label": 0
                },
                {
                    "sent": "Or if you like from here to here, that distance is of value one.",
                    "label": 0
                },
                {
                    "sent": "OK, how do I do that well?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So say I'm going to for those support vectors on each side on one side of my directed hyperplane.",
                    "label": 1
                },
                {
                    "sent": "This object here will be equal to plus one if I'm a support vector on one side and a -- 1 if I'm a support vector on the other side that dictating this condition here defines what is called a Canonical hyperplane and what I'm doing is implicitly fixing a Lambda value.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So just going back again.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Picture here I've now dictated that this guy here W X + B will be minus one.",
                    "label": 0
                },
                {
                    "sent": "This will be greater than minus one WX plus B we plus one and this will be more than plus one.",
                    "label": 0
                },
                {
                    "sent": "OK and implicitly I've fix.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At scale.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have these two equations for support vectors on both sides of my hyperplane.",
                    "label": 1
                },
                {
                    "sent": "OK and that define.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to be by Canonical playing now.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One thing I notice, of course, is I can subtract this from this.",
                    "label": 0
                },
                {
                    "sent": "It would get rid of the bees and give me a 2 on the other side.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just subtracting one equation from the other, I get this equation for support vectors on both sides OK.",
                    "label": 1
                },
                {
                    "sent": "Right, so we know that.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also note the following.",
                    "label": 0
                },
                {
                    "sent": "The margin is given by the projection of a vector X1 X 1 -- X two onto the normal to the hyperplane OK?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's just show this is a.",
                    "label": 0
                },
                {
                    "sent": "Sort of picture.",
                    "label": 0
                },
                {
                    "sent": "This is my margin.",
                    "label": 0
                },
                {
                    "sent": "I want to maximize the margin I want to find a hyperplane maximum distance for the closest points on both sides.",
                    "label": 0
                },
                {
                    "sent": "OK, now let me choose two support vectors.",
                    "label": 0
                },
                {
                    "sent": "Let this be X1 and this guy here be X2.",
                    "label": 0
                },
                {
                    "sent": "OK, well obviously that to 9 drawn in red is a vector connecting those two points.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm saying the margin.",
                    "label": 0
                },
                {
                    "sent": "01 further observation I have this separating hyperplane that's given by W dot X + B = 0, While the normal that hyperplane is W hat.",
                    "label": 1
                },
                {
                    "sent": "OK, just that's definitely from planes.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm saying that the margin the singing black is given by the projection of this vector onto the onto model W hat.",
                    "label": 0
                },
                {
                    "sent": "That's a normal to hyperplane if I project this vector onto the green normal, then I'll get the margin OK. Just plane geometry projections OK, nothing mysterious.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's what I do now.",
                    "label": 0
                },
                {
                    "sent": "I will project W 1 -- W two onto the normal normal OK, which is just given by W over MoD W. That's a normal to hyperplane.",
                    "label": 1
                },
                {
                    "sent": "I observed therefore the projection just from plane geometry will be X 1 -- X two times this W. Sorry, dotted with W MoD W. But just coming back to my previous account.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vision OK, I had already established this quite separately, so the projection of this onto W over MoD W will be equal to two over MoD W. OK, so just divide both sides by W and I'm really looking at the projection of this vector onto the normal hyperplane and hints that the margin is equal to two on MoD W OK?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sweeping the following what I've been natural gone about, then I've actually argued that the margin is given well.",
                    "label": 1
                },
                {
                    "sent": "Of what I've argued.",
                    "label": 0
                },
                {
                    "sent": "Axe",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really.",
                    "label": 0
                },
                {
                    "sent": "Is that this?",
                    "label": 0
                },
                {
                    "sent": "Is 2 or more W?",
                    "label": 0
                },
                {
                    "sent": "Actually, if I define the margin, I should really say the margins between here and here is one or more W OK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm saying.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the day that the distance.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Losing myself here.",
                    "label": 0
                },
                {
                    "sent": "The distance between separating hyperplane and the support vectors is actually given by one or MoD W, OK?",
                    "label": 1
                },
                {
                    "sent": "So if I want to maximize the margin.",
                    "label": 1
                },
                {
                    "sent": "Then I must maximize this object here, OK?",
                    "label": 0
                },
                {
                    "sent": "Make it as big as possible.",
                    "label": 0
                },
                {
                    "sent": "Now maximizing one or more W is the same thing as minimizing MoD W OK.",
                    "label": 0
                },
                {
                    "sent": "Maximize this is equivalent to minimizing just this OK.",
                    "label": 0
                },
                {
                    "sent": "So we've.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Got to the following.",
                    "label": 0
                },
                {
                    "sent": "Maximization of the of the margin OK is equivalent, and minimization of MoD W well minimizing MoD W is going to be the same sort of thing as minimizing WW.",
                    "label": 1
                },
                {
                    "sent": "OK sort of forgot the square root somewhere, but you can therefore imagine if I'm trying to maximize my margin is equivalent to minimizing.",
                    "label": 1
                },
                {
                    "sent": "Therefore this thing OK, but subject to constraints and the constraints I already had.",
                    "label": 0
                },
                {
                    "sent": "I had if I'm.",
                    "label": 0
                },
                {
                    "sent": "This object here was a plus.",
                    "label": 0
                },
                {
                    "sent": "One OK when y = + 1.",
                    "label": 0
                },
                {
                    "sent": "This was a plus.",
                    "label": 0
                },
                {
                    "sent": "One efforts or support vector, and if it wasn't support vector and on that same side of the hyperplane is greater than plus one.",
                    "label": 0
                },
                {
                    "sent": "OK so if this is a plus one this thing will be greater than or equal to plus one if I'm on the other side of the hyperplane from the support vectors minus one.",
                    "label": 0
                },
                {
                    "sent": "And if it's a non support vector it will be greater than minus one for the case of Y = -- 1.",
                    "label": 0
                },
                {
                    "sent": "On that side of the hyperplane, the end of the day that dictation of my Canonical hyperplanes can be written down as this constraint equation here OK.",
                    "label": 0
                },
                {
                    "sent": "So at the end of the day, it's anybody losses how I got either of these two.",
                    "label": 0
                },
                {
                    "sent": "OK, I defined Canonical High Plains that gave me this defined maximize my margin, which gave me this.",
                    "label": 0
                },
                {
                    "sent": "OK, I now have a constraint optimization problem because I'm trying to minimize subjects.",
                    "label": 0
                },
                {
                    "sent": "Something subject to constraints.",
                    "label": 0
                },
                {
                    "sent": "OK, that immediately sends up a little flag in my brain constrained optimization.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Indeed, I hope most people are familiar with optimization.",
                    "label": 0
                },
                {
                    "sent": "If not, doesn't really matter, but.",
                    "label": 0
                },
                {
                    "sent": "I'll just.",
                    "label": 0
                },
                {
                    "sent": "I'll just say, therefore I now approach it from the viewpoint of optimization I set down was called objective function, and that objective function has this object which I'm trying to minimize, and this is my constraint condition OK, which I had just now.",
                    "label": 0
                },
                {
                    "sent": "And these alphas are LaGrange multipliers which are introduced because it's constrained optimization.",
                    "label": 0
                },
                {
                    "sent": "So you've either heard a little branch multipliers you sell.",
                    "label": 0
                },
                {
                    "sent": "This is totally obvious, or you happen, in which case you lossed right?",
                    "label": 0
                },
                {
                    "sent": "So these are LaGrange multipliers, so this object is my primal objective function OK?",
                    "label": 0
                },
                {
                    "sent": "Now again, if you're familiar optimization, you'll say, Oh well, there's a a primal formulation of a problem and an equivalent dual formulation of the problem, and it's going to be the dual formulation.",
                    "label": 0
                },
                {
                    "sent": "I'm going to go for, and, but if you don't know anything about that doesn't matter, 'cause I'll show you just some derivatives to find solution, which will take me there.",
                    "label": 0
                },
                {
                    "sent": "So this is my object.",
                    "label": 0
                },
                {
                    "sent": "I've arrived at my primal objective function.",
                    "label": 1
                },
                {
                    "sent": "I'm going to do 2 operations, which is.",
                    "label": 0
                },
                {
                    "sent": "I wish to find an optimum.",
                    "label": 1
                },
                {
                    "sent": "Well actually minimum of this L here at the optimum.",
                    "label": 0
                },
                {
                    "sent": "What I would expect is DL by DB equals zero and the other variable I have which is the Elbe IDW to be 0.",
                    "label": 0
                },
                {
                    "sent": "Those would be conditions or my gradient had expected the optimum.",
                    "label": 0
                },
                {
                    "sent": "Let's have a look at those, so the optimum I want DL by DB equals zero.",
                    "label": 0
                },
                {
                    "sent": "Well that's the only be here that gives me this constraint here OK?",
                    "label": 0
                },
                {
                    "sent": "Secondly,",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I do deal by DW deal by DW.",
                    "label": 0
                },
                {
                    "sent": "Can quickly check that gives me W is equal to this thing here OK?",
                    "label": 0
                },
                {
                    "sent": "My next thing is to substitute back this W into my previous equation.",
                    "label": 0
                },
                {
                    "sent": "OK, if you're totally fine.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optimization this is all strikingly obvious because it's working out a Wolf jewel.",
                    "label": 0
                },
                {
                    "sent": "If you're not, well, just is OK, so I put back the WI just arrived in here, here and here.",
                    "label": 0
                },
                {
                    "sent": "OK, and that would actually give me.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uh.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Following it gives me at the end of the day this expression here, which is the jewels brush.",
                    "label": 0
                },
                {
                    "sent": "And why am I doing this jewel?",
                    "label": 0
                },
                {
                    "sent": "Well one thing notice is it's quadratic programming.",
                    "label": 0
                },
                {
                    "sent": "OK my thing which which to optimize is going to be this Alpha.",
                    "label": 0
                },
                {
                    "sent": "This quadratic looks like this.",
                    "label": 0
                },
                {
                    "sent": "OK the prime loss started from was also actually quadratic in W. OK look like this and there's a. Furman optimization theory.",
                    "label": 0
                },
                {
                    "sent": "For every primal formulation, there's a dual formulation.",
                    "label": 0
                },
                {
                    "sent": "If you solve the, obtain a solution of the primal, it's the same solution as a solution of the jewel.",
                    "label": 0
                },
                {
                    "sent": "OK, this is actually the jewel.",
                    "label": 0
                },
                {
                    "sent": "OK, now for getting all the story that led up to this point.",
                    "label": 0
                },
                {
                    "sent": "OK, I've never arrived at point.",
                    "label": 0
                },
                {
                    "sent": "I wanted to arrive at.",
                    "label": 0
                },
                {
                    "sent": "This is the object I optimize to find solution for a support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Some comments.",
                    "label": 0
                },
                {
                    "sent": "First of all, what you notice is the data.",
                    "label": 0
                },
                {
                    "sent": "Appears in this block here.",
                    "label": 0
                },
                {
                    "sent": "OK, this would be my input vector.",
                    "label": 0
                },
                {
                    "sent": "My X and number of components in it.",
                    "label": 0
                },
                {
                    "sent": "And why would be associated labels?",
                    "label": 0
                },
                {
                    "sent": "But that's the data lump OK.",
                    "label": 0
                },
                {
                    "sent": "The task I wish to do this is my previous task with them in task.",
                    "label": 0
                },
                {
                    "sent": "The dual task is actually a Max task.",
                    "label": 0
                },
                {
                    "sent": "I wish to maximize this object subject to constraints I come onto in a second, but a maximizing the Alpha.",
                    "label": 0
                },
                {
                    "sent": "After all, it's the only thing I can maximize in because the rest of it's just data.",
                    "label": 0
                },
                {
                    "sent": "OK, so I maximize this quadratic in Alpha.",
                    "label": 0
                },
                {
                    "sent": "OK, now it is constrained optimization.",
                    "label": 0
                },
                {
                    "sent": "First of all, these alphas were actually LaGrange multipliers and requirement on growth multipliers.",
                    "label": 1
                },
                {
                    "sent": "They must be positive, OK?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Secondly, there was this condition from DL by DB that this equals 0, so it is constrained optimization OK.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now there's comments.",
                    "label": 0
                },
                {
                    "sent": "Data comes in here and it's quadratic an there's a several convex problem.",
                    "label": 0
                },
                {
                    "sent": "Therefore it looks a bit like this.",
                    "label": 0
                },
                {
                    "sent": "There is one solution, it's not like a new network or whole approaches to machine learning, where unfortunately different starting point gets you a different solution.",
                    "label": 0
                },
                {
                    "sent": "Different starting points will only get you the one solution, OK, which it will be this maximally separating hyperplane.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "Why this is?",
                    "label": 0
                },
                {
                    "sent": "Well, roughly, roughly this is Alpha squared, and that's linear in Alpha, so this is a quadratic yes.",
                    "label": 0
                },
                {
                    "sent": "Is a quadratic type expression.",
                    "label": 0
                },
                {
                    "sent": "So multi dimensional quadratic.",
                    "label": 0
                },
                {
                    "sent": "Oh, quadratic optimization.",
                    "label": 0
                },
                {
                    "sent": "Well, you've got within optimization theory.",
                    "label": 0
                },
                {
                    "sent": "You got linear programming, which would be linear in Alpha.",
                    "label": 0
                },
                {
                    "sent": "I'll come on to that later.",
                    "label": 0
                },
                {
                    "sent": "OK, you got quadratic optimization, which means roughly Alpha squared times expressions.",
                    "label": 0
                },
                {
                    "sent": "You could have something horrible root Alpha and things like that.",
                    "label": 0
                },
                {
                    "sent": "That would be some sort of nonlinear optimization.",
                    "label": 0
                },
                {
                    "sent": "Now, for quadratic programming, there's been a lot of work on trying to find very efficient routines with quadratic programming.",
                    "label": 0
                },
                {
                    "sent": "You possibly have things like conjugate gradient method and so on.",
                    "label": 0
                },
                {
                    "sent": "There actually first developed for quadratic programming, so another big plus is that there's pretty rapid routines to find the optimum of this expression W OK indeed.",
                    "label": 0
                },
                {
                    "sent": "Such as the case if I had well, I've actually done this with 60,000 data points in there personally, but I know people have gone way beyond that.",
                    "label": 0
                },
                {
                    "sent": "So that means I can have EM is 60,000.",
                    "label": 0
                },
                {
                    "sent": "I've done it with Postal data.",
                    "label": 0
                },
                {
                    "sent": "OK, now that's because I can use very fast routines from quadratic programming, and if you use something many other approaches to machine learning and you'll network, try anul network with a million data points some like.",
                    "label": 0
                },
                {
                    "sent": "Back propagation, it will take you some time.",
                    "label": 0
                },
                {
                    "sent": "I can tell you so it's one big pluses.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Play rapid.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Might ask for support vector machines.",
                    "label": 0
                },
                {
                    "sent": "You could do this this afternoon if you wish.",
                    "label": 0
                },
                {
                    "sent": "Actually program this thing up using quad prog in MATLAB if you're familiar, but there is to actually optimize this with respect to Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Subject to the constraints, having found those alphas, I plant them in here OK, and then I may have now a new test data point.",
                    "label": 0
                },
                {
                    "sent": "I wanted to know what the answer is that test data point comes in here said I got it with my data and my ex is.",
                    "label": 0
                },
                {
                    "sent": "I have my Alpha so just found for my constrained quadratic programming I have Y which are my labels be our total greatness.",
                    "label": 0
                },
                {
                    "sent": "Second but I therefore have set up my decision function and it will make a decision.",
                    "label": 0
                },
                {
                    "sent": "On this new datapoint, said OK, so this is my decision function.",
                    "label": 1
                },
                {
                    "sent": "I drive now some comments.",
                    "label": 0
                },
                {
                    "sent": "If I actually do this in practice.",
                    "label": 0
                },
                {
                    "sent": "I will find somebody alphas are non zero and some of our zero Ashley the ones which are non zero alphas are the support vectors.",
                    "label": 0
                },
                {
                    "sent": "If the office is 0 then there are non support vectors.",
                    "label": 0
                },
                {
                    "sent": "In fact that's sort of clear because if I go back to my.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Virtual picture OK, here it is.",
                    "label": 0
                },
                {
                    "sent": "When I've done the learning task these non support vectors.",
                    "label": 0
                },
                {
                    "sent": "If I remove them from the data set well it makes a difference.",
                    "label": 0
                },
                {
                    "sent": "'cause that's separating hyperplane will be just where it is.",
                    "label": 1
                },
                {
                    "sent": "If I remove a support vector then my separating hyperplane would shift OK.",
                    "label": 0
                },
                {
                    "sent": "So as I expect when I actually do that optimization tasks some of the alphas are zero non support vectors, summer non zero.",
                    "label": 0
                },
                {
                    "sent": "There are support vectors.",
                    "label": 0
                },
                {
                    "sent": "Into",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is a little bit more interpretation.",
                    "label": 0
                },
                {
                    "sent": "You can actually get out of a support out of this hypothesis than just are they support vectors or not.",
                    "label": 0
                },
                {
                    "sent": "Often worth actually having a print out of these alphas, you will find some of 0 not support vectors.",
                    "label": 0
                },
                {
                    "sent": "Some alphas are non 0 support vectors sometimes alphas can be large and value OK if it's large and value.",
                    "label": 0
                },
                {
                    "sent": "What it means is that particular data point is having a big influence on where that separating hyperplane should be and that can be for two reasons.",
                    "label": 0
                },
                {
                    "sent": "One is it's correct but just unusual data point.",
                    "label": 0
                },
                {
                    "sent": "The other is it can be an outlier.",
                    "label": 0
                },
                {
                    "sent": "OK I'll give you.",
                    "label": 0
                },
                {
                    "sent": "An actual example.",
                    "label": 0
                },
                {
                    "sent": "This used in practice.",
                    "label": 0
                },
                {
                    "sent": "Some of my collaborators, MIT in Tagalog's Group uses support vector machine to distinguish two types of leukemia.",
                    "label": 0
                },
                {
                    "sent": "Lymphoblastic and myeloid leukemia is OK, sometimes mixed together.",
                    "label": 0
                },
                {
                    "sent": "Now, when they actually ran the support vector machine, they did find one patient with a large Alpha value, and as I said, could be an outlier or could be just correct, but unusual point.",
                    "label": 0
                },
                {
                    "sent": "They went back to the medics and queried this particular sample and it had been a patient, have been wrongly classified in terms of leukemia.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can spot what's going on from the alphas this.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pops back to a comment I made at the beginning that we have much greater interpretability out of this model than.",
                    "label": 0
                },
                {
                    "sent": "Then something new network OK now OK half eleven.",
                    "label": 0
                },
                {
                    "sent": "I think I'll try and get the end at least soft margins.",
                    "label": 0
                },
                {
                    "sent": "Then we might have a break right?",
                    "label": 0
                },
                {
                    "sent": "So my story so far.",
                    "label": 1
                },
                {
                    "sent": "It's been a nice simple story of data which is very separable.",
                    "label": 0
                },
                {
                    "sent": "OK, somebody asked in the audience what happens when my data is not separable.",
                    "label": 0
                },
                {
                    "sent": "The answers problem is to exploit a second point in that bound which I mentioned above.",
                    "label": 0
                },
                {
                    "sent": "I said the theoretical generalization bound does not depend on the eventuality of the space.",
                    "label": 1
                },
                {
                    "sent": "OK. And in fact there.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All.",
                    "label": 0
                },
                {
                    "sent": "This means I can map my data from input space into a high dimensional space and the reason why I want to do that.",
                    "label": 0
                },
                {
                    "sent": "There are many reasons why do it, but one of them is that by mapping from a low dimensional space to high dimensional space, if my data is horribly intermeshed in a low dimensional space, then in a high dimensional space it'll be separable OK.",
                    "label": 0
                },
                {
                    "sent": "Indeed, that's indeed what I'll be doing now.",
                    "label": 0
                },
                {
                    "sent": "OK, is that always the case?",
                    "label": 0
                },
                {
                    "sent": "If I map from non separated data into high dimensional space, it will separate.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "I've always found it's always the case.",
                    "label": 0
                },
                {
                    "sent": "Indeed, I think it's actually clear it would be achievable.",
                    "label": 0
                },
                {
                    "sent": "Some of you may know the X or Oren parity problem.",
                    "label": 0
                },
                {
                    "sent": "Don't have pens, quite sketch on the board but.",
                    "label": 0
                },
                {
                    "sent": "If I consider a cube in which I move plus minus plus minus on all the edges, OK, that data is maximally intermeshed.",
                    "label": 0
                },
                {
                    "sent": "OK now the simplest one is actually the X or problem, don't ever.",
                    "label": 0
                },
                {
                    "sent": "Pen here I will just mention plus minus minus plus OK. Now I can't draw a line which separates the pluses and minus is OK.",
                    "label": 0
                },
                {
                    "sent": "However if I go from that 2 dimensional space, two or three dimensional space and move the pluses into the board, the two minus is out.",
                    "label": 0
                },
                {
                    "sent": "Then I can easy draw hyperplane that separates the two OK in three dimensions.",
                    "label": 0
                },
                {
                    "sent": "I can do it easily in three, but I can't do plus, minus, minus.",
                    "label": 0
                },
                {
                    "sent": "Plus I can't separate with them online in two.",
                    "label": 0
                },
                {
                    "sent": "OK, so indeed if I go to this high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "I can separate the data and that's my trick to handle intermesh data.",
                    "label": 0
                },
                {
                    "sent": "Indeed, I've never found a problem I can't handle using a support vector machine Anna, right kernel?",
                    "label": 0
                },
                {
                    "sent": "Yes, a question.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "If I typically use on come onto it a Gaussian kernel, which I'm bout to use a Gaussian kernels strictly an infinite dimensional space, so I've never I've done in Paraty two spirals problem, all manner, different, horrible problems with a Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "I've never found an instance where there's some sort of problem non zero training error.",
                    "label": 0
                },
                {
                    "sent": "It may be the case, yes, yeah, and parity excellent would be the case.",
                    "label": 0
                },
                {
                    "sent": "Yes, there are some problems where everything is a support vector.",
                    "label": 0
                },
                {
                    "sent": "Typical real life datasets can be up down to 1/4 of the data or support vectors.",
                    "label": 1
                },
                {
                    "sent": "Just depends on your data.",
                    "label": 0
                },
                {
                    "sent": "OK, now OK, probably convince you it's a good idea to map to high dimensional space.",
                    "label": 1
                },
                {
                    "sent": "Now one point of course is that high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "It's going to have to be an inner product space.",
                    "label": 1
                },
                {
                    "sent": "OK, now let's just go back to.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The object I had.",
                    "label": 0
                },
                {
                    "sent": "Here we notice that my data was here as are marked.",
                    "label": 0
                },
                {
                    "sent": "In particular, I could re define the exit.",
                    "label": 0
                },
                {
                    "sent": "I could let X be X prime and X prime does yx OK.",
                    "label": 0
                },
                {
                    "sent": "So if you like really all my data is in the form of a dot product X dot X. OK, so that's where my data appears.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And hence the end of the day if I mapped to high dimensional space, I must map that inner product X dot X goes to Phi X Phi X and Phi will be my mapping function for input space to this higher dimensional space, which are called feature space.",
                    "label": 1
                },
                {
                    "sent": "Now one of the old things about kernel methods.",
                    "label": 0
                },
                {
                    "sent": "Is that I do not need to know what that mapping function is by choosing a kernel, which I'll do shortly, I implicitly define a mapping function, but I do not need to bother.",
                    "label": 0
                },
                {
                    "sent": "I never know what that mapping function is.",
                    "label": 0
                },
                {
                    "sent": "I don't care about it.",
                    "label": 0
                },
                {
                    "sent": "OK, it implicitly does it.",
                    "label": 1
                },
                {
                    "sent": "OK, so I'm saying yeah, better representation of data data can be achieved, particularly for not separable by mapping across the high dimensional space, and that's my mapping function and what I must do is map a dot Pro.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Multiplication.",
                    "label": 0
                },
                {
                    "sent": "Uh, no, it would not be not necessarily 5.",
                    "label": 0
                },
                {
                    "sent": "There is a.",
                    "label": 0
                },
                {
                    "sent": "There's a little assumption we talking about.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm going to do it this way.",
                    "label": 0
                },
                {
                    "sent": "I'm going to X goes to 5X.",
                    "label": 0
                },
                {
                    "sent": "There is a requirement for me to be able to do that.",
                    "label": 0
                },
                {
                    "sent": "One requirement or automatically is a space.",
                    "label": 0
                },
                {
                    "sent": "IMAP into must have a dot product, so it must be an inner product space.",
                    "label": 0
                },
                {
                    "sent": "It must actually be a pre Hilbert space, or indeed 'cause of connectivity.",
                    "label": 0
                },
                {
                    "sent": "A Hilbert space which was defined earlier.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are restrictions on the space.",
                    "label": 0
                },
                {
                    "sent": "I can map into, so I can't have total liberality in doing that.",
                    "label": 0
                },
                {
                    "sent": "But I have a high degree of liberality, as I point out shortly.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, but probably convince you I can do this mapping.",
                    "label": 0
                },
                {
                    "sent": "The mapping function is Fi and I've just said that this high dimensional spaces in generality called a feature space and must be a Hilbert space because there must be a concept on inner product OK?",
                    "label": 1
                },
                {
                    "sent": "In some places they actually.",
                    "label": 0
                },
                {
                    "sent": "It could be a pre Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "OK, in the product space but.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I would just say Hilbert space now.",
                    "label": 0
                },
                {
                    "sent": "What is the mapping function?",
                    "label": 0
                },
                {
                    "sent": "I've just said you don't need to know what it is because this map dot product this object here I'll set equal to the kernel kernel will just be the mapped data mapped.",
                    "label": 0
                },
                {
                    "sent": "X is OK. Now I can define various kernels and by defining those kernels I implicitly define a mapping function, but I do not need to know what that mapping function is in certain simple cases, like if I do a polynomial type mapping.",
                    "label": 0
                },
                {
                    "sent": "I can actually workout what, five years been generality?",
                    "label": 0
                },
                {
                    "sent": "I don't know what it is and I don't care either, because provided I have certain restrictions on that choice of kernel then.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then it's all OK.",
                    "label": 0
                },
                {
                    "sent": "So what is a mapping relation?",
                    "label": 0
                },
                {
                    "sent": "I say here.",
                    "label": 0
                },
                {
                    "sent": "In fact we do not need to know the formulas mapping because it's implicitly defined by the choice of kernel.",
                    "label": 1
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, but like that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yes, it's some sort of well Pfizer function.",
                    "label": 0
                },
                {
                    "sent": "It's a functional.",
                    "label": 0
                },
                {
                    "sent": "A function which Maps my data like 1, one to some very high dimensional vector in some sort of unique way.",
                    "label": 0
                },
                {
                    "sent": "OK, I don't.",
                    "label": 0
                },
                {
                    "sent": "Yes, data, my data is mapped according to a function 5X to this high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "OK, now a lot of people get disturbed.",
                    "label": 0
                },
                {
                    "sent": "Don't I really need to know what that Phi is?",
                    "label": 0
                },
                {
                    "sent": "I'm just saying that no because.",
                    "label": 0
                },
                {
                    "sent": "I don't have total freedom of choice about my kernel out there right now.",
                    "label": 0
                },
                {
                    "sent": "Restrictions I'm coming into shortly, but let me suppose I define my kernel speed.",
                    "label": 0
                },
                {
                    "sent": "This object here.",
                    "label": 0
                },
                {
                    "sent": "OK, KX1X2 is equal to Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK, it has a parameter in it talk about later, but if I choose that as my kernel, then implicitly defines what the Phi X is, but I don't need to care what that is.",
                    "label": 0
                },
                {
                    "sent": "OK, now there are various choices for.",
                    "label": 1
                },
                {
                    "sent": "Kernels OK, I'll give you typical examples of kernels and also the restrictions that apply to kernels.",
                    "label": 0
                },
                {
                    "sent": "If I choose this kernel, this is a good choice.",
                    "label": 0
                },
                {
                    "sent": "Usually often used as a coolant Gaussian kernel might call in RBF type kernel.",
                    "label": 0
                },
                {
                    "sent": "This is one choice.",
                    "label": 0
                },
                {
                    "sent": "Popular choice.",
                    "label": 0
                },
                {
                    "sent": "OK. Another type of kernel I might choose might be a polynomial.",
                    "label": 1
                },
                {
                    "sent": "OK like so there are popular choices.",
                    "label": 0
                },
                {
                    "sent": "Now what you notice?",
                    "label": 0
                },
                {
                    "sent": "If I choose a kernel?",
                    "label": 0
                },
                {
                    "sent": "And it's a it's a RBF type kernel.",
                    "label": 0
                },
                {
                    "sent": "Indeed, what I'm doing is implicitly giving you an RBF network.",
                    "label": 0
                },
                {
                    "sent": "You probably heard of RBF networks.",
                    "label": 0
                },
                {
                    "sent": "If I choose a polynomial kernel I'm giving you supplies from statistics.",
                    "label": 0
                },
                {
                    "sent": "Another choice I can go for.",
                    "label": 0
                },
                {
                    "sent": "Rarely used actually, don't even give it is attention choice for the kernel.",
                    "label": 1
                },
                {
                    "sent": "If I choose attention, I generate a new network.",
                    "label": 0
                },
                {
                    "sent": "OK, type of neural network.",
                    "label": 0
                },
                {
                    "sent": "Indeed through my choices are kernels.",
                    "label": 0
                },
                {
                    "sent": "I can actually give you a spline, some statistics, RBF networks.",
                    "label": 0
                },
                {
                    "sent": "Neural networks arrange a different models which you would be familiar to with from previous.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Previous interest in machine learning.",
                    "label": 0
                },
                {
                    "sent": "OK now indeed.",
                    "label": 0
                },
                {
                    "sent": "Legitimate kernels do not need to be defined by functions like a Gaussian OK.",
                    "label": 1
                },
                {
                    "sent": "It's about kernels can also be defined by algorithms.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll give you the structure and curl in a little while, but let's consider the following situation.",
                    "label": 0
                },
                {
                    "sent": "String kernels.",
                    "label": 1
                },
                {
                    "sent": "Now let's take the following text strings Karkat cart chart you notice is a degree of similarity with these strings.",
                    "label": 1
                },
                {
                    "sent": "OK, they've always got CAE in them and they got an R and Doug is of equal length the car.",
                    "label": 0
                },
                {
                    "sent": "But it's really quite different.",
                    "label": 0
                },
                {
                    "sent": "OK, indeed I can define things called edit codes.",
                    "label": 0
                },
                {
                    "sent": "Levenstein codes used a lot for genetic information, OK to actually give me a score or number which tells me how similar text strings are to each other OK?",
                    "label": 0
                },
                {
                    "sent": "That little algorithm for edit code actually satisfies the characteristics of a kernel, so it's a legitimate kernel to use in an SVM.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm saying therefore that there's a big class of kernels I can give you, and they will give you RBF networks, neural networks and so on, but they'll generate totally new types of learning machines that can handle text strings.",
                    "label": 0
                },
                {
                    "sent": "If I consider a neural network, there have been introduced multilayer perceptron.",
                    "label": 0
                },
                {
                    "sent": "One thing you notice about it has a fixed number of inputs, 40 inputs.",
                    "label": 0
                },
                {
                    "sent": "Goes to the hidden layer and some outputs.",
                    "label": 0
                },
                {
                    "sent": "Well, what do you do when you're handling text strings?",
                    "label": 0
                },
                {
                    "sent": "When Ashley, the number of inputs would appear to be variable 5433, etc.",
                    "label": 0
                },
                {
                    "sent": "What do I do about that?",
                    "label": 0
                },
                {
                    "sent": "So I can't use the neural network very easily?",
                    "label": 0
                },
                {
                    "sent": "Or if at all, when I'm handling text strings with text strings are hugely important and maybe reading a document and using my learning machine to do that, or I may be looking at genetic codes, CGH, etc trying to spot introns, exons or something?",
                    "label": 0
                },
                {
                    "sent": "Then I must be able to handle strings of an equal length and I can use a string kernel.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do that?",
                    "label": 0
                },
                {
                    "sent": "OK so I say here text processing by informatics you can use string.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reynolds now you can have other very interesting types of kernels and this makes this whole subject much more general.",
                    "label": 0
                },
                {
                    "sent": "In particular, I can actually have a concept of similarity of graphs.",
                    "label": 0
                },
                {
                    "sent": "OK, I've drawn a graph here, and again, I don't have a pen on the ball, but imagine I had two graphs.",
                    "label": 0
                },
                {
                    "sent": "OK, I have certain number of nodes and I draw the links, put me out of my misery.",
                    "label": 0
                },
                {
                    "sent": "Great so.",
                    "label": 0
                },
                {
                    "sent": "If I have.",
                    "label": 0
                },
                {
                    "sent": "Just a set of nodes.",
                    "label": 0
                },
                {
                    "sent": "OK like so OK. Then obviously these two graphs this this, this these are quite similar there just one link difference between them.",
                    "label": 0
                },
                {
                    "sent": "But like an easy draw another graph which is really very different.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm just saying intuitively you can easily see the must be some sort of distance measure between graph.",
                    "label": 0
                },
                {
                    "sent": "Some graphs are similar, some not OK. Well, you could also derive a kernel.",
                    "label": 0
                },
                {
                    "sent": "Start show called Diffusion Kernel Condor Lafferty, which will actually give you a measure of similarity between graphs and you can put that measure into your SVM.",
                    "label": 1
                },
                {
                    "sent": "OK, so you can have a SVM that handles graph so you can have a big wide range of learning machines.",
                    "label": 0
                },
                {
                    "sent": "Huawei might consider grass well again by informatics applications you often interested in say which genes are functionally connected to each other or not connected and you may wish to classify information.",
                    "label": 0
                },
                {
                    "sent": "Based on pathways or networks in by informatics, so graphs are important.",
                    "label": 0
                },
                {
                    "sent": "I don't say anything more.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About graph kernels.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested, I can give you references, but there must be some sort of restrictions, and kernels are very broad spaces, such things and what are they?",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Structions well, you'll find.",
                    "label": 0
                },
                {
                    "sent": "Text books in the past, the destruction that on a legitimate kernel is it must satisfy Mercer's conditions.",
                    "label": 0
                },
                {
                    "sent": "OK, here's the kernel OK, and if I choose any GX which is put it in here then this must be the case to define a legitimate kernel and that was Mercer's conditions.",
                    "label": 1
                },
                {
                    "sent": "Indeed, actually better statement of it is what you notice is.",
                    "label": 0
                },
                {
                    "sent": "This looks like the condition for positive definite, and in fact they can straighten.",
                    "label": 0
                },
                {
                    "sent": "Curl is actually very simple one.",
                    "label": 0
                },
                {
                    "sent": "The kernel must be a positive must be a positive semidefinite.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I have a kernel for which this is true, then I establish its positive semidefinite, then then it's a legitimate kernel.",
                    "label": 0
                },
                {
                    "sent": "I shall not.",
                    "label": 0
                },
                {
                    "sent": "I'm sure that it might not be positive definite.",
                    "label": 0
                },
                {
                    "sent": "About that rap.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, but that is a requirement.",
                    "label": 0
                },
                {
                    "sent": "OK, so I've got here.",
                    "label": 0
                },
                {
                    "sent": "A condition of positive semidefinite guess many will come across it CTK see if these Caesar reels and this is true then I have positive semidefinite and my kernel is OK. OK so that's the only requirement I must have on my kernel and all those previous kernels I gave you Gaussian kernel diffusion kernel.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm able to establish that.",
                    "label": 0
                },
                {
                    "sent": "Right, so I want to summarize the main steps, then this is how I approach the problem.",
                    "label": 0
                },
                {
                    "sent": "I'm given some data and I wish to do binary classification.",
                    "label": 0
                },
                {
                    "sent": "I choose my kernel.",
                    "label": 0
                },
                {
                    "sent": "Now that may be an awkward point.",
                    "label": 0
                },
                {
                    "sent": "I'm going to come back to later.",
                    "label": 0
                },
                {
                    "sent": "Actually, what is my choice of kernel?",
                    "label": 0
                },
                {
                    "sent": "Very high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "Often I just go for linear kernel.",
                    "label": 0
                },
                {
                    "sent": "You haven't all stuck, might go for Gaussian kernel OK, but let me suppose I have chosen my chosen my kernel.",
                    "label": 0
                },
                {
                    "sent": "Here it is here are introduced the data into the kernel and the wise.",
                    "label": 0
                },
                {
                    "sent": "I maximize this object with respect to the alphas and these constraints having found the alphas I put them.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into my.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Put them into my decision function, having also found the B which is my next slide.",
                    "label": 0
                },
                {
                    "sent": "the B is my bias.",
                    "label": 1
                },
                {
                    "sent": "I haven't really said anything about it, but it's relatively straightforward to argument to get what the B should be, but it is actually defined by this.",
                    "label": 0
                },
                {
                    "sent": "OK, so I find the bias haven't got the bias, the alphas, the date.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sure, my choice of Colonel.",
                    "label": 0
                },
                {
                    "sent": "They all go into my decision function.",
                    "label": 1
                },
                {
                    "sent": "I made a choice of kernel.",
                    "label": 0
                },
                {
                    "sent": "I've worked out the bias.",
                    "label": 0
                },
                {
                    "sent": "I know this.",
                    "label": 0
                },
                {
                    "sent": "I worked out the Alphaform optimization.",
                    "label": 0
                },
                {
                    "sent": "I've done everything that is now my decision function, which I use.",
                    "label": 0
                },
                {
                    "sent": "OK now I think it's 2211.",
                    "label": 0
                },
                {
                    "sent": "More time was supposed to end.",
                    "label": 0
                },
                {
                    "sent": "Coffee break.",
                    "label": 0
                },
                {
                    "sent": "Oh to 10:30.",
                    "label": 0
                },
                {
                    "sent": "If you wish you want a break now.",
                    "label": 0
                }
            ]
        }
    }
}