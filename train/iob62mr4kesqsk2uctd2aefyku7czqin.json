{
    "id": "iob62mr4kesqsk2uctd2aefyku7czqin",
    "title": "A Bayesian approach to the Poverty of the stimulus",
    "info": {
        "author": [
            "Amy Perfors, Computational Cognitive Science Group, Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, MIT"
        ],
        "published": "Oct. 31, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Human Language Technology",
            "Top->Computer Science->Natural Language Processing",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/mlcs07_perfors_aba/",
    "segmentation": [
        [
            "So in cognitive psychology, science and psychology, we're all familiar with this sort of one of the big pressing issues being for which for any given cognitive capacity or whatever, to what extent it's innate and to what extent we can say that we learn that ability or that knowledge based on the data in."
        ],
        [
            "Environment, but another orthogonal issue is about the representation that we use.",
            "So to what extent is a representation of whatever it is that's under consideration explicitly structured, and to what extent might we sort of be able to account for our behavior using something that's not so explicitly structured, say purely statistical or bottom up learning?",
            "I'm going to be kind of handwaving about what I mean about that for now, but I think you guys all sort of see."
        ],
        [
            "Familiar with this dichotomy, I'll be talking about grammar today and we can see how this dichotomy, or how both of these issues play out in the issue of the hierarchical phrase structure of language.",
            "So it's often believed by linguists that language does have hierarchical phrase structure.",
            "In other words, that the.",
            "This structure on the right is a better analysis of what a sort of sentence might, the origin or the parts of a sentence might be, rather than the structures on the left.",
            "And there's many reasons that we think that language actually does have hierarchical phrase structure, and one of the earliest reasons for this, and I still think one."
        ],
        [
            "The most powerful basically comes down to simplicity.",
            "Argument language has various dependencies structures where words or phrases or even more famous farther down the sentence are going to be unrelated.",
            "Long distance to things that occur much earlier in the sentence, and it's really hard to explain those unless you assume something like structure you could, so you could sort of as long as you know the corpus.",
            "Is finite.",
            "You could in theory use some kind of finite non structured automata or Markov model to capture this, But the intuition is that would be like hugely complex and not very simple because you have to have for each additional long distance dependency you have to put in a whole lot of loops and cycles an that's not very parsimonious, whereas some kind of context free or phrase structure grammar approach would capture that a lot better.",
            "That's the intuition that I think drives.",
            "A lot of linguists."
        ],
        [
            "A lot of us.",
            "But there's this additional argument, sort of on the other strand of the innateness versus alertness about whether this notion that language has hierarchical phrase structure is innate.",
            "So we whether kids have to start off with this notion in their heads, and we've seen talks yesterday that sort of went over this argument.",
            "I think most people here familiar with this argument, in a nutshell, it comes down to kids, don't seem to say that much.",
            "The sorts of things that you would expect them to say if they thought that at any point that it was.",
            "Not.",
            "That language didn't have structure dependence, so in other words, they don't make the kinds of mistakes like is the girl who sleeping is happy and there's some argument, I think legitimate argument about whether that claim itself is true.",
            "But that's the basic structure of the argument about why we think that this might."
        ],
        [
            "Be an 8 and as I said, there's some arguments against this one, which sort of says well, there actually might be enough data in the input that kids could could actually learn this from the input.",
            "They don't have to have it innately built in, that language has hierarchical phrase structure.",
            "Problem with that is, it's hard to sort of know how much input is enough to actually know that, so it's an unresolvable argument.",
            "But the other main response is basically to say we don't need to assume explicit structure.",
            "We can explain it by sort of statistical learning.",
            "And that will explain children's behavior, and I think both of those are sort of legitimate and very into."
        ],
        [
            "Listing arguments, but you'll notice that this because of the way that the debate has gone.",
            "The arguments have sort of only covered two out of four logical possibilities.",
            "There's very few people arguing that both this particular thing isn't, but there's no explicit structure.",
            "It's kind of hard to see why you would want to argue that I think, but but no ones arguing that.",
            "And also it's fairly.",
            "Argue that that possibly this might be learned, but still that there might be explicit structure.",
            "And So what?",
            "I"
        ],
        [
            "To do today is basically explore how we might investigate this.",
            "So take the idea of explicit structure seriously in a probabilistic model and then explore sort of whether that could be in."
        ],
        [
            "Or learned so."
        ],
        [
            "Will be basically doing is putting forth a claim on an ideal learnability claim, not a sort of kids.",
            "Actually do this necessarily kind of claim that if you just take the nature of the input an we make certain domain general assumptions about what kids can represent an what they can learn, then I'll be arguing that unideal unbiased learner could actually learn that language has hierarchical phrase structure rather than us is some sort of structure independent thing.",
            "And the reason is basically I'll be doing what a couple of people yesterday argued would be a good idea for linguists to do, which is sort of actually quantitatively try to calculate the balance between simplicity of the representation and how well it fits the data.",
            "An I'll be sort of suggesting that grammars that have hierarchical phrase structure best best capture this tradeoff, and others don't.",
            "So essentially making the same thing argument that Chomsky did back in 1957, but quantitatively an actual corpora with, you know.",
            "Computer models."
        ],
        [
            "So the plan then is basically I'll be talking about the model an because basically I'll be taking some data from a child.",
            "This corpus fitting grammars of various sorts to that child is corpus, some of them being linear that is non structure dependent and some being context free which is sort of the approximation too.",
            "Good first pass at a structure dependent grammar and then just comparing the posterior probability of those grammars on that corpus.",
            "So before I go into the nature of the grammars, how we found them, I just want to talk about the corpus of it since I'm only going to be comparing grammars that actually fit that corpus, and then I'll talk about the evaluation metric and there is."
        ],
        [
            "Also, which I've already sort of told you what they are.",
            "So our model, the data is as I said, this corpus from the child's database and I basically just took all of the Atom files from the Brown corpus.",
            "So that's 55 files for about three years and replaced each of the words there by the syntactic category.",
            "Basically, the reason for this is because because it's very hard to learn grammar unsupervised, as we all know I hand design the grammars and that's much much easier if you have.",
            "To match syntactic categories rather than words themselves, I don't think this makes much of a difference, because every grammar would have basically the exact same translation from syntactic category to words in there, and so it won't change the sort of relative fit of the data or simplicity of the data for each of them.",
            "But one thing I want to do later is do this more automated Lee, but then I also sort of removed some really complicated sentences, basically 'cause again hand designing that was very hard as it was.",
            "Um, and the ungrammatical sentences.",
            "But I did an analysis with the ungrammatical sentence is in there, nothing changes, so this is just sort of for simplicity."
        ],
        [
            "Leaving that in so just to give you a sense of what the data looks like.",
            "This is sort of the first some sentences in the corpus.",
            "Overall, there are about 21,000 different individual sentence tokens.",
            "We just looked at sort of the sentence types because basically if you're learning a grammar, all that matters is that you know.",
            "Which sentences are grammatical or not?"
        ],
        [
            "Not necessarily their frequencies.",
            "And then I wanted to look at two individual things about the data.",
            "First of all, it might be interesting to say OK at different stages in development.",
            "What grammars tend to fits the data best?",
            "What types of grammars?",
            "Maybe early on you might expect that you know something really simple would fit, but later not so much."
        ],
        [
            "And then also it would be interesting to be able to get at.",
            "Sort of, how much kids are actually understanding at different levels, right?",
            "I mean obviously a 2 year old Atom is not going to be understanding everything in the corpus.",
            "If we could somehow get that, then then that would be kind of a nice approximation, obviously."
        ],
        [
            "Both of these I can only approximate, but to approximate how much data is available we can just do basically this really rough estimate split the corpus by age, so I since there's 55 files I basically split it into, ignoring the top row, five different epics of 11 files each, so they reach, you know, a little chunks.",
            "The later epics all include the previous ones, so these get larger and larger and larger datasets, so it's A kind of that sort of tells us whether you know the nature of the data is sort of changing dramatically as a kid.",
            "Actually gets to be speaking more themselves and then just for interest sake.",
            "I want to compare this to Epic 0, which is just the first file, so it's like an hour of conversation at age 2 just to get a sense of how much data there is."
        ],
        [
            "At all.",
            "And then I also to approximate comprehension and this is sort of even a more rough approximation.",
            "Just split the data by frequency.",
            "So basically the full corpus is all the sentences at all, and then the smaller and smaller levels are.",
            "Basically getting getting rid of the least frequent senses.",
            "So in level 1 corpus is basically all those sentence types that occurred 500 times or more.",
            "These are very simple senses like noun, auxiliary adjective.",
            "He is happy you know that sort of thing occurs all the time.",
            "There's only eight of those and then you can consistently add more.",
            "Again, this is not exactly what kids are doing clearly, but it gives us sort of sense of like starting off with very simple corpus and then it gets more and more complicated as you go."
        ],
        [
            "Through OK, so those are the corporate will be comparing the grammars on and then.",
            "As for the actual grammars.",
            "Again we want to compare grammars that include that have structure dependence built in.",
            "So hierarchical grammars which are context free in this case and then linear ones that don't and basically the strength of the analysis is going to depend on how well I can sort of claim that I've actually found the best grammars of each type and I'm comparing them because if say you know in the extreme I just pick a really bad linear grammar and then say OK it does really bad.",
            "That's not very persuasive, so I'll be spending a fair amount of time, possibly too much belaboring the point of how I've tried to actually do my best to get the very best grammars there are, and what I do do is hand designed them both hand design the best context, free grammar I could derive a regular grammar from that, do a sort of local search around them using sort of split merge stocking on 100 type moves, and then also independently.",
            "I learn a linear grammar using a hierarchical sorry and HMM learner on the corpus.",
            "So basically I've tried to do as much as I can to learn a good regular grammar."
        ],
        [
            "I'll go into that a bit more detail.",
            "Again, so very quickly.",
            "Then, like I said, there's a hierarchical grammars and then various types of linear grammars or regular grammars, and it's kind of there's a lot of different ways to not be structured dependent.",
            "So on one extreme you have what I'm calling the flat grammar.",
            "You can just memorize everything.",
            "There's no structure in there.",
            "On another extreme you might say, OK, you memorize nothing.",
            "You just say anything can follow anything else, so everything is OK.",
            "Both of those have no structure dependence.",
            "Arguably, they probably wouldn't do very good, but those are extremes.",
            "And then in the middle we have sort of a regular grammar which doesn't include structure dependence, but can incorporate a lot of information that there is, and so we'll be looking at all of those."
        ],
        [
            "Just to really quickly explain how I got the context free grammars, there's actually two of them.",
            "The first is just what I call the CFG S, the sort of standard one which basically designed to be as linguistically plausible as possible.",
            "Talk to various linguists, and said, does this make sense?",
            "It's got noun phrases and verb phrases.",
            "And you know things like that.",
            "This just gives.",
            "I gave some sample productions there so you can see.",
            "And then the other one.",
            "This is basically exactly the same, but with the difference that we were finding that a lot of these productions, these recursive productions so NP goes to say NP PP.",
            "Because they're recursive, they actually predict that you'd see like a whole lot of sentences you know, not infinitely long, but getting there that they don't show up in a very small corpus.",
            "And So what I did here was basically do keep those productions in there, but include also some sort of shadow productions that are exactly the same but don't.",
            "But I mean they do the same thing, except they are not recursive.",
            "So the idea is.",
            "A lot of probability mass goes on those.",
            "It can still produce the recursive productions, but with just sort of less probability, so you lose a lot less so."
        ],
        [
            "Sense, yeah, so those are the two context free grammars.",
            "And then for the regular grammars.",
            "What I wanted to do is basically have some sense that is sort of covering the entire space of regular grammars, and if you if you sort of view it as on one end you have the flat grammar which has an exact fit to the data set, doesn't compress it at all, and on the other end you have this sort of 1 state grammar that accepts everything, which is a really poor fit but really good compression.",
            "Then you can sort of try to make a set of grammars that sort of."
        ],
        [
            "Spans that range so The Reg the colmenero regular grammar basically derived from the context.",
            "Free Grammar an every single time you would have a context free production.",
            "I just turned that into all the possible regular analog productions and this is the sort of type of grammar that Chomsky was I think had in mind when it says you have to have so many cycles in order to totally match the grammar.",
            "It's something like this.",
            "But then I thought, well, what you can?"
        ],
        [
            "Do is you can merge some of those productions that are in the regular grammar to make a sort of smaller grammar still regular an it doesn't fit nearly as well but but it's sort of a lot smaller, so here you might.",
            "So these are some example productions and like you know, say a pee pee.",
            "In this grammar wouldn't be exactly what we think of as a prepositional phrase.",
            "We're in a context free grammar because it you know some prepositional phrases might be followed by.",
            "They are in a subject they might be followed by a verb, and that in this regular grammar would be part of a prepositional phrase, right?",
            "So they've been combined, but it still sort of fits.",
            "But basically I was just trying to get it."
        ],
        [
            "Fit better and better and then the regular.",
            "The smallest regular grammar basically just did another combination of combined noun phrases, prepositional phrases, all the things that generally follow verbs into one sort of production.",
            "So this just gives you a sense.",
            "I basically tried to sort of."
        ],
        [
            "Span the range of regular grammars here in various ways, and then I did like I talked about earlier, a sort of local search around each of the hand design grammars using this, some just some sample productions.",
            "It doesn't matter that much exactly what they were.",
            "I looked around and this is a picture of all the grammars that I looked around.",
            "There's lots of them.",
            "Then to do the unsupervised automatic HMM learning I actually this might seem sketchy, but I don't think it is used.",
            "A hmm that was designed for part of speech tagging, so it learns.",
            "It learns part of Speech Productions, given a sort of corpus of just words.",
            "But if you give it a corpus of syntactic categories like mine, then it basically groups those syntactic categories.",
            "And I said those are the non terminals of the regular grammar and you can derive a regular grammar straightforwardly so basically learns and hmm, on this corpus of syntactic categories actually did fairly well.",
            "I think I mean comparable to all the hand design grammars.",
            "So."
        ],
        [
            "OK, is that all clear before I?"
        ],
        [
            "So evaluation then, as I said earlier, is basically I'm trying to do intuitively what several people yesterday argued what we want to do is linguists is sort of calculate quantitatively this tradeoff between a simplicity and the fit of a grammar to the data.",
            "We do this using Bayes rule where each of the specific grammars like that I've came about or talked about are examples of the type of grammar that, like hierarchical or linear and in the model we.",
            "Not biased, favorite.",
            "Any of them, so that would fall."
        ],
        [
            "Out of the equation and basically then I'm calculating the posterior probability of that grammar given the data, which is basically a combination of how well it fits the corpus times the complexity or the prior."
        ],
        [
            "Probability of the grammar.",
            "And this is sort of a domain general notion of complexity versus fit could do basically exactly the same thing using MDL rather than Bayes rule.",
            "It does.",
            "It captures the same thing, of course.",
            "Which exactly you use sort of slightly changes the.",
            "Your results, but I've actually done.",
            "A number of different prior measures and it really doesn't change things.",
            "I haven't done it in MDL, but it's all the same."
        ],
        [
            "Intuitive idea?",
            "I did to measure the prior probability of the grammar.",
            "Here was just basically and I enjoyed this part.",
            "Pretend I was God.",
            "I'm designing the grammar.",
            "I'm saying OK if I'm designing a grammar, what choices do I have to make?",
            "Well, I have to decide how many non terminals there are for each non terminal have to decide how many productions I'm going to use for each of those productions I have to decide what's going to be on the right hand side.",
            "And each of those is going to be selections from a distribution of the possible productions an so from that you can basically come up with the equation that says what's the probability of generating each grammar if I have to make all of those choices, and that's it.",
            "Favors grammars where you have to make fewer production or fewer choices, so fewer productions fewer non terminals to choose from.",
            "Those are going to have higher prior probability.",
            "All things being equal."
        ],
        [
            "Um?",
            "The likelihood, I think probably many people here already sort of intuitively very familiar with this.",
            "All I'm doing is calculating the probability that that grammar generated the data, which is basically the product of the probability of each parse.",
            "Each of the derivations involved, and So what that means is that grammars with recursive productions are going to have slightly lower probability, 'cause they're going to predict.",
            "You see, these really long senses that don't show up, but also also intuitively a grammar that over generalizes is going to.",
            "You know?"
        ],
        [
            "Have a slightly lower likelihood."
        ],
        [
            "So what do we find this is?",
            "I know I cannot think of how to show this other than as a table of hideous numbers.",
            "But what we find is on the data set that splits by frequency levels.",
            "So it's a measure of the comprehension of.",
            "Sort of a measure of comprehension where again the smallest corpus is the one that sort of has just the most common sentence types, and the largest has all of them.",
            "What you find is that on the smaller corpora, it actually is best to just memorize them and that sort of makes it if you have 8 sentences in your entire language, just memorize it.",
            "You know don't don't putz around with trying to make a grammar as it gets larger.",
            "It might be better to sort of learn this kind of 1 state.",
            "Hmm, which actually does you know you are learning something you're learning something about the probabilities.",
            "Of transitions between the different syntactic categories, but you're saying anything is kind of is more likely than other.",
            "But then on the larger corpora and these are still not super large.",
            "This is just 10 or more context free grammars always favored.",
            "These are log probabilities.",
            "So like the yeah, the difference between between these is actually quite large.",
            "A context free grammar will always be favored here over any of the regular grammars I tried.",
            "And actually I didn't report them 'cause they're slightly more complicated, but I found context free grammars that do even better than this.",
            "These are the very best regular grammars I ever found are."
        ],
        [
            "Are here."
        ],
        [
            "It gets even more interesting if we look at sort of the data set split by age there at all levels.",
            "The context free grammar is found, and so what's really interesting here is that even if you just look at one file, one hours worth of conversation, there's enough data in that in that input to be able to say OK. Hierarchical grammar best fits that input, and the reason is again the simplicity trade off.",
            "It's a lot smaller than the regular grammars.",
            "There are a few regular grammars that are really small, but they're sort of really badly fit, so at best captures that tradeoff.",
            "Obviously, I'm not saying that all the kid has to do is pop out of the womb in here an hours worth of input, and then they're like, yeah?",
            "Clearly you have to know a lot in order to do this, but it's again as a sort of ideal learning what's in the input kind of argument.",
            "It shows that you know it's all over the place.",
            "It's ubiquitous.",
            "The information that tells you that hierarchy."
        ],
        [
            "Programmer is probably the best fit.",
            "And then it's sort of to relate back to the original.",
            "The poverty of the stimulus argument that motivated much of this.",
            "We can look at how well each grammar actually predicts the sentence is it hasn't seen an."
        ],
        [
            "Much like one of the presentations yesterday, what you find is that the context free grammars actually can produce the correct complex interrogative sentence.",
            "This was a corpus without any complex interrogatives at all, so it can produce something like.",
            "The sentence here is do Eagles that are alive fly, but any sort of complex interrogative it will produce, but not the wrong one, not the wrong kind.",
            "That sort of the mistake that children don't make the other regular grammars, so they can't.",
            "Well the one state grammar will do anything you want basically, but the others they won't.",
            "And basically that's because they're not fitting in the right way.",
            "They don't have this notion of phrases, and so therefore they're not.",
            "They're not generalizing in this."
        ],
        [
            "Anyway, so I kind of threw a lot of out here and I think there's some.",
            "Several underlying interesting ideas here, but I think the main sort of points here is simply to, well, there's two main points I want to make.",
            "First of all is to say as an ideal learning analysis of looking what's in the data.",
            "An ideal unbiased learner who could actually search around and find these grammars like this should be able to realize simply based on the input that languages do have hierarchical structure and the reason this works is because they're looking at sort of all of the input together rather than sort of, you know, just looking for, say, complex interrogatives and all of the input together really does sort of argue that this notion of phrases is really a good way to capture what's going on.",
            "If you assume that we're doing any kind of simplicity or an fit to the data trade off that sort of.",
            "Approximate to this.",
            "And then the other thing I think is useful here is that it's an example of what we saw in the first talk yesterday.",
            "Being argued would be good for linguists to do, which is perhaps you'll disagree with me, but which is to basically enable us to rigorously kind of evaluate different sorts of grammars, or different sorts of analysis using a kind of tradeoff between the simplicity and the goodness of fit.",
            "So I think that's a sort of valuable technique to keep in mind going forward, which I think you know various people here already know and do, but it also argues that in some ways this sort of higher order knowledge, knowledge about things like hierarchical structure might be easier to learn than very basic item specific knowledge, because you can use the input from an entire data set and you're only choosing between, say, a few other certain options, and so that's also good to keep in mind when we're talking about learnability arguments in June."
        ],
        [
            "So this is my last slide, I just want to leave off or end with all my caveats.",
            "The main ones being of course this is an ideal learner thing.",
            "I'm not sure that I found the best grammars, but I think I'm getting more and more confident that I found.",
            "Then if a regular grammar exists that is better than these context free grammars, it's being hard to find.",
            "I sort of done basically, lots and lots of ways of searching and to try to find those, and I probably haven't found the best context free grammar, but there's a lot of context free grammars that beat this sort of.",
            "All the best regular grammars I've found, so I think that I'm getting more confident that I found those.",
            "Obviously can't be sure, so if so, it's only as good as that.",
            "Then the other thing I think a big issue that that also came up is this issue of how you calculate complexity and fit to the data to some extent how complex something is is going to be with respect to its representation or the underlying machine.",
            "That's sort of doing the work, and that is an issue.",
            "I think it's not.",
            "An enormous issue.",
            "Well, I mean, I haven't tried things that are just totally bizarre representations, but the prior probability was sort of a very straightforward one.",
            "It's not like I sort of made it up out of thin air and it.",
            "Comes out of how you assume the grammars are constructed, and again a sort of most straightforward MDL analysis.",
            "Or like slightly different ways of measuring the prior.",
            "I think you'd get basically the same result, but it is a kind of open question of how you know how general and how much we can use this sort of notion of simplicity and still kind of wrestling with that of, and also whether people's brains, whether we individuals care about it that much.",
            "But I think there's good reasons to think that we might.",
            "And yeah, those are the main assumptions.",
            "You can read the others as well as I can and I think I'm almost out of time.",
            "So yeah, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in cognitive psychology, science and psychology, we're all familiar with this sort of one of the big pressing issues being for which for any given cognitive capacity or whatever, to what extent it's innate and to what extent we can say that we learn that ability or that knowledge based on the data in.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Environment, but another orthogonal issue is about the representation that we use.",
                    "label": 0
                },
                {
                    "sent": "So to what extent is a representation of whatever it is that's under consideration explicitly structured, and to what extent might we sort of be able to account for our behavior using something that's not so explicitly structured, say purely statistical or bottom up learning?",
                    "label": 0
                },
                {
                    "sent": "I'm going to be kind of handwaving about what I mean about that for now, but I think you guys all sort of see.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Familiar with this dichotomy, I'll be talking about grammar today and we can see how this dichotomy, or how both of these issues play out in the issue of the hierarchical phrase structure of language.",
                    "label": 0
                },
                {
                    "sent": "So it's often believed by linguists that language does have hierarchical phrase structure.",
                    "label": 0
                },
                {
                    "sent": "In other words, that the.",
                    "label": 0
                },
                {
                    "sent": "This structure on the right is a better analysis of what a sort of sentence might, the origin or the parts of a sentence might be, rather than the structures on the left.",
                    "label": 0
                },
                {
                    "sent": "And there's many reasons that we think that language actually does have hierarchical phrase structure, and one of the earliest reasons for this, and I still think one.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The most powerful basically comes down to simplicity.",
                    "label": 0
                },
                {
                    "sent": "Argument language has various dependencies structures where words or phrases or even more famous farther down the sentence are going to be unrelated.",
                    "label": 1
                },
                {
                    "sent": "Long distance to things that occur much earlier in the sentence, and it's really hard to explain those unless you assume something like structure you could, so you could sort of as long as you know the corpus.",
                    "label": 0
                },
                {
                    "sent": "Is finite.",
                    "label": 0
                },
                {
                    "sent": "You could in theory use some kind of finite non structured automata or Markov model to capture this, But the intuition is that would be like hugely complex and not very simple because you have to have for each additional long distance dependency you have to put in a whole lot of loops and cycles an that's not very parsimonious, whereas some kind of context free or phrase structure grammar approach would capture that a lot better.",
                    "label": 0
                },
                {
                    "sent": "That's the intuition that I think drives.",
                    "label": 0
                },
                {
                    "sent": "A lot of linguists.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A lot of us.",
                    "label": 0
                },
                {
                    "sent": "But there's this additional argument, sort of on the other strand of the innateness versus alertness about whether this notion that language has hierarchical phrase structure is innate.",
                    "label": 1
                },
                {
                    "sent": "So we whether kids have to start off with this notion in their heads, and we've seen talks yesterday that sort of went over this argument.",
                    "label": 0
                },
                {
                    "sent": "I think most people here familiar with this argument, in a nutshell, it comes down to kids, don't seem to say that much.",
                    "label": 0
                },
                {
                    "sent": "The sorts of things that you would expect them to say if they thought that at any point that it was.",
                    "label": 0
                },
                {
                    "sent": "Not.",
                    "label": 0
                },
                {
                    "sent": "That language didn't have structure dependence, so in other words, they don't make the kinds of mistakes like is the girl who sleeping is happy and there's some argument, I think legitimate argument about whether that claim itself is true.",
                    "label": 1
                },
                {
                    "sent": "But that's the basic structure of the argument about why we think that this might.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be an 8 and as I said, there's some arguments against this one, which sort of says well, there actually might be enough data in the input that kids could could actually learn this from the input.",
                    "label": 0
                },
                {
                    "sent": "They don't have to have it innately built in, that language has hierarchical phrase structure.",
                    "label": 0
                },
                {
                    "sent": "Problem with that is, it's hard to sort of know how much input is enough to actually know that, so it's an unresolvable argument.",
                    "label": 0
                },
                {
                    "sent": "But the other main response is basically to say we don't need to assume explicit structure.",
                    "label": 0
                },
                {
                    "sent": "We can explain it by sort of statistical learning.",
                    "label": 0
                },
                {
                    "sent": "And that will explain children's behavior, and I think both of those are sort of legitimate and very into.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Listing arguments, but you'll notice that this because of the way that the debate has gone.",
                    "label": 0
                },
                {
                    "sent": "The arguments have sort of only covered two out of four logical possibilities.",
                    "label": 0
                },
                {
                    "sent": "There's very few people arguing that both this particular thing isn't, but there's no explicit structure.",
                    "label": 0
                },
                {
                    "sent": "It's kind of hard to see why you would want to argue that I think, but but no ones arguing that.",
                    "label": 0
                },
                {
                    "sent": "And also it's fairly.",
                    "label": 0
                },
                {
                    "sent": "Argue that that possibly this might be learned, but still that there might be explicit structure.",
                    "label": 0
                },
                {
                    "sent": "And So what?",
                    "label": 0
                },
                {
                    "sent": "I",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do today is basically explore how we might investigate this.",
                    "label": 0
                },
                {
                    "sent": "So take the idea of explicit structure seriously in a probabilistic model and then explore sort of whether that could be in.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or learned so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will be basically doing is putting forth a claim on an ideal learnability claim, not a sort of kids.",
                    "label": 0
                },
                {
                    "sent": "Actually do this necessarily kind of claim that if you just take the nature of the input an we make certain domain general assumptions about what kids can represent an what they can learn, then I'll be arguing that unideal unbiased learner could actually learn that language has hierarchical phrase structure rather than us is some sort of structure independent thing.",
                    "label": 0
                },
                {
                    "sent": "And the reason is basically I'll be doing what a couple of people yesterday argued would be a good idea for linguists to do, which is sort of actually quantitatively try to calculate the balance between simplicity of the representation and how well it fits the data.",
                    "label": 0
                },
                {
                    "sent": "An I'll be sort of suggesting that grammars that have hierarchical phrase structure best best capture this tradeoff, and others don't.",
                    "label": 0
                },
                {
                    "sent": "So essentially making the same thing argument that Chomsky did back in 1957, but quantitatively an actual corpora with, you know.",
                    "label": 0
                },
                {
                    "sent": "Computer models.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the plan then is basically I'll be talking about the model an because basically I'll be taking some data from a child.",
                    "label": 0
                },
                {
                    "sent": "This corpus fitting grammars of various sorts to that child is corpus, some of them being linear that is non structure dependent and some being context free which is sort of the approximation too.",
                    "label": 0
                },
                {
                    "sent": "Good first pass at a structure dependent grammar and then just comparing the posterior probability of those grammars on that corpus.",
                    "label": 0
                },
                {
                    "sent": "So before I go into the nature of the grammars, how we found them, I just want to talk about the corpus of it since I'm only going to be comparing grammars that actually fit that corpus, and then I'll talk about the evaluation metric and there is.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, which I've already sort of told you what they are.",
                    "label": 0
                },
                {
                    "sent": "So our model, the data is as I said, this corpus from the child's database and I basically just took all of the Atom files from the Brown corpus.",
                    "label": 0
                },
                {
                    "sent": "So that's 55 files for about three years and replaced each of the words there by the syntactic category.",
                    "label": 0
                },
                {
                    "sent": "Basically, the reason for this is because because it's very hard to learn grammar unsupervised, as we all know I hand design the grammars and that's much much easier if you have.",
                    "label": 0
                },
                {
                    "sent": "To match syntactic categories rather than words themselves, I don't think this makes much of a difference, because every grammar would have basically the exact same translation from syntactic category to words in there, and so it won't change the sort of relative fit of the data or simplicity of the data for each of them.",
                    "label": 0
                },
                {
                    "sent": "But one thing I want to do later is do this more automated Lee, but then I also sort of removed some really complicated sentences, basically 'cause again hand designing that was very hard as it was.",
                    "label": 0
                },
                {
                    "sent": "Um, and the ungrammatical sentences.",
                    "label": 0
                },
                {
                    "sent": "But I did an analysis with the ungrammatical sentence is in there, nothing changes, so this is just sort of for simplicity.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Leaving that in so just to give you a sense of what the data looks like.",
                    "label": 0
                },
                {
                    "sent": "This is sort of the first some sentences in the corpus.",
                    "label": 0
                },
                {
                    "sent": "Overall, there are about 21,000 different individual sentence tokens.",
                    "label": 0
                },
                {
                    "sent": "We just looked at sort of the sentence types because basically if you're learning a grammar, all that matters is that you know.",
                    "label": 0
                },
                {
                    "sent": "Which sentences are grammatical or not?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not necessarily their frequencies.",
                    "label": 0
                },
                {
                    "sent": "And then I wanted to look at two individual things about the data.",
                    "label": 0
                },
                {
                    "sent": "First of all, it might be interesting to say OK at different stages in development.",
                    "label": 0
                },
                {
                    "sent": "What grammars tend to fits the data best?",
                    "label": 0
                },
                {
                    "sent": "What types of grammars?",
                    "label": 0
                },
                {
                    "sent": "Maybe early on you might expect that you know something really simple would fit, but later not so much.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then also it would be interesting to be able to get at.",
                    "label": 0
                },
                {
                    "sent": "Sort of, how much kids are actually understanding at different levels, right?",
                    "label": 1
                },
                {
                    "sent": "I mean obviously a 2 year old Atom is not going to be understanding everything in the corpus.",
                    "label": 0
                },
                {
                    "sent": "If we could somehow get that, then then that would be kind of a nice approximation, obviously.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Both of these I can only approximate, but to approximate how much data is available we can just do basically this really rough estimate split the corpus by age, so I since there's 55 files I basically split it into, ignoring the top row, five different epics of 11 files each, so they reach, you know, a little chunks.",
                    "label": 0
                },
                {
                    "sent": "The later epics all include the previous ones, so these get larger and larger and larger datasets, so it's A kind of that sort of tells us whether you know the nature of the data is sort of changing dramatically as a kid.",
                    "label": 0
                },
                {
                    "sent": "Actually gets to be speaking more themselves and then just for interest sake.",
                    "label": 0
                },
                {
                    "sent": "I want to compare this to Epic 0, which is just the first file, so it's like an hour of conversation at age 2 just to get a sense of how much data there is.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At all.",
                    "label": 0
                },
                {
                    "sent": "And then I also to approximate comprehension and this is sort of even a more rough approximation.",
                    "label": 0
                },
                {
                    "sent": "Just split the data by frequency.",
                    "label": 0
                },
                {
                    "sent": "So basically the full corpus is all the sentences at all, and then the smaller and smaller levels are.",
                    "label": 0
                },
                {
                    "sent": "Basically getting getting rid of the least frequent senses.",
                    "label": 0
                },
                {
                    "sent": "So in level 1 corpus is basically all those sentence types that occurred 500 times or more.",
                    "label": 0
                },
                {
                    "sent": "These are very simple senses like noun, auxiliary adjective.",
                    "label": 0
                },
                {
                    "sent": "He is happy you know that sort of thing occurs all the time.",
                    "label": 0
                },
                {
                    "sent": "There's only eight of those and then you can consistently add more.",
                    "label": 0
                },
                {
                    "sent": "Again, this is not exactly what kids are doing clearly, but it gives us sort of sense of like starting off with very simple corpus and then it gets more and more complicated as you go.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Through OK, so those are the corporate will be comparing the grammars on and then.",
                    "label": 0
                },
                {
                    "sent": "As for the actual grammars.",
                    "label": 0
                },
                {
                    "sent": "Again we want to compare grammars that include that have structure dependence built in.",
                    "label": 0
                },
                {
                    "sent": "So hierarchical grammars which are context free in this case and then linear ones that don't and basically the strength of the analysis is going to depend on how well I can sort of claim that I've actually found the best grammars of each type and I'm comparing them because if say you know in the extreme I just pick a really bad linear grammar and then say OK it does really bad.",
                    "label": 0
                },
                {
                    "sent": "That's not very persuasive, so I'll be spending a fair amount of time, possibly too much belaboring the point of how I've tried to actually do my best to get the very best grammars there are, and what I do do is hand designed them both hand design the best context, free grammar I could derive a regular grammar from that, do a sort of local search around them using sort of split merge stocking on 100 type moves, and then also independently.",
                    "label": 0
                },
                {
                    "sent": "I learn a linear grammar using a hierarchical sorry and HMM learner on the corpus.",
                    "label": 0
                },
                {
                    "sent": "So basically I've tried to do as much as I can to learn a good regular grammar.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll go into that a bit more detail.",
                    "label": 0
                },
                {
                    "sent": "Again, so very quickly.",
                    "label": 0
                },
                {
                    "sent": "Then, like I said, there's a hierarchical grammars and then various types of linear grammars or regular grammars, and it's kind of there's a lot of different ways to not be structured dependent.",
                    "label": 0
                },
                {
                    "sent": "So on one extreme you have what I'm calling the flat grammar.",
                    "label": 0
                },
                {
                    "sent": "You can just memorize everything.",
                    "label": 0
                },
                {
                    "sent": "There's no structure in there.",
                    "label": 0
                },
                {
                    "sent": "On another extreme you might say, OK, you memorize nothing.",
                    "label": 0
                },
                {
                    "sent": "You just say anything can follow anything else, so everything is OK.",
                    "label": 0
                },
                {
                    "sent": "Both of those have no structure dependence.",
                    "label": 0
                },
                {
                    "sent": "Arguably, they probably wouldn't do very good, but those are extremes.",
                    "label": 0
                },
                {
                    "sent": "And then in the middle we have sort of a regular grammar which doesn't include structure dependence, but can incorporate a lot of information that there is, and so we'll be looking at all of those.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to really quickly explain how I got the context free grammars, there's actually two of them.",
                    "label": 0
                },
                {
                    "sent": "The first is just what I call the CFG S, the sort of standard one which basically designed to be as linguistically plausible as possible.",
                    "label": 0
                },
                {
                    "sent": "Talk to various linguists, and said, does this make sense?",
                    "label": 0
                },
                {
                    "sent": "It's got noun phrases and verb phrases.",
                    "label": 0
                },
                {
                    "sent": "And you know things like that.",
                    "label": 0
                },
                {
                    "sent": "This just gives.",
                    "label": 0
                },
                {
                    "sent": "I gave some sample productions there so you can see.",
                    "label": 0
                },
                {
                    "sent": "And then the other one.",
                    "label": 0
                },
                {
                    "sent": "This is basically exactly the same, but with the difference that we were finding that a lot of these productions, these recursive productions so NP goes to say NP PP.",
                    "label": 0
                },
                {
                    "sent": "Because they're recursive, they actually predict that you'd see like a whole lot of sentences you know, not infinitely long, but getting there that they don't show up in a very small corpus.",
                    "label": 0
                },
                {
                    "sent": "And So what I did here was basically do keep those productions in there, but include also some sort of shadow productions that are exactly the same but don't.",
                    "label": 0
                },
                {
                    "sent": "But I mean they do the same thing, except they are not recursive.",
                    "label": 0
                },
                {
                    "sent": "So the idea is.",
                    "label": 0
                },
                {
                    "sent": "A lot of probability mass goes on those.",
                    "label": 0
                },
                {
                    "sent": "It can still produce the recursive productions, but with just sort of less probability, so you lose a lot less so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sense, yeah, so those are the two context free grammars.",
                    "label": 0
                },
                {
                    "sent": "And then for the regular grammars.",
                    "label": 0
                },
                {
                    "sent": "What I wanted to do is basically have some sense that is sort of covering the entire space of regular grammars, and if you if you sort of view it as on one end you have the flat grammar which has an exact fit to the data set, doesn't compress it at all, and on the other end you have this sort of 1 state grammar that accepts everything, which is a really poor fit but really good compression.",
                    "label": 0
                },
                {
                    "sent": "Then you can sort of try to make a set of grammars that sort of.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spans that range so The Reg the colmenero regular grammar basically derived from the context.",
                    "label": 0
                },
                {
                    "sent": "Free Grammar an every single time you would have a context free production.",
                    "label": 0
                },
                {
                    "sent": "I just turned that into all the possible regular analog productions and this is the sort of type of grammar that Chomsky was I think had in mind when it says you have to have so many cycles in order to totally match the grammar.",
                    "label": 0
                },
                {
                    "sent": "It's something like this.",
                    "label": 0
                },
                {
                    "sent": "But then I thought, well, what you can?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do is you can merge some of those productions that are in the regular grammar to make a sort of smaller grammar still regular an it doesn't fit nearly as well but but it's sort of a lot smaller, so here you might.",
                    "label": 0
                },
                {
                    "sent": "So these are some example productions and like you know, say a pee pee.",
                    "label": 0
                },
                {
                    "sent": "In this grammar wouldn't be exactly what we think of as a prepositional phrase.",
                    "label": 0
                },
                {
                    "sent": "We're in a context free grammar because it you know some prepositional phrases might be followed by.",
                    "label": 0
                },
                {
                    "sent": "They are in a subject they might be followed by a verb, and that in this regular grammar would be part of a prepositional phrase, right?",
                    "label": 0
                },
                {
                    "sent": "So they've been combined, but it still sort of fits.",
                    "label": 0
                },
                {
                    "sent": "But basically I was just trying to get it.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fit better and better and then the regular.",
                    "label": 0
                },
                {
                    "sent": "The smallest regular grammar basically just did another combination of combined noun phrases, prepositional phrases, all the things that generally follow verbs into one sort of production.",
                    "label": 0
                },
                {
                    "sent": "So this just gives you a sense.",
                    "label": 0
                },
                {
                    "sent": "I basically tried to sort of.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Span the range of regular grammars here in various ways, and then I did like I talked about earlier, a sort of local search around each of the hand design grammars using this, some just some sample productions.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter that much exactly what they were.",
                    "label": 0
                },
                {
                    "sent": "I looked around and this is a picture of all the grammars that I looked around.",
                    "label": 0
                },
                {
                    "sent": "There's lots of them.",
                    "label": 0
                },
                {
                    "sent": "Then to do the unsupervised automatic HMM learning I actually this might seem sketchy, but I don't think it is used.",
                    "label": 0
                },
                {
                    "sent": "A hmm that was designed for part of speech tagging, so it learns.",
                    "label": 0
                },
                {
                    "sent": "It learns part of Speech Productions, given a sort of corpus of just words.",
                    "label": 0
                },
                {
                    "sent": "But if you give it a corpus of syntactic categories like mine, then it basically groups those syntactic categories.",
                    "label": 0
                },
                {
                    "sent": "And I said those are the non terminals of the regular grammar and you can derive a regular grammar straightforwardly so basically learns and hmm, on this corpus of syntactic categories actually did fairly well.",
                    "label": 0
                },
                {
                    "sent": "I think I mean comparable to all the hand design grammars.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, is that all clear before I?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So evaluation then, as I said earlier, is basically I'm trying to do intuitively what several people yesterday argued what we want to do is linguists is sort of calculate quantitatively this tradeoff between a simplicity and the fit of a grammar to the data.",
                    "label": 0
                },
                {
                    "sent": "We do this using Bayes rule where each of the specific grammars like that I've came about or talked about are examples of the type of grammar that, like hierarchical or linear and in the model we.",
                    "label": 0
                },
                {
                    "sent": "Not biased, favorite.",
                    "label": 0
                },
                {
                    "sent": "Any of them, so that would fall.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Out of the equation and basically then I'm calculating the posterior probability of that grammar given the data, which is basically a combination of how well it fits the corpus times the complexity or the prior.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probability of the grammar.",
                    "label": 0
                },
                {
                    "sent": "And this is sort of a domain general notion of complexity versus fit could do basically exactly the same thing using MDL rather than Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "It does.",
                    "label": 0
                },
                {
                    "sent": "It captures the same thing, of course.",
                    "label": 0
                },
                {
                    "sent": "Which exactly you use sort of slightly changes the.",
                    "label": 0
                },
                {
                    "sent": "Your results, but I've actually done.",
                    "label": 0
                },
                {
                    "sent": "A number of different prior measures and it really doesn't change things.",
                    "label": 0
                },
                {
                    "sent": "I haven't done it in MDL, but it's all the same.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Intuitive idea?",
                    "label": 0
                },
                {
                    "sent": "I did to measure the prior probability of the grammar.",
                    "label": 0
                },
                {
                    "sent": "Here was just basically and I enjoyed this part.",
                    "label": 0
                },
                {
                    "sent": "Pretend I was God.",
                    "label": 0
                },
                {
                    "sent": "I'm designing the grammar.",
                    "label": 0
                },
                {
                    "sent": "I'm saying OK if I'm designing a grammar, what choices do I have to make?",
                    "label": 0
                },
                {
                    "sent": "Well, I have to decide how many non terminals there are for each non terminal have to decide how many productions I'm going to use for each of those productions I have to decide what's going to be on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "And each of those is going to be selections from a distribution of the possible productions an so from that you can basically come up with the equation that says what's the probability of generating each grammar if I have to make all of those choices, and that's it.",
                    "label": 0
                },
                {
                    "sent": "Favors grammars where you have to make fewer production or fewer choices, so fewer productions fewer non terminals to choose from.",
                    "label": 0
                },
                {
                    "sent": "Those are going to have higher prior probability.",
                    "label": 1
                },
                {
                    "sent": "All things being equal.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The likelihood, I think probably many people here already sort of intuitively very familiar with this.",
                    "label": 0
                },
                {
                    "sent": "All I'm doing is calculating the probability that that grammar generated the data, which is basically the product of the probability of each parse.",
                    "label": 0
                },
                {
                    "sent": "Each of the derivations involved, and So what that means is that grammars with recursive productions are going to have slightly lower probability, 'cause they're going to predict.",
                    "label": 0
                },
                {
                    "sent": "You see, these really long senses that don't show up, but also also intuitively a grammar that over generalizes is going to.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have a slightly lower likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what do we find this is?",
                    "label": 0
                },
                {
                    "sent": "I know I cannot think of how to show this other than as a table of hideous numbers.",
                    "label": 0
                },
                {
                    "sent": "But what we find is on the data set that splits by frequency levels.",
                    "label": 0
                },
                {
                    "sent": "So it's a measure of the comprehension of.",
                    "label": 0
                },
                {
                    "sent": "Sort of a measure of comprehension where again the smallest corpus is the one that sort of has just the most common sentence types, and the largest has all of them.",
                    "label": 0
                },
                {
                    "sent": "What you find is that on the smaller corpora, it actually is best to just memorize them and that sort of makes it if you have 8 sentences in your entire language, just memorize it.",
                    "label": 0
                },
                {
                    "sent": "You know don't don't putz around with trying to make a grammar as it gets larger.",
                    "label": 0
                },
                {
                    "sent": "It might be better to sort of learn this kind of 1 state.",
                    "label": 0
                },
                {
                    "sent": "Hmm, which actually does you know you are learning something you're learning something about the probabilities.",
                    "label": 0
                },
                {
                    "sent": "Of transitions between the different syntactic categories, but you're saying anything is kind of is more likely than other.",
                    "label": 0
                },
                {
                    "sent": "But then on the larger corpora and these are still not super large.",
                    "label": 0
                },
                {
                    "sent": "This is just 10 or more context free grammars always favored.",
                    "label": 0
                },
                {
                    "sent": "These are log probabilities.",
                    "label": 0
                },
                {
                    "sent": "So like the yeah, the difference between between these is actually quite large.",
                    "label": 0
                },
                {
                    "sent": "A context free grammar will always be favored here over any of the regular grammars I tried.",
                    "label": 0
                },
                {
                    "sent": "And actually I didn't report them 'cause they're slightly more complicated, but I found context free grammars that do even better than this.",
                    "label": 0
                },
                {
                    "sent": "These are the very best regular grammars I ever found are.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are here.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It gets even more interesting if we look at sort of the data set split by age there at all levels.",
                    "label": 1
                },
                {
                    "sent": "The context free grammar is found, and so what's really interesting here is that even if you just look at one file, one hours worth of conversation, there's enough data in that in that input to be able to say OK. Hierarchical grammar best fits that input, and the reason is again the simplicity trade off.",
                    "label": 0
                },
                {
                    "sent": "It's a lot smaller than the regular grammars.",
                    "label": 0
                },
                {
                    "sent": "There are a few regular grammars that are really small, but they're sort of really badly fit, so at best captures that tradeoff.",
                    "label": 0
                },
                {
                    "sent": "Obviously, I'm not saying that all the kid has to do is pop out of the womb in here an hours worth of input, and then they're like, yeah?",
                    "label": 0
                },
                {
                    "sent": "Clearly you have to know a lot in order to do this, but it's again as a sort of ideal learning what's in the input kind of argument.",
                    "label": 0
                },
                {
                    "sent": "It shows that you know it's all over the place.",
                    "label": 0
                },
                {
                    "sent": "It's ubiquitous.",
                    "label": 0
                },
                {
                    "sent": "The information that tells you that hierarchy.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Programmer is probably the best fit.",
                    "label": 0
                },
                {
                    "sent": "And then it's sort of to relate back to the original.",
                    "label": 0
                },
                {
                    "sent": "The poverty of the stimulus argument that motivated much of this.",
                    "label": 0
                },
                {
                    "sent": "We can look at how well each grammar actually predicts the sentence is it hasn't seen an.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Much like one of the presentations yesterday, what you find is that the context free grammars actually can produce the correct complex interrogative sentence.",
                    "label": 0
                },
                {
                    "sent": "This was a corpus without any complex interrogatives at all, so it can produce something like.",
                    "label": 0
                },
                {
                    "sent": "The sentence here is do Eagles that are alive fly, but any sort of complex interrogative it will produce, but not the wrong one, not the wrong kind.",
                    "label": 0
                },
                {
                    "sent": "That sort of the mistake that children don't make the other regular grammars, so they can't.",
                    "label": 0
                },
                {
                    "sent": "Well the one state grammar will do anything you want basically, but the others they won't.",
                    "label": 0
                },
                {
                    "sent": "And basically that's because they're not fitting in the right way.",
                    "label": 0
                },
                {
                    "sent": "They don't have this notion of phrases, and so therefore they're not.",
                    "label": 0
                },
                {
                    "sent": "They're not generalizing in this.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyway, so I kind of threw a lot of out here and I think there's some.",
                    "label": 0
                },
                {
                    "sent": "Several underlying interesting ideas here, but I think the main sort of points here is simply to, well, there's two main points I want to make.",
                    "label": 0
                },
                {
                    "sent": "First of all is to say as an ideal learning analysis of looking what's in the data.",
                    "label": 0
                },
                {
                    "sent": "An ideal unbiased learner who could actually search around and find these grammars like this should be able to realize simply based on the input that languages do have hierarchical structure and the reason this works is because they're looking at sort of all of the input together rather than sort of, you know, just looking for, say, complex interrogatives and all of the input together really does sort of argue that this notion of phrases is really a good way to capture what's going on.",
                    "label": 0
                },
                {
                    "sent": "If you assume that we're doing any kind of simplicity or an fit to the data trade off that sort of.",
                    "label": 0
                },
                {
                    "sent": "Approximate to this.",
                    "label": 0
                },
                {
                    "sent": "And then the other thing I think is useful here is that it's an example of what we saw in the first talk yesterday.",
                    "label": 0
                },
                {
                    "sent": "Being argued would be good for linguists to do, which is perhaps you'll disagree with me, but which is to basically enable us to rigorously kind of evaluate different sorts of grammars, or different sorts of analysis using a kind of tradeoff between the simplicity and the goodness of fit.",
                    "label": 0
                },
                {
                    "sent": "So I think that's a sort of valuable technique to keep in mind going forward, which I think you know various people here already know and do, but it also argues that in some ways this sort of higher order knowledge, knowledge about things like hierarchical structure might be easier to learn than very basic item specific knowledge, because you can use the input from an entire data set and you're only choosing between, say, a few other certain options, and so that's also good to keep in mind when we're talking about learnability arguments in June.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is my last slide, I just want to leave off or end with all my caveats.",
                    "label": 0
                },
                {
                    "sent": "The main ones being of course this is an ideal learner thing.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure that I found the best grammars, but I think I'm getting more and more confident that I found.",
                    "label": 0
                },
                {
                    "sent": "Then if a regular grammar exists that is better than these context free grammars, it's being hard to find.",
                    "label": 0
                },
                {
                    "sent": "I sort of done basically, lots and lots of ways of searching and to try to find those, and I probably haven't found the best context free grammar, but there's a lot of context free grammars that beat this sort of.",
                    "label": 0
                },
                {
                    "sent": "All the best regular grammars I've found, so I think that I'm getting more confident that I found those.",
                    "label": 0
                },
                {
                    "sent": "Obviously can't be sure, so if so, it's only as good as that.",
                    "label": 0
                },
                {
                    "sent": "Then the other thing I think a big issue that that also came up is this issue of how you calculate complexity and fit to the data to some extent how complex something is is going to be with respect to its representation or the underlying machine.",
                    "label": 0
                },
                {
                    "sent": "That's sort of doing the work, and that is an issue.",
                    "label": 0
                },
                {
                    "sent": "I think it's not.",
                    "label": 0
                },
                {
                    "sent": "An enormous issue.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean, I haven't tried things that are just totally bizarre representations, but the prior probability was sort of a very straightforward one.",
                    "label": 0
                },
                {
                    "sent": "It's not like I sort of made it up out of thin air and it.",
                    "label": 0
                },
                {
                    "sent": "Comes out of how you assume the grammars are constructed, and again a sort of most straightforward MDL analysis.",
                    "label": 0
                },
                {
                    "sent": "Or like slightly different ways of measuring the prior.",
                    "label": 0
                },
                {
                    "sent": "I think you'd get basically the same result, but it is a kind of open question of how you know how general and how much we can use this sort of notion of simplicity and still kind of wrestling with that of, and also whether people's brains, whether we individuals care about it that much.",
                    "label": 0
                },
                {
                    "sent": "But I think there's good reasons to think that we might.",
                    "label": 0
                },
                {
                    "sent": "And yeah, those are the main assumptions.",
                    "label": 0
                },
                {
                    "sent": "You can read the others as well as I can and I think I'm almost out of time.",
                    "label": 0
                },
                {
                    "sent": "So yeah, thank you.",
                    "label": 0
                }
            ]
        }
    }
}