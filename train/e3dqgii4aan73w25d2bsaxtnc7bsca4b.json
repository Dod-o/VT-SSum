{
    "id": "e3dqgii4aan73w25d2bsaxtnc7bsca4b",
    "title": "Online Learning for Latent Dirichlet Allocation",
    "info": {
        "author": [
            "Matt Hoffman, Adobe Systems Incorporated"
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Optimization Methods->Stochastic Optimization"
        ]
    },
    "url": "http://videolectures.net/nips2010_hoffman_oll/",
    "segmentation": [
        [
            "Alright, thanks for that introduction and and my apologies.",
            "So yeah, so I'm going to be talking about a new fast learning method for fitting topics in the topic model latent."
        ],
        [
            "Allocation so late and early allocation or LDA is a hierarchical Bayesian topic model.",
            "It's a model of document collections and the way it explains the variation in a set of documents is in terms of this set of latent topic distributions, which are basically just distributions over the vocabulary in the corpus.",
            "So so in these topics, what you tend to see is that words that have something to do with one another tend to have high probability.",
            "So for example, a topic about business might give high probability words like Corporation money.",
            "$1,000,000 etc.",
            "So what we'd like to do is fit these topics efficiently to very large data."
        ],
        [
            "That's so the way that is done is using posterior inference, but unfortunately the true posterior as it so often is, is intractable to compute exactly, so we have to appeal to approximate inference methods.",
            "One I'll be talking about is variational Bayes, which is a deterministic alternative to the more commonly used Markov chain Monte Carlo methods and the basic idea is that instead of trying to sample from the posterior distribution where trying to optimize some other distribution that has a convenient form to be as close as possible to the true posterior we're interested in.",
            "In terms of scale, divergent and the nice thing about these methods, they tend to be fast compared to MCMC methods.",
            "They tend to converge in fewer iterations, but there still batch methods, and so they scale linearly with the number of documents and therefore can be slow for very large datasets.",
            "So what we'd like to do is not have is to be able to fit these approximate posteriors to our topics without having to look at the entire corpus every iteration and the way we do that is using a variant on the stochastic gradient method, so it turns out that you can actually derive a. I won't go into the details, but we can talk about the poster, but it turns out that you can actually derive."
        ],
        [
            "A very simple stochastic natural gradient algorithm for optimizing the variational objective function that only has to look at one or one document or a few documents at a time.",
            "And with this graph is showing is basically the progress that there's online algorithm makes compared with the corresponding batch algorithm.",
            "So accesses perplexity.",
            "It's a measure of model fit, lower is better, and so this red line is the progress made by the batch algorithm fit to just 3% of the corpus.",
            "And it's it's slow by comparison to our online method.",
            "Very slow, and so basically this is just a very, very fast way of fitting topics in LDA 2 very large datasets.",
            "And the nice thing about this method is that it's actually applicable more generally, not just the LDA.",
            "There's nothing special about LDA, so we look forward to applying it to other models, and I'll also just want to mention we've got implementations.",
            "I've got some Python code on my website and now it's been.",
            "Incorporated into the developer Web framework as well.",
            "And that's a very fast implementation, so please come to the poster.",
            "It's Wat in the probabilistic model section.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, thanks for that introduction and and my apologies.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so I'm going to be talking about a new fast learning method for fitting topics in the topic model latent.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Allocation so late and early allocation or LDA is a hierarchical Bayesian topic model.",
                    "label": 0
                },
                {
                    "sent": "It's a model of document collections and the way it explains the variation in a set of documents is in terms of this set of latent topic distributions, which are basically just distributions over the vocabulary in the corpus.",
                    "label": 1
                },
                {
                    "sent": "So so in these topics, what you tend to see is that words that have something to do with one another tend to have high probability.",
                    "label": 0
                },
                {
                    "sent": "So for example, a topic about business might give high probability words like Corporation money.",
                    "label": 0
                },
                {
                    "sent": "$1,000,000 etc.",
                    "label": 0
                },
                {
                    "sent": "So what we'd like to do is fit these topics efficiently to very large data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's so the way that is done is using posterior inference, but unfortunately the true posterior as it so often is, is intractable to compute exactly, so we have to appeal to approximate inference methods.",
                    "label": 0
                },
                {
                    "sent": "One I'll be talking about is variational Bayes, which is a deterministic alternative to the more commonly used Markov chain Monte Carlo methods and the basic idea is that instead of trying to sample from the posterior distribution where trying to optimize some other distribution that has a convenient form to be as close as possible to the true posterior we're interested in.",
                    "label": 1
                },
                {
                    "sent": "In terms of scale, divergent and the nice thing about these methods, they tend to be fast compared to MCMC methods.",
                    "label": 0
                },
                {
                    "sent": "They tend to converge in fewer iterations, but there still batch methods, and so they scale linearly with the number of documents and therefore can be slow for very large datasets.",
                    "label": 1
                },
                {
                    "sent": "So what we'd like to do is not have is to be able to fit these approximate posteriors to our topics without having to look at the entire corpus every iteration and the way we do that is using a variant on the stochastic gradient method, so it turns out that you can actually derive a. I won't go into the details, but we can talk about the poster, but it turns out that you can actually derive.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A very simple stochastic natural gradient algorithm for optimizing the variational objective function that only has to look at one or one document or a few documents at a time.",
                    "label": 1
                },
                {
                    "sent": "And with this graph is showing is basically the progress that there's online algorithm makes compared with the corresponding batch algorithm.",
                    "label": 1
                },
                {
                    "sent": "So accesses perplexity.",
                    "label": 0
                },
                {
                    "sent": "It's a measure of model fit, lower is better, and so this red line is the progress made by the batch algorithm fit to just 3% of the corpus.",
                    "label": 0
                },
                {
                    "sent": "And it's it's slow by comparison to our online method.",
                    "label": 0
                },
                {
                    "sent": "Very slow, and so basically this is just a very, very fast way of fitting topics in LDA 2 very large datasets.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing about this method is that it's actually applicable more generally, not just the LDA.",
                    "label": 0
                },
                {
                    "sent": "There's nothing special about LDA, so we look forward to applying it to other models, and I'll also just want to mention we've got implementations.",
                    "label": 0
                },
                {
                    "sent": "I've got some Python code on my website and now it's been.",
                    "label": 0
                },
                {
                    "sent": "Incorporated into the developer Web framework as well.",
                    "label": 0
                },
                {
                    "sent": "And that's a very fast implementation, so please come to the poster.",
                    "label": 0
                },
                {
                    "sent": "It's Wat in the probabilistic model section.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}