{
    "id": "o46ujavohoylml7lm6avulirr44oilni",
    "title": "On Convergence of Emphatic Temporal-Difference Learning",
    "info": {
        "author": [
            "Huizhen Yu, Department of Computing Science, University of Alberta"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_yu_difference_learning/",
    "segmentation": [
        [
            "In this paper."
        ],
        [
            "We consider approximate policy evaluation for Markov decision processes with finite state elections basis under a discounted total cost total reward criterion.",
            "We are interested in.",
            "Policy learning, so we want to.",
            "We want to evaluate simultaneously multiple policies which we call target policies from one exploratory behavior, which we call the behavior policy.",
            "All these policies are randomized stationary, so it is well known that standard temporal difference algorithms with linear function approximation need not converge for off policy learning.",
            "There are other methods based on these squares or gradient.",
            "These methods do not have this.",
            "Divergens difficulty, but they are more complex, so recently such a Mohammedan White proposed a new algorithm called Emphatic TD Learning to address these divergences, difficulty of standard teeing off policy learning discipline.",
            "In this paper we give the first convergence analysis of their algorithm, then show that it is sound for policy learning.",
            "So this emphatic TD learning or ET for short aims to solve our projected multistep Bellman equation just like a standard.",
            "You would do, but it employs a novel, very interesting weighting scheme.",
            "Whereby the state at each time step our weighted non uniformly in the TD updates.",
            "This algorithm can accommodate users interest in learning values of the target policy cell specific state.",
            "So by doing so it can potentially obtain more accurate estimates for these states of interest.",
            "Yet the algorithm maintains a salient stability property which is standard Deluxe for policy learning."
        ],
        [
            "Let me quickly.",
            "Explain how the ETA algorithm works.",
            "It takes as input a sequence of states, actions, and rewards generated by the behavior policy and a non negative function.",
            "I have states indicating our interest in each state.",
            "The Utd algorithm generates a sequence of parameters data T for the approximate value functions for a given target policy.",
            "The recursion on Theta T involves the so called eligibility trace.",
            "The temporal difference term and the important sampling wait.",
            "This is just like in Standard TD, except that the eligibility traces updated differently.",
            "It involves an emphasis weight empty for this state.",
            "T and this emphasis weight itself is calculated iteratively using a scalar sequence of iterates called following traces.",
            "By comparison, the standard of Policy TD algorithm has the constant of one in place of this emphasis weight empty, so similarly there is at least grab version of the ET algorithm which we call ESTD.",
            "the IT calculates matrix and iterate vector and this would be exactly as the policy STD except that the eligibility traces calculated differently.",
            "So without going into the details, just say that one very important property of these vertical algorithm is that the emphasis weights now reflect the occupation frequencies of the target policy rather than the behavior policy.",
            "So these weights ultimately determined the weighted Euclidean norm for the projection operator in the projected Bellman equation that this algorithm to solve.",
            "So it is this weighting scheme that will give ET D designers.",
            "Ability property that will discuss more on the next slide.",
            "So the."
        ],
        [
            "Results of these paper is about the stability and convergence of this new algorithm.",
            "We use an ODI based proof method to analyze the algorithm.",
            "We can write its associated minority as a dot equals C data plus B.",
            "Here C is asymmetric.",
            "Biomatrix BS and dimensional vector.",
            "So thanks to the emphatic weighting scheme we know due to their earlier work that this matrix C is always the negative semidefinite.",
            "We prove in our paper unnecessary sufficient condition for this matrix to be.",
            "Negative definite, it says that it is that the setup feature vectors of the emphasized states must contain linearly independent vectors.",
            "As a result of that, we also obtain sufficient condition which says that the set of feature vectors of the states of interest must contain linearly independent vectors.",
            "This condition can be satisfied easily in practice without model knowledge.",
            "So the condition that makes E negative definite can be fulfilled in practice.",
            "Without knowing the model.",
            "When the matrix Y is negative definite, we know that the desired solution they does start is a globally asymptotically stable for this Minardi so the effort in the convergence analysis is and to prove that the ET iterates asymptotically track the solution of the Minardi."
        ],
        [
            "So the main result we get is as follows.",
            "So ETD converges almost surely to the desired solution for step size is decreasing at a rate of 1 / T. This result is proved and almost minimal condition on the behavior policy under which the trace iterates can have their variance is growing to Infinity.",
            "We believe the convergence also holds for larger step sizes if we can place more conditions on the behavior policy so that it is sufficiently close to the.",
            "Target policy so I don't have time to explain the proof, so just to mention that it goes proceeding several stages, it uses, among others the property or Garden City property of the Markov chain on the joint space of states actions and traces and the convergence property of the least square version of diagram and a constraint version of the algorithm for more details."
        ],
        [
            "You can find this paper archive also in the code preceding and you can find more details about the ET algorithm archive this paper, original paper by certain Hamilton White.",
            "And finally we also have a short, much shorter.",
            "Paper to be presented at EWIL and it gives an insight for summarizes that works at both works, and I give some insight for discussion about the algorithm and some preliminary experimental results.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this paper.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We consider approximate policy evaluation for Markov decision processes with finite state elections basis under a discounted total cost total reward criterion.",
                    "label": 0
                },
                {
                    "sent": "We are interested in.",
                    "label": 0
                },
                {
                    "sent": "Policy learning, so we want to.",
                    "label": 0
                },
                {
                    "sent": "We want to evaluate simultaneously multiple policies which we call target policies from one exploratory behavior, which we call the behavior policy.",
                    "label": 1
                },
                {
                    "sent": "All these policies are randomized stationary, so it is well known that standard temporal difference algorithms with linear function approximation need not converge for off policy learning.",
                    "label": 1
                },
                {
                    "sent": "There are other methods based on these squares or gradient.",
                    "label": 0
                },
                {
                    "sent": "These methods do not have this.",
                    "label": 1
                },
                {
                    "sent": "Divergens difficulty, but they are more complex, so recently such a Mohammedan White proposed a new algorithm called Emphatic TD Learning to address these divergences, difficulty of standard teeing off policy learning discipline.",
                    "label": 0
                },
                {
                    "sent": "In this paper we give the first convergence analysis of their algorithm, then show that it is sound for policy learning.",
                    "label": 0
                },
                {
                    "sent": "So this emphatic TD learning or ET for short aims to solve our projected multistep Bellman equation just like a standard.",
                    "label": 1
                },
                {
                    "sent": "You would do, but it employs a novel, very interesting weighting scheme.",
                    "label": 1
                },
                {
                    "sent": "Whereby the state at each time step our weighted non uniformly in the TD updates.",
                    "label": 1
                },
                {
                    "sent": "This algorithm can accommodate users interest in learning values of the target policy cell specific state.",
                    "label": 0
                },
                {
                    "sent": "So by doing so it can potentially obtain more accurate estimates for these states of interest.",
                    "label": 0
                },
                {
                    "sent": "Yet the algorithm maintains a salient stability property which is standard Deluxe for policy learning.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me quickly.",
                    "label": 0
                },
                {
                    "sent": "Explain how the ETA algorithm works.",
                    "label": 0
                },
                {
                    "sent": "It takes as input a sequence of states, actions, and rewards generated by the behavior policy and a non negative function.",
                    "label": 0
                },
                {
                    "sent": "I have states indicating our interest in each state.",
                    "label": 0
                },
                {
                    "sent": "The Utd algorithm generates a sequence of parameters data T for the approximate value functions for a given target policy.",
                    "label": 1
                },
                {
                    "sent": "The recursion on Theta T involves the so called eligibility trace.",
                    "label": 0
                },
                {
                    "sent": "The temporal difference term and the important sampling wait.",
                    "label": 0
                },
                {
                    "sent": "This is just like in Standard TD, except that the eligibility traces updated differently.",
                    "label": 0
                },
                {
                    "sent": "It involves an emphasis weight empty for this state.",
                    "label": 0
                },
                {
                    "sent": "T and this emphasis weight itself is calculated iteratively using a scalar sequence of iterates called following traces.",
                    "label": 0
                },
                {
                    "sent": "By comparison, the standard of Policy TD algorithm has the constant of one in place of this emphasis weight empty, so similarly there is at least grab version of the ET algorithm which we call ESTD.",
                    "label": 0
                },
                {
                    "sent": "the IT calculates matrix and iterate vector and this would be exactly as the policy STD except that the eligibility traces calculated differently.",
                    "label": 0
                },
                {
                    "sent": "So without going into the details, just say that one very important property of these vertical algorithm is that the emphasis weights now reflect the occupation frequencies of the target policy rather than the behavior policy.",
                    "label": 1
                },
                {
                    "sent": "So these weights ultimately determined the weighted Euclidean norm for the projection operator in the projected Bellman equation that this algorithm to solve.",
                    "label": 0
                },
                {
                    "sent": "So it is this weighting scheme that will give ET D designers.",
                    "label": 0
                },
                {
                    "sent": "Ability property that will discuss more on the next slide.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Results of these paper is about the stability and convergence of this new algorithm.",
                    "label": 0
                },
                {
                    "sent": "We use an ODI based proof method to analyze the algorithm.",
                    "label": 0
                },
                {
                    "sent": "We can write its associated minority as a dot equals C data plus B.",
                    "label": 0
                },
                {
                    "sent": "Here C is asymmetric.",
                    "label": 0
                },
                {
                    "sent": "Biomatrix BS and dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "So thanks to the emphatic weighting scheme we know due to their earlier work that this matrix C is always the negative semidefinite.",
                    "label": 1
                },
                {
                    "sent": "We prove in our paper unnecessary sufficient condition for this matrix to be.",
                    "label": 1
                },
                {
                    "sent": "Negative definite, it says that it is that the setup feature vectors of the emphasized states must contain linearly independent vectors.",
                    "label": 0
                },
                {
                    "sent": "As a result of that, we also obtain sufficient condition which says that the set of feature vectors of the states of interest must contain linearly independent vectors.",
                    "label": 1
                },
                {
                    "sent": "This condition can be satisfied easily in practice without model knowledge.",
                    "label": 0
                },
                {
                    "sent": "So the condition that makes E negative definite can be fulfilled in practice.",
                    "label": 0
                },
                {
                    "sent": "Without knowing the model.",
                    "label": 0
                },
                {
                    "sent": "When the matrix Y is negative definite, we know that the desired solution they does start is a globally asymptotically stable for this Minardi so the effort in the convergence analysis is and to prove that the ET iterates asymptotically track the solution of the Minardi.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main result we get is as follows.",
                    "label": 0
                },
                {
                    "sent": "So ETD converges almost surely to the desired solution for step size is decreasing at a rate of 1 / T. This result is proved and almost minimal condition on the behavior policy under which the trace iterates can have their variance is growing to Infinity.",
                    "label": 1
                },
                {
                    "sent": "We believe the convergence also holds for larger step sizes if we can place more conditions on the behavior policy so that it is sufficiently close to the.",
                    "label": 1
                },
                {
                    "sent": "Target policy so I don't have time to explain the proof, so just to mention that it goes proceeding several stages, it uses, among others the property or Garden City property of the Markov chain on the joint space of states actions and traces and the convergence property of the least square version of diagram and a constraint version of the algorithm for more details.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can find this paper archive also in the code preceding and you can find more details about the ET algorithm archive this paper, original paper by certain Hamilton White.",
                    "label": 0
                },
                {
                    "sent": "And finally we also have a short, much shorter.",
                    "label": 0
                },
                {
                    "sent": "Paper to be presented at EWIL and it gives an insight for summarizes that works at both works, and I give some insight for discussion about the algorithm and some preliminary experimental results.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}