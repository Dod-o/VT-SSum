{
    "id": "amtde2x5gkchwevpwybrx66ld44w7xgc",
    "title": "Half transductive ranking",
    "info": {
        "author": [
            "Jason Weston, Facebook"
        ],
        "published": "June 3, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Information Extraction"
        ]
    },
    "url": "http://videolectures.net/aistats2010_weston_htr/",
    "segmentation": [
        [
            "Hello everybody.",
            "Yes, this is in collaboration with my coauthors being David Reynolds, Kareena Ramaria.",
            "I'm gonna have a lot of trouble with this laptop 'cause it sliding off.",
            "I guess I'll just hold it.",
            "So yeah, this this title half transductive ranking, it's about a novel framework for ranking is like information retrieval set up, and is kind of hybrid between functional and transductive rankings, and I'm just going to maybe I'll just jump."
        ],
        [
            "Straight to what?"
        ],
        [
            "Yeah, learning to rank is so."
        ],
        [
            "This is a setting where you have an input which is document set D and query Q an you want to output ranking of those documents.",
            "That document set such as the most relevant documents on top and you have a scoring function.",
            "So score QD here and that gives the score for only a single document and query.",
            "And then you're going to compute that score for all documents.",
            "And then just sort those scores.",
            "So yeah, so you might want to train that kind of model, and you can do that in various setups.",
            "So one way is in a functional setup."
        ],
        [
            "So you might have training queries and training documents with some kind of level of supervision.",
            "Let's say preference relations or something like that, and you learn a model score, QD, which is a function of some joint feature map.",
            "So it's written here 5 QD.",
            "So if you do do that, it's what I mean by functional is when you have a test set.",
            "Actually, if you have new test queries or new test documents, that kind of model still work 'cause you can just apply that scoring function to those to those new queries or documents.",
            "And as long as they are from same distribution, that kind of thing could still work.",
            "So lots of algorithms that do that, like ranking, perceptrons, ranking, SVM's and so on with preference relations.",
            "But you could also consider algorithms like LSI which just unsupervised.",
            "Also doing that kind of thing.",
            "So."
        ],
        [
            "That's classical, another classical setup is a transductive ranking set up.",
            "An in there you consider that you have all your documents or all your queries or both, all the ones you're ever going to see at training time, so they overlap completely with the ones at Test time.",
            "OK, and there's lots of algorithms like Laplacian Eigen Maps and the other ones written down here that do that and why is that a good thing to do?",
            "Well if you do that you can have some nice properties with the optimization problem that you can specify.",
            "You can for example learn with graph like an adjacency matrix and have one parameter vector for every document, say like an N dimensional vector.",
            "So and then learn all of those vectors based on your proximity information.",
            "And when you do that, those algorithms they don't actually rely on."
        ],
        [
            "Features, so that's the difference between this functional ranking I showed you before, which relied on these these feature."
        ],
        [
            "See if I QD so in that sense they can be considered like nonlinear algorithms, so it gives you kind of an easy way to do optimization where you get nonlinearity.",
            "Now the problem with those algorithms is you typically don't have an out of sample extension unless you do.",
            "I mean, there's various papers on that trying to make our sample extensions for those algorithms.",
            "So basically, if you had and you query or new document you don't know.",
            "What is vector?",
            "Because you learned all these vectors independently just for those those documents you had.",
            "So that leads to the."
        ],
        [
            "The title of this talk, half transductive ranking and this hybrid of the two things I just showed."
        ],
        [
            "So the setting we consider is 1 where you have a fixed set of documents that you know you given them at training time, and you know at Test time you're actually going to be ranking that set of documents.",
            "So in that sense you could do something transductive with that set of documents right is fixed, but the queries that we're going to use to rank that set of documents, we might see new ones in the test set.",
            "OK, so we're going to have a fixed set of documents we might train with a set of training queries, but test.",
            "And we might see a new query, so This is why this thing is called half transductive because we're going to develop some algorithms which sort of borrow from both the functional and transductive approaches.",
            "So for the documents we can we can do this kind of transductive representation, having a vector for every document, but for queries we're going to do something functional so that we can have an out of sample extension if you like when we have new queries.",
            "We're going to make.",
            "Learning algorithms that learn both those things at once jointly.",
            "And this is the advantage of that is that we're going to get.",
            "Nonlinear nonlinear algorithms with simple optimization also."
        ],
        [
            "Highly scalable."
        ],
        [
            "Um, yeah, so so."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everybody.",
                    "label": 0
                },
                {
                    "sent": "Yes, this is in collaboration with my coauthors being David Reynolds, Kareena Ramaria.",
                    "label": 0
                },
                {
                    "sent": "I'm gonna have a lot of trouble with this laptop 'cause it sliding off.",
                    "label": 0
                },
                {
                    "sent": "I guess I'll just hold it.",
                    "label": 0
                },
                {
                    "sent": "So yeah, this this title half transductive ranking, it's about a novel framework for ranking is like information retrieval set up, and is kind of hybrid between functional and transductive rankings, and I'm just going to maybe I'll just jump.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Straight to what?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, learning to rank is so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a setting where you have an input which is document set D and query Q an you want to output ranking of those documents.",
                    "label": 1
                },
                {
                    "sent": "That document set such as the most relevant documents on top and you have a scoring function.",
                    "label": 1
                },
                {
                    "sent": "So score QD here and that gives the score for only a single document and query.",
                    "label": 0
                },
                {
                    "sent": "And then you're going to compute that score for all documents.",
                    "label": 0
                },
                {
                    "sent": "And then just sort those scores.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so you might want to train that kind of model, and you can do that in various setups.",
                    "label": 0
                },
                {
                    "sent": "So one way is in a functional setup.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you might have training queries and training documents with some kind of level of supervision.",
                    "label": 0
                },
                {
                    "sent": "Let's say preference relations or something like that, and you learn a model score, QD, which is a function of some joint feature map.",
                    "label": 1
                },
                {
                    "sent": "So it's written here 5 QD.",
                    "label": 0
                },
                {
                    "sent": "So if you do do that, it's what I mean by functional is when you have a test set.",
                    "label": 1
                },
                {
                    "sent": "Actually, if you have new test queries or new test documents, that kind of model still work 'cause you can just apply that scoring function to those to those new queries or documents.",
                    "label": 0
                },
                {
                    "sent": "And as long as they are from same distribution, that kind of thing could still work.",
                    "label": 0
                },
                {
                    "sent": "So lots of algorithms that do that, like ranking, perceptrons, ranking, SVM's and so on with preference relations.",
                    "label": 0
                },
                {
                    "sent": "But you could also consider algorithms like LSI which just unsupervised.",
                    "label": 0
                },
                {
                    "sent": "Also doing that kind of thing.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's classical, another classical setup is a transductive ranking set up.",
                    "label": 1
                },
                {
                    "sent": "An in there you consider that you have all your documents or all your queries or both, all the ones you're ever going to see at training time, so they overlap completely with the ones at Test time.",
                    "label": 0
                },
                {
                    "sent": "OK, and there's lots of algorithms like Laplacian Eigen Maps and the other ones written down here that do that and why is that a good thing to do?",
                    "label": 0
                },
                {
                    "sent": "Well if you do that you can have some nice properties with the optimization problem that you can specify.",
                    "label": 1
                },
                {
                    "sent": "You can for example learn with graph like an adjacency matrix and have one parameter vector for every document, say like an N dimensional vector.",
                    "label": 1
                },
                {
                    "sent": "So and then learn all of those vectors based on your proximity information.",
                    "label": 0
                },
                {
                    "sent": "And when you do that, those algorithms they don't actually rely on.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Features, so that's the difference between this functional ranking I showed you before, which relied on these these feature.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See if I QD so in that sense they can be considered like nonlinear algorithms, so it gives you kind of an easy way to do optimization where you get nonlinearity.",
                    "label": 0
                },
                {
                    "sent": "Now the problem with those algorithms is you typically don't have an out of sample extension unless you do.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's various papers on that trying to make our sample extensions for those algorithms.",
                    "label": 0
                },
                {
                    "sent": "So basically, if you had and you query or new document you don't know.",
                    "label": 0
                },
                {
                    "sent": "What is vector?",
                    "label": 0
                },
                {
                    "sent": "Because you learned all these vectors independently just for those those documents you had.",
                    "label": 0
                },
                {
                    "sent": "So that leads to the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The title of this talk, half transductive ranking and this hybrid of the two things I just showed.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the setting we consider is 1 where you have a fixed set of documents that you know you given them at training time, and you know at Test time you're actually going to be ranking that set of documents.",
                    "label": 1
                },
                {
                    "sent": "So in that sense you could do something transductive with that set of documents right is fixed, but the queries that we're going to use to rank that set of documents, we might see new ones in the test set.",
                    "label": 1
                },
                {
                    "sent": "OK, so we're going to have a fixed set of documents we might train with a set of training queries, but test.",
                    "label": 0
                },
                {
                    "sent": "And we might see a new query, so This is why this thing is called half transductive because we're going to develop some algorithms which sort of borrow from both the functional and transductive approaches.",
                    "label": 0
                },
                {
                    "sent": "So for the documents we can we can do this kind of transductive representation, having a vector for every document, but for queries we're going to do something functional so that we can have an out of sample extension if you like when we have new queries.",
                    "label": 0
                },
                {
                    "sent": "We're going to make.",
                    "label": 0
                },
                {
                    "sent": "Learning algorithms that learn both those things at once jointly.",
                    "label": 0
                },
                {
                    "sent": "And this is the advantage of that is that we're going to get.",
                    "label": 0
                },
                {
                    "sent": "Nonlinear nonlinear algorithms with simple optimization also.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Highly scalable.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, yeah, so so.",
                    "label": 0
                }
            ]
        }
    }
}