{
    "id": "pkpzzdrahbwi3dgmt2wgwrduzwefob2e",
    "title": "Bayesian Inference for Plackett-Luce Ranking Models",
    "info": {
        "author": [
            "John Guiver, Microsoft Research, Cambridge, Microsoft Research"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_guiver_bip/",
    "segmentation": [
        [
            "OK, see see where um.",
            "Going to be talking about packet Lube ranking models.",
            "I've run the Littman spell checker here, but there's a serious point behind this, and that's that packet loose models are not very well studied in the machine learning community, so hopefully this talk will start to put that right and I'll I'll describe these models.",
            "They have very nice properties.",
            "But the second piece of this is the inference and the inference is.",
            "That I'm going to be using as a variational technique or power EP, which is also very underutilized mechanism within machine learning community since it came out in 2004 I I only know three papers that have used it and I think Max you have one that way that you've used Power EP.",
            "Is that right?",
            "Technical report are rejected.",
            "OK so there you go.",
            "OK, so."
        ],
        [
            "OSC rankings are important in the IR community.",
            "There's been learning to rank workshops and many learning to rank sessions in the last few cigar conferences, for example.",
            "Um, so the ranking problem is is starting to be studied by the machine learning community and in some depth.",
            "And we're going to be looking at situation where we have data in the form of multiple independent orderings of a set of K items, and these might be full ordering, so they might be partial orderings.",
            "And we'd like to learn a parameterized model over those rankings."
        ],
        [
            "So let's start off just with a bit of notation.",
            "This isn't going to be a very mathematical talk, but it's important just to get some notational fundamentals out of the way.",
            "First thing to notice is that you have a list of items.",
            "The representation for the items and the rank positions are exactly the same, so we have items numbered one to N. We have positions, sorry.",
            "One to K, we have positions one to K, and it's very easy to get confused between the two.",
            "And that the two concepts.",
            "One is the concept of ranking and ranking is a permutation which Maps item indices to position indices and we represent those by Greek letter rho.",
            "And so Ro I is a is a rank position.",
            "In a ranking and ordering.",
            "Is a permutation which Maps position indices to item indices.",
            "So here and we represent these by Omega and so Omega K is an item index.",
            "That who's who's rank position is position K and the distribution is in terms that we are going to describes is described in terms of an ordering rather than the ranking, but the tour equivalent.",
            "You can go backwards and forwards between the two and try not to get confused.",
            "OK, So what is the ranking distribution well?"
        ],
        [
            "It's a distribution over the domain of all possible rankings given a set of K items.",
            "There K factorial possible rankings, so a full fully parameterized distribution would have a probability of each of those K factorial rankings.",
            "So here you see.",
            "Three items we have 3 factorial rankings, 1231322 and three etc.",
            "Those can be rankings or orderings.",
            "And the parameters are all a probabilities of each of those rankings, so they sum up to one, and they're all positive.",
            "So a particular ranking distribution is a point in that simplex.",
            "It's a point in K dimensional space satisfying those properties.",
            "And then a model is a is a parameterized family within that simplex.",
            "So the parameterized family we're going to look at is."
        ],
        [
            "It's the plackett Luce model.",
            "And there are various nice interpretations of the plackett Luce model.",
            "So the first interpretation I want to look at is the Vars interpretation and.",
            "It starts off by having however many vases you have items.",
            "So in this case we have three items, red, blue and green.",
            "And we pretend these these vases vases rather urns because they have an infinite number of balls in.",
            "But the ratios given by VR.",
            "VP&VG are the proportion of balls image in each bars.",
            "OK, we're now going to mix these these balls up into one, so if you watch carefully, there's a bit of animation here.",
            "There we go.",
            "So so so now.",
            "So now we're going to sample from this this infinite virus.",
            "And, um.",
            "Will will pick will pick will pick one out and it turns out to be a red.",
            "OK, and the probability of picking that red is is obviously VR, VR, plus VG plus VB OK, since those are the proportions of balls within this infinite bars.",
            "And then we pick another ball out and the next ball turns out it's red again.",
            "Well, what we want actually is a permutation is a permutation of our items, and so we have to just reject this one.",
            "So we just throw it back in in the bars and we keep on picking out until we get a new color.",
            "And suppose that's green.",
            "Well, now we've taken out all the Red Bulls, and so we're just interesting green and blue, and so the probability of that that green is now VG of the VG plus VB.",
            "And then finally we pick out the blue and of course the blue is probability one.",
            "We just keep on.",
            "Putting bowls back until we get a blue one.",
            "OK, so the probability of the of the ordering red, green, blue is the product of these three factors where the last one is always the trivial factor.",
            "And so in general, that defines the packet loss."
        ],
        [
            "Model so the packet loss model says.",
            "We have an ordering Omega.",
            "And we have a parameter at vector V, so V in the Vars interpretation of these proportions of colored balls.",
            "And it's a product of factors and the factors all look like.",
            "So the fact is, prior to these factors here, which all look like like this that we saw in the previous slide.",
            "So the case factor is the V for the parameter for the particular item in the case position of the summer parameters for the item in the Cape position.",
            "To the item in the bottom position, right?",
            "So it's a it's a product of these ratios with simple terms in the numerator and these summations on the denominator.",
            "OK, so now that's for a complete org."
        ],
        [
            "Ring how about partial orderings?",
            "Well, there's two ways we can consider partial orderings.",
            "One is a sort of a top, a top end, and the other is a true partial ordering.",
            "So I've added a another color in here to make this a little bit more understanding, understandable, and.",
            "So for the top end, we're going to say we just.",
            "Say have had the items in the top two positions and all the other items are below the top two positions.",
            "Now the probability of that, so we pick out a green here probability, the green SVG over the sum of all the others.",
            "We pick out another.",
            "It's green, we reject it and pick out a blue, and that's similar like the packet loose term we saw before.",
            "But now we just stop OK, everything else is below there, so the likelihood of.",
            "This top Top 2 being green blue is given by the product of these two terms.",
            "Now partial ordering is slightly different and partial ordering we're saying.",
            "The other two balls, the red and the and the and the and the orange, can be absolutely anywhere in the ordering.",
            "They don't have to be below the two we pick out.",
            "So we pick out a green.",
            "We pick out a red, we reject it 'cause we're just interested in blue and green.",
            "We pick out the blue.",
            "And now here, with marginalized out red and Orange and we're just left with a placket.",
            "Luce distribution of over the Green and the blue.",
            "So it has it's nice property.",
            "But if you marginalized out certain items, you're still left with the same form of distribution with only the variables that remain in the problem there.",
            "So."
        ],
        [
            "Just a point of interest here.",
            "So when you're just looking at partial orderings that consists of a pair of items, this is the Bradley Terry model."
        ],
        [
            "OK, so now the plackett Luce distribution was described initially by placket in 75, but it was sort of pre.",
            "Those is really based on some ideas of loose talking about choice.",
            "An axiom of choice.",
            "For choosing things from a set and that was work in the 50s and John Martin who wrote her defensive book on on rank distributions in 95 coined the phrase plackett Luce.",
            "So what what Lou said is when you when you choose something.",
            "So suppose we the probability of choosing wine out of some set is the same as the probability of choosing, say, a subset.",
            "Say the alcoholic drinks.",
            "Time is the probability of choosing wine out of that subset, and if you do a series of choices like that in a sort of a multi stage series of choices, then you'll get the plackett Luce distribution.",
            "In fact, the Plackett Luce is the only multi stage distribution that satisfies that Axiom."
        ],
        [
            "OK, one other interpretation of packet this I wanted to describe which is very interesting and this is the Gumbel, thurstonii and model.",
            "I wonder what Felinton spell checker would have done to this, but first only and model is 1 where each item is described by a score.",
            "So we have a score on the.",
            "On this axis here.",
            "And each item is represented by score distribution on.",
            "Around that score and given given such score distributions, this induces a distribution over rankings 'cause we can sample from the red, green and blue and depending on the ordering on the score axis.",
            "This gives us a ranking.",
            "OK, so so this induces a ranked as a distribution over ranks, and if you look at the probability of a particular item being in a particular position.",
            "You see that here, for example, the red, the scores are tend to be much lower, and so it's much more likely to be in the third position and the second position than in the first position.",
            "And there's a theorem that says it's due to yell at that says if you have."
        ],
        [
            "Yeah, a thurstonii and model where every score has an identical distribution.",
            "Then the score distribution and the distribution is a Gumbel distribution, which is a sort of a.",
            "Negative exponential double X exponential with a mean term and width term.",
            "For its cumulative distribution function then.",
            "Gives rise to plackett Luce model if and only if the scores are distributed according to Gumbel distribution and that follows from some nice properties which say that if you take a Gumbel distribution.",
            "PDF and multiply by its CDF and integrate.",
            "Then you end up with a another Gumbel CDF and so so the sort of complex series of integrals that you get in working out the rank distribution.",
            "Sort of sort of chain nicely together.",
            "OK, so how do we estimate the parameters in packet loose?",
            "Well, there's."
        ],
        [
            "Sort of, the state of the art algorithm is a maximum likelihood estimator, and its was described by Hunter in 2004.",
            "It uses a maximized.",
            "Sorry, a minor eyes maximized algorithm on EM algorithm, which I think is is a terminology he coined.",
            "But it's basically something like an EM, but the step is a bit more general and so you you have a current estimate to the parameters.",
            "You find a minor Ising function that's that's below the.",
            "The the the likelihood function and then you maximize that minor rising function and you continue iterating like that.",
            "And it's it's it's it's it's very efficient.",
            "Maximum likelihood in general will overfit with sparse data, and which is very common in ranking problems.",
            "And it has this.",
            "Assumption for convergence, which basically says you can't have two.",
            "Subsets of the items where in your data.",
            "Every item in the first set beats every item in the second set, so that's a restriction on on doing that.",
            "And it's also with the EM algorithm.",
            "It's very difficult to get out error bars and or you know some.",
            "Measure of how certain you are about your estimates.",
            "So we're going to."
        ],
        [
            "Bayesian inference, so we're going to start off by looking at the factor graph so the the circles here are variables and I'm looking at 5 variables here.",
            "We're going to put gamma priors on.",
            "Remember, these variables are positive in the packet, lose parameterisations, so it makes sense for these to be gamma priors, but there are other, more technical reasons for them to be gamma priors, and those are described in the paper.",
            "Um?",
            "And then let's look at a particular packet lose factor.",
            "So this is the ordering BA E. So remember that the the the ordering BA E the placket lose likelihood.",
            "So riding the likelihoods in here is a product of.",
            "These two terms times VE over VE, which is just one, so we'll just ignore that in all cases.",
            "So here we have a partial ordering and this gives rise to these two factors for these three.",
            "Items.",
            "In general, you have N -- 1 factors for an observation of a partial ranking event items.",
            "And then you might have another observation where you just have a pairwise ranking, and that's also easily dealt with in this framework.",
            "So this is sort of a Bradley Terry factor.",
            "And it's difficult to see the lines here, but these these factors connect up to.",
            "There are variables depending on what variables are in inside the particular subfactor.",
            "OK.",
            "So."
        ],
        [
            "We're going to look at a fully factorized approximation.",
            "Of.",
            "Of this of the posterior.",
            "So the posterior is just the product of all these likelihoods.",
            "Time times the priors I've merged the prize into this notation, so N is the data data index, N = 0 will be the prior.",
            "And this this K thing will be will index will be over the subfactors of the Plackett Luce likelihoods.",
            "Which which will vary from observation to observation.",
            "And we approximate this fully factorized product of gammas so.",
            "So the posterior is approximate.",
            "By this Q the Q you can think of it two different sort of.",
            "Yeah, OK. How long have you been holding that?",
            "So the Q is.",
            "The qis of the marginals, but you can also look at the approximations over the factors, so let's."
        ],
        [
            "Quickly talk about message passing, so expectation propagation is a way of doing inference on a graphical model on a factor graph an.",
            "Basically the messages coming into a variable are.",
            "The product of those messages, or the marginal so the qis in the previous slide and the message is the product of the messages coming out of the factor are.",
            "The the approximate for the factor and there's an update rule which says you.",
            "To calculate a message going out to the factor, you multiply all the incoming messages multiplied by the factor, integrate out overall message.",
            "All variables except the target variable to the KL projection and then divide by the incoming message.",
            "The problem is that that that integral, which is actually Sona summation here is not tractable for Plackett Luce, so we can't do that.",
            "We can do something called Power EP, which."
        ],
        [
            "Uses the concept of Alpha, Divergance and Alpha divergences.",
            "Is a family of divergences.",
            "In parameterized by this Alpha parameter."
        ],
        [
            "And if you look at some of these.",
            "KL is D zero 50 Cal QPKLPQS D1.",
            "We're interested in D -- 1.",
            "OK, and if you look at the proper."
        ],
        [
            "These are these divergences.",
            "You can see the different values of Alpha, so in this graph, what we're doing is we're looking at a true distribution, which is a mixture of Gaussians and we're approximating it with the Gaussian, and depending what the value of Alpha is, it will have different approximating properties, either be 0 seeking or."
        ],
        [
            "Are avoiding and it will either be mode seeking or or mode averaging.",
            "And traditional EP is at one and mean field is at at zero in this in this spectrum, and we're going to look at our algorithm is sort of here.",
            "Um, OK so?",
            "If we look at the message passing just very briefly.",
            "What we have are just shown there."
        ],
        [
            "I turn factor of this factor.",
            "This single likelihood which has 2 two subfactors.",
            "Just look at this term and we want to know that what the message is coming out of these factors.",
            "OK, so the message coming out of these factors in this simple case boils down to an integral like this.",
            "The integral can be simplified.",
            "It turns out to be a sum of gamma zorah weighted sum gammas.",
            "We can then project that onto a gamma and now we have our message passing algorithm which is which is nice and fast and."
        ],
        [
            "So now I'm just going to finish off with a couple of quick examples.",
            "So here we have an example where the parameter we have 10 items and the and the visa for those items.",
            "The known parameters are 1, two up to 10 and we want to infer those and here we have 5 observations.",
            "These are the marginals that come out of the inference algorithm as we add in more observations, we get more and more confident of those of those values until after 5000.",
            "Quipp pretty confident about these values.",
            "The final example I want to look at is a.",
            "On it."
        ],
        [
            "Sample that Hunter looks at and it's about ranking NASCAR drivers into the 2002 season.",
            "There 80 plus drivers and they participate.",
            "Participate in every race and participate in only a few races.",
            "So so and I sort of irregular problem and the more races someone participates in.",
            "Obviously, the more certain we should be about what the true ranking is, and this first column is the number of races they race in.",
            "The second column is their average place.",
            "This is the the rank of the maximum likelihood estimator, and you see that this puts like Scott Jones and sorry PJ Jones and Scott Pruitt in in the first 2 places, but I've only raced one race, whereas for the EP these are.",
            "These are way down in the rankings and we have much more uncertainty about them as you see over in the final column last Mark Martin is running 36 races.",
            "His much more certain about his.",
            "Capabilities as a driver and then at the bottom end we see couple of drivers.",
            "Mark Morgan Shepherd and the unfortunate Dick Trickle are have lost quite a few places gone down from 78 to 83 and 74 down to 80 because they've run in several races and so we're bit more certain about their poor poor driving ability."
        ],
        [
            "OK, so I'm just going to finish there.",
            "Take questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, see see where um.",
                    "label": 0
                },
                {
                    "sent": "Going to be talking about packet Lube ranking models.",
                    "label": 1
                },
                {
                    "sent": "I've run the Littman spell checker here, but there's a serious point behind this, and that's that packet loose models are not very well studied in the machine learning community, so hopefully this talk will start to put that right and I'll I'll describe these models.",
                    "label": 0
                },
                {
                    "sent": "They have very nice properties.",
                    "label": 0
                },
                {
                    "sent": "But the second piece of this is the inference and the inference is.",
                    "label": 0
                },
                {
                    "sent": "That I'm going to be using as a variational technique or power EP, which is also very underutilized mechanism within machine learning community since it came out in 2004 I I only know three papers that have used it and I think Max you have one that way that you've used Power EP.",
                    "label": 0
                },
                {
                    "sent": "Is that right?",
                    "label": 0
                },
                {
                    "sent": "Technical report are rejected.",
                    "label": 0
                },
                {
                    "sent": "OK so there you go.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OSC rankings are important in the IR community.",
                    "label": 0
                },
                {
                    "sent": "There's been learning to rank workshops and many learning to rank sessions in the last few cigar conferences, for example.",
                    "label": 0
                },
                {
                    "sent": "Um, so the ranking problem is is starting to be studied by the machine learning community and in some depth.",
                    "label": 0
                },
                {
                    "sent": "And we're going to be looking at situation where we have data in the form of multiple independent orderings of a set of K items, and these might be full ordering, so they might be partial orderings.",
                    "label": 1
                },
                {
                    "sent": "And we'd like to learn a parameterized model over those rankings.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start off just with a bit of notation.",
                    "label": 0
                },
                {
                    "sent": "This isn't going to be a very mathematical talk, but it's important just to get some notational fundamentals out of the way.",
                    "label": 0
                },
                {
                    "sent": "First thing to notice is that you have a list of items.",
                    "label": 0
                },
                {
                    "sent": "The representation for the items and the rank positions are exactly the same, so we have items numbered one to N. We have positions, sorry.",
                    "label": 0
                },
                {
                    "sent": "One to K, we have positions one to K, and it's very easy to get confused between the two.",
                    "label": 0
                },
                {
                    "sent": "And that the two concepts.",
                    "label": 0
                },
                {
                    "sent": "One is the concept of ranking and ranking is a permutation which Maps item indices to position indices and we represent those by Greek letter rho.",
                    "label": 1
                },
                {
                    "sent": "And so Ro I is a is a rank position.",
                    "label": 0
                },
                {
                    "sent": "In a ranking and ordering.",
                    "label": 0
                },
                {
                    "sent": "Is a permutation which Maps position indices to item indices.",
                    "label": 1
                },
                {
                    "sent": "So here and we represent these by Omega and so Omega K is an item index.",
                    "label": 0
                },
                {
                    "sent": "That who's who's rank position is position K and the distribution is in terms that we are going to describes is described in terms of an ordering rather than the ranking, but the tour equivalent.",
                    "label": 0
                },
                {
                    "sent": "You can go backwards and forwards between the two and try not to get confused.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is the ranking distribution well?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a distribution over the domain of all possible rankings given a set of K items.",
                    "label": 1
                },
                {
                    "sent": "There K factorial possible rankings, so a full fully parameterized distribution would have a probability of each of those K factorial rankings.",
                    "label": 0
                },
                {
                    "sent": "So here you see.",
                    "label": 0
                },
                {
                    "sent": "Three items we have 3 factorial rankings, 1231322 and three etc.",
                    "label": 1
                },
                {
                    "sent": "Those can be rankings or orderings.",
                    "label": 0
                },
                {
                    "sent": "And the parameters are all a probabilities of each of those rankings, so they sum up to one, and they're all positive.",
                    "label": 0
                },
                {
                    "sent": "So a particular ranking distribution is a point in that simplex.",
                    "label": 1
                },
                {
                    "sent": "It's a point in K dimensional space satisfying those properties.",
                    "label": 1
                },
                {
                    "sent": "And then a model is a is a parameterized family within that simplex.",
                    "label": 0
                },
                {
                    "sent": "So the parameterized family we're going to look at is.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's the plackett Luce model.",
                    "label": 0
                },
                {
                    "sent": "And there are various nice interpretations of the plackett Luce model.",
                    "label": 0
                },
                {
                    "sent": "So the first interpretation I want to look at is the Vars interpretation and.",
                    "label": 0
                },
                {
                    "sent": "It starts off by having however many vases you have items.",
                    "label": 0
                },
                {
                    "sent": "So in this case we have three items, red, blue and green.",
                    "label": 0
                },
                {
                    "sent": "And we pretend these these vases vases rather urns because they have an infinite number of balls in.",
                    "label": 0
                },
                {
                    "sent": "But the ratios given by VR.",
                    "label": 0
                },
                {
                    "sent": "VP&VG are the proportion of balls image in each bars.",
                    "label": 0
                },
                {
                    "sent": "OK, we're now going to mix these these balls up into one, so if you watch carefully, there's a bit of animation here.",
                    "label": 0
                },
                {
                    "sent": "There we go.",
                    "label": 0
                },
                {
                    "sent": "So so so now.",
                    "label": 0
                },
                {
                    "sent": "So now we're going to sample from this this infinite virus.",
                    "label": 0
                },
                {
                    "sent": "And, um.",
                    "label": 0
                },
                {
                    "sent": "Will will pick will pick will pick one out and it turns out to be a red.",
                    "label": 0
                },
                {
                    "sent": "OK, and the probability of picking that red is is obviously VR, VR, plus VG plus VB OK, since those are the proportions of balls within this infinite bars.",
                    "label": 0
                },
                {
                    "sent": "And then we pick another ball out and the next ball turns out it's red again.",
                    "label": 0
                },
                {
                    "sent": "Well, what we want actually is a permutation is a permutation of our items, and so we have to just reject this one.",
                    "label": 0
                },
                {
                    "sent": "So we just throw it back in in the bars and we keep on picking out until we get a new color.",
                    "label": 0
                },
                {
                    "sent": "And suppose that's green.",
                    "label": 0
                },
                {
                    "sent": "Well, now we've taken out all the Red Bulls, and so we're just interesting green and blue, and so the probability of that that green is now VG of the VG plus VB.",
                    "label": 0
                },
                {
                    "sent": "And then finally we pick out the blue and of course the blue is probability one.",
                    "label": 0
                },
                {
                    "sent": "We just keep on.",
                    "label": 0
                },
                {
                    "sent": "Putting bowls back until we get a blue one.",
                    "label": 0
                },
                {
                    "sent": "OK, so the probability of the of the ordering red, green, blue is the product of these three factors where the last one is always the trivial factor.",
                    "label": 0
                },
                {
                    "sent": "And so in general, that defines the packet loss.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model so the packet loss model says.",
                    "label": 0
                },
                {
                    "sent": "We have an ordering Omega.",
                    "label": 0
                },
                {
                    "sent": "And we have a parameter at vector V, so V in the Vars interpretation of these proportions of colored balls.",
                    "label": 0
                },
                {
                    "sent": "And it's a product of factors and the factors all look like.",
                    "label": 0
                },
                {
                    "sent": "So the fact is, prior to these factors here, which all look like like this that we saw in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "So the case factor is the V for the parameter for the particular item in the case position of the summer parameters for the item in the Cape position.",
                    "label": 0
                },
                {
                    "sent": "To the item in the bottom position, right?",
                    "label": 0
                },
                {
                    "sent": "So it's a it's a product of these ratios with simple terms in the numerator and these summations on the denominator.",
                    "label": 0
                },
                {
                    "sent": "OK, so now that's for a complete org.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ring how about partial orderings?",
                    "label": 0
                },
                {
                    "sent": "Well, there's two ways we can consider partial orderings.",
                    "label": 0
                },
                {
                    "sent": "One is a sort of a top, a top end, and the other is a true partial ordering.",
                    "label": 0
                },
                {
                    "sent": "So I've added a another color in here to make this a little bit more understanding, understandable, and.",
                    "label": 0
                },
                {
                    "sent": "So for the top end, we're going to say we just.",
                    "label": 0
                },
                {
                    "sent": "Say have had the items in the top two positions and all the other items are below the top two positions.",
                    "label": 0
                },
                {
                    "sent": "Now the probability of that, so we pick out a green here probability, the green SVG over the sum of all the others.",
                    "label": 0
                },
                {
                    "sent": "We pick out another.",
                    "label": 0
                },
                {
                    "sent": "It's green, we reject it and pick out a blue, and that's similar like the packet loose term we saw before.",
                    "label": 0
                },
                {
                    "sent": "But now we just stop OK, everything else is below there, so the likelihood of.",
                    "label": 0
                },
                {
                    "sent": "This top Top 2 being green blue is given by the product of these two terms.",
                    "label": 0
                },
                {
                    "sent": "Now partial ordering is slightly different and partial ordering we're saying.",
                    "label": 0
                },
                {
                    "sent": "The other two balls, the red and the and the and the and the orange, can be absolutely anywhere in the ordering.",
                    "label": 0
                },
                {
                    "sent": "They don't have to be below the two we pick out.",
                    "label": 0
                },
                {
                    "sent": "So we pick out a green.",
                    "label": 0
                },
                {
                    "sent": "We pick out a red, we reject it 'cause we're just interested in blue and green.",
                    "label": 0
                },
                {
                    "sent": "We pick out the blue.",
                    "label": 0
                },
                {
                    "sent": "And now here, with marginalized out red and Orange and we're just left with a placket.",
                    "label": 0
                },
                {
                    "sent": "Luce distribution of over the Green and the blue.",
                    "label": 0
                },
                {
                    "sent": "So it has it's nice property.",
                    "label": 0
                },
                {
                    "sent": "But if you marginalized out certain items, you're still left with the same form of distribution with only the variables that remain in the problem there.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just a point of interest here.",
                    "label": 0
                },
                {
                    "sent": "So when you're just looking at partial orderings that consists of a pair of items, this is the Bradley Terry model.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now the plackett Luce distribution was described initially by placket in 75, but it was sort of pre.",
                    "label": 0
                },
                {
                    "sent": "Those is really based on some ideas of loose talking about choice.",
                    "label": 0
                },
                {
                    "sent": "An axiom of choice.",
                    "label": 0
                },
                {
                    "sent": "For choosing things from a set and that was work in the 50s and John Martin who wrote her defensive book on on rank distributions in 95 coined the phrase plackett Luce.",
                    "label": 0
                },
                {
                    "sent": "So what what Lou said is when you when you choose something.",
                    "label": 0
                },
                {
                    "sent": "So suppose we the probability of choosing wine out of some set is the same as the probability of choosing, say, a subset.",
                    "label": 0
                },
                {
                    "sent": "Say the alcoholic drinks.",
                    "label": 0
                },
                {
                    "sent": "Time is the probability of choosing wine out of that subset, and if you do a series of choices like that in a sort of a multi stage series of choices, then you'll get the plackett Luce distribution.",
                    "label": 1
                },
                {
                    "sent": "In fact, the Plackett Luce is the only multi stage distribution that satisfies that Axiom.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, one other interpretation of packet this I wanted to describe which is very interesting and this is the Gumbel, thurstonii and model.",
                    "label": 0
                },
                {
                    "sent": "I wonder what Felinton spell checker would have done to this, but first only and model is 1 where each item is described by a score.",
                    "label": 0
                },
                {
                    "sent": "So we have a score on the.",
                    "label": 1
                },
                {
                    "sent": "On this axis here.",
                    "label": 0
                },
                {
                    "sent": "And each item is represented by score distribution on.",
                    "label": 1
                },
                {
                    "sent": "Around that score and given given such score distributions, this induces a distribution over rankings 'cause we can sample from the red, green and blue and depending on the ordering on the score axis.",
                    "label": 0
                },
                {
                    "sent": "This gives us a ranking.",
                    "label": 0
                },
                {
                    "sent": "OK, so so this induces a ranked as a distribution over ranks, and if you look at the probability of a particular item being in a particular position.",
                    "label": 0
                },
                {
                    "sent": "You see that here, for example, the red, the scores are tend to be much lower, and so it's much more likely to be in the third position and the second position than in the first position.",
                    "label": 0
                },
                {
                    "sent": "And there's a theorem that says it's due to yell at that says if you have.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, a thurstonii and model where every score has an identical distribution.",
                    "label": 0
                },
                {
                    "sent": "Then the score distribution and the distribution is a Gumbel distribution, which is a sort of a.",
                    "label": 0
                },
                {
                    "sent": "Negative exponential double X exponential with a mean term and width term.",
                    "label": 0
                },
                {
                    "sent": "For its cumulative distribution function then.",
                    "label": 0
                },
                {
                    "sent": "Gives rise to plackett Luce model if and only if the scores are distributed according to Gumbel distribution and that follows from some nice properties which say that if you take a Gumbel distribution.",
                    "label": 1
                },
                {
                    "sent": "PDF and multiply by its CDF and integrate.",
                    "label": 0
                },
                {
                    "sent": "Then you end up with a another Gumbel CDF and so so the sort of complex series of integrals that you get in working out the rank distribution.",
                    "label": 0
                },
                {
                    "sent": "Sort of sort of chain nicely together.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we estimate the parameters in packet loose?",
                    "label": 0
                },
                {
                    "sent": "Well, there's.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sort of, the state of the art algorithm is a maximum likelihood estimator, and its was described by Hunter in 2004.",
                    "label": 0
                },
                {
                    "sent": "It uses a maximized.",
                    "label": 0
                },
                {
                    "sent": "Sorry, a minor eyes maximized algorithm on EM algorithm, which I think is is a terminology he coined.",
                    "label": 0
                },
                {
                    "sent": "But it's basically something like an EM, but the step is a bit more general and so you you have a current estimate to the parameters.",
                    "label": 0
                },
                {
                    "sent": "You find a minor Ising function that's that's below the.",
                    "label": 0
                },
                {
                    "sent": "The the the likelihood function and then you maximize that minor rising function and you continue iterating like that.",
                    "label": 0
                },
                {
                    "sent": "And it's it's it's it's it's very efficient.",
                    "label": 0
                },
                {
                    "sent": "Maximum likelihood in general will overfit with sparse data, and which is very common in ranking problems.",
                    "label": 1
                },
                {
                    "sent": "And it has this.",
                    "label": 0
                },
                {
                    "sent": "Assumption for convergence, which basically says you can't have two.",
                    "label": 0
                },
                {
                    "sent": "Subsets of the items where in your data.",
                    "label": 0
                },
                {
                    "sent": "Every item in the first set beats every item in the second set, so that's a restriction on on doing that.",
                    "label": 1
                },
                {
                    "sent": "And it's also with the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's very difficult to get out error bars and or you know some.",
                    "label": 0
                },
                {
                    "sent": "Measure of how certain you are about your estimates.",
                    "label": 0
                },
                {
                    "sent": "So we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bayesian inference, so we're going to start off by looking at the factor graph so the the circles here are variables and I'm looking at 5 variables here.",
                    "label": 1
                },
                {
                    "sent": "We're going to put gamma priors on.",
                    "label": 0
                },
                {
                    "sent": "Remember, these variables are positive in the packet, lose parameterisations, so it makes sense for these to be gamma priors, but there are other, more technical reasons for them to be gamma priors, and those are described in the paper.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And then let's look at a particular packet lose factor.",
                    "label": 0
                },
                {
                    "sent": "So this is the ordering BA E. So remember that the the the ordering BA E the placket lose likelihood.",
                    "label": 0
                },
                {
                    "sent": "So riding the likelihoods in here is a product of.",
                    "label": 0
                },
                {
                    "sent": "These two terms times VE over VE, which is just one, so we'll just ignore that in all cases.",
                    "label": 0
                },
                {
                    "sent": "So here we have a partial ordering and this gives rise to these two factors for these three.",
                    "label": 0
                },
                {
                    "sent": "Items.",
                    "label": 0
                },
                {
                    "sent": "In general, you have N -- 1 factors for an observation of a partial ranking event items.",
                    "label": 0
                },
                {
                    "sent": "And then you might have another observation where you just have a pairwise ranking, and that's also easily dealt with in this framework.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of a Bradley Terry factor.",
                    "label": 0
                },
                {
                    "sent": "And it's difficult to see the lines here, but these these factors connect up to.",
                    "label": 0
                },
                {
                    "sent": "There are variables depending on what variables are in inside the particular subfactor.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're going to look at a fully factorized approximation.",
                    "label": 0
                },
                {
                    "sent": "Of.",
                    "label": 0
                },
                {
                    "sent": "Of this of the posterior.",
                    "label": 0
                },
                {
                    "sent": "So the posterior is just the product of all these likelihoods.",
                    "label": 0
                },
                {
                    "sent": "Time times the priors I've merged the prize into this notation, so N is the data data index, N = 0 will be the prior.",
                    "label": 0
                },
                {
                    "sent": "And this this K thing will be will index will be over the subfactors of the Plackett Luce likelihoods.",
                    "label": 0
                },
                {
                    "sent": "Which which will vary from observation to observation.",
                    "label": 0
                },
                {
                    "sent": "And we approximate this fully factorized product of gammas so.",
                    "label": 1
                },
                {
                    "sent": "So the posterior is approximate.",
                    "label": 0
                },
                {
                    "sent": "By this Q the Q you can think of it two different sort of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK. How long have you been holding that?",
                    "label": 0
                },
                {
                    "sent": "So the Q is.",
                    "label": 0
                },
                {
                    "sent": "The qis of the marginals, but you can also look at the approximations over the factors, so let's.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quickly talk about message passing, so expectation propagation is a way of doing inference on a graphical model on a factor graph an.",
                    "label": 0
                },
                {
                    "sent": "Basically the messages coming into a variable are.",
                    "label": 0
                },
                {
                    "sent": "The product of those messages, or the marginal so the qis in the previous slide and the message is the product of the messages coming out of the factor are.",
                    "label": 0
                },
                {
                    "sent": "The the approximate for the factor and there's an update rule which says you.",
                    "label": 0
                },
                {
                    "sent": "To calculate a message going out to the factor, you multiply all the incoming messages multiplied by the factor, integrate out overall message.",
                    "label": 0
                },
                {
                    "sent": "All variables except the target variable to the KL projection and then divide by the incoming message.",
                    "label": 0
                },
                {
                    "sent": "The problem is that that that integral, which is actually Sona summation here is not tractable for Plackett Luce, so we can't do that.",
                    "label": 0
                },
                {
                    "sent": "We can do something called Power EP, which.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uses the concept of Alpha, Divergance and Alpha divergences.",
                    "label": 0
                },
                {
                    "sent": "Is a family of divergences.",
                    "label": 0
                },
                {
                    "sent": "In parameterized by this Alpha parameter.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you look at some of these.",
                    "label": 0
                },
                {
                    "sent": "KL is D zero 50 Cal QPKLPQS D1.",
                    "label": 0
                },
                {
                    "sent": "We're interested in D -- 1.",
                    "label": 0
                },
                {
                    "sent": "OK, and if you look at the proper.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are these divergences.",
                    "label": 0
                },
                {
                    "sent": "You can see the different values of Alpha, so in this graph, what we're doing is we're looking at a true distribution, which is a mixture of Gaussians and we're approximating it with the Gaussian, and depending what the value of Alpha is, it will have different approximating properties, either be 0 seeking or.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are avoiding and it will either be mode seeking or or mode averaging.",
                    "label": 0
                },
                {
                    "sent": "And traditional EP is at one and mean field is at at zero in this in this spectrum, and we're going to look at our algorithm is sort of here.",
                    "label": 0
                },
                {
                    "sent": "Um, OK so?",
                    "label": 0
                },
                {
                    "sent": "If we look at the message passing just very briefly.",
                    "label": 0
                },
                {
                    "sent": "What we have are just shown there.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I turn factor of this factor.",
                    "label": 0
                },
                {
                    "sent": "This single likelihood which has 2 two subfactors.",
                    "label": 0
                },
                {
                    "sent": "Just look at this term and we want to know that what the message is coming out of these factors.",
                    "label": 0
                },
                {
                    "sent": "OK, so the message coming out of these factors in this simple case boils down to an integral like this.",
                    "label": 0
                },
                {
                    "sent": "The integral can be simplified.",
                    "label": 0
                },
                {
                    "sent": "It turns out to be a sum of gamma zorah weighted sum gammas.",
                    "label": 0
                },
                {
                    "sent": "We can then project that onto a gamma and now we have our message passing algorithm which is which is nice and fast and.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm just going to finish off with a couple of quick examples.",
                    "label": 0
                },
                {
                    "sent": "So here we have an example where the parameter we have 10 items and the and the visa for those items.",
                    "label": 0
                },
                {
                    "sent": "The known parameters are 1, two up to 10 and we want to infer those and here we have 5 observations.",
                    "label": 0
                },
                {
                    "sent": "These are the marginals that come out of the inference algorithm as we add in more observations, we get more and more confident of those of those values until after 5000.",
                    "label": 0
                },
                {
                    "sent": "Quipp pretty confident about these values.",
                    "label": 0
                },
                {
                    "sent": "The final example I want to look at is a.",
                    "label": 0
                },
                {
                    "sent": "On it.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample that Hunter looks at and it's about ranking NASCAR drivers into the 2002 season.",
                    "label": 0
                },
                {
                    "sent": "There 80 plus drivers and they participate.",
                    "label": 0
                },
                {
                    "sent": "Participate in every race and participate in only a few races.",
                    "label": 0
                },
                {
                    "sent": "So so and I sort of irregular problem and the more races someone participates in.",
                    "label": 0
                },
                {
                    "sent": "Obviously, the more certain we should be about what the true ranking is, and this first column is the number of races they race in.",
                    "label": 0
                },
                {
                    "sent": "The second column is their average place.",
                    "label": 0
                },
                {
                    "sent": "This is the the rank of the maximum likelihood estimator, and you see that this puts like Scott Jones and sorry PJ Jones and Scott Pruitt in in the first 2 places, but I've only raced one race, whereas for the EP these are.",
                    "label": 0
                },
                {
                    "sent": "These are way down in the rankings and we have much more uncertainty about them as you see over in the final column last Mark Martin is running 36 races.",
                    "label": 0
                },
                {
                    "sent": "His much more certain about his.",
                    "label": 0
                },
                {
                    "sent": "Capabilities as a driver and then at the bottom end we see couple of drivers.",
                    "label": 0
                },
                {
                    "sent": "Mark Morgan Shepherd and the unfortunate Dick Trickle are have lost quite a few places gone down from 78 to 83 and 74 down to 80 because they've run in several races and so we're bit more certain about their poor poor driving ability.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm just going to finish there.",
                    "label": 0
                },
                {
                    "sent": "Take questions.",
                    "label": 0
                }
            ]
        }
    }
}