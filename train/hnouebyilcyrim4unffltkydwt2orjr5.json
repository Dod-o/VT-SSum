{
    "id": "hnouebyilcyrim4unffltkydwt2orjr5",
    "title": "TransEdge: Translating Relation-contextualized Embeddings for Knowledge Graphs",
    "info": {
        "author": [
            "Jiacheng Huang, The University of Texas at Dallas"
        ],
        "published": "Dec. 10, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2019_huang_transedge/",
    "segmentation": [
        [
            "Hello everyone, today I will introduce our research paper Transat translating relation contextualized embeddings for knowledge graphs.",
            "First, I will introduce the background and the motivation for work and knowledge.",
            "Graph is multi religious group."
        ],
        [
            "Note the note entities and direct ages are associated with types indicate indicating the specific relation between entities for each entity change in the Knowledge Graph.",
            "Note relation.",
            "No triple which represent effect.",
            "For example, this age in the Knowledge Graph represents the fact that the movie from dusk till down is has a director whose name is Robot."
        ],
        [
            "Jordan Rd Rangers Formally knowledge Graph can be can be defined as a triple in this form, where E is a set of entities and R is the set of relations and T is the set of the triples."
        ],
        [
            "Now we will discuss the Knowledge graph embeddings knowledge graph embedding techniques 6 to encode entities and their relationships into vector space, and capture the seed semantics by the geometry structure of the embeddings.",
            "In this paper, we focus on the translation cage embedding models.",
            "The most popular packaging building models is the transit in the transition for a relationship triple HRT.",
            "We use it used vectors of H&R are connected by.",
            "Transform transformation vector of that is to save the H plus our IT support pproximately equals two T the transit use the energy function in this form to to denote the errors in the model.",
            "It also uses energy function to train the embeddings for the entities.",
            "Here in this figure is an example for the embedding of the troubles.",
            "Here there's two points are the vectors for these two entities, and here is this this.",
            "This vector is the translation vector.",
            "It is the embedding for the relation located in.",
            "Another graph, embeddings have many applications such as link prediction, an entertainment VR discuss this text later."
        ],
        [
            "This translational models use transformation vectors as relationships.",
            "This models cannot model the complex relation structure of cages.",
            "Here we consider the example in the smoke knowledge graph.",
            "When Van Zandt repair has many multiple relations, for example, this entity pair has two relations, director and the editing.",
            "Here the trustee value similar as releasing values for these two relations, and when the multiple entity pairs has the only violation, then the embedding for these two entities are very similar.",
            "If we use this similar embeddings to do inference, it maybe.",
            "It may cause errors.",
            "For example, if we want to predict the writer of this movie, it may use it may output charge my ring as the output.",
            "However it is not true 'cause we can see that charge measuring is starring in this movie."
        ],
        [
            "To address this problem, exotics existing translational models are used to transform the original embedding for the entity to a sub linear subspace.",
            "Here we take transition as an example.",
            "It is used the projection transformation as the G1R and GG2R.",
            "Here it project HH&T is the original embedding for the entities IT projects H and T2.",
            "A hyperplane of our here the translation vector is translation vector from HR two TR is the embedding folder relation.",
            "Here we can see it is possible to model.",
            "Multiple entities with the same same relation.",
            "However, this this terror entities are lies in this line.",
            "So this entities are much similar to each other, so this model this model is also cannot address this problem roughly."
        ],
        [
            "Here we will see our approach trans edge.",
            "So this figure is the graph structure of the keagy.",
            "Here we can see that.",
            "For entities it is should like, it can be has explicit explicit embeddings.",
            "That is to say, for each entities it has a fixed point in the vector space.",
            "Africa's, now we are considered the ages in the knowledge graphs.",
            "In this in the Trans Age we introduce the H embeddings, that is to say the translation vector from the entity pair is the embedding for the age rather than the relationship.",
            "Based on this formulation we can see that these two these two age embedding are seeing.",
            "However they are, they can encode the different information from.",
            "Different relationships and these two these two relation.",
            "These two H embeddings are different.",
            "However they use the same relation labels."
        ],
        [
            "Now we will give the details of our model trans Edge."
        ],
        [
            "Here is this figure gives generalized information of our model.",
            "In this figure, there's two blue box.",
            "Are generalized general entity embedding forehead and terror entities and this this yellow box is aging bedding for this.",
            "This this triple.",
            "Here we use use the function file to combine the head entity and tell entity and the relation by adding together."
        ],
        [
            "Formally, the energy function of our model can written as this.",
            "We call the function, Phi is the contextualization operation.",
            "It is generated H embeddings.",
            "Here with for each entities we have two embeddings.",
            "First is H&T.",
            "It is the general embedding for entities.",
            "It is capture the geometry positions and the relation semantics and HCI interaction values for NTH&T it is used to generate the age embeddings.",
            "Here is our parametric complexity of our model.",
            "In this paper, we designed two contextualization operators.",
            "It is the first one we call it contact context compression.",
            "It is used multiple layer of perceptions to come to compute the embeddings of the H directions and the labels.",
            "Since it's used three vectors as input and output only.",
            "Only one is embedding, so we call it the compression.",
            "Refers combines the head, entity and relation together and then combine the relation and parented together and sum up these two relationship and use MLP to obtain the age embeddings.",
            "Here we use different MLP for these two part because we when we interchange the head and tear into entities the age factor should be different.",
            "The second approach is similar to Trans H. However we we project the relation to the context created by the entities.",
            "For example, in this figure, each and T1 is to enter into repair and it creates the contextual like this hyper plan and then we use the projection function to pro.",
            "Project R2 The bubble prime.",
            "We use MLP too.",
            "I'm happy to obtain the projection vectors and we also required the projection vectors.",
            "Is the unit vector so we can use this simple formula to obtain obtain the.",
            "To obtain the vector of the edge.",
            "This is the last function of our model.",
            "We use the conventional marginal ranking loss.",
            "It is similar to transit transit.",
            "Here we T is positive triple set and T minus is negative positive negative triple set.",
            "We use negative sampling to obtain this set.",
            "And we minimize this loss function.",
            "So for for positive triples we hope it is.",
            "It's the it's score.",
            "Function is smaller than gamma one and four negative triples is score energy function should be larger than gamma 2.",
            "VC points, diploid.",
            "Our approach to two different tax first isn't alignment.",
            "Here is a real world object.",
            "However, it has different UI in different knowledge base.",
            "So the entity diamonds is to find the entities in the different cages, which is referred to the same reward identity.",
            "22 Fulfill this to tackle this problem.",
            "We minimize this loss function.",
            "Here L is the previous triple loss function and this is too.",
            "This is for the entity alignment here in each step of training we first select setup entity pair which.",
            "OK, OK is the entities which are likely to be alignment here.",
            "We hope this entity pairs the store.",
            "I hope this entity pairs are similar.",
            "So is this distance should be minimized.",
            "The another task is link prediction.",
            "For example, here is we don't know the head entity of this triple.",
            "That is to save the link pretty model should can predict the entity should be the valid.",
            "And here we also minimize the loss function of the triples.",
            "It is similar to transit.",
            "Then we conduct experiments for four for our model.",
            "Firstly, experiments is for entertainment.",
            "We use conventional DP and DWI data set and computers come come consists of existing embedding based entertainment models and we also combine existing embedded models with our a loss functions.",
            "We also use the conventional metrics to evaluate performance.",
            "As we can see from this, this, this table translates always translates.",
            "CP always obtains the best performance.",
            "In addition, we find that if we use the same civilization, it always improve the performance.",
            "For example, is here.",
            "On the divide data set, we found that trust edge, CC and CP sometimes obtain the best performance.",
            "Sometimes translate CC is better than CP.",
            "However, this difference is very small.",
            "Here is only about 1.2%.",
            "Then we conducted experiments on link prediction.",
            "We use the conventional datasets.",
            "These two datasets and we compare with the most existing translate existing knowledge graph embedding models.",
            "We also use these metrics to evaluate the performance.",
            "As we can see Trans Age, CP can achieve the best hits advanced in about this.",
            "All these models and in another matrix rotate.",
            "It may be much better.",
            "We also further analyzes the embedding whether we can weather translate, can model complex structures.",
            "First we consider one entity pair with multiple relations with synthesis, keagy with double relationships of relations.",
            "That is to say, for triple HRT in the original graph we create dummy triples each are prime T where are prime is Notre relation in the original.",
            "Knowledge graph, so the thought the new knowledge graph have the same semantic knowledge is to the original autograph.",
            "However, the structure is much complex as we can see in this table, the performance of Amtrak see is significantly drops, especially in here, and it's very apparent.",
            "However, the translate shows less variation than Entrancy is sure that our model can tackle very complex.",
            "Relation structures.",
            "Here is a visualization of of our model Annam transit.",
            "Here we can see that.",
            "Invalidating translational vectors in the entrancy are also parallel.",
            "So however, our model produced flexible and robust embeddings.",
            "We also can't compared with conventional internment, we combine a log map also used attributes, so it's produced more better score, and the combination always get are the best performance.",
            "In this paper we produce we propose a novel translation model with translating the embeddings rather than the relational embeddings.",
            "And it is achieves a state of art performance on both enter alignment and link prediction.",
            "Thank you for time out.",
            "We also raise our code on the GitHub repository.",
            "You can check out it.",
            "Direct.",
            "Regarding your task of July, my heart ride with.",
            "Also wash. How do you tried your translate funding agencies and see what?",
            "Comparing with this.",
            "The ontology matching community has a lot of standard data set, so it's interesting resolution.",
            "Ultimately, I we didn't compare with this this existing ontology matching matching cause.",
            "For example, in the in the.",
            "Oh yeah, datasets there are.",
            "There are very little relationships in the data set.",
            "It is have lots of lots of attributes in the Knowledge graph, so so it is better to use the conventional instrument approaches.",
            "Is there another question?",
            "Thank you, thank you very much for your talk."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everyone, today I will introduce our research paper Transat translating relation contextualized embeddings for knowledge graphs.",
                    "label": 1
                },
                {
                    "sent": "First, I will introduce the background and the motivation for work and knowledge.",
                    "label": 0
                },
                {
                    "sent": "Graph is multi religious group.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Note the note entities and direct ages are associated with types indicate indicating the specific relation between entities for each entity change in the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "Note relation.",
                    "label": 0
                },
                {
                    "sent": "No triple which represent effect.",
                    "label": 0
                },
                {
                    "sent": "For example, this age in the Knowledge Graph represents the fact that the movie from dusk till down is has a director whose name is Robot.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jordan Rd Rangers Formally knowledge Graph can be can be defined as a triple in this form, where E is a set of entities and R is the set of relations and T is the set of the triples.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we will discuss the Knowledge graph embeddings knowledge graph embedding techniques 6 to encode entities and their relationships into vector space, and capture the seed semantics by the geometry structure of the embeddings.",
                    "label": 1
                },
                {
                    "sent": "In this paper, we focus on the translation cage embedding models.",
                    "label": 0
                },
                {
                    "sent": "The most popular packaging building models is the transit in the transition for a relationship triple HRT.",
                    "label": 1
                },
                {
                    "sent": "We use it used vectors of H&R are connected by.",
                    "label": 0
                },
                {
                    "sent": "Transform transformation vector of that is to save the H plus our IT support pproximately equals two T the transit use the energy function in this form to to denote the errors in the model.",
                    "label": 0
                },
                {
                    "sent": "It also uses energy function to train the embeddings for the entities.",
                    "label": 1
                },
                {
                    "sent": "Here in this figure is an example for the embedding of the troubles.",
                    "label": 1
                },
                {
                    "sent": "Here there's two points are the vectors for these two entities, and here is this this.",
                    "label": 0
                },
                {
                    "sent": "This vector is the translation vector.",
                    "label": 0
                },
                {
                    "sent": "It is the embedding for the relation located in.",
                    "label": 0
                },
                {
                    "sent": "Another graph, embeddings have many applications such as link prediction, an entertainment VR discuss this text later.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This translational models use transformation vectors as relationships.",
                    "label": 0
                },
                {
                    "sent": "This models cannot model the complex relation structure of cages.",
                    "label": 1
                },
                {
                    "sent": "Here we consider the example in the smoke knowledge graph.",
                    "label": 1
                },
                {
                    "sent": "When Van Zandt repair has many multiple relations, for example, this entity pair has two relations, director and the editing.",
                    "label": 0
                },
                {
                    "sent": "Here the trustee value similar as releasing values for these two relations, and when the multiple entity pairs has the only violation, then the embedding for these two entities are very similar.",
                    "label": 0
                },
                {
                    "sent": "If we use this similar embeddings to do inference, it maybe.",
                    "label": 0
                },
                {
                    "sent": "It may cause errors.",
                    "label": 0
                },
                {
                    "sent": "For example, if we want to predict the writer of this movie, it may use it may output charge my ring as the output.",
                    "label": 0
                },
                {
                    "sent": "However it is not true 'cause we can see that charge measuring is starring in this movie.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To address this problem, exotics existing translational models are used to transform the original embedding for the entity to a sub linear subspace.",
                    "label": 0
                },
                {
                    "sent": "Here we take transition as an example.",
                    "label": 0
                },
                {
                    "sent": "It is used the projection transformation as the G1R and GG2R.",
                    "label": 0
                },
                {
                    "sent": "Here it project HH&T is the original embedding for the entities IT projects H and T2.",
                    "label": 0
                },
                {
                    "sent": "A hyperplane of our here the translation vector is translation vector from HR two TR is the embedding folder relation.",
                    "label": 0
                },
                {
                    "sent": "Here we can see it is possible to model.",
                    "label": 0
                },
                {
                    "sent": "Multiple entities with the same same relation.",
                    "label": 0
                },
                {
                    "sent": "However, this this terror entities are lies in this line.",
                    "label": 0
                },
                {
                    "sent": "So this entities are much similar to each other, so this model this model is also cannot address this problem roughly.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we will see our approach trans edge.",
                    "label": 1
                },
                {
                    "sent": "So this figure is the graph structure of the keagy.",
                    "label": 1
                },
                {
                    "sent": "Here we can see that.",
                    "label": 1
                },
                {
                    "sent": "For entities it is should like, it can be has explicit explicit embeddings.",
                    "label": 0
                },
                {
                    "sent": "That is to say, for each entities it has a fixed point in the vector space.",
                    "label": 0
                },
                {
                    "sent": "Africa's, now we are considered the ages in the knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "In this in the Trans Age we introduce the H embeddings, that is to say the translation vector from the entity pair is the embedding for the age rather than the relationship.",
                    "label": 0
                },
                {
                    "sent": "Based on this formulation we can see that these two these two age embedding are seeing.",
                    "label": 0
                },
                {
                    "sent": "However they are, they can encode the different information from.",
                    "label": 0
                },
                {
                    "sent": "Different relationships and these two these two relation.",
                    "label": 0
                },
                {
                    "sent": "These two H embeddings are different.",
                    "label": 0
                },
                {
                    "sent": "However they use the same relation labels.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we will give the details of our model trans Edge.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is this figure gives generalized information of our model.",
                    "label": 0
                },
                {
                    "sent": "In this figure, there's two blue box.",
                    "label": 0
                },
                {
                    "sent": "Are generalized general entity embedding forehead and terror entities and this this yellow box is aging bedding for this.",
                    "label": 0
                },
                {
                    "sent": "This this triple.",
                    "label": 0
                },
                {
                    "sent": "Here we use use the function file to combine the head entity and tell entity and the relation by adding together.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Formally, the energy function of our model can written as this.",
                    "label": 0
                },
                {
                    "sent": "We call the function, Phi is the contextualization operation.",
                    "label": 1
                },
                {
                    "sent": "It is generated H embeddings.",
                    "label": 0
                },
                {
                    "sent": "Here with for each entities we have two embeddings.",
                    "label": 0
                },
                {
                    "sent": "First is H&T.",
                    "label": 0
                },
                {
                    "sent": "It is the general embedding for entities.",
                    "label": 1
                },
                {
                    "sent": "It is capture the geometry positions and the relation semantics and HCI interaction values for NTH&T it is used to generate the age embeddings.",
                    "label": 1
                },
                {
                    "sent": "Here is our parametric complexity of our model.",
                    "label": 0
                },
                {
                    "sent": "In this paper, we designed two contextualization operators.",
                    "label": 0
                },
                {
                    "sent": "It is the first one we call it contact context compression.",
                    "label": 0
                },
                {
                    "sent": "It is used multiple layer of perceptions to come to compute the embeddings of the H directions and the labels.",
                    "label": 0
                },
                {
                    "sent": "Since it's used three vectors as input and output only.",
                    "label": 0
                },
                {
                    "sent": "Only one is embedding, so we call it the compression.",
                    "label": 0
                },
                {
                    "sent": "Refers combines the head, entity and relation together and then combine the relation and parented together and sum up these two relationship and use MLP to obtain the age embeddings.",
                    "label": 0
                },
                {
                    "sent": "Here we use different MLP for these two part because we when we interchange the head and tear into entities the age factor should be different.",
                    "label": 0
                },
                {
                    "sent": "The second approach is similar to Trans H. However we we project the relation to the context created by the entities.",
                    "label": 0
                },
                {
                    "sent": "For example, in this figure, each and T1 is to enter into repair and it creates the contextual like this hyper plan and then we use the projection function to pro.",
                    "label": 0
                },
                {
                    "sent": "Project R2 The bubble prime.",
                    "label": 0
                },
                {
                    "sent": "We use MLP too.",
                    "label": 0
                },
                {
                    "sent": "I'm happy to obtain the projection vectors and we also required the projection vectors.",
                    "label": 0
                },
                {
                    "sent": "Is the unit vector so we can use this simple formula to obtain obtain the.",
                    "label": 0
                },
                {
                    "sent": "To obtain the vector of the edge.",
                    "label": 0
                },
                {
                    "sent": "This is the last function of our model.",
                    "label": 0
                },
                {
                    "sent": "We use the conventional marginal ranking loss.",
                    "label": 0
                },
                {
                    "sent": "It is similar to transit transit.",
                    "label": 0
                },
                {
                    "sent": "Here we T is positive triple set and T minus is negative positive negative triple set.",
                    "label": 0
                },
                {
                    "sent": "We use negative sampling to obtain this set.",
                    "label": 0
                },
                {
                    "sent": "And we minimize this loss function.",
                    "label": 0
                },
                {
                    "sent": "So for for positive triples we hope it is.",
                    "label": 0
                },
                {
                    "sent": "It's the it's score.",
                    "label": 0
                },
                {
                    "sent": "Function is smaller than gamma one and four negative triples is score energy function should be larger than gamma 2.",
                    "label": 0
                },
                {
                    "sent": "VC points, diploid.",
                    "label": 0
                },
                {
                    "sent": "Our approach to two different tax first isn't alignment.",
                    "label": 0
                },
                {
                    "sent": "Here is a real world object.",
                    "label": 0
                },
                {
                    "sent": "However, it has different UI in different knowledge base.",
                    "label": 0
                },
                {
                    "sent": "So the entity diamonds is to find the entities in the different cages, which is referred to the same reward identity.",
                    "label": 0
                },
                {
                    "sent": "22 Fulfill this to tackle this problem.",
                    "label": 0
                },
                {
                    "sent": "We minimize this loss function.",
                    "label": 0
                },
                {
                    "sent": "Here L is the previous triple loss function and this is too.",
                    "label": 0
                },
                {
                    "sent": "This is for the entity alignment here in each step of training we first select setup entity pair which.",
                    "label": 0
                },
                {
                    "sent": "OK, OK is the entities which are likely to be alignment here.",
                    "label": 0
                },
                {
                    "sent": "We hope this entity pairs the store.",
                    "label": 0
                },
                {
                    "sent": "I hope this entity pairs are similar.",
                    "label": 0
                },
                {
                    "sent": "So is this distance should be minimized.",
                    "label": 0
                },
                {
                    "sent": "The another task is link prediction.",
                    "label": 0
                },
                {
                    "sent": "For example, here is we don't know the head entity of this triple.",
                    "label": 0
                },
                {
                    "sent": "That is to save the link pretty model should can predict the entity should be the valid.",
                    "label": 0
                },
                {
                    "sent": "And here we also minimize the loss function of the triples.",
                    "label": 0
                },
                {
                    "sent": "It is similar to transit.",
                    "label": 0
                },
                {
                    "sent": "Then we conduct experiments for four for our model.",
                    "label": 0
                },
                {
                    "sent": "Firstly, experiments is for entertainment.",
                    "label": 0
                },
                {
                    "sent": "We use conventional DP and DWI data set and computers come come consists of existing embedding based entertainment models and we also combine existing embedded models with our a loss functions.",
                    "label": 0
                },
                {
                    "sent": "We also use the conventional metrics to evaluate performance.",
                    "label": 0
                },
                {
                    "sent": "As we can see from this, this, this table translates always translates.",
                    "label": 0
                },
                {
                    "sent": "CP always obtains the best performance.",
                    "label": 0
                },
                {
                    "sent": "In addition, we find that if we use the same civilization, it always improve the performance.",
                    "label": 0
                },
                {
                    "sent": "For example, is here.",
                    "label": 0
                },
                {
                    "sent": "On the divide data set, we found that trust edge, CC and CP sometimes obtain the best performance.",
                    "label": 0
                },
                {
                    "sent": "Sometimes translate CC is better than CP.",
                    "label": 0
                },
                {
                    "sent": "However, this difference is very small.",
                    "label": 0
                },
                {
                    "sent": "Here is only about 1.2%.",
                    "label": 0
                },
                {
                    "sent": "Then we conducted experiments on link prediction.",
                    "label": 0
                },
                {
                    "sent": "We use the conventional datasets.",
                    "label": 0
                },
                {
                    "sent": "These two datasets and we compare with the most existing translate existing knowledge graph embedding models.",
                    "label": 0
                },
                {
                    "sent": "We also use these metrics to evaluate the performance.",
                    "label": 0
                },
                {
                    "sent": "As we can see Trans Age, CP can achieve the best hits advanced in about this.",
                    "label": 0
                },
                {
                    "sent": "All these models and in another matrix rotate.",
                    "label": 0
                },
                {
                    "sent": "It may be much better.",
                    "label": 0
                },
                {
                    "sent": "We also further analyzes the embedding whether we can weather translate, can model complex structures.",
                    "label": 0
                },
                {
                    "sent": "First we consider one entity pair with multiple relations with synthesis, keagy with double relationships of relations.",
                    "label": 0
                },
                {
                    "sent": "That is to say, for triple HRT in the original graph we create dummy triples each are prime T where are prime is Notre relation in the original.",
                    "label": 0
                },
                {
                    "sent": "Knowledge graph, so the thought the new knowledge graph have the same semantic knowledge is to the original autograph.",
                    "label": 0
                },
                {
                    "sent": "However, the structure is much complex as we can see in this table, the performance of Amtrak see is significantly drops, especially in here, and it's very apparent.",
                    "label": 0
                },
                {
                    "sent": "However, the translate shows less variation than Entrancy is sure that our model can tackle very complex.",
                    "label": 0
                },
                {
                    "sent": "Relation structures.",
                    "label": 0
                },
                {
                    "sent": "Here is a visualization of of our model Annam transit.",
                    "label": 0
                },
                {
                    "sent": "Here we can see that.",
                    "label": 0
                },
                {
                    "sent": "Invalidating translational vectors in the entrancy are also parallel.",
                    "label": 0
                },
                {
                    "sent": "So however, our model produced flexible and robust embeddings.",
                    "label": 0
                },
                {
                    "sent": "We also can't compared with conventional internment, we combine a log map also used attributes, so it's produced more better score, and the combination always get are the best performance.",
                    "label": 0
                },
                {
                    "sent": "In this paper we produce we propose a novel translation model with translating the embeddings rather than the relational embeddings.",
                    "label": 0
                },
                {
                    "sent": "And it is achieves a state of art performance on both enter alignment and link prediction.",
                    "label": 0
                },
                {
                    "sent": "Thank you for time out.",
                    "label": 0
                },
                {
                    "sent": "We also raise our code on the GitHub repository.",
                    "label": 0
                },
                {
                    "sent": "You can check out it.",
                    "label": 0
                },
                {
                    "sent": "Direct.",
                    "label": 0
                },
                {
                    "sent": "Regarding your task of July, my heart ride with.",
                    "label": 0
                },
                {
                    "sent": "Also wash. How do you tried your translate funding agencies and see what?",
                    "label": 0
                },
                {
                    "sent": "Comparing with this.",
                    "label": 0
                },
                {
                    "sent": "The ontology matching community has a lot of standard data set, so it's interesting resolution.",
                    "label": 1
                },
                {
                    "sent": "Ultimately, I we didn't compare with this this existing ontology matching matching cause.",
                    "label": 0
                },
                {
                    "sent": "For example, in the in the.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, datasets there are.",
                    "label": 0
                },
                {
                    "sent": "There are very little relationships in the data set.",
                    "label": 0
                },
                {
                    "sent": "It is have lots of lots of attributes in the Knowledge graph, so so it is better to use the conventional instrument approaches.",
                    "label": 0
                },
                {
                    "sent": "Is there another question?",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you very much for your talk.",
                    "label": 0
                }
            ]
        }
    }
}