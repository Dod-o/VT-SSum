{
    "id": "diqaazax3q5qyicqfvnz3yrlpsk5tvns",
    "title": "Dependent Hierarchical Beta Process for Image Interpolation and Denoising",
    "info": {
        "author": [
            "Mingyuan Zhou, Department of Electrical and Computer Engineering, Duke University"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Mathematics->Statistics"
        ]
    },
    "url": "http://videolectures.net/aistats2011_zhou_dependent/",
    "segmentation": [
        [
            "So good morning everyone.",
            "So we have just had two excellent talk about the mixture membership model with the digital process.",
            "So here I'm going to talk about another important model that's the factor model and we use the beta process as the prior."
        ],
        [
            "To encourage sparsity, so you may be interested in this model if you are interested in the processing application in general, or you may be interested in this model, and if you don't believe that a simple sparsity can solve all the problems, and if you may also be interested in this model, and if you handling data matrix which may have missing entries which may be noisy or make corrupted by data outliers.",
            "So this is the outline of the talk.",
            "And."
        ],
        [
            "And so there has been significant recent interest in diction, learning and discuss coding, and this problem can generally formalized and this object function.",
            "So give us a data matrix X.",
            "We're trying to represent, and the dictionary with the constraint that representation should be sparse.",
            "And we have shown that we can easily add.",
            "This can be naturally formalized, and the sparse fact and this model framework with the Indian buffet process or the beta process used at the sparse promoting prior and the fact loading of the factor model with corresponding to the diction Adam and we can also say they have the feature or the dish.",
            "So I will use them, exchangeable that the same."
        ],
        [
            "In this talk.",
            "And So what?",
            "You briefly introduce the underlying stochastic process.",
            "So the beta process and balloon process is used as a sparse coding prior in our factor model and draw from beta process can be considered a infinite collection of atoms and each Atom is giving a weight indicating how probable for this Atom to be selected given sample.",
            "So the measure be drawn.",
            "Beta process can be.",
            "Linked to a feature usage XI with the ballooning process.",
            "So the blue so they draw from a blued process with minimal B can be consumed.",
            "The infinite connection of a binary.",
            "Talk coin flip outcomes so we can see here we have the XI equal to some inflammation of the arcade at DK and DK is the case.",
            "Adam and DK is a binary indicating whether the ice sample choose the case feature and if we look at the posterior of the measure B giving the observations.",
            "As shown in this equation is still drop available sets due to the controversy of the beta process and the balloon process and if it has been shown that if we marginalizing out to the measured maybe we can get a predictive predictive duration for new sample which is drawn from.",
            "This is considered the two permit generalization of the Indian buffet process.",
            "So it will examine the form of the posterior distribution with Measure B and the predictive predictive distribution will do sample.",
            "We can see that the parameter is only related to the submission with feature usage, which means it doesn't care about the ordering of the of the incoming samples.",
            "I can randomly permutate the ordering of the incoming sample and this posterior and predictive distribution keeps the same, and in other words, if using a latent feature model, that means that every sample is assumed to have the same probability to select the given dish.",
            "Let's say we go to the go to the breakfast buffet.",
            "And they assume everybody every one of us had the same probability.",
            "Choose the cake or choose the orange juice, or choose the milk which is obviously not true."
        ],
        [
            "So we this is also not true for the image.",
            "Like we we give example.",
            "Here we did the experiment that we have a Barber image which is 256 by 256 here and there were randomly took 80% of the pixel, which means we only observe 20% pixel and we're going to.",
            "Interpolate this missing pixels based on the factor model with the beta process beta balloon process and the sparse modding prior.",
            "And then we when we first together with this without, I was self impressed by the reconstruction.",
            "We can see the reconstruction is is actually very good considering only observe 20% of the pixels.",
            "But you can still see where the obvious artifacts that the texture in the clouds would appear in a mouse I need I it's very undesired artifacts and if you look at actually the length dictionary at.",
            "Dictionary Atom the makes sense, but they do not match the original texture really well, so this is a motion.",
            "This one of the major motivation for us to consider the covalent independence in the factor model.",
            "So the current independence here, which means the patches which are specially nearby.",
            "We should encourage them to tend to choose similar features.",
            "Therefore, which we should remove the exchange building Sumption to say that the Patch they are they don't relate to each other.",
            "It doesn't matter about the ordering, so the natural image ordering is in fact really important.",
            "And they have been lost for previous work on, but introducing the Commander did."
        ],
        [
            "This into the basing models like the last week about the dependence directly process, and there's also some recent work about removing the exchangeable exemption in the Indian buffet process.",
            "For example, the formal genetic ICP they're trying to build a tree structure to take into consideration of the codependence, and also this recent work.",
            "But the dependent Indian buffet process.",
            "So using the hierarchical Gaussian process to take into consideration the codependence as we know.",
            "A a problem with the Gaussian processes.",
            "You have too many samples.",
            "It may be very difficult to build a huge covariance matrix, for example, in our application we may for 256 by 256 image we may have 60,000 more than 60,000 patches.",
            "If you want to build a 60,000 patches covariance matrix corresponding to 60,000 patches, that would be huge to be practical.",
            "To draw from that.",
            "Therefore the computational complexity is not practical.",
            "And in this work we are trying to discover dependence and we closely following the way of the introducing the best density regression which trying to build the current dependence through kernel construction."
        ],
        [
            "So before I go through the detail of the model, I want to give some additional review of the beta process, which is the key element of your model.",
            "So a beta process is a positive living process whose level measure leaves on their productive space, which can be expressed as a product of a degenerate beta distribution and a base measure.",
            "So if the base measure buehner is continuous, then we get a measure B which corresponding to an infinite song.",
            "Over atoms will each Atom is given await an if and in this case the wait is drawing a degenerate beta distribution and if the Bina is discrete then we have the same formula but the PK the wait is now draw for a beta distribution parameterized by C&QK.",
            "So we can see here that B is discrete almost surely.",
            "Therefore we can use it as a base measure for another level of beta process draw, and in this case we can.",
            "We can we can make sure that and different draws from the same base measure B would share the same set of atoms, but."
        ],
        [
            "With different weight, and that's very important.",
            "And so then we now we're going to show how we going to impose the codependency for the HPP construction.",
            "So, first of all, we want to introduce the random walk matrix, which summarizes the relationship between samples in the colon space and like each row of this matrix would sound to one and.",
            "And they each element Aij would would reflect the distance in the coordinate space between sample I&J.",
            "So Li here next shown here.",
            "Is the covalent associate sample I and KLILJ is the kernel distance between the covariance ion correct?",
            "J&J is just normalized kernel distance.",
            "So if 2 sample nearby in the colon space AIG would be large and this has been widely used in like spectral clustering in manifold learning and diffusion analysis.",
            "So we're going to show you how to put into the HPP framework to encourage the codependence so.",
            "Remember so B is draw from BPC nabina, so it's discrete almost surely and we use this based measure for another level.",
            "Beta process draw and so we end with BJ start draw from BP.",
            "Consider BJ and BJ prime.",
            "The draw from the same BP.",
            "Therefore the share same set of atoms, but they're going to have different local probability based on their own posterior and when come to a given sample it would be corresponding to a convex compilation.",
            "Of this underlying BP job and the date is linked to the this be I threw a balloon process and if you do some math and we can show that the correlation between the feature usage at the ice location of the eye sample and the sample I prime would corresponding to the constant distance or the normalized correlation between AI on a prime AI prime recalls that AI is the ISO of the random walk matrix therefore.",
            "If two samples are nearby in the coordinate space, AI an AI prime would be highly correlated.",
            "Therefore, we can show the model is encourage ING if someone nearby in the covalent space they tend to share similar features, but if 2 sample far away from each other, then the correlation is low.",
            "But remember they still share the same set of atoms, therefore it still have possibility to be representative with similar features based on the post."
        ],
        [
            "Garrett so we can.",
            "We're going to show that we can use the DHCP.",
            "Is a sparse prior to represent to replace the simpler beta process as a sparse promoting prior for the factor model?",
            "So the first the first line equation is just the the user saying that give me a sample I it should be sparsely represented under dictionary, subjected to the approximate areas and the residue areas drop for normal and the second one is that the case diction element.",
            "Is Jaafar based measure which is normal here and the real weight is draw for normal and when we using BP we have the ZIK which indicating whether I sample should choose the case feature is drawn from ballooning paikea as we can see the paikea is not linked to the index of the sample.",
            "Therefore all the samples assumed to have the same probability to choose the case feature which is obviously not true for lots of philosophy examples.",
            "And so we're going to replace.",
            "BP is this DSP?",
            "To introduce the codependence, therefore we can see now the iks drawn ballooning paikea, so everyone would have its own feature usage probability, and this one would be a convex combination of latent feature usage at reference point and this reference points draw from beta.",
            "See Juanita K and this heater K, which indicating the global probability is also self driven beta.",
            "So this part corresponding to HB Construction and this part of the introduced the Codependence and we construct.",
            "AJ with a normalized kernel and this kernel we construct in the way they say video basis function with neighborhood constraint.",
            "This is similar to the way to build their similarity matrix in the ISO map.",
            "And so we're using factor model, so we're trying to build a dictionary or trying to infer the factor loadings factor score for it give."
        ],
        [
            "Datamatrix, but in practice we union.",
            "They may have missing data.",
            "We may also have data outliers so, but we show even the basic work is very convenient for us to handle this kind of problems.",
            "We don't need to build extra model to deal with this kind of data anomalies, so for the missing data, instead of observing the full data XI, we observe a subset of data.",
            "Why I equal to Sigma shown here?",
            "XI So Sigma and now is the sampling matrix corresponding to extracting corresponding row of the identity matrix and we can show we can factorize the.",
            "Likelihood attend or social with the four data into two parts, so one part is only associated with the observed data and other things only associated with the missing data part.",
            "Therefore, we can marginalizing out the missing data part and we only use the observed data to infer the latent parameters with the info latent parameters.",
            "We can then reconstruction of estimate our missing data.",
            "We may also have this data outliers, which is shown like for the robust PCA.",
            "Next, we have data nominees which can neither be represented in the dictionary or leader.",
            "Be appropriate under the Gaussian noise.",
            "Therefore we now write the data into three times.",
            "The first thing is the representation under dictionary.",
            "As usual.",
            "The second time is the noise drawn Gaussian as usual, and the setting is the sparse spiking noise and we corresponding to a head MoD or elemental wise product between a real vector and the binary vector, and this binary actively.",
            "Give the beta blue prior to encourage sparsity and this will be active draw for normal and so this data nominees would encourage to be absorbed into this bucket turn, and then we would be able to recover the data only based on its reputation and the dictionary.",
            "And this model is not conjugate anymore cash compared to the simpler BPS.",
            "Where is conjugate?",
            "We can directly do the gift sample."
        ],
        [
            "In France now here is we have to ensure the independence chain Metropolitan Hasting, slice sampling, and all the other parameters by is is is obtained by skip sampling and the independence change here is actually very efficient.",
            "We can also 90% of the acceptance rate and if we examine the fall of the proposal is make lots of sense.",
            "It's just saying that the feature usage at this reference point should be related to how the neighborhood samples choosing this feature.",
            "And."
        ],
        [
            "So we did experiment in image interpolation and image denoising.",
            "The first one we have missing pixels and the locations that we know the location would be some pixel and second one we have white Gaussian noise and spiky noise is more challenging because we don't know the amplitude and we don't know the location with spiky's.",
            "And here with the code and independence we introduce codependence.",
            "We mean the Patch spatial locations were trying to encourage patches nearby in the space in space should tend to share similar features."
        ],
        [
            "So this example that we as we showed before, if we directly use the simpler BP, which means only use the sparsity constraint.",
            "We get this image, but if we use the DHCP reduce the codependence.",
            "Especially in independence.",
            "Then we get a much better result and they see these artifacts like the texture in the mouse and I is no longer observed in the DSP results.",
            "So how the model works?",
            "Why it works much better?"
        ],
        [
            "This case, so I'm going to examine the give some reasons.",
            "So first of all, if you look at the BP dictionary as showing left corner, you can see the diction Atom does not really match the original texture really well.",
            "But if you show the DSP dictionary, the texture match really matched the local texture, and if we look at the AT and usage, so the more acting use would reflect the more complex of that Patch, so it did not need more features to represent itself.",
            "We can see in the DSP.",
            "Dictating usage map is weather clear back in this area.",
            "It should be smooth so it all should be used as small user.",
            "Very small number of features you should be able to represent represent itself but in the BP is like it's not very clear it's trying to use many different kind of features to present itself.",
            "So it's kind of over fitting and if we pick a diction at and look in further by the DHCP and we look at the feature using activation map you further by which means the DH people.",
            "Tell them or tell the tell the model that in this area this fact this.",
            "Atom is highly likely, but in the face this item is highly unlikely.",
            "But for the BP is just saying OK, this Atom is have the same probability to be selected in every location with image.",
            "Therefore we can see this weird artifacts of the of the texture."
        ],
        [
            "Iron Mouse, which is actually from the front close.",
            "And then we also did some other example like for the boat image, the strychnine, the Street, Nine in the original image is no longer St 9, but it is still."
        ],
        [
            "Oh, really, well, well preserved in the DHCP recovery.",
            "And if you look at the stripe pattern of the roof in the heel image original and BP recovery is not clear at all, but the HP recovery preserve the most."
        ],
        [
            "Information and so.",
            "In that case we have the missing pixels, but in this case we have the spiky noise which is more challenging because we don't know the location and we don't know the amplitude.",
            "There are uniformly at random need distributed, and if we directly with a similar BP which is only the sparse prior sparse promoting prior, we can show the learned patterns actually contain with spikes, which is not surprising because if you can see the.",
            "Or the patches across the image, which is 60,000.",
            "Here is very is very likely.",
            "You would find this Patch is the spike is dispatched to match each other.",
            "Therefore it should be considered as useful features.",
            "But if you use the DHCP constraint is only look at the local region.",
            "If you look at the local region spike to repeat the probability is significantly reduced and therefore it can learn these features which are not corrupted by spiky.",
            "Therefore you can get the best you can get, much better denoising results than the BP, which still keeps the spikes."
        ],
        [
            "Also, we did another image.",
            "The results also have similar.",
            "And the."
        ],
        [
            "So to summarize, we want we can also do a lot of work on this framework.",
            "Now.",
            "It's as we remember we have a random walk matrix, which is the is the number of samples.",
            "So the problem is that if you have millions of data samples, if you want to generate 2 new sample to predict the future usage of a new sample, then you have to keep all this data which is which is obviously not practical.",
            "So we develop a landmark DSP model which have J landmarks.",
            "Which the J2 to guide the future usage and J is much smaller than.",
            "And then we can also consider locality constraint.",
            "For manifold learning we cause cancer variational inference on online learning and the other application for this model to to show off.",
            "Maybe the Super resolution deblurring and video background foreground modeling."
        ],
        [
            "So, so this is conclusion.",
            "So in summary, we present a factor model which can be used for dictionary to encourage that similar similar samples which are closely nearby in the client space that should share similar features and is very useful when your data has missing entries or have outliers.",
            "So thank you for attention and happy to take your questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So good morning everyone.",
                    "label": 0
                },
                {
                    "sent": "So we have just had two excellent talk about the mixture membership model with the digital process.",
                    "label": 0
                },
                {
                    "sent": "So here I'm going to talk about another important model that's the factor model and we use the beta process as the prior.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To encourage sparsity, so you may be interested in this model if you are interested in the processing application in general, or you may be interested in this model, and if you don't believe that a simple sparsity can solve all the problems, and if you may also be interested in this model, and if you handling data matrix which may have missing entries which may be noisy or make corrupted by data outliers.",
                    "label": 0
                },
                {
                    "sent": "So this is the outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so there has been significant recent interest in diction, learning and discuss coding, and this problem can generally formalized and this object function.",
                    "label": 0
                },
                {
                    "sent": "So give us a data matrix X.",
                    "label": 0
                },
                {
                    "sent": "We're trying to represent, and the dictionary with the constraint that representation should be sparse.",
                    "label": 0
                },
                {
                    "sent": "And we have shown that we can easily add.",
                    "label": 0
                },
                {
                    "sent": "This can be naturally formalized, and the sparse fact and this model framework with the Indian buffet process or the beta process used at the sparse promoting prior and the fact loading of the factor model with corresponding to the diction Adam and we can also say they have the feature or the dish.",
                    "label": 1
                },
                {
                    "sent": "So I will use them, exchangeable that the same.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this talk.",
                    "label": 0
                },
                {
                    "sent": "And So what?",
                    "label": 0
                },
                {
                    "sent": "You briefly introduce the underlying stochastic process.",
                    "label": 0
                },
                {
                    "sent": "So the beta process and balloon process is used as a sparse coding prior in our factor model and draw from beta process can be considered a infinite collection of atoms and each Atom is giving a weight indicating how probable for this Atom to be selected given sample.",
                    "label": 0
                },
                {
                    "sent": "So the measure be drawn.",
                    "label": 0
                },
                {
                    "sent": "Beta process can be.",
                    "label": 0
                },
                {
                    "sent": "Linked to a feature usage XI with the ballooning process.",
                    "label": 0
                },
                {
                    "sent": "So the blue so they draw from a blued process with minimal B can be consumed.",
                    "label": 0
                },
                {
                    "sent": "The infinite connection of a binary.",
                    "label": 0
                },
                {
                    "sent": "Talk coin flip outcomes so we can see here we have the XI equal to some inflammation of the arcade at DK and DK is the case.",
                    "label": 0
                },
                {
                    "sent": "Adam and DK is a binary indicating whether the ice sample choose the case feature and if we look at the posterior of the measure B giving the observations.",
                    "label": 0
                },
                {
                    "sent": "As shown in this equation is still drop available sets due to the controversy of the beta process and the balloon process and if it has been shown that if we marginalizing out to the measured maybe we can get a predictive predictive duration for new sample which is drawn from.",
                    "label": 0
                },
                {
                    "sent": "This is considered the two permit generalization of the Indian buffet process.",
                    "label": 1
                },
                {
                    "sent": "So it will examine the form of the posterior distribution with Measure B and the predictive predictive distribution will do sample.",
                    "label": 0
                },
                {
                    "sent": "We can see that the parameter is only related to the submission with feature usage, which means it doesn't care about the ordering of the of the incoming samples.",
                    "label": 0
                },
                {
                    "sent": "I can randomly permutate the ordering of the incoming sample and this posterior and predictive distribution keeps the same, and in other words, if using a latent feature model, that means that every sample is assumed to have the same probability to select the given dish.",
                    "label": 0
                },
                {
                    "sent": "Let's say we go to the go to the breakfast buffet.",
                    "label": 0
                },
                {
                    "sent": "And they assume everybody every one of us had the same probability.",
                    "label": 0
                },
                {
                    "sent": "Choose the cake or choose the orange juice, or choose the milk which is obviously not true.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we this is also not true for the image.",
                    "label": 1
                },
                {
                    "sent": "Like we we give example.",
                    "label": 0
                },
                {
                    "sent": "Here we did the experiment that we have a Barber image which is 256 by 256 here and there were randomly took 80% of the pixel, which means we only observe 20% pixel and we're going to.",
                    "label": 0
                },
                {
                    "sent": "Interpolate this missing pixels based on the factor model with the beta process beta balloon process and the sparse modding prior.",
                    "label": 0
                },
                {
                    "sent": "And then we when we first together with this without, I was self impressed by the reconstruction.",
                    "label": 0
                },
                {
                    "sent": "We can see the reconstruction is is actually very good considering only observe 20% of the pixels.",
                    "label": 0
                },
                {
                    "sent": "But you can still see where the obvious artifacts that the texture in the clouds would appear in a mouse I need I it's very undesired artifacts and if you look at actually the length dictionary at.",
                    "label": 0
                },
                {
                    "sent": "Dictionary Atom the makes sense, but they do not match the original texture really well, so this is a motion.",
                    "label": 0
                },
                {
                    "sent": "This one of the major motivation for us to consider the covalent independence in the factor model.",
                    "label": 0
                },
                {
                    "sent": "So the current independence here, which means the patches which are specially nearby.",
                    "label": 0
                },
                {
                    "sent": "We should encourage them to tend to choose similar features.",
                    "label": 0
                },
                {
                    "sent": "Therefore, which we should remove the exchange building Sumption to say that the Patch they are they don't relate to each other.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter about the ordering, so the natural image ordering is in fact really important.",
                    "label": 0
                },
                {
                    "sent": "And they have been lost for previous work on, but introducing the Commander did.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This into the basing models like the last week about the dependence directly process, and there's also some recent work about removing the exchangeable exemption in the Indian buffet process.",
                    "label": 0
                },
                {
                    "sent": "For example, the formal genetic ICP they're trying to build a tree structure to take into consideration of the codependence, and also this recent work.",
                    "label": 0
                },
                {
                    "sent": "But the dependent Indian buffet process.",
                    "label": 0
                },
                {
                    "sent": "So using the hierarchical Gaussian process to take into consideration the codependence as we know.",
                    "label": 0
                },
                {
                    "sent": "A a problem with the Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "You have too many samples.",
                    "label": 0
                },
                {
                    "sent": "It may be very difficult to build a huge covariance matrix, for example, in our application we may for 256 by 256 image we may have 60,000 more than 60,000 patches.",
                    "label": 0
                },
                {
                    "sent": "If you want to build a 60,000 patches covariance matrix corresponding to 60,000 patches, that would be huge to be practical.",
                    "label": 0
                },
                {
                    "sent": "To draw from that.",
                    "label": 0
                },
                {
                    "sent": "Therefore the computational complexity is not practical.",
                    "label": 0
                },
                {
                    "sent": "And in this work we are trying to discover dependence and we closely following the way of the introducing the best density regression which trying to build the current dependence through kernel construction.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before I go through the detail of the model, I want to give some additional review of the beta process, which is the key element of your model.",
                    "label": 0
                },
                {
                    "sent": "So a beta process is a positive living process whose level measure leaves on their productive space, which can be expressed as a product of a degenerate beta distribution and a base measure.",
                    "label": 1
                },
                {
                    "sent": "So if the base measure buehner is continuous, then we get a measure B which corresponding to an infinite song.",
                    "label": 0
                },
                {
                    "sent": "Over atoms will each Atom is given await an if and in this case the wait is drawing a degenerate beta distribution and if the Bina is discrete then we have the same formula but the PK the wait is now draw for a beta distribution parameterized by C&QK.",
                    "label": 0
                },
                {
                    "sent": "So we can see here that B is discrete almost surely.",
                    "label": 0
                },
                {
                    "sent": "Therefore we can use it as a base measure for another level of beta process draw, and in this case we can.",
                    "label": 0
                },
                {
                    "sent": "We can we can make sure that and different draws from the same base measure B would share the same set of atoms, but.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With different weight, and that's very important.",
                    "label": 0
                },
                {
                    "sent": "And so then we now we're going to show how we going to impose the codependency for the HPP construction.",
                    "label": 0
                },
                {
                    "sent": "So, first of all, we want to introduce the random walk matrix, which summarizes the relationship between samples in the colon space and like each row of this matrix would sound to one and.",
                    "label": 1
                },
                {
                    "sent": "And they each element Aij would would reflect the distance in the coordinate space between sample I&J.",
                    "label": 0
                },
                {
                    "sent": "So Li here next shown here.",
                    "label": 0
                },
                {
                    "sent": "Is the covalent associate sample I and KLILJ is the kernel distance between the covariance ion correct?",
                    "label": 0
                },
                {
                    "sent": "J&J is just normalized kernel distance.",
                    "label": 0
                },
                {
                    "sent": "So if 2 sample nearby in the colon space AIG would be large and this has been widely used in like spectral clustering in manifold learning and diffusion analysis.",
                    "label": 0
                },
                {
                    "sent": "So we're going to show you how to put into the HPP framework to encourage the codependence so.",
                    "label": 0
                },
                {
                    "sent": "Remember so B is draw from BPC nabina, so it's discrete almost surely and we use this based measure for another level.",
                    "label": 1
                },
                {
                    "sent": "Beta process draw and so we end with BJ start draw from BP.",
                    "label": 0
                },
                {
                    "sent": "Consider BJ and BJ prime.",
                    "label": 0
                },
                {
                    "sent": "The draw from the same BP.",
                    "label": 0
                },
                {
                    "sent": "Therefore the share same set of atoms, but they're going to have different local probability based on their own posterior and when come to a given sample it would be corresponding to a convex compilation.",
                    "label": 0
                },
                {
                    "sent": "Of this underlying BP job and the date is linked to the this be I threw a balloon process and if you do some math and we can show that the correlation between the feature usage at the ice location of the eye sample and the sample I prime would corresponding to the constant distance or the normalized correlation between AI on a prime AI prime recalls that AI is the ISO of the random walk matrix therefore.",
                    "label": 0
                },
                {
                    "sent": "If two samples are nearby in the coordinate space, AI an AI prime would be highly correlated.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we can show the model is encourage ING if someone nearby in the covalent space they tend to share similar features, but if 2 sample far away from each other, then the correlation is low.",
                    "label": 0
                },
                {
                    "sent": "But remember they still share the same set of atoms, therefore it still have possibility to be representative with similar features based on the post.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Garrett so we can.",
                    "label": 0
                },
                {
                    "sent": "We're going to show that we can use the DHCP.",
                    "label": 0
                },
                {
                    "sent": "Is a sparse prior to represent to replace the simpler beta process as a sparse promoting prior for the factor model?",
                    "label": 0
                },
                {
                    "sent": "So the first the first line equation is just the the user saying that give me a sample I it should be sparsely represented under dictionary, subjected to the approximate areas and the residue areas drop for normal and the second one is that the case diction element.",
                    "label": 0
                },
                {
                    "sent": "Is Jaafar based measure which is normal here and the real weight is draw for normal and when we using BP we have the ZIK which indicating whether I sample should choose the case feature is drawn from ballooning paikea as we can see the paikea is not linked to the index of the sample.",
                    "label": 0
                },
                {
                    "sent": "Therefore all the samples assumed to have the same probability to choose the case feature which is obviously not true for lots of philosophy examples.",
                    "label": 0
                },
                {
                    "sent": "And so we're going to replace.",
                    "label": 0
                },
                {
                    "sent": "BP is this DSP?",
                    "label": 0
                },
                {
                    "sent": "To introduce the codependence, therefore we can see now the iks drawn ballooning paikea, so everyone would have its own feature usage probability, and this one would be a convex combination of latent feature usage at reference point and this reference points draw from beta.",
                    "label": 0
                },
                {
                    "sent": "See Juanita K and this heater K, which indicating the global probability is also self driven beta.",
                    "label": 0
                },
                {
                    "sent": "So this part corresponding to HB Construction and this part of the introduced the Codependence and we construct.",
                    "label": 0
                },
                {
                    "sent": "AJ with a normalized kernel and this kernel we construct in the way they say video basis function with neighborhood constraint.",
                    "label": 0
                },
                {
                    "sent": "This is similar to the way to build their similarity matrix in the ISO map.",
                    "label": 0
                },
                {
                    "sent": "And so we're using factor model, so we're trying to build a dictionary or trying to infer the factor loadings factor score for it give.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Datamatrix, but in practice we union.",
                    "label": 0
                },
                {
                    "sent": "They may have missing data.",
                    "label": 1
                },
                {
                    "sent": "We may also have data outliers so, but we show even the basic work is very convenient for us to handle this kind of problems.",
                    "label": 0
                },
                {
                    "sent": "We don't need to build extra model to deal with this kind of data anomalies, so for the missing data, instead of observing the full data XI, we observe a subset of data.",
                    "label": 0
                },
                {
                    "sent": "Why I equal to Sigma shown here?",
                    "label": 0
                },
                {
                    "sent": "XI So Sigma and now is the sampling matrix corresponding to extracting corresponding row of the identity matrix and we can show we can factorize the.",
                    "label": 0
                },
                {
                    "sent": "Likelihood attend or social with the four data into two parts, so one part is only associated with the observed data and other things only associated with the missing data part.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we can marginalizing out the missing data part and we only use the observed data to infer the latent parameters with the info latent parameters.",
                    "label": 1
                },
                {
                    "sent": "We can then reconstruction of estimate our missing data.",
                    "label": 0
                },
                {
                    "sent": "We may also have this data outliers, which is shown like for the robust PCA.",
                    "label": 0
                },
                {
                    "sent": "Next, we have data nominees which can neither be represented in the dictionary or leader.",
                    "label": 0
                },
                {
                    "sent": "Be appropriate under the Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "Therefore we now write the data into three times.",
                    "label": 0
                },
                {
                    "sent": "The first thing is the representation under dictionary.",
                    "label": 0
                },
                {
                    "sent": "As usual.",
                    "label": 0
                },
                {
                    "sent": "The second time is the noise drawn Gaussian as usual, and the setting is the sparse spiking noise and we corresponding to a head MoD or elemental wise product between a real vector and the binary vector, and this binary actively.",
                    "label": 1
                },
                {
                    "sent": "Give the beta blue prior to encourage sparsity and this will be active draw for normal and so this data nominees would encourage to be absorbed into this bucket turn, and then we would be able to recover the data only based on its reputation and the dictionary.",
                    "label": 0
                },
                {
                    "sent": "And this model is not conjugate anymore cash compared to the simpler BPS.",
                    "label": 0
                },
                {
                    "sent": "Where is conjugate?",
                    "label": 0
                },
                {
                    "sent": "We can directly do the gift sample.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In France now here is we have to ensure the independence chain Metropolitan Hasting, slice sampling, and all the other parameters by is is is obtained by skip sampling and the independence change here is actually very efficient.",
                    "label": 1
                },
                {
                    "sent": "We can also 90% of the acceptance rate and if we examine the fall of the proposal is make lots of sense.",
                    "label": 0
                },
                {
                    "sent": "It's just saying that the feature usage at this reference point should be related to how the neighborhood samples choosing this feature.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we did experiment in image interpolation and image denoising.",
                    "label": 1
                },
                {
                    "sent": "The first one we have missing pixels and the locations that we know the location would be some pixel and second one we have white Gaussian noise and spiky noise is more challenging because we don't know the amplitude and we don't know the location with spiky's.",
                    "label": 0
                },
                {
                    "sent": "And here with the code and independence we introduce codependence.",
                    "label": 1
                },
                {
                    "sent": "We mean the Patch spatial locations were trying to encourage patches nearby in the space in space should tend to share similar features.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this example that we as we showed before, if we directly use the simpler BP, which means only use the sparsity constraint.",
                    "label": 0
                },
                {
                    "sent": "We get this image, but if we use the DHCP reduce the codependence.",
                    "label": 0
                },
                {
                    "sent": "Especially in independence.",
                    "label": 0
                },
                {
                    "sent": "Then we get a much better result and they see these artifacts like the texture in the mouse and I is no longer observed in the DSP results.",
                    "label": 0
                },
                {
                    "sent": "So how the model works?",
                    "label": 0
                },
                {
                    "sent": "Why it works much better?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This case, so I'm going to examine the give some reasons.",
                    "label": 0
                },
                {
                    "sent": "So first of all, if you look at the BP dictionary as showing left corner, you can see the diction Atom does not really match the original texture really well.",
                    "label": 0
                },
                {
                    "sent": "But if you show the DSP dictionary, the texture match really matched the local texture, and if we look at the AT and usage, so the more acting use would reflect the more complex of that Patch, so it did not need more features to represent itself.",
                    "label": 0
                },
                {
                    "sent": "We can see in the DSP.",
                    "label": 0
                },
                {
                    "sent": "Dictating usage map is weather clear back in this area.",
                    "label": 0
                },
                {
                    "sent": "It should be smooth so it all should be used as small user.",
                    "label": 0
                },
                {
                    "sent": "Very small number of features you should be able to represent represent itself but in the BP is like it's not very clear it's trying to use many different kind of features to present itself.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of over fitting and if we pick a diction at and look in further by the DHCP and we look at the feature using activation map you further by which means the DH people.",
                    "label": 0
                },
                {
                    "sent": "Tell them or tell the tell the model that in this area this fact this.",
                    "label": 0
                },
                {
                    "sent": "Atom is highly likely, but in the face this item is highly unlikely.",
                    "label": 0
                },
                {
                    "sent": "But for the BP is just saying OK, this Atom is have the same probability to be selected in every location with image.",
                    "label": 0
                },
                {
                    "sent": "Therefore we can see this weird artifacts of the of the texture.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Iron Mouse, which is actually from the front close.",
                    "label": 0
                },
                {
                    "sent": "And then we also did some other example like for the boat image, the strychnine, the Street, Nine in the original image is no longer St 9, but it is still.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh, really, well, well preserved in the DHCP recovery.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the stripe pattern of the roof in the heel image original and BP recovery is not clear at all, but the HP recovery preserve the most.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Information and so.",
                    "label": 0
                },
                {
                    "sent": "In that case we have the missing pixels, but in this case we have the spiky noise which is more challenging because we don't know the location and we don't know the amplitude.",
                    "label": 0
                },
                {
                    "sent": "There are uniformly at random need distributed, and if we directly with a similar BP which is only the sparse prior sparse promoting prior, we can show the learned patterns actually contain with spikes, which is not surprising because if you can see the.",
                    "label": 0
                },
                {
                    "sent": "Or the patches across the image, which is 60,000.",
                    "label": 0
                },
                {
                    "sent": "Here is very is very likely.",
                    "label": 0
                },
                {
                    "sent": "You would find this Patch is the spike is dispatched to match each other.",
                    "label": 0
                },
                {
                    "sent": "Therefore it should be considered as useful features.",
                    "label": 0
                },
                {
                    "sent": "But if you use the DHCP constraint is only look at the local region.",
                    "label": 0
                },
                {
                    "sent": "If you look at the local region spike to repeat the probability is significantly reduced and therefore it can learn these features which are not corrupted by spiky.",
                    "label": 0
                },
                {
                    "sent": "Therefore you can get the best you can get, much better denoising results than the BP, which still keeps the spikes.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, we did another image.",
                    "label": 0
                },
                {
                    "sent": "The results also have similar.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize, we want we can also do a lot of work on this framework.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "It's as we remember we have a random walk matrix, which is the is the number of samples.",
                    "label": 0
                },
                {
                    "sent": "So the problem is that if you have millions of data samples, if you want to generate 2 new sample to predict the future usage of a new sample, then you have to keep all this data which is which is obviously not practical.",
                    "label": 0
                },
                {
                    "sent": "So we develop a landmark DSP model which have J landmarks.",
                    "label": 0
                },
                {
                    "sent": "Which the J2 to guide the future usage and J is much smaller than.",
                    "label": 0
                },
                {
                    "sent": "And then we can also consider locality constraint.",
                    "label": 1
                },
                {
                    "sent": "For manifold learning we cause cancer variational inference on online learning and the other application for this model to to show off.",
                    "label": 1
                },
                {
                    "sent": "Maybe the Super resolution deblurring and video background foreground modeling.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so this is conclusion.",
                    "label": 0
                },
                {
                    "sent": "So in summary, we present a factor model which can be used for dictionary to encourage that similar similar samples which are closely nearby in the client space that should share similar features and is very useful when your data has missing entries or have outliers.",
                    "label": 0
                },
                {
                    "sent": "So thank you for attention and happy to take your questions.",
                    "label": 0
                }
            ]
        }
    }
}