{
    "id": "z4ab5tvk5hsv6qb3ciqckj43pngmhk6i",
    "title": "Fast global convergence rates of gradient methods for high-dimensional statistical recovery",
    "info": {
        "author": [
            "Alekh Agarwal, Microsoft Research"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Regression"
        ]
    },
    "url": "http://videolectures.net/nips2010_agarwal_fgc/",
    "segmentation": [
        [
            "So we got a great talk just now about how we don't do enough statistics.",
            "So now I'm going to try and show off some of the statistics that we can actually do, and in particular this will be in the high dimensional statistical setting where we will try to prove some convergence properties for gradient methods."
        ],
        [
            "And so the setting here is in.",
            "High dimensional statistical recovery where we have a parameter space Omega which is contained in other D and each of these parameters permit arises, that is probability distribution and the data that we observe is drawn from some unknown parameter Theta star an in the high dimensional setting, it's typically assumed that the number of parameters is much much larger than the number of samples.",
            "A typical estimator of interest in these settings is the regular regularised empirical risk minimizer, which minimizes the loss function subject to the regularizer being constrained at some level row.",
            "Now, for these estimators, what people have shown statistically is that even in this regime, where number of samples is much much smaller than the number of parameters Theta hat can often be consistent in the estimation of the true parameter Theta star under certain regularity assumptions.",
            "However, for this to be a meaningful procedure, you have to be able to solve the computational problem to obtain three ahead as well from the data an."
        ],
        [
            "It happens often in high dimensional setting says that the optimization procedures to actually compute it ahead can often be quite slow.",
            "In practice an I will be a bit more concrete about why this happens in the later part of the talk, but the question that we want to ask in the stock."
        ],
        [
            "Is the optimization procedures for computing Theta hack benefit from the same assumptions that give us so much benefit in the statistical analysis an the answer, of course is going to be in affirmative for a variety of statistical models.",
            "We can actually show that we do gain from such assumptions so."
        ],
        [
            "Immigration procedure that I'm going to consider in this talk is very simple.",
            "Classical gradient descent procedure.",
            "So we start from some arbitrary initial point and at every step we take our current iterate Theta T. We take the we take the gradient at the iterate and take a step in the negative gradient direction.",
            "This will usually take us outside the constraints set.",
            "So we project down under the constraint set to obtain our next.",
            "Iterate 30 + 1.",
            "Here I'm using a constant step size, you which depends on the smoothness properties of the last function.",
            "So this is a classical algorithm.",
            "It's convergence properties are well understood.",
            "In particular, it's known that when the loss function is smooth, then this procedure converges at a sub linear rate and when the loss function is both smooth and strongly convex then this procedure converges at a linear rate.",
            "Now to understand this algorithm and how it works in the high dimensional settings, I'm going to take a particular statistical model of interest."
        ],
        [
            "Which is the sparse linear regression model an.",
            "In this model we observe the data vectors Zaizen outta the D and the regression outputs Y eyes which are generated by taking dot product between Zion some unknown parameter Theta star and corrupting it with zero mean noise now.",
            "Of course, Wendy is much, much larger than end and in general recovery of data start from samples is hard, is impossible, and so we need some further assumptions and a typical assumption is that data star has at most S non zero entries.",
            "An in such a situation of popular estimator is the last two estimator where the loss function is the squared loss between the observations and predictions and we impose an L1 norm regularizer at a certain radius role which enforces sparsity on the optimum.",
            "So now if you take the projected gradient descent algorithm from the previous slide and apply it to the sparse linear regression problem, then this is what you."
        ],
        [
            "Serve so here I show you on a log scale the distance between 30 and Terra Hat is a function of number of iterations for a variety of sample sizes on the sparse linear regression problem and the thing to note here is that we're getting a linear convergence in the log rhythmic scale.",
            "So so if you recall."
        ],
        [
            "From the previous slides, the linear convergence for projected gradient descent happens and the loss function is both smooth and strongly convex, so."
        ],
        [
            "But this one empirical result and this happens in a typical sense, so you can do it several times and you observe the same phenomenon.",
            "What it tells us is that there must be some smoothness and strong convexity properties of the loss function in sparse linear regression problem that the optimization procedure must be exploiting.",
            "But if you look."
        ],
        [
            "The current theory that tries to explain the optimization for this problem, it sort of falls short of explaining this linear convergence shown here.",
            "So something earliest work in this problem."
        ],
        [
            "Us to apply Nesterov's methods in this problem, and they assume the smoothness of the loss function for the sparse linear regression problem to obtain sub linear rates of convergence, which would be much much slower than the linear rates that we are observing here.",
            "Some works tried to prove linear convergence, but they could either prove local linear convergence which would only apply to the last few traits, or they proved a linear convergence up to the variance of the problem, so it would only apply to the initial few traits to a very rough precision, so neither of the results would explain the global linear convergence shown here.",
            "Bradys and Lawrence did actually establish global linear convergence by assuming that the data specifies a certain restricted isometry property, and they also assume smoothness of the loss function.",
            "So, so in a nutshell, it seems like the smoothness and curvature of the loss function, they seem to be sort of the key properties in determining the performance of gradient descent algorithm on these problems.",
            "So let's drill a bit deeper into what these properties are like for the sparse linear regression set up in the high dimensional problems."
        ],
        [
            "So if you take the sparse linear regression loss function and further assume that the data is generated from a very simple standard normal model, then what happens is that in high dimensions you do not have any smoothness and you do not have any curvature.",
            "So if you take the largest eigenvalue of the Hessian matrix, which determines the smoothness of the loss function, this can be as big as square root of the overan with high probability, and Wendy is much much larger than N. This quantity can be very large.",
            "At the same time, there are the Hessian matrix is at most rank N when you have only one data points, so there are at least D -- N directions in which you have no curvature whatsoever.",
            "So now you have a very very ill conditioned problem where the Hessian has no curvature in some directions and is extremely spiky in certain directions, and you do not expect to see the nice global linear convergence for gradient method as I showed you a couple of slides before, so clearly there is there is some gap happening between what we see and.",
            "What we can show theoretically here and the focus of this talk is going to be to try and bridge this gap so."
        ],
        [
            "The key observation that we make in this work is that even though you cannot establish global smoothness and strong convexity of the loss function in the high dimensional setting, if you take appropriately restricted notions of these assumptions then they can actually be established even when D is much much larger than N. So we first start with the strong convexity assumption.",
            "So we say that a loss function satisfies restricted strong convexity if the curvature is lower bounded, but only with up to an additive tolerance, and only on the feasible set.",
            "So this is very much like the standard strong convexity assumption, but with two notable exceptions, one being that it only assumes curvature up to this additive tolerance and 2nd being it's only on the set.",
            "Now, I've sort of hopefully convinced you that global strong convexity will not hold.",
            "In the high dimensional setting, but what?"
        ],
        [
            "Turns out is that if you choose your optimization set appropriately and the tolerance appropriately, then you can actually establish for various problems of interest that restricted strong convexity will actually hold with high probability.",
            "So that's the first ingredient and what this gives us in way of the analysis is that it allows us to say that if we take an iterative at T that is sufficiently far away from Fahrenheit.",
            "So if the distance between 30 minus Theta had is much bigger than the additive tolerance here, then we will actually be able to lower bound.",
            "Picture of our loss function at the iterate, so this ends up buying us some curvature on the iterates.",
            "The second ingredient is a similar assumption about the smoothness, so I say that a loss function satisfies restricted smoothness if the curvature is upper bounded.",
            "Again, modular and additive tolerance, and again only on the feasible set.",
            "And once again we will see that by appropriate choice of the two quantities, we will be able to establish smoothness in a high dimensional setting, even though it does not hold in a global unrestricted setting.",
            "Now, what these assumptions bias?"
        ],
        [
            "Is that under these assumptions I can actually establish linear convergence of my gradient descent method, So what I can show is that my interest Arity will converse with her ahead geometrically up to a precision of Delta squared, and the precision Delta squared here."
        ],
        [
            "The exact same tolerance level that was in my assumptions, and this tolerance is exactly the sort."
        ],
        [
            "Of Delta squared in the result of this theorem.",
            "The contraction factor here naturally depends on my upper and lower smoothness constants, as is natural for gradient algorithms, so so I do get a global linear convergence as my simulations seem to suggest, and this is this holds globally for all iterates, unlike some of the previous local linear convergence results, but the."
        ],
        [
            "I think that might catch your attention is that this convergence only happens up to an accuracy Delta squared.",
            "So if you're an optimization person, this might strike you as a little odd at first because we were used to seeing the convergence of 30 to 3 to head up to arbitrary numerical precision an this gives you only the convergence of finite precision.",
            "So the reason why this is still a good result in this setting is because you have to recall I'm not trying to solve any arbitrary optimization problem here.",
            "I'm trying to solve an optimization problem that's coming.",
            "Out of statistical estimation settings, so my perimetre of interest is not the optimization optimum Theta hat, but what I really want to recover is the true parameter Theta star that's generating the data now typically straight ahead and thermostat are different and in fact I can define the statistical precision of the problem to be the expected distance between Theta hat and theater star, right?",
            "So now there are two possibilities."
        ],
        [
            "One is that I can only guarantee you that my interacted at evil approached her ahead to a distance Delta, which is much much bigger than the statistical precision of the problem.",
            "In this case, that is a terrible estimator for Theta star.",
            "Be cause if you could solve the optimization problem well then."
        ],
        [
            "Could get this straight ahead, which is close to the star, but you're only getting a Theta T which is far away from Theta star which is at least Delta distance away from Theta star, so it's a terrible estimator, but on the other hand, if I can guarantee that.",
            "The distance between 30 and hit us head is on the same order as the distance between Kira had in theater star on the same order as a statistical precision.",
            "Then therapy is at most twice the statistical precision away from Theta star, so it's.",
            "Up to constant factors, it's as good an estimate or as you can get from your data really, and so the goal would be to actually establish these convergence results up to the statistical precision of the problem.",
            "An as I had noted before, the source of this tolerance, Delta is the tolerance in the restricted strong convexity and smoothness assumptions, so we will need to establish those assumptions with up to a statistical precision tolerance, and this will be done by intelligent choice of the optimization set up, the regularization radius roll.",
            "So."
        ],
        [
            "To illustrate the applications of these results, I'm going to start with a particular example of the sparse linear regression problem where now assume that my data is generated according to a multivariate Gaussian distribution with covariance matrix Sigma an.",
            "I assume that my true parameter Theta star, is sparse now.",
            "Recall that in this problem my last function was a squared loss an my.",
            "In my regularizer was the L1 norm.",
            "In this case I said the regularization radius to be the L1 norm of terrastar and with this choice of the radius rho I can establish restricted strong convexity and smoothness with high probability up to a precision equal to S logged on North.",
            "What this gives me is the convergence of Theta, deducted ahead at a geometric rate up to a precision S logged on earth, and for those of you who are familiar with the statistical analysis of this problem, you will recognize that this is actually the minimax statistical error on this problem.",
            "So what my result is giving is that Katie will approach straight ahead to the smallest possible distance.",
            "Statistically 30 will approach to start to the smallest possible distance statistically at a linear rate of convergence, and the rate of convergence here depends on the.",
            "Population on the conditioning of the population covariance, which is natural because that determines the curvature properties of your loss function, and here I'm stating the result only for exact sparsity, but the results also extend to an approximate approximately sparse setting in a natural fashion so.",
            "So, so that's that's a theoretical result for sparse linear regression, and we can verify that this linear convergence actually happens in simulations as well.",
            "So this is revisiting the same slide I had before and."
        ],
        [
            "And let's look a bit more into it.",
            "So here I assume that my parameter Theta star is exactly sparse and the population covariance from which the data is generated satisfies some sort of restricted isometry assumption an what I'm showing here is for a few different choices of sample size, the convergence of Theta, Theta hat and analog scale an it is indeed geometric, as the theory predicts and convergence rate slows down as a speeds up as the number of samples increases, which is natural because the conditioning of your.",
            "Population covariance improves as you as you observe more and more samples."
        ],
        [
            "Now, of course, these assumptions are a little restrictive, so as I said we can relax, start relaxing some of these.",
            "We can go from exact sparsity to approximate sparsity.",
            "We still observe a linear rate of convergence and the convergence rate of course slows down compared to exact sparsity, because this is a harder problem, the curvature is worse for these problems often.",
            "You can also relax the restricted isometry assumption because."
        ],
        [
            "I see an RSM are in Norway directly relying on our IP so you can relax the ripi assumption, take a very very heavily correlated design and you still get a linear rate of convergence.",
            "But again, the curvature of the model degrades and you get a slower rate of convergence, but it is still linear, so that's the theoretical and experimental story we wanted to go over for sparse linear regression now."
        ],
        [
            "I want to visit another problem which is that of low rank matrix completion.",
            "So in The Lorax matrix completion problem, we have some true matrix terrastar that's that's not told to us.",
            "What we observe is randomly sampled entries of this matrix corrupted with some zero mean noise.",
            "Now.",
            "So for instance, is like most of you are familiar with the Netflix challenge by now, so that's sort of place.",
            "This problem arises an in general when you observe only a few of a few of the entries of a matrix.",
            "It's impossible to complete the rest of the matrix without further structural.",
            "Assumptions so a common assumption in these problems is that the matrix Theta star has a rank at most R. An in this problem we take the loss function again to be the squared loss between the predictions and observations, and we take the regularizer to be the L1 norm on the singular values of matrix Theta, which is probably more familiar to most of you as the nuclear norm or the trace norm and regularization radius in this case is set to be the trace norm of the true matrix Theatre style, and with this setting we can show that once again our parameter Theta T converges at a geometric rate to Theta hat."
        ],
        [
            "Apple, a precision and this up to the log in factor is again equal to the minimax statistical precision of the problem.",
            "So once again you have an estimator that is as good as one you can get from the data.",
            "You have an.",
            "This minimax error is again natural because for an M by N matrix in Frank are there are roughly R times.",
            "I'm free parameters which is what is determining the statistical error in this case."
        ],
        [
            "So the and once again we we can verify these these predictions in simulations.",
            "So if you take the case of a exactly low rank matrix, you again observe linear rate of convergence for different choices sample size.",
            "And you can once again relax the exact low rank assumption you can impose only an approximately low rank and the results will still go through both in theory and experiments, just like they did for the case of sparsity.",
            "So."
        ],
        [
            "To wrap up what this work establishes is the global linear convergence of gradient descent under restricted strong convexity and smoothness assumptions, rather than assuming the global analogs of those which do not hold in high dimensional settings.",
            "But these restricted analogs, we can show that they hold with high probability for several models of interest, even in the high dimensional setting.",
            "As a result, what we get are, to the best of our knowledge, first global linear convergence results for high dimensional.",
            "Russian matrix completion and a variety of other problems, some of which I'll visit in the next slide, an.",
            "A key feature of our results is that this convergence only happens up to the underlying statistical precision of the model, not to an arbitrary numerical tolerance.",
            "So this outlines this very nice interplay between optimization and statistics, so the same properties that were that have been helping people all these years in showing very nice statistical convergence rates of the of the high dimensional estimators can also be exploited in the optimization procedures too.",
            "Actually establish faster rates for optimization methods."
        ],
        [
            "Now, as I mentioned, these results can be extended in a variety of ways, so the results pretty much extend to the decomposable regularizer setup introduced by Negahban at all in NIPS last year.",
            "In particular, this gives us linear convergence results for group sparsity, with nonoverlapping groups for the multi task learning problem for generalized linear models.",
            "In particular, the last one gives us the case of sparse logistic regression, for instance, and here I analyzed only a constraint version of the problem, so I was explicitly limiting the regularizer to be bounded at a certain level row.",
            "But these results can also be similarly extended for a lagrangean version where you put the regularizer in the objective function instead of putting it as a constraint.",
            "So that."
        ],
        [
            "Well, I had to say thanks a lot.",
            "It's a question.",
            "OK, so have you tried to invert your formula to obtain an upper bound on the number of iterations to reach statistical precision?",
            "Sure that that would be pretty straightforward.",
            "I mean it will be just log of one over the contraction factor, right?",
            "If you analyze the results or not.",
            "Have you analyzed?",
            "But it gives out, so I mean, sorry, I don't quite follow the question because it's just log of one over this contraction factor.",
            "So what exactly does it mean?",
            "It is a constant number of iterations given that.",
            "OK or so.",
            "Oh I, I see what you're saying.",
            "Um?",
            "So again, can you say in advance how long it's going to do right?",
            "So so so one thing that one thing, for instance, that's not fully reflected here, because here I'm presenting really a simplified version of the results is that you can.",
            "You can drive sharper results where the contraction factor depends on the sample size, and then you really start to see how the number of iterations when you invert the bound and has the number of iterations to get to an epsilon.",
            "Accuracy will depend on the sample size, so that's going to be something that will be part of the full version that will be put up soon.",
            "Not.",
            "It's very nice and winning since so definitely the right thing to do is, he says only to optimize up to the statistical error, but if that's what you want to do, then why not use something like stochastic mirror descent?",
            "In this case will also basically right, so that's a good question.",
            "So there are a couple of reasons for not doing that.",
            "So first of all, if you use.",
            "OK, so if you use the mirror descent with just L2 norm here then that doesn't work because the gradient will.",
            "The gradient is too big if you use the like LP squared with pretty close to one then the convergence then you do not end up exploiting this sort of strong convexity and the convergence rate that you get ends up being like one over epsilon squared as a function of epsilon.",
            "Don't believe that's true, so you will get one over epsilon, one over epsilon squared.",
            "OK, so I'm not familiar with how to prove that.",
            "I'd be happy to chat with you offline.",
            "Adding smoothness enough is enough for that I mean.",
            "Actually, you can come here posters tonight, but it's beyond that.",
            "I mean one over you do get one over epsilon, just with smoothness.",
            "You don't need the strong convexity.",
            "But even the smoothness.",
            "That mismatch of norms here still because this smoothness is only in L2.",
            "So you'll need smoothness in the dual norm, which I am not sure is going to be will translate properly or not so.",
            "OK. Let's thank the speaker again.",
            "Well spotlights."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we got a great talk just now about how we don't do enough statistics.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to try and show off some of the statistics that we can actually do, and in particular this will be in the high dimensional statistical setting where we will try to prove some convergence properties for gradient methods.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the setting here is in.",
                    "label": 0
                },
                {
                    "sent": "High dimensional statistical recovery where we have a parameter space Omega which is contained in other D and each of these parameters permit arises, that is probability distribution and the data that we observe is drawn from some unknown parameter Theta star an in the high dimensional setting, it's typically assumed that the number of parameters is much much larger than the number of samples.",
                    "label": 0
                },
                {
                    "sent": "A typical estimator of interest in these settings is the regular regularised empirical risk minimizer, which minimizes the loss function subject to the regularizer being constrained at some level row.",
                    "label": 0
                },
                {
                    "sent": "Now, for these estimators, what people have shown statistically is that even in this regime, where number of samples is much much smaller than the number of parameters Theta hat can often be consistent in the estimation of the true parameter Theta star under certain regularity assumptions.",
                    "label": 0
                },
                {
                    "sent": "However, for this to be a meaningful procedure, you have to be able to solve the computational problem to obtain three ahead as well from the data an.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It happens often in high dimensional setting says that the optimization procedures to actually compute it ahead can often be quite slow.",
                    "label": 0
                },
                {
                    "sent": "In practice an I will be a bit more concrete about why this happens in the later part of the talk, but the question that we want to ask in the stock.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the optimization procedures for computing Theta hack benefit from the same assumptions that give us so much benefit in the statistical analysis an the answer, of course is going to be in affirmative for a variety of statistical models.",
                    "label": 0
                },
                {
                    "sent": "We can actually show that we do gain from such assumptions so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Immigration procedure that I'm going to consider in this talk is very simple.",
                    "label": 0
                },
                {
                    "sent": "Classical gradient descent procedure.",
                    "label": 0
                },
                {
                    "sent": "So we start from some arbitrary initial point and at every step we take our current iterate Theta T. We take the we take the gradient at the iterate and take a step in the negative gradient direction.",
                    "label": 0
                },
                {
                    "sent": "This will usually take us outside the constraints set.",
                    "label": 0
                },
                {
                    "sent": "So we project down under the constraint set to obtain our next.",
                    "label": 0
                },
                {
                    "sent": "Iterate 30 + 1.",
                    "label": 0
                },
                {
                    "sent": "Here I'm using a constant step size, you which depends on the smoothness properties of the last function.",
                    "label": 1
                },
                {
                    "sent": "So this is a classical algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's convergence properties are well understood.",
                    "label": 0
                },
                {
                    "sent": "In particular, it's known that when the loss function is smooth, then this procedure converges at a sub linear rate and when the loss function is both smooth and strongly convex then this procedure converges at a linear rate.",
                    "label": 1
                },
                {
                    "sent": "Now to understand this algorithm and how it works in the high dimensional settings, I'm going to take a particular statistical model of interest.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is the sparse linear regression model an.",
                    "label": 1
                },
                {
                    "sent": "In this model we observe the data vectors Zaizen outta the D and the regression outputs Y eyes which are generated by taking dot product between Zion some unknown parameter Theta star and corrupting it with zero mean noise now.",
                    "label": 1
                },
                {
                    "sent": "Of course, Wendy is much, much larger than end and in general recovery of data start from samples is hard, is impossible, and so we need some further assumptions and a typical assumption is that data star has at most S non zero entries.",
                    "label": 0
                },
                {
                    "sent": "An in such a situation of popular estimator is the last two estimator where the loss function is the squared loss between the observations and predictions and we impose an L1 norm regularizer at a certain radius role which enforces sparsity on the optimum.",
                    "label": 0
                },
                {
                    "sent": "So now if you take the projected gradient descent algorithm from the previous slide and apply it to the sparse linear regression problem, then this is what you.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Serve so here I show you on a log scale the distance between 30 and Terra Hat is a function of number of iterations for a variety of sample sizes on the sparse linear regression problem and the thing to note here is that we're getting a linear convergence in the log rhythmic scale.",
                    "label": 0
                },
                {
                    "sent": "So so if you recall.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the previous slides, the linear convergence for projected gradient descent happens and the loss function is both smooth and strongly convex, so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But this one empirical result and this happens in a typical sense, so you can do it several times and you observe the same phenomenon.",
                    "label": 0
                },
                {
                    "sent": "What it tells us is that there must be some smoothness and strong convexity properties of the loss function in sparse linear regression problem that the optimization procedure must be exploiting.",
                    "label": 1
                },
                {
                    "sent": "But if you look.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The current theory that tries to explain the optimization for this problem, it sort of falls short of explaining this linear convergence shown here.",
                    "label": 0
                },
                {
                    "sent": "So something earliest work in this problem.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Us to apply Nesterov's methods in this problem, and they assume the smoothness of the loss function for the sparse linear regression problem to obtain sub linear rates of convergence, which would be much much slower than the linear rates that we are observing here.",
                    "label": 0
                },
                {
                    "sent": "Some works tried to prove linear convergence, but they could either prove local linear convergence which would only apply to the last few traits, or they proved a linear convergence up to the variance of the problem, so it would only apply to the initial few traits to a very rough precision, so neither of the results would explain the global linear convergence shown here.",
                    "label": 1
                },
                {
                    "sent": "Bradys and Lawrence did actually establish global linear convergence by assuming that the data specifies a certain restricted isometry property, and they also assume smoothness of the loss function.",
                    "label": 0
                },
                {
                    "sent": "So, so in a nutshell, it seems like the smoothness and curvature of the loss function, they seem to be sort of the key properties in determining the performance of gradient descent algorithm on these problems.",
                    "label": 0
                },
                {
                    "sent": "So let's drill a bit deeper into what these properties are like for the sparse linear regression set up in the high dimensional problems.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you take the sparse linear regression loss function and further assume that the data is generated from a very simple standard normal model, then what happens is that in high dimensions you do not have any smoothness and you do not have any curvature.",
                    "label": 1
                },
                {
                    "sent": "So if you take the largest eigenvalue of the Hessian matrix, which determines the smoothness of the loss function, this can be as big as square root of the overan with high probability, and Wendy is much much larger than N. This quantity can be very large.",
                    "label": 0
                },
                {
                    "sent": "At the same time, there are the Hessian matrix is at most rank N when you have only one data points, so there are at least D -- N directions in which you have no curvature whatsoever.",
                    "label": 0
                },
                {
                    "sent": "So now you have a very very ill conditioned problem where the Hessian has no curvature in some directions and is extremely spiky in certain directions, and you do not expect to see the nice global linear convergence for gradient method as I showed you a couple of slides before, so clearly there is there is some gap happening between what we see and.",
                    "label": 0
                },
                {
                    "sent": "What we can show theoretically here and the focus of this talk is going to be to try and bridge this gap so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The key observation that we make in this work is that even though you cannot establish global smoothness and strong convexity of the loss function in the high dimensional setting, if you take appropriately restricted notions of these assumptions then they can actually be established even when D is much much larger than N. So we first start with the strong convexity assumption.",
                    "label": 0
                },
                {
                    "sent": "So we say that a loss function satisfies restricted strong convexity if the curvature is lower bounded, but only with up to an additive tolerance, and only on the feasible set.",
                    "label": 1
                },
                {
                    "sent": "So this is very much like the standard strong convexity assumption, but with two notable exceptions, one being that it only assumes curvature up to this additive tolerance and 2nd being it's only on the set.",
                    "label": 1
                },
                {
                    "sent": "Now, I've sort of hopefully convinced you that global strong convexity will not hold.",
                    "label": 0
                },
                {
                    "sent": "In the high dimensional setting, but what?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turns out is that if you choose your optimization set appropriately and the tolerance appropriately, then you can actually establish for various problems of interest that restricted strong convexity will actually hold with high probability.",
                    "label": 0
                },
                {
                    "sent": "So that's the first ingredient and what this gives us in way of the analysis is that it allows us to say that if we take an iterative at T that is sufficiently far away from Fahrenheit.",
                    "label": 0
                },
                {
                    "sent": "So if the distance between 30 minus Theta had is much bigger than the additive tolerance here, then we will actually be able to lower bound.",
                    "label": 0
                },
                {
                    "sent": "Picture of our loss function at the iterate, so this ends up buying us some curvature on the iterates.",
                    "label": 0
                },
                {
                    "sent": "The second ingredient is a similar assumption about the smoothness, so I say that a loss function satisfies restricted smoothness if the curvature is upper bounded.",
                    "label": 0
                },
                {
                    "sent": "Again, modular and additive tolerance, and again only on the feasible set.",
                    "label": 0
                },
                {
                    "sent": "And once again we will see that by appropriate choice of the two quantities, we will be able to establish smoothness in a high dimensional setting, even though it does not hold in a global unrestricted setting.",
                    "label": 0
                },
                {
                    "sent": "Now, what these assumptions bias?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that under these assumptions I can actually establish linear convergence of my gradient descent method, So what I can show is that my interest Arity will converse with her ahead geometrically up to a precision of Delta squared, and the precision Delta squared here.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The exact same tolerance level that was in my assumptions, and this tolerance is exactly the sort.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of Delta squared in the result of this theorem.",
                    "label": 0
                },
                {
                    "sent": "The contraction factor here naturally depends on my upper and lower smoothness constants, as is natural for gradient algorithms, so so I do get a global linear convergence as my simulations seem to suggest, and this is this holds globally for all iterates, unlike some of the previous local linear convergence results, but the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think that might catch your attention is that this convergence only happens up to an accuracy Delta squared.",
                    "label": 0
                },
                {
                    "sent": "So if you're an optimization person, this might strike you as a little odd at first because we were used to seeing the convergence of 30 to 3 to head up to arbitrary numerical precision an this gives you only the convergence of finite precision.",
                    "label": 0
                },
                {
                    "sent": "So the reason why this is still a good result in this setting is because you have to recall I'm not trying to solve any arbitrary optimization problem here.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to solve an optimization problem that's coming.",
                    "label": 0
                },
                {
                    "sent": "Out of statistical estimation settings, so my perimetre of interest is not the optimization optimum Theta hat, but what I really want to recover is the true parameter Theta star that's generating the data now typically straight ahead and thermostat are different and in fact I can define the statistical precision of the problem to be the expected distance between Theta hat and theater star, right?",
                    "label": 0
                },
                {
                    "sent": "So now there are two possibilities.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One is that I can only guarantee you that my interacted at evil approached her ahead to a distance Delta, which is much much bigger than the statistical precision of the problem.",
                    "label": 0
                },
                {
                    "sent": "In this case, that is a terrible estimator for Theta star.",
                    "label": 0
                },
                {
                    "sent": "Be cause if you could solve the optimization problem well then.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Could get this straight ahead, which is close to the star, but you're only getting a Theta T which is far away from Theta star which is at least Delta distance away from Theta star, so it's a terrible estimator, but on the other hand, if I can guarantee that.",
                    "label": 0
                },
                {
                    "sent": "The distance between 30 and hit us head is on the same order as the distance between Kira had in theater star on the same order as a statistical precision.",
                    "label": 0
                },
                {
                    "sent": "Then therapy is at most twice the statistical precision away from Theta star, so it's.",
                    "label": 0
                },
                {
                    "sent": "Up to constant factors, it's as good an estimate or as you can get from your data really, and so the goal would be to actually establish these convergence results up to the statistical precision of the problem.",
                    "label": 0
                },
                {
                    "sent": "An as I had noted before, the source of this tolerance, Delta is the tolerance in the restricted strong convexity and smoothness assumptions, so we will need to establish those assumptions with up to a statistical precision tolerance, and this will be done by intelligent choice of the optimization set up, the regularization radius roll.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To illustrate the applications of these results, I'm going to start with a particular example of the sparse linear regression problem where now assume that my data is generated according to a multivariate Gaussian distribution with covariance matrix Sigma an.",
                    "label": 0
                },
                {
                    "sent": "I assume that my true parameter Theta star, is sparse now.",
                    "label": 0
                },
                {
                    "sent": "Recall that in this problem my last function was a squared loss an my.",
                    "label": 0
                },
                {
                    "sent": "In my regularizer was the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "In this case I said the regularization radius to be the L1 norm of terrastar and with this choice of the radius rho I can establish restricted strong convexity and smoothness with high probability up to a precision equal to S logged on North.",
                    "label": 0
                },
                {
                    "sent": "What this gives me is the convergence of Theta, deducted ahead at a geometric rate up to a precision S logged on earth, and for those of you who are familiar with the statistical analysis of this problem, you will recognize that this is actually the minimax statistical error on this problem.",
                    "label": 0
                },
                {
                    "sent": "So what my result is giving is that Katie will approach straight ahead to the smallest possible distance.",
                    "label": 0
                },
                {
                    "sent": "Statistically 30 will approach to start to the smallest possible distance statistically at a linear rate of convergence, and the rate of convergence here depends on the.",
                    "label": 0
                },
                {
                    "sent": "Population on the conditioning of the population covariance, which is natural because that determines the curvature properties of your loss function, and here I'm stating the result only for exact sparsity, but the results also extend to an approximate approximately sparse setting in a natural fashion so.",
                    "label": 0
                },
                {
                    "sent": "So, so that's that's a theoretical result for sparse linear regression, and we can verify that this linear convergence actually happens in simulations as well.",
                    "label": 0
                },
                {
                    "sent": "So this is revisiting the same slide I had before and.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let's look a bit more into it.",
                    "label": 0
                },
                {
                    "sent": "So here I assume that my parameter Theta star is exactly sparse and the population covariance from which the data is generated satisfies some sort of restricted isometry assumption an what I'm showing here is for a few different choices of sample size, the convergence of Theta, Theta hat and analog scale an it is indeed geometric, as the theory predicts and convergence rate slows down as a speeds up as the number of samples increases, which is natural because the conditioning of your.",
                    "label": 0
                },
                {
                    "sent": "Population covariance improves as you as you observe more and more samples.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, of course, these assumptions are a little restrictive, so as I said we can relax, start relaxing some of these.",
                    "label": 0
                },
                {
                    "sent": "We can go from exact sparsity to approximate sparsity.",
                    "label": 0
                },
                {
                    "sent": "We still observe a linear rate of convergence and the convergence rate of course slows down compared to exact sparsity, because this is a harder problem, the curvature is worse for these problems often.",
                    "label": 0
                },
                {
                    "sent": "You can also relax the restricted isometry assumption because.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I see an RSM are in Norway directly relying on our IP so you can relax the ripi assumption, take a very very heavily correlated design and you still get a linear rate of convergence.",
                    "label": 0
                },
                {
                    "sent": "But again, the curvature of the model degrades and you get a slower rate of convergence, but it is still linear, so that's the theoretical and experimental story we wanted to go over for sparse linear regression now.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to visit another problem which is that of low rank matrix completion.",
                    "label": 0
                },
                {
                    "sent": "So in The Lorax matrix completion problem, we have some true matrix terrastar that's that's not told to us.",
                    "label": 0
                },
                {
                    "sent": "What we observe is randomly sampled entries of this matrix corrupted with some zero mean noise.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "So for instance, is like most of you are familiar with the Netflix challenge by now, so that's sort of place.",
                    "label": 0
                },
                {
                    "sent": "This problem arises an in general when you observe only a few of a few of the entries of a matrix.",
                    "label": 0
                },
                {
                    "sent": "It's impossible to complete the rest of the matrix without further structural.",
                    "label": 0
                },
                {
                    "sent": "Assumptions so a common assumption in these problems is that the matrix Theta star has a rank at most R. An in this problem we take the loss function again to be the squared loss between the predictions and observations, and we take the regularizer to be the L1 norm on the singular values of matrix Theta, which is probably more familiar to most of you as the nuclear norm or the trace norm and regularization radius in this case is set to be the trace norm of the true matrix Theatre style, and with this setting we can show that once again our parameter Theta T converges at a geometric rate to Theta hat.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apple, a precision and this up to the log in factor is again equal to the minimax statistical precision of the problem.",
                    "label": 0
                },
                {
                    "sent": "So once again you have an estimator that is as good as one you can get from the data.",
                    "label": 0
                },
                {
                    "sent": "You have an.",
                    "label": 0
                },
                {
                    "sent": "This minimax error is again natural because for an M by N matrix in Frank are there are roughly R times.",
                    "label": 0
                },
                {
                    "sent": "I'm free parameters which is what is determining the statistical error in this case.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the and once again we we can verify these these predictions in simulations.",
                    "label": 0
                },
                {
                    "sent": "So if you take the case of a exactly low rank matrix, you again observe linear rate of convergence for different choices sample size.",
                    "label": 0
                },
                {
                    "sent": "And you can once again relax the exact low rank assumption you can impose only an approximately low rank and the results will still go through both in theory and experiments, just like they did for the case of sparsity.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To wrap up what this work establishes is the global linear convergence of gradient descent under restricted strong convexity and smoothness assumptions, rather than assuming the global analogs of those which do not hold in high dimensional settings.",
                    "label": 1
                },
                {
                    "sent": "But these restricted analogs, we can show that they hold with high probability for several models of interest, even in the high dimensional setting.",
                    "label": 1
                },
                {
                    "sent": "As a result, what we get are, to the best of our knowledge, first global linear convergence results for high dimensional.",
                    "label": 0
                },
                {
                    "sent": "Russian matrix completion and a variety of other problems, some of which I'll visit in the next slide, an.",
                    "label": 1
                },
                {
                    "sent": "A key feature of our results is that this convergence only happens up to the underlying statistical precision of the model, not to an arbitrary numerical tolerance.",
                    "label": 0
                },
                {
                    "sent": "So this outlines this very nice interplay between optimization and statistics, so the same properties that were that have been helping people all these years in showing very nice statistical convergence rates of the of the high dimensional estimators can also be exploited in the optimization procedures too.",
                    "label": 0
                },
                {
                    "sent": "Actually establish faster rates for optimization methods.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, as I mentioned, these results can be extended in a variety of ways, so the results pretty much extend to the decomposable regularizer setup introduced by Negahban at all in NIPS last year.",
                    "label": 0
                },
                {
                    "sent": "In particular, this gives us linear convergence results for group sparsity, with nonoverlapping groups for the multi task learning problem for generalized linear models.",
                    "label": 1
                },
                {
                    "sent": "In particular, the last one gives us the case of sparse logistic regression, for instance, and here I analyzed only a constraint version of the problem, so I was explicitly limiting the regularizer to be bounded at a certain level row.",
                    "label": 0
                },
                {
                    "sent": "But these results can also be similarly extended for a lagrangean version where you put the regularizer in the objective function instead of putting it as a constraint.",
                    "label": 0
                },
                {
                    "sent": "So that.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, I had to say thanks a lot.",
                    "label": 0
                },
                {
                    "sent": "It's a question.",
                    "label": 0
                },
                {
                    "sent": "OK, so have you tried to invert your formula to obtain an upper bound on the number of iterations to reach statistical precision?",
                    "label": 0
                },
                {
                    "sent": "Sure that that would be pretty straightforward.",
                    "label": 0
                },
                {
                    "sent": "I mean it will be just log of one over the contraction factor, right?",
                    "label": 0
                },
                {
                    "sent": "If you analyze the results or not.",
                    "label": 0
                },
                {
                    "sent": "Have you analyzed?",
                    "label": 0
                },
                {
                    "sent": "But it gives out, so I mean, sorry, I don't quite follow the question because it's just log of one over this contraction factor.",
                    "label": 0
                },
                {
                    "sent": "So what exactly does it mean?",
                    "label": 0
                },
                {
                    "sent": "It is a constant number of iterations given that.",
                    "label": 0
                },
                {
                    "sent": "OK or so.",
                    "label": 0
                },
                {
                    "sent": "Oh I, I see what you're saying.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So again, can you say in advance how long it's going to do right?",
                    "label": 0
                },
                {
                    "sent": "So so so one thing that one thing, for instance, that's not fully reflected here, because here I'm presenting really a simplified version of the results is that you can.",
                    "label": 0
                },
                {
                    "sent": "You can drive sharper results where the contraction factor depends on the sample size, and then you really start to see how the number of iterations when you invert the bound and has the number of iterations to get to an epsilon.",
                    "label": 0
                },
                {
                    "sent": "Accuracy will depend on the sample size, so that's going to be something that will be part of the full version that will be put up soon.",
                    "label": 0
                },
                {
                    "sent": "Not.",
                    "label": 0
                },
                {
                    "sent": "It's very nice and winning since so definitely the right thing to do is, he says only to optimize up to the statistical error, but if that's what you want to do, then why not use something like stochastic mirror descent?",
                    "label": 0
                },
                {
                    "sent": "In this case will also basically right, so that's a good question.",
                    "label": 0
                },
                {
                    "sent": "So there are a couple of reasons for not doing that.",
                    "label": 0
                },
                {
                    "sent": "So first of all, if you use.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you use the mirror descent with just L2 norm here then that doesn't work because the gradient will.",
                    "label": 0
                },
                {
                    "sent": "The gradient is too big if you use the like LP squared with pretty close to one then the convergence then you do not end up exploiting this sort of strong convexity and the convergence rate that you get ends up being like one over epsilon squared as a function of epsilon.",
                    "label": 0
                },
                {
                    "sent": "Don't believe that's true, so you will get one over epsilon, one over epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm not familiar with how to prove that.",
                    "label": 0
                },
                {
                    "sent": "I'd be happy to chat with you offline.",
                    "label": 0
                },
                {
                    "sent": "Adding smoothness enough is enough for that I mean.",
                    "label": 0
                },
                {
                    "sent": "Actually, you can come here posters tonight, but it's beyond that.",
                    "label": 0
                },
                {
                    "sent": "I mean one over you do get one over epsilon, just with smoothness.",
                    "label": 0
                },
                {
                    "sent": "You don't need the strong convexity.",
                    "label": 0
                },
                {
                    "sent": "But even the smoothness.",
                    "label": 0
                },
                {
                    "sent": "That mismatch of norms here still because this smoothness is only in L2.",
                    "label": 0
                },
                {
                    "sent": "So you'll need smoothness in the dual norm, which I am not sure is going to be will translate properly or not so.",
                    "label": 0
                },
                {
                    "sent": "OK. Let's thank the speaker again.",
                    "label": 0
                },
                {
                    "sent": "Well spotlights.",
                    "label": 0
                }
            ]
        }
    }
}