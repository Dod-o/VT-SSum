{
    "id": "g5b3vzmgho5l54brxbfojhs32mzhhd54",
    "title": "Introduction to Bayesian Nonparametrics",
    "info": {
        "author": [
            "Peter Orbanz, Institute of Computational Science, ETH Zurich"
        ],
        "published": "Jan. 15, 2013",
        "recorded": "April 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/mlss2012_orbanz_nanoparametrics/",
    "segmentation": [
        [
            "So here's my rough plan of what I'm going to talk about today and tomorrow.",
            "Today I'm going to talk about.",
            "Basic terminology, and then I will spend probably most of today's lecture talking about clustering.",
            "So about mixture models which Reuben has mentioned earlier.",
            "And then I will talk a little bit about latent feature models.",
            "And tomorrow I'm going to.",
            "I'm going to speak on some bit more advanced things, so essentially what I'm going to do is I'm going to talk about how we can construct new based on parametric models.",
            "And I would try to use it as a vehicle to present some of the work that has been done in that field.",
            "In connection with that, I'm going to talk about something that I'm also briefly going to discuss today, which is exchange ability, and if we have time tomorrow, then I'm going to talk about asymptotic.",
            "So on how you can theoretically evaluate how well your models perform.",
            "But today I'm mostly going to talk about clusterings about Derek Lee processes and models related to that.",
            "Good."
        ],
        [
            "So.",
            "I gave a tutorial with you by TE at nips on Bayesian non parametrics and if any of you have seen that tutorial the first few slides I think the 1st three or four slides are the same but after that I'm not just going to repeat the tutorial.",
            "OK, so here's a very simple.",
            "A very simple description of what we're trying to do in statistics, a kind of cartoon picture of statistics, and that is we have we have a statistical model which is a probability distribution of some observation variable X, which describes our data.",
            "And that given a parameter Theta OK and that I'm writing the fact that I'm writing that as a conditional probability here is already in a way, a Bayesian assumption, because I'm saying I'm conditioning on a random variable here, so the parameter is a random variable and that is exactly the character characterize the Bayesian model.",
            "And So what I mean by this year is probability of data given some parameter and the way I would like to think about a parameter here is to think about some kind of pattern.",
            "That underlies the data, and here is an example, so this is a simple linear regression example where the blue dots here are the data.",
            "And the red line.",
            "This linear trend here is a pattern.",
            "So in this case the pattern would be a linear function.",
            "And here is a slightly more sophisticated example, which is again a regression example, and again the pattern is a function, but in this case it's not linear anymore and this is the kind of thing that we can do with the Gaussian process.",
            "For example in basic nonparametrics.",
            "OK, so if we.",
            "If we look at this picture here.",
            "What we're trying to do is we're trying, so we're making an assumption here, usually in Bayesian statistics or generally in statistics, which is some kind of independence assumption, and basically what we're assuming is that there is this underlying pattern.",
            "This is this linear trend here and then the data that we observe is.",
            "Basically this pattern plus some randomness.",
            "Yeah, so the points are not exactly on the red line, they scatter around the line.",
            "That is some kind of randomness, but we assume that this this random deviations from the line happen independently of each other for different points, so that the assumption we're making here the idea behind inference is that the data is underlying pattern plus some independent noise, and independent is is really essential here.",
            "I would ask you to read this plus in a purely qualitative fashion.",
            "Yeah, so the noise could be multiplicative or something.",
            "I'm just trying to say this is pattern and noise OK. Alright, and so in.",
            "If we're being Bayesian, then we assume that this pattern here is a random quantity that is the basic characteristic of a Bayesian model, and the distribution of that pattern is the prior the prior distribution.",
            "What we try to compute is when we have seen data, we try to compute the posterior, which is the distribution of this pattern, given that the data looks the way it looks, OK."
        ],
        [
            "Alright, so.",
            "Now what is a nonparametric Bayesian model in a parametric model?",
            "Statistics of parametric model is a model which, first of all which has parameters of course.",
            "But it has some fine print and the fine print is the number of parameters is fixed.",
            "Or the number is at least constantly bounded with respect to sample size.",
            "So as we see more and more data, the number of parameters does not increase.",
            "Yeah, there's just a fix.",
            "Number of parameters.",
            "And the nonparametric model is.",
            "Is a model which also uses parameters and so it's really a misnomer.",
            "But it's a model which does not match the fine print.",
            "Yeah, so a nonparametric model is where the number of parameters grows with the size of the sample.",
            "And if you think of the parameters, if you think of the number of parameters as the degrees of freedom that the model has, then this means that the model can become more and more complicated as we see more and more data OK. OK, so if we want to.",
            "In Bayesian modeling, what we do is we treat the parameter as a random quantity, and we described by a probability distribution right?",
            "So we have to take the parameter space and we have to put a probability distribution on that and we call that the prior.",
            "Now if we want to do that here where the number of parameters is not bounded.",
            "Then, as we see infinitely many data points, we can't.",
            "We can't put a maximum dimension on the size of the parameter, and so we have to use an infinite dimensional space.",
            "OK, so a nonparametric model and this is not only true in Bayesian modeling, pretty much in general.",
            "One way to define nonparametric models is to say that is a statistical model which has an infinite dimensional parameter space.",
            "So here is an example.",
            "Simple density estimation example and this is not a Bayesian example.",
            "This is just standard parametric versus nonparametric statistics.",
            "So in both cases we're going to estimate a density.",
            "Using a Gaussian distribution, but we're going to use the Gaussian distribution in two different ways.",
            "So here we have a bunch of data points and we're just doing making a parametric assumptions and say OK, the data is generated by about 2 dimensional Gaussian distribution.",
            "So we fit in a Gaussian, say with maximum likelihood.",
            "We get this get this picture here and you see if there's more data coming in that may change.",
            "That may change our estimate of the parameter, but it does not change the dimensionality of the parameter, right?",
            "It's just two dimensional location parameter an two by two covariance matrix.",
            "OK, and here this is a window of a person window type estimator.",
            "So what we're doing here is each time we see a data point, we fix a Gaussian with a fixed code.",
            "We take a Gaussian with fixed covariance matrix.",
            "And put this Gaussian at the location of the data point.",
            "So we're setting the mean parameter of the Gaussian to the location of the data point.",
            "And we do that for every data point and then be normalize.",
            "We add up these densities and normalize.",
            "OK.",
            "So basically what we're doing is we're using.",
            "You can think of this as smoothing.",
            "We're doing a convolution with the Gaussian kernel and that way we smooth the data.",
            "The data is a bunch of spikes, sharp spikes and we smooth that was a Gaussian OK, and the density estimate we get that way is called a person window estimator.",
            "Alright, so you can see that in this case we get one additional location parameter for each data point that we see and the number of data points increases as the number of data points increases.",
            "The dimension of the parameter space increases.",
            "So to accommodate this unbounded number of parameters, unbounded number of parameter dimensions, we need an infinite dimensional space.",
            "No.",
            "Alright, and now if you want to be Bayesian about it, if we want to put a prior distribution on that."
        ],
        [
            "Then what we have to do is we have to put a prior distribution, infinite dimensional space and that is a nonparametric Bayesian model.",
            "So the definition that we're going to use the over nonparametric Bayesian model is a nonparametric Bayesian model.",
            "Is a Bayesian model on an infinite dimensional parameter space.",
            "OK. Is that clear so far?",
            "OK, and the interpretation the interpretation here is that the parameter space or one useful way, I think to interpret it is the parameter space is the set of possible patterns that we we assume can explain the data.",
            "So in the density estimation example that we just saw the set of possible patterns would be a set of probability distributions, right?",
            "Or a set of densities more precisely.",
            "Because we're doing density estimation, if we're doing regression, so the example on the 1st slide, then the pattern would be a smooth function.",
            "And if we do clustering, which we're going to talk about most of the time today, then this pattern here that explains the data is a partition partition of the data into separate clusters.",
            "OK, so the solution we are looking for is not an individual pattern.",
            "But a distribution over these patterns, namely the posterior distribution.",
            "So we start with a prior guest on how these patterns are distributed.",
            "We add in what the data look looks like, and then we compute the conditional distribution of the parameter given the data of this pattern given the data and say OK, this pattern is more probable in this one is less probable that is a solution that we would ideally like to get."
        ],
        [
            "OK, and now I have to say a few words about exchange ability.",
            "I'm going to get back to this tomorrow, but.",
            "We will already.",
            "It will already come up several times today and so that.",
            "The question we ask here is we made this.",
            "We are making this assumption that the data is in some sense pattern plus noise.",
            "And this is really crucial to what we're doing for different several different reasons.",
            "One reason is that if.",
            "This this noise part here?",
            "If this is correlated, then it's really hard to do inference.",
            "Yeah, it's really hard to estimate what this pattern is.",
            "But there is another, like a bit more fundamental reason, basically which is.",
            "What we really want to do here is we want we say there is something that the data have.",
            "The data points have in common.",
            "And then there is something that is just randomness and we would like to extract the common part because that is useful information, right?",
            "And then we would like to throw away the rest and that is exactly this assumption here.",
            "This is what the data points share and this is the rest that we would like to throw away.",
            "So what we have to ask is in which case is this assumption here justifiable?",
            "And it turns out that there is a very sharp character and precise sharp characterization of when we can justify this assumption.",
            "In base theorem which you have seen in Zubin's lecture earlier.",
            "This is encoded in the Bayes equation in this factorial here, so this year is the basic question.",
            "Here is the prior distribution the parameter then?",
            "Up here the P of XJ given Theta is the likelihood of datapoint number J given the parameter.",
            "This here is the evidence term, and then we have the posterior distribution of the parameter given the data.",
            "And you can see that here the.",
            "The likelihood overall data points is a product.",
            "And this product says product over distributions always expresses independence, right?",
            "So these data points are independent of each other.",
            "But given the parameter value.",
            "Yeah, and this is called conditional independence.",
            "So conditional independence in general and probabilities simply means we have a bunch of random variables.",
            "In this case the random variables X and they are conditionally independent given Theta if they are independent, provided that we know the value of Cedar and the intuition behind that is these random variables contain information about each other.",
            "But all of that information is summarized in theater.",
            "So if we know Theta, then knowing some of the random variables won't tell us anything about the others.",
            "That is conditional independence.",
            "So this year is a conditional independence assumption, and that is exactly what we have here.",
            "We basically.",
            "So we can rephrase this question and ask under what conditions can be make such a conditional independence assumption for for data.",
            "And if we look back at the.",
            "At the picture on the 1st Flight with and then we also see this condition independence assumption in there, which is in this case the red line is a theda right?",
            "And then the data points scatter around that in dependently.",
            "So if we know the location of the red line then no data point will tell us anything about what the where the other data points are.",
            "If we don't know the location of the red line and we know part of these data points now, where is the next data point going to be?",
            "We can tell that it's much more likely to be in this region than somewhere over here, so conditionally.",
            "Unconditionally they are not independent.",
            "OK, so another definition that we need, or the characterization that we need in order to.",
            "To give a criterion for this condition, independence is something quality, exchange ability and exchangeability is following.",
            "We see, and we assume that we see an infinite sequence of random variables.",
            "Yeah, that is data.",
            "So that is an infinite sequence of such blue dots here for example.",
            "And now we have it.",
            "We look at the joint distribution of these of these data.",
            "And now we ask what happens if we permute the order in which the state of these data points come in.",
            "And the distribution is called exchangeable, or also the sequences quality exchangeable if the probability of seeing a particular sequence of data points does not depend on this order, so we can permute them.",
            "And it will not change the probability of occurrence.",
            "So very simply, the order of the observations does not matter.",
            "This is a very nice.",
            "It's a very nice assumption for several reasons.",
            "One is, it's certainly true of the data is IID, right?",
            "So the data is IID.",
            "We can shuffle it around, and so, but it doesn't have to be ID, and so it's more.",
            "In a sense, it's a more more.",
            "It contains the assumption that we usually make in parametric sore.",
            "In nonparametric statistics, the IID assumption is special case, so that's a good start.",
            "In a way, it's actually the same thing as the idea assumption, But let's not get into that discussion.",
            "Another thing is that it's it's one thing is it's often true.",
            "Yeah, the other thing is, it's also relatively easy to argue.",
            "Whether this is true or not, yeah.",
            "If you know something about how your data is being generated, then it might be very hard to argue that the data is IID.",
            "But it's it's often it's reasonably easy to say.",
            "OK, the order in which the data come in.",
            "Contains information or it does not.",
            "Yeah, just if you know if you know that the generative process.",
            "OK, so this is exchangeability."
        ],
        [
            "And now there is this theorem called Definitely theorem which is.",
            "Really, one of the very basic theorems underlying Bayesian inference.",
            "And this tells us, in short, it tells us the data is conditionally ID if it's exchangeable.",
            "If anyone?",
            "If so, we can check whether this conditional IID assumption, whether that makes sense by checking whether the order in which the data points come in matters or not.",
            "And let's look at this in a bit more detail.",
            "So what we see over here is again, the joint distribution of the data points.",
            "And what we have here, on the other side is.",
            "Um?",
            "An integral over the parameter and I have now written the parameter here as a probability distribution.",
            "Yeah, the parameter is now a probability distribution.",
            "And this year is again the prior distribution.",
            "And now the prior distribution is a distribution over probability distributions.",
            "So we sample a random probability distribution from this Q.",
            "Then we fix it and we sample.",
            "These points from that distribution that we have just generated at random.",
            "And how do we sample them?",
            "IID, because we have this product here, so products IID.",
            "OK, So what this here says is.",
            "A distribution is exchangeable.",
            "If and only if.",
            "We can generate it in this way.",
            "We put it to some distribution on probability distributions we draw from that we draw a probability distribution at random, fix it.",
            "And then sample data from it IID OK. And this here is exactly if you will.",
            "This is exactly the term that we have seen here."
        ],
        [
            "In the base equation.",
            "Takes a prior.",
            "Sample parameter from it and then we get a product over the data points OK."
        ],
        [
            "OK, So what this theorem says is.",
            "If and only if the data is exchangeable, this whole this whole idea data is parent plus noise makes sense and the pattern in general is expressed by a probability distribution.",
            "And if we do something like saying on the previous slide here."
        ],
        [
            "Our pattern is really a straight line.",
            "This is not a probability distribution then that would mean that we choose a set of probability distributions that are parameterized by straight lines OK."
        ],
        [
            "Good, but the caveat here is.",
            "What this does not mean is.",
            "You start with some.",
            "Bayesian model, where your likelihood to say a Gaussian.",
            "And now you say OK, my data is exchangeable, and therefore this assumption makes sense, because in general.",
            "This year this district probability distribution.",
            "Here is an arbitrary probability distribution, and that means a set in which these probability distributions live is an infinite dimensional set.",
            "The set of all probability distributions on a given space is an infinite dimensional set.",
            "So in general we have to put a prior on infinite dimensional space here and from from an abstract perspective, this is a key motivation for for being nonparametric Bayesian models.",
            "If you want to really, if you want to really exploit the statement of this theorem.",
            "If you want to really use that, then you have to be non parametric.",
            "OK you have to use infinite dimensional parameters.",
            "OK."
        ],
        [
            "Good so.",
            "Now."
        ],
        [
            "Clustering.",
            "And has already mentioned clustering earlier.",
            "Basically, the very simple thing this is just a cartoon cartoon picture of clustering.",
            "Yeah so.",
            "This data was of course generated by sampling from a Gaussian mixture model.",
            "If anybody, yeah, can I just ask who has seen who has not seen a finite mixture model before?",
            "OK, very good, thank you so.",
            "We're going to assume that we have observations X one X2 and so on.",
            "And the modeling assumption we make in clustering is.",
            "The data is subdivided into groups.",
            "It's partitioned into groups and that means each data point belongs to exactly 1 cluster.",
            "And the in this case, the unknown pattern that I was talking off before, so I see the if you will.",
            "The unknown pattern is a partition of the data.",
            "Into disjoint groups, yeah, and if you now think of if you think of this as an infinite sequence here, then it has indices 1234 and so on, and we can partition it by partitioning the Indy says, yeah, So what we're looking for in that case is a partition of North, or if we only see a finite number of data points one through N, then we look at it for a partition of this finite set.",
            "And here on the site I have a very simple application of clustering.",
            "So what I did here is I just took a very simple image.",
            "It's basically night Sky with some trees down here.",
            "So we see a Sky, moon and some trees.",
            "And now we want to segment this image.",
            "Using clustering and the way this is being done here is we take a small sliding window, pass it over the image, we slide it over the image and at each location where the window is positioned, we extract the pixel values within that window and write them into a histogram.",
            "So we get a histogram associated which was each location in the window of the window.",
            "And that gives us a mapping from this image into a set of histograms into a space of vectors of histograms.",
            "Basically, and then we're going to apply clustering on this histogram space.",
            "Yeah, and so we get clusters.",
            "The histograms get sorted into clusters and then what I have done here is I've taken the first cluster of histograms and.",
            "Colored each location where one of these histograms was positioned in blue.",
            "Right then the other one in green and in red, so that induces basically clusters in feature space correspond to segments in the image that is very simple.",
            "Application of.",
            "Of a clustering method and what we see here is so Zubin talked about number of clusters earlier and about model selection in this case here.",
            "Here we have three clusters and here we have two clusters.",
            "Now image segmentation is Anil Post problem and actually people if you let people segment images and they totally disagree on how many segments there should usually should be in an image.",
            "I think in this case here pretty much everybody would agree that it should probably be 3 segments or that at least the moon shouldn't be locked in with the trees.",
            "So this is probably.",
            "Not as good a solution as this one."
        ],
        [
            "OK, so the way we're going to think about clustering.",
            "Is in terms of mixture models.",
            "And mixture model in general looks like this.",
            "So this year is in in.",
            "Our mix, our distribution, the distribution given by the mixture model.",
            "Yeah, so this is a distribution of data given some M OK the M is something called the mixing measure.",
            "So this is our mixing distribution.",
            "Yeah, so this is a measure against which we integrate some component.",
            "Think of this for example as a Gaussian distribution.",
            "The P of X given Theater is a Gaussian distribution with parameter Theta and then we have a distribution over theaters here.",
            "And there's various things that you can do with that.",
            "For example, if you set this year to a gamma.",
            "And theater is a scale parameter of the Gaussian.",
            "Then you get a student distribution.",
            "But the way we're going to use this in clustering is we're always going to look at at a specific form of mixture where this year is a distribution of finitely many, or sometimes infinitely many Delta spikes, and.",
            "What mixture models in general always express?",
            "And what I think everybody should always keep in mind when they look at mixtures is they expressed two stage sampling.",
            "Yeah, So what this mixture model expresses is?",
            "We sample X from this model here by first sampling theater from this distribution.",
            "Then we plug it into into this distribution and sample X from that.",
            "OK.",
            "So mixture models are two stage sampling.",
            "And now a finite mixture model in this formulation looks like this.",
            "That is again simply a mixture model.",
            "But we have a specific form of this mixing measure and that is it's a sum over Delta spikes at K possible locations.",
            "Yeah, so this is a finite number K. Here we have K different values.",
            "Theater on some parameter space.",
            "And replace the Delta spike at each of them and then we wait them with convex coefficients.",
            "This is a convex combination of Delta spikes.",
            "Convex combination means these C values here are non negative and they sum to one OK so that means this is a probability distribution and we sample when we sample from this distribution that possible values that we can get are these leaders.",
            "And they occur with probability.",
            "So thi decay occurs with probability CK, OK?",
            "So this is a finite mixture model."
        ],
        [
            "Now we can be Bayesian about this and introduce something called a Bayesian mixture model.",
            "And the way we do this is.",
            "The idea is very simple.",
            "If we if we."
        ],
        [
            "Back here and we see we define a finite mixture model in this way.",
            "But what do we have to choose where we have to choose this component distribution here?",
            "And we say, OK, let's say Gaussian.",
            "OK for Gaussian mixture model and now we have parameters and the parameters are the mix of mixing coefficients and the locations or the parameter values of the individual Gaussian components and that means all the parameters of this model are summarized in this mixing distribution OK?",
            "The mixture model is parameterized by the mixing this."
        ],
        [
            "So what we have to do is we have to generate the mixing distribution at random if we want to be based on the parameter is mixing distribution, we have to put a prior on that.",
            "And so the object we want to get is something like this.",
            "And now I have capitalized the season the theaters because they are now random variables we want to generate that at random, and Similarly, I have capitalized the.",
            "We want to generalize the generator random mixing measure and the idea in the Bayesian mixture model, so the basic mixture model is something that was introduced in the literature in the 1990s, and specifically the idea there is use a conjugate prior on each parameter.",
            "Zubin has mentioned conjugate priors earlier.",
            "Let's just quickly repeat the definition so a Bayesian model be called a basic model conjugate.",
            "If the posterior distribution is an element of the same class of distributions as a prior, and this is also caused closure, called closure undersampling, because you start with the prior, then you sample data and you condition on that data and you remain within the same class of distributions.",
            "So this class of distributions is closed under this process and.",
            "Are these pairs of conjugate priors occur except for some pathological borderline cases?",
            "They occur only in exponential families.",
            "In the parametric case.",
            "So if you slam on some regularity conditions, they only occur in exponential families, and in the next in exponential, the kind of generic formula of an exponential family looks like this, right?",
            "I suppose you've all seen that before.",
            "This year is a sufficient statistic parameter normalizing constant and a modulator, and from that you can generically.",
            "Produce a conjugate prior with this formula.",
            "OK, so there is a generic conjugate prior corresponding to each exponential family distribution and some prominent pairs are the Gaussian.",
            "Then you have a Gauss Wishart prior.",
            "So Gaussian on the main parameter of the chart on the investor shared on the covariance matrix, you have a multinomial.",
            "If you have likelihood is multinomial, then the contract prior is it directly?",
            "OK, So what?",
            "We're now going to do is we're going to assume that all we're going to require that all the mixing mixture components are exponential families, so Gaussian or some other exponential family, and then we put the respective conjugate prior on each parameter and that is called a basic mixture model, and that accounts for the theaters.",
            "But there's still the seas.",
            "And the way we deal with the seasons, we notice that if we sample from this model, then the the index of the cluster is distributed multi normally.",
            "With this vector as a parameter that is simply due to the definition of the mixture model, right?",
            "The index of the cluster we get is multinomial distributed and the parameter of the marginal distribution is this vector of coefficients.",
            "And the conjugate prior of a multinomial distribution, is it directly?",
            "So we're going to use a directly distribution on this number on this on this weight vector here.",
            "OK, so in summary, Bayesian mixture model.",
            "It's a finite mixture model we put.",
            "We use exponential family components.",
            "We use a natural conjugate prior respective natural conjugate prior on the status and the weight vector is distributed according to a direct distribution.",
            "Alright, and so why that gives us is a way that this prior generates this these.",
            "This bunch of conjugate priors together if we sample from them what they generate is a random quantity of this form, right?",
            "OK, and now."
        ],
        [
            "We're going to do this in a non parametric way and we're going to be non parametric by making the number of clusters infinite right?",
            "So here the number of parameters."
        ],
        [
            "The number of parameters is fixed and finite because we have a fixed K. If we make this K infinite, then we get an infinite number of parameters."
        ],
        [
            "And that is a nonparametric model.",
            "So we have in a non parametric way and in a non parametric model we have to generate a quantity like this year at random.",
            "We have to think about how we can do that and.",
            "Again, our restriction is that this now infinite sum overseas has some has to sum to one.",
            "And the simple distribution that we can in a sense that we can come up with that does this, is the directly process.",
            "Now and then, there's a range of other distributions which also do that.",
            "But if we just think about what is the simplest possible way that we can come up with the distribution on an object like this, what could we do so?",
            "First of all, what does simple mean?",
            "Simple means in particular.",
            "So simple means that we have as much independence as possible.",
            "If these these random errors this season thetas, if they are highly dependent, then it will be absolutely impossible to do inference in that model.",
            "OK, so it's certainly helpful for us if they're independent.",
            "And so how can we?",
            "How can we generate something like this and make these random variables as independent as possible?",
            "Well, these locations of the atoms we can certainly just start with some distribution and sample independently from that, and that's exactly what we're doing in the direct process.",
            "So we start with some distribution that we call G nought.",
            "That is, the distribution on the space of these theaters, and we sampled IID from this distribution an infinite sequence of theaters OK. Now, what about the weights we cannot possibly make these weights independent.",
            "They cannot be totally independent becausw they have to satisfy this constraint here.",
            "And that cannot be independent, right?",
            "If we draw the first weight and we know its value is 0.5, then we know that none of the other weights can have a value larger than 0.5 because they have to sum to one.",
            "So we know something about the remaining waits.",
            "A couple through the normalization constraint.",
            "What is the next best thing that we can do to be independent?",
            "The next best thing that we can do is we can instead sample independent proportions and use them to generate this weights and the way you do that is you start with the total amount of mass that you can distribute over these weights, which is just the interval 01.",
            "Think of that as a kind of budget, and now we're going to sample a proportion, which is a random variable on 01.",
            "And say the first proportion is 0.3.",
            "And then we're going to use the current budget and take 0.3 of the current budget.",
            "So in the first in the first step that simply 0.3, what remains is 0.7.",
            "OK, now we sample the second proportion.",
            "If that is 1/2, then we take 1/2 of the remaining budget.",
            "The remaining budget is 0.7.",
            "We take 0.35, and so on.",
            "So basically, each time we sample proportion and we cut off this bid and use it as our our weight.",
            "Yeah, and that means that step by step we eat up this budget and it converges.",
            "This process converges to 0 right?",
            "And that means that these weights converge to one this weight sequence.",
            "And the way we sampled these proportions where we have to put a probability distribution on values between zero and one is simplest distribution we have on the interval on the unit interval is a beta distribution.",
            "And if we set the parameters of the distributions to want to run an Alpha, then this whole model is directly process and we see that we have made two choices.",
            "Here we have to choose the Alpha.",
            "And we have to choose the distribution that we put on the thetas, and so these parameterise.",
            "The directive process distribution and because it it directly process Alpha G node.",
            "Yeah, this is a bit funny.",
            "This notation here, but it's the standard notation in the literature, and so I I just stick to it.",
            "OK, so and when you want to write that out as a formula to get something like this so the weight CK is.",
            "Generated by this this product here and what you see here is that the big product term here which ranges 2K minus one.",
            "This is exactly this process where we cut off a bit of the budget.",
            "So this is what we have.",
            "What we have done so far and this term here describes the budget that is left.",
            "And now VK is the current proportion and we take that out of the budget and that is CK.",
            "It's very simple.",
            "So.",
            "We want to generate an object like this and if we do that in a way that is basically as maximally independent as we can get under the constraints, then what we get is a direct process OK?",
            "I think I should probably take a short break.",
            "5 minutes.",
            "Is that a good idea?",
            "Yeah."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's my rough plan of what I'm going to talk about today and tomorrow.",
                    "label": 0
                },
                {
                    "sent": "Today I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Basic terminology, and then I will spend probably most of today's lecture talking about clustering.",
                    "label": 0
                },
                {
                    "sent": "So about mixture models which Reuben has mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "And then I will talk a little bit about latent feature models.",
                    "label": 0
                },
                {
                    "sent": "And tomorrow I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to speak on some bit more advanced things, so essentially what I'm going to do is I'm going to talk about how we can construct new based on parametric models.",
                    "label": 0
                },
                {
                    "sent": "And I would try to use it as a vehicle to present some of the work that has been done in that field.",
                    "label": 0
                },
                {
                    "sent": "In connection with that, I'm going to talk about something that I'm also briefly going to discuss today, which is exchange ability, and if we have time tomorrow, then I'm going to talk about asymptotic.",
                    "label": 0
                },
                {
                    "sent": "So on how you can theoretically evaluate how well your models perform.",
                    "label": 0
                },
                {
                    "sent": "But today I'm mostly going to talk about clusterings about Derek Lee processes and models related to that.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I gave a tutorial with you by TE at nips on Bayesian non parametrics and if any of you have seen that tutorial the first few slides I think the 1st three or four slides are the same but after that I'm not just going to repeat the tutorial.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's a very simple.",
                    "label": 0
                },
                {
                    "sent": "A very simple description of what we're trying to do in statistics, a kind of cartoon picture of statistics, and that is we have we have a statistical model which is a probability distribution of some observation variable X, which describes our data.",
                    "label": 0
                },
                {
                    "sent": "And that given a parameter Theta OK and that I'm writing the fact that I'm writing that as a conditional probability here is already in a way, a Bayesian assumption, because I'm saying I'm conditioning on a random variable here, so the parameter is a random variable and that is exactly the character characterize the Bayesian model.",
                    "label": 0
                },
                {
                    "sent": "And So what I mean by this year is probability of data given some parameter and the way I would like to think about a parameter here is to think about some kind of pattern.",
                    "label": 0
                },
                {
                    "sent": "That underlies the data, and here is an example, so this is a simple linear regression example where the blue dots here are the data.",
                    "label": 0
                },
                {
                    "sent": "And the red line.",
                    "label": 0
                },
                {
                    "sent": "This linear trend here is a pattern.",
                    "label": 0
                },
                {
                    "sent": "So in this case the pattern would be a linear function.",
                    "label": 0
                },
                {
                    "sent": "And here is a slightly more sophisticated example, which is again a regression example, and again the pattern is a function, but in this case it's not linear anymore and this is the kind of thing that we can do with the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "For example in basic nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we.",
                    "label": 0
                },
                {
                    "sent": "If we look at this picture here.",
                    "label": 0
                },
                {
                    "sent": "What we're trying to do is we're trying, so we're making an assumption here, usually in Bayesian statistics or generally in statistics, which is some kind of independence assumption, and basically what we're assuming is that there is this underlying pattern.",
                    "label": 0
                },
                {
                    "sent": "This is this linear trend here and then the data that we observe is.",
                    "label": 0
                },
                {
                    "sent": "Basically this pattern plus some randomness.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the points are not exactly on the red line, they scatter around the line.",
                    "label": 0
                },
                {
                    "sent": "That is some kind of randomness, but we assume that this this random deviations from the line happen independently of each other for different points, so that the assumption we're making here the idea behind inference is that the data is underlying pattern plus some independent noise, and independent is is really essential here.",
                    "label": 0
                },
                {
                    "sent": "I would ask you to read this plus in a purely qualitative fashion.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the noise could be multiplicative or something.",
                    "label": 0
                },
                {
                    "sent": "I'm just trying to say this is pattern and noise OK. Alright, and so in.",
                    "label": 0
                },
                {
                    "sent": "If we're being Bayesian, then we assume that this pattern here is a random quantity that is the basic characteristic of a Bayesian model, and the distribution of that pattern is the prior the prior distribution.",
                    "label": 0
                },
                {
                    "sent": "What we try to compute is when we have seen data, we try to compute the posterior, which is the distribution of this pattern, given that the data looks the way it looks, OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Now what is a nonparametric Bayesian model in a parametric model?",
                    "label": 0
                },
                {
                    "sent": "Statistics of parametric model is a model which, first of all which has parameters of course.",
                    "label": 0
                },
                {
                    "sent": "But it has some fine print and the fine print is the number of parameters is fixed.",
                    "label": 0
                },
                {
                    "sent": "Or the number is at least constantly bounded with respect to sample size.",
                    "label": 0
                },
                {
                    "sent": "So as we see more and more data, the number of parameters does not increase.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's just a fix.",
                    "label": 0
                },
                {
                    "sent": "Number of parameters.",
                    "label": 0
                },
                {
                    "sent": "And the nonparametric model is.",
                    "label": 0
                },
                {
                    "sent": "Is a model which also uses parameters and so it's really a misnomer.",
                    "label": 0
                },
                {
                    "sent": "But it's a model which does not match the fine print.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so a nonparametric model is where the number of parameters grows with the size of the sample.",
                    "label": 0
                },
                {
                    "sent": "And if you think of the parameters, if you think of the number of parameters as the degrees of freedom that the model has, then this means that the model can become more and more complicated as we see more and more data OK. OK, so if we want to.",
                    "label": 0
                },
                {
                    "sent": "In Bayesian modeling, what we do is we treat the parameter as a random quantity, and we described by a probability distribution right?",
                    "label": 0
                },
                {
                    "sent": "So we have to take the parameter space and we have to put a probability distribution on that and we call that the prior.",
                    "label": 0
                },
                {
                    "sent": "Now if we want to do that here where the number of parameters is not bounded.",
                    "label": 0
                },
                {
                    "sent": "Then, as we see infinitely many data points, we can't.",
                    "label": 0
                },
                {
                    "sent": "We can't put a maximum dimension on the size of the parameter, and so we have to use an infinite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "OK, so a nonparametric model and this is not only true in Bayesian modeling, pretty much in general.",
                    "label": 0
                },
                {
                    "sent": "One way to define nonparametric models is to say that is a statistical model which has an infinite dimensional parameter space.",
                    "label": 0
                },
                {
                    "sent": "So here is an example.",
                    "label": 0
                },
                {
                    "sent": "Simple density estimation example and this is not a Bayesian example.",
                    "label": 0
                },
                {
                    "sent": "This is just standard parametric versus nonparametric statistics.",
                    "label": 0
                },
                {
                    "sent": "So in both cases we're going to estimate a density.",
                    "label": 0
                },
                {
                    "sent": "Using a Gaussian distribution, but we're going to use the Gaussian distribution in two different ways.",
                    "label": 0
                },
                {
                    "sent": "So here we have a bunch of data points and we're just doing making a parametric assumptions and say OK, the data is generated by about 2 dimensional Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So we fit in a Gaussian, say with maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "We get this get this picture here and you see if there's more data coming in that may change.",
                    "label": 0
                },
                {
                    "sent": "That may change our estimate of the parameter, but it does not change the dimensionality of the parameter, right?",
                    "label": 0
                },
                {
                    "sent": "It's just two dimensional location parameter an two by two covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, and here this is a window of a person window type estimator.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing here is each time we see a data point, we fix a Gaussian with a fixed code.",
                    "label": 0
                },
                {
                    "sent": "We take a Gaussian with fixed covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "And put this Gaussian at the location of the data point.",
                    "label": 0
                },
                {
                    "sent": "So we're setting the mean parameter of the Gaussian to the location of the data point.",
                    "label": 0
                },
                {
                    "sent": "And we do that for every data point and then be normalize.",
                    "label": 0
                },
                {
                    "sent": "We add up these densities and normalize.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So basically what we're doing is we're using.",
                    "label": 0
                },
                {
                    "sent": "You can think of this as smoothing.",
                    "label": 0
                },
                {
                    "sent": "We're doing a convolution with the Gaussian kernel and that way we smooth the data.",
                    "label": 0
                },
                {
                    "sent": "The data is a bunch of spikes, sharp spikes and we smooth that was a Gaussian OK, and the density estimate we get that way is called a person window estimator.",
                    "label": 0
                },
                {
                    "sent": "Alright, so you can see that in this case we get one additional location parameter for each data point that we see and the number of data points increases as the number of data points increases.",
                    "label": 0
                },
                {
                    "sent": "The dimension of the parameter space increases.",
                    "label": 0
                },
                {
                    "sent": "So to accommodate this unbounded number of parameters, unbounded number of parameter dimensions, we need an infinite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Alright, and now if you want to be Bayesian about it, if we want to put a prior distribution on that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then what we have to do is we have to put a prior distribution, infinite dimensional space and that is a nonparametric Bayesian model.",
                    "label": 0
                },
                {
                    "sent": "So the definition that we're going to use the over nonparametric Bayesian model is a nonparametric Bayesian model.",
                    "label": 0
                },
                {
                    "sent": "Is a Bayesian model on an infinite dimensional parameter space.",
                    "label": 0
                },
                {
                    "sent": "OK. Is that clear so far?",
                    "label": 0
                },
                {
                    "sent": "OK, and the interpretation the interpretation here is that the parameter space or one useful way, I think to interpret it is the parameter space is the set of possible patterns that we we assume can explain the data.",
                    "label": 0
                },
                {
                    "sent": "So in the density estimation example that we just saw the set of possible patterns would be a set of probability distributions, right?",
                    "label": 0
                },
                {
                    "sent": "Or a set of densities more precisely.",
                    "label": 0
                },
                {
                    "sent": "Because we're doing density estimation, if we're doing regression, so the example on the 1st slide, then the pattern would be a smooth function.",
                    "label": 0
                },
                {
                    "sent": "And if we do clustering, which we're going to talk about most of the time today, then this pattern here that explains the data is a partition partition of the data into separate clusters.",
                    "label": 0
                },
                {
                    "sent": "OK, so the solution we are looking for is not an individual pattern.",
                    "label": 0
                },
                {
                    "sent": "But a distribution over these patterns, namely the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "So we start with a prior guest on how these patterns are distributed.",
                    "label": 0
                },
                {
                    "sent": "We add in what the data look looks like, and then we compute the conditional distribution of the parameter given the data of this pattern given the data and say OK, this pattern is more probable in this one is less probable that is a solution that we would ideally like to get.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and now I have to say a few words about exchange ability.",
                    "label": 0
                },
                {
                    "sent": "I'm going to get back to this tomorrow, but.",
                    "label": 0
                },
                {
                    "sent": "We will already.",
                    "label": 0
                },
                {
                    "sent": "It will already come up several times today and so that.",
                    "label": 0
                },
                {
                    "sent": "The question we ask here is we made this.",
                    "label": 0
                },
                {
                    "sent": "We are making this assumption that the data is in some sense pattern plus noise.",
                    "label": 0
                },
                {
                    "sent": "And this is really crucial to what we're doing for different several different reasons.",
                    "label": 0
                },
                {
                    "sent": "One reason is that if.",
                    "label": 0
                },
                {
                    "sent": "This this noise part here?",
                    "label": 0
                },
                {
                    "sent": "If this is correlated, then it's really hard to do inference.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's really hard to estimate what this pattern is.",
                    "label": 0
                },
                {
                    "sent": "But there is another, like a bit more fundamental reason, basically which is.",
                    "label": 0
                },
                {
                    "sent": "What we really want to do here is we want we say there is something that the data have.",
                    "label": 0
                },
                {
                    "sent": "The data points have in common.",
                    "label": 0
                },
                {
                    "sent": "And then there is something that is just randomness and we would like to extract the common part because that is useful information, right?",
                    "label": 0
                },
                {
                    "sent": "And then we would like to throw away the rest and that is exactly this assumption here.",
                    "label": 0
                },
                {
                    "sent": "This is what the data points share and this is the rest that we would like to throw away.",
                    "label": 0
                },
                {
                    "sent": "So what we have to ask is in which case is this assumption here justifiable?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that there is a very sharp character and precise sharp characterization of when we can justify this assumption.",
                    "label": 0
                },
                {
                    "sent": "In base theorem which you have seen in Zubin's lecture earlier.",
                    "label": 0
                },
                {
                    "sent": "This is encoded in the Bayes equation in this factorial here, so this year is the basic question.",
                    "label": 0
                },
                {
                    "sent": "Here is the prior distribution the parameter then?",
                    "label": 0
                },
                {
                    "sent": "Up here the P of XJ given Theta is the likelihood of datapoint number J given the parameter.",
                    "label": 0
                },
                {
                    "sent": "This here is the evidence term, and then we have the posterior distribution of the parameter given the data.",
                    "label": 1
                },
                {
                    "sent": "And you can see that here the.",
                    "label": 0
                },
                {
                    "sent": "The likelihood overall data points is a product.",
                    "label": 0
                },
                {
                    "sent": "And this product says product over distributions always expresses independence, right?",
                    "label": 0
                },
                {
                    "sent": "So these data points are independent of each other.",
                    "label": 0
                },
                {
                    "sent": "But given the parameter value.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and this is called conditional independence.",
                    "label": 0
                },
                {
                    "sent": "So conditional independence in general and probabilities simply means we have a bunch of random variables.",
                    "label": 0
                },
                {
                    "sent": "In this case the random variables X and they are conditionally independent given Theta if they are independent, provided that we know the value of Cedar and the intuition behind that is these random variables contain information about each other.",
                    "label": 0
                },
                {
                    "sent": "But all of that information is summarized in theater.",
                    "label": 0
                },
                {
                    "sent": "So if we know Theta, then knowing some of the random variables won't tell us anything about the others.",
                    "label": 0
                },
                {
                    "sent": "That is conditional independence.",
                    "label": 0
                },
                {
                    "sent": "So this year is a conditional independence assumption, and that is exactly what we have here.",
                    "label": 0
                },
                {
                    "sent": "We basically.",
                    "label": 0
                },
                {
                    "sent": "So we can rephrase this question and ask under what conditions can be make such a conditional independence assumption for for data.",
                    "label": 0
                },
                {
                    "sent": "And if we look back at the.",
                    "label": 0
                },
                {
                    "sent": "At the picture on the 1st Flight with and then we also see this condition independence assumption in there, which is in this case the red line is a theda right?",
                    "label": 0
                },
                {
                    "sent": "And then the data points scatter around that in dependently.",
                    "label": 0
                },
                {
                    "sent": "So if we know the location of the red line then no data point will tell us anything about what the where the other data points are.",
                    "label": 0
                },
                {
                    "sent": "If we don't know the location of the red line and we know part of these data points now, where is the next data point going to be?",
                    "label": 0
                },
                {
                    "sent": "We can tell that it's much more likely to be in this region than somewhere over here, so conditionally.",
                    "label": 0
                },
                {
                    "sent": "Unconditionally they are not independent.",
                    "label": 0
                },
                {
                    "sent": "OK, so another definition that we need, or the characterization that we need in order to.",
                    "label": 0
                },
                {
                    "sent": "To give a criterion for this condition, independence is something quality, exchange ability and exchangeability is following.",
                    "label": 0
                },
                {
                    "sent": "We see, and we assume that we see an infinite sequence of random variables.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that is data.",
                    "label": 0
                },
                {
                    "sent": "So that is an infinite sequence of such blue dots here for example.",
                    "label": 1
                },
                {
                    "sent": "And now we have it.",
                    "label": 0
                },
                {
                    "sent": "We look at the joint distribution of these of these data.",
                    "label": 0
                },
                {
                    "sent": "And now we ask what happens if we permute the order in which the state of these data points come in.",
                    "label": 0
                },
                {
                    "sent": "And the distribution is called exchangeable, or also the sequences quality exchangeable if the probability of seeing a particular sequence of data points does not depend on this order, so we can permute them.",
                    "label": 0
                },
                {
                    "sent": "And it will not change the probability of occurrence.",
                    "label": 0
                },
                {
                    "sent": "So very simply, the order of the observations does not matter.",
                    "label": 0
                },
                {
                    "sent": "This is a very nice.",
                    "label": 1
                },
                {
                    "sent": "It's a very nice assumption for several reasons.",
                    "label": 0
                },
                {
                    "sent": "One is, it's certainly true of the data is IID, right?",
                    "label": 0
                },
                {
                    "sent": "So the data is IID.",
                    "label": 0
                },
                {
                    "sent": "We can shuffle it around, and so, but it doesn't have to be ID, and so it's more.",
                    "label": 0
                },
                {
                    "sent": "In a sense, it's a more more.",
                    "label": 0
                },
                {
                    "sent": "It contains the assumption that we usually make in parametric sore.",
                    "label": 0
                },
                {
                    "sent": "In nonparametric statistics, the IID assumption is special case, so that's a good start.",
                    "label": 0
                },
                {
                    "sent": "In a way, it's actually the same thing as the idea assumption, But let's not get into that discussion.",
                    "label": 0
                },
                {
                    "sent": "Another thing is that it's it's one thing is it's often true.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the other thing is, it's also relatively easy to argue.",
                    "label": 0
                },
                {
                    "sent": "Whether this is true or not, yeah.",
                    "label": 0
                },
                {
                    "sent": "If you know something about how your data is being generated, then it might be very hard to argue that the data is IID.",
                    "label": 0
                },
                {
                    "sent": "But it's it's often it's reasonably easy to say.",
                    "label": 0
                },
                {
                    "sent": "OK, the order in which the data come in.",
                    "label": 0
                },
                {
                    "sent": "Contains information or it does not.",
                    "label": 0
                },
                {
                    "sent": "Yeah, just if you know if you know that the generative process.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is exchangeability.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now there is this theorem called Definitely theorem which is.",
                    "label": 0
                },
                {
                    "sent": "Really, one of the very basic theorems underlying Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "And this tells us, in short, it tells us the data is conditionally ID if it's exchangeable.",
                    "label": 0
                },
                {
                    "sent": "If anyone?",
                    "label": 0
                },
                {
                    "sent": "If so, we can check whether this conditional IID assumption, whether that makes sense by checking whether the order in which the data points come in matters or not.",
                    "label": 0
                },
                {
                    "sent": "And let's look at this in a bit more detail.",
                    "label": 0
                },
                {
                    "sent": "So what we see over here is again, the joint distribution of the data points.",
                    "label": 0
                },
                {
                    "sent": "And what we have here, on the other side is.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "An integral over the parameter and I have now written the parameter here as a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the parameter is now a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "And this year is again the prior distribution.",
                    "label": 0
                },
                {
                    "sent": "And now the prior distribution is a distribution over probability distributions.",
                    "label": 0
                },
                {
                    "sent": "So we sample a random probability distribution from this Q.",
                    "label": 0
                },
                {
                    "sent": "Then we fix it and we sample.",
                    "label": 0
                },
                {
                    "sent": "These points from that distribution that we have just generated at random.",
                    "label": 0
                },
                {
                    "sent": "And how do we sample them?",
                    "label": 0
                },
                {
                    "sent": "IID, because we have this product here, so products IID.",
                    "label": 0
                },
                {
                    "sent": "OK, So what this here says is.",
                    "label": 0
                },
                {
                    "sent": "A distribution is exchangeable.",
                    "label": 0
                },
                {
                    "sent": "If and only if.",
                    "label": 0
                },
                {
                    "sent": "We can generate it in this way.",
                    "label": 0
                },
                {
                    "sent": "We put it to some distribution on probability distributions we draw from that we draw a probability distribution at random, fix it.",
                    "label": 0
                },
                {
                    "sent": "And then sample data from it IID OK. And this here is exactly if you will.",
                    "label": 0
                },
                {
                    "sent": "This is exactly the term that we have seen here.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the base equation.",
                    "label": 0
                },
                {
                    "sent": "Takes a prior.",
                    "label": 0
                },
                {
                    "sent": "Sample parameter from it and then we get a product over the data points OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what this theorem says is.",
                    "label": 0
                },
                {
                    "sent": "If and only if the data is exchangeable, this whole this whole idea data is parent plus noise makes sense and the pattern in general is expressed by a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "And if we do something like saying on the previous slide here.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our pattern is really a straight line.",
                    "label": 0
                },
                {
                    "sent": "This is not a probability distribution then that would mean that we choose a set of probability distributions that are parameterized by straight lines OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good, but the caveat here is.",
                    "label": 0
                },
                {
                    "sent": "What this does not mean is.",
                    "label": 0
                },
                {
                    "sent": "You start with some.",
                    "label": 0
                },
                {
                    "sent": "Bayesian model, where your likelihood to say a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And now you say OK, my data is exchangeable, and therefore this assumption makes sense, because in general.",
                    "label": 0
                },
                {
                    "sent": "This year this district probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Here is an arbitrary probability distribution, and that means a set in which these probability distributions live is an infinite dimensional set.",
                    "label": 0
                },
                {
                    "sent": "The set of all probability distributions on a given space is an infinite dimensional set.",
                    "label": 0
                },
                {
                    "sent": "So in general we have to put a prior on infinite dimensional space here and from from an abstract perspective, this is a key motivation for for being nonparametric Bayesian models.",
                    "label": 0
                },
                {
                    "sent": "If you want to really, if you want to really exploit the statement of this theorem.",
                    "label": 0
                },
                {
                    "sent": "If you want to really use that, then you have to be non parametric.",
                    "label": 0
                },
                {
                    "sent": "OK you have to use infinite dimensional parameters.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good so.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clustering.",
                    "label": 0
                },
                {
                    "sent": "And has already mentioned clustering earlier.",
                    "label": 0
                },
                {
                    "sent": "Basically, the very simple thing this is just a cartoon cartoon picture of clustering.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "This data was of course generated by sampling from a Gaussian mixture model.",
                    "label": 0
                },
                {
                    "sent": "If anybody, yeah, can I just ask who has seen who has not seen a finite mixture model before?",
                    "label": 0
                },
                {
                    "sent": "OK, very good, thank you so.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume that we have observations X one X2 and so on.",
                    "label": 0
                },
                {
                    "sent": "And the modeling assumption we make in clustering is.",
                    "label": 0
                },
                {
                    "sent": "The data is subdivided into groups.",
                    "label": 0
                },
                {
                    "sent": "It's partitioned into groups and that means each data point belongs to exactly 1 cluster.",
                    "label": 0
                },
                {
                    "sent": "And the in this case, the unknown pattern that I was talking off before, so I see the if you will.",
                    "label": 0
                },
                {
                    "sent": "The unknown pattern is a partition of the data.",
                    "label": 0
                },
                {
                    "sent": "Into disjoint groups, yeah, and if you now think of if you think of this as an infinite sequence here, then it has indices 1234 and so on, and we can partition it by partitioning the Indy says, yeah, So what we're looking for in that case is a partition of North, or if we only see a finite number of data points one through N, then we look at it for a partition of this finite set.",
                    "label": 0
                },
                {
                    "sent": "And here on the site I have a very simple application of clustering.",
                    "label": 0
                },
                {
                    "sent": "So what I did here is I just took a very simple image.",
                    "label": 0
                },
                {
                    "sent": "It's basically night Sky with some trees down here.",
                    "label": 0
                },
                {
                    "sent": "So we see a Sky, moon and some trees.",
                    "label": 0
                },
                {
                    "sent": "And now we want to segment this image.",
                    "label": 0
                },
                {
                    "sent": "Using clustering and the way this is being done here is we take a small sliding window, pass it over the image, we slide it over the image and at each location where the window is positioned, we extract the pixel values within that window and write them into a histogram.",
                    "label": 0
                },
                {
                    "sent": "So we get a histogram associated which was each location in the window of the window.",
                    "label": 0
                },
                {
                    "sent": "And that gives us a mapping from this image into a set of histograms into a space of vectors of histograms.",
                    "label": 0
                },
                {
                    "sent": "Basically, and then we're going to apply clustering on this histogram space.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and so we get clusters.",
                    "label": 0
                },
                {
                    "sent": "The histograms get sorted into clusters and then what I have done here is I've taken the first cluster of histograms and.",
                    "label": 0
                },
                {
                    "sent": "Colored each location where one of these histograms was positioned in blue.",
                    "label": 0
                },
                {
                    "sent": "Right then the other one in green and in red, so that induces basically clusters in feature space correspond to segments in the image that is very simple.",
                    "label": 0
                },
                {
                    "sent": "Application of.",
                    "label": 0
                },
                {
                    "sent": "Of a clustering method and what we see here is so Zubin talked about number of clusters earlier and about model selection in this case here.",
                    "label": 0
                },
                {
                    "sent": "Here we have three clusters and here we have two clusters.",
                    "label": 0
                },
                {
                    "sent": "Now image segmentation is Anil Post problem and actually people if you let people segment images and they totally disagree on how many segments there should usually should be in an image.",
                    "label": 0
                },
                {
                    "sent": "I think in this case here pretty much everybody would agree that it should probably be 3 segments or that at least the moon shouldn't be locked in with the trees.",
                    "label": 0
                },
                {
                    "sent": "So this is probably.",
                    "label": 0
                },
                {
                    "sent": "Not as good a solution as this one.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the way we're going to think about clustering.",
                    "label": 0
                },
                {
                    "sent": "Is in terms of mixture models.",
                    "label": 0
                },
                {
                    "sent": "And mixture model in general looks like this.",
                    "label": 0
                },
                {
                    "sent": "So this year is in in.",
                    "label": 0
                },
                {
                    "sent": "Our mix, our distribution, the distribution given by the mixture model.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is a distribution of data given some M OK the M is something called the mixing measure.",
                    "label": 0
                },
                {
                    "sent": "So this is our mixing distribution.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is a measure against which we integrate some component.",
                    "label": 0
                },
                {
                    "sent": "Think of this for example as a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "The P of X given Theater is a Gaussian distribution with parameter Theta and then we have a distribution over theaters here.",
                    "label": 0
                },
                {
                    "sent": "And there's various things that you can do with that.",
                    "label": 0
                },
                {
                    "sent": "For example, if you set this year to a gamma.",
                    "label": 0
                },
                {
                    "sent": "And theater is a scale parameter of the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Then you get a student distribution.",
                    "label": 0
                },
                {
                    "sent": "But the way we're going to use this in clustering is we're always going to look at at a specific form of mixture where this year is a distribution of finitely many, or sometimes infinitely many Delta spikes, and.",
                    "label": 0
                },
                {
                    "sent": "What mixture models in general always express?",
                    "label": 0
                },
                {
                    "sent": "And what I think everybody should always keep in mind when they look at mixtures is they expressed two stage sampling.",
                    "label": 0
                },
                {
                    "sent": "Yeah, So what this mixture model expresses is?",
                    "label": 0
                },
                {
                    "sent": "We sample X from this model here by first sampling theater from this distribution.",
                    "label": 0
                },
                {
                    "sent": "Then we plug it into into this distribution and sample X from that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So mixture models are two stage sampling.",
                    "label": 0
                },
                {
                    "sent": "And now a finite mixture model in this formulation looks like this.",
                    "label": 0
                },
                {
                    "sent": "That is again simply a mixture model.",
                    "label": 0
                },
                {
                    "sent": "But we have a specific form of this mixing measure and that is it's a sum over Delta spikes at K possible locations.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is a finite number K. Here we have K different values.",
                    "label": 0
                },
                {
                    "sent": "Theater on some parameter space.",
                    "label": 0
                },
                {
                    "sent": "And replace the Delta spike at each of them and then we wait them with convex coefficients.",
                    "label": 0
                },
                {
                    "sent": "This is a convex combination of Delta spikes.",
                    "label": 0
                },
                {
                    "sent": "Convex combination means these C values here are non negative and they sum to one OK so that means this is a probability distribution and we sample when we sample from this distribution that possible values that we can get are these leaders.",
                    "label": 0
                },
                {
                    "sent": "And they occur with probability.",
                    "label": 0
                },
                {
                    "sent": "So thi decay occurs with probability CK, OK?",
                    "label": 0
                },
                {
                    "sent": "So this is a finite mixture model.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we can be Bayesian about this and introduce something called a Bayesian mixture model.",
                    "label": 0
                },
                {
                    "sent": "And the way we do this is.",
                    "label": 0
                },
                {
                    "sent": "The idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "If we if we.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Back here and we see we define a finite mixture model in this way.",
                    "label": 0
                },
                {
                    "sent": "But what do we have to choose where we have to choose this component distribution here?",
                    "label": 0
                },
                {
                    "sent": "And we say, OK, let's say Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK for Gaussian mixture model and now we have parameters and the parameters are the mix of mixing coefficients and the locations or the parameter values of the individual Gaussian components and that means all the parameters of this model are summarized in this mixing distribution OK?",
                    "label": 0
                },
                {
                    "sent": "The mixture model is parameterized by the mixing this.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we have to do is we have to generate the mixing distribution at random if we want to be based on the parameter is mixing distribution, we have to put a prior on that.",
                    "label": 0
                },
                {
                    "sent": "And so the object we want to get is something like this.",
                    "label": 0
                },
                {
                    "sent": "And now I have capitalized the season the theaters because they are now random variables we want to generate that at random, and Similarly, I have capitalized the.",
                    "label": 0
                },
                {
                    "sent": "We want to generalize the generator random mixing measure and the idea in the Bayesian mixture model, so the basic mixture model is something that was introduced in the literature in the 1990s, and specifically the idea there is use a conjugate prior on each parameter.",
                    "label": 0
                },
                {
                    "sent": "Zubin has mentioned conjugate priors earlier.",
                    "label": 0
                },
                {
                    "sent": "Let's just quickly repeat the definition so a Bayesian model be called a basic model conjugate.",
                    "label": 0
                },
                {
                    "sent": "If the posterior distribution is an element of the same class of distributions as a prior, and this is also caused closure, called closure undersampling, because you start with the prior, then you sample data and you condition on that data and you remain within the same class of distributions.",
                    "label": 0
                },
                {
                    "sent": "So this class of distributions is closed under this process and.",
                    "label": 0
                },
                {
                    "sent": "Are these pairs of conjugate priors occur except for some pathological borderline cases?",
                    "label": 0
                },
                {
                    "sent": "They occur only in exponential families.",
                    "label": 0
                },
                {
                    "sent": "In the parametric case.",
                    "label": 0
                },
                {
                    "sent": "So if you slam on some regularity conditions, they only occur in exponential families, and in the next in exponential, the kind of generic formula of an exponential family looks like this, right?",
                    "label": 0
                },
                {
                    "sent": "I suppose you've all seen that before.",
                    "label": 0
                },
                {
                    "sent": "This year is a sufficient statistic parameter normalizing constant and a modulator, and from that you can generically.",
                    "label": 0
                },
                {
                    "sent": "Produce a conjugate prior with this formula.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is a generic conjugate prior corresponding to each exponential family distribution and some prominent pairs are the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Then you have a Gauss Wishart prior.",
                    "label": 0
                },
                {
                    "sent": "So Gaussian on the main parameter of the chart on the investor shared on the covariance matrix, you have a multinomial.",
                    "label": 0
                },
                {
                    "sent": "If you have likelihood is multinomial, then the contract prior is it directly?",
                    "label": 0
                },
                {
                    "sent": "OK, So what?",
                    "label": 0
                },
                {
                    "sent": "We're now going to do is we're going to assume that all we're going to require that all the mixing mixture components are exponential families, so Gaussian or some other exponential family, and then we put the respective conjugate prior on each parameter and that is called a basic mixture model, and that accounts for the theaters.",
                    "label": 0
                },
                {
                    "sent": "But there's still the seas.",
                    "label": 0
                },
                {
                    "sent": "And the way we deal with the seasons, we notice that if we sample from this model, then the the index of the cluster is distributed multi normally.",
                    "label": 0
                },
                {
                    "sent": "With this vector as a parameter that is simply due to the definition of the mixture model, right?",
                    "label": 0
                },
                {
                    "sent": "The index of the cluster we get is multinomial distributed and the parameter of the marginal distribution is this vector of coefficients.",
                    "label": 0
                },
                {
                    "sent": "And the conjugate prior of a multinomial distribution, is it directly?",
                    "label": 0
                },
                {
                    "sent": "So we're going to use a directly distribution on this number on this on this weight vector here.",
                    "label": 0
                },
                {
                    "sent": "OK, so in summary, Bayesian mixture model.",
                    "label": 0
                },
                {
                    "sent": "It's a finite mixture model we put.",
                    "label": 1
                },
                {
                    "sent": "We use exponential family components.",
                    "label": 0
                },
                {
                    "sent": "We use a natural conjugate prior respective natural conjugate prior on the status and the weight vector is distributed according to a direct distribution.",
                    "label": 0
                },
                {
                    "sent": "Alright, and so why that gives us is a way that this prior generates this these.",
                    "label": 0
                },
                {
                    "sent": "This bunch of conjugate priors together if we sample from them what they generate is a random quantity of this form, right?",
                    "label": 0
                },
                {
                    "sent": "OK, and now.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to do this in a non parametric way and we're going to be non parametric by making the number of clusters infinite right?",
                    "label": 0
                },
                {
                    "sent": "So here the number of parameters.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The number of parameters is fixed and finite because we have a fixed K. If we make this K infinite, then we get an infinite number of parameters.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that is a nonparametric model.",
                    "label": 0
                },
                {
                    "sent": "So we have in a non parametric way and in a non parametric model we have to generate a quantity like this year at random.",
                    "label": 0
                },
                {
                    "sent": "We have to think about how we can do that and.",
                    "label": 0
                },
                {
                    "sent": "Again, our restriction is that this now infinite sum overseas has some has to sum to one.",
                    "label": 0
                },
                {
                    "sent": "And the simple distribution that we can in a sense that we can come up with that does this, is the directly process.",
                    "label": 0
                },
                {
                    "sent": "Now and then, there's a range of other distributions which also do that.",
                    "label": 0
                },
                {
                    "sent": "But if we just think about what is the simplest possible way that we can come up with the distribution on an object like this, what could we do so?",
                    "label": 0
                },
                {
                    "sent": "First of all, what does simple mean?",
                    "label": 0
                },
                {
                    "sent": "Simple means in particular.",
                    "label": 0
                },
                {
                    "sent": "So simple means that we have as much independence as possible.",
                    "label": 0
                },
                {
                    "sent": "If these these random errors this season thetas, if they are highly dependent, then it will be absolutely impossible to do inference in that model.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's certainly helpful for us if they're independent.",
                    "label": 0
                },
                {
                    "sent": "And so how can we?",
                    "label": 0
                },
                {
                    "sent": "How can we generate something like this and make these random variables as independent as possible?",
                    "label": 0
                },
                {
                    "sent": "Well, these locations of the atoms we can certainly just start with some distribution and sample independently from that, and that's exactly what we're doing in the direct process.",
                    "label": 0
                },
                {
                    "sent": "So we start with some distribution that we call G nought.",
                    "label": 0
                },
                {
                    "sent": "That is, the distribution on the space of these theaters, and we sampled IID from this distribution an infinite sequence of theaters OK. Now, what about the weights we cannot possibly make these weights independent.",
                    "label": 0
                },
                {
                    "sent": "They cannot be totally independent becausw they have to satisfy this constraint here.",
                    "label": 0
                },
                {
                    "sent": "And that cannot be independent, right?",
                    "label": 0
                },
                {
                    "sent": "If we draw the first weight and we know its value is 0.5, then we know that none of the other weights can have a value larger than 0.5 because they have to sum to one.",
                    "label": 0
                },
                {
                    "sent": "So we know something about the remaining waits.",
                    "label": 0
                },
                {
                    "sent": "A couple through the normalization constraint.",
                    "label": 0
                },
                {
                    "sent": "What is the next best thing that we can do to be independent?",
                    "label": 0
                },
                {
                    "sent": "The next best thing that we can do is we can instead sample independent proportions and use them to generate this weights and the way you do that is you start with the total amount of mass that you can distribute over these weights, which is just the interval 01.",
                    "label": 0
                },
                {
                    "sent": "Think of that as a kind of budget, and now we're going to sample a proportion, which is a random variable on 01.",
                    "label": 0
                },
                {
                    "sent": "And say the first proportion is 0.3.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to use the current budget and take 0.3 of the current budget.",
                    "label": 0
                },
                {
                    "sent": "So in the first in the first step that simply 0.3, what remains is 0.7.",
                    "label": 0
                },
                {
                    "sent": "OK, now we sample the second proportion.",
                    "label": 0
                },
                {
                    "sent": "If that is 1/2, then we take 1/2 of the remaining budget.",
                    "label": 0
                },
                {
                    "sent": "The remaining budget is 0.7.",
                    "label": 0
                },
                {
                    "sent": "We take 0.35, and so on.",
                    "label": 0
                },
                {
                    "sent": "So basically, each time we sample proportion and we cut off this bid and use it as our our weight.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and that means that step by step we eat up this budget and it converges.",
                    "label": 0
                },
                {
                    "sent": "This process converges to 0 right?",
                    "label": 0
                },
                {
                    "sent": "And that means that these weights converge to one this weight sequence.",
                    "label": 0
                },
                {
                    "sent": "And the way we sampled these proportions where we have to put a probability distribution on values between zero and one is simplest distribution we have on the interval on the unit interval is a beta distribution.",
                    "label": 0
                },
                {
                    "sent": "And if we set the parameters of the distributions to want to run an Alpha, then this whole model is directly process and we see that we have made two choices.",
                    "label": 0
                },
                {
                    "sent": "Here we have to choose the Alpha.",
                    "label": 0
                },
                {
                    "sent": "And we have to choose the distribution that we put on the thetas, and so these parameterise.",
                    "label": 0
                },
                {
                    "sent": "The directive process distribution and because it it directly process Alpha G node.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is a bit funny.",
                    "label": 0
                },
                {
                    "sent": "This notation here, but it's the standard notation in the literature, and so I I just stick to it.",
                    "label": 0
                },
                {
                    "sent": "OK, so and when you want to write that out as a formula to get something like this so the weight CK is.",
                    "label": 0
                },
                {
                    "sent": "Generated by this this product here and what you see here is that the big product term here which ranges 2K minus one.",
                    "label": 0
                },
                {
                    "sent": "This is exactly this process where we cut off a bit of the budget.",
                    "label": 0
                },
                {
                    "sent": "So this is what we have.",
                    "label": 0
                },
                {
                    "sent": "What we have done so far and this term here describes the budget that is left.",
                    "label": 0
                },
                {
                    "sent": "And now VK is the current proportion and we take that out of the budget and that is CK.",
                    "label": 0
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We want to generate an object like this and if we do that in a way that is basically as maximally independent as we can get under the constraints, then what we get is a direct process OK?",
                    "label": 0
                },
                {
                    "sent": "I think I should probably take a short break.",
                    "label": 0
                },
                {
                    "sent": "5 minutes.",
                    "label": 0
                },
                {
                    "sent": "Is that a good idea?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        }
    }
}