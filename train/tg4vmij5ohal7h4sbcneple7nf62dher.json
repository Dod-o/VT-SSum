{
    "id": "tg4vmij5ohal7h4sbcneple7nf62dher",
    "title": "Efficient Learning using Forward-Backward Splitting",
    "info": {
        "author": [
            "John Duchi, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips09_duchi_elu/",
    "segmentation": [
        [
            "To start out this talk, I'd like to begin with a little motivating example of two algorithms running.",
            "The first is sort of a classical one, and the 2nd is the one that I'm going to be describing in this talk.",
            "So let's consider minimization of a quadratic objective with L1 regularization.",
            "The true solution is at my."
        ],
        [
            "This one zero, indicated by this star and we see here the level curves of L1 regularization superimposed on the level curves of our regularization function.",
            "So a classical method for solving such problems is the subgradient method, and right here we see the iterates of the subgradient method and you can see it's going to converge to that point.",
            "But it sort of takes a long time and wanders all over the place and the method I'm going to talk about.",
            "This is what happens when we run it on the same problem, so it converges very quickly now.",
            "Let's talk about why this happens with the subgradient method."
        ],
        [
            "So the sub gradient of a function is essentially the generalization of a gradient to a non differentiable function.",
            "So what the subgradient set at a non differentiable point is is basically the set of tangents to the set of tangent lines to the function at the non differentiable point.",
            "So it looks something like this.",
            "It's this whole set of lines that underestimate the function.",
            "So usually the subgradient method is basically just picking a subgradient direction of your function and going in that direction.",
            "So why doesn't this work very well?",
            "The main problem is that sub gradients are basically not informative at the singularity's.",
            "So here we have a differentiable function in a non differentiable function.",
            "As we get closer to the minimum we see that the gradient represented by the yellow wine for the differentiable function gets much much smaller as we get close to the minimum of the sub differentiable function nothing changes, so we can't really tell when we're close to the minimum, but these points of non differentiability."
        ],
        [
            "Are the minimizing points, but landing on them using a subgradient method is like landing on the head of a needle, and it's very difficult to do so.",
            "The method I'm going to propose is an attempt to deal with that.",
            "OK."
        ],
        [
            "So just a brief outline.",
            "I'm going to talk about the algorithm, the framework we propose and show how to deal with sort of non differentiable, which will allow us to deal efficiently with non differentiable regularization functions.",
            "I'll give some convergence properties of the algorithm and then I'll show that it's simple to derive algorithms for different types of regularization in our framework and show ways to make it really usable and very high dimensional problems, especially with sparsity.",
            "Then I'll give some experimental results and.",
            "Conclusions.",
            "So the algorithm we propose is called Phobos."
        ],
        [
            "As for forward backward splitting and the overarching goal of the algorithm is to minimize the composite objective like what you see there, we have a loss function, L and regularizer R, and the goal is to deal with non differentiable regularizers.",
            "So what the method does is that it has two steps and in the first step we take."
        ],
        [
            "An unconstrained gradient of the loss or a stochastic subgradient.",
            "It doesn't really matter, but we go in the gradient."
        ],
        [
            "For the loss and then in the second phase, we're going to do something intelligent to deal with the regularization parameter and the method we use is similar to forward backward splitting which has been around since the 70s and then composite gradient methods recently from Steven Wright and his colleagues in 2009 and messed up in 2007, and at this nips Lindja had a paper on dual averaging with regularization, so these."
        ],
        [
            "Algorithms are all very related.",
            "It's a.",
            "It's an active area of research for solving composite functions like this.",
            "So the first step of our algorithm is simply to take an unconstrained gradient step on the loss or a stochastic subgradient.",
            "And you've probably all seen this is just simply gradient descent.",
            "We start at WT and we get to this intermediate point WT plus half on the loss.",
            "Now the second step is what really makes our algorithm work and sort of the important take home message of this talk and in the second step what we do is we formulate sort of an intermediate objective which is a quadratic term to keep us close to the intermediate point WT plus half which you see right here and then we add the regularizer.",
            "So essentially what we're doing is we're corrupting for any sort of overstepping we might have done when we took the gradient with the loss.",
            "So here's a graphic representation we see our last function and our regularizer.",
            "We add them up to get the true function that we're minimizing, so that's that.",
            "And now we look at this sort of quadratic function plus regularization, which looks something like."
        ],
        [
            "This sort of at a high level.",
            "You can think of this as doing prediction and correction for the regularizer or trying to make some sort of quadratic approximation to your loss, but you're trading off between staying close to the intermediate point and minimizing your regularizer.",
            "So basically the whole game in this algorithm is to find regularizers so that minimizing this function is simple, and often we'll see that that's the case because it's just a quadratic and it's completely separable.",
            "So that's the that's kind of the one of the main take home messages is that if you can solve this problem, you can minimize these types of functions efficiently."
        ],
        [
            "So now I'd like to give a high level kind of picture of what our analysis looks like.",
            "So if you look at the conditions for optimality of our of our sort of iterative method, you see that zero is in the subgradient set of this function W T + 1 minus WT, plus the subdifferential of our loss at the current point, and then sort of a forward looking subdifferential of the regularizer.",
            "So we see this equation right here the next iterate.",
            "Is a linear combination of the previous point the gradient at the previous point, and then we sort of peak ahead to see the gradient of the regularizer at our next step.",
            "And this is what allows us to push back dealing with this non differentiability.",
            "Intuitively you just because because we're cheating and looking ahead these non differentiable points don't really affect our algorithm and this equation right here is what makes the analysis go through essentially once we have this we can kind of just follow previous analysis of gradient descent or subgradient descent.",
            "And get convergence rates for our algorithm."
        ],
        [
            "So I'll briefly go over those convergence properties if we're doing batch learning, or say, like empirical risk minimization with regularization, then if we set our step size ADA to be one over root T, we're going to get a convergence rate of one over root tea, meaning after T steps where within one over root tea of the optimal value of the problem.",
            "We can also do the same thing for online learning.",
            "Where we can get average regret bounds or regret bounds?",
            "So for arbitrary convex functions L we get a sequence of them when we want to compete against sort of some static predictor.",
            "If we take the step sizes to be proportional to one over root T, we get a regret of root tea with strongly convex functions we can do slightly better, which, but these results are these types of results are well known, but we can get them in this composite framework now using our."
        ],
        [
            "Two step update.",
            "So I'm going to go through a series of derived algorithms, so I'm going to show the second step of our Phobos algorithm for three different types of regularization, and you'll see that solving these is actually not that difficult, and it allows us to efficiently solve problems with L1 regularization.",
            "Two different types of L2 regularization.",
            "And then we'll extend it to mix norms for structured problems like multi class or multitask problems.",
            "So.",
            "To begin, I'll just describe the L1 problem so."
        ],
        [
            "The second step of the algorithm, we minimize this quadratic tradeoff between the intermediate point and L1 regularization, so you can see that this is separable.",
            "And so we just need to minimize for each coordinate individually and basically what we get is this truncated gradient update.",
            "And this was studied last year at NIPS by John Langford and colleagues, and has been known in the statistics literature for awhile is iterative shrinkage thresholding.",
            "Intuitively, what we see is that if our intermediate point WT lands within minus Lambda to Lambda, it gets shrunk to 0, which is sort of in this plot we have the X axis is our intermediate point, the Y axis is the resulting.",
            "Point WT plus one.",
            "So if we land within this range, we just get truncated to zero, and otherwise you're shrunk by a factor of Lambda times the step size.",
            "So this you can see that this is quite a simple update and we actually do get zeros when we when we do it with L1 regularization without two, we have two different updates with L2 squared regularization, we actually get a new update which is essentially gradient descent with geometric shrinkage.",
            "So this is kind of in contrast to recent advances like the Pegasos algorithm does projected gradients.",
            "So here we don't actually need to do projections because somehow this shrinkage step just kind of pulls us back to some ball which is near the optimal point, and we'll see in the experiments that this works well, so we never actually need to do any types of projections here with just straight L2 regularization, meaning that our regularizer looks sort of like a cone starting at the origin.",
            "We actually get something very similar to L1."
        ],
        [
            "So if you look at this update, what you see is that if our intermediate point has two norm less than Lambda times our step size, then this term right here is going to be 0.",
            "And we're going to update the entire vector to be the all zeros vector.",
            "Otherwise it's just a sort of shrinkage term times the times the vector.",
            "So this this update sort of looks like a truncated cone.",
            "And this is, I mean, if we look back at the previous update for L1, this is very similar, except Now we're just in more dimensions and we do it to the whole vector at once.",
            "So this is a relatively simple update to implement."
        ],
        [
            "Now, for structured problems like multi class or multitask lots of times we want to do mixed norm regularization.",
            "So we have an entire matrix of weights.",
            "Each column of the matrix corresponds to one of our subproblems, say a task, or represents waits for a particular class, and we actually want to regularize rows of this matrix together in some sort of QM like this.",
            "So why would?",
            "Why might we want to do this?",
            "Because essentially the JF bro is associated with the JS feature of our problem.",
            "We only want to penalize once, so either you include the feature in your model or you don't include it at all.",
            "And by putting these these rows and say an Infinity norm which I haven't talked about, but you can also solve that using our framework or or in just a plain two norm.",
            "Essentially what we get is once one parameters in the model, all the rest of them can come in for free and it's.",
            "I think fairly clear from the form of the regularization that we can solve this update just using the previous.",
            "The previous updates on the slides because it's separable.",
            "So that's that's how we deal with mixed 1 problems now before I get into the experimental results, I'd like to talk about how we can make this algorithm efficient for really high dimensional problem."
        ],
        [
            "At least with sparsity.",
            "So imagine you're solving, say, a text application and NLP application.",
            "You're sort of the dimension of your space is huge in the millions, say, but you have in any given sort of example you have only a few non 0 features.",
            "So for example, if this is our gradient at time steps 12345 you see, sorry AT23 four and five, you see that the second parameter, the second entry of the gradient, is 0.",
            "So intuitively we would like to be able to do lazy updates and not have to deal with these these zero entries.",
            "So what are we going to do?",
            "You know, the input space is sparse, but it's huge and we want to be.",
            "We don't want to have to perform an update every time on our vector W, even though we're not seeing any entries to the gradient.",
            "So this proposition actually saves us from that."
        ],
        [
            "It allows us to do really efficient high dimensional learning so.",
            "The following to sort of.",
            "Basically, sequences of steps are equivalent, so we can either solve the update to get our next parameter WT at every time step every time we get to 0, we just do the update or we can aggregate our regularization parameters into this and just sum them up and solve the update once when we actually need the parameter.",
            "So this is very clearly much more efficient than this, and this proposition is true for Q = 1 to Infinity, which is really all we need for the applications we have in mind.",
            "But this really will allow us to do really efficient, very high dimensional solve.",
            "Very high dimensional problems."
        ],
        [
            "So now, under the experimental results.",
            "First, I'd like to talk about whether we can actually get the sparsity 'cause you remember.",
            "This is one of our goals to begin with was to deal with these non differentiable points, so this is each of these curves."
        ],
        [
            "Is the sparsity level of our weight vector W as a function of the number of steps that are.",
            "The Phobos algorithm has taken for solving an L1 regularised logistic regression problem and each line corresponds to a different regularization value.",
            "And you see that after about 30 steps, say we've essentially settled into the kind of sparsity pattern sparsity proportion that we're going to have for the rest of the problem.",
            "So we very quickly get to the right sparsity level.",
            "So it seems we're doing the right thing.",
            "Now.",
            "This is for a deterministic problem.",
            "When you're using stochastic gradient information, it's much more spiky.",
            "It's not quite as clean, so that's an area that we're looking into figuring out how to deal with, but it does seem that we can handle these non differentiable regularizers.",
            "So I also mentioned that we are able to do efficient very high dimensional learning, so this is an example of the amount of time it takes to add a sparse vector with S nonzero components to a vector and then either project it to an L1 ball.",
            "Which of these black lines or perform the Phobos lazy update to it?",
            "So last year I smell you."
        ],
        [
            "Singer in shadow shorts and I had a paper describing how to efficiently project to an L1 ball under sparse updates.",
            "The algorithm is very complicated and as we can see from this, these three lines here, it's much much lower than Phobos, so.",
            "If we look at sort of the dimension State 10,000,000, which is around here, every projection step is going to take something like 6 or 7 seconds, which is really slow for a gradient method whereas photo."
        ],
        [
            "Even in the slowest cases, taking much less than half a second, so we're getting a factor of more than 10 improvement in these high dimensional problems.",
            "We have sparse regularizers, which is very significant.",
            "Foremost, we also also described the L2 update or with L2 squared regularization.",
            "So this is just showing our convergence compared to Pegasus, which is a recent algorithm for solving SVM's.",
            "We can see that the performance is quite similar.",
            "We don't have to do a projection step and I guess we're very slightly faster, but essentially very close to this.",
            "To the Pegasos algorithm.",
            "And now I'll talk about some sort of."
        ],
        [
            "Higher dimension problems.",
            "So in this experiment we are comparing.",
            "Stochastic gradient Phobos, 22 algorithms designed to handle mixed norm problems and we're solving the 10 class amnist problem attend class digit classification problem.",
            "There are 60,000 training examples.",
            "Each of the 10 classes has 3000 features, so there's 30,000 features and were regularly we're doing black regularization, so each feature is regularised in blocks of 10 sparsa, and this coordinate descent method were specifically designed to handle these types of mixed norm block regularization problems.",
            "And so this is.",
            "This is sort of a zoomed out view of our test error rate as a function of runtime and you can see that stochastic fobos does.",
            "Decreased much more quickly, so let's zoom in a little bit.",
            "And this point is that."
        ],
        [
            "Time that sparser finishes its first iteration, and so essentially by the time these other algorithms have even finished one iteration.",
            "Focus is converge to the minimum of the function.",
            "And the other algorithms, I mean they catch up eventually about, you know an hour later, but focus in this case for these stochastic type problems with many parameters and lots and lots of data, we see as much, much faster.",
            "So just a couple of conclusions."
        ],
        [
            "What I've given is a general framework for stochastic online batch optimization with regularization at least regularly with many different kinds of regularization.",
            "We've shown that you can do mixed on regularization for multi class multi task problems, and there were efficient and really high dimensions.",
            "We have lazy updates to handle the case.",
            "Some future work that we in a few other groups are working on are putting different structural problems.",
            "Sorry, different structural assumptions of our problem into the regularization.",
            "And finding efficient updates for those cases.",
            "And.",
            "That's all I have to say, so thanks very much for listening."
        ],
        [
            "Questions.",
            "Good.",
            "Well, that was really well.",
            "So that's a question for the stochastic case.",
            "Do you have any ideas how to prove anything but convergence to the sparsity pattern that you will eventually converge to?",
            "And you mentioned it spiky, but you didn't actually.",
            "Yeah, so in the stochastic case, you know I don't want to say you guys snake oil, but it is not as clean as this and I'm not quite sure how to deal with that.",
            "So I mentioned Lindow's dual averaging paper and I think that deals very well with the problem at least in practice.",
            "But I don't have any great ideas for how to sort of formally argue that one or the other will actually converge the right sparsity."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To start out this talk, I'd like to begin with a little motivating example of two algorithms running.",
                    "label": 0
                },
                {
                    "sent": "The first is sort of a classical one, and the 2nd is the one that I'm going to be describing in this talk.",
                    "label": 0
                },
                {
                    "sent": "So let's consider minimization of a quadratic objective with L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "The true solution is at my.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one zero, indicated by this star and we see here the level curves of L1 regularization superimposed on the level curves of our regularization function.",
                    "label": 0
                },
                {
                    "sent": "So a classical method for solving such problems is the subgradient method, and right here we see the iterates of the subgradient method and you can see it's going to converge to that point.",
                    "label": 0
                },
                {
                    "sent": "But it sort of takes a long time and wanders all over the place and the method I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "This is what happens when we run it on the same problem, so it converges very quickly now.",
                    "label": 0
                },
                {
                    "sent": "Let's talk about why this happens with the subgradient method.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the sub gradient of a function is essentially the generalization of a gradient to a non differentiable function.",
                    "label": 1
                },
                {
                    "sent": "So what the subgradient set at a non differentiable point is is basically the set of tangents to the set of tangent lines to the function at the non differentiable point.",
                    "label": 0
                },
                {
                    "sent": "So it looks something like this.",
                    "label": 1
                },
                {
                    "sent": "It's this whole set of lines that underestimate the function.",
                    "label": 0
                },
                {
                    "sent": "So usually the subgradient method is basically just picking a subgradient direction of your function and going in that direction.",
                    "label": 0
                },
                {
                    "sent": "So why doesn't this work very well?",
                    "label": 0
                },
                {
                    "sent": "The main problem is that sub gradients are basically not informative at the singularity's.",
                    "label": 0
                },
                {
                    "sent": "So here we have a differentiable function in a non differentiable function.",
                    "label": 0
                },
                {
                    "sent": "As we get closer to the minimum we see that the gradient represented by the yellow wine for the differentiable function gets much much smaller as we get close to the minimum of the sub differentiable function nothing changes, so we can't really tell when we're close to the minimum, but these points of non differentiability.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are the minimizing points, but landing on them using a subgradient method is like landing on the head of a needle, and it's very difficult to do so.",
                    "label": 0
                },
                {
                    "sent": "The method I'm going to propose is an attempt to deal with that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just a brief outline.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about the algorithm, the framework we propose and show how to deal with sort of non differentiable, which will allow us to deal efficiently with non differentiable regularization functions.",
                    "label": 0
                },
                {
                    "sent": "I'll give some convergence properties of the algorithm and then I'll show that it's simple to derive algorithms for different types of regularization in our framework and show ways to make it really usable and very high dimensional problems, especially with sparsity.",
                    "label": 0
                },
                {
                    "sent": "Then I'll give some experimental results and.",
                    "label": 1
                },
                {
                    "sent": "Conclusions.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm we propose is called Phobos.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As for forward backward splitting and the overarching goal of the algorithm is to minimize the composite objective like what you see there, we have a loss function, L and regularizer R, and the goal is to deal with non differentiable regularizers.",
                    "label": 0
                },
                {
                    "sent": "So what the method does is that it has two steps and in the first step we take.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An unconstrained gradient of the loss or a stochastic subgradient.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really matter, but we go in the gradient.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the loss and then in the second phase, we're going to do something intelligent to deal with the regularization parameter and the method we use is similar to forward backward splitting which has been around since the 70s and then composite gradient methods recently from Steven Wright and his colleagues in 2009 and messed up in 2007, and at this nips Lindja had a paper on dual averaging with regularization, so these.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithms are all very related.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "It's an active area of research for solving composite functions like this.",
                    "label": 0
                },
                {
                    "sent": "So the first step of our algorithm is simply to take an unconstrained gradient step on the loss or a stochastic subgradient.",
                    "label": 0
                },
                {
                    "sent": "And you've probably all seen this is just simply gradient descent.",
                    "label": 0
                },
                {
                    "sent": "We start at WT and we get to this intermediate point WT plus half on the loss.",
                    "label": 0
                },
                {
                    "sent": "Now the second step is what really makes our algorithm work and sort of the important take home message of this talk and in the second step what we do is we formulate sort of an intermediate objective which is a quadratic term to keep us close to the intermediate point WT plus half which you see right here and then we add the regularizer.",
                    "label": 0
                },
                {
                    "sent": "So essentially what we're doing is we're corrupting for any sort of overstepping we might have done when we took the gradient with the loss.",
                    "label": 0
                },
                {
                    "sent": "So here's a graphic representation we see our last function and our regularizer.",
                    "label": 0
                },
                {
                    "sent": "We add them up to get the true function that we're minimizing, so that's that.",
                    "label": 0
                },
                {
                    "sent": "And now we look at this sort of quadratic function plus regularization, which looks something like.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This sort of at a high level.",
                    "label": 0
                },
                {
                    "sent": "You can think of this as doing prediction and correction for the regularizer or trying to make some sort of quadratic approximation to your loss, but you're trading off between staying close to the intermediate point and minimizing your regularizer.",
                    "label": 0
                },
                {
                    "sent": "So basically the whole game in this algorithm is to find regularizers so that minimizing this function is simple, and often we'll see that that's the case because it's just a quadratic and it's completely separable.",
                    "label": 0
                },
                {
                    "sent": "So that's the that's kind of the one of the main take home messages is that if you can solve this problem, you can minimize these types of functions efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'd like to give a high level kind of picture of what our analysis looks like.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the conditions for optimality of our of our sort of iterative method, you see that zero is in the subgradient set of this function W T + 1 minus WT, plus the subdifferential of our loss at the current point, and then sort of a forward looking subdifferential of the regularizer.",
                    "label": 0
                },
                {
                    "sent": "So we see this equation right here the next iterate.",
                    "label": 0
                },
                {
                    "sent": "Is a linear combination of the previous point the gradient at the previous point, and then we sort of peak ahead to see the gradient of the regularizer at our next step.",
                    "label": 0
                },
                {
                    "sent": "And this is what allows us to push back dealing with this non differentiability.",
                    "label": 0
                },
                {
                    "sent": "Intuitively you just because because we're cheating and looking ahead these non differentiable points don't really affect our algorithm and this equation right here is what makes the analysis go through essentially once we have this we can kind of just follow previous analysis of gradient descent or subgradient descent.",
                    "label": 0
                },
                {
                    "sent": "And get convergence rates for our algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll briefly go over those convergence properties if we're doing batch learning, or say, like empirical risk minimization with regularization, then if we set our step size ADA to be one over root T, we're going to get a convergence rate of one over root tea, meaning after T steps where within one over root tea of the optimal value of the problem.",
                    "label": 0
                },
                {
                    "sent": "We can also do the same thing for online learning.",
                    "label": 0
                },
                {
                    "sent": "Where we can get average regret bounds or regret bounds?",
                    "label": 1
                },
                {
                    "sent": "So for arbitrary convex functions L we get a sequence of them when we want to compete against sort of some static predictor.",
                    "label": 0
                },
                {
                    "sent": "If we take the step sizes to be proportional to one over root T, we get a regret of root tea with strongly convex functions we can do slightly better, which, but these results are these types of results are well known, but we can get them in this composite framework now using our.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two step update.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to go through a series of derived algorithms, so I'm going to show the second step of our Phobos algorithm for three different types of regularization, and you'll see that solving these is actually not that difficult, and it allows us to efficiently solve problems with L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "Two different types of L2 regularization.",
                    "label": 0
                },
                {
                    "sent": "And then we'll extend it to mix norms for structured problems like multi class or multitask problems.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To begin, I'll just describe the L1 problem so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second step of the algorithm, we minimize this quadratic tradeoff between the intermediate point and L1 regularization, so you can see that this is separable.",
                    "label": 0
                },
                {
                    "sent": "And so we just need to minimize for each coordinate individually and basically what we get is this truncated gradient update.",
                    "label": 0
                },
                {
                    "sent": "And this was studied last year at NIPS by John Langford and colleagues, and has been known in the statistics literature for awhile is iterative shrinkage thresholding.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, what we see is that if our intermediate point WT lands within minus Lambda to Lambda, it gets shrunk to 0, which is sort of in this plot we have the X axis is our intermediate point, the Y axis is the resulting.",
                    "label": 0
                },
                {
                    "sent": "Point WT plus one.",
                    "label": 0
                },
                {
                    "sent": "So if we land within this range, we just get truncated to zero, and otherwise you're shrunk by a factor of Lambda times the step size.",
                    "label": 0
                },
                {
                    "sent": "So this you can see that this is quite a simple update and we actually do get zeros when we when we do it with L1 regularization without two, we have two different updates with L2 squared regularization, we actually get a new update which is essentially gradient descent with geometric shrinkage.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of in contrast to recent advances like the Pegasos algorithm does projected gradients.",
                    "label": 0
                },
                {
                    "sent": "So here we don't actually need to do projections because somehow this shrinkage step just kind of pulls us back to some ball which is near the optimal point, and we'll see in the experiments that this works well, so we never actually need to do any types of projections here with just straight L2 regularization, meaning that our regularizer looks sort of like a cone starting at the origin.",
                    "label": 0
                },
                {
                    "sent": "We actually get something very similar to L1.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you look at this update, what you see is that if our intermediate point has two norm less than Lambda times our step size, then this term right here is going to be 0.",
                    "label": 0
                },
                {
                    "sent": "And we're going to update the entire vector to be the all zeros vector.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it's just a sort of shrinkage term times the times the vector.",
                    "label": 0
                },
                {
                    "sent": "So this this update sort of looks like a truncated cone.",
                    "label": 0
                },
                {
                    "sent": "And this is, I mean, if we look back at the previous update for L1, this is very similar, except Now we're just in more dimensions and we do it to the whole vector at once.",
                    "label": 0
                },
                {
                    "sent": "So this is a relatively simple update to implement.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, for structured problems like multi class or multitask lots of times we want to do mixed norm regularization.",
                    "label": 0
                },
                {
                    "sent": "So we have an entire matrix of weights.",
                    "label": 0
                },
                {
                    "sent": "Each column of the matrix corresponds to one of our subproblems, say a task, or represents waits for a particular class, and we actually want to regularize rows of this matrix together in some sort of QM like this.",
                    "label": 0
                },
                {
                    "sent": "So why would?",
                    "label": 0
                },
                {
                    "sent": "Why might we want to do this?",
                    "label": 0
                },
                {
                    "sent": "Because essentially the JF bro is associated with the JS feature of our problem.",
                    "label": 0
                },
                {
                    "sent": "We only want to penalize once, so either you include the feature in your model or you don't include it at all.",
                    "label": 0
                },
                {
                    "sent": "And by putting these these rows and say an Infinity norm which I haven't talked about, but you can also solve that using our framework or or in just a plain two norm.",
                    "label": 0
                },
                {
                    "sent": "Essentially what we get is once one parameters in the model, all the rest of them can come in for free and it's.",
                    "label": 0
                },
                {
                    "sent": "I think fairly clear from the form of the regularization that we can solve this update just using the previous.",
                    "label": 0
                },
                {
                    "sent": "The previous updates on the slides because it's separable.",
                    "label": 0
                },
                {
                    "sent": "So that's that's how we deal with mixed 1 problems now before I get into the experimental results, I'd like to talk about how we can make this algorithm efficient for really high dimensional problem.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At least with sparsity.",
                    "label": 0
                },
                {
                    "sent": "So imagine you're solving, say, a text application and NLP application.",
                    "label": 0
                },
                {
                    "sent": "You're sort of the dimension of your space is huge in the millions, say, but you have in any given sort of example you have only a few non 0 features.",
                    "label": 0
                },
                {
                    "sent": "So for example, if this is our gradient at time steps 12345 you see, sorry AT23 four and five, you see that the second parameter, the second entry of the gradient, is 0.",
                    "label": 0
                },
                {
                    "sent": "So intuitively we would like to be able to do lazy updates and not have to deal with these these zero entries.",
                    "label": 0
                },
                {
                    "sent": "So what are we going to do?",
                    "label": 0
                },
                {
                    "sent": "You know, the input space is sparse, but it's huge and we want to be.",
                    "label": 0
                },
                {
                    "sent": "We don't want to have to perform an update every time on our vector W, even though we're not seeing any entries to the gradient.",
                    "label": 0
                },
                {
                    "sent": "So this proposition actually saves us from that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It allows us to do really efficient high dimensional learning so.",
                    "label": 0
                },
                {
                    "sent": "The following to sort of.",
                    "label": 0
                },
                {
                    "sent": "Basically, sequences of steps are equivalent, so we can either solve the update to get our next parameter WT at every time step every time we get to 0, we just do the update or we can aggregate our regularization parameters into this and just sum them up and solve the update once when we actually need the parameter.",
                    "label": 0
                },
                {
                    "sent": "So this is very clearly much more efficient than this, and this proposition is true for Q = 1 to Infinity, which is really all we need for the applications we have in mind.",
                    "label": 0
                },
                {
                    "sent": "But this really will allow us to do really efficient, very high dimensional solve.",
                    "label": 0
                },
                {
                    "sent": "Very high dimensional problems.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now, under the experimental results.",
                    "label": 1
                },
                {
                    "sent": "First, I'd like to talk about whether we can actually get the sparsity 'cause you remember.",
                    "label": 0
                },
                {
                    "sent": "This is one of our goals to begin with was to deal with these non differentiable points, so this is each of these curves.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the sparsity level of our weight vector W as a function of the number of steps that are.",
                    "label": 0
                },
                {
                    "sent": "The Phobos algorithm has taken for solving an L1 regularised logistic regression problem and each line corresponds to a different regularization value.",
                    "label": 0
                },
                {
                    "sent": "And you see that after about 30 steps, say we've essentially settled into the kind of sparsity pattern sparsity proportion that we're going to have for the rest of the problem.",
                    "label": 0
                },
                {
                    "sent": "So we very quickly get to the right sparsity level.",
                    "label": 0
                },
                {
                    "sent": "So it seems we're doing the right thing.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This is for a deterministic problem.",
                    "label": 0
                },
                {
                    "sent": "When you're using stochastic gradient information, it's much more spiky.",
                    "label": 0
                },
                {
                    "sent": "It's not quite as clean, so that's an area that we're looking into figuring out how to deal with, but it does seem that we can handle these non differentiable regularizers.",
                    "label": 0
                },
                {
                    "sent": "So I also mentioned that we are able to do efficient very high dimensional learning, so this is an example of the amount of time it takes to add a sparse vector with S nonzero components to a vector and then either project it to an L1 ball.",
                    "label": 0
                },
                {
                    "sent": "Which of these black lines or perform the Phobos lazy update to it?",
                    "label": 0
                },
                {
                    "sent": "So last year I smell you.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Singer in shadow shorts and I had a paper describing how to efficiently project to an L1 ball under sparse updates.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is very complicated and as we can see from this, these three lines here, it's much much lower than Phobos, so.",
                    "label": 0
                },
                {
                    "sent": "If we look at sort of the dimension State 10,000,000, which is around here, every projection step is going to take something like 6 or 7 seconds, which is really slow for a gradient method whereas photo.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even in the slowest cases, taking much less than half a second, so we're getting a factor of more than 10 improvement in these high dimensional problems.",
                    "label": 0
                },
                {
                    "sent": "We have sparse regularizers, which is very significant.",
                    "label": 0
                },
                {
                    "sent": "Foremost, we also also described the L2 update or with L2 squared regularization.",
                    "label": 0
                },
                {
                    "sent": "So this is just showing our convergence compared to Pegasus, which is a recent algorithm for solving SVM's.",
                    "label": 0
                },
                {
                    "sent": "We can see that the performance is quite similar.",
                    "label": 0
                },
                {
                    "sent": "We don't have to do a projection step and I guess we're very slightly faster, but essentially very close to this.",
                    "label": 0
                },
                {
                    "sent": "To the Pegasos algorithm.",
                    "label": 0
                },
                {
                    "sent": "And now I'll talk about some sort of.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Higher dimension problems.",
                    "label": 0
                },
                {
                    "sent": "So in this experiment we are comparing.",
                    "label": 0
                },
                {
                    "sent": "Stochastic gradient Phobos, 22 algorithms designed to handle mixed norm problems and we're solving the 10 class amnist problem attend class digit classification problem.",
                    "label": 0
                },
                {
                    "sent": "There are 60,000 training examples.",
                    "label": 0
                },
                {
                    "sent": "Each of the 10 classes has 3000 features, so there's 30,000 features and were regularly we're doing black regularization, so each feature is regularised in blocks of 10 sparsa, and this coordinate descent method were specifically designed to handle these types of mixed norm block regularization problems.",
                    "label": 0
                },
                {
                    "sent": "And so this is.",
                    "label": 0
                },
                {
                    "sent": "This is sort of a zoomed out view of our test error rate as a function of runtime and you can see that stochastic fobos does.",
                    "label": 1
                },
                {
                    "sent": "Decreased much more quickly, so let's zoom in a little bit.",
                    "label": 0
                },
                {
                    "sent": "And this point is that.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time that sparser finishes its first iteration, and so essentially by the time these other algorithms have even finished one iteration.",
                    "label": 0
                },
                {
                    "sent": "Focus is converge to the minimum of the function.",
                    "label": 0
                },
                {
                    "sent": "And the other algorithms, I mean they catch up eventually about, you know an hour later, but focus in this case for these stochastic type problems with many parameters and lots and lots of data, we see as much, much faster.",
                    "label": 0
                },
                {
                    "sent": "So just a couple of conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I've given is a general framework for stochastic online batch optimization with regularization at least regularly with many different kinds of regularization.",
                    "label": 1
                },
                {
                    "sent": "We've shown that you can do mixed on regularization for multi class multi task problems, and there were efficient and really high dimensions.",
                    "label": 0
                },
                {
                    "sent": "We have lazy updates to handle the case.",
                    "label": 0
                },
                {
                    "sent": "Some future work that we in a few other groups are working on are putting different structural problems.",
                    "label": 0
                },
                {
                    "sent": "Sorry, different structural assumptions of our problem into the regularization.",
                    "label": 0
                },
                {
                    "sent": "And finding efficient updates for those cases.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "That's all I have to say, so thanks very much for listening.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "Well, that was really well.",
                    "label": 0
                },
                {
                    "sent": "So that's a question for the stochastic case.",
                    "label": 0
                },
                {
                    "sent": "Do you have any ideas how to prove anything but convergence to the sparsity pattern that you will eventually converge to?",
                    "label": 0
                },
                {
                    "sent": "And you mentioned it spiky, but you didn't actually.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in the stochastic case, you know I don't want to say you guys snake oil, but it is not as clean as this and I'm not quite sure how to deal with that.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned Lindow's dual averaging paper and I think that deals very well with the problem at least in practice.",
                    "label": 0
                },
                {
                    "sent": "But I don't have any great ideas for how to sort of formally argue that one or the other will actually converge the right sparsity.",
                    "label": 0
                }
            ]
        }
    }
}