{
    "id": "gwpohcgmnq5nfc34i4hpmyt5mgy3llwp",
    "title": "Multitask Multiple Kernel Learning (MT-MKL)",
    "info": {
        "author": [
            "Christian Widmer, T\u00fcbingen University"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Multi-Task Learning",
            "Top->Computer Science->Bioinformatics"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_widmer_mmk/",
    "segmentation": [
        [
            "So we try to use MKL to improve multi multi task learning.",
            "And essentially so this was by this was motivated by problems from the informatics.",
            "OK, so as you can see this joint work with no lights.",
            "So yes, we are too and my supervisor which."
        ],
        [
            "Now it's probably all of you know main bottleneck in supervised learning is often insufficient training data.",
            "So if you have access to information from several domains, so for instance in terms of by informatics, if you have data from different organisms, you know that and you ask the same same questions, then you know that these data are different, but may be related enough for you to benefit.",
            "So what is commonly done in multitask learning is.",
            "Enforce a coupling between the different parameter vectors and in order to do that, you need to know the structure between your tasks.",
            "So in this."
        ],
        [
            "Project we try to use MKL to infer or refined structure between our different tasks.",
            "In order to do so for one push performance and some other gain for the other."
        ],
        [
            "Gain some insight into the problem domain.",
            "Now to start off, so I'll present a framework presented in 2004, but even you and poncho.",
            "And so this is commonly used in multi task learning in context of PHMSA an it's essentially so one of the formulations which I'm showing here.",
            "So you essentially obtain parameter vectors W one through T for all of your T tasks, and while regularising each of those individually, you also penalize the deviation from the mean vector.",
            "So which is W 0 the average vector of those Tees at those W?",
            "Sorry.",
            "And this."
        ],
        [
            "Leads to a falling dual formulation which so essentially corresponds to having a kernel."
        ],
        [
            "Matrix based on all examples.",
            "So all of the examples from all the tasks and a kernel matrix that essentially takes into account only the similarities between examples from the same task.",
            "So while of this you can think of a block diagonal matrix where the individual blocks correspond to the comparisons between examples from the same task.",
            "Now this in this you can already see that this this component here captures somewhat the Inter task interactions while the component on the right is restricted to the intra task interactions, so.",
            "As straightforward extension of this ad is adding weights here in order to trade off is sometimes you might be if your tasks are more similar, you might be more interested in getting a more global model.",
            "If your task very different.",
            "This more this model focused on the individual tasks might do better.",
            "So from this slide you can already see the basic idea behind this, where you have a combination of kernels, which was the kernels capture different parts of interactions between your tasks.",
            "But of course in this.",
            "In this setting, having only two kernels.",
            "I suppose it wouldn't make sense to use multiple kernel learning, so we have to generalize this a bit."
        ],
        [
            "And to do this, I'll just briefly introduce the concept of me to task, which is really a straightforward.",
            "So you have in this case for tasks or a number of tasks.",
            "And the idea is that, say, if your tasks share some property which based on which you can assume that it's beneficial to share to somehow coupled the parameter vectors of these tasks for multi task learning.",
            "Then you pull them in a what we call a meter task, which is just a subset of tasks.",
            "And so because these different task."
        ],
        [
            "This might have different level of levels of similarity.",
            "You can do this for for several."
        ],
        [
            "Meter tasks so basically by having this set.",
            "Of meta tasks.",
            "So the things I call S here.",
            "You'll get a definition of the structure of your multitask learning scenario.",
            "Now how do?"
        ],
        [
            "Use this to generalize this.",
            "What I've showed you on the last slide.",
            "So for this we define a kernel with respect to a meter tasks as follows.",
            "So starting with this full kernel matrix.",
            "We will keep the original with respect to submit a task S sorry, so we keep the original entry if.",
            "Examples are both from tasks which are contained in the meter task S. So to illustrate what I mean by this.",
            "Consider again the example from the last slide.",
            "So this meter task contains the all of the four tasks, and obviously this corresponds to the full kernel matrix.",
            "Now if we."
        ],
        [
            "If we select the subset of task one and test two and use this formulation, then we get zeros here and keep this block and correspondingly for S3 we."
        ],
        [
            "Get this kernel matrix here now.",
            "Here you can already see the basic motivation of this.",
            "So essentially we want to.",
            "Take into account structure that we have about groupings of tasks, which groups them into.",
            "More or less similar groups.",
            "And this gives us rise.",
            "This gives rise to this linear combination of kernel matrices according to this definition, and the weights of these, we want to learn through MCL, and for this we really so in this sense."
        ],
        [
            "Really, an application of QM KL?",
            "And so basically, given this collection of meta tasks which goes in here, we just use directly use Q norm MCL presented by Mario's adult Claudia and to learn these weights of the injured individual media tasks.",
            "And so a nice feature is that in some in some settings say if we have, if we are quite confident about just the structures that we put in a PIN issue, then we wouldn't want.",
            "We wouldn't expect to.",
            "Two wouldn't seek as sparse waiting of these groups.",
            "On the contrary, if we, if we're not very confident about the structure that we put in, we might seek a solution where most of these weights are set to 0.",
            "So this Q parameter lets us control this, and that's quite a nice feature which is useful in the."
        ],
        [
            "Setting.",
            "So I've told you how to, so how we use Q MCL to optimize this?",
            "So maybe 1 remaining question is how do we define this collection of meter tasks?",
            "And in absence of any prior knowledge, one thing you can do obviously is take consider."
        ],
        [
            "The power set.",
            "So the all possible subsets of your tasks and of course so this gives you a large number of meter tasks and most groups will not capture any useful information.",
            "So in this setting would really expect too that learning sparse weights makes sense.",
            "And if we by learning sparse weights on this different meter tasks, we we hope to identify meaningful structure that identifies groups which are beneficial in a multi task setting.",
            "So of course.",
            "This is not so.",
            "This is computationally very expensive, so we get many meter tasks and to overcome this we can of course now start to incorporate additional."
        ],
        [
            "Structure if we have access to it.",
            "So in by Infomatics, a structure can get like.",
            "This is also often in form of a taxonomy.",
            "So a taxonomy is a hierarchical structure, where in our case the tasks are attached to the leaf nodes and in so in the setting that I'm personally interested in in by infomatics, this often corresponds to having a tree of different organisms, and this and you know that these guys are more related to each other than the four.",
            "OK, so if we now use this taxonomy to to define this collection of meta tasks, we essentially look at each node and group and define the 1 meter task as all leaves as descendants of that particular node.",
            "And what I've shown you here is really just three examples of these nodes.",
            "So the root node you see the full block again, the left inner node, which correspond to those two tasks.",
            "And of course a singular task.",
            "But you need to do this for all of the remaining nodes nodes as well.",
            "So.",
            "By putting in this prior knowledge, you use some reasonable structure to start with an.",
            "Intuitively, you'd be interested in maybe not learning a sparse waiting on this, because you hope that the groupings are meaningful to start with, but rather you want to somehow refine the contributions of the individual groups.",
            "So so much for the.",
            "So now I get to the experimental section as actually start with this approach where we assumed to be given a takes a hierarchy of taxonomy.",
            "And."
        ],
        [
            "This in the context of splice site recognition, which is an important problem in sequence biology.",
            "And So what you see here on the left are.",
            "AR15 organisms, all of which were all of which we have data and as you can see they are related by this given taxonomy.",
            "Now this taxonomy we used to define this this collection of meta tasks.",
            "And then successively use MKL to tune the weights of these individual meter tasks.",
            "Now the ones I circled in red are shown here on the right.",
            "So these are the results for some baseline methods and multitask MKL method for different norms.",
            "And so, as baselines are fused, three free methods, and so plane for plane, you get an individual classifier for each of the nodes.",
            "So you don't do transfer learning at all.",
            "For union, you get one global classifier.",
            "That takes into account all information.",
            "And for vanilla we essentially did the definition of kernel matrices the same way As for.",
            "To get this combination of kernels, but we haven't learned the weights, so just left them at at one OK. Now, in addition to this we tried this multi multi task multiple kernel approach using different norms.",
            "And so, in particular, we looked at Q12 and three norms, and as I said before, we.",
            "Here we would expect that these groupings already contain some some meaningful structure, so we wouldn't expect too.",
            "That sparse weights are desirable here, and this you can actually observe."
        ],
        [
            "The results so plainness are preceded by by this one arm, empty MCL, superseded by Union, where you get a global model vanilla where you do this combination but leaf always at one and the best performing one.",
            "So you see the mean on the right.",
            "Sorry are.",
            "MTM kelfer norms two and three so non sparse norms that Norm said lead to a non sparse solution.",
            "OK, as a second application, we looked at another problem from bioinformatics and here we did not assume any structure in advance about our tasks.",
            "And the problem is again, so the problem here is you want to predict whether certain peptide binds to a protein or not and you have.",
            "So the multi test setting comes in because you have actually 6 shown 4.",
            "Six of these different molecules and these molecules are some more different to each other than others.",
            "But we not do not assume to have any knowledge about this now.",
            "So we use the power space based approach and use one norm.",
            "And multi task MCL to identify the structure.",
            "Now you see the results.",
            "I mean you get."
        ],
        [
            "A bit better, maybe not too much, but it be interesting to see whether you are also identifying some reasonable structure doing this.",
            "And so one thing I didn't tell you yet was that so for each of these proteins we do have access to a biological sequence.",
            "So you can think of you have a string that corresponds to each of these things, and if you compare the strings and based on this get some similarity measure you.",
            "Get a task similarity matrix that you can compare to what you've learned through as."
        ],
        [
            "Through this power set empty MKL.",
            "And so that's what I'll show you on my last results slide.",
            "So what you see here on the left?",
            "Is the similarity measure you get from comparing the biological sequences that correspond to each of those each of those proteins?",
            "And you can see that.",
            "OK, well, you have.",
            "Clearly you have some clusters here, so these three seem to be quite similar to each other, and these three seem to be quite similar to each other.",
            "While there's some rough similarity, I guess between this guy and the three others.",
            "And what you see here on the right is the task similarities we get from the weights that we learned using this sparse combination of multi of meat as kernels.",
            "And essentially so basically what how I get these weights is so if I have a pair of tasks inj, I basically sum up the weights of all meta tags that correspond both that contain both of these."
        ],
        [
            "Tasks, so that's how I get to it to this similarity.",
            "And so these are really entirely different measures.",
            "But still you can see that you actually identify."
        ],
        [
            "Some common structures, so I would say this this three by three block.",
            "You actually identify quite well and also down here you see some so you identify this two by two by two block.",
            "But there are also some differences, but to conclude on this, we think that we actually succeed to identify some biological meaningful structure using this approach."
        ],
        [
            "OK, so that was basically it.",
            "To sum up, so we've presented the idea of.",
            "The concept of meter tasks an using MKL to to learn awaiting of these meter tasks and we've seen what to do in context of hierarchies, which is in particularly relevant to buy informatics that where you often deal with several organisms.",
            "We've seen what to do if if you don't have any prior knowledge about structure, so you can consider the powerset of of of tasks.",
            "And so I've showed you some applications of these concepts to a problem by informatics.",
            "So maybe as a future outlook.",
            "So one thing that obviously has to be done is finding a more efficient optimization strategy for this power set based approach.",
            "Maybe that's you somehow on the fly compute different GNU GNU GNU subsets, but so this is so we haven't really figured this one out yet, and also they had to have been really interesting talks about new directions and MCL here, so I'm sure maybe some of these can be used to improve this project further.",
            "So with this."
        ],
        [
            "I'd like to thank my collaborators, colleagues and supervisors, and also of course I'd like to thank the organizers of this workshop for making this possible."
        ],
        [
            "And Lastly, I'd like to thank you for your kind attention.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we try to use MKL to improve multi multi task learning.",
                    "label": 0
                },
                {
                    "sent": "And essentially so this was by this was motivated by problems from the informatics.",
                    "label": 0
                },
                {
                    "sent": "OK, so as you can see this joint work with no lights.",
                    "label": 0
                },
                {
                    "sent": "So yes, we are too and my supervisor which.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now it's probably all of you know main bottleneck in supervised learning is often insufficient training data.",
                    "label": 1
                },
                {
                    "sent": "So if you have access to information from several domains, so for instance in terms of by informatics, if you have data from different organisms, you know that and you ask the same same questions, then you know that these data are different, but may be related enough for you to benefit.",
                    "label": 1
                },
                {
                    "sent": "So what is commonly done in multitask learning is.",
                    "label": 0
                },
                {
                    "sent": "Enforce a coupling between the different parameter vectors and in order to do that, you need to know the structure between your tasks.",
                    "label": 1
                },
                {
                    "sent": "So in this.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Project we try to use MKL to infer or refined structure between our different tasks.",
                    "label": 0
                },
                {
                    "sent": "In order to do so for one push performance and some other gain for the other.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gain some insight into the problem domain.",
                    "label": 0
                },
                {
                    "sent": "Now to start off, so I'll present a framework presented in 2004, but even you and poncho.",
                    "label": 0
                },
                {
                    "sent": "And so this is commonly used in multi task learning in context of PHMSA an it's essentially so one of the formulations which I'm showing here.",
                    "label": 0
                },
                {
                    "sent": "So you essentially obtain parameter vectors W one through T for all of your T tasks, and while regularising each of those individually, you also penalize the deviation from the mean vector.",
                    "label": 0
                },
                {
                    "sent": "So which is W 0 the average vector of those Tees at those W?",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "And this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Leads to a falling dual formulation which so essentially corresponds to having a kernel.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matrix based on all examples.",
                    "label": 0
                },
                {
                    "sent": "So all of the examples from all the tasks and a kernel matrix that essentially takes into account only the similarities between examples from the same task.",
                    "label": 0
                },
                {
                    "sent": "So while of this you can think of a block diagonal matrix where the individual blocks correspond to the comparisons between examples from the same task.",
                    "label": 0
                },
                {
                    "sent": "Now this in this you can already see that this this component here captures somewhat the Inter task interactions while the component on the right is restricted to the intra task interactions, so.",
                    "label": 0
                },
                {
                    "sent": "As straightforward extension of this ad is adding weights here in order to trade off is sometimes you might be if your tasks are more similar, you might be more interested in getting a more global model.",
                    "label": 0
                },
                {
                    "sent": "If your task very different.",
                    "label": 0
                },
                {
                    "sent": "This more this model focused on the individual tasks might do better.",
                    "label": 0
                },
                {
                    "sent": "So from this slide you can already see the basic idea behind this, where you have a combination of kernels, which was the kernels capture different parts of interactions between your tasks.",
                    "label": 0
                },
                {
                    "sent": "But of course in this.",
                    "label": 0
                },
                {
                    "sent": "In this setting, having only two kernels.",
                    "label": 0
                },
                {
                    "sent": "I suppose it wouldn't make sense to use multiple kernel learning, so we have to generalize this a bit.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to do this, I'll just briefly introduce the concept of me to task, which is really a straightforward.",
                    "label": 1
                },
                {
                    "sent": "So you have in this case for tasks or a number of tasks.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that, say, if your tasks share some property which based on which you can assume that it's beneficial to share to somehow coupled the parameter vectors of these tasks for multi task learning.",
                    "label": 0
                },
                {
                    "sent": "Then you pull them in a what we call a meter task, which is just a subset of tasks.",
                    "label": 1
                },
                {
                    "sent": "And so because these different task.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This might have different level of levels of similarity.",
                    "label": 0
                },
                {
                    "sent": "You can do this for for several.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Meter tasks so basically by having this set.",
                    "label": 0
                },
                {
                    "sent": "Of meta tasks.",
                    "label": 0
                },
                {
                    "sent": "So the things I call S here.",
                    "label": 0
                },
                {
                    "sent": "You'll get a definition of the structure of your multitask learning scenario.",
                    "label": 0
                },
                {
                    "sent": "Now how do?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use this to generalize this.",
                    "label": 0
                },
                {
                    "sent": "What I've showed you on the last slide.",
                    "label": 0
                },
                {
                    "sent": "So for this we define a kernel with respect to a meter tasks as follows.",
                    "label": 0
                },
                {
                    "sent": "So starting with this full kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "We will keep the original with respect to submit a task S sorry, so we keep the original entry if.",
                    "label": 0
                },
                {
                    "sent": "Examples are both from tasks which are contained in the meter task S. So to illustrate what I mean by this.",
                    "label": 0
                },
                {
                    "sent": "Consider again the example from the last slide.",
                    "label": 0
                },
                {
                    "sent": "So this meter task contains the all of the four tasks, and obviously this corresponds to the full kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "Now if we.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we select the subset of task one and test two and use this formulation, then we get zeros here and keep this block and correspondingly for S3 we.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get this kernel matrix here now.",
                    "label": 1
                },
                {
                    "sent": "Here you can already see the basic motivation of this.",
                    "label": 0
                },
                {
                    "sent": "So essentially we want to.",
                    "label": 0
                },
                {
                    "sent": "Take into account structure that we have about groupings of tasks, which groups them into.",
                    "label": 0
                },
                {
                    "sent": "More or less similar groups.",
                    "label": 0
                },
                {
                    "sent": "And this gives us rise.",
                    "label": 1
                },
                {
                    "sent": "This gives rise to this linear combination of kernel matrices according to this definition, and the weights of these, we want to learn through MCL, and for this we really so in this sense.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really, an application of QM KL?",
                    "label": 0
                },
                {
                    "sent": "And so basically, given this collection of meta tasks which goes in here, we just use directly use Q norm MCL presented by Mario's adult Claudia and to learn these weights of the injured individual media tasks.",
                    "label": 0
                },
                {
                    "sent": "And so a nice feature is that in some in some settings say if we have, if we are quite confident about just the structures that we put in a PIN issue, then we wouldn't want.",
                    "label": 0
                },
                {
                    "sent": "We wouldn't expect to.",
                    "label": 0
                },
                {
                    "sent": "Two wouldn't seek as sparse waiting of these groups.",
                    "label": 0
                },
                {
                    "sent": "On the contrary, if we, if we're not very confident about the structure that we put in, we might seek a solution where most of these weights are set to 0.",
                    "label": 0
                },
                {
                    "sent": "So this Q parameter lets us control this, and that's quite a nice feature which is useful in the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Setting.",
                    "label": 0
                },
                {
                    "sent": "So I've told you how to, so how we use Q MCL to optimize this?",
                    "label": 0
                },
                {
                    "sent": "So maybe 1 remaining question is how do we define this collection of meter tasks?",
                    "label": 0
                },
                {
                    "sent": "And in absence of any prior knowledge, one thing you can do obviously is take consider.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The power set.",
                    "label": 0
                },
                {
                    "sent": "So the all possible subsets of your tasks and of course so this gives you a large number of meter tasks and most groups will not capture any useful information.",
                    "label": 0
                },
                {
                    "sent": "So in this setting would really expect too that learning sparse weights makes sense.",
                    "label": 0
                },
                {
                    "sent": "And if we by learning sparse weights on this different meter tasks, we we hope to identify meaningful structure that identifies groups which are beneficial in a multi task setting.",
                    "label": 1
                },
                {
                    "sent": "So of course.",
                    "label": 0
                },
                {
                    "sent": "This is not so.",
                    "label": 0
                },
                {
                    "sent": "This is computationally very expensive, so we get many meter tasks and to overcome this we can of course now start to incorporate additional.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Structure if we have access to it.",
                    "label": 0
                },
                {
                    "sent": "So in by Infomatics, a structure can get like.",
                    "label": 0
                },
                {
                    "sent": "This is also often in form of a taxonomy.",
                    "label": 0
                },
                {
                    "sent": "So a taxonomy is a hierarchical structure, where in our case the tasks are attached to the leaf nodes and in so in the setting that I'm personally interested in in by infomatics, this often corresponds to having a tree of different organisms, and this and you know that these guys are more related to each other than the four.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we now use this taxonomy to to define this collection of meta tasks, we essentially look at each node and group and define the 1 meter task as all leaves as descendants of that particular node.",
                    "label": 0
                },
                {
                    "sent": "And what I've shown you here is really just three examples of these nodes.",
                    "label": 0
                },
                {
                    "sent": "So the root node you see the full block again, the left inner node, which correspond to those two tasks.",
                    "label": 0
                },
                {
                    "sent": "And of course a singular task.",
                    "label": 0
                },
                {
                    "sent": "But you need to do this for all of the remaining nodes nodes as well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "By putting in this prior knowledge, you use some reasonable structure to start with an.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, you'd be interested in maybe not learning a sparse waiting on this, because you hope that the groupings are meaningful to start with, but rather you want to somehow refine the contributions of the individual groups.",
                    "label": 0
                },
                {
                    "sent": "So so much for the.",
                    "label": 0
                },
                {
                    "sent": "So now I get to the experimental section as actually start with this approach where we assumed to be given a takes a hierarchy of taxonomy.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This in the context of splice site recognition, which is an important problem in sequence biology.",
                    "label": 0
                },
                {
                    "sent": "And So what you see here on the left are.",
                    "label": 0
                },
                {
                    "sent": "AR15 organisms, all of which were all of which we have data and as you can see they are related by this given taxonomy.",
                    "label": 0
                },
                {
                    "sent": "Now this taxonomy we used to define this this collection of meta tasks.",
                    "label": 1
                },
                {
                    "sent": "And then successively use MKL to tune the weights of these individual meter tasks.",
                    "label": 0
                },
                {
                    "sent": "Now the ones I circled in red are shown here on the right.",
                    "label": 0
                },
                {
                    "sent": "So these are the results for some baseline methods and multitask MKL method for different norms.",
                    "label": 0
                },
                {
                    "sent": "And so, as baselines are fused, three free methods, and so plane for plane, you get an individual classifier for each of the nodes.",
                    "label": 0
                },
                {
                    "sent": "So you don't do transfer learning at all.",
                    "label": 0
                },
                {
                    "sent": "For union, you get one global classifier.",
                    "label": 0
                },
                {
                    "sent": "That takes into account all information.",
                    "label": 0
                },
                {
                    "sent": "And for vanilla we essentially did the definition of kernel matrices the same way As for.",
                    "label": 0
                },
                {
                    "sent": "To get this combination of kernels, but we haven't learned the weights, so just left them at at one OK. Now, in addition to this we tried this multi multi task multiple kernel approach using different norms.",
                    "label": 0
                },
                {
                    "sent": "And so, in particular, we looked at Q12 and three norms, and as I said before, we.",
                    "label": 0
                },
                {
                    "sent": "Here we would expect that these groupings already contain some some meaningful structure, so we wouldn't expect too.",
                    "label": 0
                },
                {
                    "sent": "That sparse weights are desirable here, and this you can actually observe.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The results so plainness are preceded by by this one arm, empty MCL, superseded by Union, where you get a global model vanilla where you do this combination but leaf always at one and the best performing one.",
                    "label": 0
                },
                {
                    "sent": "So you see the mean on the right.",
                    "label": 0
                },
                {
                    "sent": "Sorry are.",
                    "label": 0
                },
                {
                    "sent": "MTM kelfer norms two and three so non sparse norms that Norm said lead to a non sparse solution.",
                    "label": 0
                },
                {
                    "sent": "OK, as a second application, we looked at another problem from bioinformatics and here we did not assume any structure in advance about our tasks.",
                    "label": 0
                },
                {
                    "sent": "And the problem is again, so the problem here is you want to predict whether certain peptide binds to a protein or not and you have.",
                    "label": 0
                },
                {
                    "sent": "So the multi test setting comes in because you have actually 6 shown 4.",
                    "label": 0
                },
                {
                    "sent": "Six of these different molecules and these molecules are some more different to each other than others.",
                    "label": 0
                },
                {
                    "sent": "But we not do not assume to have any knowledge about this now.",
                    "label": 0
                },
                {
                    "sent": "So we use the power space based approach and use one norm.",
                    "label": 0
                },
                {
                    "sent": "And multi task MCL to identify the structure.",
                    "label": 0
                },
                {
                    "sent": "Now you see the results.",
                    "label": 0
                },
                {
                    "sent": "I mean you get.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A bit better, maybe not too much, but it be interesting to see whether you are also identifying some reasonable structure doing this.",
                    "label": 0
                },
                {
                    "sent": "And so one thing I didn't tell you yet was that so for each of these proteins we do have access to a biological sequence.",
                    "label": 0
                },
                {
                    "sent": "So you can think of you have a string that corresponds to each of these things, and if you compare the strings and based on this get some similarity measure you.",
                    "label": 0
                },
                {
                    "sent": "Get a task similarity matrix that you can compare to what you've learned through as.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Through this power set empty MKL.",
                    "label": 0
                },
                {
                    "sent": "And so that's what I'll show you on my last results slide.",
                    "label": 0
                },
                {
                    "sent": "So what you see here on the left?",
                    "label": 0
                },
                {
                    "sent": "Is the similarity measure you get from comparing the biological sequences that correspond to each of those each of those proteins?",
                    "label": 0
                },
                {
                    "sent": "And you can see that.",
                    "label": 0
                },
                {
                    "sent": "OK, well, you have.",
                    "label": 0
                },
                {
                    "sent": "Clearly you have some clusters here, so these three seem to be quite similar to each other, and these three seem to be quite similar to each other.",
                    "label": 0
                },
                {
                    "sent": "While there's some rough similarity, I guess between this guy and the three others.",
                    "label": 0
                },
                {
                    "sent": "And what you see here on the right is the task similarities we get from the weights that we learned using this sparse combination of multi of meat as kernels.",
                    "label": 0
                },
                {
                    "sent": "And essentially so basically what how I get these weights is so if I have a pair of tasks inj, I basically sum up the weights of all meta tags that correspond both that contain both of these.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tasks, so that's how I get to it to this similarity.",
                    "label": 0
                },
                {
                    "sent": "And so these are really entirely different measures.",
                    "label": 0
                },
                {
                    "sent": "But still you can see that you actually identify.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some common structures, so I would say this this three by three block.",
                    "label": 0
                },
                {
                    "sent": "You actually identify quite well and also down here you see some so you identify this two by two by two block.",
                    "label": 0
                },
                {
                    "sent": "But there are also some differences, but to conclude on this, we think that we actually succeed to identify some biological meaningful structure using this approach.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that was basically it.",
                    "label": 0
                },
                {
                    "sent": "To sum up, so we've presented the idea of.",
                    "label": 0
                },
                {
                    "sent": "The concept of meter tasks an using MKL to to learn awaiting of these meter tasks and we've seen what to do in context of hierarchies, which is in particularly relevant to buy informatics that where you often deal with several organisms.",
                    "label": 0
                },
                {
                    "sent": "We've seen what to do if if you don't have any prior knowledge about structure, so you can consider the powerset of of of tasks.",
                    "label": 0
                },
                {
                    "sent": "And so I've showed you some applications of these concepts to a problem by informatics.",
                    "label": 0
                },
                {
                    "sent": "So maybe as a future outlook.",
                    "label": 0
                },
                {
                    "sent": "So one thing that obviously has to be done is finding a more efficient optimization strategy for this power set based approach.",
                    "label": 1
                },
                {
                    "sent": "Maybe that's you somehow on the fly compute different GNU GNU GNU subsets, but so this is so we haven't really figured this one out yet, and also they had to have been really interesting talks about new directions and MCL here, so I'm sure maybe some of these can be used to improve this project further.",
                    "label": 0
                },
                {
                    "sent": "So with this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'd like to thank my collaborators, colleagues and supervisors, and also of course I'd like to thank the organizers of this workshop for making this possible.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Lastly, I'd like to thank you for your kind attention.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}