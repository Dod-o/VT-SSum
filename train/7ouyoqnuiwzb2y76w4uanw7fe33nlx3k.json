{
    "id": "7ouyoqnuiwzb2y76w4uanw7fe33nlx3k",
    "title": "Combining Bagging and Random Subspaces to Create Better Ensembles",
    "info": {
        "author": [
            "Pan\u010de Panov, Department of Knowledge Technologies, Jo\u017eef Stefan Institute"
        ],
        "published": "Oct. 8, 2007",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Ensemble Methods"
        ]
    },
    "url": "http://videolectures.net/ida07_panov_cbars/",
    "segmentation": [
        [
            "OK, good morning to everybody.",
            "My name is Pancho Pan off and I come from the use of Steven Institute and I'm going to present work done by me and Sasha Jerowski which is entitled combining bagging and random subspaces to create better answer."
        ],
        [
            "Rules.",
            "A brief outline of my talk, so I will give the motivation for this work.",
            "I will give also a brief overview of the different types of randomization methods that are used to construct ansibles like bagging, random subspaces and random forests.",
            "Also, I will present our proposed method with combining bagging and random subspaces present.",
            "I will present the experiments and results and.",
            "At the end I will give a brief summary and some pointers."
        ],
        [
            "No further work.",
            "So.",
            "As we all know, random Forest is one of the best performing Ansible method algorithms.",
            "It derives its strength by using random subsamples of the training data and a randomized base level algorithm which is in this case decision tree algorithm.",
            "Our proposal is to use a similar approach as in random forests to achieve a similar effect.",
            "As combining bagging and random subspaces, the advantages of this method would be that it is applicable to any base level algorithm.",
            "Compared to random forests, which are implemented only for trees and there is no special limit of randomizing the base level algorithm."
        ],
        [
            "Itself.",
            "So.",
            "As we all know.",
            "The main idea in Ensembl methods is to find a set of base level algorithms that are diverse in their decisions and that compliment each other in some way.",
            "So there are different possibilities how this can be achieved.",
            "One of them is by bootstrapping, sampling the training set with which we get.",
            "Training sets which are different.",
            "In the distribution of the examples or we can do a random subset of the feature space in which we generate different training sets that have different features inside and also the other possibilities to use a randomized version of the base level algorithms.",
            "So by introducing some randomization in the algorithm itself, we get different classifiers at the end."
        ],
        [
            "So I will just briefly overview the three methods that I've mentioned, so begging is 1 method that uses Bootstrap sampling with replacement.",
            "So beginning we have a training set and by Bootstrap with replacement we get set of new training sets to which we.",
            "Learn a classifier and at the end we obtain unanswerable of classifiers.",
            "So this method is very useful to use, especially with unstable algorithms, which are decision trees."
        ],
        [
            "For example.",
            "Random subspace method.",
            "Does similar thing, but in the feature space.",
            "So if we have our training set.",
            "With the features that are colored with colors in each new generated training set, we use different subset of the original features which are.",
            "Chosen randomly, so this method, which was introduced by whole in 1998, it's pretty useful to use when we have datasets that have large number of features."
        ],
        [
            "Random Forest algorithm uses the last type of randomization that I mentioned, so it uses randomized randomized version of Decision Tree algorithm.",
            "So it is a particular implementation of begging where each base level algorithm is a random tree.",
            "So in this random tree algorithm in at each node of the decision tree, building the.",
            "The algorithm randomly chooses.",
            "From a subset of features on which nodes on which attributes to split the nodes of the tree.",
            "Also, at the end we get the ansamble of classifiers or anything."
        ],
        [
            "Over trees in this case.",
            "So our proposed methods.",
            "Takes in consideration Bootstrap sampling with replacement.",
            "And also random subset selection.",
            "So and also we named our algorithm sub back.",
            "So at the beginning we."
        ],
        [
            "Half hour training set S."
        ],
        [
            "With the examples within examples, we do Bootstrap sampling with replacement to get.",
            "The bootstrap samples."
        ],
        [
            "In this we obtain for each bootstrap sample we do a random subset base selection, so we choose.",
            "Features randomly from each each."
        ],
        [
            "Training new training set.",
            "With this we obtain.",
            "Our training sets that can be used for a."
        ],
        [
            "Learning task.",
            "We give the training sets."
        ],
        [
            "We're learning the algorithm and at the end, as is the case in before we obtained an symbol."
        ],
        [
            "Off classifiers.",
            "So.",
            "For experiments, we used 19 datasets from the UCI repository and we also use the vector environment for doing the experiments.",
            "More specifically, work experimental was used.",
            "Also we implement it.",
            "Our method in this environment.",
            "Uh.",
            "Is a comparison.",
            "We compared our proposed methods to random subspace method to begging and to random forests and we tested it with three different base level algorithms.",
            "So we tried with J48 decision tree learner J Rip rule learner and one nearest neighbor algorithm.",
            "All experiments were validated by Trenton Fold Cross validation and also compare the results for.",
            "Every data set set separately, we used pairs T test and to compare the overall predictive performance of the algorithms.",
            "We use the Velcro."
        ],
        [
            "Contest.",
            "So at this first slide I present results obtained by using J 48 as a base level learner.",
            "So this table contains comparisons.",
            "Our method is the first one listed.",
            "Comparison of our method.",
            "To all of the others.",
            "The the black bullet means that for that data set, the marked.",
            "Mark the label has significant degradation compared to our method, and the circle means that on a particular data set there is statistically significant improvement compared to our method.",
            "So as as we can briefly note from from the slide.",
            "We see that.",
            "Our method is compatible with random forests.",
            "In some cases.",
            "Performs better than random subspaces and begging."
        ],
        [
            "The next slide presents the results of.",
            "Using J Ripper as a base level algorithm.",
            "In this case.",
            "The notes are the same, so we see that we get compatible performance as random subspaces, and in this case it's also noticeable that begging performs better for some datasets.",
            "And more, more particularly for those datasets that have large number of training examples."
        ],
        [
            "The last, the last slide from this part presents the results of using nearest neighbor algorithm as a base level learner.",
            "In and also in here we we can note that our algorithm is.",
            "Better.",
            "In some cases, then, begging and performs.",
            "Compatible with random subspace is this also?"
        ],
        [
            "Can be noted from the performed the Wilcoxon test on that gives the predictive that shows us the predicted performance overall datasets.",
            "So in the case of J.",
            "48.",
            "We can notice that we are compatible to random forests and we perform better than random subspaces and begging.",
            "And also of course the if we train the base classifier itself.",
            "In the second case.",
            "It is noticeable that our method is overall better than random subspaces.",
            "It performs worse.",
            "Worse, or some in some datasets compareable to begging.",
            "And it is of course better than using the base base algorithm itself.",
            "Similar observation can be can be done for using nearest neighbor.",
            "So in this case we see that we are compatible to random subspaces and perform better for begging and the nearest neighbor algorithm."
        ],
        [
            "Itself.",
            "So as a summary, as I just said that our method is compatible to random forests in case of decision trees we are compatible.",
            "In some cases a bit worse.",
            "Then begging.",
            "And in the case of Jay Reaper and we are Methodist compatible.",
            "In the case of random subspaces and nearest neighbor algorithm is a base."
        ],
        [
            "Is level learner.",
            "As further work we were we are going to investigate the diversity of our induced and symbols and compare these diversity with other mentioned methods and also we plan to use different combinations of bagging and random subspaces like.",
            "Doing bags of random subspaces, an symbols or doing random subspace is an symbols of of bags.",
            "So also we plan to compare.",
            "Back then, symbols of other randomized algorithms, like in, for example, rules, so that was my presentation.",
            "Thank you for listening.",
            "Thank you time for questions.",
            "Please my question concerning your comparison performance, did you take into account a correction from within the testing when you compare yes five classifiers and individually always comparing it to correct the people informally, yeah, for for this test we used corrected T test so.",
            "In that case, it was taken into consideration.",
            "What direction did you?",
            "Can't remember.",
            "Well, I think that's a very important point I'm looking from your experimental design, but only use but all these guys who make comparison of different alternative methods or hybrid.",
            "What is really wrong if you have M greater than two Alternative's algorithm as variants and so and you taken your comparison, you introduce biases and you give their dependencies between the testing.",
            "As you said, your user control on the Alpha values, your tests are not feasable.",
            "Let's absolutely true.",
            "What people should do instead.",
            "Instead of taking the details or the welcome so whatever they have to use test for multiple comparisons, that's absolutely must.",
            "Otherwise you have no chance to be correct.",
            "That is known as statistical theory.",
            "Moreover, one should care about the distribution, which I'm over you have.",
            "Simulation have many cases, sometimes you distribution.",
            "Check that for some journals and have an editor you get skewed distribution and the Wilcoxon tax is not longer.",
            "A very interesting competitor.",
            "And let me at one point more, and I like very much that you tested your approach on.",
            "So several datasets, real and synthetic or whatever.",
            "What we really worries is that nobody tries to classify the data sets which are useful in the sense.",
            "Is it a simple data set like like Flowers or Fisher at that time?",
            "Or is it a really complicated structure, which is of course Layton to everybody.",
            "So you have say not you have a safe trip or sweet fields of interest, and then you know how the argument should it should behave and what are the best way of everyday things are.",
            "So one thing is better testing.",
            "Be careful not only to use at Test or Wilkins Wilcoxon take multiple tests which are relevant statistics, perhaps with some adaptivity which is new and the second thing is.",
            "Take care about the structure of the problems which are involved.",
            "No problems or efficiency for in season performance as well as statistical quality control.",
            "Thank you.",
            "This was more of a comment.",
            "Cauliflower.",
            "Make it quick.",
            "Yeah, but for their first now there's two different things.",
            "The first is screwing up.",
            "Listing is a fellow how.",
            "How many samples do you do you select for the for the bootstrap creating different different bootstrap Bootstrap samples?",
            "How many so?",
            "The algorithm itself.",
            "So for the bootstrap sampling with replacement, you have to at the end get the.",
            "A new data set that has the same size as as the original 1.",
            "I mean how many different things percents?",
            "How many replicates, how many number of huh?",
            "OK, OK, so I tried several variants, so I've added the number of bootstrap samples from 50 to 100.",
            "So in the paper we reported on the.",
            "5050 classifiers 50 bootstrap samples.",
            "So in our further work we also are going to experiment with that parameter also.",
            "On the other.",
            "Relations of the other.",
            "The other selection that you performed, which is the the different features that you are selecting in this case.",
            "We are randomly selected because there are some authors that use some bias.",
            "Based on the correlation that this feature has with their with the classification, no.",
            "In this case it was a really randomly selected yes.",
            "OK, time for one quick question you came here.",
            "Or across just more they come into previous comments and I wonder whether it is really when you design A novel, classify isn't really good idea to compare it to ask many classifiers as possible with are also here because, which means you can think if I compare my new Model 210 thousand establish classifiers when I cannot control my.",
            "Familywise error rate anymore.",
            "I don't know.",
            "Actually, I'd like to be sitting right with the time to call author of the paper.",
            "This is a bit different from a study where compare everything to everything, I mean the comparison was really with one or two other approaches which were very directly relevant in the sense of being derived along the same lines.",
            "So this was not just comparison with any any learning method that you can find in Baker and in that case I found this comment very relevant, but if you just test it against everything then you should be extremely careful here.",
            "We had relatively small relatively small sets of classifiers to compare with.",
            "And you are not allowed to take 5 / 2 pairwise comparisons wrong in statistics, believe me.",
            "But then then you might get into the question of Fender Statical tests are at all applicable to testing the results of machine learning algorithms, senses home you'll get.",
            "Counterexamples to live, maybe.",
            "Yeah, I really use.",
            "There are seven stab Lish procedures in machine learning.",
            "This is a very interesting debate, but I'm afraid that it's big out of scope of sample learning is such.",
            "I mean, I agree with you completely.",
            "I I know what you're trying to say, but we're very into into developing December.",
            "I mean, we're discussing a simple methods and we have to learn from each other.",
            "The computer Sciences have to learn from statistician and vice versa.",
            "That's the best we can do.",
            "Every.",
            "OK, that's it.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, good morning to everybody.",
                    "label": 0
                },
                {
                    "sent": "My name is Pancho Pan off and I come from the use of Steven Institute and I'm going to present work done by me and Sasha Jerowski which is entitled combining bagging and random subspaces to create better answer.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rules.",
                    "label": 0
                },
                {
                    "sent": "A brief outline of my talk, so I will give the motivation for this work.",
                    "label": 0
                },
                {
                    "sent": "I will give also a brief overview of the different types of randomization methods that are used to construct ansibles like bagging, random subspaces and random forests.",
                    "label": 1
                },
                {
                    "sent": "Also, I will present our proposed method with combining bagging and random subspaces present.",
                    "label": 1
                },
                {
                    "sent": "I will present the experiments and results and.",
                    "label": 0
                },
                {
                    "sent": "At the end I will give a brief summary and some pointers.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No further work.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As we all know, random Forest is one of the best performing Ansible method algorithms.",
                    "label": 1
                },
                {
                    "sent": "It derives its strength by using random subsamples of the training data and a randomized base level algorithm which is in this case decision tree algorithm.",
                    "label": 0
                },
                {
                    "sent": "Our proposal is to use a similar approach as in random forests to achieve a similar effect.",
                    "label": 1
                },
                {
                    "sent": "As combining bagging and random subspaces, the advantages of this method would be that it is applicable to any base level algorithm.",
                    "label": 0
                },
                {
                    "sent": "Compared to random forests, which are implemented only for trees and there is no special limit of randomizing the base level algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Itself.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As we all know.",
                    "label": 0
                },
                {
                    "sent": "The main idea in Ensembl methods is to find a set of base level algorithms that are diverse in their decisions and that compliment each other in some way.",
                    "label": 1
                },
                {
                    "sent": "So there are different possibilities how this can be achieved.",
                    "label": 0
                },
                {
                    "sent": "One of them is by bootstrapping, sampling the training set with which we get.",
                    "label": 0
                },
                {
                    "sent": "Training sets which are different.",
                    "label": 0
                },
                {
                    "sent": "In the distribution of the examples or we can do a random subset of the feature space in which we generate different training sets that have different features inside and also the other possibilities to use a randomized version of the base level algorithms.",
                    "label": 0
                },
                {
                    "sent": "So by introducing some randomization in the algorithm itself, we get different classifiers at the end.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will just briefly overview the three methods that I've mentioned, so begging is 1 method that uses Bootstrap sampling with replacement.",
                    "label": 0
                },
                {
                    "sent": "So beginning we have a training set and by Bootstrap with replacement we get set of new training sets to which we.",
                    "label": 1
                },
                {
                    "sent": "Learn a classifier and at the end we obtain unanswerable of classifiers.",
                    "label": 0
                },
                {
                    "sent": "So this method is very useful to use, especially with unstable algorithms, which are decision trees.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "Random subspace method.",
                    "label": 0
                },
                {
                    "sent": "Does similar thing, but in the feature space.",
                    "label": 1
                },
                {
                    "sent": "So if we have our training set.",
                    "label": 0
                },
                {
                    "sent": "With the features that are colored with colors in each new generated training set, we use different subset of the original features which are.",
                    "label": 1
                },
                {
                    "sent": "Chosen randomly, so this method, which was introduced by whole in 1998, it's pretty useful to use when we have datasets that have large number of features.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Random Forest algorithm uses the last type of randomization that I mentioned, so it uses randomized randomized version of Decision Tree algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it is a particular implementation of begging where each base level algorithm is a random tree.",
                    "label": 1
                },
                {
                    "sent": "So in this random tree algorithm in at each node of the decision tree, building the.",
                    "label": 0
                },
                {
                    "sent": "The algorithm randomly chooses.",
                    "label": 0
                },
                {
                    "sent": "From a subset of features on which nodes on which attributes to split the nodes of the tree.",
                    "label": 0
                },
                {
                    "sent": "Also, at the end we get the ansamble of classifiers or anything.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Over trees in this case.",
                    "label": 0
                },
                {
                    "sent": "So our proposed methods.",
                    "label": 0
                },
                {
                    "sent": "Takes in consideration Bootstrap sampling with replacement.",
                    "label": 1
                },
                {
                    "sent": "And also random subset selection.",
                    "label": 0
                },
                {
                    "sent": "So and also we named our algorithm sub back.",
                    "label": 0
                },
                {
                    "sent": "So at the beginning we.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Half hour training set S.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With the examples within examples, we do Bootstrap sampling with replacement to get.",
                    "label": 0
                },
                {
                    "sent": "The bootstrap samples.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this we obtain for each bootstrap sample we do a random subset base selection, so we choose.",
                    "label": 0
                },
                {
                    "sent": "Features randomly from each each.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Training new training set.",
                    "label": 0
                },
                {
                    "sent": "With this we obtain.",
                    "label": 0
                },
                {
                    "sent": "Our training sets that can be used for a.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning task.",
                    "label": 0
                },
                {
                    "sent": "We give the training sets.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're learning the algorithm and at the end, as is the case in before we obtained an symbol.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Off classifiers.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For experiments, we used 19 datasets from the UCI repository and we also use the vector environment for doing the experiments.",
                    "label": 1
                },
                {
                    "sent": "More specifically, work experimental was used.",
                    "label": 0
                },
                {
                    "sent": "Also we implement it.",
                    "label": 0
                },
                {
                    "sent": "Our method in this environment.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Is a comparison.",
                    "label": 0
                },
                {
                    "sent": "We compared our proposed methods to random subspace method to begging and to random forests and we tested it with three different base level algorithms.",
                    "label": 1
                },
                {
                    "sent": "So we tried with J48 decision tree learner J Rip rule learner and one nearest neighbor algorithm.",
                    "label": 0
                },
                {
                    "sent": "All experiments were validated by Trenton Fold Cross validation and also compare the results for.",
                    "label": 0
                },
                {
                    "sent": "Every data set set separately, we used pairs T test and to compare the overall predictive performance of the algorithms.",
                    "label": 0
                },
                {
                    "sent": "We use the Velcro.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Contest.",
                    "label": 0
                },
                {
                    "sent": "So at this first slide I present results obtained by using J 48 as a base level learner.",
                    "label": 0
                },
                {
                    "sent": "So this table contains comparisons.",
                    "label": 0
                },
                {
                    "sent": "Our method is the first one listed.",
                    "label": 0
                },
                {
                    "sent": "Comparison of our method.",
                    "label": 0
                },
                {
                    "sent": "To all of the others.",
                    "label": 0
                },
                {
                    "sent": "The the black bullet means that for that data set, the marked.",
                    "label": 0
                },
                {
                    "sent": "Mark the label has significant degradation compared to our method, and the circle means that on a particular data set there is statistically significant improvement compared to our method.",
                    "label": 1
                },
                {
                    "sent": "So as as we can briefly note from from the slide.",
                    "label": 0
                },
                {
                    "sent": "We see that.",
                    "label": 0
                },
                {
                    "sent": "Our method is compatible with random forests.",
                    "label": 0
                },
                {
                    "sent": "In some cases.",
                    "label": 0
                },
                {
                    "sent": "Performs better than random subspaces and begging.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next slide presents the results of.",
                    "label": 0
                },
                {
                    "sent": "Using J Ripper as a base level algorithm.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "The notes are the same, so we see that we get compatible performance as random subspaces, and in this case it's also noticeable that begging performs better for some datasets.",
                    "label": 0
                },
                {
                    "sent": "And more, more particularly for those datasets that have large number of training examples.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last, the last slide from this part presents the results of using nearest neighbor algorithm as a base level learner.",
                    "label": 0
                },
                {
                    "sent": "In and also in here we we can note that our algorithm is.",
                    "label": 0
                },
                {
                    "sent": "Better.",
                    "label": 0
                },
                {
                    "sent": "In some cases, then, begging and performs.",
                    "label": 0
                },
                {
                    "sent": "Compatible with random subspace is this also?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can be noted from the performed the Wilcoxon test on that gives the predictive that shows us the predicted performance overall datasets.",
                    "label": 0
                },
                {
                    "sent": "So in the case of J.",
                    "label": 0
                },
                {
                    "sent": "48.",
                    "label": 0
                },
                {
                    "sent": "We can notice that we are compatible to random forests and we perform better than random subspaces and begging.",
                    "label": 0
                },
                {
                    "sent": "And also of course the if we train the base classifier itself.",
                    "label": 0
                },
                {
                    "sent": "In the second case.",
                    "label": 0
                },
                {
                    "sent": "It is noticeable that our method is overall better than random subspaces.",
                    "label": 0
                },
                {
                    "sent": "It performs worse.",
                    "label": 0
                },
                {
                    "sent": "Worse, or some in some datasets compareable to begging.",
                    "label": 0
                },
                {
                    "sent": "And it is of course better than using the base base algorithm itself.",
                    "label": 0
                },
                {
                    "sent": "Similar observation can be can be done for using nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "So in this case we see that we are compatible to random subspaces and perform better for begging and the nearest neighbor algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Itself.",
                    "label": 0
                },
                {
                    "sent": "So as a summary, as I just said that our method is compatible to random forests in case of decision trees we are compatible.",
                    "label": 1
                },
                {
                    "sent": "In some cases a bit worse.",
                    "label": 0
                },
                {
                    "sent": "Then begging.",
                    "label": 1
                },
                {
                    "sent": "And in the case of Jay Reaper and we are Methodist compatible.",
                    "label": 1
                },
                {
                    "sent": "In the case of random subspaces and nearest neighbor algorithm is a base.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is level learner.",
                    "label": 0
                },
                {
                    "sent": "As further work we were we are going to investigate the diversity of our induced and symbols and compare these diversity with other mentioned methods and also we plan to use different combinations of bagging and random subspaces like.",
                    "label": 1
                },
                {
                    "sent": "Doing bags of random subspaces, an symbols or doing random subspace is an symbols of of bags.",
                    "label": 0
                },
                {
                    "sent": "So also we plan to compare.",
                    "label": 0
                },
                {
                    "sent": "Back then, symbols of other randomized algorithms, like in, for example, rules, so that was my presentation.",
                    "label": 0
                },
                {
                    "sent": "Thank you for listening.",
                    "label": 0
                },
                {
                    "sent": "Thank you time for questions.",
                    "label": 0
                },
                {
                    "sent": "Please my question concerning your comparison performance, did you take into account a correction from within the testing when you compare yes five classifiers and individually always comparing it to correct the people informally, yeah, for for this test we used corrected T test so.",
                    "label": 0
                },
                {
                    "sent": "In that case, it was taken into consideration.",
                    "label": 0
                },
                {
                    "sent": "What direction did you?",
                    "label": 0
                },
                {
                    "sent": "Can't remember.",
                    "label": 0
                },
                {
                    "sent": "Well, I think that's a very important point I'm looking from your experimental design, but only use but all these guys who make comparison of different alternative methods or hybrid.",
                    "label": 0
                },
                {
                    "sent": "What is really wrong if you have M greater than two Alternative's algorithm as variants and so and you taken your comparison, you introduce biases and you give their dependencies between the testing.",
                    "label": 0
                },
                {
                    "sent": "As you said, your user control on the Alpha values, your tests are not feasable.",
                    "label": 0
                },
                {
                    "sent": "Let's absolutely true.",
                    "label": 0
                },
                {
                    "sent": "What people should do instead.",
                    "label": 0
                },
                {
                    "sent": "Instead of taking the details or the welcome so whatever they have to use test for multiple comparisons, that's absolutely must.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you have no chance to be correct.",
                    "label": 0
                },
                {
                    "sent": "That is known as statistical theory.",
                    "label": 0
                },
                {
                    "sent": "Moreover, one should care about the distribution, which I'm over you have.",
                    "label": 0
                },
                {
                    "sent": "Simulation have many cases, sometimes you distribution.",
                    "label": 0
                },
                {
                    "sent": "Check that for some journals and have an editor you get skewed distribution and the Wilcoxon tax is not longer.",
                    "label": 0
                },
                {
                    "sent": "A very interesting competitor.",
                    "label": 0
                },
                {
                    "sent": "And let me at one point more, and I like very much that you tested your approach on.",
                    "label": 0
                },
                {
                    "sent": "So several datasets, real and synthetic or whatever.",
                    "label": 0
                },
                {
                    "sent": "What we really worries is that nobody tries to classify the data sets which are useful in the sense.",
                    "label": 0
                },
                {
                    "sent": "Is it a simple data set like like Flowers or Fisher at that time?",
                    "label": 0
                },
                {
                    "sent": "Or is it a really complicated structure, which is of course Layton to everybody.",
                    "label": 0
                },
                {
                    "sent": "So you have say not you have a safe trip or sweet fields of interest, and then you know how the argument should it should behave and what are the best way of everyday things are.",
                    "label": 0
                },
                {
                    "sent": "So one thing is better testing.",
                    "label": 0
                },
                {
                    "sent": "Be careful not only to use at Test or Wilkins Wilcoxon take multiple tests which are relevant statistics, perhaps with some adaptivity which is new and the second thing is.",
                    "label": 0
                },
                {
                    "sent": "Take care about the structure of the problems which are involved.",
                    "label": 0
                },
                {
                    "sent": "No problems or efficiency for in season performance as well as statistical quality control.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "This was more of a comment.",
                    "label": 0
                },
                {
                    "sent": "Cauliflower.",
                    "label": 0
                },
                {
                    "sent": "Make it quick.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but for their first now there's two different things.",
                    "label": 0
                },
                {
                    "sent": "The first is screwing up.",
                    "label": 0
                },
                {
                    "sent": "Listing is a fellow how.",
                    "label": 0
                },
                {
                    "sent": "How many samples do you do you select for the for the bootstrap creating different different bootstrap Bootstrap samples?",
                    "label": 0
                },
                {
                    "sent": "How many so?",
                    "label": 0
                },
                {
                    "sent": "The algorithm itself.",
                    "label": 0
                },
                {
                    "sent": "So for the bootstrap sampling with replacement, you have to at the end get the.",
                    "label": 0
                },
                {
                    "sent": "A new data set that has the same size as as the original 1.",
                    "label": 0
                },
                {
                    "sent": "I mean how many different things percents?",
                    "label": 0
                },
                {
                    "sent": "How many replicates, how many number of huh?",
                    "label": 0
                },
                {
                    "sent": "OK, OK, so I tried several variants, so I've added the number of bootstrap samples from 50 to 100.",
                    "label": 0
                },
                {
                    "sent": "So in the paper we reported on the.",
                    "label": 0
                },
                {
                    "sent": "5050 classifiers 50 bootstrap samples.",
                    "label": 0
                },
                {
                    "sent": "So in our further work we also are going to experiment with that parameter also.",
                    "label": 0
                },
                {
                    "sent": "On the other.",
                    "label": 0
                },
                {
                    "sent": "Relations of the other.",
                    "label": 0
                },
                {
                    "sent": "The other selection that you performed, which is the the different features that you are selecting in this case.",
                    "label": 0
                },
                {
                    "sent": "We are randomly selected because there are some authors that use some bias.",
                    "label": 0
                },
                {
                    "sent": "Based on the correlation that this feature has with their with the classification, no.",
                    "label": 0
                },
                {
                    "sent": "In this case it was a really randomly selected yes.",
                    "label": 0
                },
                {
                    "sent": "OK, time for one quick question you came here.",
                    "label": 0
                },
                {
                    "sent": "Or across just more they come into previous comments and I wonder whether it is really when you design A novel, classify isn't really good idea to compare it to ask many classifiers as possible with are also here because, which means you can think if I compare my new Model 210 thousand establish classifiers when I cannot control my.",
                    "label": 0
                },
                {
                    "sent": "Familywise error rate anymore.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Actually, I'd like to be sitting right with the time to call author of the paper.",
                    "label": 0
                },
                {
                    "sent": "This is a bit different from a study where compare everything to everything, I mean the comparison was really with one or two other approaches which were very directly relevant in the sense of being derived along the same lines.",
                    "label": 0
                },
                {
                    "sent": "So this was not just comparison with any any learning method that you can find in Baker and in that case I found this comment very relevant, but if you just test it against everything then you should be extremely careful here.",
                    "label": 0
                },
                {
                    "sent": "We had relatively small relatively small sets of classifiers to compare with.",
                    "label": 0
                },
                {
                    "sent": "And you are not allowed to take 5 / 2 pairwise comparisons wrong in statistics, believe me.",
                    "label": 0
                },
                {
                    "sent": "But then then you might get into the question of Fender Statical tests are at all applicable to testing the results of machine learning algorithms, senses home you'll get.",
                    "label": 0
                },
                {
                    "sent": "Counterexamples to live, maybe.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I really use.",
                    "label": 0
                },
                {
                    "sent": "There are seven stab Lish procedures in machine learning.",
                    "label": 0
                },
                {
                    "sent": "This is a very interesting debate, but I'm afraid that it's big out of scope of sample learning is such.",
                    "label": 0
                },
                {
                    "sent": "I mean, I agree with you completely.",
                    "label": 0
                },
                {
                    "sent": "I I know what you're trying to say, but we're very into into developing December.",
                    "label": 0
                },
                {
                    "sent": "I mean, we're discussing a simple methods and we have to learn from each other.",
                    "label": 0
                },
                {
                    "sent": "The computer Sciences have to learn from statistician and vice versa.",
                    "label": 0
                },
                {
                    "sent": "That's the best we can do.",
                    "label": 0
                },
                {
                    "sent": "Every.",
                    "label": 0
                },
                {
                    "sent": "OK, that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}