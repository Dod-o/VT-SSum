{
    "id": "sxxi6em4ydi2czby7kpbt5yw2ik5dypi",
    "title": "Incorporating Domain Knowledge into Topic Modeling via Dirichlet Forest Priors",
    "info": {
        "author": [
            "David Andrzejewski, Computer Sciences Department, University of Wisconsin-Madison"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning->Dirichlet Processes"
        ]
    },
    "url": "http://videolectures.net/icml09_andrzejewski_idk/",
    "segmentation": [
        [
            "How we can incorporate inject some domain knowledge into the topic modeling process and why we would want to do this, and specifically how we're going to show that this could be done.",
            "So first I'm going to just dive right into.",
            "An motivating example of why we might be interested in doing this.",
            "So if any of you were at nacle a few weeks ago, you may have seen a presentation by my colleague Andrew Goldberg where he discussed a interesting."
        ],
        [
            "Purpose of Wishes submitted on line to the Times Square website in New York City, to be shredded up into confetti and dropped on the Times Square revelers for New Year's.",
            "And so these New Year's wishes, kind of.",
            "I'll give you a little taste of what they're like, so there's the very, very, very popular piece."
        ],
        [
            "On earth.",
            "The less popular but still Culona Brewery."
        ],
        [
            "Get."
        ],
        [
            "Graduate School with."
        ],
        [
            "And for our friends and family.",
            "Um?"
        ],
        [
            "To lose weight and get a boyfriend?"
        ],
        [
            "This person actually got their wish and."
        ],
        [
            "Of course, the granddaddy of them all to win the lottery.",
            "So."
        ],
        [
            "Something that you might typically do when you first get a new interesting corpus like this would be run a statistical unsupervised topic model on it, and in these models, which we'll get to in a minute, what you end up with is a set of multinomial distributions over words, and these are the topics and then every."
        ],
        [
            "Document, which in our case is a wish, is represented as a document.",
            "Specific mixture of these multinomial's.",
            "So this is 1 example topic learned from the wish data.",
            "But we can see that it's kind of mixing up two different kind of wishes.",
            "These wishes for family members and friends to recover from illness and wishes related to."
        ],
        [
            "Getting into schools.",
            "So we might want to split these two sets of words into two different topics.",
            "And the way that we're going to do this is going to be the topic of this talk.",
            "So let's say that we had this mechanism in hand and we rerun our topic model.",
            "We can see that the resulting topics separate these."
        ],
        [
            "Concepts very nicely and very importantly, we see that they split the original words that we told it to separate, but also very importantly, these two new topics also capture some of the related word."
        ],
        [
            "Which are statistically associated with the words that we told it to separate.",
            "So now we have two nice topics that correspond to very clean concepts in our wish data set.",
            "So what we just did was an example of topic."
        ],
        [
            "Modeling with domain knowledge and why might we want to do this?"
        ],
        [
            "Well, the topics might not correspond to meaningful concepts at all."
        ],
        [
            "So if you've run top modeling sometime, I'm sure you've run into this, or they might not align with your specific goals for topic modeling, so they're capturing meaningful patterns, but they're just not well aligned with your ultimate goals.",
            "So where could we get the domain?"
        ],
        [
            "Knowledge.",
            "My disconnected here.",
            "Hello.",
            "Power on.",
            "Battery dead.",
            "Big flip down enough.",
            "OK, so show for a moment.",
            "That alright so?",
            "We could, in the previous case we just."
        ],
        [
            "Use some human intuition that these guys are not really corresponding to the concepts that we're interested in, but also we could imagine using some kind of structured external source like the gene ontology with a set of predefined concepts linked to some specific vocabulary.",
            "So in there maybe we would use our domain knowledge to build a topic around this go."
        ],
        [
            "I am transcription factor activity, so the way that we're going to express these preferences about the word topic word multinomial's is with some kind of language of ways that."
        ],
        [
            "We're going to be able to do this, and it's inspired by, but very dissed."
        ],
        [
            "Ink from notions of must.",
            "An cannot link from the constrained clustering literature.",
            "So here it's very important that we get this very clear how it's different if we say that must link school in college, then we're going to be telling the model that in any given topic, multinomial school and college should have very.",
            "He was offered, yeah, I flipped it on and off because of the bad guy.",
            "But yeah, you could fix it.",
            "There's there's fresh batteries.",
            "OK, so school and college are.",
            "Letters.",
            "OK, four minutes, and then we'll fix this first.",
            "Who's next?",
            "Stoppage time OK.",
            "This should be fresh, but alright, so seems like we're back online.",
            "OK, so for every given topic multinomial, these two words should have very similar probability.",
            "So in topic one that could both have very high probability and they'd appear together in that list where the list previously was just showing the words with the highest probability for a given topic multinomial, and then in topic two they both have very low probabilities, so they do not appear in the list.",
            "So this must link is kind of binding their probabilities together across all topic word multinomial's and this is intuitive because if we're saying these guys should be together in a topic, binding them together in this way will sort of ensure that they appear together in that list of highly probable words for any given topic."
        ],
        [
            "And the flip side of this would be cannot link where we're going to say that these two words should not be appearing together in that list of highly probable words for any given topic.",
            "And the definition here is that for any for every single topic, we should never have both school and cure, have high probability in the same topic, so they can both have low probability or another topic has school high probability but cure low or another topic has cure high in school loan.",
            "These are all OK and these are kind of reasonable ways to do it because we're going to be able to use these as primitives to encode some higher level operations to apply through our topics.",
            "So the one that we had previously shown where we split up the school and illness topics was an example of this."
        ],
        [
            "Where we have two sets of words that.",
            "Align with coherent concepts, but they're together in the topic right now, and we would must link among them to tie them together, and then cannot link between these two groups to force them apart.",
            "Likewise, maybe we let's say we have two sets of words which currently occur with high probabilities in two separate topics.",
            "But we really believe they correspond to the same concept and should be in the same topic."
        ],
        [
            "So we could merge them by simply putting must links among all of them, thereby forcing their probabilities to be very similar across all topics, and hopefully they'll be one topic or two where they all appear with high probability."
        ],
        [
            "Finally, we could isolate a given set of words.",
            "So the year in 2008 these are all very common in the wish corpus, and so if you just ran LDA on the corpus off the gecko, you would get them appearing as very high probability in many of the topics, and this would be kind of polluting the topics as far as conceptual.",
            "Coherence, so if we must link them together and then cannot link them against the highly probable words, the other highly probable words in the topics rerun, LDA.",
            "They'll be kind of quarantined off into their own topic by our preferences, and we've isolated them away from the other guys, and the resulting topics should be more Coke."
        ],
        [
            "So the way we're going to do this is going to involve replacing the jewishly prior on topic word multinomial's.",
            "With our new prior, so first we're going to describe the deuschle prior.",
            "These are simplex diagrams and the deer sleigh is a prior distribution over multinomial parameters.",
            "So let's say we have a vocabulary ABC and we set the hyperparameters, which can be given as knobs on the dice factory to 111.",
            "Then you see a very uniform distribution over multinomial parameters, where if you this point is in the 8 corner, it's multinomial is going to put very high probability on a low probability, and the other guys, if you're in the centroid of this simplex triangle.",
            "Multinomial sample that puts uniform probability on a B&C.",
            "So we can see if we crank the a nob up to 20 and leave the other two at five.",
            "We get kind of a clustering of multinomial parameters near the a corner and if we leave them all the same value but crank that value up to 50, we see a very tight clustering of these points around the centroid.",
            "So these are the kind of distributions that you can express with the do."
        ],
        [
            "So if you're at the topic modeling session yesterday, I'm sure you saw this a few times, but get ready to see it again.",
            "LDA is an unsupervised graphical model for modeling these kind of topics at for documents is generative an in the plate notation I'll just step through the process really quickly for each topic."
        ],
        [
            "We're going to draw a topic word multinomial fee from a dear sleep prior.",
            "Then for each document, we're going to draw a document specific."
        ],
        [
            "Mixture weights for those topics.",
            "Data from another dishley prior."
        ],
        [
            "Then for every word in that document, we're going to draw a hidden topic.",
            "From that doc you."
        ],
        [
            "And stated distribution.",
            "And then draw a word from the topic word Molteno."
        ],
        [
            "Neil that Z index 2.",
            "And where our modification is going to be, is replacing that original deal."
        ],
        [
            "Ashley prior on the topic were multinomial with our Dearsley Forest prior and just to get an idea of some related work.",
            "The correlated topic model replaces on the other end.",
            "The directly prior on the document topic multinomial's with logistic, normal or the pachinko allocate."
        ],
        [
            "And machine.",
            "Modifies the generative process."
        ],
        [
            "Test for these thetas and Aziz as well, and in this sense, our work is sort of complementary to these other LDA extensions that were kind of attacking it from the other side."
        ],
        [
            "And trying to encode some more structure into the topic word multinomial's as opposed to the document topic ones so."
        ],
        [
            "This must link notion we want for every topic."
        ],
        [
            "Multinomial school and college.",
            "They're very similar, probability indicated by this blue line."
        ],
        [
            "First, we're going to notice that it's transitive.",
            "If we tie school to college and school to graduate college and graduate should have similar probability implicitly."
        ],
        [
            "Next, we're going to notice that this cannot be encoded by a single digit distribution on the left hand side.",
            "We have an example simplex which kind of shows what we want, where school and college for every multinomial drawn from this school and college have very similar probability, but the tradeoff between school and college and lottery.",
            "An unrelated word is allowed to float normally, so we're not enforcing that school and college always have high probability just at school and college."
        ],
        [
            "Similar on the right hand side, we're just going to show a few attempts to get this with the standard dear slay, so that for that parameter value doesn't work."
        ],
        [
            "For this one, we just get a tighter concentration, so now we up lottery.",
            "So these were three different attempts of getting this with Standard Ashley and you're just not able to do it."
        ],
        [
            "So we're going to step up to the deer sleigh tree, which can be thought of as a more expressive dice factory for generating multinomial distributions, and it's going to allow us to control the variance of different subsets of the variables.",
            "So for every internal node you sampled richly parameterized by the edge weights, and then the mass."
        ],
        [
            "Reaching the leaves is the final parameter values for your multinomial so."
        ],
        [
            "Left hand side we have a vocabulary ABC and we have these edge weights which are various multiples of beta and gamma which will get to.",
            "And notice that Ada is very large.",
            "This is beta corresponds to kind of this previously parameter, but ADA is going to be our knobs for controlling our strength of these constraints."
        ],
        [
            "So we're going to step through the one sample from this usually treated."
        ],
        [
            "So first we draw from Julie at the root.",
            ".09 goes directly to the leaf, so that's the multinomial parameter.",
            "There, the .91 gets divided up by another dearsley draw at the internal node, and you just multiply those down the edges and that final fee over ABC is the resulting sample.",
            "So this is what the equation looks like, and we don't worry about that too much.",
            "But notice that this term, which is the difference between indegree and outdegree for every internal node, if it's zero for all internal nodes, we recovered the standard deer sleigh.",
            "And for our trees, they're going to be specially constructed such that if we set that strength parameter adata one, we get this condition and recover standard dearsley.",
            "Next, the deer, say tree."
        ],
        [
            "I still can't get to the multinomial which is very important.",
            "Very nice and we can also collapse out the parameter fee and just get a collapse distribution over the observed events, similar to a dish like compound multinomial.",
            "So now let's stop their example."
        ],
        [
            "Of how we would do this to accomplish, I must link with the display tree.",
            "We have school college.",
            "We want to tie their values together so we place them under an internal node with very large edge weight.",
            "And as we kind of saw early on the simplex, if you have uniform distribution with very large uniform parameter values that are very large, you have very low variance, so doing this should make College in school have very similar probabilities for every sample.",
            "Because of Ito's large again remember?"
        ],
        [
            "And for these parameter values this is the simplex.",
            "We get.",
            "Schooling college are kind of tide together, but they're allowed to float between school, college and letter, so cannot."
        ],
        [
            "School in cancer.",
            "Their corresponding illness in the school topics concepts rather, we want them to not occur together with high probability, so no topic should be allowed to have high probability for both of them.",
            "So first this is a non transitive operation.",
            "If we say school shouldn't occur with cure."
        ],
        [
            "You're either going to not let them cancer, and cure should still allow to be have high private."
        ],
        [
            "Body in a single topic.",
            "And next, we cannot encode this with a single dish layered, usually treated, so this is an example of what the simplex would look like for that cannot link between school in cancer, we see that cancer cures still kind of very normally, but they're both separated from school.",
            "So we're going to require a mixture of these dearsley trees to accomplish our desired distribution.",
            "So this."
        ],
        [
            "Is how we're going to sample from a mixture of dislike trees, which we call the digital forest first.",
            "We're going to say the vocabulary is letters A to G, and we have a must link among A&B and the following cannot links which are represented by this graph.",
            "Now notice that A&B are kind of collapsed into a single node for purposes of this cannot link graph because A&B are tide together via must link, so that's a reasonable thing to do.",
            "So here's the cannot link graph.",
            "Maybe you see these connected components, which is very important."
        ],
        [
            "So the first thing we're going to do to sample from the dish like forest is take the connected components of the cannot link graph where the edges represent the kind of links.",
            "And for each connected component, we're going to create an internal node coming out of the root.",
            "So we can kind of think of them separately.",
            "Notice that G is not involved in any cannot link, so it just gets connected directly to root.",
            "So that's very simple and we're done with that guy.",
            "That's how words that aren't involved in any constraints are handled.",
            "Next, we're going to take the sub graph compliments for each connected component of the cannot link rack.",
            "So remember, if cannot link biz between 2 words, they should not both appear with high probability in the same topic.",
            "By taking the complement were saying it's OK for them to occur with high probability in the same topic.",
            "So AB&C they are OK to occur together but not enf because they're not connected in the complement."
        ],
        [
            "Then we're going to take the Max."
        ],
        [
            "More cliques of each connected component and remembering that now these complement edges mean OK.",
            "The maximal clique for each connected component is the maximal set of words, which is OK to occur together with high probability in a given topic without violating our notion of a cannot link.",
            "So this is going to show how we."
        ],
        [
            "Going to sample.",
            "A maximal clique for each connected component, so we see that we have."
        ],
        [
            "Q is which is our index value and there's these two possible subtrees.",
            "Remember Ada is large so."
        ],
        [
            "The subtree on the left is going to shut most of the probability too."
        ],
        [
            "The ABC Clique and the subject on the right is going to shut most of the probability to the declic so."
        ],
        [
            "We sample AQ value, it's one the ABC clique, so we insert that particular subtree into our growing their slate tree.",
            "And we have this must link at the bottom as before, so that's just kind of simple.",
            "Like must link was.",
            "Then we repeat."
        ],
        [
            "This procedure for the next connected component with again 2."
        ],
        [
            "Subtrees that are going to sort of select a maximal clique.",
            "Here we select F, so we insert that subtree."
        ],
        [
            "And this is exactly our final dearsley tree for a single topic, and we can see that it's going to shut most of the probability going down the left edge to the ABC click, and then it's going to be evenly distributed among A&B wetlands there, and it's going to select F in the other click or for the other connected component rather so then for our topic we then just sample a multinomial distribution from this seriously tree and we would repeat this process for every topic where maybe we'd like different values each time around, so this construction.",
            "Procedure implicitly defines a mic."
        ],
        [
            "Overdue sleep trees and then the multinomial parameters are sampled from the richly tree for each topic.",
            "So this can't just says and I said before for every topic and for every connected component of the cannot link graph, we're going to sample AQ value to select the clique, then for every topic that's going to result in building a deuschle tree.",
            "Then we just sample our topic word multinomial fee from that during slavery.",
            "And because of the controversy between the richly tree and the."
        ],
        [
            "Show me all we can do collapsed Gibbs sampling, which is nice, but now we also must sample over these Q assignments, which decided which maximal clique were picking for each connected component an again that's going to be different for every single topic, so every topic sort of has its own dear slate redefined by these."
        ],
        [
            "So this is going to be an experiment which just shows how this would work on very very simple synthetic data.",
            "The corpus is just over this vocabulary abcede and our prior knowledge is that B&C should be in the same topic and these are a PCA representations of the learning topics from many, many, many different runs of the model with this prior.",
            "So cluster one is that is the topic shown there that few one is a beefy two is over CD and so on and we can see that B&C are only occurring in the same topic for cluster three.",
            "So 81 on the far left corresponds to standard LDA and we kind of get these three modes of our topics and you can see as ADA the spring parameters cranked up.",
            "The samples increasingly concentrate in three which is the topic decomposition that."
        ],
        [
            "All these are constraint.",
            "So this is just another similar example where we're going to say that we should be isolated from agency, so we're going to put cannot links between B&A&B&C and we have very simple corpus and we can see the breakdown over the learn topics over many different runs again and again as we increase ADA, we gradually see the samples of the topics congregate in the one mode which obeys our constraints."
        ],
        [
            "So finally, we're just going to step through the original wish example where these are the topics learned by LDA on the original wish corpus, and here we see stop."
        ],
        [
            "Words kind of polluting a lot of them, so we're going to place an isolate operation on these stopwords must linking them together and then cannot linking them against the other topics and we see that when we rerun inference we end up with these two."
        ],
        [
            "Guys, these two topics which basically consists of all stopwords.",
            "And the other topics make a lot more conceptual sense.",
            "They're starting to look more like recognizable themes, of which."
        ],
        [
            "But we notice this one, which is the original example, which is going to be mixing up the recovery from illness and they get into school concepts."
        ],
        [
            "So we're going to split by must linking among the concept words and then cannot link in between."
        ],
        [
            "And these two topics at the bottom correspond to those.",
            "And again, very importantly, the other words in these topics generalize beyond the constraint that we explicitly provided a lot of terms which are statistically related to school appear in the school topic.",
            "And a lot of terms statistically related to illness but not encoding our constraints appear in the other top."
        ],
        [
            "Like finally, we see that these two topics are sort of loving romance related, so we're going to merge them together with must links.",
            "And these are these final topics where we see that the merge topic at that app shows a lot of these love and romance topics.",
            "However, we can see that the data has actually overridden our prior somewhat and that these lose weight words which were not included appear at the top.",
            "And this is just a very very strong statistical Association in the data."
        ],
        [
            "There are a lot of wishes of the form lose weight and meet a boyfriend so.",
            "It is an instance where the data is still kind of overriding our prior.",
            "So that's that's the direction for spry, and how we can express our domain knowledge about what we expect the topic word multinomial to look like, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How we can incorporate inject some domain knowledge into the topic modeling process and why we would want to do this, and specifically how we're going to show that this could be done.",
                    "label": 1
                },
                {
                    "sent": "So first I'm going to just dive right into.",
                    "label": 0
                },
                {
                    "sent": "An motivating example of why we might be interested in doing this.",
                    "label": 0
                },
                {
                    "sent": "So if any of you were at nacle a few weeks ago, you may have seen a presentation by my colleague Andrew Goldberg where he discussed a interesting.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Purpose of Wishes submitted on line to the Times Square website in New York City, to be shredded up into confetti and dropped on the Times Square revelers for New Year's.",
                    "label": 1
                },
                {
                    "sent": "And so these New Year's wishes, kind of.",
                    "label": 0
                },
                {
                    "sent": "I'll give you a little taste of what they're like, so there's the very, very, very popular piece.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On earth.",
                    "label": 0
                },
                {
                    "sent": "The less popular but still Culona Brewery.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graduate School with.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for our friends and family.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To lose weight and get a boyfriend?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This person actually got their wish and.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, the granddaddy of them all to win the lottery.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something that you might typically do when you first get a new interesting corpus like this would be run a statistical unsupervised topic model on it, and in these models, which we'll get to in a minute, what you end up with is a set of multinomial distributions over words, and these are the topics and then every.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Document, which in our case is a wish, is represented as a document.",
                    "label": 0
                },
                {
                    "sent": "Specific mixture of these multinomial's.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 example topic learned from the wish data.",
                    "label": 0
                },
                {
                    "sent": "But we can see that it's kind of mixing up two different kind of wishes.",
                    "label": 0
                },
                {
                    "sent": "These wishes for family members and friends to recover from illness and wishes related to.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Getting into schools.",
                    "label": 0
                },
                {
                    "sent": "So we might want to split these two sets of words into two different topics.",
                    "label": 1
                },
                {
                    "sent": "And the way that we're going to do this is going to be the topic of this talk.",
                    "label": 0
                },
                {
                    "sent": "So let's say that we had this mechanism in hand and we rerun our topic model.",
                    "label": 0
                },
                {
                    "sent": "We can see that the resulting topics separate these.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Concepts very nicely and very importantly, we see that they split the original words that we told it to separate, but also very importantly, these two new topics also capture some of the related word.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which are statistically associated with the words that we told it to separate.",
                    "label": 0
                },
                {
                    "sent": "So now we have two nice topics that correspond to very clean concepts in our wish data set.",
                    "label": 0
                },
                {
                    "sent": "So what we just did was an example of topic.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Modeling with domain knowledge and why might we want to do this?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, the topics might not correspond to meaningful concepts at all.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you've run top modeling sometime, I'm sure you've run into this, or they might not align with your specific goals for topic modeling, so they're capturing meaningful patterns, but they're just not well aligned with your ultimate goals.",
                    "label": 0
                },
                {
                    "sent": "So where could we get the domain?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Knowledge.",
                    "label": 0
                },
                {
                    "sent": "My disconnected here.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "Power on.",
                    "label": 0
                },
                {
                    "sent": "Battery dead.",
                    "label": 0
                },
                {
                    "sent": "Big flip down enough.",
                    "label": 0
                },
                {
                    "sent": "OK, so show for a moment.",
                    "label": 0
                },
                {
                    "sent": "That alright so?",
                    "label": 0
                },
                {
                    "sent": "We could, in the previous case we just.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use some human intuition that these guys are not really corresponding to the concepts that we're interested in, but also we could imagine using some kind of structured external source like the gene ontology with a set of predefined concepts linked to some specific vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So in there maybe we would use our domain knowledge to build a topic around this go.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I am transcription factor activity, so the way that we're going to express these preferences about the word topic word multinomial's is with some kind of language of ways that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to be able to do this, and it's inspired by, but very dissed.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ink from notions of must.",
                    "label": 0
                },
                {
                    "sent": "An cannot link from the constrained clustering literature.",
                    "label": 0
                },
                {
                    "sent": "So here it's very important that we get this very clear how it's different if we say that must link school in college, then we're going to be telling the model that in any given topic, multinomial school and college should have very.",
                    "label": 0
                },
                {
                    "sent": "He was offered, yeah, I flipped it on and off because of the bad guy.",
                    "label": 0
                },
                {
                    "sent": "But yeah, you could fix it.",
                    "label": 0
                },
                {
                    "sent": "There's there's fresh batteries.",
                    "label": 0
                },
                {
                    "sent": "OK, so school and college are.",
                    "label": 0
                },
                {
                    "sent": "Letters.",
                    "label": 0
                },
                {
                    "sent": "OK, four minutes, and then we'll fix this first.",
                    "label": 0
                },
                {
                    "sent": "Who's next?",
                    "label": 0
                },
                {
                    "sent": "Stoppage time OK.",
                    "label": 0
                },
                {
                    "sent": "This should be fresh, but alright, so seems like we're back online.",
                    "label": 0
                },
                {
                    "sent": "OK, so for every given topic multinomial, these two words should have very similar probability.",
                    "label": 0
                },
                {
                    "sent": "So in topic one that could both have very high probability and they'd appear together in that list where the list previously was just showing the words with the highest probability for a given topic multinomial, and then in topic two they both have very low probabilities, so they do not appear in the list.",
                    "label": 0
                },
                {
                    "sent": "So this must link is kind of binding their probabilities together across all topic word multinomial's and this is intuitive because if we're saying these guys should be together in a topic, binding them together in this way will sort of ensure that they appear together in that list of highly probable words for any given topic.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the flip side of this would be cannot link where we're going to say that these two words should not be appearing together in that list of highly probable words for any given topic.",
                    "label": 0
                },
                {
                    "sent": "And the definition here is that for any for every single topic, we should never have both school and cure, have high probability in the same topic, so they can both have low probability or another topic has school high probability but cure low or another topic has cure high in school loan.",
                    "label": 0
                },
                {
                    "sent": "These are all OK and these are kind of reasonable ways to do it because we're going to be able to use these as primitives to encode some higher level operations to apply through our topics.",
                    "label": 0
                },
                {
                    "sent": "So the one that we had previously shown where we split up the school and illness topics was an example of this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where we have two sets of words that.",
                    "label": 0
                },
                {
                    "sent": "Align with coherent concepts, but they're together in the topic right now, and we would must link among them to tie them together, and then cannot link between these two groups to force them apart.",
                    "label": 0
                },
                {
                    "sent": "Likewise, maybe we let's say we have two sets of words which currently occur with high probabilities in two separate topics.",
                    "label": 0
                },
                {
                    "sent": "But we really believe they correspond to the same concept and should be in the same topic.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we could merge them by simply putting must links among all of them, thereby forcing their probabilities to be very similar across all topics, and hopefully they'll be one topic or two where they all appear with high probability.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, we could isolate a given set of words.",
                    "label": 0
                },
                {
                    "sent": "So the year in 2008 these are all very common in the wish corpus, and so if you just ran LDA on the corpus off the gecko, you would get them appearing as very high probability in many of the topics, and this would be kind of polluting the topics as far as conceptual.",
                    "label": 1
                },
                {
                    "sent": "Coherence, so if we must link them together and then cannot link them against the highly probable words, the other highly probable words in the topics rerun, LDA.",
                    "label": 0
                },
                {
                    "sent": "They'll be kind of quarantined off into their own topic by our preferences, and we've isolated them away from the other guys, and the resulting topics should be more Coke.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way we're going to do this is going to involve replacing the jewishly prior on topic word multinomial's.",
                    "label": 0
                },
                {
                    "sent": "With our new prior, so first we're going to describe the deuschle prior.",
                    "label": 0
                },
                {
                    "sent": "These are simplex diagrams and the deer sleigh is a prior distribution over multinomial parameters.",
                    "label": 0
                },
                {
                    "sent": "So let's say we have a vocabulary ABC and we set the hyperparameters, which can be given as knobs on the dice factory to 111.",
                    "label": 0
                },
                {
                    "sent": "Then you see a very uniform distribution over multinomial parameters, where if you this point is in the 8 corner, it's multinomial is going to put very high probability on a low probability, and the other guys, if you're in the centroid of this simplex triangle.",
                    "label": 0
                },
                {
                    "sent": "Multinomial sample that puts uniform probability on a B&C.",
                    "label": 0
                },
                {
                    "sent": "So we can see if we crank the a nob up to 20 and leave the other two at five.",
                    "label": 0
                },
                {
                    "sent": "We get kind of a clustering of multinomial parameters near the a corner and if we leave them all the same value but crank that value up to 50, we see a very tight clustering of these points around the centroid.",
                    "label": 0
                },
                {
                    "sent": "So these are the kind of distributions that you can express with the do.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you're at the topic modeling session yesterday, I'm sure you saw this a few times, but get ready to see it again.",
                    "label": 0
                },
                {
                    "sent": "LDA is an unsupervised graphical model for modeling these kind of topics at for documents is generative an in the plate notation I'll just step through the process really quickly for each topic.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to draw a topic word multinomial fee from a dear sleep prior.",
                    "label": 0
                },
                {
                    "sent": "Then for each document, we're going to draw a document specific.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mixture weights for those topics.",
                    "label": 0
                },
                {
                    "sent": "Data from another dishley prior.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then for every word in that document, we're going to draw a hidden topic.",
                    "label": 0
                },
                {
                    "sent": "From that doc you.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And stated distribution.",
                    "label": 0
                },
                {
                    "sent": "And then draw a word from the topic word Molteno.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Neil that Z index 2.",
                    "label": 0
                },
                {
                    "sent": "And where our modification is going to be, is replacing that original deal.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ashley prior on the topic were multinomial with our Dearsley Forest prior and just to get an idea of some related work.",
                    "label": 0
                },
                {
                    "sent": "The correlated topic model replaces on the other end.",
                    "label": 0
                },
                {
                    "sent": "The directly prior on the document topic multinomial's with logistic, normal or the pachinko allocate.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And machine.",
                    "label": 0
                },
                {
                    "sent": "Modifies the generative process.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Test for these thetas and Aziz as well, and in this sense, our work is sort of complementary to these other LDA extensions that were kind of attacking it from the other side.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And trying to encode some more structure into the topic word multinomial's as opposed to the document topic ones so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This must link notion we want for every topic.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Multinomial school and college.",
                    "label": 0
                },
                {
                    "sent": "They're very similar, probability indicated by this blue line.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, we're going to notice that it's transitive.",
                    "label": 0
                },
                {
                    "sent": "If we tie school to college and school to graduate college and graduate should have similar probability implicitly.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next, we're going to notice that this cannot be encoded by a single digit distribution on the left hand side.",
                    "label": 1
                },
                {
                    "sent": "We have an example simplex which kind of shows what we want, where school and college for every multinomial drawn from this school and college have very similar probability, but the tradeoff between school and college and lottery.",
                    "label": 0
                },
                {
                    "sent": "An unrelated word is allowed to float normally, so we're not enforcing that school and college always have high probability just at school and college.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similar on the right hand side, we're just going to show a few attempts to get this with the standard dear slay, so that for that parameter value doesn't work.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this one, we just get a tighter concentration, so now we up lottery.",
                    "label": 0
                },
                {
                    "sent": "So these were three different attempts of getting this with Standard Ashley and you're just not able to do it.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to step up to the deer sleigh tree, which can be thought of as a more expressive dice factory for generating multinomial distributions, and it's going to allow us to control the variance of different subsets of the variables.",
                    "label": 0
                },
                {
                    "sent": "So for every internal node you sampled richly parameterized by the edge weights, and then the mass.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reaching the leaves is the final parameter values for your multinomial so.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Left hand side we have a vocabulary ABC and we have these edge weights which are various multiples of beta and gamma which will get to.",
                    "label": 0
                },
                {
                    "sent": "And notice that Ada is very large.",
                    "label": 0
                },
                {
                    "sent": "This is beta corresponds to kind of this previously parameter, but ADA is going to be our knobs for controlling our strength of these constraints.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to step through the one sample from this usually treated.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first we draw from Julie at the root.",
                    "label": 0
                },
                {
                    "sent": ".09 goes directly to the leaf, so that's the multinomial parameter.",
                    "label": 0
                },
                {
                    "sent": "There, the .91 gets divided up by another dearsley draw at the internal node, and you just multiply those down the edges and that final fee over ABC is the resulting sample.",
                    "label": 0
                },
                {
                    "sent": "So this is what the equation looks like, and we don't worry about that too much.",
                    "label": 0
                },
                {
                    "sent": "But notice that this term, which is the difference between indegree and outdegree for every internal node, if it's zero for all internal nodes, we recovered the standard deer sleigh.",
                    "label": 1
                },
                {
                    "sent": "And for our trees, they're going to be specially constructed such that if we set that strength parameter adata one, we get this condition and recover standard dearsley.",
                    "label": 0
                },
                {
                    "sent": "Next, the deer, say tree.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I still can't get to the multinomial which is very important.",
                    "label": 0
                },
                {
                    "sent": "Very nice and we can also collapse out the parameter fee and just get a collapse distribution over the observed events, similar to a dish like compound multinomial.",
                    "label": 0
                },
                {
                    "sent": "So now let's stop their example.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of how we would do this to accomplish, I must link with the display tree.",
                    "label": 0
                },
                {
                    "sent": "We have school college.",
                    "label": 0
                },
                {
                    "sent": "We want to tie their values together so we place them under an internal node with very large edge weight.",
                    "label": 1
                },
                {
                    "sent": "And as we kind of saw early on the simplex, if you have uniform distribution with very large uniform parameter values that are very large, you have very low variance, so doing this should make College in school have very similar probabilities for every sample.",
                    "label": 0
                },
                {
                    "sent": "Because of Ito's large again remember?",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for these parameter values this is the simplex.",
                    "label": 0
                },
                {
                    "sent": "We get.",
                    "label": 0
                },
                {
                    "sent": "Schooling college are kind of tide together, but they're allowed to float between school, college and letter, so cannot.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "School in cancer.",
                    "label": 0
                },
                {
                    "sent": "Their corresponding illness in the school topics concepts rather, we want them to not occur together with high probability, so no topic should be allowed to have high probability for both of them.",
                    "label": 1
                },
                {
                    "sent": "So first this is a non transitive operation.",
                    "label": 0
                },
                {
                    "sent": "If we say school shouldn't occur with cure.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're either going to not let them cancer, and cure should still allow to be have high private.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Body in a single topic.",
                    "label": 0
                },
                {
                    "sent": "And next, we cannot encode this with a single dish layered, usually treated, so this is an example of what the simplex would look like for that cannot link between school in cancer, we see that cancer cures still kind of very normally, but they're both separated from school.",
                    "label": 0
                },
                {
                    "sent": "So we're going to require a mixture of these dearsley trees to accomplish our desired distribution.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is how we're going to sample from a mixture of dislike trees, which we call the digital forest first.",
                    "label": 0
                },
                {
                    "sent": "We're going to say the vocabulary is letters A to G, and we have a must link among A&B and the following cannot links which are represented by this graph.",
                    "label": 0
                },
                {
                    "sent": "Now notice that A&B are kind of collapsed into a single node for purposes of this cannot link graph because A&B are tide together via must link, so that's a reasonable thing to do.",
                    "label": 0
                },
                {
                    "sent": "So here's the cannot link graph.",
                    "label": 0
                },
                {
                    "sent": "Maybe you see these connected components, which is very important.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first thing we're going to do to sample from the dish like forest is take the connected components of the cannot link graph where the edges represent the kind of links.",
                    "label": 0
                },
                {
                    "sent": "And for each connected component, we're going to create an internal node coming out of the root.",
                    "label": 0
                },
                {
                    "sent": "So we can kind of think of them separately.",
                    "label": 0
                },
                {
                    "sent": "Notice that G is not involved in any cannot link, so it just gets connected directly to root.",
                    "label": 0
                },
                {
                    "sent": "So that's very simple and we're done with that guy.",
                    "label": 0
                },
                {
                    "sent": "That's how words that aren't involved in any constraints are handled.",
                    "label": 0
                },
                {
                    "sent": "Next, we're going to take the sub graph compliments for each connected component of the cannot link rack.",
                    "label": 0
                },
                {
                    "sent": "So remember, if cannot link biz between 2 words, they should not both appear with high probability in the same topic.",
                    "label": 0
                },
                {
                    "sent": "By taking the complement were saying it's OK for them to occur with high probability in the same topic.",
                    "label": 0
                },
                {
                    "sent": "So AB&C they are OK to occur together but not enf because they're not connected in the complement.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we're going to take the Max.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More cliques of each connected component and remembering that now these complement edges mean OK.",
                    "label": 0
                },
                {
                    "sent": "The maximal clique for each connected component is the maximal set of words, which is OK to occur together with high probability in a given topic without violating our notion of a cannot link.",
                    "label": 0
                },
                {
                    "sent": "So this is going to show how we.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to sample.",
                    "label": 0
                },
                {
                    "sent": "A maximal clique for each connected component, so we see that we have.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Q is which is our index value and there's these two possible subtrees.",
                    "label": 0
                },
                {
                    "sent": "Remember Ada is large so.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The subtree on the left is going to shut most of the probability too.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The ABC Clique and the subject on the right is going to shut most of the probability to the declic so.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We sample AQ value, it's one the ABC clique, so we insert that particular subtree into our growing their slate tree.",
                    "label": 0
                },
                {
                    "sent": "And we have this must link at the bottom as before, so that's just kind of simple.",
                    "label": 0
                },
                {
                    "sent": "Like must link was.",
                    "label": 0
                },
                {
                    "sent": "Then we repeat.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This procedure for the next connected component with again 2.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Subtrees that are going to sort of select a maximal clique.",
                    "label": 0
                },
                {
                    "sent": "Here we select F, so we insert that subtree.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is exactly our final dearsley tree for a single topic, and we can see that it's going to shut most of the probability going down the left edge to the ABC click, and then it's going to be evenly distributed among A&B wetlands there, and it's going to select F in the other click or for the other connected component rather so then for our topic we then just sample a multinomial distribution from this seriously tree and we would repeat this process for every topic where maybe we'd like different values each time around, so this construction.",
                    "label": 0
                },
                {
                    "sent": "Procedure implicitly defines a mic.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Overdue sleep trees and then the multinomial parameters are sampled from the richly tree for each topic.",
                    "label": 0
                },
                {
                    "sent": "So this can't just says and I said before for every topic and for every connected component of the cannot link graph, we're going to sample AQ value to select the clique, then for every topic that's going to result in building a deuschle tree.",
                    "label": 0
                },
                {
                    "sent": "Then we just sample our topic word multinomial fee from that during slavery.",
                    "label": 0
                },
                {
                    "sent": "And because of the controversy between the richly tree and the.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show me all we can do collapsed Gibbs sampling, which is nice, but now we also must sample over these Q assignments, which decided which maximal clique were picking for each connected component an again that's going to be different for every single topic, so every topic sort of has its own dear slate redefined by these.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is going to be an experiment which just shows how this would work on very very simple synthetic data.",
                    "label": 0
                },
                {
                    "sent": "The corpus is just over this vocabulary abcede and our prior knowledge is that B&C should be in the same topic and these are a PCA representations of the learning topics from many, many, many different runs of the model with this prior.",
                    "label": 1
                },
                {
                    "sent": "So cluster one is that is the topic shown there that few one is a beefy two is over CD and so on and we can see that B&C are only occurring in the same topic for cluster three.",
                    "label": 0
                },
                {
                    "sent": "So 81 on the far left corresponds to standard LDA and we kind of get these three modes of our topics and you can see as ADA the spring parameters cranked up.",
                    "label": 0
                },
                {
                    "sent": "The samples increasingly concentrate in three which is the topic decomposition that.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All these are constraint.",
                    "label": 0
                },
                {
                    "sent": "So this is just another similar example where we're going to say that we should be isolated from agency, so we're going to put cannot links between B&A&B&C and we have very simple corpus and we can see the breakdown over the learn topics over many different runs again and again as we increase ADA, we gradually see the samples of the topics congregate in the one mode which obeys our constraints.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally, we're just going to step through the original wish example where these are the topics learned by LDA on the original wish corpus, and here we see stop.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Words kind of polluting a lot of them, so we're going to place an isolate operation on these stopwords must linking them together and then cannot linking them against the other topics and we see that when we rerun inference we end up with these two.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Guys, these two topics which basically consists of all stopwords.",
                    "label": 0
                },
                {
                    "sent": "And the other topics make a lot more conceptual sense.",
                    "label": 0
                },
                {
                    "sent": "They're starting to look more like recognizable themes, of which.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we notice this one, which is the original example, which is going to be mixing up the recovery from illness and they get into school concepts.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to split by must linking among the concept words and then cannot link in between.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these two topics at the bottom correspond to those.",
                    "label": 0
                },
                {
                    "sent": "And again, very importantly, the other words in these topics generalize beyond the constraint that we explicitly provided a lot of terms which are statistically related to school appear in the school topic.",
                    "label": 0
                },
                {
                    "sent": "And a lot of terms statistically related to illness but not encoding our constraints appear in the other top.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like finally, we see that these two topics are sort of loving romance related, so we're going to merge them together with must links.",
                    "label": 0
                },
                {
                    "sent": "And these are these final topics where we see that the merge topic at that app shows a lot of these love and romance topics.",
                    "label": 0
                },
                {
                    "sent": "However, we can see that the data has actually overridden our prior somewhat and that these lose weight words which were not included appear at the top.",
                    "label": 0
                },
                {
                    "sent": "And this is just a very very strong statistical Association in the data.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are a lot of wishes of the form lose weight and meet a boyfriend so.",
                    "label": 0
                },
                {
                    "sent": "It is an instance where the data is still kind of overriding our prior.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the direction for spry, and how we can express our domain knowledge about what we expect the topic word multinomial to look like, thanks.",
                    "label": 0
                }
            ]
        }
    }
}