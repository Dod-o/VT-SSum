{
    "id": "g2hxzu5h74tpt3y4imoeiev4lyfenums",
    "title": "Lightweight Implementations of Probabilistic Programming Languages Via Transformational Compilation",
    "info": {
        "author": [
            "David Wingate, Computational Cognitive Science Group, Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, MIT"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science->Programming Languages"
        ]
    },
    "url": "http://videolectures.net/aistats2011_wingate_lightweight/",
    "segmentation": [
        [
            "So we'll be talking about work we've done with our collaborators on probabilistic programming.",
            "The official title of the paper and talk is lightweight implementations of probabilistic programming languages via transformation compilation, but as terrible title, and I thought as much."
        ],
        [
            "Title might be implementing public programming languages without the agonizing pain, so we'll go."
        ],
        [
            "Through some research we've been done on how you can implement probabilistic programming languages, hopefully without too much difficulty.",
            "So the goal is to create probabilistic programming languages, which help us express complex probabilistic models, and I'll give you some examples of what that means in just a minute.",
            "An observation is that we would like to leverage existing language infrastructures of existing languages.",
            "So there's maybe no reason to invent a new language.",
            "There's plenty of languages out there, and we would like to use what other people have done with those languages to help us build interesting probabilistic programming languages.",
            "So things like compilers, parallelization, libraries, profilers, debuggers, etc.",
            "So the contribution of this paper is a method to help us transform any language into a probabilistic programming language.",
            "So this means that you could create, for example, stochastic version of Matlab which will talk about.",
            "You could also use it, at least in principle, to create a stochastic version of JavaScript or Visual Basic so."
        ],
        [
            "Outline will be.",
            "I'll talk quickly about what problems the programming is and then talk about our contribution, which is how you implement these at least.",
            "One way to implement these fairly straightforward.",
            "OK, so probably program.",
            "What's the big idea?",
            "The big idea is to use a programming."
        ],
        [
            "Language to express a probabilistic model, not to implement a probabilistic model, but to express 1 to define one so."
        ],
        [
            "The the the rhythm and that shows like this, you write down some sort of randomized algorithm or some sort of function which makes a bunch of random choices.",
            "You didn't fix the output of that function.",
            "That's equivalent to conditioning the model and then you want to reason about what random choices your program would have had to have made in order to produce the output with high probability.",
            "It's kind of like running the program backwards.",
            "You start with the output of the program and figure out what needed to happen within the program in order to generate the output.",
            "OK, and then the goal is to write some sort of maybe called probability compiler or interpreter or generic inference algorithm to perform inference in any program."
        ],
        [
            "The user could potentially write.",
            "OK, so let me give you some examples about how we define these things.",
            "So probabilistic programs define distributions by defining a distribution over possible execution traces of a program.",
            "So here's a little example of a Gaussian mixture model.",
            "So here I'll be using this Matlab S notation throughout the talk, so we define some sort of function which returns some variable X, call it, and this function will name GMM.",
            "We first start by sampling some random variable Rand.",
            "This is a uniform random variable is greater than .5, then X is negative .2 plus Gaussian.",
            "So in MATLAB, that's Brandon, else it's .2 + .5 times rated.",
            "So you can see that if when we sample different choices for this round up here will actually execute different sequences of instructions as we move through this program.",
            "So if we run this once, we get some output.",
            "If we run it again, we get a different output.",
            "We keep running this overtime.",
            "We build up histogram of outputs.",
            "Which looks like a mixture of two Gaussians.",
            "So this is how we define the distribution.",
            "Of course our goal is will be to condition this and then do inference at it.",
            "And I'll start talking about that in just a minute and a point to note here is that more complex."
        ],
        [
            "Distributions are created compositionally by combining many small pieces, so I just invented one more slide here.",
            "Since we've been talking about topic models so much, I thought I'd give you an example.",
            "I think I got it right.",
            "I hope I did.",
            "Dave lies in the audience, so this is LDA.",
            "So sample a bunch of topics at the start from some sort of perhaps seriously, then, for each document you sample a topic distribution for each word you sample a topic for that word, then sample the actual work.",
            "And again the goal will be take this program condition its output.",
            "And do inference in the result."
        ],
        [
            "OK, so let me give you one more example, which is how we define nonparametric distributions in languages like this.",
            "We do it with a technique which was first suggested in the church probabilistic programming paper and UI a couple of years ago.",
            "We just called stochastic memorization.",
            "So the idea is that were given some sort of stochastic function G maybe randen stochastic memorizer is a new function which returns is a function which returns a new function F and it treats G as the base measure, then calls to F will return previous or new value from G4 according to, for example, additional process.",
            "So to see this in action, here's a little way to give you a dear sleep process with a base measure that has a Gaussian.",
            "So G is some Gaussian function.",
            "F is some new function, so DPM is a stochastic memorizer.",
            "It accepts as arguments GA base measure.",
            "And then a concentration parameter and returns a new function and calls to F. Now return new values according to some deer sleep process.",
            "OK.",
            "So we can use this to help us define more complex distribution.",
            "So here's an example of some increasingly complex mixture models.",
            "Here's our simple Gaussian mixture model with three components."
        ],
        [
            "So K is 3 mute.",
            "There's 3, three means an we generate 100 data points, and for each one we have to sample some sort of some sort of indicator variable, which tells us which major component was generated from, and then we grab the mean of that mixture component an add some random Gaussian noise.",
            "So if the idea behind probabilistic programming is it makes us easy.",
            "It makes it easy to explore new models and iterate models efficiently.",
            "So if for example you wanted some sort of deer Slayer process mixture model, that might be as easy as.",
            "As this so I'll take a base measure, which is a Gaussian, and I'll stochastically memorize it to create a new function, and then in order to generate 100 new data points, I sample from this memoized function plus noise.",
            "Calls to this B will be distributed according to Adir sleep process with a Gaussian as a base measure.",
            "But this function that gets memorized can be any function including another memoized function.",
            "So a hierarchical dearsley process would just be the addition of 1 extra line of code.",
            "So now we memorize the Gaussian and then we memorize the result of the memorization.",
            "So the point is that this is this is not.",
            "This is not.",
            "It's not complicated.",
            "Now to explore new models and to define new models in a way that makes intuitive sense.",
            "This is probably superior.",
            "Give you one more example just to tell you what I think is a particularly interesting point about these languages, which is if your language has something like in avalyn and you can reason about the structure of the model itself, perhaps like this.",
            "So here's a little probabilistic program which will attempt to induce a program, so the idea is we sample some text of code from a probabilistic context free grammar."
        ],
        [
            "We then evaluate text to give us a function and then generate our data by calling that function on some inputs and maybe add some noise.",
            "We actually ran this little program in our stochastic Matlab probabilistic programming language, not a little curve that looks like this.",
            "What this is saying is, is saying as the model is doing inference as time is increasing, likelihood is going up.",
            "It's learning something.",
            "Learning something about what.",
            "It's learning something about the actual structure of a function which Maps inputs to outputs.",
            "The point is you can mix and match this with any of the other language constructs that have already showed you.",
            "OK, so."
        ],
        [
            "The problem is that programming in a nutshell, so now like to take us to the topic of this talk, which is how we."
        ],
        [
            "Actually implement these things in some sort of reasonable way.",
            "So the goal will be ultimately to perform inference in an arbitrary program, and approach will be MCMC, unsurprisingly, So what we'll need is some mechanism for making proposals and scoring and scoring them, therefore will need some ability to control the execution of these programs, hopefully without the."
        ],
        [
            "Rising paint, so one observation about these execution traces is the following, so here's a little model which goes through an samples M from a class on it, then samples.",
            "Am gammas and then it samples M Gaussians?",
            "So what I'd like to do is walk you through the execution of this program.",
            "So here's what would happen if you were to just run this program in Matlab, you would encounter several random choices.",
            "So the first thing you encounter is this M. So you sample some Python variable, and here I'm going to show that with blue and let's suppose we happily sample two.",
            "You then iterate this loop 2 times and you'll sample two out two gamma variables and then you move down here and you'll sample.",
            "To two Gaussian variables.",
            "If you were to execute this program again, you might get a different a different value for M An.",
            "Suppose, for example, we sampled one.",
            "Then we would sample just one gamma, and then we would sample just one one Gaussian.",
            "OK, so one observation.",
            "Is that?",
            "Two, if two traces.",
            "We call these, but we call these execution traces."
        ],
        [
            "So this is an execution trace as it moves through the program.",
            "If two choice if two traces make all of the same choices and their execution paths will be identical.",
            "Because everything else in the program is deterministic, so we'll use that fact to our advantage.",
            "This suggests the following.",
            "The following approach for controlling the execute."
        ],
        [
            "And of these, these these programs via transformation compilation.",
            "So the idea is we take your probabilistic program and we give every random choice that your program makes a name, and I'll talk about those names in just a minute.",
            "I then rewrite your code to make it deterministic.",
            "So I take all of the random choices out and I put new deterministic functions in and those new deterministic functions use the name of the random choice as it's encountered in the program to look up its value in some sort of database of randomness.",
            "And now if I fiddle with the values in the database of randomness.",
            "I can imples."
        ],
        [
            "Control the execution traces of your programs.",
            "Building on this, then we can build inference algorithms, so to do MCMC over execution traces, here's the general idea you're given in some sort of execution trace, you reconsider some random choice that you made in order to generate that trace, and then you update the trace using as much random, reusing as much randomness as possible.",
            "Scoring, accept, reject.",
            "The official algorithm looks like this, and it's not that complicated.",
            "The idea is the idea is.",
            "I should say this is.",
            "I mean this is pretty standard.",
            "This is pretty standard MH sort of sorts of proposals and scoring.",
            "The one trick is that you have these databases which help you manage all of the random variables.",
            "Now I've highlighted in bold the idea on .3 here that we want to reuse as much randomness as possible.",
            "Why is that?",
            "Well, the intuition is if you have some execution through some program space.",
            "And you want to do some proposal and come up with a new trace.",
            "You want the two traces to be pretty close together, or else you're probably going to reject them, so you want to walk carefully through this space.",
            "If your moves are too big, then you'll typically reject."
        ],
        [
            "OK, so now let's get to these names.",
            "What about what names should we use in order to name the random choices in your program in order to reuse as much randomness as possible and therefore get high acceptance rates?",
            "Idea is to name them according to their structural position in a trace.",
            "So to contrast, structural position in trace with naive position and trace, let's go back to our simple very simple model.",
            "Here we have those three sets of random variables.",
            "Here's what we call might call naive naming.",
            "As you're executing the trace, you encounter different sets of these random variables, and if you just named them, say according to their order in which they were encountered during execution.",
            "So name the first variable one named, the second variable to, etc.",
            "The problem is that you wouldn't be able to reuse things very well, and the reason is that the reason is that these numbers can change, so, so in the first example we sample.",
            "Seven variables total in the second trace here.",
            "Let's say that when the plus one is equal to two, we sample that we get to the second random variable.",
            "That's a gamma.",
            "We look it up in the database.",
            "That's fine, we can reuse that.",
            "We could do the 3rd.",
            "It's again.",
            "We can reuse that.",
            "Now we get to the 4th.",
            "Oh, wait."
        ],
        [
            "The database says.",
            "The database says that that is a gamma, but actually our traces shifted things just a little bit.",
            "We're down here in the in the normal section.",
            "Now there's a mismatch and we have to throw out the rest of the trace so more complicated distributions make this scheme affectively unworkable.",
            "We would much rather have something that looks like this."
        ],
        [
            "So you want to name random variables roughly according to their line number that they occur in the program.",
            "So, for example, we might name our first random variable 11, so it's the first random variable that occurs on the first line in our program.",
            "We might want to name these random variables 313233, so this is the.",
            "This is basically the first gamma here.",
            "The second Gamma, third gamma, etc, and then this would be something like the first first normal, the second normal, then the third normal.",
            "So now if we want to go to the database, we say we look up.",
            "We look up the name no problem.",
            "We can reuse this.",
            "We can reuse this.",
            "This one happens to missing, but that's OK. And then we can reuse these ones without problem.",
            "So we have aligned these traces as much as possible, which allows us to reuse as much randomness as possible, which allows us to get a high acceptance rate."
        ],
        [
            "'cause how do you actually generate these names?",
            "Well, we augment.",
            "We're transforming source code anyway, so we're going to augment our transformed source code with additional name generating code and in the paper we go through the details of this.",
            "We have specifications for two different kinds of languages, imperative languages as well as functional languages.",
            "And we use this to implement two different probabilistic programming languages.",
            "This is the actual technique we used to implement stochastic MATLAB on the imperative side and a new implementation of the MIT Church language which is called bear on the functional side.",
            "I'll just go quickly through the imperative naming specification on the stochastic Matlab side, because I think it's a little bit easier to understand the idea that we begin executing our function with a set of stacks, and we track the state of these stacks as we execute the program.",
            "So for example, we have a function stack align stack in a loop stack, and as we're entering new functions or new lines or new loops, we augment values of these stacks and we use the set of stacks at any given point in time as the name of the random variables.",
            "OK, so here's one example from the church side, just to show you how this works in a functional language.",
            "I only do this because because it allows us to because the example shows a recursive recursive model.",
            "So here's how you might define a geometric distribution.",
            "Hopefully he's on the list, right?",
            "So we defined a geometric distribution as a function which takes a parameter.",
            "This is the weight of the distribution, and we flip a coin with that with that."
        ],
        [
            "Wait, if it comes up heads we return 1, otherwise we call ourselves an ADD 1 to the result.",
            "So this will just sit there and recurse down until somebody flips heads which point it will end and then pop out and add up the numbers and that will be the sample from geometric distribution.",
            "And what are naming schemes schemes do?",
            "Is the thread additional information through this program?",
            "Which names the random choices that you encounter as you go through this recursive this recursive model.",
            "And that then it gets expanded into execution traces at runtime, which looks something like this.",
            "They're complicated.",
            "We will go through them.",
            "So the point of this actually is that you."
        ],
        [
            "At minimal interpretive overhead, but you're a lot, but you can control execution traces fairly efficiently.",
            "So the reason for this is that we can improve, we can.",
            "Leverages native ecosystem of the different languages, so just to give you 1 quick experimental results on this, we compared bear are lightweight implementation with the original MIT Church implementation of the Church programming probabilistic programming language on two different models.",
            "Here's a hidden Markov model in.",
            "Here is a latent dear slay allocation model.",
            "And on the left you see that as the number of latent states in this HMM gross MIT Church takes longer and longer and longer to do inference, whereas bear takes almost constant amounts of time.",
            "And the reason for that is that this lightweight implementation technique.",
            "Incurs minimal overhead and as a result, if you have a good compiler, it can generate very very fast code.",
            "Now, asymptotically the two are a little bit different aspects of the two algorithms that there actually is a crossover point which is illustrated here in this late and early allocation model, where my church will eventually do better than bare, but importantly, the implementation is quite simple, so the bear implementation had a code base that won't emphasize of the MIT Church implementation.",
            "OK, just a little bit of time remaining."
        ],
        [
            "Like to talk about how you can alternative ways to do inference in these models, and some new new inference options in probabilistic programming languages.",
            "So an important point is that our model is now."
        ],
        [
            "In a machine readable and executable format.",
            "It's just a program, and so we can give the code.",
            "We can play games with the way that we interpret that code.",
            "For example, we can give the code different sorts of nonstandard interpretations to help us accumulate information about random variables.",
            "So, for example, we can augment your code and do automatic differentiation, and then what that would allow us to do is compute the gradient of the likelihood with respect to all of the random choices that you make in your program, and use that then to build things like hybrid MCMC or Hamiltonian MCMC.",
            "Inference algorithms.",
            "You can use program analysis tools from compiler design, for example to identify known efficient substructures.",
            "So maybe you can identify if there's a chain like maybe do some sort of forward backward thing or or perhaps we identify some sort of.",
            "Efficient collapsible structure, maybe a conjugate pair?",
            "Dearsley multinomial and say the user forgot to collapse this out.",
            "I'll collapse it out for him.",
            "Could also do other sorts of operator overloading to track interesting information about your program, so I'll just give one quick example of this and then wrap up so in the.",
            "We all are familiar with Kumar Kumar problem.",
            "You have some sort of set of a set of diseases which are hidden and they are sparsely connected with many different symptoms which are observed and there's a low base rate on many of these diseases.",
            "The idea is you want to infer something about the diseases given the symptoms."
        ],
        [
            "And one way we thought that would be interesting to do this is with something we call dynamic dependency analysis.",
            "So the idea is that we use operator overloading to dynamically track fine grained dependencies in this model, so.",
            "Kind of the observation that the goal of the thinking was.",
            "But suppose we map inference in this thing.",
            "We're going to do some sort of stochastic greedy Hill climbing search in.",
            "This will maybe.",
            "How do you construct a good proposal where you maybe want to change many variables at once?",
            "And the idea would be constructing good proposal by generating a random proposal and then tracking information about that new random proposal and then getting rid of all the bad parts of the proposal, thereby leaving a good proposal.",
            "So suppose lots of changes to two.",
            "Maybe the state of these hidden diseases track the dependency structure of those variables as they flow through the model and hit the observations and then let the observations vote on whether or not they liked those changes and then back propagate information and inhibit the bad parts of the proposal."
        ],
        [
            "It turns out that this this works beautifully for searching these.",
            "At least this particular space.",
            "So here's an example.",
            "Performance iterations on the horizontal axis log likelihood on the vertical axis.",
            "Here's single site Gibbs and its performance.",
            "You can see it mixing going up, burning in mixing, but this this new op."
        ],
        [
            "You're overloading.",
            "Sort of constructing good proposals.",
            "Mechanism converges much, much faster.",
            "And this was important for us when it's when it's expensive to evaluate these functions.",
            "So single site gives is.",
            "It is inefficient because in these probabilistic programming languages, what happens is you pick one random variable, you propose a change, and then you re execute the program.",
            "So if we executing the program is expensive, you want to re execute it as few as few times as possible.",
            "So here's an example of this.",
            "Simple mesh inference example.",
            "So here's the let me just walk you through the program.",
            "This is an actual.",
            "This is an actual stochastic Matlab program.",
            "The idea is you're given some sort of base mesh, in this case sphere."
        ],
        [
            "And you want to figure out what the mesh parameters would have to be in order to render to something that looks like this face.",
            "So the idea is we're going to take a mesh.",
            "We're going to create a new mesh with our base mesh plus a bunch of random perturbations to the original vertices, and then we're going to importantly render the image.",
            "And this is some complicated crazy hairy GPU based thing.",
            "Ministik, but otherwise opaque to our program.",
            "And then we're going to add some noise and generate the final image.",
            "And so let me show you what the results of running are running these massively parallel dynamic dependency analysis.",
            "So I'm sorry I forgot to set this up a little bit, which is for any setting of the mesh parameters here.",
            "There's a sparse dependency between the triangle that creates the vertices which create one of these triangles and the pixels and the likelihoods of these pixels.",
            "Given this rendering and you can use that sparsity to your advantage.",
            "So here's what it looks like when you run it.",
            "And you can see this is actually running in about real time, and the point is that it's making proposals to thousands and thousands of random variables at once, and it's doing it fairly quickly because which is important because this rendering step is somewhat expensive, even though it is hardware seller."
        ],
        [
            "So.",
            "So that's my talk.",
            "So in summary, we've presented a generic technique for implementing probabilistic programming languages, and the idea is we name these random choices.",
            "We use the names to help us manipulate execution traces with a database of randomness practice a little bit about some structural naming conventions that we use to help us reuse randomness whenever possible, and then talked about how we use transformational compilation to actually implement these.",
            "We actually used this technique to implement now three different languages functional, which is this bear language.",
            "An imperative language which is our stochastic Matlab and there's a new one Pi stock which is actually stochastic Python.",
            "And then just the final point.",
            "Which is that, because these machines, because these models are now machine readable, there are many.",
            "What I think are fun and interesting possibilities both for."
        ],
        [
            "Listen for inference so thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we'll be talking about work we've done with our collaborators on probabilistic programming.",
                    "label": 0
                },
                {
                    "sent": "The official title of the paper and talk is lightweight implementations of probabilistic programming languages via transformation compilation, but as terrible title, and I thought as much.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Title might be implementing public programming languages without the agonizing pain, so we'll go.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Through some research we've been done on how you can implement probabilistic programming languages, hopefully without too much difficulty.",
                    "label": 0
                },
                {
                    "sent": "So the goal is to create probabilistic programming languages, which help us express complex probabilistic models, and I'll give you some examples of what that means in just a minute.",
                    "label": 0
                },
                {
                    "sent": "An observation is that we would like to leverage existing language infrastructures of existing languages.",
                    "label": 0
                },
                {
                    "sent": "So there's maybe no reason to invent a new language.",
                    "label": 0
                },
                {
                    "sent": "There's plenty of languages out there, and we would like to use what other people have done with those languages to help us build interesting probabilistic programming languages.",
                    "label": 0
                },
                {
                    "sent": "So things like compilers, parallelization, libraries, profilers, debuggers, etc.",
                    "label": 0
                },
                {
                    "sent": "So the contribution of this paper is a method to help us transform any language into a probabilistic programming language.",
                    "label": 0
                },
                {
                    "sent": "So this means that you could create, for example, stochastic version of Matlab which will talk about.",
                    "label": 0
                },
                {
                    "sent": "You could also use it, at least in principle, to create a stochastic version of JavaScript or Visual Basic so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Outline will be.",
                    "label": 0
                },
                {
                    "sent": "I'll talk quickly about what problems the programming is and then talk about our contribution, which is how you implement these at least.",
                    "label": 0
                },
                {
                    "sent": "One way to implement these fairly straightforward.",
                    "label": 0
                },
                {
                    "sent": "OK, so probably program.",
                    "label": 0
                },
                {
                    "sent": "What's the big idea?",
                    "label": 0
                },
                {
                    "sent": "The big idea is to use a programming.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Language to express a probabilistic model, not to implement a probabilistic model, but to express 1 to define one so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The the the rhythm and that shows like this, you write down some sort of randomized algorithm or some sort of function which makes a bunch of random choices.",
                    "label": 1
                },
                {
                    "sent": "You didn't fix the output of that function.",
                    "label": 0
                },
                {
                    "sent": "That's equivalent to conditioning the model and then you want to reason about what random choices your program would have had to have made in order to produce the output with high probability.",
                    "label": 1
                },
                {
                    "sent": "It's kind of like running the program backwards.",
                    "label": 0
                },
                {
                    "sent": "You start with the output of the program and figure out what needed to happen within the program in order to generate the output.",
                    "label": 1
                },
                {
                    "sent": "OK, and then the goal is to write some sort of maybe called probability compiler or interpreter or generic inference algorithm to perform inference in any program.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The user could potentially write.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me give you some examples about how we define these things.",
                    "label": 0
                },
                {
                    "sent": "So probabilistic programs define distributions by defining a distribution over possible execution traces of a program.",
                    "label": 1
                },
                {
                    "sent": "So here's a little example of a Gaussian mixture model.",
                    "label": 0
                },
                {
                    "sent": "So here I'll be using this Matlab S notation throughout the talk, so we define some sort of function which returns some variable X, call it, and this function will name GMM.",
                    "label": 0
                },
                {
                    "sent": "We first start by sampling some random variable Rand.",
                    "label": 0
                },
                {
                    "sent": "This is a uniform random variable is greater than .5, then X is negative .2 plus Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So in MATLAB, that's Brandon, else it's .2 + .5 times rated.",
                    "label": 0
                },
                {
                    "sent": "So you can see that if when we sample different choices for this round up here will actually execute different sequences of instructions as we move through this program.",
                    "label": 0
                },
                {
                    "sent": "So if we run this once, we get some output.",
                    "label": 0
                },
                {
                    "sent": "If we run it again, we get a different output.",
                    "label": 0
                },
                {
                    "sent": "We keep running this overtime.",
                    "label": 0
                },
                {
                    "sent": "We build up histogram of outputs.",
                    "label": 0
                },
                {
                    "sent": "Which looks like a mixture of two Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So this is how we define the distribution.",
                    "label": 0
                },
                {
                    "sent": "Of course our goal is will be to condition this and then do inference at it.",
                    "label": 0
                },
                {
                    "sent": "And I'll start talking about that in just a minute and a point to note here is that more complex.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distributions are created compositionally by combining many small pieces, so I just invented one more slide here.",
                    "label": 0
                },
                {
                    "sent": "Since we've been talking about topic models so much, I thought I'd give you an example.",
                    "label": 0
                },
                {
                    "sent": "I think I got it right.",
                    "label": 0
                },
                {
                    "sent": "I hope I did.",
                    "label": 0
                },
                {
                    "sent": "Dave lies in the audience, so this is LDA.",
                    "label": 0
                },
                {
                    "sent": "So sample a bunch of topics at the start from some sort of perhaps seriously, then, for each document you sample a topic distribution for each word you sample a topic for that word, then sample the actual work.",
                    "label": 0
                },
                {
                    "sent": "And again the goal will be take this program condition its output.",
                    "label": 0
                },
                {
                    "sent": "And do inference in the result.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me give you one more example, which is how we define nonparametric distributions in languages like this.",
                    "label": 0
                },
                {
                    "sent": "We do it with a technique which was first suggested in the church probabilistic programming paper and UI a couple of years ago.",
                    "label": 0
                },
                {
                    "sent": "We just called stochastic memorization.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that were given some sort of stochastic function G maybe randen stochastic memorizer is a new function which returns is a function which returns a new function F and it treats G as the base measure, then calls to F will return previous or new value from G4 according to, for example, additional process.",
                    "label": 1
                },
                {
                    "sent": "So to see this in action, here's a little way to give you a dear sleep process with a base measure that has a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So G is some Gaussian function.",
                    "label": 0
                },
                {
                    "sent": "F is some new function, so DPM is a stochastic memorizer.",
                    "label": 0
                },
                {
                    "sent": "It accepts as arguments GA base measure.",
                    "label": 0
                },
                {
                    "sent": "And then a concentration parameter and returns a new function and calls to F. Now return new values according to some deer sleep process.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we can use this to help us define more complex distribution.",
                    "label": 0
                },
                {
                    "sent": "So here's an example of some increasingly complex mixture models.",
                    "label": 0
                },
                {
                    "sent": "Here's our simple Gaussian mixture model with three components.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So K is 3 mute.",
                    "label": 0
                },
                {
                    "sent": "There's 3, three means an we generate 100 data points, and for each one we have to sample some sort of some sort of indicator variable, which tells us which major component was generated from, and then we grab the mean of that mixture component an add some random Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "So if the idea behind probabilistic programming is it makes us easy.",
                    "label": 0
                },
                {
                    "sent": "It makes it easy to explore new models and iterate models efficiently.",
                    "label": 0
                },
                {
                    "sent": "So if for example you wanted some sort of deer Slayer process mixture model, that might be as easy as.",
                    "label": 0
                },
                {
                    "sent": "As this so I'll take a base measure, which is a Gaussian, and I'll stochastically memorize it to create a new function, and then in order to generate 100 new data points, I sample from this memoized function plus noise.",
                    "label": 0
                },
                {
                    "sent": "Calls to this B will be distributed according to Adir sleep process with a Gaussian as a base measure.",
                    "label": 0
                },
                {
                    "sent": "But this function that gets memorized can be any function including another memoized function.",
                    "label": 0
                },
                {
                    "sent": "So a hierarchical dearsley process would just be the addition of 1 extra line of code.",
                    "label": 0
                },
                {
                    "sent": "So now we memorize the Gaussian and then we memorize the result of the memorization.",
                    "label": 0
                },
                {
                    "sent": "So the point is that this is this is not.",
                    "label": 0
                },
                {
                    "sent": "This is not.",
                    "label": 0
                },
                {
                    "sent": "It's not complicated.",
                    "label": 0
                },
                {
                    "sent": "Now to explore new models and to define new models in a way that makes intuitive sense.",
                    "label": 0
                },
                {
                    "sent": "This is probably superior.",
                    "label": 0
                },
                {
                    "sent": "Give you one more example just to tell you what I think is a particularly interesting point about these languages, which is if your language has something like in avalyn and you can reason about the structure of the model itself, perhaps like this.",
                    "label": 0
                },
                {
                    "sent": "So here's a little probabilistic program which will attempt to induce a program, so the idea is we sample some text of code from a probabilistic context free grammar.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We then evaluate text to give us a function and then generate our data by calling that function on some inputs and maybe add some noise.",
                    "label": 0
                },
                {
                    "sent": "We actually ran this little program in our stochastic Matlab probabilistic programming language, not a little curve that looks like this.",
                    "label": 0
                },
                {
                    "sent": "What this is saying is, is saying as the model is doing inference as time is increasing, likelihood is going up.",
                    "label": 0
                },
                {
                    "sent": "It's learning something.",
                    "label": 0
                },
                {
                    "sent": "Learning something about what.",
                    "label": 0
                },
                {
                    "sent": "It's learning something about the actual structure of a function which Maps inputs to outputs.",
                    "label": 1
                },
                {
                    "sent": "The point is you can mix and match this with any of the other language constructs that have already showed you.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem is that programming in a nutshell, so now like to take us to the topic of this talk, which is how we.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually implement these things in some sort of reasonable way.",
                    "label": 0
                },
                {
                    "sent": "So the goal will be ultimately to perform inference in an arbitrary program, and approach will be MCMC, unsurprisingly, So what we'll need is some mechanism for making proposals and scoring and scoring them, therefore will need some ability to control the execution of these programs, hopefully without the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rising paint, so one observation about these execution traces is the following, so here's a little model which goes through an samples M from a class on it, then samples.",
                    "label": 0
                },
                {
                    "sent": "Am gammas and then it samples M Gaussians?",
                    "label": 0
                },
                {
                    "sent": "So what I'd like to do is walk you through the execution of this program.",
                    "label": 0
                },
                {
                    "sent": "So here's what would happen if you were to just run this program in Matlab, you would encounter several random choices.",
                    "label": 0
                },
                {
                    "sent": "So the first thing you encounter is this M. So you sample some Python variable, and here I'm going to show that with blue and let's suppose we happily sample two.",
                    "label": 0
                },
                {
                    "sent": "You then iterate this loop 2 times and you'll sample two out two gamma variables and then you move down here and you'll sample.",
                    "label": 0
                },
                {
                    "sent": "To two Gaussian variables.",
                    "label": 0
                },
                {
                    "sent": "If you were to execute this program again, you might get a different a different value for M An.",
                    "label": 0
                },
                {
                    "sent": "Suppose, for example, we sampled one.",
                    "label": 0
                },
                {
                    "sent": "Then we would sample just one gamma, and then we would sample just one one Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK, so one observation.",
                    "label": 0
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "Two, if two traces.",
                    "label": 0
                },
                {
                    "sent": "We call these, but we call these execution traces.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is an execution trace as it moves through the program.",
                    "label": 0
                },
                {
                    "sent": "If two choice if two traces make all of the same choices and their execution paths will be identical.",
                    "label": 1
                },
                {
                    "sent": "Because everything else in the program is deterministic, so we'll use that fact to our advantage.",
                    "label": 0
                },
                {
                    "sent": "This suggests the following.",
                    "label": 0
                },
                {
                    "sent": "The following approach for controlling the execute.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of these, these these programs via transformation compilation.",
                    "label": 0
                },
                {
                    "sent": "So the idea is we take your probabilistic program and we give every random choice that your program makes a name, and I'll talk about those names in just a minute.",
                    "label": 1
                },
                {
                    "sent": "I then rewrite your code to make it deterministic.",
                    "label": 1
                },
                {
                    "sent": "So I take all of the random choices out and I put new deterministic functions in and those new deterministic functions use the name of the random choice as it's encountered in the program to look up its value in some sort of database of randomness.",
                    "label": 1
                },
                {
                    "sent": "And now if I fiddle with the values in the database of randomness.",
                    "label": 0
                },
                {
                    "sent": "I can imples.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Control the execution traces of your programs.",
                    "label": 0
                },
                {
                    "sent": "Building on this, then we can build inference algorithms, so to do MCMC over execution traces, here's the general idea you're given in some sort of execution trace, you reconsider some random choice that you made in order to generate that trace, and then you update the trace using as much random, reusing as much randomness as possible.",
                    "label": 1
                },
                {
                    "sent": "Scoring, accept, reject.",
                    "label": 0
                },
                {
                    "sent": "The official algorithm looks like this, and it's not that complicated.",
                    "label": 0
                },
                {
                    "sent": "The idea is the idea is.",
                    "label": 0
                },
                {
                    "sent": "I should say this is.",
                    "label": 0
                },
                {
                    "sent": "I mean this is pretty standard.",
                    "label": 0
                },
                {
                    "sent": "This is pretty standard MH sort of sorts of proposals and scoring.",
                    "label": 0
                },
                {
                    "sent": "The one trick is that you have these databases which help you manage all of the random variables.",
                    "label": 0
                },
                {
                    "sent": "Now I've highlighted in bold the idea on .3 here that we want to reuse as much randomness as possible.",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "Well, the intuition is if you have some execution through some program space.",
                    "label": 0
                },
                {
                    "sent": "And you want to do some proposal and come up with a new trace.",
                    "label": 0
                },
                {
                    "sent": "You want the two traces to be pretty close together, or else you're probably going to reject them, so you want to walk carefully through this space.",
                    "label": 0
                },
                {
                    "sent": "If your moves are too big, then you'll typically reject.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now let's get to these names.",
                    "label": 0
                },
                {
                    "sent": "What about what names should we use in order to name the random choices in your program in order to reuse as much randomness as possible and therefore get high acceptance rates?",
                    "label": 0
                },
                {
                    "sent": "Idea is to name them according to their structural position in a trace.",
                    "label": 1
                },
                {
                    "sent": "So to contrast, structural position in trace with naive position and trace, let's go back to our simple very simple model.",
                    "label": 0
                },
                {
                    "sent": "Here we have those three sets of random variables.",
                    "label": 0
                },
                {
                    "sent": "Here's what we call might call naive naming.",
                    "label": 0
                },
                {
                    "sent": "As you're executing the trace, you encounter different sets of these random variables, and if you just named them, say according to their order in which they were encountered during execution.",
                    "label": 0
                },
                {
                    "sent": "So name the first variable one named, the second variable to, etc.",
                    "label": 0
                },
                {
                    "sent": "The problem is that you wouldn't be able to reuse things very well, and the reason is that the reason is that these numbers can change, so, so in the first example we sample.",
                    "label": 0
                },
                {
                    "sent": "Seven variables total in the second trace here.",
                    "label": 0
                },
                {
                    "sent": "Let's say that when the plus one is equal to two, we sample that we get to the second random variable.",
                    "label": 0
                },
                {
                    "sent": "That's a gamma.",
                    "label": 1
                },
                {
                    "sent": "We look it up in the database.",
                    "label": 0
                },
                {
                    "sent": "That's fine, we can reuse that.",
                    "label": 0
                },
                {
                    "sent": "We could do the 3rd.",
                    "label": 0
                },
                {
                    "sent": "It's again.",
                    "label": 0
                },
                {
                    "sent": "We can reuse that.",
                    "label": 0
                },
                {
                    "sent": "Now we get to the 4th.",
                    "label": 0
                },
                {
                    "sent": "Oh, wait.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The database says.",
                    "label": 0
                },
                {
                    "sent": "The database says that that is a gamma, but actually our traces shifted things just a little bit.",
                    "label": 0
                },
                {
                    "sent": "We're down here in the in the normal section.",
                    "label": 1
                },
                {
                    "sent": "Now there's a mismatch and we have to throw out the rest of the trace so more complicated distributions make this scheme affectively unworkable.",
                    "label": 0
                },
                {
                    "sent": "We would much rather have something that looks like this.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you want to name random variables roughly according to their line number that they occur in the program.",
                    "label": 1
                },
                {
                    "sent": "So, for example, we might name our first random variable 11, so it's the first random variable that occurs on the first line in our program.",
                    "label": 0
                },
                {
                    "sent": "We might want to name these random variables 313233, so this is the.",
                    "label": 0
                },
                {
                    "sent": "This is basically the first gamma here.",
                    "label": 0
                },
                {
                    "sent": "The second Gamma, third gamma, etc, and then this would be something like the first first normal, the second normal, then the third normal.",
                    "label": 0
                },
                {
                    "sent": "So now if we want to go to the database, we say we look up.",
                    "label": 0
                },
                {
                    "sent": "We look up the name no problem.",
                    "label": 0
                },
                {
                    "sent": "We can reuse this.",
                    "label": 0
                },
                {
                    "sent": "We can reuse this.",
                    "label": 0
                },
                {
                    "sent": "This one happens to missing, but that's OK. And then we can reuse these ones without problem.",
                    "label": 0
                },
                {
                    "sent": "So we have aligned these traces as much as possible, which allows us to reuse as much randomness as possible, which allows us to get a high acceptance rate.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "'cause how do you actually generate these names?",
                    "label": 0
                },
                {
                    "sent": "Well, we augment.",
                    "label": 0
                },
                {
                    "sent": "We're transforming source code anyway, so we're going to augment our transformed source code with additional name generating code and in the paper we go through the details of this.",
                    "label": 0
                },
                {
                    "sent": "We have specifications for two different kinds of languages, imperative languages as well as functional languages.",
                    "label": 0
                },
                {
                    "sent": "And we use this to implement two different probabilistic programming languages.",
                    "label": 0
                },
                {
                    "sent": "This is the actual technique we used to implement stochastic MATLAB on the imperative side and a new implementation of the MIT Church language which is called bear on the functional side.",
                    "label": 1
                },
                {
                    "sent": "I'll just go quickly through the imperative naming specification on the stochastic Matlab side, because I think it's a little bit easier to understand the idea that we begin executing our function with a set of stacks, and we track the state of these stacks as we execute the program.",
                    "label": 1
                },
                {
                    "sent": "So for example, we have a function stack align stack in a loop stack, and as we're entering new functions or new lines or new loops, we augment values of these stacks and we use the set of stacks at any given point in time as the name of the random variables.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's one example from the church side, just to show you how this works in a functional language.",
                    "label": 0
                },
                {
                    "sent": "I only do this because because it allows us to because the example shows a recursive recursive model.",
                    "label": 0
                },
                {
                    "sent": "So here's how you might define a geometric distribution.",
                    "label": 0
                },
                {
                    "sent": "Hopefully he's on the list, right?",
                    "label": 0
                },
                {
                    "sent": "So we defined a geometric distribution as a function which takes a parameter.",
                    "label": 0
                },
                {
                    "sent": "This is the weight of the distribution, and we flip a coin with that with that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wait, if it comes up heads we return 1, otherwise we call ourselves an ADD 1 to the result.",
                    "label": 0
                },
                {
                    "sent": "So this will just sit there and recurse down until somebody flips heads which point it will end and then pop out and add up the numbers and that will be the sample from geometric distribution.",
                    "label": 0
                },
                {
                    "sent": "And what are naming schemes schemes do?",
                    "label": 0
                },
                {
                    "sent": "Is the thread additional information through this program?",
                    "label": 0
                },
                {
                    "sent": "Which names the random choices that you encounter as you go through this recursive this recursive model.",
                    "label": 0
                },
                {
                    "sent": "And that then it gets expanded into execution traces at runtime, which looks something like this.",
                    "label": 0
                },
                {
                    "sent": "They're complicated.",
                    "label": 0
                },
                {
                    "sent": "We will go through them.",
                    "label": 0
                },
                {
                    "sent": "So the point of this actually is that you.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At minimal interpretive overhead, but you're a lot, but you can control execution traces fairly efficiently.",
                    "label": 0
                },
                {
                    "sent": "So the reason for this is that we can improve, we can.",
                    "label": 1
                },
                {
                    "sent": "Leverages native ecosystem of the different languages, so just to give you 1 quick experimental results on this, we compared bear are lightweight implementation with the original MIT Church implementation of the Church programming probabilistic programming language on two different models.",
                    "label": 0
                },
                {
                    "sent": "Here's a hidden Markov model in.",
                    "label": 0
                },
                {
                    "sent": "Here is a latent dear slay allocation model.",
                    "label": 0
                },
                {
                    "sent": "And on the left you see that as the number of latent states in this HMM gross MIT Church takes longer and longer and longer to do inference, whereas bear takes almost constant amounts of time.",
                    "label": 0
                },
                {
                    "sent": "And the reason for that is that this lightweight implementation technique.",
                    "label": 0
                },
                {
                    "sent": "Incurs minimal overhead and as a result, if you have a good compiler, it can generate very very fast code.",
                    "label": 0
                },
                {
                    "sent": "Now, asymptotically the two are a little bit different aspects of the two algorithms that there actually is a crossover point which is illustrated here in this late and early allocation model, where my church will eventually do better than bare, but importantly, the implementation is quite simple, so the bear implementation had a code base that won't emphasize of the MIT Church implementation.",
                    "label": 0
                },
                {
                    "sent": "OK, just a little bit of time remaining.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like to talk about how you can alternative ways to do inference in these models, and some new new inference options in probabilistic programming languages.",
                    "label": 0
                },
                {
                    "sent": "So an important point is that our model is now.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In a machine readable and executable format.",
                    "label": 0
                },
                {
                    "sent": "It's just a program, and so we can give the code.",
                    "label": 1
                },
                {
                    "sent": "We can play games with the way that we interpret that code.",
                    "label": 0
                },
                {
                    "sent": "For example, we can give the code different sorts of nonstandard interpretations to help us accumulate information about random variables.",
                    "label": 0
                },
                {
                    "sent": "So, for example, we can augment your code and do automatic differentiation, and then what that would allow us to do is compute the gradient of the likelihood with respect to all of the random choices that you make in your program, and use that then to build things like hybrid MCMC or Hamiltonian MCMC.",
                    "label": 0
                },
                {
                    "sent": "Inference algorithms.",
                    "label": 0
                },
                {
                    "sent": "You can use program analysis tools from compiler design, for example to identify known efficient substructures.",
                    "label": 1
                },
                {
                    "sent": "So maybe you can identify if there's a chain like maybe do some sort of forward backward thing or or perhaps we identify some sort of.",
                    "label": 0
                },
                {
                    "sent": "Efficient collapsible structure, maybe a conjugate pair?",
                    "label": 0
                },
                {
                    "sent": "Dearsley multinomial and say the user forgot to collapse this out.",
                    "label": 0
                },
                {
                    "sent": "I'll collapse it out for him.",
                    "label": 0
                },
                {
                    "sent": "Could also do other sorts of operator overloading to track interesting information about your program, so I'll just give one quick example of this and then wrap up so in the.",
                    "label": 0
                },
                {
                    "sent": "We all are familiar with Kumar Kumar problem.",
                    "label": 0
                },
                {
                    "sent": "You have some sort of set of a set of diseases which are hidden and they are sparsely connected with many different symptoms which are observed and there's a low base rate on many of these diseases.",
                    "label": 0
                },
                {
                    "sent": "The idea is you want to infer something about the diseases given the symptoms.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one way we thought that would be interesting to do this is with something we call dynamic dependency analysis.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we use operator overloading to dynamically track fine grained dependencies in this model, so.",
                    "label": 1
                },
                {
                    "sent": "Kind of the observation that the goal of the thinking was.",
                    "label": 0
                },
                {
                    "sent": "But suppose we map inference in this thing.",
                    "label": 0
                },
                {
                    "sent": "We're going to do some sort of stochastic greedy Hill climbing search in.",
                    "label": 0
                },
                {
                    "sent": "This will maybe.",
                    "label": 0
                },
                {
                    "sent": "How do you construct a good proposal where you maybe want to change many variables at once?",
                    "label": 1
                },
                {
                    "sent": "And the idea would be constructing good proposal by generating a random proposal and then tracking information about that new random proposal and then getting rid of all the bad parts of the proposal, thereby leaving a good proposal.",
                    "label": 1
                },
                {
                    "sent": "So suppose lots of changes to two.",
                    "label": 0
                },
                {
                    "sent": "Maybe the state of these hidden diseases track the dependency structure of those variables as they flow through the model and hit the observations and then let the observations vote on whether or not they liked those changes and then back propagate information and inhibit the bad parts of the proposal.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It turns out that this this works beautifully for searching these.",
                    "label": 0
                },
                {
                    "sent": "At least this particular space.",
                    "label": 0
                },
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "Performance iterations on the horizontal axis log likelihood on the vertical axis.",
                    "label": 0
                },
                {
                    "sent": "Here's single site Gibbs and its performance.",
                    "label": 0
                },
                {
                    "sent": "You can see it mixing going up, burning in mixing, but this this new op.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're overloading.",
                    "label": 0
                },
                {
                    "sent": "Sort of constructing good proposals.",
                    "label": 0
                },
                {
                    "sent": "Mechanism converges much, much faster.",
                    "label": 0
                },
                {
                    "sent": "And this was important for us when it's when it's expensive to evaluate these functions.",
                    "label": 0
                },
                {
                    "sent": "So single site gives is.",
                    "label": 0
                },
                {
                    "sent": "It is inefficient because in these probabilistic programming languages, what happens is you pick one random variable, you propose a change, and then you re execute the program.",
                    "label": 0
                },
                {
                    "sent": "So if we executing the program is expensive, you want to re execute it as few as few times as possible.",
                    "label": 0
                },
                {
                    "sent": "So here's an example of this.",
                    "label": 0
                },
                {
                    "sent": "Simple mesh inference example.",
                    "label": 0
                },
                {
                    "sent": "So here's the let me just walk you through the program.",
                    "label": 0
                },
                {
                    "sent": "This is an actual.",
                    "label": 0
                },
                {
                    "sent": "This is an actual stochastic Matlab program.",
                    "label": 0
                },
                {
                    "sent": "The idea is you're given some sort of base mesh, in this case sphere.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you want to figure out what the mesh parameters would have to be in order to render to something that looks like this face.",
                    "label": 0
                },
                {
                    "sent": "So the idea is we're going to take a mesh.",
                    "label": 0
                },
                {
                    "sent": "We're going to create a new mesh with our base mesh plus a bunch of random perturbations to the original vertices, and then we're going to importantly render the image.",
                    "label": 0
                },
                {
                    "sent": "And this is some complicated crazy hairy GPU based thing.",
                    "label": 0
                },
                {
                    "sent": "Ministik, but otherwise opaque to our program.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to add some noise and generate the final image.",
                    "label": 0
                },
                {
                    "sent": "And so let me show you what the results of running are running these massively parallel dynamic dependency analysis.",
                    "label": 0
                },
                {
                    "sent": "So I'm sorry I forgot to set this up a little bit, which is for any setting of the mesh parameters here.",
                    "label": 0
                },
                {
                    "sent": "There's a sparse dependency between the triangle that creates the vertices which create one of these triangles and the pixels and the likelihoods of these pixels.",
                    "label": 0
                },
                {
                    "sent": "Given this rendering and you can use that sparsity to your advantage.",
                    "label": 0
                },
                {
                    "sent": "So here's what it looks like when you run it.",
                    "label": 0
                },
                {
                    "sent": "And you can see this is actually running in about real time, and the point is that it's making proposals to thousands and thousands of random variables at once, and it's doing it fairly quickly because which is important because this rendering step is somewhat expensive, even though it is hardware seller.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So that's my talk.",
                    "label": 0
                },
                {
                    "sent": "So in summary, we've presented a generic technique for implementing probabilistic programming languages, and the idea is we name these random choices.",
                    "label": 1
                },
                {
                    "sent": "We use the names to help us manipulate execution traces with a database of randomness practice a little bit about some structural naming conventions that we use to help us reuse randomness whenever possible, and then talked about how we use transformational compilation to actually implement these.",
                    "label": 1
                },
                {
                    "sent": "We actually used this technique to implement now three different languages functional, which is this bear language.",
                    "label": 0
                },
                {
                    "sent": "An imperative language which is our stochastic Matlab and there's a new one Pi stock which is actually stochastic Python.",
                    "label": 0
                },
                {
                    "sent": "And then just the final point.",
                    "label": 0
                },
                {
                    "sent": "Which is that, because these machines, because these models are now machine readable, there are many.",
                    "label": 0
                },
                {
                    "sent": "What I think are fun and interesting possibilities both for.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Listen for inference so thanks.",
                    "label": 0
                }
            ]
        }
    }
}