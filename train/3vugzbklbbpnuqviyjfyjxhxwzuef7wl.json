{
    "id": "3vugzbklbbpnuqviyjfyjxhxwzuef7wl",
    "title": "Augmenting Dual Decomposition for MAP Inference",
    "info": {
        "author": [
            "Andr\u00e9 F. T. Martins, Language Technologies Institute, Carnegie Mellon University"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_martins_add/",
    "segmentation": [
        [
            "So this is joint work with no Smith and direction from CMU and federal and modify grid from IST in Portugal."
        ],
        [
            "And so the thing we are interested here is to compute the maximum posteriori in a graphical model.",
            "And this is an NP hard problem in general for general graph."
        ],
        [
            "People have thought about LP relaxations to approximate map computation."
        ],
        [
            "And popular methods recently was using the dual decomposition technique.",
            "And do it.",
            "We disagreed an algorithm, so the idea is that the split the graph into several pieces and then you perform inference in each of these pieces by dualizing and do updates using the subgradient algorithm."
        ],
        [
            "So there are some good points about this approach.",
            "It's very simple.",
            "It converges.",
            "You can parallelize it."
        ],
        [
            "However, it becomes too slow if you have many slaves or problems.",
            "So if you split your graph into many pieces."
        ],
        [
            "And the proposal here is to combine this idea of dual decomposition.",
            "We documented Lagrangian methods."
        ],
        [
            "We are going to obtain still parallelizable algorithm, but now much much faster to reach consensus between all these slave district problems.",
            "We're going to see that we can handle global hard constraints efficiently, and we're going to show some experiments in easing models and in parsing of natural language."
        ],
        [
            "OK."
        ],
        [
            "So let's start."
        ],
        [
            "By formulating the problem, let's suppose that you have a vector of discrete random variables with the Gibbs distribution like that."
        ],
        [
            "Where a is a set of factors and PTI are unary lock potentials and find a higher hurdle of potential."
        ],
        [
            "So we can represent these with the factor graph.",
            "And."
        ],
        [
            "Here we're going to assume you want to allow for deterministic potentials, so we're going to allow things."
        ],
        [
            "Like for example, enforcing that from a group of variables, only one of them is one of the other ones have to be 0."
        ],
        [
            "Or we other kinds of things that we want to do is to enforce that the overall assignment of variables encode some valid code word in some fixed vocabulary."
        ],
        [
            "Or you might even add a new variable to our model that is supposed to behave as the conjunction of existing variables.",
            "Or some other logical function?"
        ],
        [
            "So our goal is going to be to compute the assignment of variables that which maximizes the overall probability, and this is represented by this combinatorial problem over here.",
            "Now this can be turned into a continuous."
        ],
        [
            "Problem by adding marginal variable."
        ],
        [
            "So here M of G is the marginal polytope of the graph.",
            "In other words, the sets of marginals that are realizable for some choice of potentials.",
            "And we are stacking all potentials into vectors ttime and 5A.",
            "And replacing our variables by these marginal variables."
        ],
        [
            "So although this is now continuous problem, it's still very hard to solve it.",
            "It's still NP hard because the marginal polytope doesn't have in general a concise representation, and So what often is that?"
        ],
        [
            "One is to use a local approximation of this polytope.",
            "So that local approximation essentially instead of requiring globally consistent marginals, only checks for local consistency where local here means at each factor.",
            "So in other words, it constraints at each factory constraints marginals of variables that are attached to that factor and marginals of the overall assignment of that factor to be locally consistent."
        ],
        [
            "So as a remark, this is another approximation of the marginal polytope.",
            "And the integer points in it correspond exactly to the vertices of the marginal polytope."
        ],
        [
            "So the LP relaxation consists in replacing M of Gmail of."
        ],
        [
            "So he gets this upper bound over here."
        ],
        [
            "Now let's see how decomposition is applied here."
        ],
        [
            "So this was done by commodities and colleagues three years ago."
        ],
        [
            "We the first thing is."
        ],
        [
            "Split our graph."
        ],
        [
            "Two pieces, so in this case we just split these variables and create some replicas on variables that are shared by several factors."
        ],
        [
            "So this was the original problem that you wanted to solve.",
            "the LP relaxation that we have just seen."
        ],
        [
            "And by splitting we create some copies, some local copies of marginal variables at each factor, and we introduce these coupling constraints that make sure that everything here is consistent.",
            "So this makes sure that these variables agree on their overlaps."
        ],
        [
            "So note that we can rewrite this objective function as a sum of functions for each factor OK, and so this problem will be very easy to solve if not for this copy constraints, because everything will be separable."
        ],
        [
            "So the idea of decomposition is."
        ],
        [
            "To dualize out these LaGrange, this coupling constraints by introducing LaGrange multipliers.",
            "And I'm going to denote these LaGrange multipliers by Lambda A, so this is the lab range multiplier with respect to factor a, an variable I."
        ],
        [
            "And the dual formulation look like looks like this.",
            "So we want to minimize the LaGrange dual function, which can be written as a sum for all factors of an inner maximization problem.",
            "Subject to the constraint that the LaGrange variables belong to these affine subsets."
        ],
        [
            "So we now have one slice of problem for each factor I'm referring to this maximization problem over here, and this can be done in parallel becausw.",
            "These things are completely independent over across each factor."
        ],
        [
            "And the master problem consists in minimizing this function, and this can be done by using the projected subgradient."
        ],
        [
            "So we can note that subgradient respect to Lambda A is going to consist on the optimum marginal variable.",
            "The solution of this problem and this problem is nothing but the map inference problem.",
            "But now local to factor A.",
            "So we break this problem into several map.",
            "So problems each for factor 8."
        ],
        [
            "And the projection step on to these set Lambda is going to consistently in a simple centering operation, so overall."
        ],
        [
            "So this gradient algorithm we do, the composition looks like this.",
            "We for each iteration you go through each factor.",
            "We can do this in parallel.",
            "We compute scenario potentials by taking into consideration the current assignment of LaGrange variables and we just do map inference at that factor.",
            "So this is supposed to be an easy operation.",
            "Then we reassemble all the map assignments that you got, and you compute the average for each variable.",
            "Finally, we do an update of the LaGrange multipliers by subtracting out the average over the solution that we have obtained for that factor.",
            "So this converges for some choice of step size sequence.",
            "However, it is too slow if you have many slaves, so our contribution here is superposing augmented."
        ],
        [
            "Method for these to speed up things.",
            "And let's first."
        ],
        [
            "Write the augmented Lagrangian function.",
            "It looks like this, so this is just a Lagrangian function and you are subtracting a residual term which is a quadratic penalization of the couple constraints and you have.",
            "So in other words, we had our coupling constraints that make sure that everything agrees in their overlaps and you are penalizing violations of these constraints by adding these residual term over here."
        ],
        [
            "So the thing that you want to do is to maximize this augmented Lagrangian function respect from you and new and to minimize it with respect to Lambda."
        ],
        [
            "But now you have a problem, which is the fact that we don't have a separable problem anymore.",
            "So this thing is breaking this probability."
        ],
        [
            "Now we're going to use a very old method optimization called alternating direction method of multipliers or a DMM, where essentially we turn this into two alternating optimization problems."
        ],
        [
            "So first you optimize with respect to mu.",
            "And this can actually be done in close form.",
            "This is just an average open averaging operation."
        ],
        [
            "And then we maximize this with respect and you and that part can be carried out in parallel at each factor.",
            "We're going to see that in a minute."
        ],
        [
            "Finally, we do apply the method of multipliers to update the LaGrange multipliers.",
            "So we make a step in the LaGrange variables."
        ],
        [
            "Overall, the algorithm is quite similar to what we had before for dual decomposition with subgradients.",
            "The only difference is that instead of a map inference problem, we now have a quadratic problem at each slave.",
            "OK, so there are other differences, but this is the most important one."
        ],
        [
            "Now.",
            "This problem is actually looks like a projection until the marginal polytope, and it's actually taking that form for some."
        ],
        [
            "Ticular cases, and there's some conditions, so this is always going to converge, and there are some conditions.",
            "It's also converges even when you just approximately solve this problem."
        ],
        [
            "Now let's see how to solve the quadratic problem for some factors of interest.",
            "For binary pairwise factors, there is a closed form solution.",
            "It's not difficult to derive it.",
            "It's actually in the paper."
        ],
        [
            "Now you are also interested in the case where you have some hard constraint factors, factors that impose our constraints over the over the variables.",
            "And these are important for many applications, so these are factors that have deterministic potentials that are essentially indicator functions of acceptance of some acceptance set essay."
        ],
        [
            "It turns out that in that case the quadratic problem that you need to solve is nothing but a quadratic projection and using projection onto the convex soul of that acceptance set."
        ],
        [
            "And for many hard factors that impose logical constraints, this can be done quite efficiently by requiring only a sort operation.",
            "So I'm not going into details here about this, but this is similar to projecting onto an L1 ball, where you can also solve that problem by doing a sort, so we can actually generalize that for many.",
            "Marginal polytopes of interest in this."
        ],
        [
            "Now, if you have larger slaves and this often arises in prices, sometimes we want to consider a larger piece of the graph.",
            "We can still solve approximately this quadratic problem by doing something like cycle projection algorithms or everything, or any other approach.",
            "This is still work in progress."
        ],
        [
            "So let's now see some expert."
        ],
        [
            "Once.",
            "We tested this in binary pairwise MRF's using models, agreed."
        ],
        [
            "And we compared the HMM against the subgradient method."
        ],
        [
            "And against Maxim Diffusion, which star updates?",
            "So maximum diffusion is essentially performing block coordinate descent in the dual.",
            "Buy a message passing scheme."
        ],
        [
            "And so these are the results.",
            "The continuous line is denoting the progress in the dual and the dashed lines are the primal objectives.",
            "So we can see that this upgrade and is very slow.",
            "It has many slides for this example.",
            "So this is 1 slide per head.",
            "And maximum diffusion is still slower, so that is the blue line while."
        ],
        [
            "Our methods is faster than both, specially if you look at the progress in the primal, it's progress is quite fast, so it converges faster to good solution in the primal.",
            "And our interpretation for that is that we have this quadratic term that in some sense pushes for primal feasibility.",
            "So you get you get a primal feasible solution quite fast."
        ],
        [
            "OK, so the second problem that we're going to consider is parsing of dependency parsing and this is actually a problem that requires hard constraint factors.",
            "So, just to summarize, we have a sentence like this and the goal is to recover the syntactic structure of the sentence.",
            "In other words, we want to get these dependency links over here.",
            "And the overall constraint is that all the sets of links has to define a tree, so this has to be a legal tree."
        ],
        [
            "I'm not going into details on this, can be done on models to achieve this, but I'm just comparing results with two existing models for this problem.",
            "When is a is a model that has a factor in forcing the overall thing to be 3 and the other model is based on multicommodity flows.",
            "So it has several factors that in second together enforce the overall structure to be tree and.",
            "We use the 2nd order model that essentially tries to correlate pairs of arcs and these are the results that we obtain, so we can see that the DMM is faster than this.",
            "A gradient for both flow and three models.",
            "And still faster than then star MSD.",
            "So even the left side desire experiments with synthetic data and this ones are in the Pantry Bank, which is a corpus of real sentences."
        ],
        [
            "OK, so this is what this is.",
            "What you have for this."
        ],
        [
            "Oh"
        ],
        [
            "So to conclude, we have a new algorithm for LP map inference.",
            "It's this algorithm is decomposable.",
            "All slides can be solved in parallel.",
            "We essentially are allowing the simplicity of decomposition with the power of augmented Lagrangian methods.",
            "We think that this is suitable particularly for problems that have many slaves, or that you cannot easily break into into.",
            "Into pieces of large slaves.",
            "I didn't talk about that, but you can actually provide some optimality certificates for the LP relaxation by taking into by using.",
            "Examining the quadratic penalty, you can actually check if you are close to a primal feasible solution.",
            "You can save a lot of computation by caching and warm starting, and I'd like to mention some related work in celebrating do decomposition by which Daphne, Kohler and colleagues.",
            "This was a recent paper in ICML.",
            "It's a different approach from hours it does.",
            "Entropic projections, but it's a different approach, and there's also some work by product graphic Kumar and others on doing quadratic projections to solve the problem in the primal.",
            "So both these approaches are different from this one, but they share some.",
            "Some points are shared between all these approaches.",
            "In the future, we plan to extend this for larger slaves and to investigate whether we can do approximate HMM steps for those problems."
        ],
        [
            "That's solid yeah, thanks.",
            "How easy.",
            "Binary connections for general command.",
            "For generating you motivated with factor graphs.",
            "Binary potential sensory limited things and.",
            "Or what about ternary and other?",
            "Yeah, so you can come up with as soon as you can solve that quadratic problem, either exactly or approximated.",
            "You can still use these methods so you don't need to have binary potentials.",
            "Use this for.",
            "Multi valued variables and general factors.",
            "General limitation is ability of solving the quadratic program.",
            "But I can come up with ideas about how to approximate those problems.",
            "So in the limit you can combine some ideas of approximate methods to solve the QP and just plug them in into this algorithm.",
            "Small question, then we take the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is joint work with no Smith and direction from CMU and federal and modify grid from IST in Portugal.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the thing we are interested here is to compute the maximum posteriori in a graphical model.",
                    "label": 0
                },
                {
                    "sent": "And this is an NP hard problem in general for general graph.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People have thought about LP relaxations to approximate map computation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And popular methods recently was using the dual decomposition technique.",
                    "label": 1
                },
                {
                    "sent": "And do it.",
                    "label": 0
                },
                {
                    "sent": "We disagreed an algorithm, so the idea is that the split the graph into several pieces and then you perform inference in each of these pieces by dualizing and do updates using the subgradient algorithm.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are some good points about this approach.",
                    "label": 0
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                },
                {
                    "sent": "It converges.",
                    "label": 0
                },
                {
                    "sent": "You can parallelize it.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, it becomes too slow if you have many slaves or problems.",
                    "label": 0
                },
                {
                    "sent": "So if you split your graph into many pieces.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the proposal here is to combine this idea of dual decomposition.",
                    "label": 0
                },
                {
                    "sent": "We documented Lagrangian methods.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are going to obtain still parallelizable algorithm, but now much much faster to reach consensus between all these slave district problems.",
                    "label": 0
                },
                {
                    "sent": "We're going to see that we can handle global hard constraints efficiently, and we're going to show some experiments in easing models and in parsing of natural language.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By formulating the problem, let's suppose that you have a vector of discrete random variables with the Gibbs distribution like that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where a is a set of factors and PTI are unary lock potentials and find a higher hurdle of potential.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can represent these with the factor graph.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we're going to assume you want to allow for deterministic potentials, so we're going to allow things.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like for example, enforcing that from a group of variables, only one of them is one of the other ones have to be 0.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or we other kinds of things that we want to do is to enforce that the overall assignment of variables encode some valid code word in some fixed vocabulary.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or you might even add a new variable to our model that is supposed to behave as the conjunction of existing variables.",
                    "label": 0
                },
                {
                    "sent": "Or some other logical function?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our goal is going to be to compute the assignment of variables that which maximizes the overall probability, and this is represented by this combinatorial problem over here.",
                    "label": 0
                },
                {
                    "sent": "Now this can be turned into a continuous.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem by adding marginal variable.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here M of G is the marginal polytope of the graph.",
                    "label": 1
                },
                {
                    "sent": "In other words, the sets of marginals that are realizable for some choice of potentials.",
                    "label": 0
                },
                {
                    "sent": "And we are stacking all potentials into vectors ttime and 5A.",
                    "label": 0
                },
                {
                    "sent": "And replacing our variables by these marginal variables.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So although this is now continuous problem, it's still very hard to solve it.",
                    "label": 0
                },
                {
                    "sent": "It's still NP hard because the marginal polytope doesn't have in general a concise representation, and So what often is that?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One is to use a local approximation of this polytope.",
                    "label": 1
                },
                {
                    "sent": "So that local approximation essentially instead of requiring globally consistent marginals, only checks for local consistency where local here means at each factor.",
                    "label": 0
                },
                {
                    "sent": "So in other words, it constraints at each factory constraints marginals of variables that are attached to that factor and marginals of the overall assignment of that factor to be locally consistent.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as a remark, this is another approximation of the marginal polytope.",
                    "label": 0
                },
                {
                    "sent": "And the integer points in it correspond exactly to the vertices of the marginal polytope.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the LP relaxation consists in replacing M of Gmail of.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So he gets this upper bound over here.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's see how decomposition is applied here.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this was done by commodities and colleagues three years ago.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We the first thing is.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Split our graph.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two pieces, so in this case we just split these variables and create some replicas on variables that are shared by several factors.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this was the original problem that you wanted to solve.",
                    "label": 0
                },
                {
                    "sent": "the LP relaxation that we have just seen.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And by splitting we create some copies, some local copies of marginal variables at each factor, and we introduce these coupling constraints that make sure that everything here is consistent.",
                    "label": 0
                },
                {
                    "sent": "So this makes sure that these variables agree on their overlaps.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So note that we can rewrite this objective function as a sum of functions for each factor OK, and so this problem will be very easy to solve if not for this copy constraints, because everything will be separable.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea of decomposition is.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To dualize out these LaGrange, this coupling constraints by introducing LaGrange multipliers.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to denote these LaGrange multipliers by Lambda A, so this is the lab range multiplier with respect to factor a, an variable I.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the dual formulation look like looks like this.",
                    "label": 0
                },
                {
                    "sent": "So we want to minimize the LaGrange dual function, which can be written as a sum for all factors of an inner maximization problem.",
                    "label": 0
                },
                {
                    "sent": "Subject to the constraint that the LaGrange variables belong to these affine subsets.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we now have one slice of problem for each factor I'm referring to this maximization problem over here, and this can be done in parallel becausw.",
                    "label": 0
                },
                {
                    "sent": "These things are completely independent over across each factor.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the master problem consists in minimizing this function, and this can be done by using the projected subgradient.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can note that subgradient respect to Lambda A is going to consist on the optimum marginal variable.",
                    "label": 0
                },
                {
                    "sent": "The solution of this problem and this problem is nothing but the map inference problem.",
                    "label": 0
                },
                {
                    "sent": "But now local to factor A.",
                    "label": 0
                },
                {
                    "sent": "So we break this problem into several map.",
                    "label": 0
                },
                {
                    "sent": "So problems each for factor 8.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the projection step on to these set Lambda is going to consistently in a simple centering operation, so overall.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this gradient algorithm we do, the composition looks like this.",
                    "label": 0
                },
                {
                    "sent": "We for each iteration you go through each factor.",
                    "label": 0
                },
                {
                    "sent": "We can do this in parallel.",
                    "label": 0
                },
                {
                    "sent": "We compute scenario potentials by taking into consideration the current assignment of LaGrange variables and we just do map inference at that factor.",
                    "label": 0
                },
                {
                    "sent": "So this is supposed to be an easy operation.",
                    "label": 0
                },
                {
                    "sent": "Then we reassemble all the map assignments that you got, and you compute the average for each variable.",
                    "label": 0
                },
                {
                    "sent": "Finally, we do an update of the LaGrange multipliers by subtracting out the average over the solution that we have obtained for that factor.",
                    "label": 0
                },
                {
                    "sent": "So this converges for some choice of step size sequence.",
                    "label": 0
                },
                {
                    "sent": "However, it is too slow if you have many slaves, so our contribution here is superposing augmented.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Method for these to speed up things.",
                    "label": 0
                },
                {
                    "sent": "And let's first.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Write the augmented Lagrangian function.",
                    "label": 1
                },
                {
                    "sent": "It looks like this, so this is just a Lagrangian function and you are subtracting a residual term which is a quadratic penalization of the couple constraints and you have.",
                    "label": 0
                },
                {
                    "sent": "So in other words, we had our coupling constraints that make sure that everything agrees in their overlaps and you are penalizing violations of these constraints by adding these residual term over here.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the thing that you want to do is to maximize this augmented Lagrangian function respect from you and new and to minimize it with respect to Lambda.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But now you have a problem, which is the fact that we don't have a separable problem anymore.",
                    "label": 0
                },
                {
                    "sent": "So this thing is breaking this probability.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we're going to use a very old method optimization called alternating direction method of multipliers or a DMM, where essentially we turn this into two alternating optimization problems.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first you optimize with respect to mu.",
                    "label": 0
                },
                {
                    "sent": "And this can actually be done in close form.",
                    "label": 0
                },
                {
                    "sent": "This is just an average open averaging operation.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we maximize this with respect and you and that part can be carried out in parallel at each factor.",
                    "label": 0
                },
                {
                    "sent": "We're going to see that in a minute.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, we do apply the method of multipliers to update the LaGrange multipliers.",
                    "label": 0
                },
                {
                    "sent": "So we make a step in the LaGrange variables.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Overall, the algorithm is quite similar to what we had before for dual decomposition with subgradients.",
                    "label": 0
                },
                {
                    "sent": "The only difference is that instead of a map inference problem, we now have a quadratic problem at each slave.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are other differences, but this is the most important one.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This problem is actually looks like a projection until the marginal polytope, and it's actually taking that form for some.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ticular cases, and there's some conditions, so this is always going to converge, and there are some conditions.",
                    "label": 0
                },
                {
                    "sent": "It's also converges even when you just approximately solve this problem.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's see how to solve the quadratic problem for some factors of interest.",
                    "label": 0
                },
                {
                    "sent": "For binary pairwise factors, there is a closed form solution.",
                    "label": 0
                },
                {
                    "sent": "It's not difficult to derive it.",
                    "label": 0
                },
                {
                    "sent": "It's actually in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now you are also interested in the case where you have some hard constraint factors, factors that impose our constraints over the over the variables.",
                    "label": 0
                },
                {
                    "sent": "And these are important for many applications, so these are factors that have deterministic potentials that are essentially indicator functions of acceptance of some acceptance set essay.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It turns out that in that case the quadratic problem that you need to solve is nothing but a quadratic projection and using projection onto the convex soul of that acceptance set.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for many hard factors that impose logical constraints, this can be done quite efficiently by requiring only a sort operation.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going into details here about this, but this is similar to projecting onto an L1 ball, where you can also solve that problem by doing a sort, so we can actually generalize that for many.",
                    "label": 0
                },
                {
                    "sent": "Marginal polytopes of interest in this.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, if you have larger slaves and this often arises in prices, sometimes we want to consider a larger piece of the graph.",
                    "label": 0
                },
                {
                    "sent": "We can still solve approximately this quadratic problem by doing something like cycle projection algorithms or everything, or any other approach.",
                    "label": 0
                },
                {
                    "sent": "This is still work in progress.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's now see some expert.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once.",
                    "label": 0
                },
                {
                    "sent": "We tested this in binary pairwise MRF's using models, agreed.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we compared the HMM against the subgradient method.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And against Maxim Diffusion, which star updates?",
                    "label": 0
                },
                {
                    "sent": "So maximum diffusion is essentially performing block coordinate descent in the dual.",
                    "label": 0
                },
                {
                    "sent": "Buy a message passing scheme.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so these are the results.",
                    "label": 0
                },
                {
                    "sent": "The continuous line is denoting the progress in the dual and the dashed lines are the primal objectives.",
                    "label": 0
                },
                {
                    "sent": "So we can see that this upgrade and is very slow.",
                    "label": 0
                },
                {
                    "sent": "It has many slides for this example.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 slide per head.",
                    "label": 0
                },
                {
                    "sent": "And maximum diffusion is still slower, so that is the blue line while.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our methods is faster than both, specially if you look at the progress in the primal, it's progress is quite fast, so it converges faster to good solution in the primal.",
                    "label": 0
                },
                {
                    "sent": "And our interpretation for that is that we have this quadratic term that in some sense pushes for primal feasibility.",
                    "label": 0
                },
                {
                    "sent": "So you get you get a primal feasible solution quite fast.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the second problem that we're going to consider is parsing of dependency parsing and this is actually a problem that requires hard constraint factors.",
                    "label": 0
                },
                {
                    "sent": "So, just to summarize, we have a sentence like this and the goal is to recover the syntactic structure of the sentence.",
                    "label": 0
                },
                {
                    "sent": "In other words, we want to get these dependency links over here.",
                    "label": 0
                },
                {
                    "sent": "And the overall constraint is that all the sets of links has to define a tree, so this has to be a legal tree.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm not going into details on this, can be done on models to achieve this, but I'm just comparing results with two existing models for this problem.",
                    "label": 0
                },
                {
                    "sent": "When is a is a model that has a factor in forcing the overall thing to be 3 and the other model is based on multicommodity flows.",
                    "label": 0
                },
                {
                    "sent": "So it has several factors that in second together enforce the overall structure to be tree and.",
                    "label": 0
                },
                {
                    "sent": "We use the 2nd order model that essentially tries to correlate pairs of arcs and these are the results that we obtain, so we can see that the DMM is faster than this.",
                    "label": 0
                },
                {
                    "sent": "A gradient for both flow and three models.",
                    "label": 0
                },
                {
                    "sent": "And still faster than then star MSD.",
                    "label": 0
                },
                {
                    "sent": "So even the left side desire experiments with synthetic data and this ones are in the Pantry Bank, which is a corpus of real sentences.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is what this is.",
                    "label": 0
                },
                {
                    "sent": "What you have for this.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to conclude, we have a new algorithm for LP map inference.",
                    "label": 0
                },
                {
                    "sent": "It's this algorithm is decomposable.",
                    "label": 0
                },
                {
                    "sent": "All slides can be solved in parallel.",
                    "label": 0
                },
                {
                    "sent": "We essentially are allowing the simplicity of decomposition with the power of augmented Lagrangian methods.",
                    "label": 0
                },
                {
                    "sent": "We think that this is suitable particularly for problems that have many slaves, or that you cannot easily break into into.",
                    "label": 0
                },
                {
                    "sent": "Into pieces of large slaves.",
                    "label": 0
                },
                {
                    "sent": "I didn't talk about that, but you can actually provide some optimality certificates for the LP relaxation by taking into by using.",
                    "label": 0
                },
                {
                    "sent": "Examining the quadratic penalty, you can actually check if you are close to a primal feasible solution.",
                    "label": 0
                },
                {
                    "sent": "You can save a lot of computation by caching and warm starting, and I'd like to mention some related work in celebrating do decomposition by which Daphne, Kohler and colleagues.",
                    "label": 0
                },
                {
                    "sent": "This was a recent paper in ICML.",
                    "label": 0
                },
                {
                    "sent": "It's a different approach from hours it does.",
                    "label": 0
                },
                {
                    "sent": "Entropic projections, but it's a different approach, and there's also some work by product graphic Kumar and others on doing quadratic projections to solve the problem in the primal.",
                    "label": 0
                },
                {
                    "sent": "So both these approaches are different from this one, but they share some.",
                    "label": 0
                },
                {
                    "sent": "Some points are shared between all these approaches.",
                    "label": 0
                },
                {
                    "sent": "In the future, we plan to extend this for larger slaves and to investigate whether we can do approximate HMM steps for those problems.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's solid yeah, thanks.",
                    "label": 0
                },
                {
                    "sent": "How easy.",
                    "label": 0
                },
                {
                    "sent": "Binary connections for general command.",
                    "label": 0
                },
                {
                    "sent": "For generating you motivated with factor graphs.",
                    "label": 0
                },
                {
                    "sent": "Binary potential sensory limited things and.",
                    "label": 0
                },
                {
                    "sent": "Or what about ternary and other?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you can come up with as soon as you can solve that quadratic problem, either exactly or approximated.",
                    "label": 0
                },
                {
                    "sent": "You can still use these methods so you don't need to have binary potentials.",
                    "label": 0
                },
                {
                    "sent": "Use this for.",
                    "label": 0
                },
                {
                    "sent": "Multi valued variables and general factors.",
                    "label": 0
                },
                {
                    "sent": "General limitation is ability of solving the quadratic program.",
                    "label": 0
                },
                {
                    "sent": "But I can come up with ideas about how to approximate those problems.",
                    "label": 0
                },
                {
                    "sent": "So in the limit you can combine some ideas of approximate methods to solve the QP and just plug them in into this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Small question, then we take the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}