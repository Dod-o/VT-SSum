{
    "id": "r2bcfqpaetfrdgzlka4qrfpjrm4z5sjr",
    "title": "Efficiently approximating Markov tree bagging for high-dimensional density estimation",
    "info": {
        "author": [
            "Fran\u00e7ois Schnitzler, Department of Electrical Engineering and Computer Science, University of Li\u00e8ge"
        ],
        "published": "Oct. 3, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_schnitzler_efficiently/",
    "segmentation": [
        [
            "I'm a PhD student from the University of here, working under the supervision of Professor We Wanker and what I'm going to present today is joint work with the team of Professor Philip Laurel from the University of North in France."
        ],
        [
            "So we are working on developing new methods for improving learning of probabilistic graphical models from data in high dimensional settings for probabilistic inference.",
            "So to give you a few examples, this could be useful.",
            "For example in bioinformatics for modeling genes or proteins and predicting the most likely outcome of a complex combination of diseases and treatments, or to select the most appropriate medicine too.",
            "Healer complex illness like cancer, another application field we are very interested in is power networks.",
            "So take us an illustration of the power transmission network in Western Europe which is composed of well depending on the voltage 3000 to 10,000 transmission nodes.",
            "You could use graphical models to predict the power flows in and out of a country based on the local consumption or to predict the most likely production of.",
            "Wind or solar power plants based on local weather.",
            "2 main challenges for applying probabilistic graphical models to those problems, algorithmic complexity and the usually extremely low number of some parts available.",
            "Compared to the number of variables, one potential solution is to use really simple models like Markov trees.",
            "Markov trees are Bayesian networks where each variable has at most one parents and they allow arguments to be quite efficient.",
            "However, due to the.",
            "Constraints on the model space.",
            "They cannot really represent complex problems.",
            "One way to expand their modeling capacities while still retaining the interesting scaling behaviors is the use of mixtures of trees, and in this work what we're trying to do is to accelerate the argument for learning such models.",
            "So in this talk I will first explain what are mixtures of trees and present the argument that we're trying to accelerate.",
            "Then I will explain why and how we accelerate this algorithm.",
            "And finally I will present some results of."
        ],
        [
            "Or simulation.",
            "So a mixture of trees on the right is composed over sets of Markov trees and sets of weights denoted by mute and the probability density it encodes is the weighted average of the probability densities encoded by each tree.",
            "So I would like to stress that a mixture of trees is an ensemble method that all tree is defined on or viable.",
            "So a mixture of tree is not a forest, which in this context.",
            "This is a Markov tree with some edges are missing, so you have disconnected components in the graph."
        ],
        [
            "So mixtures of trees are originally being developed as a way to reduce the shortcoming of both simple and complex graphical models.",
            "The the models are simple.",
            "There are trees, so the algorithms are simple as well, and actually they are of quadratic complexity for learning and of linear complexity in the number of viable for probabilistic reasoning.",
            "But the combination of several trees allow in principle to represent a richer class of densities.",
            "So there are two ways.",
            "Two to learn mixtures of three in the maximum likelihood framework.",
            "Each tree is viewed as a mode of the distribution and the goal of the different arguments is to fit each tree to one mode of the distribution.",
            "So one tree can be seen as an alternative explanation for the problem, but in this work we will work in the variance reduction framework, where the mixtures are seen as a tool for violence reduction as an approximation to true Bayesian learning.",
            "So as opposed to the.",
            "More classical approach of selecting only the best model, what we want to do here is to consider all possibilities and to average the prediction based on the probabilities based.",
            "Sorry we want to average the prediction of all models based on the probabilities of those models according to the data sets."
        ],
        [
            "And more precisely, we use here bootstrapping algorithm, so this is a meta argument that is.",
            "Used to compensate for lack of data by applying a given algorithm on different bootstrap replicates of datasets and averaging the prediction of those different models.",
            "A bootstrap replicate is obtained from an original sample by drawing.",
            "Some parts with replacement and copying them into the bootstrap replicates, and as you can see on this graph that displays the kullback Leibler divergent.",
            "Do the target model of 1 maximum likelihood tree and a mixture of bagged maximum likelihood tree of increasing size.",
            "This approach can be quite useful.",
            "One working with mark off trees.",
            "And you can also see that the more trees we have in the mixture, the better the model is because the score is decreasing."
        ],
        [
            "So ideally we would like to have as many trees as possible, but the complexity of this method is the number of trees M times the complexity of learning one maximum likelihood tree on the bootstrap replicate.",
            "So what we want to do in this work is to approximate this bag mixture of maximum likelihood tree in order to be able to learn as many tree as possible under a given computational framework.",
            "So the bottleneck of the algorithms for learning a maximum likelihood tree, which is also called the show you algorithm, is the number of edges considered.",
            "Because this argument in the two step process in the first step, the mutual information of.",
            "Every pair of variable is computed, and this defines an adjacency matrix.",
            "And the second step of the method maximum weight spanning tree is extracted from this graph and this defines the structure of the tree model.",
            "Since army to learning is straightforward and very efficient, I will not discuss it here, but that would make a third step of the AG."
        ],
        [
            "So in order to reduce the number of edges considered, what we do is that we first.",
            "Compute the maximum likelihood tree on the original data set.",
            "And then we want to consider only a subset of edges for building each tree.",
            "So this gives us a reduction.",
            "In complexity and actually it decouples the quadratic term of the number of trees in the mixture.",
            "So we developed two methods for learning those mixtures, and in practice we have an argument that is more or less 10 times."
        ],
        [
            "Faster.",
            "The first approach we developed, and which is called.",
            "Inertial tries to.",
            "Use always the same number of edges, which is here denoted by K and which is a parameter of the algorithm.",
            "So the set of edges for building a tree is.",
            "Composed of the edges of the previous three so you have N -- 1 edges and you complete this set by adding K minus and plus one randomly selected edges.",
            "Among all possible edges.",
            "This can be view of the other stochastic work in the space of Markov tree structures, where after an infinite number of generated trees, all possible tree structures will have been considered."
        ],
        [
            "In the other method, which we called the skeleton based approach, the.",
            "Set of edges CONSIDERED is fixed and identical for all trees that we generate.",
            "And it is based on.",
            "The computation of the first three of the model on the original data sets.",
            "So since we will be using the same set of edges for tree, we want to have strong edges in that set because the show a new algorithm will build a maximum weight spanning tree.",
            "So we want edges with.",
            "High weight.",
            "And.",
            "When the difference of between two edges is higher, the reactive ordering of those edges with respect to their weight is likely to stay the same even when those weights are computed on a bootstrap replicate.",
            "So what we do in our argument is that we.",
            "Computes the mutual information of all edges for building the 1st three and then we consider for subsequent trees only those edges who scores above threshold."
        ],
        [
            "So considering only addresses, mutual information is higher than the threshold amounts to structural regularization of the show and new algorithm so as to penalize model complexity in terms of the number of edges.",
            "This can be seen as testing the.",
            "Independence of every pair of viable and including.",
            "In the model only.",
            "Those edges, rain dependencies rejected so.",
            "What we are doing is that we are comparing the mutual information too.",
            "Dynamic threshold that we derived from Ki squared test based on the number of.",
            "Some parts based on the cardinality of the variables and also based on the postulated P value Alpha which is the parimeter of this algorithm.",
            "Now the number of candidate edges depends on both Alpha and the data set, so.",
            "The complexity of the algorithm and the speed following one tree is not as directly controllable as in the initial method, and this is.",
            "That"
        ],
        [
            "Difference with the initial method.",
            "We evaluate its arguments on both synthetic and more realistic datasets.",
            "I will first present the.",
            "The results we obtained on synthetic Bayesian networks of 200 and 1000 variables, those Bayesian networks or generated by sampling the number of parents of each variable between zero and five, and by randomly selecting those.",
            "Parents by respecting a topological ordering so as to be sure we do not introduce cycles.",
            "We quantified the quality of the models learned by the pullback library divergance, which is a measure of similarity between distributions, and therefore the lower the score, the better the model.",
            "And actually we use the Monte Carlo estimation of that callback library divergance for."
        ],
        [
            "Computational reason.",
            "On this work you can see an illustration of the tradeoff between.",
            "Complexity and accuracy permitted by our algorithm.",
            "You can see that the mixture of maximum likelihood tree in Blues and both our approximation in green and magenta are getting increasingly better than the maximum likelihood tree when the number of trees is increasing, and while on this graph at least, both approximation seems to match the mixture of bag maximum accuracy trees quite closely, however.",
            "The two approximation or approximately 10 times faster than the mixture of bag maximum likely."
        ],
        [
            "Trees.",
            "The parameter Alpha of the skeleton based method controls both the convergence speed of the mixture generated and its accuracy.",
            "You can see here on the left of the graph that smaller Alpha.",
            "On those synthetic datasets initially lead to better accuracy than.",
            "The maximum likelihood tree and then higher.",
            "Alpha or weather?",
            "When Alpha is higher, the mixture is.",
            "Quickly.",
            "Improving and quickly gets.",
            "Better than mistress.",
            "Generated with a smaller Alpha, this is actually normal becausw.",
            "Smaller unfair means stronger regularization and 1st smaller number of candidate edges, which means that we.",
            "Can generate less diverse trees cause every tree generated.",
            "Would be composed only of the addressing US.",
            "So the convergence will be faster, but the mixture will be worse approximation of the mixture of unconstrained maximum likelihood tree and therefore the accuracy will also be or."
        ],
        [
            "This graph shows that it's necessary to initialize the initial method by your maximum likelihood tree computed on all possible edges.",
            "The black curves.",
            "Correspond to mixtures builds by random start variant of the initial method where the 1st three of the mixture is.",
            "Computed only on a subset of.",
            "Capital K randomly selected edges.",
            "If both methods are seen as a stochastic work in the space of Markov tree structures, the random style Ryan would start very far from the neighborhood of good structures, or whereas the first variant I presented so the green one would starts.",
            "With a much more sensible initial guess."
        ],
        [
            "Moreover, in order to reach the neighborhood of good structures, the random starve ion will still need to spend time considering and exploring all possible edges.",
            "And actually you can see here that the convergence of the.",
            "Random Star Ryan is tide to the number of candidate edges considered at each iteration.",
            "When that number is doubled or halfed, the convergence speed varies greatly, whereas the.",
            "Standard violent is only marginally affected.",
            "So.",
            "Based on those results we.",
            "Believe that.",
            "Considering all possible edges systematically when building the first tree is more sensible."
        ],
        [
            "We also consider well going on.",
            "Model more realistic model and actually we use nine models among those generated or gathered by aliferis static off and our session chair, and they made those.",
            "Networks and data sets available in an online supplement of two of the recent GMA papers.",
            "On those datasets, we validated our model by computing the.",
            "Negative log likelihood of an independent sample sets.",
            "So we have 9.",
            "Models ranging from.",
            "200 to 801 variables and we apply darwins on learning sets of.",
            "200 and 500 samples so that gives us 18 problem settings.",
            "In two problems settings, both approximation were better than the maximum likelihood tree.",
            "In eight other experiments.",
            "Only the skeleton based approximation was better and in the remaining 8 cases we could not conclude yet and more refined analysis is currently on."
        ],
        [
            "Away.",
            "We don't need presents.",
            "The results we obtained on.",
            "Three cases and the first one is one of the two cases were both approximations are better than one maximum likelihood tree.",
            "Here in Rider, and as you can see on those more realistic datasets, the initial approximation is green is not really doing a good job at approximating the mixture of bag maximum rating tree.",
            "However, the skeleton based approach here is doing.",
            "Really well."
        ],
        [
            "Under the.",
            "Gene datasets, things gets even worse for the initial approximation because after a few terms it starts.",
            "Getting a mother that is.",
            "Not as accurate as the maximum likely tree.",
            "However, the Scranton based approximation is here still."
        ],
        [
            "Doing good job and actually the skeleton based approximation was for sufficiently large mixture always better than the maximum likelihood tree.",
            "There was, however, one case is this one.",
            "With the addition of trees.",
            "Originally.",
            "Degrades the accuracy of the model.",
            "However, you can see that very quickly.",
            "Further, addition of terms starts improving the accuracy of the model and after sometimes the.",
            "Accuracy of the mixture starts to get better than the accuracy of the maximum likelihood tree.",
            "We have not investigated this behavior in detail, however we noted here that.",
            "The mixture of black maximum likelihood tree.",
            "Is in this particular case also converging?",
            "Much slowly.",
            "And.",
            "Actually, as he are not yet overtaken the maximum rocketry, so.",
            "Based on that subversion, we conclude that the skeleton based approximation is even in that strange case, doing a good job at approximating the mixture of back maximum likelihood tree."
        ],
        [
            "So before closing this presentation, I would like to recap everything Re proposing this work, two methods for approximating mixture of back maximum likelihood trees by exploiting previously computed trees to reduce the number of edges we consider for building.",
            "Street based on the ropes or version and our simulation, the skeleton based approximation seems to be.",
            "The best of the two methods.",
            "So in this method we are working with a fixed set of candidate edges based on the comparison of the mutual information computed for building in the first 3.",
            "Two or threshold derived from a G square test.",
            "And this method does a good job at approximating the mixture of maximum likelihood tryan most of the time they actually.",
            "Both those mixtures.",
            "Can lead to a better accuracy than a regularised forest.",
            "And in the future we would like to try to.",
            "Apply another regularization when learning each tree on the bootstrap replicate.",
            "And what we think could also be interesting is to develop methods where the set of candidate edges is progressively expanded to include weaker and combine the qualities of both our methods.",
            "So if you have any questions.",
            "This time.",
            "Yes, sure it's time for a couple of short questions.",
            "Yes.",
            "Results in terms of the divergance.",
            "I was wondering in terms of structure if you can infer something about the quality of.",
            "Yes, so we have not looked at the quality of the structure because we are running multiple structures so we could of course count the frequency of occurrence of edges in our model.",
            "But we feel that since we are learning those model not for structural discovery but for inference, the negative log likelihood and kullback Leibler divergent some more accurate measure for what we're trying to achieve.",
            "However, if you can suggest a way to measure the structures, I would be really interested.",
            "I was wanting to such high dimensional and publish the structure which is more interesting than.",
            "They look like lead OK to density estimation problem.",
            "But since we have so many variables, I was wondering if such a small improvement in log likelihood is really well relevant.",
            "We spent 2 two informing a good structure so just.",
            "Yeah, so for possible uses just.",
            "I mean, if you want to perform inference.",
            "Even if you have the correct structure, you still have problems because you know influence on click also do not scale so.",
            "Yeah, that that's why we're using simple models, I mean.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm a PhD student from the University of here, working under the supervision of Professor We Wanker and what I'm going to present today is joint work with the team of Professor Philip Laurel from the University of North in France.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we are working on developing new methods for improving learning of probabilistic graphical models from data in high dimensional settings for probabilistic inference.",
                    "label": 0
                },
                {
                    "sent": "So to give you a few examples, this could be useful.",
                    "label": 0
                },
                {
                    "sent": "For example in bioinformatics for modeling genes or proteins and predicting the most likely outcome of a complex combination of diseases and treatments, or to select the most appropriate medicine too.",
                    "label": 0
                },
                {
                    "sent": "Healer complex illness like cancer, another application field we are very interested in is power networks.",
                    "label": 1
                },
                {
                    "sent": "So take us an illustration of the power transmission network in Western Europe which is composed of well depending on the voltage 3000 to 10,000 transmission nodes.",
                    "label": 0
                },
                {
                    "sent": "You could use graphical models to predict the power flows in and out of a country based on the local consumption or to predict the most likely production of.",
                    "label": 0
                },
                {
                    "sent": "Wind or solar power plants based on local weather.",
                    "label": 1
                },
                {
                    "sent": "2 main challenges for applying probabilistic graphical models to those problems, algorithmic complexity and the usually extremely low number of some parts available.",
                    "label": 0
                },
                {
                    "sent": "Compared to the number of variables, one potential solution is to use really simple models like Markov trees.",
                    "label": 1
                },
                {
                    "sent": "Markov trees are Bayesian networks where each variable has at most one parents and they allow arguments to be quite efficient.",
                    "label": 0
                },
                {
                    "sent": "However, due to the.",
                    "label": 0
                },
                {
                    "sent": "Constraints on the model space.",
                    "label": 0
                },
                {
                    "sent": "They cannot really represent complex problems.",
                    "label": 0
                },
                {
                    "sent": "One way to expand their modeling capacities while still retaining the interesting scaling behaviors is the use of mixtures of trees, and in this work what we're trying to do is to accelerate the argument for learning such models.",
                    "label": 0
                },
                {
                    "sent": "So in this talk I will first explain what are mixtures of trees and present the argument that we're trying to accelerate.",
                    "label": 0
                },
                {
                    "sent": "Then I will explain why and how we accelerate this algorithm.",
                    "label": 0
                },
                {
                    "sent": "And finally I will present some results of.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or simulation.",
                    "label": 0
                },
                {
                    "sent": "So a mixture of trees on the right is composed over sets of Markov trees and sets of weights denoted by mute and the probability density it encodes is the weighted average of the probability densities encoded by each tree.",
                    "label": 0
                },
                {
                    "sent": "So I would like to stress that a mixture of trees is an ensemble method that all tree is defined on or viable.",
                    "label": 1
                },
                {
                    "sent": "So a mixture of tree is not a forest, which in this context.",
                    "label": 1
                },
                {
                    "sent": "This is a Markov tree with some edges are missing, so you have disconnected components in the graph.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So mixtures of trees are originally being developed as a way to reduce the shortcoming of both simple and complex graphical models.",
                    "label": 0
                },
                {
                    "sent": "The the models are simple.",
                    "label": 0
                },
                {
                    "sent": "There are trees, so the algorithms are simple as well, and actually they are of quadratic complexity for learning and of linear complexity in the number of viable for probabilistic reasoning.",
                    "label": 0
                },
                {
                    "sent": "But the combination of several trees allow in principle to represent a richer class of densities.",
                    "label": 0
                },
                {
                    "sent": "So there are two ways.",
                    "label": 1
                },
                {
                    "sent": "Two to learn mixtures of three in the maximum likelihood framework.",
                    "label": 0
                },
                {
                    "sent": "Each tree is viewed as a mode of the distribution and the goal of the different arguments is to fit each tree to one mode of the distribution.",
                    "label": 0
                },
                {
                    "sent": "So one tree can be seen as an alternative explanation for the problem, but in this work we will work in the variance reduction framework, where the mixtures are seen as a tool for violence reduction as an approximation to true Bayesian learning.",
                    "label": 0
                },
                {
                    "sent": "So as opposed to the.",
                    "label": 0
                },
                {
                    "sent": "More classical approach of selecting only the best model, what we want to do here is to consider all possibilities and to average the prediction based on the probabilities based.",
                    "label": 0
                },
                {
                    "sent": "Sorry we want to average the prediction of all models based on the probabilities of those models according to the data sets.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And more precisely, we use here bootstrapping algorithm, so this is a meta argument that is.",
                    "label": 1
                },
                {
                    "sent": "Used to compensate for lack of data by applying a given algorithm on different bootstrap replicates of datasets and averaging the prediction of those different models.",
                    "label": 0
                },
                {
                    "sent": "A bootstrap replicate is obtained from an original sample by drawing.",
                    "label": 1
                },
                {
                    "sent": "Some parts with replacement and copying them into the bootstrap replicates, and as you can see on this graph that displays the kullback Leibler divergent.",
                    "label": 1
                },
                {
                    "sent": "Do the target model of 1 maximum likelihood tree and a mixture of bagged maximum likelihood tree of increasing size.",
                    "label": 0
                },
                {
                    "sent": "This approach can be quite useful.",
                    "label": 0
                },
                {
                    "sent": "One working with mark off trees.",
                    "label": 0
                },
                {
                    "sent": "And you can also see that the more trees we have in the mixture, the better the model is because the score is decreasing.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So ideally we would like to have as many trees as possible, but the complexity of this method is the number of trees M times the complexity of learning one maximum likelihood tree on the bootstrap replicate.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do in this work is to approximate this bag mixture of maximum likelihood tree in order to be able to learn as many tree as possible under a given computational framework.",
                    "label": 0
                },
                {
                    "sent": "So the bottleneck of the algorithms for learning a maximum likelihood tree, which is also called the show you algorithm, is the number of edges considered.",
                    "label": 0
                },
                {
                    "sent": "Because this argument in the two step process in the first step, the mutual information of.",
                    "label": 0
                },
                {
                    "sent": "Every pair of variable is computed, and this defines an adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "And the second step of the method maximum weight spanning tree is extracted from this graph and this defines the structure of the tree model.",
                    "label": 0
                },
                {
                    "sent": "Since army to learning is straightforward and very efficient, I will not discuss it here, but that would make a third step of the AG.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in order to reduce the number of edges considered, what we do is that we first.",
                    "label": 0
                },
                {
                    "sent": "Compute the maximum likelihood tree on the original data set.",
                    "label": 1
                },
                {
                    "sent": "And then we want to consider only a subset of edges for building each tree.",
                    "label": 0
                },
                {
                    "sent": "So this gives us a reduction.",
                    "label": 0
                },
                {
                    "sent": "In complexity and actually it decouples the quadratic term of the number of trees in the mixture.",
                    "label": 1
                },
                {
                    "sent": "So we developed two methods for learning those mixtures, and in practice we have an argument that is more or less 10 times.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Faster.",
                    "label": 0
                },
                {
                    "sent": "The first approach we developed, and which is called.",
                    "label": 0
                },
                {
                    "sent": "Inertial tries to.",
                    "label": 0
                },
                {
                    "sent": "Use always the same number of edges, which is here denoted by K and which is a parameter of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the set of edges for building a tree is.",
                    "label": 0
                },
                {
                    "sent": "Composed of the edges of the previous three so you have N -- 1 edges and you complete this set by adding K minus and plus one randomly selected edges.",
                    "label": 1
                },
                {
                    "sent": "Among all possible edges.",
                    "label": 0
                },
                {
                    "sent": "This can be view of the other stochastic work in the space of Markov tree structures, where after an infinite number of generated trees, all possible tree structures will have been considered.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the other method, which we called the skeleton based approach, the.",
                    "label": 1
                },
                {
                    "sent": "Set of edges CONSIDERED is fixed and identical for all trees that we generate.",
                    "label": 0
                },
                {
                    "sent": "And it is based on.",
                    "label": 1
                },
                {
                    "sent": "The computation of the first three of the model on the original data sets.",
                    "label": 1
                },
                {
                    "sent": "So since we will be using the same set of edges for tree, we want to have strong edges in that set because the show a new algorithm will build a maximum weight spanning tree.",
                    "label": 0
                },
                {
                    "sent": "So we want edges with.",
                    "label": 0
                },
                {
                    "sent": "High weight.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "When the difference of between two edges is higher, the reactive ordering of those edges with respect to their weight is likely to stay the same even when those weights are computed on a bootstrap replicate.",
                    "label": 1
                },
                {
                    "sent": "So what we do in our argument is that we.",
                    "label": 0
                },
                {
                    "sent": "Computes the mutual information of all edges for building the 1st three and then we consider for subsequent trees only those edges who scores above threshold.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So considering only addresses, mutual information is higher than the threshold amounts to structural regularization of the show and new algorithm so as to penalize model complexity in terms of the number of edges.",
                    "label": 1
                },
                {
                    "sent": "This can be seen as testing the.",
                    "label": 0
                },
                {
                    "sent": "Independence of every pair of viable and including.",
                    "label": 0
                },
                {
                    "sent": "In the model only.",
                    "label": 0
                },
                {
                    "sent": "Those edges, rain dependencies rejected so.",
                    "label": 1
                },
                {
                    "sent": "What we are doing is that we are comparing the mutual information too.",
                    "label": 1
                },
                {
                    "sent": "Dynamic threshold that we derived from Ki squared test based on the number of.",
                    "label": 0
                },
                {
                    "sent": "Some parts based on the cardinality of the variables and also based on the postulated P value Alpha which is the parimeter of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now the number of candidate edges depends on both Alpha and the data set, so.",
                    "label": 0
                },
                {
                    "sent": "The complexity of the algorithm and the speed following one tree is not as directly controllable as in the initial method, and this is.",
                    "label": 0
                },
                {
                    "sent": "That",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Difference with the initial method.",
                    "label": 0
                },
                {
                    "sent": "We evaluate its arguments on both synthetic and more realistic datasets.",
                    "label": 0
                },
                {
                    "sent": "I will first present the.",
                    "label": 0
                },
                {
                    "sent": "The results we obtained on synthetic Bayesian networks of 200 and 1000 variables, those Bayesian networks or generated by sampling the number of parents of each variable between zero and five, and by randomly selecting those.",
                    "label": 1
                },
                {
                    "sent": "Parents by respecting a topological ordering so as to be sure we do not introduce cycles.",
                    "label": 0
                },
                {
                    "sent": "We quantified the quality of the models learned by the pullback library divergance, which is a measure of similarity between distributions, and therefore the lower the score, the better the model.",
                    "label": 0
                },
                {
                    "sent": "And actually we use the Monte Carlo estimation of that callback library divergance for.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Computational reason.",
                    "label": 0
                },
                {
                    "sent": "On this work you can see an illustration of the tradeoff between.",
                    "label": 0
                },
                {
                    "sent": "Complexity and accuracy permitted by our algorithm.",
                    "label": 0
                },
                {
                    "sent": "You can see that the mixture of maximum likelihood tree in Blues and both our approximation in green and magenta are getting increasingly better than the maximum likelihood tree when the number of trees is increasing, and while on this graph at least, both approximation seems to match the mixture of bag maximum accuracy trees quite closely, however.",
                    "label": 0
                },
                {
                    "sent": "The two approximation or approximately 10 times faster than the mixture of bag maximum likely.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Trees.",
                    "label": 0
                },
                {
                    "sent": "The parameter Alpha of the skeleton based method controls both the convergence speed of the mixture generated and its accuracy.",
                    "label": 1
                },
                {
                    "sent": "You can see here on the left of the graph that smaller Alpha.",
                    "label": 0
                },
                {
                    "sent": "On those synthetic datasets initially lead to better accuracy than.",
                    "label": 0
                },
                {
                    "sent": "The maximum likelihood tree and then higher.",
                    "label": 0
                },
                {
                    "sent": "Alpha or weather?",
                    "label": 0
                },
                {
                    "sent": "When Alpha is higher, the mixture is.",
                    "label": 0
                },
                {
                    "sent": "Quickly.",
                    "label": 0
                },
                {
                    "sent": "Improving and quickly gets.",
                    "label": 0
                },
                {
                    "sent": "Better than mistress.",
                    "label": 0
                },
                {
                    "sent": "Generated with a smaller Alpha, this is actually normal becausw.",
                    "label": 0
                },
                {
                    "sent": "Smaller unfair means stronger regularization and 1st smaller number of candidate edges, which means that we.",
                    "label": 0
                },
                {
                    "sent": "Can generate less diverse trees cause every tree generated.",
                    "label": 0
                },
                {
                    "sent": "Would be composed only of the addressing US.",
                    "label": 0
                },
                {
                    "sent": "So the convergence will be faster, but the mixture will be worse approximation of the mixture of unconstrained maximum likelihood tree and therefore the accuracy will also be or.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This graph shows that it's necessary to initialize the initial method by your maximum likelihood tree computed on all possible edges.",
                    "label": 0
                },
                {
                    "sent": "The black curves.",
                    "label": 0
                },
                {
                    "sent": "Correspond to mixtures builds by random start variant of the initial method where the 1st three of the mixture is.",
                    "label": 0
                },
                {
                    "sent": "Computed only on a subset of.",
                    "label": 0
                },
                {
                    "sent": "Capital K randomly selected edges.",
                    "label": 0
                },
                {
                    "sent": "If both methods are seen as a stochastic work in the space of Markov tree structures, the random style Ryan would start very far from the neighborhood of good structures, or whereas the first variant I presented so the green one would starts.",
                    "label": 0
                },
                {
                    "sent": "With a much more sensible initial guess.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Moreover, in order to reach the neighborhood of good structures, the random starve ion will still need to spend time considering and exploring all possible edges.",
                    "label": 0
                },
                {
                    "sent": "And actually you can see here that the convergence of the.",
                    "label": 0
                },
                {
                    "sent": "Random Star Ryan is tide to the number of candidate edges considered at each iteration.",
                    "label": 0
                },
                {
                    "sent": "When that number is doubled or halfed, the convergence speed varies greatly, whereas the.",
                    "label": 0
                },
                {
                    "sent": "Standard violent is only marginally affected.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Based on those results we.",
                    "label": 0
                },
                {
                    "sent": "Believe that.",
                    "label": 0
                },
                {
                    "sent": "Considering all possible edges systematically when building the first tree is more sensible.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also consider well going on.",
                    "label": 0
                },
                {
                    "sent": "Model more realistic model and actually we use nine models among those generated or gathered by aliferis static off and our session chair, and they made those.",
                    "label": 0
                },
                {
                    "sent": "Networks and data sets available in an online supplement of two of the recent GMA papers.",
                    "label": 0
                },
                {
                    "sent": "On those datasets, we validated our model by computing the.",
                    "label": 0
                },
                {
                    "sent": "Negative log likelihood of an independent sample sets.",
                    "label": 1
                },
                {
                    "sent": "So we have 9.",
                    "label": 0
                },
                {
                    "sent": "Models ranging from.",
                    "label": 0
                },
                {
                    "sent": "200 to 801 variables and we apply darwins on learning sets of.",
                    "label": 1
                },
                {
                    "sent": "200 and 500 samples so that gives us 18 problem settings.",
                    "label": 1
                },
                {
                    "sent": "In two problems settings, both approximation were better than the maximum likelihood tree.",
                    "label": 0
                },
                {
                    "sent": "In eight other experiments.",
                    "label": 0
                },
                {
                    "sent": "Only the skeleton based approximation was better and in the remaining 8 cases we could not conclude yet and more refined analysis is currently on.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Away.",
                    "label": 0
                },
                {
                    "sent": "We don't need presents.",
                    "label": 0
                },
                {
                    "sent": "The results we obtained on.",
                    "label": 0
                },
                {
                    "sent": "Three cases and the first one is one of the two cases were both approximations are better than one maximum likelihood tree.",
                    "label": 1
                },
                {
                    "sent": "Here in Rider, and as you can see on those more realistic datasets, the initial approximation is green is not really doing a good job at approximating the mixture of bag maximum rating tree.",
                    "label": 0
                },
                {
                    "sent": "However, the skeleton based approach here is doing.",
                    "label": 0
                },
                {
                    "sent": "Really well.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Under the.",
                    "label": 0
                },
                {
                    "sent": "Gene datasets, things gets even worse for the initial approximation because after a few terms it starts.",
                    "label": 0
                },
                {
                    "sent": "Getting a mother that is.",
                    "label": 0
                },
                {
                    "sent": "Not as accurate as the maximum likely tree.",
                    "label": 0
                },
                {
                    "sent": "However, the Scranton based approximation is here still.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doing good job and actually the skeleton based approximation was for sufficiently large mixture always better than the maximum likelihood tree.",
                    "label": 0
                },
                {
                    "sent": "There was, however, one case is this one.",
                    "label": 0
                },
                {
                    "sent": "With the addition of trees.",
                    "label": 1
                },
                {
                    "sent": "Originally.",
                    "label": 1
                },
                {
                    "sent": "Degrades the accuracy of the model.",
                    "label": 0
                },
                {
                    "sent": "However, you can see that very quickly.",
                    "label": 0
                },
                {
                    "sent": "Further, addition of terms starts improving the accuracy of the model and after sometimes the.",
                    "label": 0
                },
                {
                    "sent": "Accuracy of the mixture starts to get better than the accuracy of the maximum likelihood tree.",
                    "label": 0
                },
                {
                    "sent": "We have not investigated this behavior in detail, however we noted here that.",
                    "label": 0
                },
                {
                    "sent": "The mixture of black maximum likelihood tree.",
                    "label": 0
                },
                {
                    "sent": "Is in this particular case also converging?",
                    "label": 0
                },
                {
                    "sent": "Much slowly.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Actually, as he are not yet overtaken the maximum rocketry, so.",
                    "label": 0
                },
                {
                    "sent": "Based on that subversion, we conclude that the skeleton based approximation is even in that strange case, doing a good job at approximating the mixture of back maximum likelihood tree.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before closing this presentation, I would like to recap everything Re proposing this work, two methods for approximating mixture of back maximum likelihood trees by exploiting previously computed trees to reduce the number of edges we consider for building.",
                    "label": 0
                },
                {
                    "sent": "Street based on the ropes or version and our simulation, the skeleton based approximation seems to be.",
                    "label": 0
                },
                {
                    "sent": "The best of the two methods.",
                    "label": 0
                },
                {
                    "sent": "So in this method we are working with a fixed set of candidate edges based on the comparison of the mutual information computed for building in the first 3.",
                    "label": 1
                },
                {
                    "sent": "Two or threshold derived from a G square test.",
                    "label": 0
                },
                {
                    "sent": "And this method does a good job at approximating the mixture of maximum likelihood tryan most of the time they actually.",
                    "label": 0
                },
                {
                    "sent": "Both those mixtures.",
                    "label": 0
                },
                {
                    "sent": "Can lead to a better accuracy than a regularised forest.",
                    "label": 0
                },
                {
                    "sent": "And in the future we would like to try to.",
                    "label": 0
                },
                {
                    "sent": "Apply another regularization when learning each tree on the bootstrap replicate.",
                    "label": 0
                },
                {
                    "sent": "And what we think could also be interesting is to develop methods where the set of candidate edges is progressively expanded to include weaker and combine the qualities of both our methods.",
                    "label": 0
                },
                {
                    "sent": "So if you have any questions.",
                    "label": 0
                },
                {
                    "sent": "This time.",
                    "label": 0
                },
                {
                    "sent": "Yes, sure it's time for a couple of short questions.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Results in terms of the divergance.",
                    "label": 0
                },
                {
                    "sent": "I was wondering in terms of structure if you can infer something about the quality of.",
                    "label": 0
                },
                {
                    "sent": "Yes, so we have not looked at the quality of the structure because we are running multiple structures so we could of course count the frequency of occurrence of edges in our model.",
                    "label": 1
                },
                {
                    "sent": "But we feel that since we are learning those model not for structural discovery but for inference, the negative log likelihood and kullback Leibler divergent some more accurate measure for what we're trying to achieve.",
                    "label": 0
                },
                {
                    "sent": "However, if you can suggest a way to measure the structures, I would be really interested.",
                    "label": 0
                },
                {
                    "sent": "I was wanting to such high dimensional and publish the structure which is more interesting than.",
                    "label": 0
                },
                {
                    "sent": "They look like lead OK to density estimation problem.",
                    "label": 0
                },
                {
                    "sent": "But since we have so many variables, I was wondering if such a small improvement in log likelihood is really well relevant.",
                    "label": 0
                },
                {
                    "sent": "We spent 2 two informing a good structure so just.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so for possible uses just.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you want to perform inference.",
                    "label": 0
                },
                {
                    "sent": "Even if you have the correct structure, you still have problems because you know influence on click also do not scale so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that that's why we're using simple models, I mean.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}