{
    "id": "dnhbb73jizhpif22ztybrhay537or4ci",
    "title": "Representation, Approximation and Learning of Submodular Functions Using Low-rank Decision Trees",
    "info": {
        "author": [
            "Pravesh Kothari, Department of Computer Science, University of Texas at Austin"
        ],
        "published": "May 15, 2014",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_kothari_trees/",
    "segmentation": [
        [
            "Alright, so I'm going to talk about representation, approximation and learning of submodular functions using low rank decision trees.",
            "This is a joint work as outside with metallic Feldman and Jan Vondrak of IBM Research.",
            "So let me start by defining words."
        ],
        [
            "Modular functions are so modular, functions are from abroad.",
            "Class of family functions called as set functions which are defined on subsets of integers from one to 10 and take real values.",
            "So a set function F is submodular if it satisfies this property of diminishing marginal returns.",
            "Which means that if you have two sets as anti says that S is included in T, then the marginal gain in adding a new element to S in the value of the function is a higher or equal to the marginal gain on adding the same new element to the value of the function to the larger set T. So, so that's the property of diminishing marginal returns that defines submodular functions."
        ],
        [
            "We will equivalently look at these functions to be defined on the Boolean cube by associating every binary string to a set to a subset of integers.",
            "Wentworth, in the natural way of looking at it as an indicator string of a set.",
            "Alright, so."
        ],
        [
            "A submodular functions have a rich history of work on it in like variety of different contexts.",
            "For example, I guess the most important applications of submodular functions happen to be incremental optimization, where they are viewed as discrete analogue of convex functions.",
            "This connection actually can be made precise because there exists a convex continuous extension of submodular functions called us lower extension, so modular functions appear in special cases as generalization of special instances of problems that you see in common.",
            "Position, for example graph cut functions rank functions of matroids set, covering functions all happen to be submodular.",
            "They also happen to have applications in other problems of interest here, like plant location and sensor placement.",
            "The application in sensor placement stems from the idea that the information given to you by a set of sensors happens to be a submodular function, and you can use your methods of submodular optimization to get an optimal sensor placement algorithm.",
            "There are other important use happens to be in algorithmic game theory.",
            "Do precisely the property I mentioned the last slide that of diminishing marginal returns.",
            "This is a property which you interpret to be had by utility functions of agents.",
            "When you're modeling a game theoretic system, your prices, so naturally you'd expect applications in algorithm game theory and economix because of this property and it has been studied extensively in this context."
        ],
        [
            "Our application or our main concern would be with learning submodular functions and in this context the problem was first introduced by Balkan and hardware.",
            "In 2011 they were motivated by learning and predicting submodular functions when they model pricing and utility functions of agents in game theoretic systems, they wanted to demands of agents and they were also motivated by some applications to advertisements.",
            "So with all these applications that define this new model of learning called SP Mac.",
            "Which wanted to, which was intended to capture a learning theoretic view when you are interested in commercial optimization and this model is short for probably mostly approximately correct.",
            "So let me let me just give you a technical note here that in all the works on learning, and in fact for the rest of the talk, let's just think about only non negative submodular functions.",
            "Alright, so let me let me now tell you what this P Mac model is and what Barker and how we proved in their first paper so."
        ],
        [
            "So feedback model the learner gets to see random examples from your target submodular function.",
            "The unknown submodular function and the job of the learner is to use this random examples to construct a hypothesis edge which multiplicatively approximates your unknown submodular function at all but an epsilon probability mass of points.",
            "So that's your pee Mak model, and in this model."
        ],
        [
            "And how we studied the problem for off learning submodular functions on arbitrary distributions, and they showed that the problem in fact is very hard in this setting.",
            "They gave an algorithm which produces a order routine multiplicative approximation with probability 1 minus epsilon in polynomial time for all submodular functions.",
            "And they also showed nearly matching lower bound for any polynomial time algorithm that wants to learn submodular functions for the easier case of product distributions, they showed that.",
            "One can get a log one over epsilon multiplicative approximation in time with probability 1 minus epsilon for the class of 1 Lipschitz submodular functions with minimum value at least one.",
            "And so this is the state of art in Barker and Harvey."
        ],
        [
            "Gupta at all.",
            "Look at the problem motivated by applications to privacy and they define this model of learning.",
            "Consider this model of learning with respect to additive error.",
            "So in this model the learner gets to make queries to the function so it can ask for the value of the unknown function at any point of its choice and the job of the learner is to produce a hypothesis which additively approximates.",
            "If so, we would consider this L1 error.",
            "The average additive error for this model, and this will.",
            "This will also be interested.",
            "This will also be the case of interest for our purpose, so this will be the notion of error that we will deal with."
        ],
        [
            "In the previous model, what do you observe system?"
        ],
        [
            "Worries in the in the backing Harvey model in the pmac model, you're supposed to learn the function.",
            "Just some random examples.",
            "Random query results, so they're not really query, so you get random points and then the value of the unknown function.",
            "Real value of the function at that point.",
            "Alright, so that's the the model."
        ],
        [
            "And in this model they showed that one can learn submodular functions on all product distributions with into the one or epsilon squared time where epsilon is your error."
        ],
        [
            "Jackie at all, observed that one can in fact just use random examples to construct a low degree polynomial approximator a degree, one over epsilon squared polynomial approximator.",
            "For submodular functions.",
            "And they use this idea to give a algorithm to learn them in the same time.",
            "The good thing about this algorithm is that they also work in the agnostic setting owing to the approximation by polynomials, which could be learned agnostically by just polynomial regression.",
            "So what's agnostic model?",
            "Let's briefly."
        ],
        [
            "Alright, so in this model we actually get an arbitrary function which is not guaranteed to be submodular.",
            "Have any structure at all, and what we are required to do is produce a hypothesis which does competitively best with respect to the best fitting submodular function.",
            "So it's supposed to give us Optus epsilon error where up to the error of the best fitting submodular function, right?"
        ],
        [
            "So, uh, in a special case of submodular function, we take values in the discrete range from integers zero to K, rush, nikova and Yaroslav, save in 2013 showed that there exists an algorithm that uses value query access to the function and produces a hypothesis in polynomial time."
        ],
        [
            "For all constant error and polynomials are constant size ranges, they considered this stronger notion of error culture disagreement error, which means, since the range is just great, we just want to bound the number of points where the function does not equal to the unknown functions value."
        ],
        [
            "So with this context, now we can look at our algorithmic results which would have which would see in a moment.",
            "So for PAC learning, we show that we can get a polynomial time algorithm in Northern Square to the one epsilon to four for learning discuss.",
            "This improves upon the best known time bound for the PAC learning from the previous works.",
            "We also show that we can improve the time bound of agnostic learning submodular functions if we allowed queries and this algorithm runs in time polynomial in North and to the website on square.",
            "So we can show that in the special case when the submodular function takes values in a discrete range, we can improve upon Rush, Nico and yellow socks at work and get an algorithm that runs in Poly and two to the K and one or epsilon, right?"
        ],
        [
            "We also complement our results were nearly matching lower bounds and essentially the message of this lower bounds, which I'll go into it in a bit, is that our algorithms impact noise setting on nearly optimal."
        ],
        [
            "Alright, so how do we get about this results I guess?",
            "Of more importance to us is this representation approximation results for submodular functions as a consequence of this, which get our learning and learning results.",
            "And So what are these results?",
            "Well, we show that submodular functions are approximated by shallow real value decision trees.",
            "I'll define these objects in a moment.",
            "If you can't realize what exactly they are.",
            "But as a corollary, we can show that submodular functions are efficiently approximated by hunters.",
            "So what are hunters?",
            "Well, these are functions that depend only on a few variables.",
            "So how many variables will be required to approximate the model function well?"
        ],
        [
            "The number of variables just depends on your accuracy.",
            "So whatever dimension you take this a modular function to be in.",
            "You can just approximate approximate it to any constant error with the function of constantly many variables.",
            "So we got a proof of this result using about approximation by decision trees.",
            "We also showed that this implies a simple proof for submodular functions being approximated by low degree polynomials just re proving the result of Cherokee at all that we considered earlier."
        ],
        [
            "Right, so since structural results are for prime interest here, let me let me tell you what I'm going to talk about is actual results in the remaining part of the talk.",
            "So this is the overview of what we will discuss.",
            "We will first tell you about how we can represent some model functions by low rank decision trees with Lipschitz submodular functions at the leaves.",
            "Then we will use this representation to construct an approximation of submodular functions using real value decision trees, which means the leaves will have real values at the end, and then we will use this.",
            "We will use a general result which will prove.",
            "That low rank binary decision trees can be approximated by low depth decision trees and this will give us our final approximation of submodular functions as low depth decision trees.",
            "So without further ado, let's let's get into this."
        ],
        [
            "So at first topic of interest is representing the modular functions using low rank decision trees of Lipschitz or more?"
        ],
        [
            "Functions so one of these objects, recalling briefly, so we have a decision tree.",
            "This is a normal decision tree that we all know about, except that the leaves we have all four Lipschitz armor in a function for some parameter Alpha.",
            "So what?",
            "I will I will come to it.",
            "I'll come to in a moment.",
            "OK, so So what are our objects while we're discussing this decision, trees which have Alpha Lipschitz functions at the leaves?",
            "So what are Alpha left function?",
            "It just shows that the marginal difference.",
            "On one distance, one neighbors of the value of the function is at most Alpha.",
            "OK, so let's recall briefly terminology for trees that we have.",
            "We have Lee."
        ],
        [
            "Then the longest path in the tree would be called as a depth of the tree."
        ],
        [
            "And we will discuss as she asked about low rank decision Tree which now."
        ],
        [
            "I'll define so this is a.",
            "This is a well studied notion.",
            "The rank of a decision tree can be defined recursively as follows.",
            "OK, so let's consider a decision tree tea with left and right subtrees as Steven Ti O."
        ],
        [
            "So ranking is defined as zero if if your tree itself is just a single load.",
            "Believe otherwise it has two children.",
            "If the rank of the two children is different, you just set it as the maximum of the rank of the children.",
            "Otherwise you just add 1 to the rank of the children.",
            "So that's your rank, and in 2 two way to understand rank is to think of it as the depth of the largest complete binary tree that you can embed in your given tree.",
            "Alright, so that's rank of a tree.",
            "Now that we understand what the objects we're dealing with, we can."
        ],
        [
            "State of results.",
            "So we show that some model functions can be exactly computed by decision trees of rank 2 by Alpha with Alpha Lipschitz Armada functions at each leaf.",
            "We will see wireless functions are important in a moment, OK."
        ],
        [
            "So this result of ours is based on the decomposition of submodular functions into Lipschitz similar functions, which was used by Gupta at all in their paper also."
        ],
        [
            "So let me show you quickly how we can actually construct this decision tree approximation for just monotone submodular case.",
            "This is the easier case to understand.",
            "We can extend our proof in order in case in a moment.",
            "So suppose your function happens to be Alpha lip shades in the beginning itself.",
            "So at that time you do nothing and just set it as a leave your done where other."
        ],
        [
            "Guys, we can argue by submodularity that there must exist a variable, say X3 such that when you add this element 3 to the empty set, the value of the function rises by at least Alpha.",
            "So."
        ],
        [
            "So in this case you make extra node and you recurse on."
        ],
        [
            "Children.",
            "Alright, so that's your procedure.",
            "This can go on for as long as it wants.",
            "What we want to argue is that."
        ],
        [
            "The end object that we construct is a low rank decision tree.",
            "So why is the rank of the decision tree produce low?"
        ],
        [
            "Well, the reason is very simple.",
            "By monotonicity we can argue that the number of left turns in the tree is at most one by Alpha.",
            "Notice that in each left turn the value of the function must increase by at least Alpha, and we already considering that this range of the function is bounded between zero and one, so that gives us a simple proof that for monotone submodular functions one can approximate them.",
            "One can actually come exactly, compute them by low rank decision trees of Alpha lifted submodular function."
        ],
        [
            "Alright, So what do we do in order left?",
            "But we use this observation which is also used by Gupta at all that if S is submodular then so is F of S compliment.",
            "I would not go into details here, but this is this is standard.",
            "This has been used also before.",
            "Alright, so now that we believe that some other functions can be computed by all furniture model function decision trees."
        ],
        [
            "The next we go to the next part of the talk which is approximating them by real value decision trees."
        ],
        [
            "Alright, so this is the object we have for some computing a similar function for the previous part.",
            "What we're going to do is replace."
        ],
        [
            "Is Alpha lecture some other functions?"
        ],
        [
            "By real values.",
            "OK, So what are we going to do?",
            "Well, the key idea here, which is also used in previous work, is that you can replace each Alpha submodular function by its expectation and not incur a lot of error.",
            "So there are actually very good concentration inequalities for Lipschitz submodular functions starting with the work of Boucheron, Massart and legacy, for example, this is the inequality we would need that F does not deviate from its expectation the basically the average error.",
            "Of F from its expectation is at most square root Alpha.",
            "If F is Alpha Lipschitz.",
            "Alright, so the user says I need to approximate each leave by a constant, it's expectation and we get a real valued decision tree."
        ],
        [
            "So this gives us a Riddle that submodular functions which we are considering to be normalized with zero and one.",
            "Since we're dealing with Eddie Vedder are approximated within error epsilon by real value decision trees of rank at most 4 over epsilon squared."
        ],
        [
            "So now we go to our third part of a structural results which is approximating low rank binary decision trees by load up decision trees.",
            "So this part itself is a standard result.",
            "We will just apply this to our submodular function approximation to get our final corollary."
        ],
        [
            "So what do we show here?",
            "Will consider a binary decision tree T an.",
            "We will truncate it to depth D which is order R Plus log one over epsilon, where R is the rank of the tree."
        ],
        [
            "And we can show that the disagreement between this T and the truncated tree is at most happens with probability at most epsilon.",
            "This is a general truncation procedure for any language entry, and."
        ],
        [
            "Generalizes the truncation based on size, which was used by cash limits and Mansour in the 1993 paper on Decision Tree learning."
        ],
        [
            "So we can apply this result as A at over submodular function approximation to obtain that for every submodular function F there is a real value decision tree of depth at most one over epsilon squared that L1 approximates within epsilon, so that gives us our final approximation.",
            "Results are modular functions."
        ],
        [
            "This, as I said before, also is a quick proof of degree bound of structural."
        ],
        [
            "Your ex is beautiful.",
            "Distributing the happy to.",
            "Actually it works for any product distribution also.",
            "But you could think of it as uniform distribution in a special case.",
            "Yes alright?"
        ],
        [
            "So now we will quickly discuss applications for learning."
        ],
        [
            "So, So what application is back learning?",
            "We show that there exists an algorithm which given just random example access to the function which returns the hypothesis that Albert approximated with epsilon and depends on just two to one square variables.",
            "This algorithm runs in time till 9 squared to the ones to for an users just log out many examples for any constant error so."
        ],
        [
            "The way we obtained this result is basically using our result from the previous part.",
            "We show that.",
            "Standard Way implies that there is a low degree polynomial approximator that depends on just a few variables.",
            "Now in general it is not known how to find the influential variables, even if you know that there are few.",
            "But for some modular function we can utilize their properties and show that just looking at degree one and degree 2 for your coefficient of these functions can give us what the influential variables are.",
            "Using this procedure we can just do L1 regression in the end to obtain our approximator.",
            "So that's that's how we get our pack learning algorithm."
        ],
        [
            "We can also get agnostic learning algorithm which works with queries in time polynomial in Enter Square and uses the Polygon examples for any constant error.",
            "So here we actually combine our results in the previous part with we can combine our reserve apart with the result of the political climate zone learning decision trees agnostic learning decision trees and but we would lose attribute efficiency.",
            "That is, we would use more number of examples and required to reduce the number of examples.",
            "What we can do is use the attribute efficient version of question which Mansur.",
            "An agnostic posting of Feldman and Kelly Kennedy."
        ],
        [
            "So we talk about nearly matching lower bounds or whatever lower bounds.",
            "So we show that PAC learning even a monotone submodular function requires exponentially more epsilon value queries, and the proof happens.",
            "The proof goes by embedding any arbitrary Boolean function in a monotone submodular function of a slightly higher dimension.",
            "So basically you can reduce learning of any Boolean function to submodular function is slightly high dimension, and that's how we get our lower bound here."
        ],
        [
            "This shows that we're back Gnostic algorithm, which queries are optimal up to the exponent in epsilon one epsilon."
        ],
        [
            "In the computational side, we show that agnostically learning even monotone submodular functions.",
            "If you can beat enter the small of one or epsilon to three algorithm, then you get a faster algorithm for the problem of learning parities with noise."
        ],
        [
            "Problem is a notoriously hard problem, and the best known algorithm that we know even now is just ended the point 8K.",
            "So OK, that we believe it's hard."
        ],
        [
            "And this proof works by ensuring that their existing monitors are modular function which is highly correlated with parity."
        ],
        [
            "So this shows that the agnostic algorithm attraction at all is optimal up to the exponent on epsilon."
        ],
        [
            "Alright, so that's the summary so far.",
            "We can approximate submodular function by load of decision trees and as a consequence we can do almost optimal packing agnostic learning algorithms for the class."
        ],
        [
            "In some follow up work, let me just briefly tell you want to follow work has happened after this.",
            "So Feldman Vondrak actually have improved the hotel approximation that we talked about.",
            "In this talk we can show that we can actually approximates a model function by functions of justice polynomial in one over epsilon variables and they also gave a faster algorithm for learning this class and the P Mac.",
            "The challenging pimc model of Balkan Harvey in other follow-up work within joint work with Vitaly Feldman, we show that one can actually give a fully polynomial time algorithm that is polynomial.",
            "And then one over epsilon 4 pack and P Mac learning a special case of submodular functions, the coverage functions and these are also well studied as valuation functions in game theory, right?",
            "So there's a follow up work."
        ],
        [
            "So the open question that remains is what can you do about more general distributions?",
            "We know that on arbitrary distributions the lower bound of bulk and Harvey also holds even in the winter setting in.",
            "But for some transformation, on the other hand, we know that product distributions are certainly easy.",
            "So what can we do in between?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so I'm going to talk about representation, approximation and learning of submodular functions using low rank decision trees.",
                    "label": 1
                },
                {
                    "sent": "This is a joint work as outside with metallic Feldman and Jan Vondrak of IBM Research.",
                    "label": 0
                },
                {
                    "sent": "So let me start by defining words.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Modular functions are so modular, functions are from abroad.",
                    "label": 0
                },
                {
                    "sent": "Class of family functions called as set functions which are defined on subsets of integers from one to 10 and take real values.",
                    "label": 0
                },
                {
                    "sent": "So a set function F is submodular if it satisfies this property of diminishing marginal returns.",
                    "label": 1
                },
                {
                    "sent": "Which means that if you have two sets as anti says that S is included in T, then the marginal gain in adding a new element to S in the value of the function is a higher or equal to the marginal gain on adding the same new element to the value of the function to the larger set T. So, so that's the property of diminishing marginal returns that defines submodular functions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will equivalently look at these functions to be defined on the Boolean cube by associating every binary string to a set to a subset of integers.",
                    "label": 0
                },
                {
                    "sent": "Wentworth, in the natural way of looking at it as an indicator string of a set.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A submodular functions have a rich history of work on it in like variety of different contexts.",
                    "label": 0
                },
                {
                    "sent": "For example, I guess the most important applications of submodular functions happen to be incremental optimization, where they are viewed as discrete analogue of convex functions.",
                    "label": 1
                },
                {
                    "sent": "This connection actually can be made precise because there exists a convex continuous extension of submodular functions called us lower extension, so modular functions appear in special cases as generalization of special instances of problems that you see in common.",
                    "label": 1
                },
                {
                    "sent": "Position, for example graph cut functions rank functions of matroids set, covering functions all happen to be submodular.",
                    "label": 1
                },
                {
                    "sent": "They also happen to have applications in other problems of interest here, like plant location and sensor placement.",
                    "label": 0
                },
                {
                    "sent": "The application in sensor placement stems from the idea that the information given to you by a set of sensors happens to be a submodular function, and you can use your methods of submodular optimization to get an optimal sensor placement algorithm.",
                    "label": 0
                },
                {
                    "sent": "There are other important use happens to be in algorithmic game theory.",
                    "label": 0
                },
                {
                    "sent": "Do precisely the property I mentioned the last slide that of diminishing marginal returns.",
                    "label": 0
                },
                {
                    "sent": "This is a property which you interpret to be had by utility functions of agents.",
                    "label": 0
                },
                {
                    "sent": "When you're modeling a game theoretic system, your prices, so naturally you'd expect applications in algorithm game theory and economix because of this property and it has been studied extensively in this context.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our application or our main concern would be with learning submodular functions and in this context the problem was first introduced by Balkan and hardware.",
                    "label": 1
                },
                {
                    "sent": "In 2011 they were motivated by learning and predicting submodular functions when they model pricing and utility functions of agents in game theoretic systems, they wanted to demands of agents and they were also motivated by some applications to advertisements.",
                    "label": 1
                },
                {
                    "sent": "So with all these applications that define this new model of learning called SP Mac.",
                    "label": 0
                },
                {
                    "sent": "Which wanted to, which was intended to capture a learning theoretic view when you are interested in commercial optimization and this model is short for probably mostly approximately correct.",
                    "label": 1
                },
                {
                    "sent": "So let me let me just give you a technical note here that in all the works on learning, and in fact for the rest of the talk, let's just think about only non negative submodular functions.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let me let me now tell you what this P Mac model is and what Barker and how we proved in their first paper so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So feedback model the learner gets to see random examples from your target submodular function.",
                    "label": 1
                },
                {
                    "sent": "The unknown submodular function and the job of the learner is to use this random examples to construct a hypothesis edge which multiplicatively approximates your unknown submodular function at all but an epsilon probability mass of points.",
                    "label": 0
                },
                {
                    "sent": "So that's your pee Mak model, and in this model.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And how we studied the problem for off learning submodular functions on arbitrary distributions, and they showed that the problem in fact is very hard in this setting.",
                    "label": 1
                },
                {
                    "sent": "They gave an algorithm which produces a order routine multiplicative approximation with probability 1 minus epsilon in polynomial time for all submodular functions.",
                    "label": 1
                },
                {
                    "sent": "And they also showed nearly matching lower bound for any polynomial time algorithm that wants to learn submodular functions for the easier case of product distributions, they showed that.",
                    "label": 0
                },
                {
                    "sent": "One can get a log one over epsilon multiplicative approximation in time with probability 1 minus epsilon for the class of 1 Lipschitz submodular functions with minimum value at least one.",
                    "label": 0
                },
                {
                    "sent": "And so this is the state of art in Barker and Harvey.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gupta at all.",
                    "label": 0
                },
                {
                    "sent": "Look at the problem motivated by applications to privacy and they define this model of learning.",
                    "label": 0
                },
                {
                    "sent": "Consider this model of learning with respect to additive error.",
                    "label": 0
                },
                {
                    "sent": "So in this model the learner gets to make queries to the function so it can ask for the value of the unknown function at any point of its choice and the job of the learner is to produce a hypothesis which additively approximates.",
                    "label": 0
                },
                {
                    "sent": "If so, we would consider this L1 error.",
                    "label": 0
                },
                {
                    "sent": "The average additive error for this model, and this will.",
                    "label": 0
                },
                {
                    "sent": "This will also be interested.",
                    "label": 0
                },
                {
                    "sent": "This will also be the case of interest for our purpose, so this will be the notion of error that we will deal with.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the previous model, what do you observe system?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Worries in the in the backing Harvey model in the pmac model, you're supposed to learn the function.",
                    "label": 0
                },
                {
                    "sent": "Just some random examples.",
                    "label": 0
                },
                {
                    "sent": "Random query results, so they're not really query, so you get random points and then the value of the unknown function.",
                    "label": 0
                },
                {
                    "sent": "Real value of the function at that point.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's the the model.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in this model they showed that one can learn submodular functions on all product distributions with into the one or epsilon squared time where epsilon is your error.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Jackie at all, observed that one can in fact just use random examples to construct a low degree polynomial approximator a degree, one over epsilon squared polynomial approximator.",
                    "label": 0
                },
                {
                    "sent": "For submodular functions.",
                    "label": 0
                },
                {
                    "sent": "And they use this idea to give a algorithm to learn them in the same time.",
                    "label": 0
                },
                {
                    "sent": "The good thing about this algorithm is that they also work in the agnostic setting owing to the approximation by polynomials, which could be learned agnostically by just polynomial regression.",
                    "label": 1
                },
                {
                    "sent": "So what's agnostic model?",
                    "label": 0
                },
                {
                    "sent": "Let's briefly.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so in this model we actually get an arbitrary function which is not guaranteed to be submodular.",
                    "label": 0
                },
                {
                    "sent": "Have any structure at all, and what we are required to do is produce a hypothesis which does competitively best with respect to the best fitting submodular function.",
                    "label": 0
                },
                {
                    "sent": "So it's supposed to give us Optus epsilon error where up to the error of the best fitting submodular function, right?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, uh, in a special case of submodular function, we take values in the discrete range from integers zero to K, rush, nikova and Yaroslav, save in 2013 showed that there exists an algorithm that uses value query access to the function and produces a hypothesis in polynomial time.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For all constant error and polynomials are constant size ranges, they considered this stronger notion of error culture disagreement error, which means, since the range is just great, we just want to bound the number of points where the function does not equal to the unknown functions value.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with this context, now we can look at our algorithmic results which would have which would see in a moment.",
                    "label": 1
                },
                {
                    "sent": "So for PAC learning, we show that we can get a polynomial time algorithm in Northern Square to the one epsilon to four for learning discuss.",
                    "label": 0
                },
                {
                    "sent": "This improves upon the best known time bound for the PAC learning from the previous works.",
                    "label": 0
                },
                {
                    "sent": "We also show that we can improve the time bound of agnostic learning submodular functions if we allowed queries and this algorithm runs in time polynomial in North and to the website on square.",
                    "label": 1
                },
                {
                    "sent": "So we can show that in the special case when the submodular function takes values in a discrete range, we can improve upon Rush, Nico and yellow socks at work and get an algorithm that runs in Poly and two to the K and one or epsilon, right?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also complement our results were nearly matching lower bounds and essentially the message of this lower bounds, which I'll go into it in a bit, is that our algorithms impact noise setting on nearly optimal.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so how do we get about this results I guess?",
                    "label": 0
                },
                {
                    "sent": "Of more importance to us is this representation approximation results for submodular functions as a consequence of this, which get our learning and learning results.",
                    "label": 0
                },
                {
                    "sent": "And So what are these results?",
                    "label": 0
                },
                {
                    "sent": "Well, we show that submodular functions are approximated by shallow real value decision trees.",
                    "label": 1
                },
                {
                    "sent": "I'll define these objects in a moment.",
                    "label": 0
                },
                {
                    "sent": "If you can't realize what exactly they are.",
                    "label": 0
                },
                {
                    "sent": "But as a corollary, we can show that submodular functions are efficiently approximated by hunters.",
                    "label": 0
                },
                {
                    "sent": "So what are hunters?",
                    "label": 1
                },
                {
                    "sent": "Well, these are functions that depend only on a few variables.",
                    "label": 0
                },
                {
                    "sent": "So how many variables will be required to approximate the model function well?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The number of variables just depends on your accuracy.",
                    "label": 1
                },
                {
                    "sent": "So whatever dimension you take this a modular function to be in.",
                    "label": 0
                },
                {
                    "sent": "You can just approximate approximate it to any constant error with the function of constantly many variables.",
                    "label": 0
                },
                {
                    "sent": "So we got a proof of this result using about approximation by decision trees.",
                    "label": 0
                },
                {
                    "sent": "We also showed that this implies a simple proof for submodular functions being approximated by low degree polynomials just re proving the result of Cherokee at all that we considered earlier.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so since structural results are for prime interest here, let me let me tell you what I'm going to talk about is actual results in the remaining part of the talk.",
                    "label": 0
                },
                {
                    "sent": "So this is the overview of what we will discuss.",
                    "label": 0
                },
                {
                    "sent": "We will first tell you about how we can represent some model functions by low rank decision trees with Lipschitz submodular functions at the leaves.",
                    "label": 1
                },
                {
                    "sent": "Then we will use this representation to construct an approximation of submodular functions using real value decision trees, which means the leaves will have real values at the end, and then we will use this.",
                    "label": 0
                },
                {
                    "sent": "We will use a general result which will prove.",
                    "label": 0
                },
                {
                    "sent": "That low rank binary decision trees can be approximated by low depth decision trees and this will give us our final approximation of submodular functions as low depth decision trees.",
                    "label": 1
                },
                {
                    "sent": "So without further ado, let's let's get into this.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at first topic of interest is representing the modular functions using low rank decision trees of Lipschitz or more?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functions so one of these objects, recalling briefly, so we have a decision tree.",
                    "label": 0
                },
                {
                    "sent": "This is a normal decision tree that we all know about, except that the leaves we have all four Lipschitz armor in a function for some parameter Alpha.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "I will I will come to it.",
                    "label": 0
                },
                {
                    "sent": "I'll come to in a moment.",
                    "label": 0
                },
                {
                    "sent": "OK, so So what are our objects while we're discussing this decision, trees which have Alpha Lipschitz functions at the leaves?",
                    "label": 0
                },
                {
                    "sent": "So what are Alpha left function?",
                    "label": 0
                },
                {
                    "sent": "It just shows that the marginal difference.",
                    "label": 0
                },
                {
                    "sent": "On one distance, one neighbors of the value of the function is at most Alpha.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's recall briefly terminology for trees that we have.",
                    "label": 0
                },
                {
                    "sent": "We have Lee.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the longest path in the tree would be called as a depth of the tree.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we will discuss as she asked about low rank decision Tree which now.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll define so this is a.",
                    "label": 0
                },
                {
                    "sent": "This is a well studied notion.",
                    "label": 0
                },
                {
                    "sent": "The rank of a decision tree can be defined recursively as follows.",
                    "label": 1
                },
                {
                    "sent": "OK, so let's consider a decision tree tea with left and right subtrees as Steven Ti O.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So ranking is defined as zero if if your tree itself is just a single load.",
                    "label": 0
                },
                {
                    "sent": "Believe otherwise it has two children.",
                    "label": 0
                },
                {
                    "sent": "If the rank of the two children is different, you just set it as the maximum of the rank of the children.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you just add 1 to the rank of the children.",
                    "label": 0
                },
                {
                    "sent": "So that's your rank, and in 2 two way to understand rank is to think of it as the depth of the largest complete binary tree that you can embed in your given tree.",
                    "label": 1
                },
                {
                    "sent": "Alright, so that's rank of a tree.",
                    "label": 0
                },
                {
                    "sent": "Now that we understand what the objects we're dealing with, we can.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "State of results.",
                    "label": 0
                },
                {
                    "sent": "So we show that some model functions can be exactly computed by decision trees of rank 2 by Alpha with Alpha Lipschitz Armada functions at each leaf.",
                    "label": 1
                },
                {
                    "sent": "We will see wireless functions are important in a moment, OK.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this result of ours is based on the decomposition of submodular functions into Lipschitz similar functions, which was used by Gupta at all in their paper also.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me show you quickly how we can actually construct this decision tree approximation for just monotone submodular case.",
                    "label": 0
                },
                {
                    "sent": "This is the easier case to understand.",
                    "label": 0
                },
                {
                    "sent": "We can extend our proof in order in case in a moment.",
                    "label": 0
                },
                {
                    "sent": "So suppose your function happens to be Alpha lip shades in the beginning itself.",
                    "label": 0
                },
                {
                    "sent": "So at that time you do nothing and just set it as a leave your done where other.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Guys, we can argue by submodularity that there must exist a variable, say X3 such that when you add this element 3 to the empty set, the value of the function rises by at least Alpha.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case you make extra node and you recurse on.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Children.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's your procedure.",
                    "label": 0
                },
                {
                    "sent": "This can go on for as long as it wants.",
                    "label": 0
                },
                {
                    "sent": "What we want to argue is that.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The end object that we construct is a low rank decision tree.",
                    "label": 0
                },
                {
                    "sent": "So why is the rank of the decision tree produce low?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, the reason is very simple.",
                    "label": 0
                },
                {
                    "sent": "By monotonicity we can argue that the number of left turns in the tree is at most one by Alpha.",
                    "label": 1
                },
                {
                    "sent": "Notice that in each left turn the value of the function must increase by at least Alpha, and we already considering that this range of the function is bounded between zero and one, so that gives us a simple proof that for monotone submodular functions one can approximate them.",
                    "label": 0
                },
                {
                    "sent": "One can actually come exactly, compute them by low rank decision trees of Alpha lifted submodular function.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, So what do we do in order left?",
                    "label": 0
                },
                {
                    "sent": "But we use this observation which is also used by Gupta at all that if S is submodular then so is F of S compliment.",
                    "label": 0
                },
                {
                    "sent": "I would not go into details here, but this is this is standard.",
                    "label": 0
                },
                {
                    "sent": "This has been used also before.",
                    "label": 0
                },
                {
                    "sent": "Alright, so now that we believe that some other functions can be computed by all furniture model function decision trees.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next we go to the next part of the talk which is approximating them by real value decision trees.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so this is the object we have for some computing a similar function for the previous part.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is replace.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is Alpha lecture some other functions?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By real values.",
                    "label": 0
                },
                {
                    "sent": "OK, So what are we going to do?",
                    "label": 0
                },
                {
                    "sent": "Well, the key idea here, which is also used in previous work, is that you can replace each Alpha submodular function by its expectation and not incur a lot of error.",
                    "label": 0
                },
                {
                    "sent": "So there are actually very good concentration inequalities for Lipschitz submodular functions starting with the work of Boucheron, Massart and legacy, for example, this is the inequality we would need that F does not deviate from its expectation the basically the average error.",
                    "label": 0
                },
                {
                    "sent": "Of F from its expectation is at most square root Alpha.",
                    "label": 0
                },
                {
                    "sent": "If F is Alpha Lipschitz.",
                    "label": 1
                },
                {
                    "sent": "Alright, so the user says I need to approximate each leave by a constant, it's expectation and we get a real valued decision tree.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this gives us a Riddle that submodular functions which we are considering to be normalized with zero and one.",
                    "label": 0
                },
                {
                    "sent": "Since we're dealing with Eddie Vedder are approximated within error epsilon by real value decision trees of rank at most 4 over epsilon squared.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we go to our third part of a structural results which is approximating low rank binary decision trees by load up decision trees.",
                    "label": 1
                },
                {
                    "sent": "So this part itself is a standard result.",
                    "label": 0
                },
                {
                    "sent": "We will just apply this to our submodular function approximation to get our final corollary.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do we show here?",
                    "label": 0
                },
                {
                    "sent": "Will consider a binary decision tree T an.",
                    "label": 1
                },
                {
                    "sent": "We will truncate it to depth D which is order R Plus log one over epsilon, where R is the rank of the tree.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can show that the disagreement between this T and the truncated tree is at most happens with probability at most epsilon.",
                    "label": 0
                },
                {
                    "sent": "This is a general truncation procedure for any language entry, and.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generalizes the truncation based on size, which was used by cash limits and Mansour in the 1993 paper on Decision Tree learning.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can apply this result as A at over submodular function approximation to obtain that for every submodular function F there is a real value decision tree of depth at most one over epsilon squared that L1 approximates within epsilon, so that gives us our final approximation.",
                    "label": 0
                },
                {
                    "sent": "Results are modular functions.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This, as I said before, also is a quick proof of degree bound of structural.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your ex is beautiful.",
                    "label": 0
                },
                {
                    "sent": "Distributing the happy to.",
                    "label": 0
                },
                {
                    "sent": "Actually it works for any product distribution also.",
                    "label": 0
                },
                {
                    "sent": "But you could think of it as uniform distribution in a special case.",
                    "label": 0
                },
                {
                    "sent": "Yes alright?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we will quickly discuss applications for learning.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, So what application is back learning?",
                    "label": 0
                },
                {
                    "sent": "We show that there exists an algorithm which given just random example access to the function which returns the hypothesis that Albert approximated with epsilon and depends on just two to one square variables.",
                    "label": 1
                },
                {
                    "sent": "This algorithm runs in time till 9 squared to the ones to for an users just log out many examples for any constant error so.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The way we obtained this result is basically using our result from the previous part.",
                    "label": 0
                },
                {
                    "sent": "We show that.",
                    "label": 0
                },
                {
                    "sent": "Standard Way implies that there is a low degree polynomial approximator that depends on just a few variables.",
                    "label": 0
                },
                {
                    "sent": "Now in general it is not known how to find the influential variables, even if you know that there are few.",
                    "label": 1
                },
                {
                    "sent": "But for some modular function we can utilize their properties and show that just looking at degree one and degree 2 for your coefficient of these functions can give us what the influential variables are.",
                    "label": 0
                },
                {
                    "sent": "Using this procedure we can just do L1 regression in the end to obtain our approximator.",
                    "label": 0
                },
                {
                    "sent": "So that's that's how we get our pack learning algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can also get agnostic learning algorithm which works with queries in time polynomial in Enter Square and uses the Polygon examples for any constant error.",
                    "label": 1
                },
                {
                    "sent": "So here we actually combine our results in the previous part with we can combine our reserve apart with the result of the political climate zone learning decision trees agnostic learning decision trees and but we would lose attribute efficiency.",
                    "label": 0
                },
                {
                    "sent": "That is, we would use more number of examples and required to reduce the number of examples.",
                    "label": 1
                },
                {
                    "sent": "What we can do is use the attribute efficient version of question which Mansur.",
                    "label": 0
                },
                {
                    "sent": "An agnostic posting of Feldman and Kelly Kennedy.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we talk about nearly matching lower bounds or whatever lower bounds.",
                    "label": 0
                },
                {
                    "sent": "So we show that PAC learning even a monotone submodular function requires exponentially more epsilon value queries, and the proof happens.",
                    "label": 1
                },
                {
                    "sent": "The proof goes by embedding any arbitrary Boolean function in a monotone submodular function of a slightly higher dimension.",
                    "label": 0
                },
                {
                    "sent": "So basically you can reduce learning of any Boolean function to submodular function is slightly high dimension, and that's how we get our lower bound here.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This shows that we're back Gnostic algorithm, which queries are optimal up to the exponent in epsilon one epsilon.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the computational side, we show that agnostically learning even monotone submodular functions.",
                    "label": 0
                },
                {
                    "sent": "If you can beat enter the small of one or epsilon to three algorithm, then you get a faster algorithm for the problem of learning parities with noise.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem is a notoriously hard problem, and the best known algorithm that we know even now is just ended the point 8K.",
                    "label": 0
                },
                {
                    "sent": "So OK, that we believe it's hard.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this proof works by ensuring that their existing monitors are modular function which is highly correlated with parity.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this shows that the agnostic algorithm attraction at all is optimal up to the exponent on epsilon.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so that's the summary so far.",
                    "label": 0
                },
                {
                    "sent": "We can approximate submodular function by load of decision trees and as a consequence we can do almost optimal packing agnostic learning algorithms for the class.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In some follow up work, let me just briefly tell you want to follow work has happened after this.",
                    "label": 0
                },
                {
                    "sent": "So Feldman Vondrak actually have improved the hotel approximation that we talked about.",
                    "label": 0
                },
                {
                    "sent": "In this talk we can show that we can actually approximates a model function by functions of justice polynomial in one over epsilon variables and they also gave a faster algorithm for learning this class and the P Mac.",
                    "label": 0
                },
                {
                    "sent": "The challenging pimc model of Balkan Harvey in other follow-up work within joint work with Vitaly Feldman, we show that one can actually give a fully polynomial time algorithm that is polynomial.",
                    "label": 1
                },
                {
                    "sent": "And then one over epsilon 4 pack and P Mac learning a special case of submodular functions, the coverage functions and these are also well studied as valuation functions in game theory, right?",
                    "label": 0
                },
                {
                    "sent": "So there's a follow up work.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the open question that remains is what can you do about more general distributions?",
                    "label": 1
                },
                {
                    "sent": "We know that on arbitrary distributions the lower bound of bulk and Harvey also holds even in the winter setting in.",
                    "label": 0
                },
                {
                    "sent": "But for some transformation, on the other hand, we know that product distributions are certainly easy.",
                    "label": 0
                },
                {
                    "sent": "So what can we do in between?",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}