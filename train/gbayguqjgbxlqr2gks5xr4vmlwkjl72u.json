{
    "id": "gbayguqjgbxlqr2gks5xr4vmlwkjl72u",
    "title": "CyCLaDEs: A Decentralized Cache for Triple Pattern Fragments",
    "info": {
        "author": [
            "Pauline Folz, University of Nantes"
        ],
        "published": "July 28, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2016_folz_pattern_fragments/",
    "segmentation": [
        [
            "So hello everyone, I'm pulling for the PhD students at the University of North unknown Metropool and I'm going to present you cyclists.",
            "Addison tries cash for triple pattern fragments.",
            "Work done with Alice carefully on Pascal marriage."
        ],
        [
            "So recently data fragments introduce a new way to consume linked data.",
            "Compared to sparkle in points linked that LGF server only manage simple triple pattern fragments queries, while the clients perform the joins so the Lord has been redistributed between the clients on the server."
        ],
        [
            "In this approach, caches also play an important role, so we have two type of caches.",
            "We have the HTTP caches on the server side on the local cache on the client's eyes.",
            "On both of those caches contains triple pattern fragments on triple pattern fragments are more likely to be reused.",
            "Locali on across clients."
        ],
        [
            "So this is the AGF approach.",
            "Here you have the side of the data provider with an LDF server on.",
            "HTTP cache on here you have the data consumer on each.",
            "Clients have his own local cache on clients.",
            "Perform HTTP calls over the data provider.",
            "So suppose that this clients want to perform the queries authors of books.",
            "This query will be decomposed in triple patterns queries and will generate 152 HTTP calls.",
            "So here in the background.",
            "We have the triple Python fragments on for each triple pattern fragments, the client will first check his local cache on.",
            "If you don't have it, ask the data provider so the starting point of our paper is what happens."
        ],
        [
            "If clients collaborate so."
        ],
        [
            "If clients collaborate, they will be able to share their local cache so it will reduce the load on the server.",
            "But the challenge is if we have 1 million clients in the network, we cannot connect contact all those clients each time we are performing queries.",
            "So how a client can find a triple pattern fragments quickly in another client cash.",
            "So there is also there is already some cash distribution in the state of art.",
            "One of them is."
        ],
        [
            "The DHT where the cash will be distributed among the participants, so the advantage is if we are looking for a triple pattern fragments, we will find it if it exists.",
            "The drawbacks is if one query generates 2000 cores, we will have 2000s Jesse access where each gesture access is inloggen ups where is a number of peers.",
            "So the latency introduced by the GHT.",
            "It's not suitable for our approach, so we move to another existing app."
        ],
        [
            "Launch, which is the behavioral cash.",
            "So in this approach, similar nodes are connected together.",
            "The advantage is that the lookup latency is zero up, but the drop box is that we are not sure to find the triple pattern fragments even if it exists.",
            "So this approach has been experimented with browsing histories on the web, but not with triple pattern fragments queries.",
            "So we are going to extend this approach.",
            "People, patterns, queries."
        ],
        [
            "In our approach cyclods, we are showing that clients which perform similar queries in the past will perform similar queries in the future.",
            "So each client will have a fixed sized of best similar neighbors with zero up latency on each time.",
            "Clients are going to perform queries for each course.",
            "There is first check their local cache.",
            "If they don't have it, they will ask their neighbors cash in parallel.",
            "If none of their neighbors have it, they will go to the server.",
            "So with this approach, we want to know how many neighbors catch it we can get."
        ],
        [
            "So this is the existing approach of LGF, so clients do not know the neighbors, so they cannot collaborate together.",
            "So if we want them to collaborate, we need to connect those clients."
        ],
        [
            "And we are going to connect those clients through random peer sampling overlay.",
            "Each node will maintains a partial view of the entire network on this partial view will be composed of random nodes.",
            "So here we can see that C1C3 on C2 and can collaborate with them.",
            "So we need to maintain this overlap for that will use shuffle faces.",
            "It will renew periodically the neighbors on it will help us to under the term onto a word network partition.",
            "So from that we use the protocol cyclones."
        ],
        [
            "So the archaeans overlay on show the connectivity between all clients on without distinctions.",
            "For example, if we took in consideration see Sweet, he's performing queries over DB pedia.",
            "SI SIX is performing queries overdrawn bank they are connected together through the RPS, but they are not similar because they are not performing queries on the same data set.",
            "So to under this similarity we need a circle overlay as proposed in the gospel approach this second."
        ],
        [
            "They will be the cluster overlay network, so it's not as a profile based on the history of past queries on we can see here that three is not connected to C1C2 on C5 on those three clients also perform queries over DB pedia.",
            "As previously, we need to maintain this network on will use shuffle faces and so to do it.",
            "So with the shuffle phases it will help us to obtain better neighbors on each time and not will shuffle.",
            "It will ranks on selects.",
            "Best neighbors banks on the similarity of their profiles.",
            "But how do we profile nodes?"
        ],
        [
            "So when not executives queries, it produce a stream of triple pattern fragments on the cash is a window on this stream, so the profile needs to be a summary of the recent past.",
            "The recent past can be seen as the frequency of the colors to recently use predicates, so I'm going to show you the node algorithm."
        ],
        [
            "Not profiling algorithm, sorry.",
            "So here you have the size."
        ],
        [
            "The profile, so it's a number of predicates we are going to take took into account we have this."
        ],
        [
            "Of three parts produced by user query, we have that."
        ],
        [
            "Stump of the node."
        ],
        [
            "On for each predicate in the stream."
        ],
        [
            "Is the predicate was in the profile will increment his frequency on a date is timestamp."
        ],
        [
            "Is a predicate was not in the profile.",
            "We will add a new entry."
        ],
        [
            "On if the profile exceed the size we set, we will remove the oldest on tree, so I'm going to illustrate that with a small example."
        ],
        [
            "So here you have the stream of triple patterns on above.",
            "You have the timestamp of the node, so consider not having a profile of three different predicates.",
            "This profile will be built during query processing's on 1st.",
            "We are going to consider P1.",
            "So it was not in the profile so we add it with frequency one on timestamp one so."
        ],
        [
            "Only we consider P2.",
            "It was not in the profile.",
            "We add enough place so we added then P."
        ],
        [
            "Shrinked was not in the profile.",
            "We add enough place, so we added pizza."
        ],
        [
            "It was already in the profile so we just update the entry."
        ],
        [
            "Again, we have dates on tree."
        ],
        [
            "Update some tree."
        ],
        [
            "Three was also already in the profile, so we just need to update the entry."
        ],
        [
            "If they don't re again on here, we can see that the Nets."
        ],
        [
            "Kate, it's before but before is not in the profile already on.",
            "The profile is full so we will need to remove the oldest on three.",
            "So here's the oldest entry is P1 because he had the timestamp of 1, so we are going to remove P1 on ad P4 instead.",
            "On here we can see that P4 is not the most recent predicate on P2 is the oldest predicate in the profile.",
            "So now that we know how to profile nodes, we are able to compare them."
        ],
        [
            "So we are going to compare notes thanks to their profiles on.",
            "We are going to use the generalized similarity for that.",
            "So suppose here that we want to run nodes for C5.",
            "Here we have the nodes area of the predicates on 10 is the frequency of P1 for the Nazi 5.",
            "Above you have the results on.",
            "We can see that C9 is more similar to see five than C8.",
            "So now I'm going to show you how the shuffling works with the ranking."
        ],
        [
            "So here we have a network of 10 clients we are going to focus on the on C5 on C6.",
            "Each node of 200 MPs represented with blue arrows on.",
            "I've for cluster peers represented, represented with Red Rose on C5 is going to shuffle with C6.",
            "Here on this side we have the state.",
            "I found the shuffling.",
            "We can see that C5 erase C6 from his random peers, an ad 3 instead on for the cluster overlay we can see that C5 removes the eight from his cluster peers on add C9 instead because C9 is more similar to C5.",
            "Then see eight as we see previously.",
            "So to summarize our approach."
        ],
        [
            "Suppose the C one is going to perform the query authors of books for each triple pattern query.",
            "For each triple back down query, it will first try to resolve it locally.",
            "If you don't have it, you will ask his neighbors C3C5 on C2 in parallel.",
            "If none of his neighbors are the triple pattern fragments, you will ask the data provider.",
            "So if the data provider have it in his HTTP cache, it will send it directly to see one on.",
            "If you don't have it, the LGF server will recompute zone Sir for this triple pattern FRAGMENTE.",
            "So now I'm going to show you the experiments we back form."
        ],
        [
            "So in C clouds we have one absorption that's clients which perform similar queries in the past will perform similar queries in the future, so we are in the context of web applications on we choose the burning benchmark for that we set up when LGF server exposing those data sets we set up when HTTP cache on we build the networks of extended LDF clients."
        ],
        [
            "Each clients as a query mix of 25 queries generated from 12 templates for all the experiments, shuffles phases Orchard every 10 seconds.",
            "On the experiments we've done in two facets.",
            "First ones were more prone to bootstrap.",
            "The random sampling overlay cluster overlay on HTTP cache on real round where we perform our measure."
        ],
        [
            "So in the first experiments we perform two of them without cycle ads on with cyclists on.",
            "We vary the number of clients on the X axis.",
            "We have the number of clients igrec access without the percentage of total calls.",
            "Growing represents the percentage of calls under Locali.",
            "Arrange the percentage of calls under in the neighborhood.",
            "On yellow, the percentage of curl under.",
            "In the LGF server.",
            "So if we look at the experiments, wiser results.",
            "I class we can see that 40% of the cores on dealt locally, which is quite good on if we look at the experiments with cyclas we can see that 20% of the calls previously under by the neighborhood by the server.",
            "Sorry, I know under by the neighborhood whatever the number of clients.",
            "So currently we wanted to see the impact of the data set size."
        ],
        [
            "So for that we set up three bledman.",
            "And 10,000,000 on one, hundreds, millions.",
            "And we can see that varying the size of the data sets reduced the number.",
            "The percentage of course on there locali because when we increase the size of the data sets, query generates more HTTP curves.",
            "However, we can see that.",
            "Behavior high cache of bitter resistance to the size of the data sets.",
            "Uncerd we wanted to see if psych Labs will be able to detect communities."
        ],
        [
            "So for that we set up.",
            "Two communities with two different vibe data sets.",
            "We add 50 clients for each data set, so all those clients were in the same network, so they share the same random sampling.",
            "I want to know if circulars profile will be able to detect those two communities."
        ],
        [
            "So far that we performed two experiments.",
            "The first one with the profile size of five.",
            "So we are going to take into account five different predicates and a profile size of 30.",
            "The graph represents the state of the cluster overlay or not represent appear in this cluster.",
            "Overlay on edges means that not 30 of is not 97 in his cluster view.",
            "On here we can see that.",
            "With a profile size of 30 we have.",
            "We have more well defined communities because the profile is more accurate.",
            "So."
        ],
        [
            "Two concludes cyclones builds a behavior highly decentralised cash for LGF clients on this decentralized cash reduce the calls to the server in the context of web applications on by Collaborating's clients build a Federation of data consumer on one step further in the contribution to query processing fee."
        ],
        [
            "Rocks will be first to measure the impact on the execution times on.",
            "Secondly Cyclas we bring the data to the queries.",
            "Another approach could be to bring the queries to the data.",
            "For example, if I have to perform queries with three different predicates, but none of these predicates are in my cache, but one of my neighbors are those three predicates, I can ask him to perform the query formation."
        ],
        [
            "Thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So hello everyone, I'm pulling for the PhD students at the University of North unknown Metropool and I'm going to present you cyclists.",
                    "label": 0
                },
                {
                    "sent": "Addison tries cash for triple pattern fragments.",
                    "label": 1
                },
                {
                    "sent": "Work done with Alice carefully on Pascal marriage.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So recently data fragments introduce a new way to consume linked data.",
                    "label": 0
                },
                {
                    "sent": "Compared to sparkle in points linked that LGF server only manage simple triple pattern fragments queries, while the clients perform the joins so the Lord has been redistributed between the clients on the server.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this approach, caches also play an important role, so we have two type of caches.",
                    "label": 1
                },
                {
                    "sent": "We have the HTTP caches on the server side on the local cache on the client's eyes.",
                    "label": 0
                },
                {
                    "sent": "On both of those caches contains triple pattern fragments on triple pattern fragments are more likely to be reused.",
                    "label": 1
                },
                {
                    "sent": "Locali on across clients.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the AGF approach.",
                    "label": 0
                },
                {
                    "sent": "Here you have the side of the data provider with an LDF server on.",
                    "label": 1
                },
                {
                    "sent": "HTTP cache on here you have the data consumer on each.",
                    "label": 0
                },
                {
                    "sent": "Clients have his own local cache on clients.",
                    "label": 0
                },
                {
                    "sent": "Perform HTTP calls over the data provider.",
                    "label": 0
                },
                {
                    "sent": "So suppose that this clients want to perform the queries authors of books.",
                    "label": 1
                },
                {
                    "sent": "This query will be decomposed in triple patterns queries and will generate 152 HTTP calls.",
                    "label": 0
                },
                {
                    "sent": "So here in the background.",
                    "label": 0
                },
                {
                    "sent": "We have the triple Python fragments on for each triple pattern fragments, the client will first check his local cache on.",
                    "label": 0
                },
                {
                    "sent": "If you don't have it, ask the data provider so the starting point of our paper is what happens.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If clients collaborate so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If clients collaborate, they will be able to share their local cache so it will reduce the load on the server.",
                    "label": 1
                },
                {
                    "sent": "But the challenge is if we have 1 million clients in the network, we cannot connect contact all those clients each time we are performing queries.",
                    "label": 0
                },
                {
                    "sent": "So how a client can find a triple pattern fragments quickly in another client cash.",
                    "label": 1
                },
                {
                    "sent": "So there is also there is already some cash distribution in the state of art.",
                    "label": 0
                },
                {
                    "sent": "One of them is.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The DHT where the cash will be distributed among the participants, so the advantage is if we are looking for a triple pattern fragments, we will find it if it exists.",
                    "label": 1
                },
                {
                    "sent": "The drawbacks is if one query generates 2000 cores, we will have 2000s Jesse access where each gesture access is inloggen ups where is a number of peers.",
                    "label": 1
                },
                {
                    "sent": "So the latency introduced by the GHT.",
                    "label": 0
                },
                {
                    "sent": "It's not suitable for our approach, so we move to another existing app.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Launch, which is the behavioral cash.",
                    "label": 0
                },
                {
                    "sent": "So in this approach, similar nodes are connected together.",
                    "label": 1
                },
                {
                    "sent": "The advantage is that the lookup latency is zero up, but the drop box is that we are not sure to find the triple pattern fragments even if it exists.",
                    "label": 1
                },
                {
                    "sent": "So this approach has been experimented with browsing histories on the web, but not with triple pattern fragments queries.",
                    "label": 0
                },
                {
                    "sent": "So we are going to extend this approach.",
                    "label": 0
                },
                {
                    "sent": "People, patterns, queries.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In our approach cyclods, we are showing that clients which perform similar queries in the past will perform similar queries in the future.",
                    "label": 1
                },
                {
                    "sent": "So each client will have a fixed sized of best similar neighbors with zero up latency on each time.",
                    "label": 0
                },
                {
                    "sent": "Clients are going to perform queries for each course.",
                    "label": 0
                },
                {
                    "sent": "There is first check their local cache.",
                    "label": 1
                },
                {
                    "sent": "If they don't have it, they will ask their neighbors cash in parallel.",
                    "label": 0
                },
                {
                    "sent": "If none of their neighbors have it, they will go to the server.",
                    "label": 0
                },
                {
                    "sent": "So with this approach, we want to know how many neighbors catch it we can get.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the existing approach of LGF, so clients do not know the neighbors, so they cannot collaborate together.",
                    "label": 0
                },
                {
                    "sent": "So if we want them to collaborate, we need to connect those clients.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we are going to connect those clients through random peer sampling overlay.",
                    "label": 1
                },
                {
                    "sent": "Each node will maintains a partial view of the entire network on this partial view will be composed of random nodes.",
                    "label": 1
                },
                {
                    "sent": "So here we can see that C1C3 on C2 and can collaborate with them.",
                    "label": 1
                },
                {
                    "sent": "So we need to maintain this overlap for that will use shuffle faces.",
                    "label": 0
                },
                {
                    "sent": "It will renew periodically the neighbors on it will help us to under the term onto a word network partition.",
                    "label": 0
                },
                {
                    "sent": "So from that we use the protocol cyclones.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the archaeans overlay on show the connectivity between all clients on without distinctions.",
                    "label": 1
                },
                {
                    "sent": "For example, if we took in consideration see Sweet, he's performing queries over DB pedia.",
                    "label": 0
                },
                {
                    "sent": "SI SIX is performing queries overdrawn bank they are connected together through the RPS, but they are not similar because they are not performing queries on the same data set.",
                    "label": 0
                },
                {
                    "sent": "So to under this similarity we need a circle overlay as proposed in the gospel approach this second.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They will be the cluster overlay network, so it's not as a profile based on the history of past queries on we can see here that three is not connected to C1C2 on C5 on those three clients also perform queries over DB pedia.",
                    "label": 1
                },
                {
                    "sent": "As previously, we need to maintain this network on will use shuffle faces and so to do it.",
                    "label": 1
                },
                {
                    "sent": "So with the shuffle phases it will help us to obtain better neighbors on each time and not will shuffle.",
                    "label": 1
                },
                {
                    "sent": "It will ranks on selects.",
                    "label": 0
                },
                {
                    "sent": "Best neighbors banks on the similarity of their profiles.",
                    "label": 0
                },
                {
                    "sent": "But how do we profile nodes?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when not executives queries, it produce a stream of triple pattern fragments on the cash is a window on this stream, so the profile needs to be a summary of the recent past.",
                    "label": 0
                },
                {
                    "sent": "The recent past can be seen as the frequency of the colors to recently use predicates, so I'm going to show you the node algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not profiling algorithm, sorry.",
                    "label": 0
                },
                {
                    "sent": "So here you have the size.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The profile, so it's a number of predicates we are going to take took into account we have this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of three parts produced by user query, we have that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stump of the node.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On for each predicate in the stream.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the predicate was in the profile will increment his frequency on a date is timestamp.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a predicate was not in the profile.",
                    "label": 0
                },
                {
                    "sent": "We will add a new entry.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On if the profile exceed the size we set, we will remove the oldest on tree, so I'm going to illustrate that with a small example.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here you have the stream of triple patterns on above.",
                    "label": 0
                },
                {
                    "sent": "You have the timestamp of the node, so consider not having a profile of three different predicates.",
                    "label": 0
                },
                {
                    "sent": "This profile will be built during query processing's on 1st.",
                    "label": 0
                },
                {
                    "sent": "We are going to consider P1.",
                    "label": 0
                },
                {
                    "sent": "So it was not in the profile so we add it with frequency one on timestamp one so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only we consider P2.",
                    "label": 0
                },
                {
                    "sent": "It was not in the profile.",
                    "label": 0
                },
                {
                    "sent": "We add enough place so we added then P.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shrinked was not in the profile.",
                    "label": 0
                },
                {
                    "sent": "We add enough place, so we added pizza.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It was already in the profile so we just update the entry.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, we have dates on tree.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Update some tree.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three was also already in the profile, so we just need to update the entry.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If they don't re again on here, we can see that the Nets.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kate, it's before but before is not in the profile already on.",
                    "label": 0
                },
                {
                    "sent": "The profile is full so we will need to remove the oldest on three.",
                    "label": 0
                },
                {
                    "sent": "So here's the oldest entry is P1 because he had the timestamp of 1, so we are going to remove P1 on ad P4 instead.",
                    "label": 0
                },
                {
                    "sent": "On here we can see that P4 is not the most recent predicate on P2 is the oldest predicate in the profile.",
                    "label": 0
                },
                {
                    "sent": "So now that we know how to profile nodes, we are able to compare them.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we are going to compare notes thanks to their profiles on.",
                    "label": 0
                },
                {
                    "sent": "We are going to use the generalized similarity for that.",
                    "label": 0
                },
                {
                    "sent": "So suppose here that we want to run nodes for C5.",
                    "label": 0
                },
                {
                    "sent": "Here we have the nodes area of the predicates on 10 is the frequency of P1 for the Nazi 5.",
                    "label": 0
                },
                {
                    "sent": "Above you have the results on.",
                    "label": 0
                },
                {
                    "sent": "We can see that C9 is more similar to see five than C8.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to show you how the shuffling works with the ranking.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we have a network of 10 clients we are going to focus on the on C5 on C6.",
                    "label": 0
                },
                {
                    "sent": "Each node of 200 MPs represented with blue arrows on.",
                    "label": 0
                },
                {
                    "sent": "I've for cluster peers represented, represented with Red Rose on C5 is going to shuffle with C6.",
                    "label": 0
                },
                {
                    "sent": "Here on this side we have the state.",
                    "label": 0
                },
                {
                    "sent": "I found the shuffling.",
                    "label": 0
                },
                {
                    "sent": "We can see that C5 erase C6 from his random peers, an ad 3 instead on for the cluster overlay we can see that C5 removes the eight from his cluster peers on add C9 instead because C9 is more similar to C5.",
                    "label": 0
                },
                {
                    "sent": "Then see eight as we see previously.",
                    "label": 0
                },
                {
                    "sent": "So to summarize our approach.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Suppose the C one is going to perform the query authors of books for each triple pattern query.",
                    "label": 0
                },
                {
                    "sent": "For each triple back down query, it will first try to resolve it locally.",
                    "label": 0
                },
                {
                    "sent": "If you don't have it, you will ask his neighbors C3C5 on C2 in parallel.",
                    "label": 0
                },
                {
                    "sent": "If none of his neighbors are the triple pattern fragments, you will ask the data provider.",
                    "label": 0
                },
                {
                    "sent": "So if the data provider have it in his HTTP cache, it will send it directly to see one on.",
                    "label": 0
                },
                {
                    "sent": "If you don't have it, the LGF server will recompute zone Sir for this triple pattern FRAGMENTE.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to show you the experiments we back form.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in C clouds we have one absorption that's clients which perform similar queries in the past will perform similar queries in the future, so we are in the context of web applications on we choose the burning benchmark for that we set up when LGF server exposing those data sets we set up when HTTP cache on we build the networks of extended LDF clients.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Each clients as a query mix of 25 queries generated from 12 templates for all the experiments, shuffles phases Orchard every 10 seconds.",
                    "label": 1
                },
                {
                    "sent": "On the experiments we've done in two facets.",
                    "label": 0
                },
                {
                    "sent": "First ones were more prone to bootstrap.",
                    "label": 1
                },
                {
                    "sent": "The random sampling overlay cluster overlay on HTTP cache on real round where we perform our measure.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the first experiments we perform two of them without cycle ads on with cyclists on.",
                    "label": 0
                },
                {
                    "sent": "We vary the number of clients on the X axis.",
                    "label": 0
                },
                {
                    "sent": "We have the number of clients igrec access without the percentage of total calls.",
                    "label": 0
                },
                {
                    "sent": "Growing represents the percentage of calls under Locali.",
                    "label": 0
                },
                {
                    "sent": "Arrange the percentage of calls under in the neighborhood.",
                    "label": 0
                },
                {
                    "sent": "On yellow, the percentage of curl under.",
                    "label": 0
                },
                {
                    "sent": "In the LGF server.",
                    "label": 0
                },
                {
                    "sent": "So if we look at the experiments, wiser results.",
                    "label": 0
                },
                {
                    "sent": "I class we can see that 40% of the cores on dealt locally, which is quite good on if we look at the experiments with cyclas we can see that 20% of the calls previously under by the neighborhood by the server.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I know under by the neighborhood whatever the number of clients.",
                    "label": 0
                },
                {
                    "sent": "So currently we wanted to see the impact of the data set size.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for that we set up three bledman.",
                    "label": 0
                },
                {
                    "sent": "And 10,000,000 on one, hundreds, millions.",
                    "label": 0
                },
                {
                    "sent": "And we can see that varying the size of the data sets reduced the number.",
                    "label": 0
                },
                {
                    "sent": "The percentage of course on there locali because when we increase the size of the data sets, query generates more HTTP curves.",
                    "label": 0
                },
                {
                    "sent": "However, we can see that.",
                    "label": 0
                },
                {
                    "sent": "Behavior high cache of bitter resistance to the size of the data sets.",
                    "label": 0
                },
                {
                    "sent": "Uncerd we wanted to see if psych Labs will be able to detect communities.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for that we set up.",
                    "label": 0
                },
                {
                    "sent": "Two communities with two different vibe data sets.",
                    "label": 0
                },
                {
                    "sent": "We add 50 clients for each data set, so all those clients were in the same network, so they share the same random sampling.",
                    "label": 0
                },
                {
                    "sent": "I want to know if circulars profile will be able to detect those two communities.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So far that we performed two experiments.",
                    "label": 0
                },
                {
                    "sent": "The first one with the profile size of five.",
                    "label": 0
                },
                {
                    "sent": "So we are going to take into account five different predicates and a profile size of 30.",
                    "label": 0
                },
                {
                    "sent": "The graph represents the state of the cluster overlay or not represent appear in this cluster.",
                    "label": 0
                },
                {
                    "sent": "Overlay on edges means that not 30 of is not 97 in his cluster view.",
                    "label": 0
                },
                {
                    "sent": "On here we can see that.",
                    "label": 0
                },
                {
                    "sent": "With a profile size of 30 we have.",
                    "label": 0
                },
                {
                    "sent": "We have more well defined communities because the profile is more accurate.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two concludes cyclones builds a behavior highly decentralised cash for LGF clients on this decentralized cash reduce the calls to the server in the context of web applications on by Collaborating's clients build a Federation of data consumer on one step further in the contribution to query processing fee.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rocks will be first to measure the impact on the execution times on.",
                    "label": 0
                },
                {
                    "sent": "Secondly Cyclas we bring the data to the queries.",
                    "label": 1
                },
                {
                    "sent": "Another approach could be to bring the queries to the data.",
                    "label": 0
                },
                {
                    "sent": "For example, if I have to perform queries with three different predicates, but none of these predicates are in my cache, but one of my neighbors are those three predicates, I can ask him to perform the query formation.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                }
            ]
        }
    }
}