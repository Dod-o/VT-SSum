{
    "id": "kpxmyfvjzqc3xejdcyiipmfpyez4dbrb",
    "title": "A New Algorithm for Compressed Counting with Applications in Shannon Entropy Estimation in Dynamic Data",
    "info": {
        "author": [
            "Ping Li, Department of Statistical Science, Cornell University"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Information Theory"
        ]
    },
    "url": "http://videolectures.net/colt2011_li_data/",
    "segmentation": [
        [
            "So the problem statement."
        ],
        [
            "Is that assume that we have very non vector of D?",
            "You could be larger than than two 64 in the network application for example.",
            "And she can be icy papers over the two to the to the, to the 112 to 128, and so it's very long vector, so the goal is to count the frequency moment.",
            "So A is an element that the value so you raised to office power than some of the more.",
            "So it's a if you could do this is second moment after one just the sum.",
            "So an in particular we're interested in the Shannon entropy.",
            "This is the empirical formula Shannon entropy.",
            "So the task is actually trivial, because how hard it is just.",
            "Accumulate all those numbers right?",
            "And it's a trivial if data static.",
            "However, this study will consider we consider streaming data the data string case."
        ],
        [
            "So the data stream.",
            "Is.",
            "It's very common in real life.",
            "Many real life applications.",
            "So, so the basic model is that assuming that empty there's incoming elements.",
            "The first little I indicates the location.",
            "The capital I is the increment.",
            "So according to this linear updating rule, so a particular time T one of the elements of the vector is updated.",
            "So this is the general very popular model called 10 Stein data Stream model.",
            "And now the goal is to count the frequency moments.",
            "And note that the A the the.",
            "Elements.",
            "In this vector, the value is always larger than larger than 0 equal to 0 in most of the cases.",
            "In most of the applications.",
            "Even though the increments can be negative, but usually the A is a positive or negative.",
            "So the observation that the nothing to one this is a trivial task.",
            "Because we just accumulate all the increments.",
            "It's a trivial task.",
            "And a Sunday night.",
            "But when after not equal to 1, that has become very hard because we have the we have to count.",
            "We had to use the counters, but these very large number we the reason we need to use the counter is because the Alpha this is nonlinear operation.",
            "So in order to compute something raised to offer power exactly, you really have to use a D counters, otherwise they cannot do it.",
            "But when after one this is actually trivial, just need to accumulate all this other changes.",
            "So this is interesting and I notice there's like in 2007.",
            "So I started working on it because there must be something something must be wrong, right?",
            "Let me something must be doable here.",
            "Because whenever you want we know why it is difficult, but whatever you want is trivial.",
            "So.",
            "So I start work and this is when I move to move to Cornell."
        ],
        [
            "And I will tell you the results.",
            "But now we just give a little more cartoon illustration of the test and data stream model.",
            "This is a time T. Suppose this is the IP or IP pair or just some location.",
            "I want to D so all the elements are zero at 2 to one, but I took two material to one for example user like 3 order 10 books it become.",
            "Hands number become ten and attempted to make 2.",
            "For example user one order five books.",
            "So now now now become five.",
            "I take 3 to order the user the user 3 left he regretted.",
            "He cancel.",
            "Eight books now become two and notice that this number even though this increments can be negative.",
            "Usually this value can only be larger than now equal to 0.",
            "This is because if you want to cancel 18 books you know you cannot do it because you didn't order 18 books.",
            "So you can only do so that's that."
        ],
        [
            "That's the model, and so this is very natural that in many network applications because.",
            "And the network traffic and natural high rate data streams and the D can be the size of IP addresses and port numbers, and more popularly is actually the the compilation of the orange and destinations is appear, so that's why D can be particularly large.",
            "And and so the elements value represents the total bytes of packages up to time T's address.",
            "I so edge will be the shanty views network traffic distributions.",
            "So anomaly events such as network failure and canal of service attacks usually change the distribution of traffic which maybe capture are affected by the entropy measurements.",
            "And it's critical to be able to measure or even approximate their entropies.",
            "An other summary statistic in real life using small."
        ],
        [
            "So I'm a little bit more details on this denial over distributed denial of service attack is a representative example of anomalies, and this attack attempts to make a computer is unavailable to intended users is by forcing user to result computer or exhaust resource resource source for service hosting sites, for example, had custom maybe just saturate the victim victim machines by sending many external communication requests and the tactical changes at Target site such as banks.",
            "The current government and military, so it's a serious thing and so therefore.",
            "But however, the attack normally changes the statistic.",
            "Tribulations of traffic which could be very reliable captured by.",
            "And by the abnormal variations in the match."
        ],
        [
            "So Shannon, entropy.",
            "So here's a here's the examples.",
            "I copy this copy this figure from a from a DARPA conference and apologize for the low quality because I asked the authors for the original plots and never getting replies.",
            "And so basically you can view this.",
            "So this is Shannon entropy measurements for network and you can view this access at the time is just a representation of time.",
            "So at the time when this attack occur.",
            "We can see that the sound change of Shannon entropy.",
            "So if we can catch this moment and do something, I don't know what to do yet, but it's not my measure.",
            "But if we can capture that at that moment, so so may be very useful.",
            "OK, so maybe people in the audience know how to do it, but."
        ],
        [
            "OK, so so now the outline of our approach is that the goal is developer efficient algorithm.",
            "NASA made the frequency moment and Shannon entropy, so this is again this definition of frequency moment.",
            "This definition of Shannon entropy and it's actually it's interesting that we know that at least people who develop this, Randy entropy Attalus entropy, they know that Shannon entropy can be well approximated by letting the Alpha which is exponents go go to one and using either entropy or thoughtless entropy.",
            "So the basic idea is that since the F1, the first moment is trivial, we just count exactly so we can able to estimate the first moment with awful close to one.",
            "Close enough.",
            "Maybe we get very good estimate the entropies.",
            "So this is very good idea.",
            "However, the problem is here is divided one 1 /, 1 minus Alpha.",
            "We know that the variance estimators F you have various, then the variance is reflected by the square of these coefficients, right?",
            "So you get a variance with the constant with coefficients proportional to the 1 -- 1 / 1 minus Alpha squared.",
            "So this is a huge.",
            "So you say, well I get a good estimate of Alpha, but somehow you various blows up, it's not going to do good.",
            "So therefore the estimate has to be driven accurate.",
            "So it's not an echo of variance has to be.",
            "Usually if you say there is a constant, but now you various has to be proportional to the Delta square, so it has to be generally accurate.",
            "So that's a challenge."
        ],
        [
            "And so we we found the solution.",
            "The solutions we do the metrical compress counting, which I call I could name in 2007.",
            "Then I posted my first paper in archive in 2008 and accomplished counting and adapts.",
            "We should basically called maximum skill stable random projections.",
            "We know random projections we probably made everybody know random projection which uses symmetric is normal distributions and.",
            "Many people also knows stable random projections to instead of his normal users table for you.",
            "For example koshi.",
            "But however, they also metric.",
            "So now the trick is that using skewed distribution is still symmetric.",
            "May be wondering why skill?",
            "Well, the data skill that are positive.",
            "So that's the intuition.",
            "But we need to workout the map so we use in school.",
            "But how school maximum skill?",
            "So we suppose is a is a.",
            "Is that string?",
            "I would know that is vector and we multiply conceptually multiplied with the matrix R and the right size.",
            "So now we get a vector new vector which is only size 99.",
            "KK is small.",
            "So here are the choices of the of the end of the entries are just is off a stable beta skill.",
            "Actually better into one always because we show that in the early in the 2000 seven 2008 paper will show.",
            "You need to use a maximum skill.",
            "So so now we choose offer less than one.",
            "And using this estimator.",
            "So we do the projections we use choose choose Alpha computer Delta, will this estimator?",
            "It looks strange at beginning if you ask me how the driver the honest answer is actually yes there yeah, so it's hard to guess like that as well.",
            "If you start looking at this 2007 or 2011 so it look at every day but not every day.",
            "But some of you have many, many guesses.",
            "Some of the guests were actually work and once you get the answer the rest of work actually standard.",
            "So the difficulties.",
            "Guess the answer.",
            "So once again answer.",
            "So that is almost non bias and variance has a nice hazard.",
            "Desired property is Delta squared.",
            "And one important thing I need to clarify that the R is actually the K times larger as the than the original matrix, so that some people maybe think that's ridiculous because you say is large enough, but you have to something this K times are large.",
            "So here's the trick is the is actually it's not random pseudorandom.",
            "So it's never materialized in inventor projection.",
            "In machine learning we usually we do not care much because usually the metric is small enough.",
            "We don't want the random nature small enough do not care about that, but here the random matrix big enough we actually have to really worry about it.",
            "We had to use pseudorandom numbers, so that means we just I.",
            "Details next slice.",
            "So basically we just regenerate the entries as needed because the pseudorandom is actually deterministic.",
            "Once the random random random seeds or the parameters are selected."
        ],
        [
            "So, so you might be wondering why why this whole thing works well for us over the linear is a linear stable and just learning projection works.",
            "It works because the streaming model is linear model.",
            "And our projection is linear, so.",
            "So so basically, so as long as you have a new entries, you just update the update, the projection projection values, because it's everything is linear here, so therefore you only need to regenerate entries or R. According to this index, IT&J.",
            "And we know how to generate the random numbers RA X + B minus Mother P. That's the simplest one, which doesn't work too well, but you will see that you just user users.",
            "Have a coding coding value you have choose random A&B, model P, and then you actually get a pseudorandom number which is reasonably well well approximated.",
            "And you can do better than that by using for universal hashing or other better hashing schemes, but in general that's that's a standard trick, and I'm not particularly interested, but I just.",
            "I just assume that we're able to regenerate this number and on the fly so you have more questions.",
            "We can discuss offline, so this is a solved linear projection part and the stable random projection part.",
            "Is this, so if the entries are Alpha, beta, stable, Alpha, beta and one parameters beta one always so bad because this is linear projection, so by the property of the stable distributions and usually people familiar with the normal or symmetric but for skill is the same, you get the same Alpha and beta.",
            "Now this index parameter, I mean now the scale parameter.",
            "Instead of why you become exactly the frequency moments we care about, so that's the trick.",
            "Now it becomes statistical problem or change from.",
            "Yeah, now becomes a problem so we can do some estimation."
        ],
        [
            "Work on it.",
            "So, so I've been working on this for quite many years, and so I was pretty good at doing the geometric mean harmonic mean estimator.",
            "So that's my my my.",
            "This guess so when I do the geometric mean and workout all the variances, I notice that it's the variance is proportional to 1 minus Alpha, so that people find the property in 2009 even though that didn't work in 2007 and and.",
            "And at that time was very happy where because you get 1 minus Alpha, so that's very very very good."
        ],
        [
            "And we can see that in the previous my previous work, if we use a symmetric instead of skill.",
            "And the variance is actually proportional tool or the other things that buys with two plus Alpha squared, so it's not goes to zero.",
            "The various has to be equal to 0 at rate of 1 minus Alpha square and buys actually is a constant.",
            "So therefore this line here.",
            "And if we do the geometric mean you get this line.",
            "So this is the.",
            "This is much better so that the moment is dream."
        ],
        [
            "Happy and but also work on this harmonic mean estimator slightly better, but it's still one mass offer."
        ],
        [
            "So we can see this, but obviously better.",
            "But when you go to here actually not as getting so."
        ],
        [
            "So so, so those are very nice except.",
            "Now I realize we really need off 1 minus Alpha Square instead of one minute off a, so we need a better than geometric mean, harmonic mean and the reasons is actually clear because geometric mean is essentially treat every element of the same every sample, more or less the same.",
            "So we have a skewed distribution, but we're using the next symmetric estimator, so it's not going to work too well.",
            "So now retrospectively is clear why this doesn't work well, even though it's."
        ],
        [
            "Better than before."
        ],
        [
            "So so therefore we use this estimator.",
            "It's it's it magnifies the smallest elements.",
            "If you see that you actually magnified smallest elements the most, so it's almost cost users minimum value, but not exactly the minimum, so it's close to the minimum.",
            "And actually had a report and couple years back using the minimal I can prove something similar, but not as good."
        ],
        [
            "And now do an empirical study.",
            "So one of the reasons we like Co is because they encourage empirical results.",
            "I remember when I start writing theory papers, material friends told me that don't include any empirical results because that's going to only going to hurt and that was that was designed to me and but the experience told me that the truth in many places, but it's not true for code so so I'm and I always like like like like the empirical studies.",
            "Really, the exciting part to me, and so because we really care about the.",
            "The accuracy, so I don't have streaming data, but if we only care about accuracy, we actually actually actually I can do any user any vector and just compute the entropy and moments.",
            "So I don't use streaming data by using static data, but because the goal is to compute the entropy ideal computer accuracy to compare the accuracy, so that suffices.",
            "So how do I choose vector?",
            "I choose a term document matrix, so 8 English words.",
            "So I'd like to use English words, because there is a clear what the vector means.",
            "So basically the entries of this vector just the number of times this, how many times this would occur."
        ],
        [
            "In that in that in that in that course.",
            "So, for example, the word twist appear .4 * 4.4% of the time in that.",
            "4% of documents and the entropy is.",
            "Five and or the other things.",
            "So I choose all the other words so that we see a continuous day.",
            "We cover full range over sparsity and entropies."
        ],
        [
            "So we can.",
            "So now we can see if so, this is some people familiar with symmetric stable and projections, and so we can see that if you use symmetric projections and we measure the mean squared error, we can see as Delta close to 0.",
            "As Delta close to 0.",
            "Is magnified magnified because variance proportion one over Delta Square, so that therefore you actually is not doing doing too well at the buyer.",
            "The bias is smaller.",
            "That's why you can see the bias is smaller variance increases.",
            "So this is a very nice example of bias variance tradeoff there actually.",
            "And if you so usually what people do is they choose very large Delta very logic K so as not scale small enough, you're going to.",
            "You're going to compensate this variance, and if you watch carefully choose this offer.",
            "So maybe still OK if you're careful, very large K so you will find that when we do experiments K logic a lot easier to do and but we can see that.",
            "So if we do symmetric, it's not going to work well."
        ],
        [
            "And if you use mathematical compress counting but not using this talk, not using the estimated this type by using the estimate of a geometric mean, which is better because the variance proportional to 1D.",
            "However, we need Dollar Square so therefore we can see the variance of the estimator of the entropy still magnifies.",
            "As as Delta goes to 0, but it's much better compared to this figure."
        ],
        [
            "Much better, it's probably not good enough.",
            "We still had to do some variance bias trade off here and but, however, using this using the new estimator in this talk we can see that as not as Delta goes to zero.",
            "Small enough we can see a flat range, so that means the daughter doesn't matter anymore.",
            "That's because the biased bias is cancelled, but this does not increase, so therefore we get very good estimator.",
            "So this is actually providers.",
            "I think you provide a full solution to this problem.",
            "Which entropy used to use to used to be considered to be a very hard problem, but not really trivial problem.",
            "And.",
            "And in after this."
        ],
        [
            "So we can see this for all the words and will see very similar pattern.",
            "And if you do, symmetric projection is not going to work too well.",
            "If you use the geometric mean is not going to work too well either, is better, but if using this estimate in this talk is much better."
        ],
        [
            "As as very similar pattern."
        ],
        [
            "And."
        ],
        [
            "Yeah."
        ],
        [
            "Yeah.",
            "So, so now that the the one question is how to choose Delta and the sample size.",
            "OK, we know now in this talk we know that it's actually doesn't matter any much anymore, but before before the.",
            "Mention of this estimator people people actually worry about how small is Delta?",
            "So in the empirical study this just basically carefully choose Delta and using their logic eight they use about 20,000.",
            "And maybe even larger than 20,000 K. So the Delta.",
            "So this is doing pretty well, and there's a theory work which showed that if you want to survive to be small, you make some assumption and show that Delta is smaller than 10,000 -- 5.",
            "Doesn't seem to have too much, but ended minus five.",
            "If we can do that is going to work.",
            "And if you use the geometric mean, the the sample complexity is one of a new square in order achieved.",
            "In order to achieve a.",
            "And this last May in order achieve the additive error on you.",
            "So sample complexity one of New Square but divided by Delta because of our variance is only proportional to Delta.",
            "But with this new estimator, it's the the sample complexity.",
            "We can actually calculate almost exactly is 6 over new square, so you can look at the paper and see how do we calculate this tail bounds almost almost exactly and the highway.",
            "We notice that because Delta, even though it's small.",
            "At a sum of some associated condition, you can treat as constant is small by the constant.",
            "And so therefore you can play some play with it if it's small, but it's also constant if you actually get a big number like attenta, then log M addition instead of six.",
            "You can use that as A and also use the geometric mean estimator.",
            "So in that box paper and so to some people this is optimal because because Delta even the smallest constant, so it didn't help the practitioners.",
            "But if you view that as constant then it's fine.",
            "Yeah, so I don't quite get the logic, but because I'm practitioners but but I, but I think that's accepted.",
            "Concept so anyway, so that's."
        ],
        [
            "And that's the that's the sample size choice.",
            "And we also have the statistics optimality.",
            "So we compute the fish information, which is we can see the fish information is upper bounded by one of the other square and our estimator exactly achieve this bound.",
            "So it's optimal up to a constant.",
            "However, it's actually nearly 100% optimal if we can solve useful numerical."
        ],
        [
            "Value this integrals.",
            "It's almost 100% efficient.",
            "It's not exactly efficient.",
            "I will stop here, so any questions.",
            "OK, let's thank the speaker first.",
            "Can you estimate cross entropy of two streams?",
            "That's something I'm and Trump is working on and.",
            "But not exactly cross entropy, but we know the two strings and things related to two streams yet, but that's a.",
            "In that case the we cannot assume that element to be nonnegative anymore because they actually relate to the differences and we know how to do it more or less, and I was about to write a paper, but since I mean I'm traveling so I couldn't finish the paper otherwise, I will tell you, yeah, but basically I think more less.",
            "I know how to do it.",
            "Yeah, I will probably post paper, maybe in half a year or seven month so you concentrate completely on the case.",
            "Alpha is 1 or you want to approach that is then you get the Shannon entropy.",
            "I was wondering, but maybe sometimes you would want to do this for Alpha different from 1."
        ],
        [
            "So do you have any results for that?",
            "Well, we know we know the results far he offering two, but after you choose a random projection people care about and for skewed distributions skewness only matters when offers between zero and two.",
            "So when are equal to two is skewness doesn't matter, it's just normal, it's just symmetric, so so.",
            "But this basically gives the full range of if you consider the geometric mean, so this gives the full range of us, however.",
            "For offer less than one.",
            "Actually."
        ],
        [
            "Actually you can see here for all for less than one, this estimator is works full for the full range of offer less than one.",
            "You actually OK?",
            "Yeah, it's in the picture.",
            "Yeah, you actually need to care about this because offer equal to 0 is important is that matches the number of distinct elements and for for entropy.",
            "Actually you need to correct some people actually use empirical entropy not good enough, they actually with some corrections using the using the user.",
            "This one.",
            "So that's really important and it's actually for this point.",
            "It's 100% efficient 4.5 we know it's 100% efficient and it's actually in the middle is probably also 100% efficient.",
            "But small.",
            "Percent there.",
            "There is indeed a range here."
        ],
        [
            "That went on for larger than well.",
            "What is the best estimator?",
            "And I haven't done any.",
            "We can do pretty well, but maybe we can improve here.",
            "Yeah, OK, I was interested in Alpha smaller than one, especially 0.",
            "So it's yeah, it's a.",
            "It's a software you can just use your estimator, which yeah, which it becomes the same estimator as a as in the.",
            "We become the same estimator as in the stock paper because when Alpha go to zero, it becomes symmetrical.",
            "The skewness doesn't matter anymore, it's the same as basically the same as a symmetric.",
            "Alright, so let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the problem statement.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is that assume that we have very non vector of D?",
                    "label": 0
                },
                {
                    "sent": "You could be larger than than two 64 in the network application for example.",
                    "label": 0
                },
                {
                    "sent": "And she can be icy papers over the two to the to the, to the 112 to 128, and so it's very long vector, so the goal is to count the frequency moment.",
                    "label": 1
                },
                {
                    "sent": "So A is an element that the value so you raised to office power than some of the more.",
                    "label": 0
                },
                {
                    "sent": "So it's a if you could do this is second moment after one just the sum.",
                    "label": 1
                },
                {
                    "sent": "So an in particular we're interested in the Shannon entropy.",
                    "label": 0
                },
                {
                    "sent": "This is the empirical formula Shannon entropy.",
                    "label": 0
                },
                {
                    "sent": "So the task is actually trivial, because how hard it is just.",
                    "label": 0
                },
                {
                    "sent": "Accumulate all those numbers right?",
                    "label": 0
                },
                {
                    "sent": "And it's a trivial if data static.",
                    "label": 0
                },
                {
                    "sent": "However, this study will consider we consider streaming data the data string case.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the data stream.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "It's very common in real life.",
                    "label": 0
                },
                {
                    "sent": "Many real life applications.",
                    "label": 0
                },
                {
                    "sent": "So, so the basic model is that assuming that empty there's incoming elements.",
                    "label": 0
                },
                {
                    "sent": "The first little I indicates the location.",
                    "label": 0
                },
                {
                    "sent": "The capital I is the increment.",
                    "label": 0
                },
                {
                    "sent": "So according to this linear updating rule, so a particular time T one of the elements of the vector is updated.",
                    "label": 0
                },
                {
                    "sent": "So this is the general very popular model called 10 Stein data Stream model.",
                    "label": 0
                },
                {
                    "sent": "And now the goal is to count the frequency moments.",
                    "label": 0
                },
                {
                    "sent": "And note that the A the the.",
                    "label": 0
                },
                {
                    "sent": "Elements.",
                    "label": 0
                },
                {
                    "sent": "In this vector, the value is always larger than larger than 0 equal to 0 in most of the cases.",
                    "label": 0
                },
                {
                    "sent": "In most of the applications.",
                    "label": 0
                },
                {
                    "sent": "Even though the increments can be negative, but usually the A is a positive or negative.",
                    "label": 0
                },
                {
                    "sent": "So the observation that the nothing to one this is a trivial task.",
                    "label": 0
                },
                {
                    "sent": "Because we just accumulate all the increments.",
                    "label": 0
                },
                {
                    "sent": "It's a trivial task.",
                    "label": 0
                },
                {
                    "sent": "And a Sunday night.",
                    "label": 0
                },
                {
                    "sent": "But when after not equal to 1, that has become very hard because we have the we have to count.",
                    "label": 0
                },
                {
                    "sent": "We had to use the counters, but these very large number we the reason we need to use the counter is because the Alpha this is nonlinear operation.",
                    "label": 0
                },
                {
                    "sent": "So in order to compute something raised to offer power exactly, you really have to use a D counters, otherwise they cannot do it.",
                    "label": 0
                },
                {
                    "sent": "But when after one this is actually trivial, just need to accumulate all this other changes.",
                    "label": 0
                },
                {
                    "sent": "So this is interesting and I notice there's like in 2007.",
                    "label": 0
                },
                {
                    "sent": "So I started working on it because there must be something something must be wrong, right?",
                    "label": 0
                },
                {
                    "sent": "Let me something must be doable here.",
                    "label": 0
                },
                {
                    "sent": "Because whenever you want we know why it is difficult, but whatever you want is trivial.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So I start work and this is when I move to move to Cornell.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I will tell you the results.",
                    "label": 0
                },
                {
                    "sent": "But now we just give a little more cartoon illustration of the test and data stream model.",
                    "label": 1
                },
                {
                    "sent": "This is a time T. Suppose this is the IP or IP pair or just some location.",
                    "label": 0
                },
                {
                    "sent": "I want to D so all the elements are zero at 2 to one, but I took two material to one for example user like 3 order 10 books it become.",
                    "label": 0
                },
                {
                    "sent": "Hands number become ten and attempted to make 2.",
                    "label": 0
                },
                {
                    "sent": "For example user one order five books.",
                    "label": 0
                },
                {
                    "sent": "So now now now become five.",
                    "label": 1
                },
                {
                    "sent": "I take 3 to order the user the user 3 left he regretted.",
                    "label": 0
                },
                {
                    "sent": "He cancel.",
                    "label": 0
                },
                {
                    "sent": "Eight books now become two and notice that this number even though this increments can be negative.",
                    "label": 0
                },
                {
                    "sent": "Usually this value can only be larger than now equal to 0.",
                    "label": 0
                },
                {
                    "sent": "This is because if you want to cancel 18 books you know you cannot do it because you didn't order 18 books.",
                    "label": 0
                },
                {
                    "sent": "So you can only do so that's that.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's the model, and so this is very natural that in many network applications because.",
                    "label": 0
                },
                {
                    "sent": "And the network traffic and natural high rate data streams and the D can be the size of IP addresses and port numbers, and more popularly is actually the the compilation of the orange and destinations is appear, so that's why D can be particularly large.",
                    "label": 1
                },
                {
                    "sent": "And and so the elements value represents the total bytes of packages up to time T's address.",
                    "label": 1
                },
                {
                    "sent": "I so edge will be the shanty views network traffic distributions.",
                    "label": 1
                },
                {
                    "sent": "So anomaly events such as network failure and canal of service attacks usually change the distribution of traffic which maybe capture are affected by the entropy measurements.",
                    "label": 0
                },
                {
                    "sent": "And it's critical to be able to measure or even approximate their entropies.",
                    "label": 0
                },
                {
                    "sent": "An other summary statistic in real life using small.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm a little bit more details on this denial over distributed denial of service attack is a representative example of anomalies, and this attack attempts to make a computer is unavailable to intended users is by forcing user to result computer or exhaust resource resource source for service hosting sites, for example, had custom maybe just saturate the victim victim machines by sending many external communication requests and the tactical changes at Target site such as banks.",
                    "label": 1
                },
                {
                    "sent": "The current government and military, so it's a serious thing and so therefore.",
                    "label": 1
                },
                {
                    "sent": "But however, the attack normally changes the statistic.",
                    "label": 1
                },
                {
                    "sent": "Tribulations of traffic which could be very reliable captured by.",
                    "label": 1
                },
                {
                    "sent": "And by the abnormal variations in the match.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Shannon, entropy.",
                    "label": 0
                },
                {
                    "sent": "So here's a here's the examples.",
                    "label": 0
                },
                {
                    "sent": "I copy this copy this figure from a from a DARPA conference and apologize for the low quality because I asked the authors for the original plots and never getting replies.",
                    "label": 0
                },
                {
                    "sent": "And so basically you can view this.",
                    "label": 0
                },
                {
                    "sent": "So this is Shannon entropy measurements for network and you can view this access at the time is just a representation of time.",
                    "label": 0
                },
                {
                    "sent": "So at the time when this attack occur.",
                    "label": 0
                },
                {
                    "sent": "We can see that the sound change of Shannon entropy.",
                    "label": 0
                },
                {
                    "sent": "So if we can catch this moment and do something, I don't know what to do yet, but it's not my measure.",
                    "label": 0
                },
                {
                    "sent": "But if we can capture that at that moment, so so may be very useful.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe people in the audience know how to do it, but.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so now the outline of our approach is that the goal is developer efficient algorithm.",
                    "label": 1
                },
                {
                    "sent": "NASA made the frequency moment and Shannon entropy, so this is again this definition of frequency moment.",
                    "label": 0
                },
                {
                    "sent": "This definition of Shannon entropy and it's actually it's interesting that we know that at least people who develop this, Randy entropy Attalus entropy, they know that Shannon entropy can be well approximated by letting the Alpha which is exponents go go to one and using either entropy or thoughtless entropy.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is that since the F1, the first moment is trivial, we just count exactly so we can able to estimate the first moment with awful close to one.",
                    "label": 0
                },
                {
                    "sent": "Close enough.",
                    "label": 0
                },
                {
                    "sent": "Maybe we get very good estimate the entropies.",
                    "label": 0
                },
                {
                    "sent": "So this is very good idea.",
                    "label": 0
                },
                {
                    "sent": "However, the problem is here is divided one 1 /, 1 minus Alpha.",
                    "label": 0
                },
                {
                    "sent": "We know that the variance estimators F you have various, then the variance is reflected by the square of these coefficients, right?",
                    "label": 0
                },
                {
                    "sent": "So you get a variance with the constant with coefficients proportional to the 1 -- 1 / 1 minus Alpha squared.",
                    "label": 0
                },
                {
                    "sent": "So this is a huge.",
                    "label": 0
                },
                {
                    "sent": "So you say, well I get a good estimate of Alpha, but somehow you various blows up, it's not going to do good.",
                    "label": 0
                },
                {
                    "sent": "So therefore the estimate has to be driven accurate.",
                    "label": 0
                },
                {
                    "sent": "So it's not an echo of variance has to be.",
                    "label": 0
                },
                {
                    "sent": "Usually if you say there is a constant, but now you various has to be proportional to the Delta square, so it has to be generally accurate.",
                    "label": 0
                },
                {
                    "sent": "So that's a challenge.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so we we found the solution.",
                    "label": 0
                },
                {
                    "sent": "The solutions we do the metrical compress counting, which I call I could name in 2007.",
                    "label": 0
                },
                {
                    "sent": "Then I posted my first paper in archive in 2008 and accomplished counting and adapts.",
                    "label": 0
                },
                {
                    "sent": "We should basically called maximum skill stable random projections.",
                    "label": 1
                },
                {
                    "sent": "We know random projections we probably made everybody know random projection which uses symmetric is normal distributions and.",
                    "label": 0
                },
                {
                    "sent": "Many people also knows stable random projections to instead of his normal users table for you.",
                    "label": 0
                },
                {
                    "sent": "For example koshi.",
                    "label": 0
                },
                {
                    "sent": "But however, they also metric.",
                    "label": 0
                },
                {
                    "sent": "So now the trick is that using skewed distribution is still symmetric.",
                    "label": 0
                },
                {
                    "sent": "May be wondering why skill?",
                    "label": 0
                },
                {
                    "sent": "Well, the data skill that are positive.",
                    "label": 0
                },
                {
                    "sent": "So that's the intuition.",
                    "label": 0
                },
                {
                    "sent": "But we need to workout the map so we use in school.",
                    "label": 0
                },
                {
                    "sent": "But how school maximum skill?",
                    "label": 0
                },
                {
                    "sent": "So we suppose is a is a.",
                    "label": 0
                },
                {
                    "sent": "Is that string?",
                    "label": 0
                },
                {
                    "sent": "I would know that is vector and we multiply conceptually multiplied with the matrix R and the right size.",
                    "label": 1
                },
                {
                    "sent": "So now we get a vector new vector which is only size 99.",
                    "label": 1
                },
                {
                    "sent": "KK is small.",
                    "label": 1
                },
                {
                    "sent": "So here are the choices of the of the end of the entries are just is off a stable beta skill.",
                    "label": 0
                },
                {
                    "sent": "Actually better into one always because we show that in the early in the 2000 seven 2008 paper will show.",
                    "label": 0
                },
                {
                    "sent": "You need to use a maximum skill.",
                    "label": 0
                },
                {
                    "sent": "So so now we choose offer less than one.",
                    "label": 0
                },
                {
                    "sent": "And using this estimator.",
                    "label": 0
                },
                {
                    "sent": "So we do the projections we use choose choose Alpha computer Delta, will this estimator?",
                    "label": 0
                },
                {
                    "sent": "It looks strange at beginning if you ask me how the driver the honest answer is actually yes there yeah, so it's hard to guess like that as well.",
                    "label": 0
                },
                {
                    "sent": "If you start looking at this 2007 or 2011 so it look at every day but not every day.",
                    "label": 0
                },
                {
                    "sent": "But some of you have many, many guesses.",
                    "label": 0
                },
                {
                    "sent": "Some of the guests were actually work and once you get the answer the rest of work actually standard.",
                    "label": 0
                },
                {
                    "sent": "So the difficulties.",
                    "label": 0
                },
                {
                    "sent": "Guess the answer.",
                    "label": 0
                },
                {
                    "sent": "So once again answer.",
                    "label": 0
                },
                {
                    "sent": "So that is almost non bias and variance has a nice hazard.",
                    "label": 0
                },
                {
                    "sent": "Desired property is Delta squared.",
                    "label": 0
                },
                {
                    "sent": "And one important thing I need to clarify that the R is actually the K times larger as the than the original matrix, so that some people maybe think that's ridiculous because you say is large enough, but you have to something this K times are large.",
                    "label": 0
                },
                {
                    "sent": "So here's the trick is the is actually it's not random pseudorandom.",
                    "label": 0
                },
                {
                    "sent": "So it's never materialized in inventor projection.",
                    "label": 0
                },
                {
                    "sent": "In machine learning we usually we do not care much because usually the metric is small enough.",
                    "label": 0
                },
                {
                    "sent": "We don't want the random nature small enough do not care about that, but here the random matrix big enough we actually have to really worry about it.",
                    "label": 0
                },
                {
                    "sent": "We had to use pseudorandom numbers, so that means we just I.",
                    "label": 1
                },
                {
                    "sent": "Details next slice.",
                    "label": 0
                },
                {
                    "sent": "So basically we just regenerate the entries as needed because the pseudorandom is actually deterministic.",
                    "label": 0
                },
                {
                    "sent": "Once the random random random seeds or the parameters are selected.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so you might be wondering why why this whole thing works well for us over the linear is a linear stable and just learning projection works.",
                    "label": 1
                },
                {
                    "sent": "It works because the streaming model is linear model.",
                    "label": 0
                },
                {
                    "sent": "And our projection is linear, so.",
                    "label": 0
                },
                {
                    "sent": "So so basically, so as long as you have a new entries, you just update the update, the projection projection values, because it's everything is linear here, so therefore you only need to regenerate entries or R. According to this index, IT&J.",
                    "label": 1
                },
                {
                    "sent": "And we know how to generate the random numbers RA X + B minus Mother P. That's the simplest one, which doesn't work too well, but you will see that you just user users.",
                    "label": 0
                },
                {
                    "sent": "Have a coding coding value you have choose random A&B, model P, and then you actually get a pseudorandom number which is reasonably well well approximated.",
                    "label": 0
                },
                {
                    "sent": "And you can do better than that by using for universal hashing or other better hashing schemes, but in general that's that's a standard trick, and I'm not particularly interested, but I just.",
                    "label": 0
                },
                {
                    "sent": "I just assume that we're able to regenerate this number and on the fly so you have more questions.",
                    "label": 0
                },
                {
                    "sent": "We can discuss offline, so this is a solved linear projection part and the stable random projection part.",
                    "label": 1
                },
                {
                    "sent": "Is this, so if the entries are Alpha, beta, stable, Alpha, beta and one parameters beta one always so bad because this is linear projection, so by the property of the stable distributions and usually people familiar with the normal or symmetric but for skill is the same, you get the same Alpha and beta.",
                    "label": 0
                },
                {
                    "sent": "Now this index parameter, I mean now the scale parameter.",
                    "label": 0
                },
                {
                    "sent": "Instead of why you become exactly the frequency moments we care about, so that's the trick.",
                    "label": 0
                },
                {
                    "sent": "Now it becomes statistical problem or change from.",
                    "label": 0
                },
                {
                    "sent": "Yeah, now becomes a problem so we can do some estimation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work on it.",
                    "label": 0
                },
                {
                    "sent": "So, so I've been working on this for quite many years, and so I was pretty good at doing the geometric mean harmonic mean estimator.",
                    "label": 1
                },
                {
                    "sent": "So that's my my my.",
                    "label": 0
                },
                {
                    "sent": "This guess so when I do the geometric mean and workout all the variances, I notice that it's the variance is proportional to 1 minus Alpha, so that people find the property in 2009 even though that didn't work in 2007 and and.",
                    "label": 0
                },
                {
                    "sent": "And at that time was very happy where because you get 1 minus Alpha, so that's very very very good.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can see that in the previous my previous work, if we use a symmetric instead of skill.",
                    "label": 0
                },
                {
                    "sent": "And the variance is actually proportional tool or the other things that buys with two plus Alpha squared, so it's not goes to zero.",
                    "label": 0
                },
                {
                    "sent": "The various has to be equal to 0 at rate of 1 minus Alpha square and buys actually is a constant.",
                    "label": 0
                },
                {
                    "sent": "So therefore this line here.",
                    "label": 0
                },
                {
                    "sent": "And if we do the geometric mean you get this line.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "This is much better so that the moment is dream.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Happy and but also work on this harmonic mean estimator slightly better, but it's still one mass offer.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can see this, but obviously better.",
                    "label": 0
                },
                {
                    "sent": "But when you go to here actually not as getting so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so, so those are very nice except.",
                    "label": 0
                },
                {
                    "sent": "Now I realize we really need off 1 minus Alpha Square instead of one minute off a, so we need a better than geometric mean, harmonic mean and the reasons is actually clear because geometric mean is essentially treat every element of the same every sample, more or less the same.",
                    "label": 0
                },
                {
                    "sent": "So we have a skewed distribution, but we're using the next symmetric estimator, so it's not going to work too well.",
                    "label": 0
                },
                {
                    "sent": "So now retrospectively is clear why this doesn't work well, even though it's.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Better than before.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so therefore we use this estimator.",
                    "label": 0
                },
                {
                    "sent": "It's it's it magnifies the smallest elements.",
                    "label": 0
                },
                {
                    "sent": "If you see that you actually magnified smallest elements the most, so it's almost cost users minimum value, but not exactly the minimum, so it's close to the minimum.",
                    "label": 0
                },
                {
                    "sent": "And actually had a report and couple years back using the minimal I can prove something similar, but not as good.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now do an empirical study.",
                    "label": 1
                },
                {
                    "sent": "So one of the reasons we like Co is because they encourage empirical results.",
                    "label": 0
                },
                {
                    "sent": "I remember when I start writing theory papers, material friends told me that don't include any empirical results because that's going to only going to hurt and that was that was designed to me and but the experience told me that the truth in many places, but it's not true for code so so I'm and I always like like like like the empirical studies.",
                    "label": 0
                },
                {
                    "sent": "Really, the exciting part to me, and so because we really care about the.",
                    "label": 0
                },
                {
                    "sent": "The accuracy, so I don't have streaming data, but if we only care about accuracy, we actually actually actually I can do any user any vector and just compute the entropy and moments.",
                    "label": 0
                },
                {
                    "sent": "So I don't use streaming data by using static data, but because the goal is to compute the entropy ideal computer accuracy to compare the accuracy, so that suffices.",
                    "label": 0
                },
                {
                    "sent": "So how do I choose vector?",
                    "label": 1
                },
                {
                    "sent": "I choose a term document matrix, so 8 English words.",
                    "label": 0
                },
                {
                    "sent": "So I'd like to use English words, because there is a clear what the vector means.",
                    "label": 0
                },
                {
                    "sent": "So basically the entries of this vector just the number of times this, how many times this would occur.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In that in that in that in that course.",
                    "label": 0
                },
                {
                    "sent": "So, for example, the word twist appear .4 * 4.4% of the time in that.",
                    "label": 0
                },
                {
                    "sent": "4% of documents and the entropy is.",
                    "label": 0
                },
                {
                    "sent": "Five and or the other things.",
                    "label": 0
                },
                {
                    "sent": "So I choose all the other words so that we see a continuous day.",
                    "label": 0
                },
                {
                    "sent": "We cover full range over sparsity and entropies.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "So now we can see if so, this is some people familiar with symmetric stable and projections, and so we can see that if you use symmetric projections and we measure the mean squared error, we can see as Delta close to 0.",
                    "label": 0
                },
                {
                    "sent": "As Delta close to 0.",
                    "label": 0
                },
                {
                    "sent": "Is magnified magnified because variance proportion one over Delta Square, so that therefore you actually is not doing doing too well at the buyer.",
                    "label": 0
                },
                {
                    "sent": "The bias is smaller.",
                    "label": 0
                },
                {
                    "sent": "That's why you can see the bias is smaller variance increases.",
                    "label": 0
                },
                {
                    "sent": "So this is a very nice example of bias variance tradeoff there actually.",
                    "label": 0
                },
                {
                    "sent": "And if you so usually what people do is they choose very large Delta very logic K so as not scale small enough, you're going to.",
                    "label": 0
                },
                {
                    "sent": "You're going to compensate this variance, and if you watch carefully choose this offer.",
                    "label": 0
                },
                {
                    "sent": "So maybe still OK if you're careful, very large K so you will find that when we do experiments K logic a lot easier to do and but we can see that.",
                    "label": 0
                },
                {
                    "sent": "So if we do symmetric, it's not going to work well.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you use mathematical compress counting but not using this talk, not using the estimated this type by using the estimate of a geometric mean, which is better because the variance proportional to 1D.",
                    "label": 0
                },
                {
                    "sent": "However, we need Dollar Square so therefore we can see the variance of the estimator of the entropy still magnifies.",
                    "label": 0
                },
                {
                    "sent": "As as Delta goes to 0, but it's much better compared to this figure.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Much better, it's probably not good enough.",
                    "label": 0
                },
                {
                    "sent": "We still had to do some variance bias trade off here and but, however, using this using the new estimator in this talk we can see that as not as Delta goes to zero.",
                    "label": 0
                },
                {
                    "sent": "Small enough we can see a flat range, so that means the daughter doesn't matter anymore.",
                    "label": 0
                },
                {
                    "sent": "That's because the biased bias is cancelled, but this does not increase, so therefore we get very good estimator.",
                    "label": 0
                },
                {
                    "sent": "So this is actually providers.",
                    "label": 0
                },
                {
                    "sent": "I think you provide a full solution to this problem.",
                    "label": 0
                },
                {
                    "sent": "Which entropy used to use to used to be considered to be a very hard problem, but not really trivial problem.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And in after this.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can see this for all the words and will see very similar pattern.",
                    "label": 0
                },
                {
                    "sent": "And if you do, symmetric projection is not going to work too well.",
                    "label": 0
                },
                {
                    "sent": "If you use the geometric mean is not going to work too well either, is better, but if using this estimate in this talk is much better.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As as very similar pattern.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So, so now that the the one question is how to choose Delta and the sample size.",
                    "label": 0
                },
                {
                    "sent": "OK, we know now in this talk we know that it's actually doesn't matter any much anymore, but before before the.",
                    "label": 0
                },
                {
                    "sent": "Mention of this estimator people people actually worry about how small is Delta?",
                    "label": 0
                },
                {
                    "sent": "So in the empirical study this just basically carefully choose Delta and using their logic eight they use about 20,000.",
                    "label": 0
                },
                {
                    "sent": "And maybe even larger than 20,000 K. So the Delta.",
                    "label": 0
                },
                {
                    "sent": "So this is doing pretty well, and there's a theory work which showed that if you want to survive to be small, you make some assumption and show that Delta is smaller than 10,000 -- 5.",
                    "label": 0
                },
                {
                    "sent": "Doesn't seem to have too much, but ended minus five.",
                    "label": 0
                },
                {
                    "sent": "If we can do that is going to work.",
                    "label": 0
                },
                {
                    "sent": "And if you use the geometric mean, the the sample complexity is one of a new square in order achieved.",
                    "label": 0
                },
                {
                    "sent": "In order to achieve a.",
                    "label": 0
                },
                {
                    "sent": "And this last May in order achieve the additive error on you.",
                    "label": 0
                },
                {
                    "sent": "So sample complexity one of New Square but divided by Delta because of our variance is only proportional to Delta.",
                    "label": 0
                },
                {
                    "sent": "But with this new estimator, it's the the sample complexity.",
                    "label": 0
                },
                {
                    "sent": "We can actually calculate almost exactly is 6 over new square, so you can look at the paper and see how do we calculate this tail bounds almost almost exactly and the highway.",
                    "label": 0
                },
                {
                    "sent": "We notice that because Delta, even though it's small.",
                    "label": 0
                },
                {
                    "sent": "At a sum of some associated condition, you can treat as constant is small by the constant.",
                    "label": 0
                },
                {
                    "sent": "And so therefore you can play some play with it if it's small, but it's also constant if you actually get a big number like attenta, then log M addition instead of six.",
                    "label": 0
                },
                {
                    "sent": "You can use that as A and also use the geometric mean estimator.",
                    "label": 0
                },
                {
                    "sent": "So in that box paper and so to some people this is optimal because because Delta even the smallest constant, so it didn't help the practitioners.",
                    "label": 0
                },
                {
                    "sent": "But if you view that as constant then it's fine.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I don't quite get the logic, but because I'm practitioners but but I, but I think that's accepted.",
                    "label": 0
                },
                {
                    "sent": "Concept so anyway, so that's.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's the that's the sample size choice.",
                    "label": 0
                },
                {
                    "sent": "And we also have the statistics optimality.",
                    "label": 0
                },
                {
                    "sent": "So we compute the fish information, which is we can see the fish information is upper bounded by one of the other square and our estimator exactly achieve this bound.",
                    "label": 1
                },
                {
                    "sent": "So it's optimal up to a constant.",
                    "label": 1
                },
                {
                    "sent": "However, it's actually nearly 100% optimal if we can solve useful numerical.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Value this integrals.",
                    "label": 0
                },
                {
                    "sent": "It's almost 100% efficient.",
                    "label": 0
                },
                {
                    "sent": "It's not exactly efficient.",
                    "label": 0
                },
                {
                    "sent": "I will stop here, so any questions.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank the speaker first.",
                    "label": 0
                },
                {
                    "sent": "Can you estimate cross entropy of two streams?",
                    "label": 0
                },
                {
                    "sent": "That's something I'm and Trump is working on and.",
                    "label": 0
                },
                {
                    "sent": "But not exactly cross entropy, but we know the two strings and things related to two streams yet, but that's a.",
                    "label": 0
                },
                {
                    "sent": "In that case the we cannot assume that element to be nonnegative anymore because they actually relate to the differences and we know how to do it more or less, and I was about to write a paper, but since I mean I'm traveling so I couldn't finish the paper otherwise, I will tell you, yeah, but basically I think more less.",
                    "label": 0
                },
                {
                    "sent": "I know how to do it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I will probably post paper, maybe in half a year or seven month so you concentrate completely on the case.",
                    "label": 0
                },
                {
                    "sent": "Alpha is 1 or you want to approach that is then you get the Shannon entropy.",
                    "label": 0
                },
                {
                    "sent": "I was wondering, but maybe sometimes you would want to do this for Alpha different from 1.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So do you have any results for that?",
                    "label": 0
                },
                {
                    "sent": "Well, we know we know the results far he offering two, but after you choose a random projection people care about and for skewed distributions skewness only matters when offers between zero and two.",
                    "label": 0
                },
                {
                    "sent": "So when are equal to two is skewness doesn't matter, it's just normal, it's just symmetric, so so.",
                    "label": 0
                },
                {
                    "sent": "But this basically gives the full range of if you consider the geometric mean, so this gives the full range of us, however.",
                    "label": 0
                },
                {
                    "sent": "For offer less than one.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually you can see here for all for less than one, this estimator is works full for the full range of offer less than one.",
                    "label": 0
                },
                {
                    "sent": "You actually OK?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's in the picture.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you actually need to care about this because offer equal to 0 is important is that matches the number of distinct elements and for for entropy.",
                    "label": 0
                },
                {
                    "sent": "Actually you need to correct some people actually use empirical entropy not good enough, they actually with some corrections using the using the user.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "So that's really important and it's actually for this point.",
                    "label": 0
                },
                {
                    "sent": "It's 100% efficient 4.5 we know it's 100% efficient and it's actually in the middle is probably also 100% efficient.",
                    "label": 0
                },
                {
                    "sent": "But small.",
                    "label": 0
                },
                {
                    "sent": "Percent there.",
                    "label": 0
                },
                {
                    "sent": "There is indeed a range here.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That went on for larger than well.",
                    "label": 0
                },
                {
                    "sent": "What is the best estimator?",
                    "label": 0
                },
                {
                    "sent": "And I haven't done any.",
                    "label": 0
                },
                {
                    "sent": "We can do pretty well, but maybe we can improve here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, I was interested in Alpha smaller than one, especially 0.",
                    "label": 0
                },
                {
                    "sent": "So it's yeah, it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a software you can just use your estimator, which yeah, which it becomes the same estimator as a as in the.",
                    "label": 0
                },
                {
                    "sent": "We become the same estimator as in the stock paper because when Alpha go to zero, it becomes symmetrical.",
                    "label": 0
                },
                {
                    "sent": "The skewness doesn't matter anymore, it's the same as basically the same as a symmetric.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}