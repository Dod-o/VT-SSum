{
    "id": "rmpmscmgcjwt5jabvb5m7jzafoqa36cy",
    "title": "Relative Entropy",
    "info": {
        "author": [
            "Sergio Verdu, Princeton University"
        ],
        "published": "Jan. 25, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/nips09_verdu_re/",
    "segmentation": [
        [
            "Sergio Verdu is the Eugene Higgins professor of electrical engineering at Princeton University, and he is one of the Premier information theorists of our time.",
            "Sergio is probably best known for his work on multiuser detection, which is fundamental and critical to modern wireless communications.",
            "He's made fundamental contributions to information theory in its fundamentals in terms of finding the capacity of several classes of important channels that are again critical in modern wireless communication systems, and he's also.",
            "Developed and uncovered.",
            "Some surprising and very cool connections between maximum or I'm sure.",
            "Mean square error estimation and mutual information, and I think he's going to tell us a little bit about some of that today.",
            "Sergio has too many awards and accolades to mention all of them and less.",
            "I took up his entire slot, but I'll mention just a few.",
            "He's a fellow of the IEEE.",
            "He received the IEEE third Millennium Metal in 2000, the Claude Shannon Award in 2007, and I think he was one of the youngest people ever to receive that honor, and he received the Richard Hamming Medal in 2008.",
            "And on a personal note, Sergio is from Barcelona, Spain, and he can probably find the best coffee in Princeton if you ask Sergio.",
            "Thank you again and take it away.",
            "Thank you Ron.",
            "Well, I'd like to find the best coffee in Vancouver.",
            "Then it's probably not too difficult, so it's a great pleasure to be here.",
            "Thank you for the invitation to address this audience.",
            "I had heard a lot of good things about this conference, but actually never had the opportunity to be here.",
            "So it's a special pleasure to give the first talk in the conference, so.",
            "What I am going to do here as you can tell from the title, is going to be a very wide audience.",
            "Talk thoroughly in the morning.",
            "We're not going to get into mathematics.",
            "It's going to be mainly a historical overview, touching on some of the big problems that relative entropy has found itself itself useful.",
            "Alright, so here's the definition to start with."
        ],
        [
            "The definition of relative entropy.",
            "You can tell immediately that this of course P&Q are probability distributions.",
            "You can tell immediately that P if P is equal to Q, then this relative entropy is equal to 0.",
            "You cannot tell immediately that if P is not equal to Q, that these relative entropy is actually going to be non 0.",
            "Cause this is an expectation here.",
            "But is the expectation of a quantity that can be either positive or negative?",
            "OK, so actually.",
            "Especially some mathematicians prefer."
        ],
        [
            "To use an alternative representation where you take the expectation with respect to the reference, measure the queue rather than the P, and this actually turns out to be perhaps more natural in some in some applications."
        ],
        [
            "Alright, so the most important cases are of course the discrete case and the continuous case.",
            "When both P&QI should say very important is that P&Q have to be distributions defined on the same.",
            "Set on the same measurable space.",
            "So if this space is finite or is countably infinite, then we have this representation for the relative entropy if.",
            "We are."
        ],
        [
            "In Euclidean space, and these are continuous distributions, then we have that representation in terms of the density functions.",
            "OK, so the reason why this is related to information theory is because there is a logarithm there and.",
            "And the reason why physicists like it is because it has units.",
            "Relative entropy.",
            "Has you know?"
        ],
        [
            "Which inherits from the log from the basis of the logarithm.",
            "And here I have listed a bit tongue in cheek.",
            "Some of the most.",
            "Famous units for the logarithm in information theory.",
            "OK, so."
        ],
        [
            "You know when some when something is used by many, many different people who don't necessarily talk to each other, then what happens is that it ends up having many names and this is this is a drawing from the 17th century with all the known names of God.",
            "So relative entropy is not quite the same, but.",
            "You know it comes close in the number of diff."
        ],
        [
            "Names that it has received over the years.",
            "I have listed here some of them, the ones that would feed on one slide that you can easily read.",
            "Now a lot of them have the word information.",
            "And some of them of course have cool black library will see that we'll see what cool black library have to do with this.",
            "But you can see already that there is a lot of flavor of decisions.",
            "Discrimination, weight of evidence and so on.",
            "And also there is this word divergance that happens also quite quite commonly.",
            "In fact, for most of my career, I used to call this divergance before I converted to relative entropy.",
            "I used to be very dogmatic.",
            "I refuse to call it relative entropy.",
            "Becausw Claude Shannon in 1948.",
            "In his paper he calls relative entropy something else.",
            "He says relative entropy, just a passing comment, then nobody else.",
            "Abided by that terminology, but he said the entropy divided by the logarithm of the alphabet size is the relative entropy.",
            "But anyway, then I converted not too long ago to the to the common Canon in information theory, and I call it relative entropy.",
            "OK, so the first appearance of."
        ],
        [
            "Of relative entropy is in this very important paper by world.",
            "In 1945 where he.",
            "Post and solve the problem of sequential hypothesis testing.",
            "The easier problem of non sequential hypothesis testing was only solved 10 years later, so he didn't actually gave it a name, but you can see in the denominator of this formula.",
            "This was the expected number of observations necessary for reaching a decision with a certain.",
            "Quality you can see that the nominator of that equation, what we find is indeed the relative entropy.",
            "Now, right?"
        ],
        [
            "About the same time.",
            "Jeffreys also very famous statistician came up with.",
            "Symmetrized version of relative entropy.",
            "Which is now commonly referred to as Jeffries Divergance.",
            "And we would expect that because we're kind of programmed to.",
            "Think that important important quantities should be symmetric, so we are programmed to think that perhaps this is more fundamental than the relative entropy, but in fact is known this is, this is rarely useful, whereas relative entropy in all its ugliness or asymmetric city turns out to be of enormous importance."
        ],
        [
            "Alright, so the first formal definition, and that's why it's very frequently referred to as the callback library.",
            "Divergents or distance?",
            "Happened in this paper in 1951.",
            "Cool black and libel are are calling them in information for discrimination.",
            "Alright, so the definition already immediately recognizes that what we're defining there is actually the expectation of a certain random variable.",
            "And you already see the word information there.",
            "And in fact, in the paper on information and sufficiency.",
            "So what was the motivation of cool wagon lilar to introduce this measure?",
            "OK, so here's the first part."
        ],
        [
            "I got from the introduction.",
            "Generalizes to the abstract case, Shannon's definition of information winners information is essentially the same as Shannon, although their motivation was different, and Shannon apparently has investigated the concept more completely.",
            "OK, so the last sentence is a big understatement.",
            "In fact, we'll out come back to this later, and I'll I'll tell you more in detail what was what.",
            "Was it that winner had to do with this?",
            "But in a nutshell.",
            "Shannon was interested in in what we say the operational role of these quantities.",
            "Setting up an engineering problem and then showing that this quantity as well as entropy, as well as mutual information, are the answers to those problems whereas.",
            "Where is Wiener?",
            "Did not say he only used one of these quantities and essentially said that he thought it would be an interesting exercise to actually optimize this quantity when the variance of the random variable is fixed.",
            "OK, so the."
        ],
        [
            "Most the most important special case of.",
            "Relative entropy is the mutual information.",
            "So as as we're going to comment later, the relative entropy is a great way to measure the similarity between two distributions and asymmetric way of.",
            "Of doing that and.",
            "We can apply that definition to the special case where the 1st.",
            "Distribution is a joint distribution and the second one is the product of the marginals.",
            "So if.",
            "If these random variables X&Y are independent, then of course we're going to get 0.",
            "Otherwise, we're going to get unknown negative.",
            "A negative number.",
            "OK, so this is extremely, extremely general way of defining of gauging the degree of dependence.",
            "Of course we all have grown up with correlation coefficient and so on, but for example that one only applies to real valued random variables.",
            "Here X&Y can be whatever you want, X can be discrete, Y can be continuous.",
            "Anything you want.",
            "You can, you can throw it inside this mutual information.",
            "Of course, you could also define it the other way.",
            "You could define the the relative entropy of the product of the marginals relative to the joint distribution, and that's what we call lautem information with mutual spelled backwards.",
            "And you know, it's not as famous as this one as mutual information, but you can still at least write one paper about it.",
            "Alright, so."
        ],
        [
            "The relative entropy is the mother of all information measures.",
            "Mutual information is a special case, entropies, special case of mutual information.",
            "Moreover, if you have finite alphabet, then you can see in the bottom equation here that the relative entropy of sorry of distribution with respect to the.",
            "Equally likely, distribution uniformly distributed on that alphabet.",
            "That's as the logarithm of the alphabet size minus the entropy.",
            "OK, so you can.",
            "You can define entropy in that special case through relative entropy and in the more general case just do it like this.",
            "You say the entropy of a random variable is as the mutual information of that random variable with itself.",
            "If you think about it, then it's going to be just the relative entropy of the joint distribution with the product of the two marginals, the joint distribution when X is equal to X."
        ],
        [
            "OK, so very important is.",
            "Conditional relative entropy, just like regular Shannon entropy, it's very important to also have the conditional entropy.",
            "Here we can do the same thing, and here's the definition in fact.",
            "Conditional relative entropy is more general than relative entropy, but.",
            "It's also the other way around, because this is just a special case of the other definition of definition of unconditional relative entropy.",
            "So let me explain this a little bit.",
            "What you have here is 3 objects, so the conditional relative entropy is defined for three objects rather than the relative entropies defined for two.",
            "Here we have conditional distribution Y given X.",
            "Another one Z.",
            "Given X&PX.",
            "So think of this.",
            "Of these conditional distributions, you know sometimes they are called transition kernels or Markov kernels.",
            "Things like that you can think of them even in the absence of having defined joint distributions between.",
            "X&Z an YNX, but in any case, if you define those joint distributions X&Y.",
            "The product of the conditional Y given X&X and the condition of C given X and the same PX.",
            "Then you can just look at the relative entropy between two those two joint distributions and that's what we call the conditional relative entropy.",
            "Another way of seeing this is simply say well.",
            "I'm going to find the the usual relative entropy for two distributions.",
            "The two distributions would be PY, given that X is equal to a given value MPC.",
            "Given that X is equal to a, now you do that for all values of A and you average over the distribution of PX.",
            "So that's what this.",
            "This notation means just like what you do for entropy or what you do for mutual information.",
            "OK, so in fact now."
        ],
        [
            "We can use this.",
            "These definition of conditional relative entropy to redefine mutual information and in fact this is actually more useful than the original definition.",
            "So it's actually quite a bit.",
            "You get a lot more mileage by looking at mutual information in this way then in the other way so.",
            "What what I have here is just a special case of conditional relative entropy, where the second the reference measure here is not a conditional measure.",
            "It turns out to be actually an unconditional measure, which is the one that I get simply by taking the marginal of the joint distribution that.",
            "Results from just.",
            "Multiplying these two alright, so I have I have.",
            "Shona a little figure here.",
            "Each of these dots of the grey dots represent a conditional distribution of Y.",
            "Given that X is takes a certain value.",
            "OK, now suppose that you have some weight on these values of X. Alright, so then you can define the center of gravity of that figure.",
            "The center of gravity of that figure.",
            "Depending on what the weight of the grey dots is, is what I have depicted here as the red dot.",
            "I don't know whether you can see the colors from far away, but that would be the PY, so that would be the.",
            "The average of these distribution, the mixture of these distribution according to the PX.",
            "OK, so now imagine that you would like to see essentially what is like the the average radius of this figure.",
            "The average distance from PY to each of those dots.",
            "Average according to the weights of the dots.",
            "And that's.",
            "Linear chain information.",
            "So the mutual information, in a way, is just a way to.",
            "Gates, how far apart these conditional distributions are.",
            "But doing that without proper weighting of those conditional distributions and the proper weighting is this PX.",
            "Alright, no, not immediately obvious from from this form, but mutual information is indeed symmetrical with X&Y, and that you can see from the original definition.",
            "OK, now this one of the immediate consequences of this form for mutual information.",
            "Is this for."
        ],
        [
            "Mila and this is extremely useful for me.",
            "This is the most useful representation and information theory, even though you don't really find it very often.",
            "But Q here is something auxiliary you choose.",
            "You choose what Q is, and if you're smart, inducing in choosing Q, you get a lot of mileage in many different problems.",
            "OK, so you may want to use these.",
            "Keep this in mind for the future.",
            "You have the advantage that not a lot of people use it so.",
            "It's always nice.",
            "Um?",
            "OK, so now we've been talking about distances and blah blah blah.",
            "Why?",
            "Why are we?"
        ],
        [
            "Saying that this is a distance well.",
            "Big cause, even though this is the expectation of a random variable that can be positive or negative, you can prove there is no negative an it's zero if and only if P is equal to Q. Alright, so to prove this inequality, the best way to do that is to use Jensen's inequality.",
            "In the alternative representation in the representation of relative entropy, where you take the expectation with respect to the reference measure, not with respect to P. You can also do it in the original one, but it's not as elegant.",
            "OK, so now this especially the physicist like to call it Gibbs inequality, and this is actually the most important inequality in information theory without a doubt.",
            "I'm.",
            "So you use Jensen's inequality once to prove this one and then.",
            "99% of the time you don't have to use Jensen's inequality again or use ugly lower bounds on the logarithm.",
            "You just use this and then as a bonus you'll get conditions under which whatever inequality you want to show actually holds with equality.",
            "OK, so."
        ],
        [
            "This is, as I say here, frequently known as Gibbs inequality.",
            "Now, Gibbs was kind of like the father of.",
            "Statistical mechanics as we know it, modern mathematical, statistical mechanics, thermodynamics and that's not what he showed.",
            "Actually, among other reasons, Becausw relative entropy had not been invented yet.",
            "This was 1901.",
            "This was well before it was actually well before Jensen.",
            "It was well before the definition of convex functions actually.",
            "Of course, convex functions have been defined before for, say, real valued functions and functions on Euclidean spaces, but the modern definition of convex functions in abstract linear spaces that is actually due to Jensen and that was after this beginning of the 20th century, even though it sounds like a very fundamental concept, is.",
            "Is about 100 years old now?",
            "Alright, so Gibbs actually what he showed an it's in that inequality for 47 at the bottom.",
            "Actually what he showed was a special case of the nonnegativity of mutual information.",
            "That's what that.",
            "Inequality is showing, so it's not quite the nonnegativity of relative entropy.",
            "OK, so let me show you now."
        ],
        [
            "Couple of Inequality's and they are very important and particularly in information theory.",
            "These are the bread and butter of the uses of relative entropy and.",
            "They really show why this is connected to the notion of information.",
            "So here the idea is that you have to think of this as a black box that has some kind of randomness inside.",
            "So we have a black box and then we drive that black box with.",
            "Random object that has distribution either PX or QX.",
            "And of course, those two distributions then induce an output distribution.",
            "So all this is saying is that.",
            "The relative entropy at the input between these two distributions is greater equal than the relative entropy at the output with equality.",
            "When will you have equality?",
            "Well, the special case that you will have equality is when here what you have is just a one to one transformation.",
            "Alright, I want to one transformation is not going to change information, but more more importantly or more generally, you're going to have equality.",
            "If what you produce here is a sufficient statistic for these pair of distributions, OK, so immediately we see there this notion of information of preserving information is essentially equivalent to preserving relative entropy.",
            "No."
        ],
        [
            "Now this one is kind of dual here.",
            "What we have is 2 black boxes, but only one input distribution.",
            "So.",
            "Depending on the black box, we have a different output distribution and what we're saying is that the relative entropy conditioned on knowing the input is greater than the relative entropy.",
            "Not conditioned on knowing the input so.",
            "In other words, he's always better to know than not to know.",
            "At least for these purposes.",
            "OK, so this is really all there is.",
            "Here is the convexity.",
            "The fact then?",
            "A relative entropy is convex in the pair of distributions P&Q.",
            "Now."
        ],
        [
            "Another very important property is the chain rule, and no matter how many random variables you have, you have the joint distribution of several random variables.",
            "But you do have to have the same number on both sides, unlike mutual information where you can have anything you want on both sides.",
            "Here, remember that you have to have distributions that are defined on the same set, so you can do this one at a time.",
            "No matter how involved these joint distributions are, you can take the relative entropy, but now the relative entropy is conditioned, and that's why conditioning is so important when you deal with any of these information measures.",
            "So you take one condition on all the others that you have already taken.",
            "OK, so now."
        ],
        [
            "Let me go back to that intriguing comment saying that.",
            "The Cool Rock library divergance or the relative entropy generalizes channels.",
            "Definition of information and winners information.",
            "Now let me just point out now that I read these Shannon's definition of information, actually Shannon.",
            "Never mentioned that word in his paper.",
            "In fact, he did mention information theory.",
            "In one place in his paper, but he never says anything about.",
            "These being a measure of information, or he never even defined mutual information in his paper for one matter for that matter."
        ],
        [
            "OK, So what?",
            "Both defined winner and Shannon who we have here in this picture.",
            "In Shannon, define the discrete entropy and also this what we call now differential entropy, although he also called entropy and winner also define the differential entropy about the same time.",
            "Also in 1948 in a book that became a big bestseller called Cybernetics.",
            "Um?",
            "And essentially winner the only the only reason he gave for defining it that way was that OK von Neumann had told him that that."
        ],
        [
            "Nothing was a good thing to look at.",
            "Now this is from an excerpt from a paper he wrote later the Shannon Wiener definition of Quantity of information for it belongs to the two of us equally.",
            "Now.",
            "Shadow"
        ],
        [
            "It was a little bit more circumstance circumspect about this definition and then the mathematician that actually realized the first mathematician who realize that.",
            "What Shannon had done was really revolutionary was Andre Kolmogorov in the Soviet Union."
        ],
        [
            "And.",
            "And he he got it right.",
            "It has no physical interpretation.",
            "So the first sin that one makes when when you introduce this quantity is to take the logarithm of a probability density function.",
            "And that's a big no no.",
            "Alright, so when we took logarithms up to now, we always took logarithms of quantities that don't have any units, so to speak.",
            "OK, they were ratios of probabilities ratios of PDFs or rather nicoulin derivatives in the most general case, so you always have to think of the probability density functions as being one over.",
            "Meters or seconds or whatever your.",
            "Your underlying Euclidian space means, so this is the kind of integral that should be declared illegal, and no wonder it gives leads to a lot of trouble to many people that tend to use it thinking that it satisfies the same properties as entropy.",
            "But then they run into trouble.",
            "So why has this been so important in information theory?",
            "If you take the leading textbook in information theory by covering Thomas, it devotes a whole chapter to this.",
            "Well, essentially becausw relative entropy took a long time to get into information theory, even though it was defined by Kullback Leibler only three years after channel.",
            "It's been extremely slow in getting into the toolbox of the information theorist, so.",
            "That void essentially was taken by this differential entropy."
        ],
        [
            "Alright, and the main reason is this formula.",
            "The differential entropy.",
            "Actually, if you take the relative entropy of a given distribution with given mean invariants, and you take the relative entropy with respect to the Gaussian distribution with the same mean and the same variance, that of course is going to give you a number that will gauge how non normal that distribution is and that is you can write as a constant minus the differential entropy of that PX.",
            "Alright, so this is really why.",
            "The reason why this differential entropy now is important, 'cause this is important and of course.",
            "It also says then and this is what winner showed.",
            "At the same time as Shannon, that among all distributions then have the same variance.",
            "The Gaussian is the one that has the largest differential entropy."
        ],
        [
            "And that also.",
            "Explains the rationale for the maximum entropy principle that, again, is quite.",
            "Quite common in applications where.",
            "OK, we don't have enough information to say what our distribution is, so we have some.",
            "Information maybe about the second moment of a distribution from observations, and then we would like to actually from their bootstrap a distribution.",
            "So a typical thing to do is, we say, well, let's see what is the maximum entropy or the maximum differential entropy.",
            "To be more precise?",
            "Alright, so you can write that you can write that entropy as minus the relative entropy of the distribution with respect to the Gaussian distribution that has the same 2nd order statistics and then the Gaussian in that same Gaussian distribution with respect to a Gaussian distribution.",
            "That's what I mean by this.",
            "A Gaussian distribution that is IID.",
            "OK, so.",
            "In the, in the absence of any other knowledge, then if all you know are they are the 2nd order statistics, then by maximising this with respect to PX, what you're doing is you're making this equal to 0.",
            "So you're making the PX equal to the Gaussian that has.",
            "The same 2nd 2nd order statistics OK, and then what?",
            "You're what you're left with is with the the relative entropy of the Gaussian with a given.",
            "2nd order statistics in the ID.",
            "So in other words, what this is doing is measuring.",
            "Or saying well, I would like to view reality in the closest way to the simplest possible explanation.",
            "The simplest possible explanation being something being iid Gaussian an my observations.",
            "What is the closest explanation of my observations to that pure reality?",
            "And that's the maximum entropy principle."
        ],
        [
            "Now another very interesting use for non Gaussian Ness.",
            "Is essentially the central limit theorem, but the non asymptotic version of the Central Limit theorem.",
            "So what do I mean by that?",
            "Well?",
            "You take any probability density function, you convolve it with itself and you normalize it so that you keep the variance the same that you started with looks more rounded.",
            "You do it again.",
            "Looks even more rounded, and then you know what's going to happen.",
            "It's going to become normal.",
            "So Shannon, actually 1948 showed.",
            "Then when you do this process.",
            "Once, so you take a PDF and you convolve it with itself then.",
            "You turn it indeed into a more Gaussian looking random variable, and when I say more Gaussian looking random variable, he did it through the differential entropy and that's what he called the entropy power inequality.",
            "Actually he did improve the entropy power inequality that took quite a bit of effort, about 10 years later.",
            "Now for a long time, even though you know that it's a very simple thing to conjecture for a long time it was an open problem to show that at each step that non Gaussian is that relative entropy with respect to the Gaussian distribution actually is monotonically decreasing.",
            "And of course by doing this then you can prove the central limit theorem.",
            "So it's like a strong form.",
            "Of the central Limit Theorem provided of course that you start with a PDF if you don't start with a PDF, then at each step you have a discrete distribution and then this is always Infinity.",
            "So discrete distribution then is going to have infinite relative entropy with respect to the Gaussian, even though of course we know that the characteristic functions are going to converge.",
            "OK, so this was actually finally shown in two of the force in functional analysis in 2004 and then we came up with a much simpler proof.",
            "Based on the connection between information theory and estimation theory that we will see later in some detail.",
            "OK, So what you're doing here?",
            "What you're doing here in the.",
            "In the world of distributions with a given variance and two sided distributions, you could think of doing this.",
            "In other words, you could think of doing this, say for example in the world of distributions that leave on the positive real line and have a given mean.",
            "And that's what I do here.",
            "Alright, so."
        ],
        [
            "So you take the relative entropy of PX with respect to.",
            "An exponential distribution with a certain mean well, that's a constant that depends on that mean minus the differential entropy of that distribution.",
            "OK, So what would be a good name for that before we call that thing non Gaussian Ness, we could also call it a normality, but since this one is the memoryless distribution, is the only one that has the memoryless property we could call this the memory over distribution?",
            "This and then you could also think of the maximum entropy principle for distributions that live on the real line and have given mean, and you can do this with absolute values absolute first moment, any moment you want.",
            "OK, so now this."
        ],
        [
            "Is another property of relative entropy is just a variational representation that say, says that if you give yourself the freedom to choose any R here where this expectation is still with respect to P, then the maximum you get is actually by choosing R = P and this is actually quite easy to show, but it's really.",
            "Is really equivalent to a famous result in large deviations.",
            "I won't get into the details, but.",
            "Relative entropy in large deviations takes of course a very important role, and is because of the fact that the Legendre transform of accumulative generating functional is in fact that relative entropy, so that this is just an optimization problem, and that this is all there is to it to that optimization problem."
        ],
        [
            "Now.",
            "Another very interesting, intriguing feature of these relative entropies then.",
            "Is not a distance to start with.",
            "We know that it's not a distance because it's not symmetric.",
            "But in many ways, acts like Euclidean distance squared.",
            "OK, so many ways the intuition by thinking of it as a square of a distance is actually pretty good.",
            "So here I have taken 3 arbitrary distributions, PO P1 and Q and then taking the equal mixture of POM P1 to obtain P half and then you can prove this identity in general.",
            "So this is like the parallelogram law.",
            "Where, of course, the sum of these two squares is equal to the sum of the squares of the diagonals.",
            "All right, so that that would be completely equivalent if we were taking these relative entropies to be squared distances.",
            "There is also Python."
        ],
        [
            "Theorem this one comes from the 1975 is duties are.",
            "So this is when you have some distribution Q some distribution Q and then you have a linear space of distributions.",
            "What we call a linear family of distribution.",
            "So it's defined by some linear inequalities and essentially you want to project that Q onto that linear space so that Q sub L is the closest element in the linear space.",
            "The linear family of distributions.",
            "The closest one to Q and in fact you can show that this is unique, just like you would do in.",
            "In Euclidean space?",
            "Or generally in Hilbert space because of the convexity of the relative entropy, so then?",
            "If you choose any PNEP in the linear space then then the relative entropy between P&Q is you can you can decompose it as a relative entropy between P and the approximation and the approximation NQ just like if you had a right angle there and you would have orthogonality.",
            "Think of this as the square of the hypotenuse A and the sum of the squares in the right angles.",
            "OK, this does not hold in general.",
            "You need this space L to be flat and that's why.",
            "Here for example I have a linear family of distributions.",
            "So for example if you have an exponential family, that's also going to work."
        ],
        [
            "OK, so now we come to the more interesting part of the talk.",
            "And.",
            "You know, in information theory we like to define this information.",
            "Measures like entropy, mutual information, relative entropy and so on.",
            "But at the end of the day, what we really like to do is to post questions.",
            "That are of relevant to the engineer of relevance to the engineer or to the statistician, and then show that the answer to these questions.",
            "These answers are really those information measures and so here I have listed five different problems for which the answer turns out to be the relative entropy.",
            "Alright, so."
        ],
        [
            "The first one is a large deviations type of question.",
            "We saw before.",
            "In what way are there and if entropy is related to the central limit theorem so large deviations relative entropy actually is where where it has thrived very prominently?",
            "And in a nutshell, this is what happened.",
            "What's happening now think of having two distributions P&Q and then this epsilon P. This is a little neighborhood of the distribution P. OK, so.",
            "Think of this Y one through YN.",
            "Alright, that's just a realization of, say, N IID random variables and what you could do is take the empirical distribution of this vector of random variables.",
            "You take this empirical distribution assuming that the true distribution is Q.",
            "That's why I have this big Q here.",
            "Alright, so we know from the law of large numbers then.",
            "Eventually, this empirical distribution is going to converge to Q. OK, so now this is telling us.",
            "What is going to be the probability that this empirical distribution is going to be in a little neighborhood of P?",
            "Is going to be visiting a little neighborhood of P after N samples.",
            "Alright, so if any is very small, 3 or 4, you know maybe there is a chance that you would be impersonating P by the sequence, but it's going to be very unlikely that you are going to be in a neighborhood of P when N is large because you should really be close to Q. Alright, so the relative entropy is actually gauging that.",
            "That probability that probability is going to be exponentially decreasing in an the exponent is really the relative entropy.",
            "So that's the difficulty of impersonating P by Q.",
            "Now imagine that Q.",
            "Matching that there are some.",
            "Some letters in this alphabet that have zero probability according to Q.",
            "And positive probability according to P. Well, there is no way that you can impersonate a distribution.",
            "Did you can impersonate P with Q because no matter what you do, that empirical distribution of Y one through YN will never have positive probability in those letters, so you will never be able to impersonate P. You'll never be able to be close to P, and that's why relative entropy in that case is Infinity.",
            "OK. You cannot do it.",
            "Where is the other way around around?",
            "You may very well be able to do it, maybe may very well be that if the true distribution is P then after a while you are still at a very very close to Q.",
            "In that case DPQ is is a finite number.",
            "OK, so this result is in a paper right turn off.",
            "In fact, it's interesting 'cause he attributes the result.",
            "To Stein.",
            "And.",
            "And it turns out that the story is that then they ask sustain about this, and he said, I have no clue.",
            "I have never seen this result.",
            "And so you know it's called this time this time's lemma, and so on.",
            "But then it turned out that what happened was the churn of submitted the paper, and then one of the reviewers said, oh, you know, I've seen this result in an unpublished paper by Stein.",
            "So then turn off.",
            "Just referenced this Stein thing even without even looking at it because it had not been published.",
            "And so it's stuck like this time slimmer.",
            "It's really do to turn off, and it's due to turn off in this paper.",
            "Usually people reference the wrong paper by turn off.",
            "In order to reference these results."
        ],
        [
            "Now this one is a law of large numbers, type of characterization of relative entropy, and.",
            "This is kind of like a bashan.",
            "Type of analysis.",
            "Say after you have several observations and the observations come from P. What is the reliability of rejecting Q when P is true?",
            "So.",
            "What you're going to do is you are going to find the posterior probability of both hypothesis and then decide.",
            "Cording to which one is larger?",
            "So if you were to take.",
            "That number, the ratio of the two posterior probabilities, take the logarithm and then divide by N. Then that actually is going to.",
            "Two converts to the relative entropy of P&Q.",
            "So in fact this is the kind of Bashan result than on patients can live with becausw.",
            "It actually does not depend on the priority probability of the hypothesis, even though unless there is such a priori probability, you cannot define the quantities here at the end of the day it washes out OK, so if this is large then you would expect that it's going to be.",
            "Unlikely that you're going to decide the wrong hypothesis.",
            "You're going to do.",
            "Be able to reject Q when P is true with high reliability."
        ],
        [
            "Now in information theory.",
            "Or when we do lossless data compression, for example, we do Huffman coding, we have 1/2 man code that selects variable length representations for different elements in the source.",
            "And that code is designed to minimize the average length, so you design it for some distribution Q.",
            "But then suppose that the true distribution is P. OK. Well then you have to pay a penalty and that penalty is DPQ the relative entropy of P with respect to Q Now.",
            "Notice that what I said actually is not quite what I have in the slide.",
            "In the slide, I say the asymptotic excess rate, 'cause to be precise.",
            "That result is not true.",
            "Is not true for any length, it's only true asymptotically.",
            "So the fact that this is the penalty for the mismatch in data compression is true in the limit as N goes to Infinity.",
            "In fact, you may have no penalty whatsoever 'cause you may have two distributions that share the same Huffman code, so.",
            "That way you see that there is really not necessarily any asymptotic penalty.",
            "But you have IID distributions.",
            "When N goes to Infinity and a Half Men, coats are all sufficiently far apart, then indeed you incur in a penalty.",
            "Now here's another one that he."
        ],
        [
            "To do with reliable communication, but in Shannon's original formulation, channel capacity is the number of bits you push through the channel divided by the time, the time it takes to push the bits through the channel.",
            "Now here imagine that instead of.",
            "Instead of paying by the time you pay by the dollar or by the euro, so some symbols cost you 11 unit and some symbols are free.",
            "Alright, so now what happens is that at the output you don't see what you send or you only see some distribution induced by the channel.",
            "So the maximum number of bits per unit cost.",
            "Then that's going to be deep Q the relative entropy between P&Q.",
            "Alright, so in essence when?",
            "When you normalize by cost rather than time and one of the symbols have 0 cost, that's important then.",
            "Rather than having to maximize mutual information like we do in the formula for channel capacity, you maximize over relative entropy.",
            "OK, so here is the connection."
        ],
        [
            "And with estimation that.",
            "That I alluded to before, so you saw that there is this.",
            "This interpretation of relative entropy as mismatched data compression.",
            "Well there is also according to this formula, and this is actually a very recent formula which I got published earlier this year.",
            "You can write this relative entropy for arbitrary distributions P&Q.",
            "As an integration of the difference between 2 mean square errors and then this integration is over signal to noise ratio.",
            "So.",
            "Actually, I think I have another slide."
        ],
        [
            "Here that gives these in a little bit more.",
            "Detail although I'm running out of time, so here we have the mean squared error.",
            "Of course you all know it's attained by the conditional mean.",
            "Alright, so if the noise."
        ],
        [
            "This Gaussian if I observe this X in Gaussian noise, then I can write that conditional mean as the ratio of two integrals.",
            "But now imagine that I have in mind the wrong distribution for X. Alright, so I have a mismatch in my msee estimator.",
            "Alright, so this would be the case."
        ],
        [
            "This would be the case where X is Gaussian, in which case what you have is of course a linear estimator.",
            "That would be the case when X is."
        ],
        [
            "Equippable then you have hyperbolic tangent.",
            "Alright, so I'm going to the."
        ],
        [
            "No, the MSE at a particular signal to noise ratio.",
            "By this notation MSE.",
            "Now mismatch estimation."
        ],
        [
            "Is when?",
            "And this MSE subcu tells us that the distribution I'm thinking is true is Q, but in fact the X is distributed according to P. So in fact this MCQ actually also depends on P because P is the truth and Q is what I think it is alright.",
            "So the MSE would be the MSE when I think.",
            "P is the true one, and it is indeed the true one.",
            "Alright, so then."
        ],
        [
            "The most common case of mismatch estimation is when I think that Q is Gaussian, because then the estimator is the linear estimator OK, and in that case of course you all know that MSQ then does not depend on the actual distribution of X.",
            "Right, because the the mean squared error is.",
            "Since it's a linear estimator, is only going to depend on the 2nd order properties of X. OK, so then the relative."
        ],
        [
            "Entropy and the MSE as I said before, I related through the integral of this mismatched MSE.",
            "And of course, by definition, then each of the mismatch intimacy is greater than the minimum mean square error.",
            "So in this formula in this formula you do see that relative entropy is negative when Q is not equal to P, whereas in the original formula you have to do Jensen's inequality in your head.",
            "OK, so I think I'll stop here.",
            "And just maybe to wrap up.",
            "Just to say that relative entropy has been very slow in taking its rightful place in information theory and statistics and probability, I think people realized much earlier on than it was very important, but I think now nowadays is becoming more and more important, and we're finding all these new connections that.",
            "Up to now, where were hidden and some of the open problems that had been open for awhile have only been shown recently.",
            "Thank you to these thanks to these.",
            "A connection with estimation, so thank you very much.",
            "Time for a few questions for Sergio.",
            "Maybe I'll start off.",
            "I had one question.",
            "When one person whose name didn't come up is ready, I was wondering if you could say a little bit about Renyi entropy and its position in place in this discussion.",
            "So yeah, thank you.",
            "So in essence, rainy.",
            "Yeah he had a lot of different contributions in this area, but.",
            "One thing that he did, he said well when we.",
            "When we defined entropy, maybe we could change that logarithm for something else.",
            "And as long as we have a function that retain some of the analytical properties, then there might be something interesting that we can say, and indeed he defined the rainy entropy and then from there you can also define the rainy relative entropy and so on.",
            "And there have been some what we say.",
            "Operational meanings of that rainy entropy, rainy mutual information, but.",
            "By and large heavens, it hasn't been that successful in information theory, there have been a handful of results, so at the level of proving properties, like for example, the data processing theorem that I showed, you can have at the output less divergences at the input, and so on.",
            "All that works through, but at the level of actually solving problems that are of interest to the statistician of the engineer, it has not been.",
            "Was successful.",
            "Of course, there's also been the whole issue about F divergences, Alice Silver Divergance and so on were again, the logarithm is replaced by another quantity, and from time to time these things do yield some new insights, but much less than this, and of course the basic reason is simply that the logarithm of a product is the sum of the logarithms.",
            "Yeah, that's that's a good question.",
            "Who was the first one to prove it?",
            "Home.",
            "Well, I think it's it's inherent in callback labeler.",
            "That you know, they know that the.",
            "The information for this combination, actually they they show several and they say they don't have a theorem that says this is not negative, but they do say it is non negative so they obviously I don't know whether it is so using Jensen's inequality or using the fact that the lower is is upper bound is lower bounded by 1 -- 1 / X.",
            "You can also prove it that way.",
            "But who knows, maybe I assumed that world when he used it.",
            "He probably also notice that.",
            "One use of mutual entropy in in this community has been as a training loss for regression problems so.",
            "Minimizing the mean squared error.",
            "There's also been work.",
            "Minimizing the conditional entropy.",
            "I was wondering if you reside on the connection between these two.",
            "Sorry if you.",
            "I was wondering if your new result.",
            "And the relationship between me and conditional entropy shed any light on the relationship between these two very different ways.",
            "Question, that's a good question.",
            "I don't know the one.",
            "Was actually quite surprising is that.",
            "This results on.",
            "I'm trying to go fast, but I guess this results in limits on MSE, an mutual information lead to.",
            "Result it has nothing to do with information theory and it's in continuous time nonlinear filtering, so the idea there is that we can we have a general formula that relates the non causal.",
            "Minimum is mean square error and the causal minimum mean square error.",
            "So regardless of whether the input is Gaussian or not, if you observe it in Gaussian noise in white Gaussian noise, if you give me the causal minimum mean square function for all signal to noise ratio.",
            "The non causal one and vice versa.",
            "That's that's the kind of result that.",
            "Important, but we should be able to.",
            "You know, it's tough in estimation and nonlinear filtering, but the only way we know how to prove it is through mutual information.",
            "So perhaps you know something like that can happen also.",
            "OK well.",
            "I think you'll be around.",
            "Yeah, sure couple days.",
            "So if people would like to discuss things further with Sergio, he'll be around at the conference.",
            "And why don't we give him a nice thank you again for a great collector?",
            "Thank you.",
            "Sir."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sergio Verdu is the Eugene Higgins professor of electrical engineering at Princeton University, and he is one of the Premier information theorists of our time.",
                    "label": 1
                },
                {
                    "sent": "Sergio is probably best known for his work on multiuser detection, which is fundamental and critical to modern wireless communications.",
                    "label": 0
                },
                {
                    "sent": "He's made fundamental contributions to information theory in its fundamentals in terms of finding the capacity of several classes of important channels that are again critical in modern wireless communication systems, and he's also.",
                    "label": 0
                },
                {
                    "sent": "Developed and uncovered.",
                    "label": 0
                },
                {
                    "sent": "Some surprising and very cool connections between maximum or I'm sure.",
                    "label": 0
                },
                {
                    "sent": "Mean square error estimation and mutual information, and I think he's going to tell us a little bit about some of that today.",
                    "label": 0
                },
                {
                    "sent": "Sergio has too many awards and accolades to mention all of them and less.",
                    "label": 0
                },
                {
                    "sent": "I took up his entire slot, but I'll mention just a few.",
                    "label": 0
                },
                {
                    "sent": "He's a fellow of the IEEE.",
                    "label": 0
                },
                {
                    "sent": "He received the IEEE third Millennium Metal in 2000, the Claude Shannon Award in 2007, and I think he was one of the youngest people ever to receive that honor, and he received the Richard Hamming Medal in 2008.",
                    "label": 0
                },
                {
                    "sent": "And on a personal note, Sergio is from Barcelona, Spain, and he can probably find the best coffee in Princeton if you ask Sergio.",
                    "label": 0
                },
                {
                    "sent": "Thank you again and take it away.",
                    "label": 0
                },
                {
                    "sent": "Thank you Ron.",
                    "label": 0
                },
                {
                    "sent": "Well, I'd like to find the best coffee in Vancouver.",
                    "label": 0
                },
                {
                    "sent": "Then it's probably not too difficult, so it's a great pleasure to be here.",
                    "label": 0
                },
                {
                    "sent": "Thank you for the invitation to address this audience.",
                    "label": 0
                },
                {
                    "sent": "I had heard a lot of good things about this conference, but actually never had the opportunity to be here.",
                    "label": 0
                },
                {
                    "sent": "So it's a special pleasure to give the first talk in the conference, so.",
                    "label": 0
                },
                {
                    "sent": "What I am going to do here as you can tell from the title, is going to be a very wide audience.",
                    "label": 0
                },
                {
                    "sent": "Talk thoroughly in the morning.",
                    "label": 0
                },
                {
                    "sent": "We're not going to get into mathematics.",
                    "label": 0
                },
                {
                    "sent": "It's going to be mainly a historical overview, touching on some of the big problems that relative entropy has found itself itself useful.",
                    "label": 1
                },
                {
                    "sent": "Alright, so here's the definition to start with.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The definition of relative entropy.",
                    "label": 0
                },
                {
                    "sent": "You can tell immediately that this of course P&Q are probability distributions.",
                    "label": 0
                },
                {
                    "sent": "You can tell immediately that P if P is equal to Q, then this relative entropy is equal to 0.",
                    "label": 1
                },
                {
                    "sent": "You cannot tell immediately that if P is not equal to Q, that these relative entropy is actually going to be non 0.",
                    "label": 0
                },
                {
                    "sent": "Cause this is an expectation here.",
                    "label": 0
                },
                {
                    "sent": "But is the expectation of a quantity that can be either positive or negative?",
                    "label": 0
                },
                {
                    "sent": "OK, so actually.",
                    "label": 0
                },
                {
                    "sent": "Especially some mathematicians prefer.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To use an alternative representation where you take the expectation with respect to the reference, measure the queue rather than the P, and this actually turns out to be perhaps more natural in some in some applications.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so the most important cases are of course the discrete case and the continuous case.",
                    "label": 0
                },
                {
                    "sent": "When both P&QI should say very important is that P&Q have to be distributions defined on the same.",
                    "label": 0
                },
                {
                    "sent": "Set on the same measurable space.",
                    "label": 0
                },
                {
                    "sent": "So if this space is finite or is countably infinite, then we have this representation for the relative entropy if.",
                    "label": 0
                },
                {
                    "sent": "We are.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In Euclidean space, and these are continuous distributions, then we have that representation in terms of the density functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so the reason why this is related to information theory is because there is a logarithm there and.",
                    "label": 0
                },
                {
                    "sent": "And the reason why physicists like it is because it has units.",
                    "label": 0
                },
                {
                    "sent": "Relative entropy.",
                    "label": 0
                },
                {
                    "sent": "Has you know?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which inherits from the log from the basis of the logarithm.",
                    "label": 0
                },
                {
                    "sent": "And here I have listed a bit tongue in cheek.",
                    "label": 0
                },
                {
                    "sent": "Some of the most.",
                    "label": 0
                },
                {
                    "sent": "Famous units for the logarithm in information theory.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know when some when something is used by many, many different people who don't necessarily talk to each other, then what happens is that it ends up having many names and this is this is a drawing from the 17th century with all the known names of God.",
                    "label": 0
                },
                {
                    "sent": "So relative entropy is not quite the same, but.",
                    "label": 0
                },
                {
                    "sent": "You know it comes close in the number of diff.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Names that it has received over the years.",
                    "label": 0
                },
                {
                    "sent": "I have listed here some of them, the ones that would feed on one slide that you can easily read.",
                    "label": 0
                },
                {
                    "sent": "Now a lot of them have the word information.",
                    "label": 0
                },
                {
                    "sent": "And some of them of course have cool black library will see that we'll see what cool black library have to do with this.",
                    "label": 0
                },
                {
                    "sent": "But you can see already that there is a lot of flavor of decisions.",
                    "label": 0
                },
                {
                    "sent": "Discrimination, weight of evidence and so on.",
                    "label": 0
                },
                {
                    "sent": "And also there is this word divergance that happens also quite quite commonly.",
                    "label": 0
                },
                {
                    "sent": "In fact, for most of my career, I used to call this divergance before I converted to relative entropy.",
                    "label": 0
                },
                {
                    "sent": "I used to be very dogmatic.",
                    "label": 0
                },
                {
                    "sent": "I refuse to call it relative entropy.",
                    "label": 0
                },
                {
                    "sent": "Becausw Claude Shannon in 1948.",
                    "label": 0
                },
                {
                    "sent": "In his paper he calls relative entropy something else.",
                    "label": 0
                },
                {
                    "sent": "He says relative entropy, just a passing comment, then nobody else.",
                    "label": 0
                },
                {
                    "sent": "Abided by that terminology, but he said the entropy divided by the logarithm of the alphabet size is the relative entropy.",
                    "label": 0
                },
                {
                    "sent": "But anyway, then I converted not too long ago to the to the common Canon in information theory, and I call it relative entropy.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first appearance of.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of relative entropy is in this very important paper by world.",
                    "label": 0
                },
                {
                    "sent": "In 1945 where he.",
                    "label": 0
                },
                {
                    "sent": "Post and solve the problem of sequential hypothesis testing.",
                    "label": 0
                },
                {
                    "sent": "The easier problem of non sequential hypothesis testing was only solved 10 years later, so he didn't actually gave it a name, but you can see in the denominator of this formula.",
                    "label": 0
                },
                {
                    "sent": "This was the expected number of observations necessary for reaching a decision with a certain.",
                    "label": 0
                },
                {
                    "sent": "Quality you can see that the nominator of that equation, what we find is indeed the relative entropy.",
                    "label": 0
                },
                {
                    "sent": "Now, right?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About the same time.",
                    "label": 0
                },
                {
                    "sent": "Jeffreys also very famous statistician came up with.",
                    "label": 0
                },
                {
                    "sent": "Symmetrized version of relative entropy.",
                    "label": 0
                },
                {
                    "sent": "Which is now commonly referred to as Jeffries Divergance.",
                    "label": 0
                },
                {
                    "sent": "And we would expect that because we're kind of programmed to.",
                    "label": 0
                },
                {
                    "sent": "Think that important important quantities should be symmetric, so we are programmed to think that perhaps this is more fundamental than the relative entropy, but in fact is known this is, this is rarely useful, whereas relative entropy in all its ugliness or asymmetric city turns out to be of enormous importance.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so the first formal definition, and that's why it's very frequently referred to as the callback library.",
                    "label": 0
                },
                {
                    "sent": "Divergents or distance?",
                    "label": 0
                },
                {
                    "sent": "Happened in this paper in 1951.",
                    "label": 0
                },
                {
                    "sent": "Cool black and libel are are calling them in information for discrimination.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the definition already immediately recognizes that what we're defining there is actually the expectation of a certain random variable.",
                    "label": 0
                },
                {
                    "sent": "And you already see the word information there.",
                    "label": 0
                },
                {
                    "sent": "And in fact, in the paper on information and sufficiency.",
                    "label": 0
                },
                {
                    "sent": "So what was the motivation of cool wagon lilar to introduce this measure?",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the first part.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I got from the introduction.",
                    "label": 0
                },
                {
                    "sent": "Generalizes to the abstract case, Shannon's definition of information winners information is essentially the same as Shannon, although their motivation was different, and Shannon apparently has investigated the concept more completely.",
                    "label": 0
                },
                {
                    "sent": "OK, so the last sentence is a big understatement.",
                    "label": 0
                },
                {
                    "sent": "In fact, we'll out come back to this later, and I'll I'll tell you more in detail what was what.",
                    "label": 0
                },
                {
                    "sent": "Was it that winner had to do with this?",
                    "label": 0
                },
                {
                    "sent": "But in a nutshell.",
                    "label": 0
                },
                {
                    "sent": "Shannon was interested in in what we say the operational role of these quantities.",
                    "label": 0
                },
                {
                    "sent": "Setting up an engineering problem and then showing that this quantity as well as entropy, as well as mutual information, are the answers to those problems whereas.",
                    "label": 0
                },
                {
                    "sent": "Where is Wiener?",
                    "label": 0
                },
                {
                    "sent": "Did not say he only used one of these quantities and essentially said that he thought it would be an interesting exercise to actually optimize this quantity when the variance of the random variable is fixed.",
                    "label": 0
                },
                {
                    "sent": "OK, so the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most the most important special case of.",
                    "label": 0
                },
                {
                    "sent": "Relative entropy is the mutual information.",
                    "label": 0
                },
                {
                    "sent": "So as as we're going to comment later, the relative entropy is a great way to measure the similarity between two distributions and asymmetric way of.",
                    "label": 0
                },
                {
                    "sent": "Of doing that and.",
                    "label": 0
                },
                {
                    "sent": "We can apply that definition to the special case where the 1st.",
                    "label": 0
                },
                {
                    "sent": "Distribution is a joint distribution and the second one is the product of the marginals.",
                    "label": 0
                },
                {
                    "sent": "So if.",
                    "label": 0
                },
                {
                    "sent": "If these random variables X&Y are independent, then of course we're going to get 0.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, we're going to get unknown negative.",
                    "label": 0
                },
                {
                    "sent": "A negative number.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is extremely, extremely general way of defining of gauging the degree of dependence.",
                    "label": 0
                },
                {
                    "sent": "Of course we all have grown up with correlation coefficient and so on, but for example that one only applies to real valued random variables.",
                    "label": 0
                },
                {
                    "sent": "Here X&Y can be whatever you want, X can be discrete, Y can be continuous.",
                    "label": 0
                },
                {
                    "sent": "Anything you want.",
                    "label": 0
                },
                {
                    "sent": "You can, you can throw it inside this mutual information.",
                    "label": 0
                },
                {
                    "sent": "Of course, you could also define it the other way.",
                    "label": 0
                },
                {
                    "sent": "You could define the the relative entropy of the product of the marginals relative to the joint distribution, and that's what we call lautem information with mutual spelled backwards.",
                    "label": 0
                },
                {
                    "sent": "And you know, it's not as famous as this one as mutual information, but you can still at least write one paper about it.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The relative entropy is the mother of all information measures.",
                    "label": 0
                },
                {
                    "sent": "Mutual information is a special case, entropies, special case of mutual information.",
                    "label": 0
                },
                {
                    "sent": "Moreover, if you have finite alphabet, then you can see in the bottom equation here that the relative entropy of sorry of distribution with respect to the.",
                    "label": 0
                },
                {
                    "sent": "Equally likely, distribution uniformly distributed on that alphabet.",
                    "label": 0
                },
                {
                    "sent": "That's as the logarithm of the alphabet size minus the entropy.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can.",
                    "label": 0
                },
                {
                    "sent": "You can define entropy in that special case through relative entropy and in the more general case just do it like this.",
                    "label": 0
                },
                {
                    "sent": "You say the entropy of a random variable is as the mutual information of that random variable with itself.",
                    "label": 0
                },
                {
                    "sent": "If you think about it, then it's going to be just the relative entropy of the joint distribution with the product of the two marginals, the joint distribution when X is equal to X.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so very important is.",
                    "label": 0
                },
                {
                    "sent": "Conditional relative entropy, just like regular Shannon entropy, it's very important to also have the conditional entropy.",
                    "label": 0
                },
                {
                    "sent": "Here we can do the same thing, and here's the definition in fact.",
                    "label": 0
                },
                {
                    "sent": "Conditional relative entropy is more general than relative entropy, but.",
                    "label": 0
                },
                {
                    "sent": "It's also the other way around, because this is just a special case of the other definition of definition of unconditional relative entropy.",
                    "label": 0
                },
                {
                    "sent": "So let me explain this a little bit.",
                    "label": 0
                },
                {
                    "sent": "What you have here is 3 objects, so the conditional relative entropy is defined for three objects rather than the relative entropies defined for two.",
                    "label": 0
                },
                {
                    "sent": "Here we have conditional distribution Y given X.",
                    "label": 0
                },
                {
                    "sent": "Another one Z.",
                    "label": 0
                },
                {
                    "sent": "Given X&PX.",
                    "label": 0
                },
                {
                    "sent": "So think of this.",
                    "label": 0
                },
                {
                    "sent": "Of these conditional distributions, you know sometimes they are called transition kernels or Markov kernels.",
                    "label": 0
                },
                {
                    "sent": "Things like that you can think of them even in the absence of having defined joint distributions between.",
                    "label": 0
                },
                {
                    "sent": "X&Z an YNX, but in any case, if you define those joint distributions X&Y.",
                    "label": 0
                },
                {
                    "sent": "The product of the conditional Y given X&X and the condition of C given X and the same PX.",
                    "label": 0
                },
                {
                    "sent": "Then you can just look at the relative entropy between two those two joint distributions and that's what we call the conditional relative entropy.",
                    "label": 0
                },
                {
                    "sent": "Another way of seeing this is simply say well.",
                    "label": 0
                },
                {
                    "sent": "I'm going to find the the usual relative entropy for two distributions.",
                    "label": 0
                },
                {
                    "sent": "The two distributions would be PY, given that X is equal to a given value MPC.",
                    "label": 0
                },
                {
                    "sent": "Given that X is equal to a, now you do that for all values of A and you average over the distribution of PX.",
                    "label": 0
                },
                {
                    "sent": "So that's what this.",
                    "label": 0
                },
                {
                    "sent": "This notation means just like what you do for entropy or what you do for mutual information.",
                    "label": 0
                },
                {
                    "sent": "OK, so in fact now.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can use this.",
                    "label": 0
                },
                {
                    "sent": "These definition of conditional relative entropy to redefine mutual information and in fact this is actually more useful than the original definition.",
                    "label": 0
                },
                {
                    "sent": "So it's actually quite a bit.",
                    "label": 0
                },
                {
                    "sent": "You get a lot more mileage by looking at mutual information in this way then in the other way so.",
                    "label": 0
                },
                {
                    "sent": "What what I have here is just a special case of conditional relative entropy, where the second the reference measure here is not a conditional measure.",
                    "label": 0
                },
                {
                    "sent": "It turns out to be actually an unconditional measure, which is the one that I get simply by taking the marginal of the joint distribution that.",
                    "label": 0
                },
                {
                    "sent": "Results from just.",
                    "label": 0
                },
                {
                    "sent": "Multiplying these two alright, so I have I have.",
                    "label": 0
                },
                {
                    "sent": "Shona a little figure here.",
                    "label": 0
                },
                {
                    "sent": "Each of these dots of the grey dots represent a conditional distribution of Y.",
                    "label": 0
                },
                {
                    "sent": "Given that X is takes a certain value.",
                    "label": 0
                },
                {
                    "sent": "OK, now suppose that you have some weight on these values of X. Alright, so then you can define the center of gravity of that figure.",
                    "label": 0
                },
                {
                    "sent": "The center of gravity of that figure.",
                    "label": 0
                },
                {
                    "sent": "Depending on what the weight of the grey dots is, is what I have depicted here as the red dot.",
                    "label": 0
                },
                {
                    "sent": "I don't know whether you can see the colors from far away, but that would be the PY, so that would be the.",
                    "label": 0
                },
                {
                    "sent": "The average of these distribution, the mixture of these distribution according to the PX.",
                    "label": 0
                },
                {
                    "sent": "OK, so now imagine that you would like to see essentially what is like the the average radius of this figure.",
                    "label": 0
                },
                {
                    "sent": "The average distance from PY to each of those dots.",
                    "label": 0
                },
                {
                    "sent": "Average according to the weights of the dots.",
                    "label": 0
                },
                {
                    "sent": "And that's.",
                    "label": 0
                },
                {
                    "sent": "Linear chain information.",
                    "label": 0
                },
                {
                    "sent": "So the mutual information, in a way, is just a way to.",
                    "label": 0
                },
                {
                    "sent": "Gates, how far apart these conditional distributions are.",
                    "label": 0
                },
                {
                    "sent": "But doing that without proper weighting of those conditional distributions and the proper weighting is this PX.",
                    "label": 0
                },
                {
                    "sent": "Alright, no, not immediately obvious from from this form, but mutual information is indeed symmetrical with X&Y, and that you can see from the original definition.",
                    "label": 0
                },
                {
                    "sent": "OK, now this one of the immediate consequences of this form for mutual information.",
                    "label": 0
                },
                {
                    "sent": "Is this for.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mila and this is extremely useful for me.",
                    "label": 0
                },
                {
                    "sent": "This is the most useful representation and information theory, even though you don't really find it very often.",
                    "label": 0
                },
                {
                    "sent": "But Q here is something auxiliary you choose.",
                    "label": 0
                },
                {
                    "sent": "You choose what Q is, and if you're smart, inducing in choosing Q, you get a lot of mileage in many different problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so you may want to use these.",
                    "label": 0
                },
                {
                    "sent": "Keep this in mind for the future.",
                    "label": 0
                },
                {
                    "sent": "You have the advantage that not a lot of people use it so.",
                    "label": 0
                },
                {
                    "sent": "It's always nice.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so now we've been talking about distances and blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Why are we?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Saying that this is a distance well.",
                    "label": 0
                },
                {
                    "sent": "Big cause, even though this is the expectation of a random variable that can be positive or negative, you can prove there is no negative an it's zero if and only if P is equal to Q. Alright, so to prove this inequality, the best way to do that is to use Jensen's inequality.",
                    "label": 0
                },
                {
                    "sent": "In the alternative representation in the representation of relative entropy, where you take the expectation with respect to the reference measure, not with respect to P. You can also do it in the original one, but it's not as elegant.",
                    "label": 0
                },
                {
                    "sent": "OK, so now this especially the physicist like to call it Gibbs inequality, and this is actually the most important inequality in information theory without a doubt.",
                    "label": 1
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So you use Jensen's inequality once to prove this one and then.",
                    "label": 0
                },
                {
                    "sent": "99% of the time you don't have to use Jensen's inequality again or use ugly lower bounds on the logarithm.",
                    "label": 0
                },
                {
                    "sent": "You just use this and then as a bonus you'll get conditions under which whatever inequality you want to show actually holds with equality.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is, as I say here, frequently known as Gibbs inequality.",
                    "label": 0
                },
                {
                    "sent": "Now, Gibbs was kind of like the father of.",
                    "label": 0
                },
                {
                    "sent": "Statistical mechanics as we know it, modern mathematical, statistical mechanics, thermodynamics and that's not what he showed.",
                    "label": 0
                },
                {
                    "sent": "Actually, among other reasons, Becausw relative entropy had not been invented yet.",
                    "label": 0
                },
                {
                    "sent": "This was 1901.",
                    "label": 0
                },
                {
                    "sent": "This was well before it was actually well before Jensen.",
                    "label": 0
                },
                {
                    "sent": "It was well before the definition of convex functions actually.",
                    "label": 0
                },
                {
                    "sent": "Of course, convex functions have been defined before for, say, real valued functions and functions on Euclidean spaces, but the modern definition of convex functions in abstract linear spaces that is actually due to Jensen and that was after this beginning of the 20th century, even though it sounds like a very fundamental concept, is.",
                    "label": 0
                },
                {
                    "sent": "Is about 100 years old now?",
                    "label": 0
                },
                {
                    "sent": "Alright, so Gibbs actually what he showed an it's in that inequality for 47 at the bottom.",
                    "label": 0
                },
                {
                    "sent": "Actually what he showed was a special case of the nonnegativity of mutual information.",
                    "label": 0
                },
                {
                    "sent": "That's what that.",
                    "label": 0
                },
                {
                    "sent": "Inequality is showing, so it's not quite the nonnegativity of relative entropy.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me show you now.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Couple of Inequality's and they are very important and particularly in information theory.",
                    "label": 0
                },
                {
                    "sent": "These are the bread and butter of the uses of relative entropy and.",
                    "label": 0
                },
                {
                    "sent": "They really show why this is connected to the notion of information.",
                    "label": 0
                },
                {
                    "sent": "So here the idea is that you have to think of this as a black box that has some kind of randomness inside.",
                    "label": 0
                },
                {
                    "sent": "So we have a black box and then we drive that black box with.",
                    "label": 0
                },
                {
                    "sent": "Random object that has distribution either PX or QX.",
                    "label": 0
                },
                {
                    "sent": "And of course, those two distributions then induce an output distribution.",
                    "label": 0
                },
                {
                    "sent": "So all this is saying is that.",
                    "label": 0
                },
                {
                    "sent": "The relative entropy at the input between these two distributions is greater equal than the relative entropy at the output with equality.",
                    "label": 0
                },
                {
                    "sent": "When will you have equality?",
                    "label": 0
                },
                {
                    "sent": "Well, the special case that you will have equality is when here what you have is just a one to one transformation.",
                    "label": 0
                },
                {
                    "sent": "Alright, I want to one transformation is not going to change information, but more more importantly or more generally, you're going to have equality.",
                    "label": 0
                },
                {
                    "sent": "If what you produce here is a sufficient statistic for these pair of distributions, OK, so immediately we see there this notion of information of preserving information is essentially equivalent to preserving relative entropy.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this one is kind of dual here.",
                    "label": 0
                },
                {
                    "sent": "What we have is 2 black boxes, but only one input distribution.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Depending on the black box, we have a different output distribution and what we're saying is that the relative entropy conditioned on knowing the input is greater than the relative entropy.",
                    "label": 0
                },
                {
                    "sent": "Not conditioned on knowing the input so.",
                    "label": 0
                },
                {
                    "sent": "In other words, he's always better to know than not to know.",
                    "label": 0
                },
                {
                    "sent": "At least for these purposes.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is really all there is.",
                    "label": 0
                },
                {
                    "sent": "Here is the convexity.",
                    "label": 0
                },
                {
                    "sent": "The fact then?",
                    "label": 0
                },
                {
                    "sent": "A relative entropy is convex in the pair of distributions P&Q.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another very important property is the chain rule, and no matter how many random variables you have, you have the joint distribution of several random variables.",
                    "label": 0
                },
                {
                    "sent": "But you do have to have the same number on both sides, unlike mutual information where you can have anything you want on both sides.",
                    "label": 0
                },
                {
                    "sent": "Here, remember that you have to have distributions that are defined on the same set, so you can do this one at a time.",
                    "label": 0
                },
                {
                    "sent": "No matter how involved these joint distributions are, you can take the relative entropy, but now the relative entropy is conditioned, and that's why conditioning is so important when you deal with any of these information measures.",
                    "label": 0
                },
                {
                    "sent": "So you take one condition on all the others that you have already taken.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me go back to that intriguing comment saying that.",
                    "label": 0
                },
                {
                    "sent": "The Cool Rock library divergance or the relative entropy generalizes channels.",
                    "label": 0
                },
                {
                    "sent": "Definition of information and winners information.",
                    "label": 0
                },
                {
                    "sent": "Now let me just point out now that I read these Shannon's definition of information, actually Shannon.",
                    "label": 0
                },
                {
                    "sent": "Never mentioned that word in his paper.",
                    "label": 0
                },
                {
                    "sent": "In fact, he did mention information theory.",
                    "label": 0
                },
                {
                    "sent": "In one place in his paper, but he never says anything about.",
                    "label": 0
                },
                {
                    "sent": "These being a measure of information, or he never even defined mutual information in his paper for one matter for that matter.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what?",
                    "label": 0
                },
                {
                    "sent": "Both defined winner and Shannon who we have here in this picture.",
                    "label": 0
                },
                {
                    "sent": "In Shannon, define the discrete entropy and also this what we call now differential entropy, although he also called entropy and winner also define the differential entropy about the same time.",
                    "label": 0
                },
                {
                    "sent": "Also in 1948 in a book that became a big bestseller called Cybernetics.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And essentially winner the only the only reason he gave for defining it that way was that OK von Neumann had told him that that.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nothing was a good thing to look at.",
                    "label": 0
                },
                {
                    "sent": "Now this is from an excerpt from a paper he wrote later the Shannon Wiener definition of Quantity of information for it belongs to the two of us equally.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Shadow",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It was a little bit more circumstance circumspect about this definition and then the mathematician that actually realized the first mathematician who realize that.",
                    "label": 0
                },
                {
                    "sent": "What Shannon had done was really revolutionary was Andre Kolmogorov in the Soviet Union.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And he he got it right.",
                    "label": 0
                },
                {
                    "sent": "It has no physical interpretation.",
                    "label": 0
                },
                {
                    "sent": "So the first sin that one makes when when you introduce this quantity is to take the logarithm of a probability density function.",
                    "label": 0
                },
                {
                    "sent": "And that's a big no no.",
                    "label": 0
                },
                {
                    "sent": "Alright, so when we took logarithms up to now, we always took logarithms of quantities that don't have any units, so to speak.",
                    "label": 0
                },
                {
                    "sent": "OK, they were ratios of probabilities ratios of PDFs or rather nicoulin derivatives in the most general case, so you always have to think of the probability density functions as being one over.",
                    "label": 0
                },
                {
                    "sent": "Meters or seconds or whatever your.",
                    "label": 0
                },
                {
                    "sent": "Your underlying Euclidian space means, so this is the kind of integral that should be declared illegal, and no wonder it gives leads to a lot of trouble to many people that tend to use it thinking that it satisfies the same properties as entropy.",
                    "label": 0
                },
                {
                    "sent": "But then they run into trouble.",
                    "label": 0
                },
                {
                    "sent": "So why has this been so important in information theory?",
                    "label": 0
                },
                {
                    "sent": "If you take the leading textbook in information theory by covering Thomas, it devotes a whole chapter to this.",
                    "label": 0
                },
                {
                    "sent": "Well, essentially becausw relative entropy took a long time to get into information theory, even though it was defined by Kullback Leibler only three years after channel.",
                    "label": 0
                },
                {
                    "sent": "It's been extremely slow in getting into the toolbox of the information theorist, so.",
                    "label": 0
                },
                {
                    "sent": "That void essentially was taken by this differential entropy.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, and the main reason is this formula.",
                    "label": 0
                },
                {
                    "sent": "The differential entropy.",
                    "label": 0
                },
                {
                    "sent": "Actually, if you take the relative entropy of a given distribution with given mean invariants, and you take the relative entropy with respect to the Gaussian distribution with the same mean and the same variance, that of course is going to give you a number that will gauge how non normal that distribution is and that is you can write as a constant minus the differential entropy of that PX.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is really why.",
                    "label": 0
                },
                {
                    "sent": "The reason why this differential entropy now is important, 'cause this is important and of course.",
                    "label": 0
                },
                {
                    "sent": "It also says then and this is what winner showed.",
                    "label": 0
                },
                {
                    "sent": "At the same time as Shannon, that among all distributions then have the same variance.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian is the one that has the largest differential entropy.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that also.",
                    "label": 0
                },
                {
                    "sent": "Explains the rationale for the maximum entropy principle that, again, is quite.",
                    "label": 1
                },
                {
                    "sent": "Quite common in applications where.",
                    "label": 0
                },
                {
                    "sent": "OK, we don't have enough information to say what our distribution is, so we have some.",
                    "label": 0
                },
                {
                    "sent": "Information maybe about the second moment of a distribution from observations, and then we would like to actually from their bootstrap a distribution.",
                    "label": 0
                },
                {
                    "sent": "So a typical thing to do is, we say, well, let's see what is the maximum entropy or the maximum differential entropy.",
                    "label": 0
                },
                {
                    "sent": "To be more precise?",
                    "label": 0
                },
                {
                    "sent": "Alright, so you can write that you can write that entropy as minus the relative entropy of the distribution with respect to the Gaussian distribution that has the same 2nd order statistics and then the Gaussian in that same Gaussian distribution with respect to a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "That's what I mean by this.",
                    "label": 0
                },
                {
                    "sent": "A Gaussian distribution that is IID.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "In the, in the absence of any other knowledge, then if all you know are they are the 2nd order statistics, then by maximising this with respect to PX, what you're doing is you're making this equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So you're making the PX equal to the Gaussian that has.",
                    "label": 0
                },
                {
                    "sent": "The same 2nd 2nd order statistics OK, and then what?",
                    "label": 0
                },
                {
                    "sent": "You're what you're left with is with the the relative entropy of the Gaussian with a given.",
                    "label": 0
                },
                {
                    "sent": "2nd order statistics in the ID.",
                    "label": 0
                },
                {
                    "sent": "So in other words, what this is doing is measuring.",
                    "label": 0
                },
                {
                    "sent": "Or saying well, I would like to view reality in the closest way to the simplest possible explanation.",
                    "label": 0
                },
                {
                    "sent": "The simplest possible explanation being something being iid Gaussian an my observations.",
                    "label": 0
                },
                {
                    "sent": "What is the closest explanation of my observations to that pure reality?",
                    "label": 0
                },
                {
                    "sent": "And that's the maximum entropy principle.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now another very interesting use for non Gaussian Ness.",
                    "label": 0
                },
                {
                    "sent": "Is essentially the central limit theorem, but the non asymptotic version of the Central Limit theorem.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean by that?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "You take any probability density function, you convolve it with itself and you normalize it so that you keep the variance the same that you started with looks more rounded.",
                    "label": 0
                },
                {
                    "sent": "You do it again.",
                    "label": 0
                },
                {
                    "sent": "Looks even more rounded, and then you know what's going to happen.",
                    "label": 0
                },
                {
                    "sent": "It's going to become normal.",
                    "label": 0
                },
                {
                    "sent": "So Shannon, actually 1948 showed.",
                    "label": 0
                },
                {
                    "sent": "Then when you do this process.",
                    "label": 0
                },
                {
                    "sent": "Once, so you take a PDF and you convolve it with itself then.",
                    "label": 0
                },
                {
                    "sent": "You turn it indeed into a more Gaussian looking random variable, and when I say more Gaussian looking random variable, he did it through the differential entropy and that's what he called the entropy power inequality.",
                    "label": 0
                },
                {
                    "sent": "Actually he did improve the entropy power inequality that took quite a bit of effort, about 10 years later.",
                    "label": 0
                },
                {
                    "sent": "Now for a long time, even though you know that it's a very simple thing to conjecture for a long time it was an open problem to show that at each step that non Gaussian is that relative entropy with respect to the Gaussian distribution actually is monotonically decreasing.",
                    "label": 0
                },
                {
                    "sent": "And of course by doing this then you can prove the central limit theorem.",
                    "label": 0
                },
                {
                    "sent": "So it's like a strong form.",
                    "label": 0
                },
                {
                    "sent": "Of the central Limit Theorem provided of course that you start with a PDF if you don't start with a PDF, then at each step you have a discrete distribution and then this is always Infinity.",
                    "label": 0
                },
                {
                    "sent": "So discrete distribution then is going to have infinite relative entropy with respect to the Gaussian, even though of course we know that the characteristic functions are going to converge.",
                    "label": 0
                },
                {
                    "sent": "OK, so this was actually finally shown in two of the force in functional analysis in 2004 and then we came up with a much simpler proof.",
                    "label": 0
                },
                {
                    "sent": "Based on the connection between information theory and estimation theory that we will see later in some detail.",
                    "label": 0
                },
                {
                    "sent": "OK, So what you're doing here?",
                    "label": 0
                },
                {
                    "sent": "What you're doing here in the.",
                    "label": 0
                },
                {
                    "sent": "In the world of distributions with a given variance and two sided distributions, you could think of doing this.",
                    "label": 0
                },
                {
                    "sent": "In other words, you could think of doing this, say for example in the world of distributions that leave on the positive real line and have a given mean.",
                    "label": 0
                },
                {
                    "sent": "And that's what I do here.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you take the relative entropy of PX with respect to.",
                    "label": 0
                },
                {
                    "sent": "An exponential distribution with a certain mean well, that's a constant that depends on that mean minus the differential entropy of that distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, So what would be a good name for that before we call that thing non Gaussian Ness, we could also call it a normality, but since this one is the memoryless distribution, is the only one that has the memoryless property we could call this the memory over distribution?",
                    "label": 0
                },
                {
                    "sent": "This and then you could also think of the maximum entropy principle for distributions that live on the real line and have given mean, and you can do this with absolute values absolute first moment, any moment you want.",
                    "label": 0
                },
                {
                    "sent": "OK, so now this.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is another property of relative entropy is just a variational representation that say, says that if you give yourself the freedom to choose any R here where this expectation is still with respect to P, then the maximum you get is actually by choosing R = P and this is actually quite easy to show, but it's really.",
                    "label": 0
                },
                {
                    "sent": "Is really equivalent to a famous result in large deviations.",
                    "label": 0
                },
                {
                    "sent": "I won't get into the details, but.",
                    "label": 0
                },
                {
                    "sent": "Relative entropy in large deviations takes of course a very important role, and is because of the fact that the Legendre transform of accumulative generating functional is in fact that relative entropy, so that this is just an optimization problem, and that this is all there is to it to that optimization problem.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Another very interesting, intriguing feature of these relative entropies then.",
                    "label": 0
                },
                {
                    "sent": "Is not a distance to start with.",
                    "label": 0
                },
                {
                    "sent": "We know that it's not a distance because it's not symmetric.",
                    "label": 0
                },
                {
                    "sent": "But in many ways, acts like Euclidean distance squared.",
                    "label": 0
                },
                {
                    "sent": "OK, so many ways the intuition by thinking of it as a square of a distance is actually pretty good.",
                    "label": 0
                },
                {
                    "sent": "So here I have taken 3 arbitrary distributions, PO P1 and Q and then taking the equal mixture of POM P1 to obtain P half and then you can prove this identity in general.",
                    "label": 0
                },
                {
                    "sent": "So this is like the parallelogram law.",
                    "label": 0
                },
                {
                    "sent": "Where, of course, the sum of these two squares is equal to the sum of the squares of the diagonals.",
                    "label": 0
                },
                {
                    "sent": "All right, so that that would be completely equivalent if we were taking these relative entropies to be squared distances.",
                    "label": 0
                },
                {
                    "sent": "There is also Python.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Theorem this one comes from the 1975 is duties are.",
                    "label": 0
                },
                {
                    "sent": "So this is when you have some distribution Q some distribution Q and then you have a linear space of distributions.",
                    "label": 0
                },
                {
                    "sent": "What we call a linear family of distribution.",
                    "label": 0
                },
                {
                    "sent": "So it's defined by some linear inequalities and essentially you want to project that Q onto that linear space so that Q sub L is the closest element in the linear space.",
                    "label": 0
                },
                {
                    "sent": "The linear family of distributions.",
                    "label": 0
                },
                {
                    "sent": "The closest one to Q and in fact you can show that this is unique, just like you would do in.",
                    "label": 0
                },
                {
                    "sent": "In Euclidean space?",
                    "label": 0
                },
                {
                    "sent": "Or generally in Hilbert space because of the convexity of the relative entropy, so then?",
                    "label": 0
                },
                {
                    "sent": "If you choose any PNEP in the linear space then then the relative entropy between P&Q is you can you can decompose it as a relative entropy between P and the approximation and the approximation NQ just like if you had a right angle there and you would have orthogonality.",
                    "label": 0
                },
                {
                    "sent": "Think of this as the square of the hypotenuse A and the sum of the squares in the right angles.",
                    "label": 0
                },
                {
                    "sent": "OK, this does not hold in general.",
                    "label": 0
                },
                {
                    "sent": "You need this space L to be flat and that's why.",
                    "label": 0
                },
                {
                    "sent": "Here for example I have a linear family of distributions.",
                    "label": 0
                },
                {
                    "sent": "So for example if you have an exponential family, that's also going to work.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we come to the more interesting part of the talk.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You know, in information theory we like to define this information.",
                    "label": 0
                },
                {
                    "sent": "Measures like entropy, mutual information, relative entropy and so on.",
                    "label": 0
                },
                {
                    "sent": "But at the end of the day, what we really like to do is to post questions.",
                    "label": 0
                },
                {
                    "sent": "That are of relevant to the engineer of relevance to the engineer or to the statistician, and then show that the answer to these questions.",
                    "label": 0
                },
                {
                    "sent": "These answers are really those information measures and so here I have listed five different problems for which the answer turns out to be the relative entropy.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first one is a large deviations type of question.",
                    "label": 0
                },
                {
                    "sent": "We saw before.",
                    "label": 0
                },
                {
                    "sent": "In what way are there and if entropy is related to the central limit theorem so large deviations relative entropy actually is where where it has thrived very prominently?",
                    "label": 0
                },
                {
                    "sent": "And in a nutshell, this is what happened.",
                    "label": 0
                },
                {
                    "sent": "What's happening now think of having two distributions P&Q and then this epsilon P. This is a little neighborhood of the distribution P. OK, so.",
                    "label": 0
                },
                {
                    "sent": "Think of this Y one through YN.",
                    "label": 0
                },
                {
                    "sent": "Alright, that's just a realization of, say, N IID random variables and what you could do is take the empirical distribution of this vector of random variables.",
                    "label": 0
                },
                {
                    "sent": "You take this empirical distribution assuming that the true distribution is Q.",
                    "label": 0
                },
                {
                    "sent": "That's why I have this big Q here.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we know from the law of large numbers then.",
                    "label": 0
                },
                {
                    "sent": "Eventually, this empirical distribution is going to converge to Q. OK, so now this is telling us.",
                    "label": 0
                },
                {
                    "sent": "What is going to be the probability that this empirical distribution is going to be in a little neighborhood of P?",
                    "label": 0
                },
                {
                    "sent": "Is going to be visiting a little neighborhood of P after N samples.",
                    "label": 0
                },
                {
                    "sent": "Alright, so if any is very small, 3 or 4, you know maybe there is a chance that you would be impersonating P by the sequence, but it's going to be very unlikely that you are going to be in a neighborhood of P when N is large because you should really be close to Q. Alright, so the relative entropy is actually gauging that.",
                    "label": 0
                },
                {
                    "sent": "That probability that probability is going to be exponentially decreasing in an the exponent is really the relative entropy.",
                    "label": 0
                },
                {
                    "sent": "So that's the difficulty of impersonating P by Q.",
                    "label": 0
                },
                {
                    "sent": "Now imagine that Q.",
                    "label": 0
                },
                {
                    "sent": "Matching that there are some.",
                    "label": 0
                },
                {
                    "sent": "Some letters in this alphabet that have zero probability according to Q.",
                    "label": 0
                },
                {
                    "sent": "And positive probability according to P. Well, there is no way that you can impersonate a distribution.",
                    "label": 0
                },
                {
                    "sent": "Did you can impersonate P with Q because no matter what you do, that empirical distribution of Y one through YN will never have positive probability in those letters, so you will never be able to impersonate P. You'll never be able to be close to P, and that's why relative entropy in that case is Infinity.",
                    "label": 0
                },
                {
                    "sent": "OK. You cannot do it.",
                    "label": 0
                },
                {
                    "sent": "Where is the other way around around?",
                    "label": 0
                },
                {
                    "sent": "You may very well be able to do it, maybe may very well be that if the true distribution is P then after a while you are still at a very very close to Q.",
                    "label": 0
                },
                {
                    "sent": "In that case DPQ is is a finite number.",
                    "label": 0
                },
                {
                    "sent": "OK, so this result is in a paper right turn off.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's interesting 'cause he attributes the result.",
                    "label": 0
                },
                {
                    "sent": "To Stein.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that the story is that then they ask sustain about this, and he said, I have no clue.",
                    "label": 0
                },
                {
                    "sent": "I have never seen this result.",
                    "label": 0
                },
                {
                    "sent": "And so you know it's called this time this time's lemma, and so on.",
                    "label": 0
                },
                {
                    "sent": "But then it turned out that what happened was the churn of submitted the paper, and then one of the reviewers said, oh, you know, I've seen this result in an unpublished paper by Stein.",
                    "label": 0
                },
                {
                    "sent": "So then turn off.",
                    "label": 0
                },
                {
                    "sent": "Just referenced this Stein thing even without even looking at it because it had not been published.",
                    "label": 0
                },
                {
                    "sent": "And so it's stuck like this time slimmer.",
                    "label": 0
                },
                {
                    "sent": "It's really do to turn off, and it's due to turn off in this paper.",
                    "label": 0
                },
                {
                    "sent": "Usually people reference the wrong paper by turn off.",
                    "label": 0
                },
                {
                    "sent": "In order to reference these results.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this one is a law of large numbers, type of characterization of relative entropy, and.",
                    "label": 0
                },
                {
                    "sent": "This is kind of like a bashan.",
                    "label": 0
                },
                {
                    "sent": "Type of analysis.",
                    "label": 0
                },
                {
                    "sent": "Say after you have several observations and the observations come from P. What is the reliability of rejecting Q when P is true?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What you're going to do is you are going to find the posterior probability of both hypothesis and then decide.",
                    "label": 0
                },
                {
                    "sent": "Cording to which one is larger?",
                    "label": 0
                },
                {
                    "sent": "So if you were to take.",
                    "label": 0
                },
                {
                    "sent": "That number, the ratio of the two posterior probabilities, take the logarithm and then divide by N. Then that actually is going to.",
                    "label": 0
                },
                {
                    "sent": "Two converts to the relative entropy of P&Q.",
                    "label": 0
                },
                {
                    "sent": "So in fact this is the kind of Bashan result than on patients can live with becausw.",
                    "label": 0
                },
                {
                    "sent": "It actually does not depend on the priority probability of the hypothesis, even though unless there is such a priori probability, you cannot define the quantities here at the end of the day it washes out OK, so if this is large then you would expect that it's going to be.",
                    "label": 0
                },
                {
                    "sent": "Unlikely that you're going to decide the wrong hypothesis.",
                    "label": 0
                },
                {
                    "sent": "You're going to do.",
                    "label": 0
                },
                {
                    "sent": "Be able to reject Q when P is true with high reliability.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now in information theory.",
                    "label": 0
                },
                {
                    "sent": "Or when we do lossless data compression, for example, we do Huffman coding, we have 1/2 man code that selects variable length representations for different elements in the source.",
                    "label": 0
                },
                {
                    "sent": "And that code is designed to minimize the average length, so you design it for some distribution Q.",
                    "label": 0
                },
                {
                    "sent": "But then suppose that the true distribution is P. OK. Well then you have to pay a penalty and that penalty is DPQ the relative entropy of P with respect to Q Now.",
                    "label": 0
                },
                {
                    "sent": "Notice that what I said actually is not quite what I have in the slide.",
                    "label": 0
                },
                {
                    "sent": "In the slide, I say the asymptotic excess rate, 'cause to be precise.",
                    "label": 0
                },
                {
                    "sent": "That result is not true.",
                    "label": 0
                },
                {
                    "sent": "Is not true for any length, it's only true asymptotically.",
                    "label": 0
                },
                {
                    "sent": "So the fact that this is the penalty for the mismatch in data compression is true in the limit as N goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "In fact, you may have no penalty whatsoever 'cause you may have two distributions that share the same Huffman code, so.",
                    "label": 0
                },
                {
                    "sent": "That way you see that there is really not necessarily any asymptotic penalty.",
                    "label": 0
                },
                {
                    "sent": "But you have IID distributions.",
                    "label": 0
                },
                {
                    "sent": "When N goes to Infinity and a Half Men, coats are all sufficiently far apart, then indeed you incur in a penalty.",
                    "label": 0
                },
                {
                    "sent": "Now here's another one that he.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do with reliable communication, but in Shannon's original formulation, channel capacity is the number of bits you push through the channel divided by the time, the time it takes to push the bits through the channel.",
                    "label": 0
                },
                {
                    "sent": "Now here imagine that instead of.",
                    "label": 0
                },
                {
                    "sent": "Instead of paying by the time you pay by the dollar or by the euro, so some symbols cost you 11 unit and some symbols are free.",
                    "label": 0
                },
                {
                    "sent": "Alright, so now what happens is that at the output you don't see what you send or you only see some distribution induced by the channel.",
                    "label": 0
                },
                {
                    "sent": "So the maximum number of bits per unit cost.",
                    "label": 0
                },
                {
                    "sent": "Then that's going to be deep Q the relative entropy between P&Q.",
                    "label": 0
                },
                {
                    "sent": "Alright, so in essence when?",
                    "label": 0
                },
                {
                    "sent": "When you normalize by cost rather than time and one of the symbols have 0 cost, that's important then.",
                    "label": 0
                },
                {
                    "sent": "Rather than having to maximize mutual information like we do in the formula for channel capacity, you maximize over relative entropy.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is the connection.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with estimation that.",
                    "label": 0
                },
                {
                    "sent": "That I alluded to before, so you saw that there is this.",
                    "label": 0
                },
                {
                    "sent": "This interpretation of relative entropy as mismatched data compression.",
                    "label": 0
                },
                {
                    "sent": "Well there is also according to this formula, and this is actually a very recent formula which I got published earlier this year.",
                    "label": 0
                },
                {
                    "sent": "You can write this relative entropy for arbitrary distributions P&Q.",
                    "label": 0
                },
                {
                    "sent": "As an integration of the difference between 2 mean square errors and then this integration is over signal to noise ratio.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Actually, I think I have another slide.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here that gives these in a little bit more.",
                    "label": 0
                },
                {
                    "sent": "Detail although I'm running out of time, so here we have the mean squared error.",
                    "label": 0
                },
                {
                    "sent": "Of course you all know it's attained by the conditional mean.",
                    "label": 0
                },
                {
                    "sent": "Alright, so if the noise.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This Gaussian if I observe this X in Gaussian noise, then I can write that conditional mean as the ratio of two integrals.",
                    "label": 0
                },
                {
                    "sent": "But now imagine that I have in mind the wrong distribution for X. Alright, so I have a mismatch in my msee estimator.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this would be the case.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This would be the case where X is Gaussian, in which case what you have is of course a linear estimator.",
                    "label": 0
                },
                {
                    "sent": "That would be the case when X is.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Equippable then you have hyperbolic tangent.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'm going to the.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, the MSE at a particular signal to noise ratio.",
                    "label": 0
                },
                {
                    "sent": "By this notation MSE.",
                    "label": 0
                },
                {
                    "sent": "Now mismatch estimation.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is when?",
                    "label": 0
                },
                {
                    "sent": "And this MSE subcu tells us that the distribution I'm thinking is true is Q, but in fact the X is distributed according to P. So in fact this MCQ actually also depends on P because P is the truth and Q is what I think it is alright.",
                    "label": 0
                },
                {
                    "sent": "So the MSE would be the MSE when I think.",
                    "label": 0
                },
                {
                    "sent": "P is the true one, and it is indeed the true one.",
                    "label": 0
                },
                {
                    "sent": "Alright, so then.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The most common case of mismatch estimation is when I think that Q is Gaussian, because then the estimator is the linear estimator OK, and in that case of course you all know that MSQ then does not depend on the actual distribution of X.",
                    "label": 1
                },
                {
                    "sent": "Right, because the the mean squared error is.",
                    "label": 0
                },
                {
                    "sent": "Since it's a linear estimator, is only going to depend on the 2nd order properties of X. OK, so then the relative.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Entropy and the MSE as I said before, I related through the integral of this mismatched MSE.",
                    "label": 0
                },
                {
                    "sent": "And of course, by definition, then each of the mismatch intimacy is greater than the minimum mean square error.",
                    "label": 0
                },
                {
                    "sent": "So in this formula in this formula you do see that relative entropy is negative when Q is not equal to P, whereas in the original formula you have to do Jensen's inequality in your head.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think I'll stop here.",
                    "label": 0
                },
                {
                    "sent": "And just maybe to wrap up.",
                    "label": 0
                },
                {
                    "sent": "Just to say that relative entropy has been very slow in taking its rightful place in information theory and statistics and probability, I think people realized much earlier on than it was very important, but I think now nowadays is becoming more and more important, and we're finding all these new connections that.",
                    "label": 0
                },
                {
                    "sent": "Up to now, where were hidden and some of the open problems that had been open for awhile have only been shown recently.",
                    "label": 0
                },
                {
                    "sent": "Thank you to these thanks to these.",
                    "label": 0
                },
                {
                    "sent": "A connection with estimation, so thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Time for a few questions for Sergio.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll start off.",
                    "label": 0
                },
                {
                    "sent": "I had one question.",
                    "label": 0
                },
                {
                    "sent": "When one person whose name didn't come up is ready, I was wondering if you could say a little bit about Renyi entropy and its position in place in this discussion.",
                    "label": 0
                },
                {
                    "sent": "So yeah, thank you.",
                    "label": 0
                },
                {
                    "sent": "So in essence, rainy.",
                    "label": 0
                },
                {
                    "sent": "Yeah he had a lot of different contributions in this area, but.",
                    "label": 0
                },
                {
                    "sent": "One thing that he did, he said well when we.",
                    "label": 0
                },
                {
                    "sent": "When we defined entropy, maybe we could change that logarithm for something else.",
                    "label": 0
                },
                {
                    "sent": "And as long as we have a function that retain some of the analytical properties, then there might be something interesting that we can say, and indeed he defined the rainy entropy and then from there you can also define the rainy relative entropy and so on.",
                    "label": 0
                },
                {
                    "sent": "And there have been some what we say.",
                    "label": 0
                },
                {
                    "sent": "Operational meanings of that rainy entropy, rainy mutual information, but.",
                    "label": 0
                },
                {
                    "sent": "By and large heavens, it hasn't been that successful in information theory, there have been a handful of results, so at the level of proving properties, like for example, the data processing theorem that I showed, you can have at the output less divergences at the input, and so on.",
                    "label": 0
                },
                {
                    "sent": "All that works through, but at the level of actually solving problems that are of interest to the statistician of the engineer, it has not been.",
                    "label": 0
                },
                {
                    "sent": "Was successful.",
                    "label": 0
                },
                {
                    "sent": "Of course, there's also been the whole issue about F divergences, Alice Silver Divergance and so on were again, the logarithm is replaced by another quantity, and from time to time these things do yield some new insights, but much less than this, and of course the basic reason is simply that the logarithm of a product is the sum of the logarithms.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's a good question.",
                    "label": 0
                },
                {
                    "sent": "Who was the first one to prove it?",
                    "label": 0
                },
                {
                    "sent": "Home.",
                    "label": 0
                },
                {
                    "sent": "Well, I think it's it's inherent in callback labeler.",
                    "label": 0
                },
                {
                    "sent": "That you know, they know that the.",
                    "label": 0
                },
                {
                    "sent": "The information for this combination, actually they they show several and they say they don't have a theorem that says this is not negative, but they do say it is non negative so they obviously I don't know whether it is so using Jensen's inequality or using the fact that the lower is is upper bound is lower bounded by 1 -- 1 / X.",
                    "label": 0
                },
                {
                    "sent": "You can also prove it that way.",
                    "label": 0
                },
                {
                    "sent": "But who knows, maybe I assumed that world when he used it.",
                    "label": 0
                },
                {
                    "sent": "He probably also notice that.",
                    "label": 0
                },
                {
                    "sent": "One use of mutual entropy in in this community has been as a training loss for regression problems so.",
                    "label": 0
                },
                {
                    "sent": "Minimizing the mean squared error.",
                    "label": 0
                },
                {
                    "sent": "There's also been work.",
                    "label": 0
                },
                {
                    "sent": "Minimizing the conditional entropy.",
                    "label": 0
                },
                {
                    "sent": "I was wondering if you reside on the connection between these two.",
                    "label": 0
                },
                {
                    "sent": "Sorry if you.",
                    "label": 0
                },
                {
                    "sent": "I was wondering if your new result.",
                    "label": 0
                },
                {
                    "sent": "And the relationship between me and conditional entropy shed any light on the relationship between these two very different ways.",
                    "label": 0
                },
                {
                    "sent": "Question, that's a good question.",
                    "label": 0
                },
                {
                    "sent": "I don't know the one.",
                    "label": 0
                },
                {
                    "sent": "Was actually quite surprising is that.",
                    "label": 0
                },
                {
                    "sent": "This results on.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to go fast, but I guess this results in limits on MSE, an mutual information lead to.",
                    "label": 0
                },
                {
                    "sent": "Result it has nothing to do with information theory and it's in continuous time nonlinear filtering, so the idea there is that we can we have a general formula that relates the non causal.",
                    "label": 0
                },
                {
                    "sent": "Minimum is mean square error and the causal minimum mean square error.",
                    "label": 0
                },
                {
                    "sent": "So regardless of whether the input is Gaussian or not, if you observe it in Gaussian noise in white Gaussian noise, if you give me the causal minimum mean square function for all signal to noise ratio.",
                    "label": 0
                },
                {
                    "sent": "The non causal one and vice versa.",
                    "label": 0
                },
                {
                    "sent": "That's that's the kind of result that.",
                    "label": 0
                },
                {
                    "sent": "Important, but we should be able to.",
                    "label": 0
                },
                {
                    "sent": "You know, it's tough in estimation and nonlinear filtering, but the only way we know how to prove it is through mutual information.",
                    "label": 0
                },
                {
                    "sent": "So perhaps you know something like that can happen also.",
                    "label": 0
                },
                {
                    "sent": "OK well.",
                    "label": 0
                },
                {
                    "sent": "I think you'll be around.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure couple days.",
                    "label": 0
                },
                {
                    "sent": "So if people would like to discuss things further with Sergio, he'll be around at the conference.",
                    "label": 0
                },
                {
                    "sent": "And why don't we give him a nice thank you again for a great collector?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Sir.",
                    "label": 0
                }
            ]
        }
    }
}