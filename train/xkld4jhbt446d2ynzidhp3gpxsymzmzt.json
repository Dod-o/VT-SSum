{
    "id": "xkld4jhbt446d2ynzidhp3gpxsymzmzt",
    "title": "NLP and Deep Learning 2: Compositional Deep Learning",
    "info": {
        "author": [
            "Christopher Manning, Computer Science Department, Stanford University"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_manning_deep_learning/",
    "segmentation": [
        [
            "So what I wanted to do in this afternoon session is talk about the idea of."
        ],
        [
            "Oppositionality in deep learning systems.",
            "I think this is.",
            "I think this is an important issue for the development of richer learning models and deep learning, which is only really just being begun to be explored and so."
        ],
        [
            "I'd like to advance this notion that artificial intelligence requires being able to understand bigger things from knowing about smaller parts.",
            "Now this is clearly not the only thing that artificial intelligence involves, but I think it's one of the large ways that human beings possess intelligence that once you start to figure out parts of things, you can then reuse those parts to understand bigger things, and you have an accumulative effect of being able to be more intelligent specifically.",
            "Going back to natural language.",
            "The machine of the machine is going to be artificially intelligent.",
            "Is that the answer?",
            "But by analogy, I think humans do the same process.",
            "OK, so getting back to the case of language.",
            "I mean, if we're really going to make progress and having natural language understanding and systems that work well on human languages, we can't just have good representations of words and word embeddings that we need to also be able to deal with larger semantic units.",
            "So the kind of things would like to know is how can we tell when bigger units than just words are similar in meaning.",
            "So how can we tell?",
            "That the snowboarders leaping over the mogul is similar in meaning to a person on a snowboard jumps into the air that we want to be able to workout, meaning relationships of various sorts between phrases, clauses and sentence is, and I think it is man."
        ],
        [
            "The face that the way people work out the meaning of larger text units is by means of semantic composition of the smaller elements.",
            "We know about the meanings of words and of the way meanings combine into phrases and clauses, and we put that together to understand the meaning of sentence is.",
            "And so, in our deep learning context, that suggests that not only do we want to have word."
        ],
        [
            "Does but would also like to be able to represent phrases and sentences as vectors, so as well as having a word space web Monday and Tuesday are close together in France and Germany are close together.",
            "We'd like to go beyond that and say, well, we have phrases like the country of my birth and the place where I was born.",
            "Can we extend our neural network ideas to Fraser's, and the idea is, well, maybe we could also map those into our vector space and that they should turn up in their location.",
            "That is somewhere close to where single word locations occur, and So what we can do is want to develop models that allow us to do that."
        ],
        [
            "And so, how might we do that?",
            "Well, the idea I'm going to propose is this idea that we exploit the principle of compositionality.",
            "I mean, this isn't something special to deep learning.",
            "Essentially all theories of natural language semantics build on this idea of the principle of compositionality as human beings put together.",
            "New sentence is everyday when they speak, they have never been heard before, and so we can only have a model of how humans understand the novel sensors that they read or hear each day.",
            "If they can workout their meanings in terms of the parts that they're made from, and So what we're going to say is that the meaning here, the vector of a sentence is determined by knowing that the meanings of the words and we talked about those this morning, and then having a method that combines those meanings.",
            "So what we're going to want to do is have a procedure where we can then calculate the meanings of Fraser's from the meanings of words.",
            "And I'm going to discuss a number of ways in which we've tried to do some of that.",
            "I'm particularly interested in language, but this idea of compositionality and making use of it for understanding doesn't only apply to language."
        ],
        [
            "So when we look at complex equations, we use an idea of compositionality, and we can split that up and say, well, there's this normalizing bid up.",
            "The front that divides off, and then when I look at this spirit, I can split that up again that this is measuring distance from the mean, and that's then being turned into a Gaussian, so that has subparts, and it's this compositionality that lets us understand equations, but also when we look at vision."
        ],
        [
            "Pictures also have parts, so when we look at a scene like this that we can see the various parts that is made of that there's a church and it has a roof and the second story in the first story and the second story has Windows Insider and there's a group of people which comprises of parts of the individual people and the human beings have their parts as the body parts, and so you might want to compose the meaning of a visual scene as well.",
            "And so the general question is can."
        ],
        [
            "We come up with good ways to build meaning composition functions in deep learning systems and in this section I'm going to explore a particular approach to doing that and it built."
        ],
        [
            "Also on this conjecture.",
            "So one way you could think about doing things is that you could attempt to model larger units of language with a simple uniform architecture.",
            "Something like a sequence model or 1 dimensional convolutional neural network.",
            "But the conjecture that I want to explore is that maybe we can produce a better meaning composition function for language by modeling an input specific compositional tree.",
            "And so what's the distinction here?"
        ],
        [
            "So for a sentence like the cat sat on the mat well, one possibility is that building from the word vectors, we could build something like a uniform generic structure as you get for one dimensional convolutional neural network, that doesn't seem very natural for semantic composition because we end up with nodes in the center nodes in our convolutional network that represent our arbitrary fragments of structure this.",
            "No represents cat sat on that.",
            "That doesn't make much sense if we're thinking of wanting to build up the meaning of a sentence from its parts, it seems to make more sense to say, well, what are the actual natural parts of which this sentence is comprised, and to build up from."
        ],
        [
            "Those parts, so we have a noun phrase of the cat, and we have announced the noun phrase of the mat and that goes into a prepositional phrase of on the mat, and then the sitting is on the mat and it's the cap that's sitting on the mat.",
            "In other words, we're producing a syntactic structure for the sentence.",
            "And then what we will want to do so you can put the sentence in, and so now the nodes that we have represent the natural semantic units of the sentence, and so we will compute.",
            "A meaning of the sentence by doing composition on that representation.",
            "OK, so you know if I just give a balanced position at the beginning going in this direction isn't necessarily right.",
            "In fact, I'm not even.",
            "I suspect it's something that only a minority of people interest in deep learning would subscribe to.",
            "I think the majority of people in deep learning have a bias towards the simplicity of architecture and would prefer to have some simple, universal, simple uniform architecture.",
            "Like a sequence model and just say give it a lot of data and the neural network will learn.",
            "I will try and present at least a little bit of evidence that it seems that sometimes these kind of input dependent compositional structures do give you an advantage, and essentially what this issue connects into is to what extent should we be making you serve."
        ],
        [
            "Stronger priors or universals of language in the models that we build.",
            "So in the area of language in particular, the issue of sort of priors and universals of language is a very controversial issue.",
            "So on the one hand there's Noam Chomsky in the 1950s, pretty much before any machine learning existed decided that language learning was impossible, and therefore the entire of language structure had to be innate, and he's refused to change his position in any way.",
            "In the intervening 65 years, and so there's this strong argument for the indebtedness of language.",
            "On the other hand.",
            "That there's a very strong bias of mung many in the empirical machine learning community to want to think that there's no substantive priors over things like language and will just sort of feed in a lot of data, and the models will learn.",
            "And I think that actually there are very reasonable middle grounds where there probably are bias ease priors in human brains, and how they work that really do direct in particular.",
            "Directions how learning occurs and therefore their existence could aid learning, and presumably we might want to use those priors and machine learning models as well.",
            "And many of these things are things that you can empirically notice and aren't just sort of dictates of Chomskyan universal grammar, and so I thought I'd just show one example of this that comes from some of the work of Paul Smolensky and students."
        ],
        [
            "And so way back in the 1960s at Stanford, actually, Joseph Greenberg was an early descriptive linguist who did his own very influential work on universals of language and his approach was purely empirical.",
            "That was sort of looking at large connections of collections of languages and how that he behaved, and looking for patterns.",
            "And so he developed a theory of length of statistical language universals of likely patternings that you have.",
            "And so one pattern kind of patterning he looked at with things like word orders.",
            "So if you have a noun, nouns can normally be modified by adjectives, an numbers, and what you find is that by far the most common thing and languages is you get what's called the harmonic patterns where both of these dependants occur on the same side of the noun.",
            "So you get both numbers and adjectives before the noun.",
            "Or actually much more commonly than that, you get both numbers and adjectives after the noun, but both of them occur on the same side.",
            "I'm not sure I have as good an explanation for this, but if you get different orderings as this strong preference to put the number beforehand and the adjective afterwards, I mean, maybe you can tell some kind of functional story as to why that's preferable to get the count in earlier when expressing ideas at the time that Greenberg did his work.",
            "In the 60s, he knew of no languages.",
            "They were down in this 4th quadrant, and so he sort of had that as an impossibility.",
            "But some have since been discovered, but languages down there are very, very rare, and so this suggests perhaps you could just think that this is a fluke of how human languages ended up, but it seems it is actually more than that, and that there's some kind of substantive learning bias that humans beings have that prefers these kind of orderings.",
            "And so that's something that's been."
        ],
        [
            "I'm exhibited in some experiments by Julie Cuthbertson Culbertson and this particular set of results.",
            "I'm going to show you is with experiments on adults, but she's also done experiments with children.",
            "So here what people were being was happening was people were being given artificial language data of nouns modified by adjectives and numbers, and in the input they were given.",
            "There was the predominant word order.",
            "Which is which side of the now?",
            "And you're putting the adjective and the number.",
            "But it was never fully consistent.",
            "In fact, in the data that they were given, it was always the case that for both nouns and adjectives, 70% of the time the word appeared on one side and 30% on the other side, and so people were exposed to data of this sort for awhile, and then they were asked to produce data that they had sort of learned about phrases in this imaginary language and what they meant.",
            "And then they were asked how do you?",
            "How would you say in this language some other freezers and they would then recording where they put the adjective and the number on which side of the noun and when they did this experiment, really interesting results emerged which reflect this same bicis that you see in the distribution of human languages.",
            "So the data that the humans were actually given in the experiment was.",
            "For the artificial languages, was always at one of these points, reflecting the fact that 70% of the data was on one side and 30% on the other side.",
            "But when the humans then produce their own examples of the language, almost invariably they moved away from there and that they presented.",
            "Presented a more marked ordering of putting a category on one side or the other, which is a common phenomenon you also see in child language acquisition.",
            "Children want to regularize the languages that they hear, so they moved to a more regular stance than the language that they're hearing around them.",
            "So for people who are hearing the language where the adjective was after the noun, 70% of the time and the number was after the noun 70% of the time.",
            "Nearly all of them were moving to a more extreme position where it was almost always in that order.",
            "But note that there's this really interesting exception for the people who heard the language, which was the one that isn't meant to occur or barely occurs in human languages.",
            "None of these people regularize to having that pattern marked even more strongly.",
            "All of them actually moved it away from that corner of the quadrant.",
            "So it seems like there really is some strong pre programmed human bias to not like to not like having the language type where you get adjectives before the noun and numbers after the noun.",
            "I believe you know, I'm not quite sure I suspect so I suspect it will be your average first year psychology students in an American University.",
            "Yeah.",
            "I mean, it's.",
            "So you can't.",
            "I can't prove clearly there could be interference.",
            "On the other hand, people happy to produce languages of the other three types, right?",
            "And English is only one of these types and not the most common.",
            "Yeah.",
            "Tried.",
            "Not that I know, yeah.",
            "Examples.",
            "Yeah.",
            "Yeah, but I mean, I think the point I want to make if you just regard this as machine learning, it is trivial to build a learning device that would precisely learn the data which are exposed and you'd hear this and you'd reproduce that, right?",
            "That's not tricky.",
            "You guys could all do that.",
            "What's interesting is that it seems like that's not what the human language learning device is doing.",
            "It's instead.",
            "Both having a general preference to regularize and the preference to keep away from this corner and to the extent you know.",
            "Regardless, this might be just sort of an artifact of how human language learning works or something, but to the extent that there are those strong biases in human language is possible that if we're wanting to build a machine learner of language, we'd want to have it have similar biases.",
            "OK, so that was sort of that was sort of an aside getting back to the main point.",
            "I mean, what I'm?"
        ],
        [
            "Interested in is what we want to do, meaning composition and we're going to assume that we're building up meanings by composing the units of a sentence that are meaningful using this kind of input dependent tree structure."
        ],
        [
            "That raises the obvious."
        ],
        [
            "Fashionista where did these tree structures come from?",
            "Here are three answers.",
            "I mean, one answer is you could just be using a conventional statistical parser to get tree structures for sentences such as our Stanford parsers, PCF.",
            "G. And sometimes we've done that.",
            "A second answer is you could do a neural network parser, and I guess recently we've actually worked on a neural network dependency parser and I'll show a very brief advertisement for that next.",
            "But both of those are sort of a prior stage of processing before then working out meaning.",
            "And this is actually something that's also debated in the human language.",
            "Understanding literature, to what extent is that true that humans first pass syntactically and then interpret semantically versus the two of those being closely tide together?",
            "Well, the third possibility is that you could be building up the tree structure at the same time as you're building up the meaning in a tree recursive neural network, and that's something that we've also done some work with.",
            "But mainly you have actually done one or two and.",
            "You'll see that in quite a bit of the work that I present."
        ],
        [
            "Here's my brief advertisement for the neural Network Dependency parser.",
            "So in general in recent years in NLP there's been this huge use of transition based dependency parsers, which are dependency parsers that, in a greedy fashion, head through a sentence from left to right, and so they maintain a stack of what they built so far in a buffer of words coming up, and then at that position they decide from that configuration what their next move is.",
            "And the conventional way of doing that is you build a linear classifier over a huge bunch of features and you use lots of feature templates and what people have found is that making the right decision strongly depends on having interaction terms in the model where you're taking three or even four elements, and considering jointly if you're in that configuration, what you should do, and so as a result of that you get these kind of very complex features.",
            "In the models and so that they have multiple problems, they have the problems of sparsity of features you haven't haven't seen before, and so on.",
            "But if you just take the engineering perspective, something that's kind of interesting is that in conventional linear classifiers, having these kind of features turns out to be what makes this.",
            "The parse is somewhat slow, so on the one hand people like these transition based dependency parsers because they've.",
            "Fast, much faster than conventional dynamic program parsers, but if you actually look at what they're spending their time on doing, their spending most of their time actually computing these features so that's the slowest part of their operation, because the actual classification operation is exceptionally quick."
        ],
        [
            "And so.",
            "Last year Dan Qichen and I worked on a neural network based dependency parser, which is then using distributed representations of not only words but also part of speech tags and dependency labels, and as then putting that through a neural network structure to predict doing things, and it can be faster precisely because there's not doing that feature combination."
        ],
        [
            "Step and so here are just a couple of the results from that.",
            "So Malt parser is a conventional transition based parser and these are sort of accuracy numbers out of 100 unlabeled and labeled and it's quite fast.",
            "There are other kinds of dependency parsers that use graph algorithms which are slightly more accurate, but way way slower.",
            "And then there are pauses that.",
            "Do a teeny bit better and get slower again, but the really nice thing is using this neural network dependency parser, you can build a parser that's actually faster than the conventional parser, because that doesn't do the feature computation But basically works as accurately as very strong dependency parsers, and so we thought this was a pretty good result, but our dear friends at Google thought that this was such a good architecture that they could make it but.",
            "They could make it better, so they decided to sort of, you know, through their thousand GPU's at it and through through a combination of extremely extensive hyperparameter optimization, increasing the dimensionality of the representations used an up, training it on further data they've actually managed to push these numbers up so they've now I've got the currently best performing dependency parser.",
            "Using essentially that same simple neural net architecture."
        ],
        [
            "OK, yes.",
            "I mean, there's certainly yeah, there's certainly absolutely people working on free structure parsing, but you know, nevertheless, I think it's fair to say if you look at the last 10 years of history in NLP in NLP, there's been this enormous sea change in the direction of people preferring dependency grammars independency representations, which hasn't happened in linguistics, at least not in this country.",
            "Which is very still very dominated by phrase structure representations.",
            "OK, so back to meaning, composition and so."
        ],
        [
            "Wanted to sort of show you some of the kind of models that we've tried to develop for, meaning, composition, and so the basic idea is that we're developing these tree structured recursive neural networks where we got some tree typology and we're going to compute using a neural network of representation for a parent node based on inputs from the children being run through a neural network, and this idea of tree RN ends is actually developed in the 90s.",
            "By Golon Kushla, who actually developed all the math of the basic tree RN ends as well, but like a bunch of other ideas and deep learning, it's only more recently it seemed to be a successful buildable things, OK?"
        ],
        [
            "So the first version that we did was a simple concatenation tree RNA, and so we had representations for the two children which might be words already Frasers.",
            "We concatenated those two vectors together.",
            "We multiplied them by a weight matrix, so there would be a kind of a 2 N by N matrix for our dimensionality.",
            "So we reduce back to the dimensionality of a word vector, and then we put that through a non linearity like 10 H. And then we get out the output of our network and this already worked.",
            "Kind of reasonably."
        ],
        [
            "So here are just a few results where we then use this model to assess semantic similarity between sentence is where we calculated for sentence it's semantic representation by building up through this structure using one of these train network and then asking for a particular sentence, what sentence is are most similar to each other and it's doing a fairly decent job, so Knight Ridder wouldn't comment on the offer.",
            "Harsco declined to say what country placed the order, not exactly the same, but you know there's sort of this.",
            "Same kind of discourse function of declining to comment.",
            "Coastal wouldn't disclose the terms that seem even slightly better.",
            "It just seems like it's doing a pretty decent job of representing a semantics of a sentence inside a vector.",
            "I mean, on the other hand, this version isn't actually that great."
        ],
        [
            "So I mean, if this is our model, why is this not a very satisfactory model of meaning composition for human languages?",
            "Any ideas?",
            "The two things shouldn't be confined, yeah?",
            "Right, yeah, so so in this model we just kind of concatenate the two vectors and then we multiply it by matrix.",
            "So that kind of does some kind of linear transformation in the space.",
            "But there's nothing that really combines their meanings together.",
            "So unconventional semantics, a lot of what we're doing is having one word take another as its argument and building meaning of the composite.",
            "So if you think of a clear case like kind of very sleepy.",
            "It seems like you have a notion of sleepiness and various taking.",
            "Sleepiness's argument is intensifying it to produce very sleepy, and we can't do anything like this here.",
            "'cause we just got a very word vector which is concatenating on.",
            "Then we're doing some linear transformation, stick it through the non linearity, but we don't have any way that the words really interact with each other."
        ],
        [
            "OK, so that was bad.",
            "I mean this one other thing, that sort of bad as well, which is our composition function is just a single weight matrix.",
            "I mean, even if we weren't fixing the problem of the words interacting with each other, you might think that having a single weight matrix just can't possibly be good 'cause there are different structures in language, and it seems like depending on.",
            "What you're combining you'd want to do something different, so if you want a representation.",
            "For open door that's mainly about it being a door, it seems like where if you had something like slept uneasily.",
            "That's mainly about sleeping, so you know even which of the left or the right side you should be paying most attention to is going to vary depending on the phrase.",
            "OK, no interaction, so it seems like that's not really adequate for human language composition function.",
            "I mean, of course they showed it did sort of work a bit, forgetting those meaning similarities.",
            "And I think part of why did work some what is that simple?",
            "You know a baseline method for putting together word meanings.",
            "Get a sentence.",
            "Meaning is simply to do vector averaging.",
            "You just take the vectors for all the words and average them together.",
            "And you know that actually sort of works to a certain extent because most of the time you get this sort of additive combination of word meanings.",
            "That does give you a sort of representation of the meaning of the sentence, and so this model is a bit better than that.",
            "Because it can learn a simple kind of linear transformation, composition function and so therefore can do something, and so we wanted to do better than that, and so the 1st.",
            "Thing you could want to deal with is this easy.",
            "One composition functions, a single weight matrix, and so that's something that we did in some later work."
        ],
        [
            "I'm for passing and so this the idea of this work was that maybe we could separate out syntax and semantics in a fairly reasonable and productive way and use our neural network to focus on semantics.",
            "But because we have a separate syntax, we can then have different weight combination matrices depending on what we're combining, and so the idea is if you think about statistical parsing.",
            "Conventional methods, probabilistic context free grammars.",
            "They do a pretty good job and modeling basic syntax of knowing what kinds of sentence structures are possible or good or not.",
            "The place that they fall down is that they don't have a sufficiently good model of semantics.",
            "They don't know how likely it is that certain phrases will modify other phrases, or whether certain things are likely to be combined together in a conjunction or not.",
            "So what we could hope to do is use.",
            "The neural network to be modeling semantic ability to combine and to be choosing out of the Pozas licensed by a syntactic PC FG and well.",
            "If we do that, we could build is a syntactically untied recursive neural network where we use different.",
            "Weight matrices based on what categories we combining.",
            "So for each word and phrase it now has both a vector representation and a category, which might be noun or verb, noun phrase, verb phrase.",
            "So then when we're combining two things together, we know that we're combining together, say, an adjective and a noun, and therefore we can use a similar kind of tree recursive neural network.",
            "But we can use the particular weight matrix.",
            "Which is the one for combining together adjectives and nouns.",
            "And so that actually worked quite successfully to give us a good parser."
        ],
        [
            "So here are some numbers on parsing, so the numbers at the top, the traditional Stanford parser, which is a bit off the pace of leading pauses these days and then here are some of the other, more modern parsers affected.",
            "PC FGS Collins parser Berkeley parser which is pretty much the best standalone English parser to this day.",
            "And simply using kind of tree RNN pozza doesn't work great, but if we can, if we instead moved to this syntactically untied model where we using a combination of the PC FG with recursive neural network parser that that allows us to produce a very competitive parser, slightly better than the Berkeley parser, there's some some other pauses that are better.",
            "Again that these are basically ones that do some kind of self training or re ranking.",
            "On more data.",
            "So that was kind of nice, and if we look at the."
        ],
        [
            "Matrices that we learn here.",
            "You can actually see that they do learn something.",
            "About what is the, what things to pay attention to in sentence is so these are the the weight matrices that are multiplying the two children so that they end by two N matrices and the thing that you need to know is we start off by pre initializing them with this sort of double diagonal prior and that's reflecting the fact of if you don't know much else, the best thing to do is just the average the two.",
            "Word vectors together, but then they're learning from there, and so we've kind of got from dark blue to red for weights their way from normal.",
            "So this is when you're combining a noun phrase with the conjunction the man and, and so it knows to mainly pay attention to the left hand side.",
            "The man.",
            "Here's where we've got a possessive pronoun.",
            "A noun phrase.",
            "So this is something like her briefcase, and it knows here that briefcase has most of the semantics attached."
        ],
        [
            "Do it if you have some things like adjective, noun, modifyers, then it knows that both sides are important, so it's something like a plastic trail.",
            "You want to know about the plastic and the trial, and that's in contrast to when you've got to determine a renowned where.",
            "It's the noun that's important, and the and the determiner isn't very important.",
            "OK, so that was kind of nice and worked better and I've got a question.",
            "So no, but I think your observation is completely right, and that would be a good idea to do.",
            "Yeah, so in this work there were matrices of weights.",
            "I mean the kind of idea could think of exploring as modeling these with reduced rank matrices, and so that you could factorize them and have a reduced rank factorization.",
            "So you have a smaller number of parameters I mean.",
            "With the wisdom of hindsight, it's totally obvious if you look at these pictures that we suffer quite badly from the undertraining of parameters, and in this example that I show here, that's most obvious here, so this is a phrase combination that's relatively unusual, an adjective being modified by an adverb phrase, and that's pretty uncommon, because if the adverb phrase is only one word long, it has another structure that I don't have up here, and so they're relatively few of these.",
            "And So what you can kind of see is that this is barely walked away from its initialization because it hasn't been seen often.",
            "So yeah, good point, definitely true.",
            "OK, so that allowed us to make some progress because we could sort of model different weight matrices for different configurations and that actually seemed to be pretty useful from proving parsing, but it still isn't allowing the two word vectors to interact to produce the meaning of the whole, and so we wanted to be able to do something of that sort."
        ],
        [
            "So that leads us into version three, and as you noticing in the green, I mean all of these versions were being primarily developed by Richard, so she is going to be around later in the week presenting stuff so you can also hear more from him then.",
            "So if you look at conventional semantics for natural language as the dominant model is you have functions with arguments.",
            "So if you have a verb like give, you'd modify model give as a function that takes 3 arguments.",
            "John gave the dog a bone, or if you have modifyers like very, very happy, very would be a function that takes as argument the adjective whose meaning it's going to modify.",
            "So if you then move into the.",
            "Vector Space world, it seems like the obvious equivalent for having functions and arguments is that if you have a matrix, then the matrix connect like a function on a word vector as its argument.",
            "However, there's the problem that in this world, depending on the sentence is not quite clear which words are going to be the functions and which of words are going to be arguments.",
            "Depending on the structure.",
            "So the way we dealt with that is by for each word.",
            "It has both a vector representation and the matrix representation, and so then when we workout a representation for a phrase, we're going to do that by making use of weight matrices and both the vectors we gotta wait matrix and then we're using the matrix times the vector both ways around, so it's considering each one is the function and the argument, and then is using a weight matrix.",
            "To learn which of those choices is a good choice or how to Mac mix them and so that will give us a new vector representation for the parent node.",
            "But we also."
        ],
        [
            "Workout a new matrix representation for the parent node.",
            "So we're taking the two matrices of the children and combining them with the new weight matrix to make a matrix representation of the parent.",
            "OK, and we were able to do some useful things with this as well, so one."
        ],
        [
            "Example of a task that we did with this is learning about semantic relationships and classifying them.",
            "So the idea of this task was that you wanted to learn about different semantic relationships that occur in the center and such as component whole relationships.",
            "So in my apartment has a pretty large kitchen apartment and kitchen are in a component whole relationship and so we're doing that by building up a representation for the sentence with this matrix vector model.",
            "And then at the node where the two words of interest joined together, we were then having a classifier that then classified if a relation held between the two words, and if So what relation it was and so."
        ],
        [
            "It worked pretty well for us, so this was a shared task that a number of systems had worked on previously.",
            "So simple system got about 60% right?",
            "The best system in the shared task got 82.2% right?",
            "Simply using our basic RNN got stuff out 75%, which was mediocre.",
            "Moving to the matrix vector R&N as a pure model got significantly better than that and then.",
            "By throwing in some of the extra kind of linguistic analysis, not as much as this system used, but some information about named entities and Wordnet semantic classes were able to build that up to a state of the art system.",
            "So that was pretty good.",
            "But it seemed like."
        ],
        [
            "There was some flaws to this system which actually relate again to your question from a minute ago, which is estimating the parameters of these matrices.",
            "So the bad thing that happened was that for each word, rather than just having a vector, it also has a matrix attached to it, and that's a lot of parameters.",
            "And again, if we were doing this again, there are other ways that we might think of dealing with this about how using reduced rank representations of those matrices so that the less parameters involved.",
            "But we didn't actually do that at the time, so we just had the matrix of parameters and the only way we dealt with things is by reducing the size of the vectors.",
            "So we're using 25 dimensional vectors here to reduce the number of parameters.",
            "But that was sort of doubly bad becausw.",
            "Well, everything we know about word vectors shows that you do better by using bigger word vectors than 25 dimensional vectors, but even with that size we've still got 625 parameters.",
            "In there, which is a lot, and it's doubly difficult fact we didn't have a good way of pre training the matrices, so there are lots of good ways of pretraining the word vectors like I talked about this morning.",
            "Methods like word to vec, it's very easy to train on huge amounts of text data, but we didn't have a similar method for pretraining the matrix parameters.",
            "So far.",
            "Move."
        ],
        [
            "And on to version four, what we were interested in was having a way for the two word vectors to interact with each other without suffering from the massive increase in the number of parameters of the matrix vector model."
        ],
        [
            "And so that thing lead into the recursive neural tensor network, and I'll explain a little bit about that.",
            "I'll go through it and main."
        ],
        [
            "Place in which we demonstrated this isn't looking at the task of sentiment detection, so this is working out whether a piece of text is positive, negative or neutral.",
            "I mean to a certain extent's sentiment analysis can be a pretty easy problem, so if you've got a piece of text and you see words like love, great, impressed, marvelous, you don't really need more than a bag of words model to say.",
            "Well there are lots of positive words in this piece of text.",
            "Therefore, it's probably a positive piece of text and to some extent that's true that if you have long documents, you can use big words, classifiers and they work pretty well, but that's much less true for short documents, and it's much less true for certain styles of writing, such as movie reviews.",
            "So on Rotten Tomatoes you find snippets like this with this cast, and this subject matter.",
            "The movie should have been funnier and more entertaining, so the only kind of.",
            "Obvious bag of words clues funnier entertaining, both of which are positive words.",
            "But what you're meant to take away from this snippet is that the person didn't think much of this movie.",
            "So if we're going to be able to get the sentiment classification of these kind of examples right, it seems like we're going to have to do more.",
            "We're actually going to understand the meaning of sentence is and have some kind of decent method of meaning composition.",
            "And there is a question.",
            "Well, that's another example, right?",
            "Yeah, so treatment of negation is another reason for why we want to do semantic composition.",
            "So we if it says not that funny that that counts as a negative rather than being the word funny in a bag of words model, yeah, absolutely.",
            "OK, so.",
            "Yeah, good point.",
            "Yeah.",
            "So it turns out that for a lot of the simple stuff of semantic composition you can get it with two word units.",
            "I deliberately chose not that funny, so the not was separated from the funny, but if you just look at the raw statistics alot of the time you find things like not easy and bigrams get it, so there's actually.",
            "Another student of mine said are weighing voter paper on baselines and bigrams, which talked about that for semantic.",
            "For sentiment classification that just using bigram features is very effective and can in fact outperform many of the more complex models that people build.",
            "So yeah, absolutely, but it it won't really work.",
            "For an example like this, right?",
            "Is not really going to get this right, just with bigram features.",
            "So to be able to do better at modelling semantic composition, it seemed like one of the things that would be useful to have is just more supervision and labeling of data to understand what is the meaning of freezers and are they read as positive or negative and."
        ],
        [
            "So what we did was we took the data set of Pang and Lee for Rotten Tomatoes reviews.",
            "Sentences have been widely used before, which was previously just.",
            "Sentence and the sentiment being either positive or negative as shown by the two tomatoes.",
            "And then we were taking syntactic parses of these sentences and then labeling every phrase as positive or negative.",
            "And so you can see then that you can get these different structures where you can have sensors that have positive pieces but end up negative overall or negative piece down here, but most of the sentence is positive, and so it's actually labeling for different parts of a sentence, what their classification is.",
            "So having this richer supervised data helps.",
            "All methods."
        ],
        [
            "Doing sentiment classification, so here results for bigram naive Bayes.",
            "Model is just talked about before, and some of our earlier recursive neural network models of training just on sentence labels and training with this sentiment.",
            "Label Tree Bank and you can see that all the models go up, including even the Naive Bayes bigram model goes up from 79 to 83%.",
            "You get this nice 4% boost, 'cause it just gives you a lot more supervised data which specifically labels for word pairs.",
            "What humans think of their sentiment.",
            "It's a little aside.",
            "It's a curious fact, but it's been shown multiple times that commonly for sentiment analysis tasks, people actually get better numbers using Naive Bayes classifiers than using things like SVM and logistic regression.",
            "OK, so that's great, but if you start looking at the details, if you looked at hard cases like hard negation cases, they're still mostly incorrect, and so that led to this interest of having a more complex model that could better model."
        ],
        [
            "Position and so that was the idea of this recursive neural tensor network, and so the idea is, well, we'd like to have a multiplicative relationship between two word vectors, but rather than simply just multiplying them together, we're going to mediate that multiplication by having a matrix in between, so it can decide how to put the meanings together.",
            "Well, if we just take vector matrix vector and multiply these together.",
            "We only get one number out, but what we can do is have."
        ],
        [
            "Multiple slices of matrix, each of which will give a number out, and so if we have a number of slices equal to the dimensionality of our word vector will then get out of freeze vector of the same dimensionality.",
            "So the I've sort of drawn it split out here, but the end result is where then using here a tensor there is a 3 dimensional matrix representation.",
            "OK, so this tensor was the centerpiece of our model."
        ],
        [
            "But we added on a conventional old fashion, your network layer just for completeness, and we had biased."
        ],
        [
            "And that gave us our overall structure for our newer tensor network.",
            "OK, so this model is now a new model that again allows the word meanings to interact, but without just using a single tensor to then kind of control the composition of word meanings."
        ],
        [
            "And that did work nicely better.",
            "So here's our recursive neural tensor network.",
            "It also gains considerably from having the sentiment treebank, but it gains much more and improving performance versus our other neural network models, it seems to be a better semantic composition function."
        ],
        [
            "This is just another side, but this is related when we're talking about antonyms.",
            "Before.",
            "If you are then fine tuning word representations on sentiment labeled data, you can't actually read any of the words here, but they are colored based on their positive to negative nurse, and what you find is the space reorganizes itself.",
            "So its first principle component then becomes the kind of sentiment dimension between the words, which is kind of cool.",
            "OK."
        ],
        [
            "OK, so the recursive neural tensor network actually does a much better job as a meaning composition function, and so here's a couple of examples of this.",
            "So one structure commonly see in these movie reviews.",
            "These contrast of sentence is there are slow and repetitive parts, but it has just enough spice to keep it interesting, and the model is able to correctly capture that.",
            "This part of the sentence is negative.",
            "This part of the sentence is positive, and when the two parts go together.",
            "The overall end result is a positive sentence."
        ],
        [
            "As another example, that's quite telling.",
            "So when you do negation.",
            "If it's sort of not good, the negation is sort of lowering the pozitivna's, but what happens if you have a negative word in Una Gator?",
            "So something like not dull?",
            "That should be making your estimation of the movie more positive.",
            "And what you find is if you do not great all of the models learn the not sort of should regard things as less positive and so not great producers.",
            "A reduction in the sentiment score versus great.",
            "Every model gets that right.",
            "But if you then look at and a gated negative like not dull, the previous models aren't able to do anything useful with that.",
            "Whereas this recursive neural tensor network.",
            "Can accurately predict that that should cause a substantial increase in positive nurse.",
            "These these differences are actually asymmetric, which actually seems to be linguistically the right result that not dull sort of means.",
            "OK, it doesn't actually mean fantastic.",
            "OK, so this seemed a really nice result and we had meaning composition working great for us.",
            "Unfortunately, shortly after that there was."
        ],
        [
            "This big disappointment.",
            "Becausw quackle Entoma Schmeeckle off at Google, produced this new model.",
            "The paragraph vector model and the paragraph vector model is really a sort of a pretty simple extension of word to VEC.",
            "It's kind of if you've got the word to VEC code lying around and you want to slightly more what you do is you add on one more vector which represents the whole sentence and you also train a representation for the whole sentence.",
            "But there's absolutely apart from that, it's just a bag of words model and has absolutely no lovely linguistic semantic composition, and dealing with negation and any of those things.",
            "Not unfortunately.",
            "Paragraph vector kind of did too.",
            "Two, 2% ish better than our lovely RN TN so this was a defeat for doing good models of meaning composition.",
            "Allumette, I'll mention is in the side that I think this paragraph vector was another of the models where they did a lot of work on tuning and tuning their hyperparameters to make the numbers go up.",
            "Since I think most people have tried to build their own paragraph vector of had difficulty getting's good results as they report in the paper.",
            "But nevertheless there you go disappointment question.",
            "Um?",
            "So I think they are relatively few, but there are a few.",
            "Yeah, I mean, it's sort of.",
            "Commonly get some wrong.",
            "Yeah.",
            "No, I mean, I think that's a good idea, and clearly something that's been shown in recent work, especially in vision, as you can get enormous gains from techniques for data augmentation, I think sometimes it's easier to see how to do data augmentation, vision and language, but nevertheless you can have ideas for doing in language as well, so I think that's a very good idea, but we weren't doing it, we were just training it on the 10,000 examples in the training set.",
            "Thought paperclips um.",
            "You could try it.",
            "I mean it seems like it actually be a fairly reasonable thing to do.",
            "I mean, so if a dependency is kind of like a bigram, that's a bigram at a distance.",
            "I'm not sure if anyone's actually tried that.",
            "I don't actually have a result to tell you, but you could at least believe or hope the dependency bigram, naive Bayes model, or do a little bit better than an adjacent word, bigram.",
            "They phase model that might actually just be a fun thing to try.",
            "Actually know an answer to that.",
            "Do you have another?",
            "So it's actually just the snippet, so this is Rotten Tomatoes snippets, so each one is just one sentence long, yeah?",
            "Not very formally, though.",
            "We have played with it a little bit and there's a demo of this online so you can actually try it out for yourself.",
            "I mean, to a reasonable extent it generalizes, but there are lots of things that it gets wrong that it wouldn't get wrong if you're exposed to more data.",
            "I mean, the first thing that everyone notices when they play around with it at home on the online.",
            "Demo is that it doesn't know what to do with swear words.",
            "'cause it turns out there are no swear words and published movie reviews in major newspapers.",
            "But I mean, yeah, so it works reasonably on other domains.",
            "But I mean, I think there's just a lot of water.",
            "The sentiment bearing words that varies according to domains, and it isn't able to sort of really work that out.",
            "So that if you've got other kinds of products, like laptops that the kind of things that express good sentiment, just different from movies, right people are concerned like long is.",
            "Is good for things like battery life where long is not so good for movies.",
            "Most of the time.",
            "OK, so we're a little bit sad about that.",
            "Fortunately, around the same time."
        ],
        [
            "I'm.",
            "Another pair of people.",
            "No, it's I with working with Claire Cardie, they published a paper where they built a deep recursive neural network for compositionality, and they were able to produce results that will also better than ours, but they were using a recursive neural network, and so one of the reasons they did that is that way they could use the sort of high dimensionality 300, dimension, 800 dimension word vectors that have been found to be very successful, while having a more modest phrase or dimension.",
            "But the other one is in all of our work, although the tree structure recursive neural networks are in some sense deep, because they're sort of feeding into the layers of the tree at each particular node of the tree, there was just a single layer neural network, whereas what they're doing, and that's what it's trying to indicate off here is using multi layer neural networks at each node in the tree, and that was useful for them."
        ],
        [
            "But now I'll just show a little bit of out latest work on a newer.",
            "Meaning composition function as to how we've managed to beat the michaelov paragraph vectors and so.",
            "This is work on then exploring using Trias DM models, so we're still trying to represent the meaning of a sentence as a vector in a way that does accurate semantic composition over a tree structure.",
            "But what we're doing now is adopting the idea of LSTM models to improve our meaning, composition function, and I think."
        ],
        [
            "It's the case that LSTM models haven't really been talked about during this summer school, and I think it is the case that when Phil Bonsam turns up, he's going to spend lots of time talking about LS TM's, so I don't want to go a lot into the general statements of LS DMS, so I'll try and do my version of the one slide introduction.",
            "OK, so for sequence models, so recurrent neural networks that are sequence models, conventional recurrent neural networks.",
            "Had lots of problems of vanishing.",
            "Exploding gradients, hard to train, blah blah blah blah and in recent years people have had huge success.",
            "In fact unbelievably good success by using these LS TM computational units which are long short term memory units and LST EMS affectively have these gates where you have an extra bit of neural network that predicts.",
            "Predicts a vector between zero and one, which then says how much to preserve different kinds of information, so you're getting the import as to how much attention to pay the import.",
            "You're getting the out port as to how much you're outputing, what's in your internal state, and most importantly, you're getting the preservation of information, so from one time step to the next, you then gotta forget gate that says how much should you remember what was in your internal state.",
            "Add the previous time versus how much should you mainly be making your new hidden state be dependent on what you're seeing in the new import.",
            "And so these models have been very successful, and so our idea was even though trees aren't as deep as recurrent sequence models along may."
        ],
        [
            "Maybe we could have the benefits of using a long short term memory combined with using the structure of sentence is in our tree structured LS DMS."
        ],
        [
            "So that's the model that we set about building.",
            "So we have word vectors for each node of the tree, and so then for each node of the tree there's a headword.",
            "So for the tall tree, the head word is tree, and then for the other dependants we're using a kind of a tree generalization of an LS TM.",
            "So these are the non.",
            "This is the import and then these are non head children which are then going through forget gates.",
            "And combining into the parent node and so this the forget gates are then able to model in our semantic composition how much attention to pay to the different non head children."
        ],
        [
            "And so this gives us a generalization of LS DMS to trees with any branching factor.",
            "And So what we had confess."
        ],
        [
            "Find is that when we get information from different places, we can decide which parts of to remember and which parts of it to forget.",
            "As we build a representation for bigger freezers.",
            "And so it turned out that this kind of idea of using LS TM units seems to actually be very effective for improving our semantic composition units."
        ],
        [
            "So here are some results on that.",
            "So this was our our NTN at.",
            "Sorry these results aren't fully consistent.",
            "There are two sentiment tasks and there's just a positive negative task which is a two way tasks.",
            "Which of those numbers I showed before with numbers in the 80s and then there was a more finegrained task of doing a five way sentiment judgment as to how positive or negative the accuracies for which are much lower.",
            "Sorry I probably should have used the consistent set of numbers, but these are the numbers from the five way tasks.",
            "So this was the accuracy of our our NTN before which was beaten out by the paragraph vector is we talked about.",
            "He is assigned Cardist deep recursive neural network and then compared to these guys we were able to do one or 2% better by moving to a Trio STM, but in addition to that, here's the result that's interesting, at least to me.",
            "We also build a model that was a sequence L STM.",
            "And what we found is that we could do a lot better here we're doing 4 1/2% better by using our trio SDM rather than just a sequence model.",
            "And so in terms of this question is to, do you just want to use a generic architecture like a sequence model or a convolutional model, or is it useful to have these kind of input specific semantic composition models, at least according to our results, that putting an LS TM over tree structures?",
            "Giving you a much more effective way of learning a good semantic composition function than just using a sequence model on this task.",
            "So that seems quite a bit."
        ],
        [
            "Willing to me as well As for sentiment, we also use the same model for.",
            "A semantic similarity task.",
            "So here are some results from that.",
            "So the baseline is just sort of averaging your word vectors and that gives you a correlation of 75 point 8%.",
            "This had been done as a shared task.",
            "These were the best two systems on the shared task which weren't deep learning system, so that got up to 84% LS teams are marvelous, just running and LST em out of the box was beating the best result in the shared task.",
            "By over 1%.",
            "But again, that real SDM is actually working a fair bit better again, and getting extra 1 1/2%.",
            "And so again that does seem that there's some value in modeling tree structure and semantic meaning.",
            "Composition for sentence is.",
            "As we are using it here, you're getting only one tree.",
            "I mean, so these results were just using the standard PCF deposit to give the sentence structure.",
            "I mean it can produce in best output with probabilities and so you could have produced say 20 best and God and outputs and waited them and put them together.",
            "But we weren't doing that, we were just using the one best parse.",
            "Yeah.",
            "Yes."
        ],
        [
            "I mean, I mean.",
            "You know, no, because we already knew that because this was the result with, uh, we've gotten, you know, two years ago, and we know, knew that these two people had beaten us in the mean time.",
            "So you know, that wasn't a surprise.",
            "Those are the numbers that we already knew.",
            "Going into this.",
            "I mean, I guess this is just the progress of research right at the time we published the RNTN, you know, those were the best numbers on this task.",
            "But then multiple people had, long and produced even better numbers.",
            "I mean, you know his, perhaps not too surprising that this number is better, because you'd kind of think that having deeper neural network at each node could give you some value, and indeed you do, and it's still in a way surprises me that this works so well.",
            "I mean, this suggests that you know the fact that this works as well as it does suggest that actually this meaning composition function isn't very good.",
            "And you know, I think in honesty.",
            "That's sort of still true that although we've done various generations of these meaning composition functions, if you're actually from the point of view of a linguistic semanticist, saying how well are we able to capture sentence meaning and vectors, I think actually it's fairly modest.",
            "You know, it works fairly well for simple tasks like sentiment, but it's far from great as a meaning composition function, yeah?",
            "Or is it just an artifact?",
            "I don't have a good sense of where it's coming from to be honest.",
            "So.",
            "So I guess yeah, so the paragraph vector has this extra vector that it's learning to represent just that sentence, and the way it's trained is that that extra unit aims to learn information that is useful for the decision task that is complementary to the information contained in the word vectors, and it seems like in practice that is effective for this task of.",
            "That come, learn and record the overall sentiment of the sentence, and it seems to succeed on that rather well.",
            "I mean, you know.",
            "I I could have the following conjecture, but with no proof at all.",
            "I mean, maybe the reason that paragraph vector can do so well on this task is it's actually a very simple meaning analysis task at the end of the day, all you have to do is say thumbs up thumbs down or thumbs partly up.",
            "If we're doing the five way, one right that it's not actually requiring a kind of a complex meaning representation, as you might need in a natural language understanding task where you have to represent different operators and scopes and things like that.",
            "And it could be that if you move to a task like that that the kind of tree structure models would do much better.",
            "In paragraph, vector would fall apart, but that's just conjecture.",
            "I don't know, yeah.",
            "Yeah, I think that is possibly true at Stanford.",
            "Sort of semanticist Chris Potts is actually sort of a livewire's been very interested in computing computational modeling, and I guess Chris Potts's reaction was well, if you want to show compositionality, and in language looking at sentiment is just a bizarre choice 'cause there's almost no semantic phenomena that's less compositional than sentiment now, and I think that's partly true.",
            "I mean, it's not fully true, 'cause obviously you do get these kind of compositional things going on, like when negation and the contrast putting together.",
            "But nevertheless, I think his sentence.",
            "His basic point is valid that if you think of many other natural language understanding tasks, there's a lot more meaning composition going on.",
            "So it's 1000.",
            "There are thousands of age of negative and positive ones.",
            "Yeah.",
            "Random.",
            "Um?",
            "Yeah.",
            "Yeah, I mean, that's actually in some sense that's a negative result as well, actually.",
            "I'm not now.",
            "I think we haven't done it with random trees, but we did try doing experiment of saying, well, let's suppose you always just put a left branching structure over sentence, essentially turning it into a sequence model.",
            "And the answer is you do lose a bit.",
            "That's kind of like this, but you don't lose totally, hasn't it doesn't totally fall apart, so you know, I think the message I want to give us the tree structure does give some positive value.",
            "But it's not that you can't do anything at all if you aren't representing these semantic units of the sentence.",
            "And you know that's maybe what you'd expect, right?",
            "'cause you know inside a recurrent neural network it can simulate phrasal units by devoting some of its units to building up representations of certain phrases, meaning so you know if you want to argue against my conjecture, you could argue, or saying no, no, just use a recurrent neural network and it will decide to learn how to represent constituency in whatever way is useful for the task.",
            "And in a theoretical sense, I think that is doable, but in practice, you know.",
            "Not everything the fact that model has the representational capacity doesn't actually mean it or necessarily successfully learn how to deploy it.",
            "And at least our results are that we are doing significantly better with our tree structured models.",
            "OK."
        ],
        [
            "May"
        ],
        [
            "Yeah, so let me just show a couple more examples of that real SDM.",
            "It's kind of interesting looking at what the forget gates learn, 'cause you can sort of see what the model pays attention to.",
            "So these are the activations of the forget gates, so white is preserved.",
            "Blacks gets forgotten about, so if you have a waste in those to pay attention to waste of good performances.",
            "If you're doing sentiment analysis, it's the good that's the most important thing, and it pays attention to that and then of good performance as it pays attention to the good.",
            "But then when you put together waste of good performances, that waste is the most important thing.",
            "And it's the one that primarily goes into the parent mode.",
            "I'm here is just one."
        ],
        [
            "Ad hoc example of how tree structure seems to help versus an LSD M. So let's consider these two sentence is where we flip the part where the positive and the negative appears is actually pretty good in the first few minutes, but the longer the movie goes, the worse it gets versus the longer the movie goes, the worse it gets.",
            "But it was actually pretty good in the first few minutes.",
            "So ingeneral LST EMS.",
            "Do still seem to have a recency bias that they pay most attention to what they've seen more recently, and So what you find is that the LSTM model.",
            "Wait, I think I got this wrong in my labeling.",
            "Whoops, sorry my slide.",
            "My slide is.",
            "Buggy with the pluses and minuses.",
            "Whoops.",
            "I made a Buck in doing this.",
            "What I want the answer to know this is the real answer I just made.",
            "Copy and paste errors in my in my slide so the LSD M says that this one is negative and this one is positive and so it seems like it's mainly influenced by what comes late in the sentence, whereas the tree Alice TM is correctly able to get that this one is negative and this one is negative.",
            "Two I. Flip all those things, they out change it afterwards.",
            "Whoops, yeah, so that's sort of just sort of an example, but it suggests that you can kind of better get the semantics of a sentence out using a trias TM.",
            "I little bit agree with you.",
            "Though I still think.",
            "Yeah, so I mean this example is constructed of, you know, the first examples are found example in the second example is constructed and I agree it's less natural and it does sort of.",
            "Yeah a little bit agree with you though, I still kind of suspect you should regard it as a negative example overall.",
            "Yeah.",
            "No, we did not try that.",
            "So.",
            "Inside yeah, in some sense you're right.",
            "I think that that should be able to provide some kind of solution.",
            "I guess you know a lot of the bidirectional out of steam used, such As for machine translation.",
            "You're just sort of having a representation above each word, which is then being used later on in your empty system.",
            "I guess for something like this where you ended up doing classification task, you would have to decide how to deploy it.",
            "I guess you take the two at each end and then concatenate them and classify something.",
            "Yeah, that that may well fix that.",
            "I could believe that.",
            "Yes.",
            "For language, I'm not sure that you lose like, so envision.",
            "There's been some use of tree structures, but I think for visual scene analysis there is a good argument that you lose by putting tree structures be 'cause there are lots of adjacency relationships, and if you're putting a tree structure of a visual scene, you're keeping some of the adjacency relationships, but you're losing other ones.",
            "But I think that that actually isn't true of human languages, that I think human languages really do have a constituency structure of groups of words going together.",
            "Semantic units that have a meaning, and that it isn't necessary to be kind of considering all the different analysis at once.",
            "But you know that kind of relates to my conjecture as to whether these input dependent trees are the right way to go as opposed to using something like a convolutional structure.",
            "Yeah.",
            "Yes.",
            "That's a good question that there's been significant debate about.",
            "Yeah, so at one level it seems like it just has to be unreasonable, but as the sentence gets longer and longer, the intuition is surely you can't represent the entire meaning of a sentence inside one vector, and that feels like it's true.",
            "On the other hand, you actually have a huge representational capacity in a real valued vector of reasonable dimension, and I think so far at least, we haven't probably come to the limits of what you can represent.",
            "So most dramatically in work of the sort of RNN encoder decoders that being used for machine translation, such as that Google, it seems like.",
            "To an amazing degree, you are able to encode the meaning of a whole sentence into a vector and reproduce it out in the translation.",
            "And it's not very.",
            "There's no clear evidence that you're exceeding the ability as to what you can store in one of them.",
            "Natural sentences have abounded size in a few like that.",
            "Nobody.",
            "I'm so sure so there are affectively certainly limits.",
            "An ingeneral spoken sentence is a shorter.",
            "I mean though there are quite long.",
            "Sentence is right like there are certainly sentence is that exist that are well over 100 words and actually languages vary in how long sentences are.",
            "Arabic really likes long sentence is right that if you look in the statistics for written English, sentence is over.",
            "70 words are pretty rare for English, but if you then go to an Arabic corpus, you're still finding quite a lot of them.",
            "I guess I guess Arab, I presume in the Arabic world people aren't taught about run joining sentences with an being bad style and so you get all of these very long sentences with and and and all joined together into one big sentence.",
            "OK um.",
            "So I did actually have a teeny bit more material talking about natural language inference, but it's also it's 1226 already, sorry.",
            "That's not my Clock is saying that's because still West Coast time I meant to be ending in 4 minutes.",
            "So I'm tempted to say I should just go to the conclusion slide.",
            "And if people have more questions I can do a couple more question results for a minute.",
            "My conclusions."
        ],
        [
            "Slide So what we want to do in deep learning NLP systems actually do language understanding and build things like dialogue systems and question answering system.",
            "So we don't just want word meanings.",
            "We want meanings of larger units which we calculate in some kind of compositional manner.",
            "And in particular I've tried to argue here of the sort of usefulness of actually doing these.",
            "Import specific compositional structures that represent the kind of natural compositional structure of human language sentences."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I wanted to do in this afternoon session is talk about the idea of.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oppositionality in deep learning systems.",
                    "label": 0
                },
                {
                    "sent": "I think this is.",
                    "label": 0
                },
                {
                    "sent": "I think this is an important issue for the development of richer learning models and deep learning, which is only really just being begun to be explored and so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'd like to advance this notion that artificial intelligence requires being able to understand bigger things from knowing about smaller parts.",
                    "label": 1
                },
                {
                    "sent": "Now this is clearly not the only thing that artificial intelligence involves, but I think it's one of the large ways that human beings possess intelligence that once you start to figure out parts of things, you can then reuse those parts to understand bigger things, and you have an accumulative effect of being able to be more intelligent specifically.",
                    "label": 0
                },
                {
                    "sent": "Going back to natural language.",
                    "label": 0
                },
                {
                    "sent": "The machine of the machine is going to be artificially intelligent.",
                    "label": 0
                },
                {
                    "sent": "Is that the answer?",
                    "label": 0
                },
                {
                    "sent": "But by analogy, I think humans do the same process.",
                    "label": 0
                },
                {
                    "sent": "OK, so getting back to the case of language.",
                    "label": 0
                },
                {
                    "sent": "I mean, if we're really going to make progress and having natural language understanding and systems that work well on human languages, we can't just have good representations of words and word embeddings that we need to also be able to deal with larger semantic units.",
                    "label": 0
                },
                {
                    "sent": "So the kind of things would like to know is how can we tell when bigger units than just words are similar in meaning.",
                    "label": 0
                },
                {
                    "sent": "So how can we tell?",
                    "label": 0
                },
                {
                    "sent": "That the snowboarders leaping over the mogul is similar in meaning to a person on a snowboard jumps into the air that we want to be able to workout, meaning relationships of various sorts between phrases, clauses and sentence is, and I think it is man.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The face that the way people work out the meaning of larger text units is by means of semantic composition of the smaller elements.",
                    "label": 1
                },
                {
                    "sent": "We know about the meanings of words and of the way meanings combine into phrases and clauses, and we put that together to understand the meaning of sentence is.",
                    "label": 0
                },
                {
                    "sent": "And so, in our deep learning context, that suggests that not only do we want to have word.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Does but would also like to be able to represent phrases and sentences as vectors, so as well as having a word space web Monday and Tuesday are close together in France and Germany are close together.",
                    "label": 0
                },
                {
                    "sent": "We'd like to go beyond that and say, well, we have phrases like the country of my birth and the place where I was born.",
                    "label": 1
                },
                {
                    "sent": "Can we extend our neural network ideas to Fraser's, and the idea is, well, maybe we could also map those into our vector space and that they should turn up in their location.",
                    "label": 0
                },
                {
                    "sent": "That is somewhere close to where single word locations occur, and So what we can do is want to develop models that allow us to do that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so, how might we do that?",
                    "label": 0
                },
                {
                    "sent": "Well, the idea I'm going to propose is this idea that we exploit the principle of compositionality.",
                    "label": 1
                },
                {
                    "sent": "I mean, this isn't something special to deep learning.",
                    "label": 0
                },
                {
                    "sent": "Essentially all theories of natural language semantics build on this idea of the principle of compositionality as human beings put together.",
                    "label": 0
                },
                {
                    "sent": "New sentence is everyday when they speak, they have never been heard before, and so we can only have a model of how humans understand the novel sensors that they read or hear each day.",
                    "label": 0
                },
                {
                    "sent": "If they can workout their meanings in terms of the parts that they're made from, and So what we're going to say is that the meaning here, the vector of a sentence is determined by knowing that the meanings of the words and we talked about those this morning, and then having a method that combines those meanings.",
                    "label": 1
                },
                {
                    "sent": "So what we're going to want to do is have a procedure where we can then calculate the meanings of Fraser's from the meanings of words.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to discuss a number of ways in which we've tried to do some of that.",
                    "label": 0
                },
                {
                    "sent": "I'm particularly interested in language, but this idea of compositionality and making use of it for understanding doesn't only apply to language.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when we look at complex equations, we use an idea of compositionality, and we can split that up and say, well, there's this normalizing bid up.",
                    "label": 0
                },
                {
                    "sent": "The front that divides off, and then when I look at this spirit, I can split that up again that this is measuring distance from the mean, and that's then being turned into a Gaussian, so that has subparts, and it's this compositionality that lets us understand equations, but also when we look at vision.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pictures also have parts, so when we look at a scene like this that we can see the various parts that is made of that there's a church and it has a roof and the second story in the first story and the second story has Windows Insider and there's a group of people which comprises of parts of the individual people and the human beings have their parts as the body parts, and so you might want to compose the meaning of a visual scene as well.",
                    "label": 0
                },
                {
                    "sent": "And so the general question is can.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We come up with good ways to build meaning composition functions in deep learning systems and in this section I'm going to explore a particular approach to doing that and it built.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also on this conjecture.",
                    "label": 0
                },
                {
                    "sent": "So one way you could think about doing things is that you could attempt to model larger units of language with a simple uniform architecture.",
                    "label": 1
                },
                {
                    "sent": "Something like a sequence model or 1 dimensional convolutional neural network.",
                    "label": 1
                },
                {
                    "sent": "But the conjecture that I want to explore is that maybe we can produce a better meaning composition function for language by modeling an input specific compositional tree.",
                    "label": 0
                },
                {
                    "sent": "And so what's the distinction here?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for a sentence like the cat sat on the mat well, one possibility is that building from the word vectors, we could build something like a uniform generic structure as you get for one dimensional convolutional neural network, that doesn't seem very natural for semantic composition because we end up with nodes in the center nodes in our convolutional network that represent our arbitrary fragments of structure this.",
                    "label": 1
                },
                {
                    "sent": "No represents cat sat on that.",
                    "label": 0
                },
                {
                    "sent": "That doesn't make much sense if we're thinking of wanting to build up the meaning of a sentence from its parts, it seems to make more sense to say, well, what are the actual natural parts of which this sentence is comprised, and to build up from.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Those parts, so we have a noun phrase of the cat, and we have announced the noun phrase of the mat and that goes into a prepositional phrase of on the mat, and then the sitting is on the mat and it's the cap that's sitting on the mat.",
                    "label": 1
                },
                {
                    "sent": "In other words, we're producing a syntactic structure for the sentence.",
                    "label": 0
                },
                {
                    "sent": "And then what we will want to do so you can put the sentence in, and so now the nodes that we have represent the natural semantic units of the sentence, and so we will compute.",
                    "label": 0
                },
                {
                    "sent": "A meaning of the sentence by doing composition on that representation.",
                    "label": 0
                },
                {
                    "sent": "OK, so you know if I just give a balanced position at the beginning going in this direction isn't necessarily right.",
                    "label": 0
                },
                {
                    "sent": "In fact, I'm not even.",
                    "label": 0
                },
                {
                    "sent": "I suspect it's something that only a minority of people interest in deep learning would subscribe to.",
                    "label": 0
                },
                {
                    "sent": "I think the majority of people in deep learning have a bias towards the simplicity of architecture and would prefer to have some simple, universal, simple uniform architecture.",
                    "label": 0
                },
                {
                    "sent": "Like a sequence model and just say give it a lot of data and the neural network will learn.",
                    "label": 0
                },
                {
                    "sent": "I will try and present at least a little bit of evidence that it seems that sometimes these kind of input dependent compositional structures do give you an advantage, and essentially what this issue connects into is to what extent should we be making you serve.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stronger priors or universals of language in the models that we build.",
                    "label": 0
                },
                {
                    "sent": "So in the area of language in particular, the issue of sort of priors and universals of language is a very controversial issue.",
                    "label": 1
                },
                {
                    "sent": "So on the one hand there's Noam Chomsky in the 1950s, pretty much before any machine learning existed decided that language learning was impossible, and therefore the entire of language structure had to be innate, and he's refused to change his position in any way.",
                    "label": 0
                },
                {
                    "sent": "In the intervening 65 years, and so there's this strong argument for the indebtedness of language.",
                    "label": 0
                },
                {
                    "sent": "On the other hand.",
                    "label": 0
                },
                {
                    "sent": "That there's a very strong bias of mung many in the empirical machine learning community to want to think that there's no substantive priors over things like language and will just sort of feed in a lot of data, and the models will learn.",
                    "label": 0
                },
                {
                    "sent": "And I think that actually there are very reasonable middle grounds where there probably are bias ease priors in human brains, and how they work that really do direct in particular.",
                    "label": 0
                },
                {
                    "sent": "Directions how learning occurs and therefore their existence could aid learning, and presumably we might want to use those priors and machine learning models as well.",
                    "label": 0
                },
                {
                    "sent": "And many of these things are things that you can empirically notice and aren't just sort of dictates of Chomskyan universal grammar, and so I thought I'd just show one example of this that comes from some of the work of Paul Smolensky and students.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so way back in the 1960s at Stanford, actually, Joseph Greenberg was an early descriptive linguist who did his own very influential work on universals of language and his approach was purely empirical.",
                    "label": 0
                },
                {
                    "sent": "That was sort of looking at large connections of collections of languages and how that he behaved, and looking for patterns.",
                    "label": 0
                },
                {
                    "sent": "And so he developed a theory of length of statistical language universals of likely patternings that you have.",
                    "label": 0
                },
                {
                    "sent": "And so one pattern kind of patterning he looked at with things like word orders.",
                    "label": 0
                },
                {
                    "sent": "So if you have a noun, nouns can normally be modified by adjectives, an numbers, and what you find is that by far the most common thing and languages is you get what's called the harmonic patterns where both of these dependants occur on the same side of the noun.",
                    "label": 0
                },
                {
                    "sent": "So you get both numbers and adjectives before the noun.",
                    "label": 0
                },
                {
                    "sent": "Or actually much more commonly than that, you get both numbers and adjectives after the noun, but both of them occur on the same side.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure I have as good an explanation for this, but if you get different orderings as this strong preference to put the number beforehand and the adjective afterwards, I mean, maybe you can tell some kind of functional story as to why that's preferable to get the count in earlier when expressing ideas at the time that Greenberg did his work.",
                    "label": 0
                },
                {
                    "sent": "In the 60s, he knew of no languages.",
                    "label": 0
                },
                {
                    "sent": "They were down in this 4th quadrant, and so he sort of had that as an impossibility.",
                    "label": 0
                },
                {
                    "sent": "But some have since been discovered, but languages down there are very, very rare, and so this suggests perhaps you could just think that this is a fluke of how human languages ended up, but it seems it is actually more than that, and that there's some kind of substantive learning bias that humans beings have that prefers these kind of orderings.",
                    "label": 0
                },
                {
                    "sent": "And so that's something that's been.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm exhibited in some experiments by Julie Cuthbertson Culbertson and this particular set of results.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you is with experiments on adults, but she's also done experiments with children.",
                    "label": 0
                },
                {
                    "sent": "So here what people were being was happening was people were being given artificial language data of nouns modified by adjectives and numbers, and in the input they were given.",
                    "label": 0
                },
                {
                    "sent": "There was the predominant word order.",
                    "label": 0
                },
                {
                    "sent": "Which is which side of the now?",
                    "label": 0
                },
                {
                    "sent": "And you're putting the adjective and the number.",
                    "label": 0
                },
                {
                    "sent": "But it was never fully consistent.",
                    "label": 0
                },
                {
                    "sent": "In fact, in the data that they were given, it was always the case that for both nouns and adjectives, 70% of the time the word appeared on one side and 30% on the other side, and so people were exposed to data of this sort for awhile, and then they were asked to produce data that they had sort of learned about phrases in this imaginary language and what they meant.",
                    "label": 0
                },
                {
                    "sent": "And then they were asked how do you?",
                    "label": 0
                },
                {
                    "sent": "How would you say in this language some other freezers and they would then recording where they put the adjective and the number on which side of the noun and when they did this experiment, really interesting results emerged which reflect this same bicis that you see in the distribution of human languages.",
                    "label": 0
                },
                {
                    "sent": "So the data that the humans were actually given in the experiment was.",
                    "label": 0
                },
                {
                    "sent": "For the artificial languages, was always at one of these points, reflecting the fact that 70% of the data was on one side and 30% on the other side.",
                    "label": 0
                },
                {
                    "sent": "But when the humans then produce their own examples of the language, almost invariably they moved away from there and that they presented.",
                    "label": 0
                },
                {
                    "sent": "Presented a more marked ordering of putting a category on one side or the other, which is a common phenomenon you also see in child language acquisition.",
                    "label": 0
                },
                {
                    "sent": "Children want to regularize the languages that they hear, so they moved to a more regular stance than the language that they're hearing around them.",
                    "label": 0
                },
                {
                    "sent": "So for people who are hearing the language where the adjective was after the noun, 70% of the time and the number was after the noun 70% of the time.",
                    "label": 0
                },
                {
                    "sent": "Nearly all of them were moving to a more extreme position where it was almost always in that order.",
                    "label": 0
                },
                {
                    "sent": "But note that there's this really interesting exception for the people who heard the language, which was the one that isn't meant to occur or barely occurs in human languages.",
                    "label": 0
                },
                {
                    "sent": "None of these people regularize to having that pattern marked even more strongly.",
                    "label": 0
                },
                {
                    "sent": "All of them actually moved it away from that corner of the quadrant.",
                    "label": 0
                },
                {
                    "sent": "So it seems like there really is some strong pre programmed human bias to not like to not like having the language type where you get adjectives before the noun and numbers after the noun.",
                    "label": 0
                },
                {
                    "sent": "I believe you know, I'm not quite sure I suspect so I suspect it will be your average first year psychology students in an American University.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's.",
                    "label": 0
                },
                {
                    "sent": "So you can't.",
                    "label": 0
                },
                {
                    "sent": "I can't prove clearly there could be interference.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, people happy to produce languages of the other three types, right?",
                    "label": 0
                },
                {
                    "sent": "And English is only one of these types and not the most common.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Tried.",
                    "label": 0
                },
                {
                    "sent": "Not that I know, yeah.",
                    "label": 0
                },
                {
                    "sent": "Examples.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I mean, I think the point I want to make if you just regard this as machine learning, it is trivial to build a learning device that would precisely learn the data which are exposed and you'd hear this and you'd reproduce that, right?",
                    "label": 0
                },
                {
                    "sent": "That's not tricky.",
                    "label": 0
                },
                {
                    "sent": "You guys could all do that.",
                    "label": 0
                },
                {
                    "sent": "What's interesting is that it seems like that's not what the human language learning device is doing.",
                    "label": 0
                },
                {
                    "sent": "It's instead.",
                    "label": 0
                },
                {
                    "sent": "Both having a general preference to regularize and the preference to keep away from this corner and to the extent you know.",
                    "label": 0
                },
                {
                    "sent": "Regardless, this might be just sort of an artifact of how human language learning works or something, but to the extent that there are those strong biases in human language is possible that if we're wanting to build a machine learner of language, we'd want to have it have similar biases.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was sort of that was sort of an aside getting back to the main point.",
                    "label": 0
                },
                {
                    "sent": "I mean, what I'm?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interested in is what we want to do, meaning composition and we're going to assume that we're building up meanings by composing the units of a sentence that are meaningful using this kind of input dependent tree structure.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That raises the obvious.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fashionista where did these tree structures come from?",
                    "label": 1
                },
                {
                    "sent": "Here are three answers.",
                    "label": 1
                },
                {
                    "sent": "I mean, one answer is you could just be using a conventional statistical parser to get tree structures for sentences such as our Stanford parsers, PCF.",
                    "label": 0
                },
                {
                    "sent": "G. And sometimes we've done that.",
                    "label": 1
                },
                {
                    "sent": "A second answer is you could do a neural network parser, and I guess recently we've actually worked on a neural network dependency parser and I'll show a very brief advertisement for that next.",
                    "label": 1
                },
                {
                    "sent": "But both of those are sort of a prior stage of processing before then working out meaning.",
                    "label": 0
                },
                {
                    "sent": "And this is actually something that's also debated in the human language.",
                    "label": 0
                },
                {
                    "sent": "Understanding literature, to what extent is that true that humans first pass syntactically and then interpret semantically versus the two of those being closely tide together?",
                    "label": 0
                },
                {
                    "sent": "Well, the third possibility is that you could be building up the tree structure at the same time as you're building up the meaning in a tree recursive neural network, and that's something that we've also done some work with.",
                    "label": 0
                },
                {
                    "sent": "But mainly you have actually done one or two and.",
                    "label": 0
                },
                {
                    "sent": "You'll see that in quite a bit of the work that I present.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's my brief advertisement for the neural Network Dependency parser.",
                    "label": 0
                },
                {
                    "sent": "So in general in recent years in NLP there's been this huge use of transition based dependency parsers, which are dependency parsers that, in a greedy fashion, head through a sentence from left to right, and so they maintain a stack of what they built so far in a buffer of words coming up, and then at that position they decide from that configuration what their next move is.",
                    "label": 0
                },
                {
                    "sent": "And the conventional way of doing that is you build a linear classifier over a huge bunch of features and you use lots of feature templates and what people have found is that making the right decision strongly depends on having interaction terms in the model where you're taking three or even four elements, and considering jointly if you're in that configuration, what you should do, and so as a result of that you get these kind of very complex features.",
                    "label": 0
                },
                {
                    "sent": "In the models and so that they have multiple problems, they have the problems of sparsity of features you haven't haven't seen before, and so on.",
                    "label": 0
                },
                {
                    "sent": "But if you just take the engineering perspective, something that's kind of interesting is that in conventional linear classifiers, having these kind of features turns out to be what makes this.",
                    "label": 0
                },
                {
                    "sent": "The parse is somewhat slow, so on the one hand people like these transition based dependency parsers because they've.",
                    "label": 0
                },
                {
                    "sent": "Fast, much faster than conventional dynamic program parsers, but if you actually look at what they're spending their time on doing, their spending most of their time actually computing these features so that's the slowest part of their operation, because the actual classification operation is exceptionally quick.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Last year Dan Qichen and I worked on a neural network based dependency parser, which is then using distributed representations of not only words but also part of speech tags and dependency labels, and as then putting that through a neural network structure to predict doing things, and it can be faster precisely because there's not doing that feature combination.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Step and so here are just a couple of the results from that.",
                    "label": 0
                },
                {
                    "sent": "So Malt parser is a conventional transition based parser and these are sort of accuracy numbers out of 100 unlabeled and labeled and it's quite fast.",
                    "label": 0
                },
                {
                    "sent": "There are other kinds of dependency parsers that use graph algorithms which are slightly more accurate, but way way slower.",
                    "label": 0
                },
                {
                    "sent": "And then there are pauses that.",
                    "label": 0
                },
                {
                    "sent": "Do a teeny bit better and get slower again, but the really nice thing is using this neural network dependency parser, you can build a parser that's actually faster than the conventional parser, because that doesn't do the feature computation But basically works as accurately as very strong dependency parsers, and so we thought this was a pretty good result, but our dear friends at Google thought that this was such a good architecture that they could make it but.",
                    "label": 0
                },
                {
                    "sent": "They could make it better, so they decided to sort of, you know, through their thousand GPU's at it and through through a combination of extremely extensive hyperparameter optimization, increasing the dimensionality of the representations used an up, training it on further data they've actually managed to push these numbers up so they've now I've got the currently best performing dependency parser.",
                    "label": 0
                },
                {
                    "sent": "Using essentially that same simple neural net architecture.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, yes.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's certainly yeah, there's certainly absolutely people working on free structure parsing, but you know, nevertheless, I think it's fair to say if you look at the last 10 years of history in NLP in NLP, there's been this enormous sea change in the direction of people preferring dependency grammars independency representations, which hasn't happened in linguistics, at least not in this country.",
                    "label": 0
                },
                {
                    "sent": "Which is very still very dominated by phrase structure representations.",
                    "label": 0
                },
                {
                    "sent": "OK, so back to meaning, composition and so.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wanted to sort of show you some of the kind of models that we've tried to develop for, meaning, composition, and so the basic idea is that we're developing these tree structured recursive neural networks where we got some tree typology and we're going to compute using a neural network of representation for a parent node based on inputs from the children being run through a neural network, and this idea of tree RN ends is actually developed in the 90s.",
                    "label": 0
                },
                {
                    "sent": "By Golon Kushla, who actually developed all the math of the basic tree RN ends as well, but like a bunch of other ideas and deep learning, it's only more recently it seemed to be a successful buildable things, OK?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first version that we did was a simple concatenation tree RNA, and so we had representations for the two children which might be words already Frasers.",
                    "label": 0
                },
                {
                    "sent": "We concatenated those two vectors together.",
                    "label": 0
                },
                {
                    "sent": "We multiplied them by a weight matrix, so there would be a kind of a 2 N by N matrix for our dimensionality.",
                    "label": 0
                },
                {
                    "sent": "So we reduce back to the dimensionality of a word vector, and then we put that through a non linearity like 10 H. And then we get out the output of our network and this already worked.",
                    "label": 0
                },
                {
                    "sent": "Kind of reasonably.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are just a few results where we then use this model to assess semantic similarity between sentence is where we calculated for sentence it's semantic representation by building up through this structure using one of these train network and then asking for a particular sentence, what sentence is are most similar to each other and it's doing a fairly decent job, so Knight Ridder wouldn't comment on the offer.",
                    "label": 0
                },
                {
                    "sent": "Harsco declined to say what country placed the order, not exactly the same, but you know there's sort of this.",
                    "label": 1
                },
                {
                    "sent": "Same kind of discourse function of declining to comment.",
                    "label": 1
                },
                {
                    "sent": "Coastal wouldn't disclose the terms that seem even slightly better.",
                    "label": 1
                },
                {
                    "sent": "It just seems like it's doing a pretty decent job of representing a semantics of a sentence inside a vector.",
                    "label": 0
                },
                {
                    "sent": "I mean, on the other hand, this version isn't actually that great.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I mean, if this is our model, why is this not a very satisfactory model of meaning composition for human languages?",
                    "label": 0
                },
                {
                    "sent": "Any ideas?",
                    "label": 0
                },
                {
                    "sent": "The two things shouldn't be confined, yeah?",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, so so in this model we just kind of concatenate the two vectors and then we multiply it by matrix.",
                    "label": 0
                },
                {
                    "sent": "So that kind of does some kind of linear transformation in the space.",
                    "label": 0
                },
                {
                    "sent": "But there's nothing that really combines their meanings together.",
                    "label": 0
                },
                {
                    "sent": "So unconventional semantics, a lot of what we're doing is having one word take another as its argument and building meaning of the composite.",
                    "label": 0
                },
                {
                    "sent": "So if you think of a clear case like kind of very sleepy.",
                    "label": 0
                },
                {
                    "sent": "It seems like you have a notion of sleepiness and various taking.",
                    "label": 0
                },
                {
                    "sent": "Sleepiness's argument is intensifying it to produce very sleepy, and we can't do anything like this here.",
                    "label": 0
                },
                {
                    "sent": "'cause we just got a very word vector which is concatenating on.",
                    "label": 0
                },
                {
                    "sent": "Then we're doing some linear transformation, stick it through the non linearity, but we don't have any way that the words really interact with each other.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that was bad.",
                    "label": 0
                },
                {
                    "sent": "I mean this one other thing, that sort of bad as well, which is our composition function is just a single weight matrix.",
                    "label": 0
                },
                {
                    "sent": "I mean, even if we weren't fixing the problem of the words interacting with each other, you might think that having a single weight matrix just can't possibly be good 'cause there are different structures in language, and it seems like depending on.",
                    "label": 0
                },
                {
                    "sent": "What you're combining you'd want to do something different, so if you want a representation.",
                    "label": 0
                },
                {
                    "sent": "For open door that's mainly about it being a door, it seems like where if you had something like slept uneasily.",
                    "label": 0
                },
                {
                    "sent": "That's mainly about sleeping, so you know even which of the left or the right side you should be paying most attention to is going to vary depending on the phrase.",
                    "label": 0
                },
                {
                    "sent": "OK, no interaction, so it seems like that's not really adequate for human language composition function.",
                    "label": 0
                },
                {
                    "sent": "I mean, of course they showed it did sort of work a bit, forgetting those meaning similarities.",
                    "label": 0
                },
                {
                    "sent": "And I think part of why did work some what is that simple?",
                    "label": 0
                },
                {
                    "sent": "You know a baseline method for putting together word meanings.",
                    "label": 0
                },
                {
                    "sent": "Get a sentence.",
                    "label": 0
                },
                {
                    "sent": "Meaning is simply to do vector averaging.",
                    "label": 0
                },
                {
                    "sent": "You just take the vectors for all the words and average them together.",
                    "label": 0
                },
                {
                    "sent": "And you know that actually sort of works to a certain extent because most of the time you get this sort of additive combination of word meanings.",
                    "label": 0
                },
                {
                    "sent": "That does give you a sort of representation of the meaning of the sentence, and so this model is a bit better than that.",
                    "label": 0
                },
                {
                    "sent": "Because it can learn a simple kind of linear transformation, composition function and so therefore can do something, and so we wanted to do better than that, and so the 1st.",
                    "label": 0
                },
                {
                    "sent": "Thing you could want to deal with is this easy.",
                    "label": 0
                },
                {
                    "sent": "One composition functions, a single weight matrix, and so that's something that we did in some later work.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm for passing and so this the idea of this work was that maybe we could separate out syntax and semantics in a fairly reasonable and productive way and use our neural network to focus on semantics.",
                    "label": 0
                },
                {
                    "sent": "But because we have a separate syntax, we can then have different weight combination matrices depending on what we're combining, and so the idea is if you think about statistical parsing.",
                    "label": 0
                },
                {
                    "sent": "Conventional methods, probabilistic context free grammars.",
                    "label": 0
                },
                {
                    "sent": "They do a pretty good job and modeling basic syntax of knowing what kinds of sentence structures are possible or good or not.",
                    "label": 0
                },
                {
                    "sent": "The place that they fall down is that they don't have a sufficiently good model of semantics.",
                    "label": 0
                },
                {
                    "sent": "They don't know how likely it is that certain phrases will modify other phrases, or whether certain things are likely to be combined together in a conjunction or not.",
                    "label": 0
                },
                {
                    "sent": "So what we could hope to do is use.",
                    "label": 0
                },
                {
                    "sent": "The neural network to be modeling semantic ability to combine and to be choosing out of the Pozas licensed by a syntactic PC FG and well.",
                    "label": 0
                },
                {
                    "sent": "If we do that, we could build is a syntactically untied recursive neural network where we use different.",
                    "label": 1
                },
                {
                    "sent": "Weight matrices based on what categories we combining.",
                    "label": 0
                },
                {
                    "sent": "So for each word and phrase it now has both a vector representation and a category, which might be noun or verb, noun phrase, verb phrase.",
                    "label": 0
                },
                {
                    "sent": "So then when we're combining two things together, we know that we're combining together, say, an adjective and a noun, and therefore we can use a similar kind of tree recursive neural network.",
                    "label": 0
                },
                {
                    "sent": "But we can use the particular weight matrix.",
                    "label": 1
                },
                {
                    "sent": "Which is the one for combining together adjectives and nouns.",
                    "label": 0
                },
                {
                    "sent": "And so that actually worked quite successfully to give us a good parser.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some numbers on parsing, so the numbers at the top, the traditional Stanford parser, which is a bit off the pace of leading pauses these days and then here are some of the other, more modern parsers affected.",
                    "label": 0
                },
                {
                    "sent": "PC FGS Collins parser Berkeley parser which is pretty much the best standalone English parser to this day.",
                    "label": 0
                },
                {
                    "sent": "And simply using kind of tree RNN pozza doesn't work great, but if we can, if we instead moved to this syntactically untied model where we using a combination of the PC FG with recursive neural network parser that that allows us to produce a very competitive parser, slightly better than the Berkeley parser, there's some some other pauses that are better.",
                    "label": 0
                },
                {
                    "sent": "Again that these are basically ones that do some kind of self training or re ranking.",
                    "label": 0
                },
                {
                    "sent": "On more data.",
                    "label": 0
                },
                {
                    "sent": "So that was kind of nice, and if we look at the.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matrices that we learn here.",
                    "label": 0
                },
                {
                    "sent": "You can actually see that they do learn something.",
                    "label": 0
                },
                {
                    "sent": "About what is the, what things to pay attention to in sentence is so these are the the weight matrices that are multiplying the two children so that they end by two N matrices and the thing that you need to know is we start off by pre initializing them with this sort of double diagonal prior and that's reflecting the fact of if you don't know much else, the best thing to do is just the average the two.",
                    "label": 0
                },
                {
                    "sent": "Word vectors together, but then they're learning from there, and so we've kind of got from dark blue to red for weights their way from normal.",
                    "label": 0
                },
                {
                    "sent": "So this is when you're combining a noun phrase with the conjunction the man and, and so it knows to mainly pay attention to the left hand side.",
                    "label": 0
                },
                {
                    "sent": "The man.",
                    "label": 0
                },
                {
                    "sent": "Here's where we've got a possessive pronoun.",
                    "label": 0
                },
                {
                    "sent": "A noun phrase.",
                    "label": 0
                },
                {
                    "sent": "So this is something like her briefcase, and it knows here that briefcase has most of the semantics attached.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do it if you have some things like adjective, noun, modifyers, then it knows that both sides are important, so it's something like a plastic trail.",
                    "label": 0
                },
                {
                    "sent": "You want to know about the plastic and the trial, and that's in contrast to when you've got to determine a renowned where.",
                    "label": 0
                },
                {
                    "sent": "It's the noun that's important, and the and the determiner isn't very important.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was kind of nice and worked better and I've got a question.",
                    "label": 0
                },
                {
                    "sent": "So no, but I think your observation is completely right, and that would be a good idea to do.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in this work there were matrices of weights.",
                    "label": 0
                },
                {
                    "sent": "I mean the kind of idea could think of exploring as modeling these with reduced rank matrices, and so that you could factorize them and have a reduced rank factorization.",
                    "label": 0
                },
                {
                    "sent": "So you have a smaller number of parameters I mean.",
                    "label": 0
                },
                {
                    "sent": "With the wisdom of hindsight, it's totally obvious if you look at these pictures that we suffer quite badly from the undertraining of parameters, and in this example that I show here, that's most obvious here, so this is a phrase combination that's relatively unusual, an adjective being modified by an adverb phrase, and that's pretty uncommon, because if the adverb phrase is only one word long, it has another structure that I don't have up here, and so they're relatively few of these.",
                    "label": 0
                },
                {
                    "sent": "And So what you can kind of see is that this is barely walked away from its initialization because it hasn't been seen often.",
                    "label": 0
                },
                {
                    "sent": "So yeah, good point, definitely true.",
                    "label": 0
                },
                {
                    "sent": "OK, so that allowed us to make some progress because we could sort of model different weight matrices for different configurations and that actually seemed to be pretty useful from proving parsing, but it still isn't allowing the two word vectors to interact to produce the meaning of the whole, and so we wanted to be able to do something of that sort.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that leads us into version three, and as you noticing in the green, I mean all of these versions were being primarily developed by Richard, so she is going to be around later in the week presenting stuff so you can also hear more from him then.",
                    "label": 0
                },
                {
                    "sent": "So if you look at conventional semantics for natural language as the dominant model is you have functions with arguments.",
                    "label": 0
                },
                {
                    "sent": "So if you have a verb like give, you'd modify model give as a function that takes 3 arguments.",
                    "label": 0
                },
                {
                    "sent": "John gave the dog a bone, or if you have modifyers like very, very happy, very would be a function that takes as argument the adjective whose meaning it's going to modify.",
                    "label": 0
                },
                {
                    "sent": "So if you then move into the.",
                    "label": 0
                },
                {
                    "sent": "Vector Space world, it seems like the obvious equivalent for having functions and arguments is that if you have a matrix, then the matrix connect like a function on a word vector as its argument.",
                    "label": 0
                },
                {
                    "sent": "However, there's the problem that in this world, depending on the sentence is not quite clear which words are going to be the functions and which of words are going to be arguments.",
                    "label": 0
                },
                {
                    "sent": "Depending on the structure.",
                    "label": 0
                },
                {
                    "sent": "So the way we dealt with that is by for each word.",
                    "label": 0
                },
                {
                    "sent": "It has both a vector representation and the matrix representation, and so then when we workout a representation for a phrase, we're going to do that by making use of weight matrices and both the vectors we gotta wait matrix and then we're using the matrix times the vector both ways around, so it's considering each one is the function and the argument, and then is using a weight matrix.",
                    "label": 0
                },
                {
                    "sent": "To learn which of those choices is a good choice or how to Mac mix them and so that will give us a new vector representation for the parent node.",
                    "label": 0
                },
                {
                    "sent": "But we also.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Workout a new matrix representation for the parent node.",
                    "label": 0
                },
                {
                    "sent": "So we're taking the two matrices of the children and combining them with the new weight matrix to make a matrix representation of the parent.",
                    "label": 0
                },
                {
                    "sent": "OK, and we were able to do some useful things with this as well, so one.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example of a task that we did with this is learning about semantic relationships and classifying them.",
                    "label": 0
                },
                {
                    "sent": "So the idea of this task was that you wanted to learn about different semantic relationships that occur in the center and such as component whole relationships.",
                    "label": 0
                },
                {
                    "sent": "So in my apartment has a pretty large kitchen apartment and kitchen are in a component whole relationship and so we're doing that by building up a representation for the sentence with this matrix vector model.",
                    "label": 0
                },
                {
                    "sent": "And then at the node where the two words of interest joined together, we were then having a classifier that then classified if a relation held between the two words, and if So what relation it was and so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It worked pretty well for us, so this was a shared task that a number of systems had worked on previously.",
                    "label": 0
                },
                {
                    "sent": "So simple system got about 60% right?",
                    "label": 0
                },
                {
                    "sent": "The best system in the shared task got 82.2% right?",
                    "label": 0
                },
                {
                    "sent": "Simply using our basic RNN got stuff out 75%, which was mediocre.",
                    "label": 0
                },
                {
                    "sent": "Moving to the matrix vector R&N as a pure model got significantly better than that and then.",
                    "label": 0
                },
                {
                    "sent": "By throwing in some of the extra kind of linguistic analysis, not as much as this system used, but some information about named entities and Wordnet semantic classes were able to build that up to a state of the art system.",
                    "label": 0
                },
                {
                    "sent": "So that was pretty good.",
                    "label": 0
                },
                {
                    "sent": "But it seemed like.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There was some flaws to this system which actually relate again to your question from a minute ago, which is estimating the parameters of these matrices.",
                    "label": 0
                },
                {
                    "sent": "So the bad thing that happened was that for each word, rather than just having a vector, it also has a matrix attached to it, and that's a lot of parameters.",
                    "label": 0
                },
                {
                    "sent": "And again, if we were doing this again, there are other ways that we might think of dealing with this about how using reduced rank representations of those matrices so that the less parameters involved.",
                    "label": 0
                },
                {
                    "sent": "But we didn't actually do that at the time, so we just had the matrix of parameters and the only way we dealt with things is by reducing the size of the vectors.",
                    "label": 0
                },
                {
                    "sent": "So we're using 25 dimensional vectors here to reduce the number of parameters.",
                    "label": 0
                },
                {
                    "sent": "But that was sort of doubly bad becausw.",
                    "label": 0
                },
                {
                    "sent": "Well, everything we know about word vectors shows that you do better by using bigger word vectors than 25 dimensional vectors, but even with that size we've still got 625 parameters.",
                    "label": 0
                },
                {
                    "sent": "In there, which is a lot, and it's doubly difficult fact we didn't have a good way of pre training the matrices, so there are lots of good ways of pretraining the word vectors like I talked about this morning.",
                    "label": 0
                },
                {
                    "sent": "Methods like word to vec, it's very easy to train on huge amounts of text data, but we didn't have a similar method for pretraining the matrix parameters.",
                    "label": 0
                },
                {
                    "sent": "So far.",
                    "label": 0
                },
                {
                    "sent": "Move.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And on to version four, what we were interested in was having a way for the two word vectors to interact with each other without suffering from the massive increase in the number of parameters of the matrix vector model.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so that thing lead into the recursive neural tensor network, and I'll explain a little bit about that.",
                    "label": 0
                },
                {
                    "sent": "I'll go through it and main.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Place in which we demonstrated this isn't looking at the task of sentiment detection, so this is working out whether a piece of text is positive, negative or neutral.",
                    "label": 0
                },
                {
                    "sent": "I mean to a certain extent's sentiment analysis can be a pretty easy problem, so if you've got a piece of text and you see words like love, great, impressed, marvelous, you don't really need more than a bag of words model to say.",
                    "label": 0
                },
                {
                    "sent": "Well there are lots of positive words in this piece of text.",
                    "label": 0
                },
                {
                    "sent": "Therefore, it's probably a positive piece of text and to some extent that's true that if you have long documents, you can use big words, classifiers and they work pretty well, but that's much less true for short documents, and it's much less true for certain styles of writing, such as movie reviews.",
                    "label": 0
                },
                {
                    "sent": "So on Rotten Tomatoes you find snippets like this with this cast, and this subject matter.",
                    "label": 0
                },
                {
                    "sent": "The movie should have been funnier and more entertaining, so the only kind of.",
                    "label": 0
                },
                {
                    "sent": "Obvious bag of words clues funnier entertaining, both of which are positive words.",
                    "label": 0
                },
                {
                    "sent": "But what you're meant to take away from this snippet is that the person didn't think much of this movie.",
                    "label": 0
                },
                {
                    "sent": "So if we're going to be able to get the sentiment classification of these kind of examples right, it seems like we're going to have to do more.",
                    "label": 0
                },
                {
                    "sent": "We're actually going to understand the meaning of sentence is and have some kind of decent method of meaning composition.",
                    "label": 0
                },
                {
                    "sent": "And there is a question.",
                    "label": 0
                },
                {
                    "sent": "Well, that's another example, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so treatment of negation is another reason for why we want to do semantic composition.",
                    "label": 0
                },
                {
                    "sent": "So we if it says not that funny that that counts as a negative rather than being the word funny in a bag of words model, yeah, absolutely.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, good point.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that for a lot of the simple stuff of semantic composition you can get it with two word units.",
                    "label": 0
                },
                {
                    "sent": "I deliberately chose not that funny, so the not was separated from the funny, but if you just look at the raw statistics alot of the time you find things like not easy and bigrams get it, so there's actually.",
                    "label": 0
                },
                {
                    "sent": "Another student of mine said are weighing voter paper on baselines and bigrams, which talked about that for semantic.",
                    "label": 0
                },
                {
                    "sent": "For sentiment classification that just using bigram features is very effective and can in fact outperform many of the more complex models that people build.",
                    "label": 0
                },
                {
                    "sent": "So yeah, absolutely, but it it won't really work.",
                    "label": 0
                },
                {
                    "sent": "For an example like this, right?",
                    "label": 0
                },
                {
                    "sent": "Is not really going to get this right, just with bigram features.",
                    "label": 0
                },
                {
                    "sent": "So to be able to do better at modelling semantic composition, it seemed like one of the things that would be useful to have is just more supervision and labeling of data to understand what is the meaning of freezers and are they read as positive or negative and.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we did was we took the data set of Pang and Lee for Rotten Tomatoes reviews.",
                    "label": 0
                },
                {
                    "sent": "Sentences have been widely used before, which was previously just.",
                    "label": 0
                },
                {
                    "sent": "Sentence and the sentiment being either positive or negative as shown by the two tomatoes.",
                    "label": 0
                },
                {
                    "sent": "And then we were taking syntactic parses of these sentences and then labeling every phrase as positive or negative.",
                    "label": 0
                },
                {
                    "sent": "And so you can see then that you can get these different structures where you can have sensors that have positive pieces but end up negative overall or negative piece down here, but most of the sentence is positive, and so it's actually labeling for different parts of a sentence, what their classification is.",
                    "label": 0
                },
                {
                    "sent": "So having this richer supervised data helps.",
                    "label": 0
                },
                {
                    "sent": "All methods.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing sentiment classification, so here results for bigram naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "Model is just talked about before, and some of our earlier recursive neural network models of training just on sentence labels and training with this sentiment.",
                    "label": 0
                },
                {
                    "sent": "Label Tree Bank and you can see that all the models go up, including even the Naive Bayes bigram model goes up from 79 to 83%.",
                    "label": 0
                },
                {
                    "sent": "You get this nice 4% boost, 'cause it just gives you a lot more supervised data which specifically labels for word pairs.",
                    "label": 0
                },
                {
                    "sent": "What humans think of their sentiment.",
                    "label": 0
                },
                {
                    "sent": "It's a little aside.",
                    "label": 0
                },
                {
                    "sent": "It's a curious fact, but it's been shown multiple times that commonly for sentiment analysis tasks, people actually get better numbers using Naive Bayes classifiers than using things like SVM and logistic regression.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's great, but if you start looking at the details, if you looked at hard cases like hard negation cases, they're still mostly incorrect, and so that led to this interest of having a more complex model that could better model.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Position and so that was the idea of this recursive neural tensor network, and so the idea is, well, we'd like to have a multiplicative relationship between two word vectors, but rather than simply just multiplying them together, we're going to mediate that multiplication by having a matrix in between, so it can decide how to put the meanings together.",
                    "label": 0
                },
                {
                    "sent": "Well, if we just take vector matrix vector and multiply these together.",
                    "label": 0
                },
                {
                    "sent": "We only get one number out, but what we can do is have.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Multiple slices of matrix, each of which will give a number out, and so if we have a number of slices equal to the dimensionality of our word vector will then get out of freeze vector of the same dimensionality.",
                    "label": 0
                },
                {
                    "sent": "So the I've sort of drawn it split out here, but the end result is where then using here a tensor there is a 3 dimensional matrix representation.",
                    "label": 0
                },
                {
                    "sent": "OK, so this tensor was the centerpiece of our model.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we added on a conventional old fashion, your network layer just for completeness, and we had biased.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that gave us our overall structure for our newer tensor network.",
                    "label": 0
                },
                {
                    "sent": "OK, so this model is now a new model that again allows the word meanings to interact, but without just using a single tensor to then kind of control the composition of word meanings.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that did work nicely better.",
                    "label": 0
                },
                {
                    "sent": "So here's our recursive neural tensor network.",
                    "label": 1
                },
                {
                    "sent": "It also gains considerably from having the sentiment treebank, but it gains much more and improving performance versus our other neural network models, it seems to be a better semantic composition function.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is just another side, but this is related when we're talking about antonyms.",
                    "label": 0
                },
                {
                    "sent": "Before.",
                    "label": 0
                },
                {
                    "sent": "If you are then fine tuning word representations on sentiment labeled data, you can't actually read any of the words here, but they are colored based on their positive to negative nurse, and what you find is the space reorganizes itself.",
                    "label": 0
                },
                {
                    "sent": "So its first principle component then becomes the kind of sentiment dimension between the words, which is kind of cool.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the recursive neural tensor network actually does a much better job as a meaning composition function, and so here's a couple of examples of this.",
                    "label": 0
                },
                {
                    "sent": "So one structure commonly see in these movie reviews.",
                    "label": 0
                },
                {
                    "sent": "These contrast of sentence is there are slow and repetitive parts, but it has just enough spice to keep it interesting, and the model is able to correctly capture that.",
                    "label": 0
                },
                {
                    "sent": "This part of the sentence is negative.",
                    "label": 0
                },
                {
                    "sent": "This part of the sentence is positive, and when the two parts go together.",
                    "label": 0
                },
                {
                    "sent": "The overall end result is a positive sentence.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As another example, that's quite telling.",
                    "label": 0
                },
                {
                    "sent": "So when you do negation.",
                    "label": 0
                },
                {
                    "sent": "If it's sort of not good, the negation is sort of lowering the pozitivna's, but what happens if you have a negative word in Una Gator?",
                    "label": 0
                },
                {
                    "sent": "So something like not dull?",
                    "label": 0
                },
                {
                    "sent": "That should be making your estimation of the movie more positive.",
                    "label": 0
                },
                {
                    "sent": "And what you find is if you do not great all of the models learn the not sort of should regard things as less positive and so not great producers.",
                    "label": 0
                },
                {
                    "sent": "A reduction in the sentiment score versus great.",
                    "label": 0
                },
                {
                    "sent": "Every model gets that right.",
                    "label": 0
                },
                {
                    "sent": "But if you then look at and a gated negative like not dull, the previous models aren't able to do anything useful with that.",
                    "label": 0
                },
                {
                    "sent": "Whereas this recursive neural tensor network.",
                    "label": 0
                },
                {
                    "sent": "Can accurately predict that that should cause a substantial increase in positive nurse.",
                    "label": 0
                },
                {
                    "sent": "These these differences are actually asymmetric, which actually seems to be linguistically the right result that not dull sort of means.",
                    "label": 0
                },
                {
                    "sent": "OK, it doesn't actually mean fantastic.",
                    "label": 0
                },
                {
                    "sent": "OK, so this seemed a really nice result and we had meaning composition working great for us.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, shortly after that there was.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This big disappointment.",
                    "label": 0
                },
                {
                    "sent": "Becausw quackle Entoma Schmeeckle off at Google, produced this new model.",
                    "label": 0
                },
                {
                    "sent": "The paragraph vector model and the paragraph vector model is really a sort of a pretty simple extension of word to VEC.",
                    "label": 0
                },
                {
                    "sent": "It's kind of if you've got the word to VEC code lying around and you want to slightly more what you do is you add on one more vector which represents the whole sentence and you also train a representation for the whole sentence.",
                    "label": 0
                },
                {
                    "sent": "But there's absolutely apart from that, it's just a bag of words model and has absolutely no lovely linguistic semantic composition, and dealing with negation and any of those things.",
                    "label": 0
                },
                {
                    "sent": "Not unfortunately.",
                    "label": 0
                },
                {
                    "sent": "Paragraph vector kind of did too.",
                    "label": 0
                },
                {
                    "sent": "Two, 2% ish better than our lovely RN TN so this was a defeat for doing good models of meaning composition.",
                    "label": 0
                },
                {
                    "sent": "Allumette, I'll mention is in the side that I think this paragraph vector was another of the models where they did a lot of work on tuning and tuning their hyperparameters to make the numbers go up.",
                    "label": 0
                },
                {
                    "sent": "Since I think most people have tried to build their own paragraph vector of had difficulty getting's good results as they report in the paper.",
                    "label": 0
                },
                {
                    "sent": "But nevertheless there you go disappointment question.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So I think they are relatively few, but there are a few.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, it's sort of.",
                    "label": 0
                },
                {
                    "sent": "Commonly get some wrong.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, I mean, I think that's a good idea, and clearly something that's been shown in recent work, especially in vision, as you can get enormous gains from techniques for data augmentation, I think sometimes it's easier to see how to do data augmentation, vision and language, but nevertheless you can have ideas for doing in language as well, so I think that's a very good idea, but we weren't doing it, we were just training it on the 10,000 examples in the training set.",
                    "label": 0
                },
                {
                    "sent": "Thought paperclips um.",
                    "label": 0
                },
                {
                    "sent": "You could try it.",
                    "label": 0
                },
                {
                    "sent": "I mean it seems like it actually be a fairly reasonable thing to do.",
                    "label": 0
                },
                {
                    "sent": "I mean, so if a dependency is kind of like a bigram, that's a bigram at a distance.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if anyone's actually tried that.",
                    "label": 0
                },
                {
                    "sent": "I don't actually have a result to tell you, but you could at least believe or hope the dependency bigram, naive Bayes model, or do a little bit better than an adjacent word, bigram.",
                    "label": 0
                },
                {
                    "sent": "They phase model that might actually just be a fun thing to try.",
                    "label": 0
                },
                {
                    "sent": "Actually know an answer to that.",
                    "label": 0
                },
                {
                    "sent": "Do you have another?",
                    "label": 0
                },
                {
                    "sent": "So it's actually just the snippet, so this is Rotten Tomatoes snippets, so each one is just one sentence long, yeah?",
                    "label": 0
                },
                {
                    "sent": "Not very formally, though.",
                    "label": 0
                },
                {
                    "sent": "We have played with it a little bit and there's a demo of this online so you can actually try it out for yourself.",
                    "label": 0
                },
                {
                    "sent": "I mean, to a reasonable extent it generalizes, but there are lots of things that it gets wrong that it wouldn't get wrong if you're exposed to more data.",
                    "label": 0
                },
                {
                    "sent": "I mean, the first thing that everyone notices when they play around with it at home on the online.",
                    "label": 0
                },
                {
                    "sent": "Demo is that it doesn't know what to do with swear words.",
                    "label": 0
                },
                {
                    "sent": "'cause it turns out there are no swear words and published movie reviews in major newspapers.",
                    "label": 0
                },
                {
                    "sent": "But I mean, yeah, so it works reasonably on other domains.",
                    "label": 0
                },
                {
                    "sent": "But I mean, I think there's just a lot of water.",
                    "label": 0
                },
                {
                    "sent": "The sentiment bearing words that varies according to domains, and it isn't able to sort of really work that out.",
                    "label": 0
                },
                {
                    "sent": "So that if you've got other kinds of products, like laptops that the kind of things that express good sentiment, just different from movies, right people are concerned like long is.",
                    "label": 0
                },
                {
                    "sent": "Is good for things like battery life where long is not so good for movies.",
                    "label": 0
                },
                {
                    "sent": "Most of the time.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're a little bit sad about that.",
                    "label": 0
                },
                {
                    "sent": "Fortunately, around the same time.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Another pair of people.",
                    "label": 0
                },
                {
                    "sent": "No, it's I with working with Claire Cardie, they published a paper where they built a deep recursive neural network for compositionality, and they were able to produce results that will also better than ours, but they were using a recursive neural network, and so one of the reasons they did that is that way they could use the sort of high dimensionality 300, dimension, 800 dimension word vectors that have been found to be very successful, while having a more modest phrase or dimension.",
                    "label": 0
                },
                {
                    "sent": "But the other one is in all of our work, although the tree structure recursive neural networks are in some sense deep, because they're sort of feeding into the layers of the tree at each particular node of the tree, there was just a single layer neural network, whereas what they're doing, and that's what it's trying to indicate off here is using multi layer neural networks at each node in the tree, and that was useful for them.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But now I'll just show a little bit of out latest work on a newer.",
                    "label": 0
                },
                {
                    "sent": "Meaning composition function as to how we've managed to beat the michaelov paragraph vectors and so.",
                    "label": 0
                },
                {
                    "sent": "This is work on then exploring using Trias DM models, so we're still trying to represent the meaning of a sentence as a vector in a way that does accurate semantic composition over a tree structure.",
                    "label": 0
                },
                {
                    "sent": "But what we're doing now is adopting the idea of LSTM models to improve our meaning, composition function, and I think.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's the case that LSTM models haven't really been talked about during this summer school, and I think it is the case that when Phil Bonsam turns up, he's going to spend lots of time talking about LS TM's, so I don't want to go a lot into the general statements of LS DMS, so I'll try and do my version of the one slide introduction.",
                    "label": 0
                },
                {
                    "sent": "OK, so for sequence models, so recurrent neural networks that are sequence models, conventional recurrent neural networks.",
                    "label": 0
                },
                {
                    "sent": "Had lots of problems of vanishing.",
                    "label": 0
                },
                {
                    "sent": "Exploding gradients, hard to train, blah blah blah blah and in recent years people have had huge success.",
                    "label": 0
                },
                {
                    "sent": "In fact unbelievably good success by using these LS TM computational units which are long short term memory units and LST EMS affectively have these gates where you have an extra bit of neural network that predicts.",
                    "label": 0
                },
                {
                    "sent": "Predicts a vector between zero and one, which then says how much to preserve different kinds of information, so you're getting the import as to how much attention to pay the import.",
                    "label": 0
                },
                {
                    "sent": "You're getting the out port as to how much you're outputing, what's in your internal state, and most importantly, you're getting the preservation of information, so from one time step to the next, you then gotta forget gate that says how much should you remember what was in your internal state.",
                    "label": 0
                },
                {
                    "sent": "Add the previous time versus how much should you mainly be making your new hidden state be dependent on what you're seeing in the new import.",
                    "label": 0
                },
                {
                    "sent": "And so these models have been very successful, and so our idea was even though trees aren't as deep as recurrent sequence models along may.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe we could have the benefits of using a long short term memory combined with using the structure of sentence is in our tree structured LS DMS.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the model that we set about building.",
                    "label": 0
                },
                {
                    "sent": "So we have word vectors for each node of the tree, and so then for each node of the tree there's a headword.",
                    "label": 0
                },
                {
                    "sent": "So for the tall tree, the head word is tree, and then for the other dependants we're using a kind of a tree generalization of an LS TM.",
                    "label": 0
                },
                {
                    "sent": "So these are the non.",
                    "label": 0
                },
                {
                    "sent": "This is the import and then these are non head children which are then going through forget gates.",
                    "label": 0
                },
                {
                    "sent": "And combining into the parent node and so this the forget gates are then able to model in our semantic composition how much attention to pay to the different non head children.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this gives us a generalization of LS DMS to trees with any branching factor.",
                    "label": 0
                },
                {
                    "sent": "And So what we had confess.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find is that when we get information from different places, we can decide which parts of to remember and which parts of it to forget.",
                    "label": 0
                },
                {
                    "sent": "As we build a representation for bigger freezers.",
                    "label": 0
                },
                {
                    "sent": "And so it turned out that this kind of idea of using LS TM units seems to actually be very effective for improving our semantic composition units.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some results on that.",
                    "label": 0
                },
                {
                    "sent": "So this was our our NTN at.",
                    "label": 0
                },
                {
                    "sent": "Sorry these results aren't fully consistent.",
                    "label": 0
                },
                {
                    "sent": "There are two sentiment tasks and there's just a positive negative task which is a two way tasks.",
                    "label": 0
                },
                {
                    "sent": "Which of those numbers I showed before with numbers in the 80s and then there was a more finegrained task of doing a five way sentiment judgment as to how positive or negative the accuracies for which are much lower.",
                    "label": 0
                },
                {
                    "sent": "Sorry I probably should have used the consistent set of numbers, but these are the numbers from the five way tasks.",
                    "label": 0
                },
                {
                    "sent": "So this was the accuracy of our our NTN before which was beaten out by the paragraph vector is we talked about.",
                    "label": 0
                },
                {
                    "sent": "He is assigned Cardist deep recursive neural network and then compared to these guys we were able to do one or 2% better by moving to a Trio STM, but in addition to that, here's the result that's interesting, at least to me.",
                    "label": 0
                },
                {
                    "sent": "We also build a model that was a sequence L STM.",
                    "label": 0
                },
                {
                    "sent": "And what we found is that we could do a lot better here we're doing 4 1/2% better by using our trio SDM rather than just a sequence model.",
                    "label": 0
                },
                {
                    "sent": "And so in terms of this question is to, do you just want to use a generic architecture like a sequence model or a convolutional model, or is it useful to have these kind of input specific semantic composition models, at least according to our results, that putting an LS TM over tree structures?",
                    "label": 0
                },
                {
                    "sent": "Giving you a much more effective way of learning a good semantic composition function than just using a sequence model on this task.",
                    "label": 0
                },
                {
                    "sent": "So that seems quite a bit.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Willing to me as well As for sentiment, we also use the same model for.",
                    "label": 0
                },
                {
                    "sent": "A semantic similarity task.",
                    "label": 0
                },
                {
                    "sent": "So here are some results from that.",
                    "label": 0
                },
                {
                    "sent": "So the baseline is just sort of averaging your word vectors and that gives you a correlation of 75 point 8%.",
                    "label": 0
                },
                {
                    "sent": "This had been done as a shared task.",
                    "label": 0
                },
                {
                    "sent": "These were the best two systems on the shared task which weren't deep learning system, so that got up to 84% LS teams are marvelous, just running and LST em out of the box was beating the best result in the shared task.",
                    "label": 0
                },
                {
                    "sent": "By over 1%.",
                    "label": 0
                },
                {
                    "sent": "But again, that real SDM is actually working a fair bit better again, and getting extra 1 1/2%.",
                    "label": 0
                },
                {
                    "sent": "And so again that does seem that there's some value in modeling tree structure and semantic meaning.",
                    "label": 0
                },
                {
                    "sent": "Composition for sentence is.",
                    "label": 0
                },
                {
                    "sent": "As we are using it here, you're getting only one tree.",
                    "label": 0
                },
                {
                    "sent": "I mean, so these results were just using the standard PCF deposit to give the sentence structure.",
                    "label": 0
                },
                {
                    "sent": "I mean it can produce in best output with probabilities and so you could have produced say 20 best and God and outputs and waited them and put them together.",
                    "label": 0
                },
                {
                    "sent": "But we weren't doing that, we were just using the one best parse.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean, I mean.",
                    "label": 0
                },
                {
                    "sent": "You know, no, because we already knew that because this was the result with, uh, we've gotten, you know, two years ago, and we know, knew that these two people had beaten us in the mean time.",
                    "label": 0
                },
                {
                    "sent": "So you know, that wasn't a surprise.",
                    "label": 0
                },
                {
                    "sent": "Those are the numbers that we already knew.",
                    "label": 0
                },
                {
                    "sent": "Going into this.",
                    "label": 0
                },
                {
                    "sent": "I mean, I guess this is just the progress of research right at the time we published the RNTN, you know, those were the best numbers on this task.",
                    "label": 0
                },
                {
                    "sent": "But then multiple people had, long and produced even better numbers.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know his, perhaps not too surprising that this number is better, because you'd kind of think that having deeper neural network at each node could give you some value, and indeed you do, and it's still in a way surprises me that this works so well.",
                    "label": 0
                },
                {
                    "sent": "I mean, this suggests that you know the fact that this works as well as it does suggest that actually this meaning composition function isn't very good.",
                    "label": 0
                },
                {
                    "sent": "And you know, I think in honesty.",
                    "label": 0
                },
                {
                    "sent": "That's sort of still true that although we've done various generations of these meaning composition functions, if you're actually from the point of view of a linguistic semanticist, saying how well are we able to capture sentence meaning and vectors, I think actually it's fairly modest.",
                    "label": 0
                },
                {
                    "sent": "You know, it works fairly well for simple tasks like sentiment, but it's far from great as a meaning composition function, yeah?",
                    "label": 0
                },
                {
                    "sent": "Or is it just an artifact?",
                    "label": 0
                },
                {
                    "sent": "I don't have a good sense of where it's coming from to be honest.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So I guess yeah, so the paragraph vector has this extra vector that it's learning to represent just that sentence, and the way it's trained is that that extra unit aims to learn information that is useful for the decision task that is complementary to the information contained in the word vectors, and it seems like in practice that is effective for this task of.",
                    "label": 0
                },
                {
                    "sent": "That come, learn and record the overall sentiment of the sentence, and it seems to succeed on that rather well.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know.",
                    "label": 0
                },
                {
                    "sent": "I I could have the following conjecture, but with no proof at all.",
                    "label": 0
                },
                {
                    "sent": "I mean, maybe the reason that paragraph vector can do so well on this task is it's actually a very simple meaning analysis task at the end of the day, all you have to do is say thumbs up thumbs down or thumbs partly up.",
                    "label": 0
                },
                {
                    "sent": "If we're doing the five way, one right that it's not actually requiring a kind of a complex meaning representation, as you might need in a natural language understanding task where you have to represent different operators and scopes and things like that.",
                    "label": 0
                },
                {
                    "sent": "And it could be that if you move to a task like that that the kind of tree structure models would do much better.",
                    "label": 0
                },
                {
                    "sent": "In paragraph, vector would fall apart, but that's just conjecture.",
                    "label": 0
                },
                {
                    "sent": "I don't know, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think that is possibly true at Stanford.",
                    "label": 0
                },
                {
                    "sent": "Sort of semanticist Chris Potts is actually sort of a livewire's been very interested in computing computational modeling, and I guess Chris Potts's reaction was well, if you want to show compositionality, and in language looking at sentiment is just a bizarre choice 'cause there's almost no semantic phenomena that's less compositional than sentiment now, and I think that's partly true.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not fully true, 'cause obviously you do get these kind of compositional things going on, like when negation and the contrast putting together.",
                    "label": 0
                },
                {
                    "sent": "But nevertheless, I think his sentence.",
                    "label": 0
                },
                {
                    "sent": "His basic point is valid that if you think of many other natural language understanding tasks, there's a lot more meaning composition going on.",
                    "label": 0
                },
                {
                    "sent": "So it's 1000.",
                    "label": 0
                },
                {
                    "sent": "There are thousands of age of negative and positive ones.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Random.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, that's actually in some sense that's a negative result as well, actually.",
                    "label": 0
                },
                {
                    "sent": "I'm not now.",
                    "label": 0
                },
                {
                    "sent": "I think we haven't done it with random trees, but we did try doing experiment of saying, well, let's suppose you always just put a left branching structure over sentence, essentially turning it into a sequence model.",
                    "label": 0
                },
                {
                    "sent": "And the answer is you do lose a bit.",
                    "label": 0
                },
                {
                    "sent": "That's kind of like this, but you don't lose totally, hasn't it doesn't totally fall apart, so you know, I think the message I want to give us the tree structure does give some positive value.",
                    "label": 0
                },
                {
                    "sent": "But it's not that you can't do anything at all if you aren't representing these semantic units of the sentence.",
                    "label": 0
                },
                {
                    "sent": "And you know that's maybe what you'd expect, right?",
                    "label": 0
                },
                {
                    "sent": "'cause you know inside a recurrent neural network it can simulate phrasal units by devoting some of its units to building up representations of certain phrases, meaning so you know if you want to argue against my conjecture, you could argue, or saying no, no, just use a recurrent neural network and it will decide to learn how to represent constituency in whatever way is useful for the task.",
                    "label": 0
                },
                {
                    "sent": "And in a theoretical sense, I think that is doable, but in practice, you know.",
                    "label": 0
                },
                {
                    "sent": "Not everything the fact that model has the representational capacity doesn't actually mean it or necessarily successfully learn how to deploy it.",
                    "label": 0
                },
                {
                    "sent": "And at least our results are that we are doing significantly better with our tree structured models.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "May",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so let me just show a couple more examples of that real SDM.",
                    "label": 0
                },
                {
                    "sent": "It's kind of interesting looking at what the forget gates learn, 'cause you can sort of see what the model pays attention to.",
                    "label": 0
                },
                {
                    "sent": "So these are the activations of the forget gates, so white is preserved.",
                    "label": 0
                },
                {
                    "sent": "Blacks gets forgotten about, so if you have a waste in those to pay attention to waste of good performances.",
                    "label": 0
                },
                {
                    "sent": "If you're doing sentiment analysis, it's the good that's the most important thing, and it pays attention to that and then of good performance as it pays attention to the good.",
                    "label": 0
                },
                {
                    "sent": "But then when you put together waste of good performances, that waste is the most important thing.",
                    "label": 0
                },
                {
                    "sent": "And it's the one that primarily goes into the parent mode.",
                    "label": 0
                },
                {
                    "sent": "I'm here is just one.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ad hoc example of how tree structure seems to help versus an LSD M. So let's consider these two sentence is where we flip the part where the positive and the negative appears is actually pretty good in the first few minutes, but the longer the movie goes, the worse it gets versus the longer the movie goes, the worse it gets.",
                    "label": 0
                },
                {
                    "sent": "But it was actually pretty good in the first few minutes.",
                    "label": 0
                },
                {
                    "sent": "So ingeneral LST EMS.",
                    "label": 0
                },
                {
                    "sent": "Do still seem to have a recency bias that they pay most attention to what they've seen more recently, and So what you find is that the LSTM model.",
                    "label": 0
                },
                {
                    "sent": "Wait, I think I got this wrong in my labeling.",
                    "label": 0
                },
                {
                    "sent": "Whoops, sorry my slide.",
                    "label": 0
                },
                {
                    "sent": "My slide is.",
                    "label": 0
                },
                {
                    "sent": "Buggy with the pluses and minuses.",
                    "label": 0
                },
                {
                    "sent": "Whoops.",
                    "label": 0
                },
                {
                    "sent": "I made a Buck in doing this.",
                    "label": 0
                },
                {
                    "sent": "What I want the answer to know this is the real answer I just made.",
                    "label": 0
                },
                {
                    "sent": "Copy and paste errors in my in my slide so the LSD M says that this one is negative and this one is positive and so it seems like it's mainly influenced by what comes late in the sentence, whereas the tree Alice TM is correctly able to get that this one is negative and this one is negative.",
                    "label": 0
                },
                {
                    "sent": "Two I. Flip all those things, they out change it afterwards.",
                    "label": 0
                },
                {
                    "sent": "Whoops, yeah, so that's sort of just sort of an example, but it suggests that you can kind of better get the semantics of a sentence out using a trias TM.",
                    "label": 0
                },
                {
                    "sent": "I little bit agree with you.",
                    "label": 0
                },
                {
                    "sent": "Though I still think.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I mean this example is constructed of, you know, the first examples are found example in the second example is constructed and I agree it's less natural and it does sort of.",
                    "label": 0
                },
                {
                    "sent": "Yeah a little bit agree with you though, I still kind of suspect you should regard it as a negative example overall.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, we did not try that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Inside yeah, in some sense you're right.",
                    "label": 0
                },
                {
                    "sent": "I think that that should be able to provide some kind of solution.",
                    "label": 0
                },
                {
                    "sent": "I guess you know a lot of the bidirectional out of steam used, such As for machine translation.",
                    "label": 0
                },
                {
                    "sent": "You're just sort of having a representation above each word, which is then being used later on in your empty system.",
                    "label": 0
                },
                {
                    "sent": "I guess for something like this where you ended up doing classification task, you would have to decide how to deploy it.",
                    "label": 0
                },
                {
                    "sent": "I guess you take the two at each end and then concatenate them and classify something.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that that may well fix that.",
                    "label": 0
                },
                {
                    "sent": "I could believe that.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "For language, I'm not sure that you lose like, so envision.",
                    "label": 0
                },
                {
                    "sent": "There's been some use of tree structures, but I think for visual scene analysis there is a good argument that you lose by putting tree structures be 'cause there are lots of adjacency relationships, and if you're putting a tree structure of a visual scene, you're keeping some of the adjacency relationships, but you're losing other ones.",
                    "label": 0
                },
                {
                    "sent": "But I think that that actually isn't true of human languages, that I think human languages really do have a constituency structure of groups of words going together.",
                    "label": 0
                },
                {
                    "sent": "Semantic units that have a meaning, and that it isn't necessary to be kind of considering all the different analysis at once.",
                    "label": 0
                },
                {
                    "sent": "But you know that kind of relates to my conjecture as to whether these input dependent trees are the right way to go as opposed to using something like a convolutional structure.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "That's a good question that there's been significant debate about.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so at one level it seems like it just has to be unreasonable, but as the sentence gets longer and longer, the intuition is surely you can't represent the entire meaning of a sentence inside one vector, and that feels like it's true.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, you actually have a huge representational capacity in a real valued vector of reasonable dimension, and I think so far at least, we haven't probably come to the limits of what you can represent.",
                    "label": 0
                },
                {
                    "sent": "So most dramatically in work of the sort of RNN encoder decoders that being used for machine translation, such as that Google, it seems like.",
                    "label": 0
                },
                {
                    "sent": "To an amazing degree, you are able to encode the meaning of a whole sentence into a vector and reproduce it out in the translation.",
                    "label": 0
                },
                {
                    "sent": "And it's not very.",
                    "label": 0
                },
                {
                    "sent": "There's no clear evidence that you're exceeding the ability as to what you can store in one of them.",
                    "label": 0
                },
                {
                    "sent": "Natural sentences have abounded size in a few like that.",
                    "label": 0
                },
                {
                    "sent": "Nobody.",
                    "label": 0
                },
                {
                    "sent": "I'm so sure so there are affectively certainly limits.",
                    "label": 0
                },
                {
                    "sent": "An ingeneral spoken sentence is a shorter.",
                    "label": 0
                },
                {
                    "sent": "I mean though there are quite long.",
                    "label": 0
                },
                {
                    "sent": "Sentence is right like there are certainly sentence is that exist that are well over 100 words and actually languages vary in how long sentences are.",
                    "label": 0
                },
                {
                    "sent": "Arabic really likes long sentence is right that if you look in the statistics for written English, sentence is over.",
                    "label": 0
                },
                {
                    "sent": "70 words are pretty rare for English, but if you then go to an Arabic corpus, you're still finding quite a lot of them.",
                    "label": 0
                },
                {
                    "sent": "I guess I guess Arab, I presume in the Arabic world people aren't taught about run joining sentences with an being bad style and so you get all of these very long sentences with and and and all joined together into one big sentence.",
                    "label": 0
                },
                {
                    "sent": "OK um.",
                    "label": 0
                },
                {
                    "sent": "So I did actually have a teeny bit more material talking about natural language inference, but it's also it's 1226 already, sorry.",
                    "label": 0
                },
                {
                    "sent": "That's not my Clock is saying that's because still West Coast time I meant to be ending in 4 minutes.",
                    "label": 0
                },
                {
                    "sent": "So I'm tempted to say I should just go to the conclusion slide.",
                    "label": 0
                },
                {
                    "sent": "And if people have more questions I can do a couple more question results for a minute.",
                    "label": 0
                },
                {
                    "sent": "My conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slide So what we want to do in deep learning NLP systems actually do language understanding and build things like dialogue systems and question answering system.",
                    "label": 0
                },
                {
                    "sent": "So we don't just want word meanings.",
                    "label": 0
                },
                {
                    "sent": "We want meanings of larger units which we calculate in some kind of compositional manner.",
                    "label": 0
                },
                {
                    "sent": "And in particular I've tried to argue here of the sort of usefulness of actually doing these.",
                    "label": 0
                },
                {
                    "sent": "Import specific compositional structures that represent the kind of natural compositional structure of human language sentences.",
                    "label": 0
                }
            ]
        }
    }
}