{
    "id": "rdjsbnk6m6tj4qfqkr5t23ja5ax6gzfq",
    "title": "Learning Concept Mappings from Instance Similarity",
    "info": {
        "author": [
            "Shenghui Wang, Department of Computer Science, Faculty of Sciences, Vrije Universiteit Amsterdam (VU)"
        ],
        "published": "Nov. 24, 2008",
        "recorded": "October 2008",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc08_wang_lcm/",
    "segmentation": [
        [
            "Good morning everyone.",
            "Welcome here and would like to talk about our work learning concept mappings from instant similarity and I'm sure we want from free University of Amsterdam.",
            "This is joint work with my colleague Stephen and Gwen from University of Amsterdam."
        ],
        [
            "This is the outline of the content I would like to talk about today.",
            "Of course, first you have to introduce what kind of mapping we do and what kind of techniques we are focusing on at the moment.",
            "And then we will introduce the method we proposed using classification based on their instance similarity.",
            "Of course about how to represent the concept using instances and how to calculate the similarity between them and what kind of class fee.",
            "Fires we use to use to classify concept mapping based on this instance similarity.",
            "And what kind of research question we would like to answer from those questions?",
            "We do some experiments and would like to share some results with you and a very short summary in the end.",
            "I'm."
        ],
        [
            "A bit of context.",
            "We are in a stage project and semantic interoperability to access cultural heritage.",
            "Culture, heritage, institution.",
            "Institutions include their libraries, museums, archives, everything.",
            "Cultural heritage objects are normally annotated by different thesauri.",
            "Unfortunately, different collections are annotated by different is alright if you want to access different collections through the same one single thesaurus, it would be a bit difficult if you don't have a link between those alright.",
            "But the problem itself is not trivial, because the thesaurus thesauri are normally big, contains 10s of thousands of concepts, and collections are normally very huge.",
            "Millions of books, or millions of artifacts are really common.",
            "Of course, the altar genius, including books, manuscripts, illustrations, multimedia and objects and different countries have their own culture, heritage collections.",
            "Multilingual problems are not very common.",
            "We have to address.",
            "And Steven has."
        ],
        [
            "Introduced our instance based work last year.",
            "Because we are in a very good position because we have instances have millions of the minimum and the basic idea of the common instance based method.",
            "We have concept schemes."
        ],
        [
            "They are used to annotate different instances by measuring how much the two concepts an instance extensional information."
        ],
        [
            "We could ask how much?",
            "How strongly are those two concepts related?"
        ],
        [
            "This is.",
            "The advantage of this is they are simply to implement.",
            "And we have got already got interesting results.",
            "As we reported last year.",
            "And but disadvantages is this method requires significant common amount of common instances.",
            "If the two collections doesn't have any common instances that were the instance, but the simple instance based method doesn't really work.",
            "And another point is instances are not only literals, they have a lot.",
            "A lot of information by themselves.",
            "They are very informative and by throwing those information.",
            "And do the mapping by himself.",
            "This is really a great pity.",
            "So the method we proposed here."
        ],
        [
            "Is the same situation we have the schemes, concept schemes and each concept and not take different set of instances.",
            "These two groups of instances doesn't really need to be an overlapping each other.",
            "And somehow?"
        ],
        [
            "If you find some similarities between those instances of between those two sets of instances and the same way, if we can find a way to basically using this similarity between groups of the instances of those counts."
        ],
        [
            "We can ask answer the same kind of question how strong other related if the similarity are strong and relatedness as strong between these two concepts, you can say these two concepts could be mapped together.",
            "In order to make this method work, there are two things you have to take care of.",
            "What kind of how do we use instances to make the representation for the concepts?",
            "And based on those concept representation, what kind of similarity use and what kind of classifiers you use and to correlate the relationship between instance similarity to the concept mappings?"
        ],
        [
            "First, I would like to introduce how we represent concepts using instances.",
            "This is a simple case.",
            "We use the book collections as instances so we have concept here.",
            "Concept one are used to in annotate two books, three books and books itself themselves are represented by creator, title, publisher, descriptions, whatever, and we put all the information of the instances together but field by field and so creator of three instances put together and title together and each feature, like creator title, correspond to a bag of words after simple natural language processing like.",
            "Removing stopping words and like amortization or so.",
            "Each bag of words correspond to a vector of words.",
            "And then for we do the same thing for each concept we find.",
            "And based on those representation, we could easily compute cosine distance between the vector of words.",
            "So on the pair feature.",
            "The right hand side is so in the end the similarity between two concepts are represented by the right hand side feature, F1F2F3 end and so on.",
            "Depends on how much how many features are used to represent instances.",
            "So in at this point."
        ],
        [
            "We could say each pair of concept.",
            "It can be treated as a point in a similarity space, so its position is based on the."
        ],
        [
            "Feature of the pair, like the right hand side."
        ],
        [
            "F1F23 an hour.",
            "How positive is the label of a point?",
            "Which represents whether the pair is a positive mapping or a negative mapping is correlated with the position of this point in this space.",
            "So we have a lot of pairs of concepts and we know somehow in the beginning some parts are, some points are labeled as positive mappings and some points labeled as negative mappings.",
            "We project them into a same semantic spaces.",
            "By looking at the space and study the probability distribution, we would know in which kind of places or space position, how much, how much probability for this pair for this point to be to be labeled as positive mapping or negative mapping.",
            "Um?",
            "So when the new pairs of color concept come in, it's it's trivial to calculate the same kind of similarity feature as a right hand side, and we can project it into this space based on the position we can calculate how much how possible it could be, map labeled as.",
            "Positive mapping or how much how possible the it's labeled as negative mappings based on these two probability, we can decide whether this pairs new pairs of concept is a mapping or not."
        ],
        [
            "And there are of course there was a bit math here, but I would not go into detail.",
            "You can the can find those in the paper and it is really a simple method, but."
        ],
        [
            "Works quite well here."
        ],
        [
            "So.",
            "Um?",
            "The kind of question research question would like to answer in this paper is.",
            "Others benefits from feature similarity of instance is significant enough to predict concept mappings.",
            "And can our approach apply to copper which doesn't have any joint instances?",
            "Can this method applied to heterogeneous collections because it is very common.",
            "Different collections are heterogeneous represented if you don't count cope this problem, the method is quite limited and last point is a.",
            "Can we make some kind of qualitative use of the learn model way, namely those weightings for those features?",
            "Are they just a number or say something else?",
            "From"
        ],
        [
            "Those questions we start our experiments.",
            "We have two cases.",
            "One is the mapping DTT, bring and Brinkman which are used to annotate books in KB, which is National Library of Netherlands.",
            "Books are homogeneously represented.",
            "This is and we have millions of books, two millions of books.",
            "The second case is mapping DT Brinkman, which annotate books in KB2 DTA, which are used to annotate multimedia objects in the building cloud, which is Netherland vision sound and vision institution which collect all the broadcast of materials in Netherland and of course objects.",
            "There are differently ever presented from the books in the library.",
            "Would like to check whether it it works.",
            "Evaluation is using standard error rate to measure the performance of the classifier which is the misclassified.",
            "The number of misclassified, like examples over the OR classified samples, and use 10 folds cross validation to validate our performance and testing out some special datasets."
        ],
        [
            "First experiment is.",
            "Has to check whether this work.",
            "This similarity between instances.",
            "This method does.",
            "Does it really work?"
        ],
        [
            "So we compared with compare our method with some standard methods which include the Falcon, which is kind of one of the best system mapping we find which loads our library data, which is not trivial and we also compare with lexical similarity based methods and using the Dakar measure which we used for the last year for the instance based methods and we also put everything of instance into a single bag.",
            "Ignoring where there from where, whether they're from title or features, title or authors or whatever.",
            "And the last one is the we used 28 features for library book case.",
            "As you can see our method has the lowest error rate, which is a very good sign.",
            "At least it says the feature based the feature similarity based method works too.",
            "They have enough predictive power to predict ontology concept mappings.",
            "So."
        ],
        [
            "The next one is does it really work if we don't have enjoyed instances?",
            "Previously we used only 25 sorry 250,000 books because the ideally in text, so a simple instance based methods can work, but if.",
            "Does this more small part contribute a lot to our method to work?",
            "We basically divide the data set into two situations.",
            "One we use the whole data set to collections together and the second case is we remove the middle part of 250,000.",
            "During next books we see we want to check whether it to really contribute a lot or it really make our method fail."
        ],
        [
            "Of course.",
            "It doesn't fail.",
            "Without joint instances, of course it has less information, but the error rate are still quite low, which means this is a very exciting news because if two collection doesn't have any common instances, our method code still work, although if you have pre if you really have jury annotated instances, of course it will have better performance.",
            "This is this encouraged us to.",
            "To apply our method in two different collections and.",
            "The later experiments."
        ],
        [
            "We're showing you.",
            "Of course, as I said before, heterogeneous collections are very hominina culture heritage domain.",
            "So can this approach work for the if I have instances from different collections?",
            "And you might have noticed one of the important."
        ],
        [
            "Aspect is to select features we because we use features to calculate the similarities.",
            "And."
        ],
        [
            "Objects from different collections are obviously represented differently, So what kind of features were used to calculate the similarity between features?",
            "Naively, you can.",
            "Also, you can always use exhaustive combination by calculate the similarity between title and author between title and date whatever."
        ],
        [
            "Then it will result in a very long high dimensional.",
            "Vector in order to learn something with such huge dimension data, you need more training data set in order to avoid overfitting.",
            "So."
        ],
        [
            "So.",
            "There was two ways to reduce the dimension manually selection you have.",
            "You can ask people to say OK.",
            "The title here is should contain a similar.",
            "Formation about that description or something so you can manually calculate the select pairs for calculating similarity.",
            "And also you can use mutual information to predict what kind of to find the most informative fields, field pairs.",
            "And this is a completely automatic if you have some.",
            "Information to make mutual information to work.",
            "And so we did some experiments to compare different ways of selecting the feature.",
            "I'm."
        ],
        [
            "So would like to say whether this is feature selection can be automated or somehow semi automate."
        ],
        [
            "It.",
            "Of course we found it is.",
            "Positive example answer.",
            "Um?",
            "Using mutual information actually gives the lowest error rate, which is really good.",
            "Message because we can leave few people out of the loop with you, they don't need to come here to manually select what kind of features fields correspond to each other, and we don't need to find nor enormous training data set to afford the exhaustive selection.",
            "So we can use the mutual information to say to select the info, interesting fears to calculate similarity.",
            "I'm.",
            "So."
        ],
        [
            "OK, so and then it's going to a very.",
            "Classic issue for the machine learning methods.",
            "Any machine learning methods you may you should know that.",
            "It's quite data independent dependent.",
            "It learns what you gave them and it predicts what he learned.",
            "And so for the KB case library case we.",
            "Actually, manually evaluate spare pairs of concepts and build a Golden standard, which is caused a lot of time and energy for people to do.",
            "And still we have only 700 something good mappings.",
            "And of course you can use lexical mappings as a starting point to tray.",
            "But there was bias there.",
            "I will show you later immediately.",
            "And you can also use a background and using background knowledge to find the mappings between two SSRS.",
            "This alright.",
            "Um?",
            "So if one of the something here to say this concept from this ontology or thesaurus and one another concept from another six hours are somehow linked, so we can say they are.",
            "Mapped together so this is one of the ontology matching technique.",
            "We used it.",
            "To find the training data."
        ],
        [
            "Starting the beginning.",
            "So here are some situations about what kind of training positive examples we learn.",
            "We start with by lexical mapping.",
            "We find 2000 and 1000 something mappings, but non Mexico ones are very few.",
            "But they are very good.",
            "But guess what?",
            "We know that even two concept has exactly well different label or anything but they really mean the same thing.",
            "This is the what really kind of.",
            "A goal of ontology matching.",
            "You go over the semantic lexical barrier.",
            "So we got some positive examples and we randomly generalized.",
            "Generate the negative examples to to build this kind of training data set.",
            "And.",
            "And the purpose of to raise the training set issue is because training set is quite important for learning process.",
            "So what you gave to the learner or classifier to learn will influence the performance of the in the end."
        ],
        [
            "So we got the buyers actually from the training set.",
            "If we train on the lexical mappings, so those concept clearly has very similar labels, and if we testing on the non lexical ones, it has higher error rate than the other way around.",
            "So this is quite important message we got because.",
            "If you find similarity from the metadata fields.",
            "And they are actually much more or more informative than the one you get to label the lexical similarity between two concepts.",
            "Actually, you can see if you're trained on the non lexical ones.",
            "You have much less an error rate, less error rate than the other way round so.",
            "So the message I want to say is.",
            "Try to use the information with the metadata fields and they're more in they have more predictive power than the lexical information of concepts.",
            "And."
        ],
        [
            "Another ratio is.",
            "The issue.",
            "About the training set is the.",
            "The positive and a negative ratio in the training set.",
            "OK, thanks.",
            "As you know, the positive examples and the negatives I'm in reality is much you have much less positive mappings between the two things are I so the way you prepare our training set the ratio you put into a training set is quite somehow.",
            "And sensitive.",
            "To predict your to decide your predicted power in the end.",
            "We testing with using a different training set with the different negative positive ratio and testing on a different data one to one and one 2001 is positive and the other one is the negative forms, so they have different performance.",
            "And so.",
            "We message the lesson we."
        ],
        [
            "One is in practice the training data should be chosen to contain the representative ratio to the reality, but you can't just gave one positive example with 1 million negative example because that will fail the classifier.",
            "They don't have enough material to learn and to give the predictive to gain some predictive capacity to both types of examples.",
            "I'm."
        ],
        [
            "So last experiment we did is to check.",
            "Whether we could use some make qualitative use of those of the model we learned.",
            "Anne.",
            "As you know that the value of the learning result the Lambda reflects important importance of the feature in the process of determining similarity between concepts.",
            "So we actually look at those lambdas and to see which kind of similarity pairs similarity fields."
        ],
        [
            "Are there so in this table we can see that?",
            "Left hand side field and right inside field if the the similarity between them are calculated, they are very informative to predict the right concept mappings.",
            "So I couldn't say this is a perfect mapping between metadata fields, but they obviously and contribute are important information to say those fears are somehow related.",
            "If you want to do some.",
            "Kind of intelligent queries or something.",
            "This mappings between fields might be useful for you to do this query expansion or something."
        ],
        [
            "OK, that's about it, and this is a small summary and we use the learning method too.",
            "And to study the tool to study the relationship between similarity between instances and the concept concept mappings.",
            "So it works for heterogeneous collections and it doesn't really require joint instances.",
            "Of course, if you have that will be better and.",
            "The features can be language dependent independent, so somehow.",
            "A bit beyond not really constrained by the language, American barium and it also contributes informed important information for the metadata fields mapping, so in the future would like to try a lot of different collections and we are currently experimenting with the different languages collect collections in different languages and of course the smarter measure of similarity can contribute more to the last classifier.",
            "Um learner can also combine with lexical and structural further combined classifier in the end.",
            "So that's where there are a lot of things to do, but."
        ],
        [
            "This is what we present today.",
            "Thank you.",
            "Thanks.",
            "Question about 5 minutes question.",
            "I have two quick questions.",
            "Firstly, would you talk more about the performance of the algorithm?",
            "Like how much time it takes to finish and mapping process and the second question is, would you turn the page, turn slide to a table showing feature selection?",
            "Feature selection."
        ],
        [
            "Yeah yes next maybe."
        ],
        [
            "Table.",
            "Yes.",
            "Mutual information error rate is even better than manual selection is very interesting, so could you explain more about this?",
            "Yeah.",
            "The manual selection is the human using their domain knowledge to say this feature.",
            "These fears might be interested by might be containing the same information, but actually in the reality in different collections people might put different information into different parts and with same kind of name, but for the domain knowledge the human they might not realize how much different information is put into the fear these think.",
            "They put the same result information, so by mutual information we have some positive and negative examples.",
            "We learn and using this kind of automatic ways you know this similarity between these two fields actually contributes more to the concept mappings.",
            "So that might be because sometimes we put the temporal or spatial information to the annotation, but whereas human you wouldn't say annotation of one side compared to the temporal or spatial and not outside, so that's why it happens for in this case.",
            "And As for the computation issue.",
            "For the inference, it's just linear combination.",
            "Calculator combination is linear to the number of features you use for the training.",
            "It's based on the actually.",
            "Sorry."
        ],
        [
            "Training is based on the iterative quasi Newton method, which is might, which is quite efficient but iterative.",
            "Depends on how precise you want your classifier to learn.",
            "You maximize likelihood of the methods, so if you can afford less precise, you can stop earlier so 4000.",
            "For thousands of pairs of datasets we learn it is quite fast, like half minutes is the model is learned.",
            "So yeah, I think.",
            "It's.",
            "Yes.",
            "Question.",
            "Just a simple question.",
            "You measure this based on the ability to the classes, which are the tag given by in both Desiree to really discrete to be very discriminate.",
            "Or isn't it?",
            "Sorry, what is the method is based on the fact that you will be able globally to find the classes on the on the on the club, in the cloud of points you will be find a class inner space limitation.",
            "I was wondering how how this happened in reality and did you?",
            "Did you look into art cases where two tags are really, let's say, relatively correlated and you are not able to discriminate them, for instance?",
            "I still didn't get the tag problem OK by tags, I mean classes here hosted funds, active examples you mean?",
            "Yeah, but I imagine that in the corpora, the copies that you have, you cannot invent example when you when you don't have them.",
            "So is it the case that there are?",
            "Classes which are really basically 80% overlaps overlapping and that does it related to a training set.",
            "We positive examples we will find.",
            "Yeah, but the point then you evaluated against the we have to ask people to.",
            "Well one way is to ask people to give me the positive examples to say these two concepts are mapped.",
            "Or you can use in the different settings to find the lexical mappings or background knowledge gave the mappings.",
            "That's the positive examples based on those positive examples and some negative examples you learn the correlation between instance similarity to the labels, like most positive and the way you describe it is really looks like a process you ask people to give you more example until you got the correct results.",
            "So doesn't work this way.",
            "You ask people to give you example.",
            "You've got example, you learn and you observe with regard to your.",
            "They said no OK, and my point was is there in a test set up classes that you have trouble to discriminate with your example.",
            "So the measure we used is tenfold cross cross validation.",
            "So the data set we have the positive and negative examples.",
            "So we divide into 10 folds, using blindfolds to learn and testing on the line.",
            "The rest one so the rest are already judged by human.",
            "Some are positive examples or negative examples.",
            "So this is a one way to classify to measure the performance of the classifier, but the I'm sure that people have the judgment to say the positive example in the testing sets are good or not.",
            "That's what we learn.",
            "But maybe I can add what I understand of your question is whether we also did some quantitative evaluation of the results we get.",
            "So when we find two things that are not similar whether we checked why they're not similar, and if we didn't find something that we would believe to be similar, why we didn't, and I think the quick answer is that we haven't looked at at at the results qualitatively.",
            "Yep.",
            "OK, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good morning everyone.",
                    "label": 0
                },
                {
                    "sent": "Welcome here and would like to talk about our work learning concept mappings from instant similarity and I'm sure we want from free University of Amsterdam.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with my colleague Stephen and Gwen from University of Amsterdam.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the outline of the content I would like to talk about today.",
                    "label": 0
                },
                {
                    "sent": "Of course, first you have to introduce what kind of mapping we do and what kind of techniques we are focusing on at the moment.",
                    "label": 0
                },
                {
                    "sent": "And then we will introduce the method we proposed using classification based on their instance similarity.",
                    "label": 1
                },
                {
                    "sent": "Of course about how to represent the concept using instances and how to calculate the similarity between them and what kind of class fee.",
                    "label": 0
                },
                {
                    "sent": "Fires we use to use to classify concept mapping based on this instance similarity.",
                    "label": 0
                },
                {
                    "sent": "And what kind of research question we would like to answer from those questions?",
                    "label": 0
                },
                {
                    "sent": "We do some experiments and would like to share some results with you and a very short summary in the end.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A bit of context.",
                    "label": 0
                },
                {
                    "sent": "We are in a stage project and semantic interoperability to access cultural heritage.",
                    "label": 1
                },
                {
                    "sent": "Culture, heritage, institution.",
                    "label": 0
                },
                {
                    "sent": "Institutions include their libraries, museums, archives, everything.",
                    "label": 0
                },
                {
                    "sent": "Cultural heritage objects are normally annotated by different thesauri.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, different collections are annotated by different is alright if you want to access different collections through the same one single thesaurus, it would be a bit difficult if you don't have a link between those alright.",
                    "label": 1
                },
                {
                    "sent": "But the problem itself is not trivial, because the thesaurus thesauri are normally big, contains 10s of thousands of concepts, and collections are normally very huge.",
                    "label": 1
                },
                {
                    "sent": "Millions of books, or millions of artifacts are really common.",
                    "label": 0
                },
                {
                    "sent": "Of course, the altar genius, including books, manuscripts, illustrations, multimedia and objects and different countries have their own culture, heritage collections.",
                    "label": 0
                },
                {
                    "sent": "Multilingual problems are not very common.",
                    "label": 0
                },
                {
                    "sent": "We have to address.",
                    "label": 0
                },
                {
                    "sent": "And Steven has.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Introduced our instance based work last year.",
                    "label": 0
                },
                {
                    "sent": "Because we are in a very good position because we have instances have millions of the minimum and the basic idea of the common instance based method.",
                    "label": 1
                },
                {
                    "sent": "We have concept schemes.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They are used to annotate different instances by measuring how much the two concepts an instance extensional information.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We could ask how much?",
                    "label": 0
                },
                {
                    "sent": "How strongly are those two concepts related?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "The advantage of this is they are simply to implement.",
                    "label": 1
                },
                {
                    "sent": "And we have got already got interesting results.",
                    "label": 1
                },
                {
                    "sent": "As we reported last year.",
                    "label": 0
                },
                {
                    "sent": "And but disadvantages is this method requires significant common amount of common instances.",
                    "label": 1
                },
                {
                    "sent": "If the two collections doesn't have any common instances that were the instance, but the simple instance based method doesn't really work.",
                    "label": 0
                },
                {
                    "sent": "And another point is instances are not only literals, they have a lot.",
                    "label": 0
                },
                {
                    "sent": "A lot of information by themselves.",
                    "label": 0
                },
                {
                    "sent": "They are very informative and by throwing those information.",
                    "label": 0
                },
                {
                    "sent": "And do the mapping by himself.",
                    "label": 0
                },
                {
                    "sent": "This is really a great pity.",
                    "label": 0
                },
                {
                    "sent": "So the method we proposed here.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the same situation we have the schemes, concept schemes and each concept and not take different set of instances.",
                    "label": 0
                },
                {
                    "sent": "These two groups of instances doesn't really need to be an overlapping each other.",
                    "label": 0
                },
                {
                    "sent": "And somehow?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you find some similarities between those instances of between those two sets of instances and the same way, if we can find a way to basically using this similarity between groups of the instances of those counts.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can ask answer the same kind of question how strong other related if the similarity are strong and relatedness as strong between these two concepts, you can say these two concepts could be mapped together.",
                    "label": 0
                },
                {
                    "sent": "In order to make this method work, there are two things you have to take care of.",
                    "label": 0
                },
                {
                    "sent": "What kind of how do we use instances to make the representation for the concepts?",
                    "label": 0
                },
                {
                    "sent": "And based on those concept representation, what kind of similarity use and what kind of classifiers you use and to correlate the relationship between instance similarity to the concept mappings?",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, I would like to introduce how we represent concepts using instances.",
                    "label": 0
                },
                {
                    "sent": "This is a simple case.",
                    "label": 0
                },
                {
                    "sent": "We use the book collections as instances so we have concept here.",
                    "label": 0
                },
                {
                    "sent": "Concept one are used to in annotate two books, three books and books itself themselves are represented by creator, title, publisher, descriptions, whatever, and we put all the information of the instances together but field by field and so creator of three instances put together and title together and each feature, like creator title, correspond to a bag of words after simple natural language processing like.",
                    "label": 1
                },
                {
                    "sent": "Removing stopping words and like amortization or so.",
                    "label": 1
                },
                {
                    "sent": "Each bag of words correspond to a vector of words.",
                    "label": 1
                },
                {
                    "sent": "And then for we do the same thing for each concept we find.",
                    "label": 1
                },
                {
                    "sent": "And based on those representation, we could easily compute cosine distance between the vector of words.",
                    "label": 0
                },
                {
                    "sent": "So on the pair feature.",
                    "label": 0
                },
                {
                    "sent": "The right hand side is so in the end the similarity between two concepts are represented by the right hand side feature, F1F2F3 end and so on.",
                    "label": 0
                },
                {
                    "sent": "Depends on how much how many features are used to represent instances.",
                    "label": 0
                },
                {
                    "sent": "So in at this point.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We could say each pair of concept.",
                    "label": 0
                },
                {
                    "sent": "It can be treated as a point in a similarity space, so its position is based on the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Feature of the pair, like the right hand side.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "F1F23 an hour.",
                    "label": 0
                },
                {
                    "sent": "How positive is the label of a point?",
                    "label": 1
                },
                {
                    "sent": "Which represents whether the pair is a positive mapping or a negative mapping is correlated with the position of this point in this space.",
                    "label": 1
                },
                {
                    "sent": "So we have a lot of pairs of concepts and we know somehow in the beginning some parts are, some points are labeled as positive mappings and some points labeled as negative mappings.",
                    "label": 0
                },
                {
                    "sent": "We project them into a same semantic spaces.",
                    "label": 0
                },
                {
                    "sent": "By looking at the space and study the probability distribution, we would know in which kind of places or space position, how much, how much probability for this pair for this point to be to be labeled as positive mapping or negative mapping.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So when the new pairs of color concept come in, it's it's trivial to calculate the same kind of similarity feature as a right hand side, and we can project it into this space based on the position we can calculate how much how possible it could be, map labeled as.",
                    "label": 0
                },
                {
                    "sent": "Positive mapping or how much how possible the it's labeled as negative mappings based on these two probability, we can decide whether this pairs new pairs of concept is a mapping or not.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there are of course there was a bit math here, but I would not go into detail.",
                    "label": 0
                },
                {
                    "sent": "You can the can find those in the paper and it is really a simple method, but.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Works quite well here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The kind of question research question would like to answer in this paper is.",
                    "label": 0
                },
                {
                    "sent": "Others benefits from feature similarity of instance is significant enough to predict concept mappings.",
                    "label": 0
                },
                {
                    "sent": "And can our approach apply to copper which doesn't have any joint instances?",
                    "label": 1
                },
                {
                    "sent": "Can this method applied to heterogeneous collections because it is very common.",
                    "label": 1
                },
                {
                    "sent": "Different collections are heterogeneous represented if you don't count cope this problem, the method is quite limited and last point is a.",
                    "label": 0
                },
                {
                    "sent": "Can we make some kind of qualitative use of the learn model way, namely those weightings for those features?",
                    "label": 1
                },
                {
                    "sent": "Are they just a number or say something else?",
                    "label": 0
                },
                {
                    "sent": "From",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those questions we start our experiments.",
                    "label": 0
                },
                {
                    "sent": "We have two cases.",
                    "label": 0
                },
                {
                    "sent": "One is the mapping DTT, bring and Brinkman which are used to annotate books in KB, which is National Library of Netherlands.",
                    "label": 0
                },
                {
                    "sent": "Books are homogeneously represented.",
                    "label": 0
                },
                {
                    "sent": "This is and we have millions of books, two millions of books.",
                    "label": 0
                },
                {
                    "sent": "The second case is mapping DT Brinkman, which annotate books in KB2 DTA, which are used to annotate multimedia objects in the building cloud, which is Netherland vision sound and vision institution which collect all the broadcast of materials in Netherland and of course objects.",
                    "label": 0
                },
                {
                    "sent": "There are differently ever presented from the books in the library.",
                    "label": 0
                },
                {
                    "sent": "Would like to check whether it it works.",
                    "label": 0
                },
                {
                    "sent": "Evaluation is using standard error rate to measure the performance of the classifier which is the misclassified.",
                    "label": 0
                },
                {
                    "sent": "The number of misclassified, like examples over the OR classified samples, and use 10 folds cross validation to validate our performance and testing out some special datasets.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First experiment is.",
                    "label": 0
                },
                {
                    "sent": "Has to check whether this work.",
                    "label": 0
                },
                {
                    "sent": "This similarity between instances.",
                    "label": 0
                },
                {
                    "sent": "This method does.",
                    "label": 0
                },
                {
                    "sent": "Does it really work?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we compared with compare our method with some standard methods which include the Falcon, which is kind of one of the best system mapping we find which loads our library data, which is not trivial and we also compare with lexical similarity based methods and using the Dakar measure which we used for the last year for the instance based methods and we also put everything of instance into a single bag.",
                    "label": 0
                },
                {
                    "sent": "Ignoring where there from where, whether they're from title or features, title or authors or whatever.",
                    "label": 0
                },
                {
                    "sent": "And the last one is the we used 28 features for library book case.",
                    "label": 0
                },
                {
                    "sent": "As you can see our method has the lowest error rate, which is a very good sign.",
                    "label": 0
                },
                {
                    "sent": "At least it says the feature based the feature similarity based method works too.",
                    "label": 0
                },
                {
                    "sent": "They have enough predictive power to predict ontology concept mappings.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next one is does it really work if we don't have enjoyed instances?",
                    "label": 0
                },
                {
                    "sent": "Previously we used only 25 sorry 250,000 books because the ideally in text, so a simple instance based methods can work, but if.",
                    "label": 0
                },
                {
                    "sent": "Does this more small part contribute a lot to our method to work?",
                    "label": 0
                },
                {
                    "sent": "We basically divide the data set into two situations.",
                    "label": 0
                },
                {
                    "sent": "One we use the whole data set to collections together and the second case is we remove the middle part of 250,000.",
                    "label": 0
                },
                {
                    "sent": "During next books we see we want to check whether it to really contribute a lot or it really make our method fail.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "It doesn't fail.",
                    "label": 0
                },
                {
                    "sent": "Without joint instances, of course it has less information, but the error rate are still quite low, which means this is a very exciting news because if two collection doesn't have any common instances, our method code still work, although if you have pre if you really have jury annotated instances, of course it will have better performance.",
                    "label": 0
                },
                {
                    "sent": "This is this encouraged us to.",
                    "label": 0
                },
                {
                    "sent": "To apply our method in two different collections and.",
                    "label": 0
                },
                {
                    "sent": "The later experiments.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're showing you.",
                    "label": 0
                },
                {
                    "sent": "Of course, as I said before, heterogeneous collections are very hominina culture heritage domain.",
                    "label": 0
                },
                {
                    "sent": "So can this approach work for the if I have instances from different collections?",
                    "label": 0
                },
                {
                    "sent": "And you might have noticed one of the important.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Aspect is to select features we because we use features to calculate the similarities.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Objects from different collections are obviously represented differently, So what kind of features were used to calculate the similarity between features?",
                    "label": 0
                },
                {
                    "sent": "Naively, you can.",
                    "label": 0
                },
                {
                    "sent": "Also, you can always use exhaustive combination by calculate the similarity between title and author between title and date whatever.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then it will result in a very long high dimensional.",
                    "label": 1
                },
                {
                    "sent": "Vector in order to learn something with such huge dimension data, you need more training data set in order to avoid overfitting.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There was two ways to reduce the dimension manually selection you have.",
                    "label": 0
                },
                {
                    "sent": "You can ask people to say OK.",
                    "label": 0
                },
                {
                    "sent": "The title here is should contain a similar.",
                    "label": 0
                },
                {
                    "sent": "Formation about that description or something so you can manually calculate the select pairs for calculating similarity.",
                    "label": 0
                },
                {
                    "sent": "And also you can use mutual information to predict what kind of to find the most informative fields, field pairs.",
                    "label": 1
                },
                {
                    "sent": "And this is a completely automatic if you have some.",
                    "label": 0
                },
                {
                    "sent": "Information to make mutual information to work.",
                    "label": 0
                },
                {
                    "sent": "And so we did some experiments to compare different ways of selecting the feature.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So would like to say whether this is feature selection can be automated or somehow semi automate.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "Of course we found it is.",
                    "label": 0
                },
                {
                    "sent": "Positive example answer.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Using mutual information actually gives the lowest error rate, which is really good.",
                    "label": 1
                },
                {
                    "sent": "Message because we can leave few people out of the loop with you, they don't need to come here to manually select what kind of features fields correspond to each other, and we don't need to find nor enormous training data set to afford the exhaustive selection.",
                    "label": 1
                },
                {
                    "sent": "So we can use the mutual information to say to select the info, interesting fears to calculate similarity.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so and then it's going to a very.",
                    "label": 0
                },
                {
                    "sent": "Classic issue for the machine learning methods.",
                    "label": 0
                },
                {
                    "sent": "Any machine learning methods you may you should know that.",
                    "label": 0
                },
                {
                    "sent": "It's quite data independent dependent.",
                    "label": 0
                },
                {
                    "sent": "It learns what you gave them and it predicts what he learned.",
                    "label": 0
                },
                {
                    "sent": "And so for the KB case library case we.",
                    "label": 0
                },
                {
                    "sent": "Actually, manually evaluate spare pairs of concepts and build a Golden standard, which is caused a lot of time and energy for people to do.",
                    "label": 1
                },
                {
                    "sent": "And still we have only 700 something good mappings.",
                    "label": 0
                },
                {
                    "sent": "And of course you can use lexical mappings as a starting point to tray.",
                    "label": 0
                },
                {
                    "sent": "But there was bias there.",
                    "label": 0
                },
                {
                    "sent": "I will show you later immediately.",
                    "label": 0
                },
                {
                    "sent": "And you can also use a background and using background knowledge to find the mappings between two SSRS.",
                    "label": 0
                },
                {
                    "sent": "This alright.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So if one of the something here to say this concept from this ontology or thesaurus and one another concept from another six hours are somehow linked, so we can say they are.",
                    "label": 0
                },
                {
                    "sent": "Mapped together so this is one of the ontology matching technique.",
                    "label": 0
                },
                {
                    "sent": "We used it.",
                    "label": 0
                },
                {
                    "sent": "To find the training data.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Starting the beginning.",
                    "label": 0
                },
                {
                    "sent": "So here are some situations about what kind of training positive examples we learn.",
                    "label": 0
                },
                {
                    "sent": "We start with by lexical mapping.",
                    "label": 0
                },
                {
                    "sent": "We find 2000 and 1000 something mappings, but non Mexico ones are very few.",
                    "label": 0
                },
                {
                    "sent": "But they are very good.",
                    "label": 0
                },
                {
                    "sent": "But guess what?",
                    "label": 0
                },
                {
                    "sent": "We know that even two concept has exactly well different label or anything but they really mean the same thing.",
                    "label": 0
                },
                {
                    "sent": "This is the what really kind of.",
                    "label": 0
                },
                {
                    "sent": "A goal of ontology matching.",
                    "label": 0
                },
                {
                    "sent": "You go over the semantic lexical barrier.",
                    "label": 0
                },
                {
                    "sent": "So we got some positive examples and we randomly generalized.",
                    "label": 1
                },
                {
                    "sent": "Generate the negative examples to to build this kind of training data set.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "And the purpose of to raise the training set issue is because training set is quite important for learning process.",
                    "label": 1
                },
                {
                    "sent": "So what you gave to the learner or classifier to learn will influence the performance of the in the end.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we got the buyers actually from the training set.",
                    "label": 1
                },
                {
                    "sent": "If we train on the lexical mappings, so those concept clearly has very similar labels, and if we testing on the non lexical ones, it has higher error rate than the other way around.",
                    "label": 0
                },
                {
                    "sent": "So this is quite important message we got because.",
                    "label": 0
                },
                {
                    "sent": "If you find similarity from the metadata fields.",
                    "label": 0
                },
                {
                    "sent": "And they are actually much more or more informative than the one you get to label the lexical similarity between two concepts.",
                    "label": 0
                },
                {
                    "sent": "Actually, you can see if you're trained on the non lexical ones.",
                    "label": 1
                },
                {
                    "sent": "You have much less an error rate, less error rate than the other way round so.",
                    "label": 0
                },
                {
                    "sent": "So the message I want to say is.",
                    "label": 0
                },
                {
                    "sent": "Try to use the information with the metadata fields and they're more in they have more predictive power than the lexical information of concepts.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another ratio is.",
                    "label": 0
                },
                {
                    "sent": "The issue.",
                    "label": 0
                },
                {
                    "sent": "About the training set is the.",
                    "label": 1
                },
                {
                    "sent": "The positive and a negative ratio in the training set.",
                    "label": 1
                },
                {
                    "sent": "OK, thanks.",
                    "label": 0
                },
                {
                    "sent": "As you know, the positive examples and the negatives I'm in reality is much you have much less positive mappings between the two things are I so the way you prepare our training set the ratio you put into a training set is quite somehow.",
                    "label": 1
                },
                {
                    "sent": "And sensitive.",
                    "label": 0
                },
                {
                    "sent": "To predict your to decide your predicted power in the end.",
                    "label": 0
                },
                {
                    "sent": "We testing with using a different training set with the different negative positive ratio and testing on a different data one to one and one 2001 is positive and the other one is the negative forms, so they have different performance.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "We message the lesson we.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One is in practice the training data should be chosen to contain the representative ratio to the reality, but you can't just gave one positive example with 1 million negative example because that will fail the classifier.",
                    "label": 1
                },
                {
                    "sent": "They don't have enough material to learn and to give the predictive to gain some predictive capacity to both types of examples.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So last experiment we did is to check.",
                    "label": 0
                },
                {
                    "sent": "Whether we could use some make qualitative use of those of the model we learned.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "As you know that the value of the learning result the Lambda reflects important importance of the feature in the process of determining similarity between concepts.",
                    "label": 1
                },
                {
                    "sent": "So we actually look at those lambdas and to see which kind of similarity pairs similarity fields.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are there so in this table we can see that?",
                    "label": 0
                },
                {
                    "sent": "Left hand side field and right inside field if the the similarity between them are calculated, they are very informative to predict the right concept mappings.",
                    "label": 0
                },
                {
                    "sent": "So I couldn't say this is a perfect mapping between metadata fields, but they obviously and contribute are important information to say those fears are somehow related.",
                    "label": 0
                },
                {
                    "sent": "If you want to do some.",
                    "label": 0
                },
                {
                    "sent": "Kind of intelligent queries or something.",
                    "label": 0
                },
                {
                    "sent": "This mappings between fields might be useful for you to do this query expansion or something.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, that's about it, and this is a small summary and we use the learning method too.",
                    "label": 1
                },
                {
                    "sent": "And to study the tool to study the relationship between similarity between instances and the concept concept mappings.",
                    "label": 0
                },
                {
                    "sent": "So it works for heterogeneous collections and it doesn't really require joint instances.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you have that will be better and.",
                    "label": 0
                },
                {
                    "sent": "The features can be language dependent independent, so somehow.",
                    "label": 0
                },
                {
                    "sent": "A bit beyond not really constrained by the language, American barium and it also contributes informed important information for the metadata fields mapping, so in the future would like to try a lot of different collections and we are currently experimenting with the different languages collect collections in different languages and of course the smarter measure of similarity can contribute more to the last classifier.",
                    "label": 1
                },
                {
                    "sent": "Um learner can also combine with lexical and structural further combined classifier in the end.",
                    "label": 0
                },
                {
                    "sent": "So that's where there are a lot of things to do, but.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is what we present today.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Question about 5 minutes question.",
                    "label": 0
                },
                {
                    "sent": "I have two quick questions.",
                    "label": 0
                },
                {
                    "sent": "Firstly, would you talk more about the performance of the algorithm?",
                    "label": 0
                },
                {
                    "sent": "Like how much time it takes to finish and mapping process and the second question is, would you turn the page, turn slide to a table showing feature selection?",
                    "label": 0
                },
                {
                    "sent": "Feature selection.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah yes next maybe.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Table.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Mutual information error rate is even better than manual selection is very interesting, so could you explain more about this?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "The manual selection is the human using their domain knowledge to say this feature.",
                    "label": 0
                },
                {
                    "sent": "These fears might be interested by might be containing the same information, but actually in the reality in different collections people might put different information into different parts and with same kind of name, but for the domain knowledge the human they might not realize how much different information is put into the fear these think.",
                    "label": 0
                },
                {
                    "sent": "They put the same result information, so by mutual information we have some positive and negative examples.",
                    "label": 0
                },
                {
                    "sent": "We learn and using this kind of automatic ways you know this similarity between these two fields actually contributes more to the concept mappings.",
                    "label": 0
                },
                {
                    "sent": "So that might be because sometimes we put the temporal or spatial information to the annotation, but whereas human you wouldn't say annotation of one side compared to the temporal or spatial and not outside, so that's why it happens for in this case.",
                    "label": 0
                },
                {
                    "sent": "And As for the computation issue.",
                    "label": 0
                },
                {
                    "sent": "For the inference, it's just linear combination.",
                    "label": 0
                },
                {
                    "sent": "Calculator combination is linear to the number of features you use for the training.",
                    "label": 1
                },
                {
                    "sent": "It's based on the actually.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Training is based on the iterative quasi Newton method, which is might, which is quite efficient but iterative.",
                    "label": 1
                },
                {
                    "sent": "Depends on how precise you want your classifier to learn.",
                    "label": 0
                },
                {
                    "sent": "You maximize likelihood of the methods, so if you can afford less precise, you can stop earlier so 4000.",
                    "label": 0
                },
                {
                    "sent": "For thousands of pairs of datasets we learn it is quite fast, like half minutes is the model is learned.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I think.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Just a simple question.",
                    "label": 0
                },
                {
                    "sent": "You measure this based on the ability to the classes, which are the tag given by in both Desiree to really discrete to be very discriminate.",
                    "label": 0
                },
                {
                    "sent": "Or isn't it?",
                    "label": 0
                },
                {
                    "sent": "Sorry, what is the method is based on the fact that you will be able globally to find the classes on the on the on the club, in the cloud of points you will be find a class inner space limitation.",
                    "label": 0
                },
                {
                    "sent": "I was wondering how how this happened in reality and did you?",
                    "label": 0
                },
                {
                    "sent": "Did you look into art cases where two tags are really, let's say, relatively correlated and you are not able to discriminate them, for instance?",
                    "label": 0
                },
                {
                    "sent": "I still didn't get the tag problem OK by tags, I mean classes here hosted funds, active examples you mean?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I imagine that in the corpora, the copies that you have, you cannot invent example when you when you don't have them.",
                    "label": 0
                },
                {
                    "sent": "So is it the case that there are?",
                    "label": 0
                },
                {
                    "sent": "Classes which are really basically 80% overlaps overlapping and that does it related to a training set.",
                    "label": 0
                },
                {
                    "sent": "We positive examples we will find.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but the point then you evaluated against the we have to ask people to.",
                    "label": 0
                },
                {
                    "sent": "Well one way is to ask people to give me the positive examples to say these two concepts are mapped.",
                    "label": 0
                },
                {
                    "sent": "Or you can use in the different settings to find the lexical mappings or background knowledge gave the mappings.",
                    "label": 0
                },
                {
                    "sent": "That's the positive examples based on those positive examples and some negative examples you learn the correlation between instance similarity to the labels, like most positive and the way you describe it is really looks like a process you ask people to give you more example until you got the correct results.",
                    "label": 0
                },
                {
                    "sent": "So doesn't work this way.",
                    "label": 0
                },
                {
                    "sent": "You ask people to give you example.",
                    "label": 0
                },
                {
                    "sent": "You've got example, you learn and you observe with regard to your.",
                    "label": 0
                },
                {
                    "sent": "They said no OK, and my point was is there in a test set up classes that you have trouble to discriminate with your example.",
                    "label": 0
                },
                {
                    "sent": "So the measure we used is tenfold cross cross validation.",
                    "label": 0
                },
                {
                    "sent": "So the data set we have the positive and negative examples.",
                    "label": 0
                },
                {
                    "sent": "So we divide into 10 folds, using blindfolds to learn and testing on the line.",
                    "label": 0
                },
                {
                    "sent": "The rest one so the rest are already judged by human.",
                    "label": 0
                },
                {
                    "sent": "Some are positive examples or negative examples.",
                    "label": 0
                },
                {
                    "sent": "So this is a one way to classify to measure the performance of the classifier, but the I'm sure that people have the judgment to say the positive example in the testing sets are good or not.",
                    "label": 0
                },
                {
                    "sent": "That's what we learn.",
                    "label": 0
                },
                {
                    "sent": "But maybe I can add what I understand of your question is whether we also did some quantitative evaluation of the results we get.",
                    "label": 0
                },
                {
                    "sent": "So when we find two things that are not similar whether we checked why they're not similar, and if we didn't find something that we would believe to be similar, why we didn't, and I think the quick answer is that we haven't looked at at at the results qualitatively.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}