{
    "id": "6to4i2udrignsmsgi7bob7f4bwud247p",
    "title": "Variable Selection is Hard",
    "info": {
        "author": [
            "Justin Thaler, Yahoo! Research"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_thaler_variable_selection/",
    "segmentation": [
        [
            "Input to our problem."
        ],
        [
            "Is a matrix B with M, Rosen, P columns and suppose I promise you that there's some K sparse vector X star such that B * X star is the all ones vector, so the goal is we're going to demand that our algorithm for this problem output some vector X, who sparsity is approximately K, and who that be Maps to something reasonably close to the all ones vector.",
            "So we're allowing two forms of approximation here.",
            "We're demanding or vector output.",
            "Our algorithm output a vector that is approximately case barsan that is only approximately equal to the all ones vector.",
            "So I'm going to call the slop that we allow on the sparsity.",
            "I'm going to call that G which think of as a function growing with the number of columns of the matrix, and I'm going to allow the slop in the accuracy.",
            "That is, how close BX is to the all ones vector.",
            "I'm going to call that slop H, which you should also think of as a function.",
            "OK, so problems like this and it's noisy variants are central to.",
            "Model selection in statistics for the obvious reason that sparse solutions are both simple and tend to have low generalization error.",
            "So we're interested in determining how hard problems like this are."
        ],
        [
            "So there's a trivial exact algorithm for the problem that runs in time roughly into the K. What this does it doesn't quite do what I wrote on the slide it for every subset of the coordinates of X, every subset of size K. It just uses, say Gaussian elimination, to determine if there's a solution only using those coordinates.",
            "OK, so this has no slop in either the sparsity or the accuracy, but it's not efficient.",
            "It takes time.",
            "End of the K, and there are many eficient algorithms for the problem which sacrificed at least one of the two desiderata, either sparsity or accuracy.",
            "So many algorithms based on L1 relaxation, such as lasso.",
            "They relaxed the sparsity constraint which is non convex to a convex constraint.",
            "They allow they cheat on the accuracy, but they don't have to cheat on the sparsity and there are others that sheet on the sparsity but not the accuracy.",
            "But all of the analysis say that these algorithms might have to cheat a whole lot if the matrix is very ill conditioned, and in fact some of the guarantees become meaningless if the matrix is sufficiently low condition.",
            "So the main result of this work is that, at least based on the standard complexity assumption, this situation is inherent.",
            "That is, there is no efficient algorithm that works for general matrices, even if you allow the algorithm to cheat on both the sparsity and the accuracy."
        ],
        [
            "So to be a little more precise about what we show, to say it in a sentence, we show that the problem is hard even if the slop on the sparsity is allowed to grow nearly polynomially quickly in the number of columns P and the slop in, the accuracy is allowed to grow nearly linearly quickly in M. OK so just to say quantitatively what we show the hardness result holds if the slop and the accuracy this parameter G grows like 2 to the log to the one minus Delta P for any Delta strictly greater than zero.",
            "So this is almost polynomial P. If we could handle Delta equals zero, this would actually be polynomial P, But we can't quite get there.",
            "We believe it's hard even in that case.",
            "Hyper Bullet Point in a second about that and then the harness holds even if the slop in the accuracy grows nearly linearly in M. So let's say M to the one minus some other positive constant.",
            "OK, so and assuming a strong enough existence of strong enough PCPS, which I think are quite reasonable to believe exists.",
            "The problem is hard.",
            "Even if the slop in the accuracy grows polynomially quickly in P. So I just want to close."
        ],
        [
            "We've got one minute left to briefly compared to prior work in one sentence, which is to say that there have been several other hardness results for problems like this, but we are the first hardness result that holds even if the algorithm can cheat on both the sparsity and the accuracy, so that's the main contribution of the work, so thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Input to our problem.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is a matrix B with M, Rosen, P columns and suppose I promise you that there's some K sparse vector X star such that B * X star is the all ones vector, so the goal is we're going to demand that our algorithm for this problem output some vector X, who sparsity is approximately K, and who that be Maps to something reasonably close to the all ones vector.",
                    "label": 0
                },
                {
                    "sent": "So we're allowing two forms of approximation here.",
                    "label": 0
                },
                {
                    "sent": "We're demanding or vector output.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm output a vector that is approximately case barsan that is only approximately equal to the all ones vector.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to call the slop that we allow on the sparsity.",
                    "label": 0
                },
                {
                    "sent": "I'm going to call that G which think of as a function growing with the number of columns of the matrix, and I'm going to allow the slop in the accuracy.",
                    "label": 0
                },
                {
                    "sent": "That is, how close BX is to the all ones vector.",
                    "label": 0
                },
                {
                    "sent": "I'm going to call that slop H, which you should also think of as a function.",
                    "label": 0
                },
                {
                    "sent": "OK, so problems like this and it's noisy variants are central to.",
                    "label": 1
                },
                {
                    "sent": "Model selection in statistics for the obvious reason that sparse solutions are both simple and tend to have low generalization error.",
                    "label": 0
                },
                {
                    "sent": "So we're interested in determining how hard problems like this are.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's a trivial exact algorithm for the problem that runs in time roughly into the K. What this does it doesn't quite do what I wrote on the slide it for every subset of the coordinates of X, every subset of size K. It just uses, say Gaussian elimination, to determine if there's a solution only using those coordinates.",
                    "label": 0
                },
                {
                    "sent": "OK, so this has no slop in either the sparsity or the accuracy, but it's not efficient.",
                    "label": 0
                },
                {
                    "sent": "It takes time.",
                    "label": 0
                },
                {
                    "sent": "End of the K, and there are many eficient algorithms for the problem which sacrificed at least one of the two desiderata, either sparsity or accuracy.",
                    "label": 0
                },
                {
                    "sent": "So many algorithms based on L1 relaxation, such as lasso.",
                    "label": 0
                },
                {
                    "sent": "They relaxed the sparsity constraint which is non convex to a convex constraint.",
                    "label": 0
                },
                {
                    "sent": "They allow they cheat on the accuracy, but they don't have to cheat on the sparsity and there are others that sheet on the sparsity but not the accuracy.",
                    "label": 1
                },
                {
                    "sent": "But all of the analysis say that these algorithms might have to cheat a whole lot if the matrix is very ill conditioned, and in fact some of the guarantees become meaningless if the matrix is sufficiently low condition.",
                    "label": 0
                },
                {
                    "sent": "So the main result of this work is that, at least based on the standard complexity assumption, this situation is inherent.",
                    "label": 1
                },
                {
                    "sent": "That is, there is no efficient algorithm that works for general matrices, even if you allow the algorithm to cheat on both the sparsity and the accuracy.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to be a little more precise about what we show, to say it in a sentence, we show that the problem is hard even if the slop on the sparsity is allowed to grow nearly polynomially quickly in the number of columns P and the slop in, the accuracy is allowed to grow nearly linearly quickly in M. OK so just to say quantitatively what we show the hardness result holds if the slop and the accuracy this parameter G grows like 2 to the log to the one minus Delta P for any Delta strictly greater than zero.",
                    "label": 1
                },
                {
                    "sent": "So this is almost polynomial P. If we could handle Delta equals zero, this would actually be polynomial P, But we can't quite get there.",
                    "label": 0
                },
                {
                    "sent": "We believe it's hard even in that case.",
                    "label": 0
                },
                {
                    "sent": "Hyper Bullet Point in a second about that and then the harness holds even if the slop in the accuracy grows nearly linearly in M. So let's say M to the one minus some other positive constant.",
                    "label": 0
                },
                {
                    "sent": "OK, so and assuming a strong enough existence of strong enough PCPS, which I think are quite reasonable to believe exists.",
                    "label": 0
                },
                {
                    "sent": "The problem is hard.",
                    "label": 0
                },
                {
                    "sent": "Even if the slop in the accuracy grows polynomially quickly in P. So I just want to close.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've got one minute left to briefly compared to prior work in one sentence, which is to say that there have been several other hardness results for problems like this, but we are the first hardness result that holds even if the algorithm can cheat on both the sparsity and the accuracy, so that's the main contribution of the work, so thank you.",
                    "label": 0
                }
            ]
        }
    }
}