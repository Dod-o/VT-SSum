{
    "id": "i3wenn6ikpoxmx3hbko4sircmtej4zzt",
    "title": "Learning to Rank on a Cluster using Boosted Decision Trees",
    "info": {
        "author": [
            "Krysta M. Svore, Microsoft Research"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_svore_lrc/",
    "segmentation": [
        [
            "Hi, I'm Krista Sabori and this is joint work with Chris Burgess and we also a lot of things to offer Declan, Matt Richardson and other people in our group for helping with this implementation.",
            "I'm going to talk about a continuation really of the previous talk.",
            "Also boosted decision trees and how we use those for ranking on a cluster."
        ],
        [
            "So obviously the motivation here is that for us, especially at Microsoft, we have extremely large datasets available to us up to terabytes, a day of search and user interaction.",
            "Data hardware is cheap enough we can buy machines to set up our own cluster, and if we can really decrease the model training time, then we might be able to solve problems that we haven't previously been able to have good solutions for now that we have so much data available.",
            "It also gives us faster throughput for experimentation.",
            "So what we do here is we actually distribute Lambda Mart, which is boosted regression tree.",
            "Into the Friedman we just heard about an here the it's a gradient boosted decision tree and the gradients are approximated using Lambda functions.",
            "Of course they are constructed incrementally, so you have a summation over all of these weak hypothesis and the weak hypothesis here again, are these regression trees an for this work?",
            "We're looking at roughly 100 to 200 leaves.",
            "And we chose to use Lambda Mart as this was one of the key components of the winning system for the Yahoo Learning Drink challenge that occur die CML."
        ],
        [
            "Spring.",
            "So we take actually two approaches.",
            "So the first is feature distribution and the 2nd is data distribution.",
            "So feature distribution we essentially heard about just previously, so there were going.",
            "We actually just assume that the data fits in memory on every node, and then we distribute the tree split or the vertex split computations.",
            "And this will produce a model that's equivalent to the model that is trained with the centralized model training on all the data on the single machine in a single place, right?",
            "And then the data distribution technique.",
            "Here we want to consider how do we get around scaling?",
            "We don't want to be linear in the number of examples, so we consider we want to be constant communication constant in the number of examples.",
            "So here we actually distribute the data across the cluster.",
            "So the data here does not fit on main memory of a single machine, so you have to move to more machines to train on all of the data, and we actually attempt to use this data sitting on the various cluster nodes as a kind of massive cross validation.",
            "So each node is training a week hypothesis and ideas or the hope is that those hypothesis are diverse, but we look at two methods for selecting the week hypotheses among all of these nodes.",
            "One is random and then one."
        ],
        [
            "It is based on the best cross validation accuracy, so just quickly in one minute.",
            "Here the time comparisons.",
            "So on the left.",
            "This is for the feature distributed version.",
            "We have two types of cluster nodes that we looked at.",
            "This is we have 15 million query URL pairs here.",
            "As the number of workers grows we're going from 1 to 32 workers.",
            "You can see that we're achieving a pretty good."
        ],
        [
            "Some speed up.",
            "On the right, this is for the data distributed version, so the number of workers.",
            "Here we assume that each node can only store 3500 queries, so we're simulating the case where you've run out of memory and the dotted line is the centralized version.",
            "Training on more and more data on a single machine, the solid is for the.",
            "For the selection strategy based on accuracy and then the dashed line, the Dash line here is choosing a random weak hypothesis among your nodes so you can see that we have more substantial speedup in the data distributed case.",
            "Une."
        ],
        [
            "Fortunately for well for accuracy, the data distributed version we do have gains by using the data on the additional nodes, and those are significant gains compared to using a single worker.",
            "But unfortunately, if we actually."
        ],
        [
            "3rd Train on all the data in one place.",
            "That's the better strategy.",
            "So here the dotted line is the centralized model.",
            "Training on more and more data as you have the number of nodes increasing and you can see that there is actually still a significant gap here."
        ],
        [
            "Accuracy, so in the future what we want to be able to do is have something that is communication constant in the number of training examples, but achieves accuracy as good or maybe better than the centralized model.",
            "And of course we'd like to move to an asynchronous approach here.",
            "This is all synchronous, so talk to me more later for more details."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, I'm Krista Sabori and this is joint work with Chris Burgess and we also a lot of things to offer Declan, Matt Richardson and other people in our group for helping with this implementation.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about a continuation really of the previous talk.",
                    "label": 0
                },
                {
                    "sent": "Also boosted decision trees and how we use those for ranking on a cluster.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So obviously the motivation here is that for us, especially at Microsoft, we have extremely large datasets available to us up to terabytes, a day of search and user interaction.",
                    "label": 0
                },
                {
                    "sent": "Data hardware is cheap enough we can buy machines to set up our own cluster, and if we can really decrease the model training time, then we might be able to solve problems that we haven't previously been able to have good solutions for now that we have so much data available.",
                    "label": 0
                },
                {
                    "sent": "It also gives us faster throughput for experimentation.",
                    "label": 0
                },
                {
                    "sent": "So what we do here is we actually distribute Lambda Mart, which is boosted regression tree.",
                    "label": 0
                },
                {
                    "sent": "Into the Friedman we just heard about an here the it's a gradient boosted decision tree and the gradients are approximated using Lambda functions.",
                    "label": 0
                },
                {
                    "sent": "Of course they are constructed incrementally, so you have a summation over all of these weak hypothesis and the weak hypothesis here again, are these regression trees an for this work?",
                    "label": 0
                },
                {
                    "sent": "We're looking at roughly 100 to 200 leaves.",
                    "label": 0
                },
                {
                    "sent": "And we chose to use Lambda Mart as this was one of the key components of the winning system for the Yahoo Learning Drink challenge that occur die CML.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Spring.",
                    "label": 0
                },
                {
                    "sent": "So we take actually two approaches.",
                    "label": 0
                },
                {
                    "sent": "So the first is feature distribution and the 2nd is data distribution.",
                    "label": 1
                },
                {
                    "sent": "So feature distribution we essentially heard about just previously, so there were going.",
                    "label": 0
                },
                {
                    "sent": "We actually just assume that the data fits in memory on every node, and then we distribute the tree split or the vertex split computations.",
                    "label": 1
                },
                {
                    "sent": "And this will produce a model that's equivalent to the model that is trained with the centralized model training on all the data on the single machine in a single place, right?",
                    "label": 0
                },
                {
                    "sent": "And then the data distribution technique.",
                    "label": 0
                },
                {
                    "sent": "Here we want to consider how do we get around scaling?",
                    "label": 0
                },
                {
                    "sent": "We don't want to be linear in the number of examples, so we consider we want to be constant communication constant in the number of examples.",
                    "label": 0
                },
                {
                    "sent": "So here we actually distribute the data across the cluster.",
                    "label": 0
                },
                {
                    "sent": "So the data here does not fit on main memory of a single machine, so you have to move to more machines to train on all of the data, and we actually attempt to use this data sitting on the various cluster nodes as a kind of massive cross validation.",
                    "label": 1
                },
                {
                    "sent": "So each node is training a week hypothesis and ideas or the hope is that those hypothesis are diverse, but we look at two methods for selecting the week hypotheses among all of these nodes.",
                    "label": 0
                },
                {
                    "sent": "One is random and then one.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is based on the best cross validation accuracy, so just quickly in one minute.",
                    "label": 0
                },
                {
                    "sent": "Here the time comparisons.",
                    "label": 0
                },
                {
                    "sent": "So on the left.",
                    "label": 0
                },
                {
                    "sent": "This is for the feature distributed version.",
                    "label": 0
                },
                {
                    "sent": "We have two types of cluster nodes that we looked at.",
                    "label": 1
                },
                {
                    "sent": "This is we have 15 million query URL pairs here.",
                    "label": 0
                },
                {
                    "sent": "As the number of workers grows we're going from 1 to 32 workers.",
                    "label": 0
                },
                {
                    "sent": "You can see that we're achieving a pretty good.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some speed up.",
                    "label": 0
                },
                {
                    "sent": "On the right, this is for the data distributed version, so the number of workers.",
                    "label": 0
                },
                {
                    "sent": "Here we assume that each node can only store 3500 queries, so we're simulating the case where you've run out of memory and the dotted line is the centralized version.",
                    "label": 1
                },
                {
                    "sent": "Training on more and more data on a single machine, the solid is for the.",
                    "label": 0
                },
                {
                    "sent": "For the selection strategy based on accuracy and then the dashed line, the Dash line here is choosing a random weak hypothesis among your nodes so you can see that we have more substantial speedup in the data distributed case.",
                    "label": 0
                },
                {
                    "sent": "Une.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fortunately for well for accuracy, the data distributed version we do have gains by using the data on the additional nodes, and those are significant gains compared to using a single worker.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately, if we actually.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "3rd Train on all the data in one place.",
                    "label": 0
                },
                {
                    "sent": "That's the better strategy.",
                    "label": 0
                },
                {
                    "sent": "So here the dotted line is the centralized model.",
                    "label": 0
                },
                {
                    "sent": "Training on more and more data as you have the number of nodes increasing and you can see that there is actually still a significant gap here.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Accuracy, so in the future what we want to be able to do is have something that is communication constant in the number of training examples, but achieves accuracy as good or maybe better than the centralized model.",
                    "label": 0
                },
                {
                    "sent": "And of course we'd like to move to an asynchronous approach here.",
                    "label": 1
                },
                {
                    "sent": "This is all synchronous, so talk to me more later for more details.",
                    "label": 0
                }
            ]
        }
    }
}