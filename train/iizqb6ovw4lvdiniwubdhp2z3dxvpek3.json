{
    "id": "iizqb6ovw4lvdiniwubdhp2z3dxvpek3",
    "title": "SPARQLGX: Efficient Distributed Evaluation of SPARQL with Apache Spark",
    "info": {
        "author": [
            "Damien Graux, Laboratoire LIG"
        ],
        "published": "Nov. 10, 2016",
        "recorded": "October 2016",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2016_graux_apache_spark/",
    "segmentation": [
        [
            "Thank you very much.",
            "I just saying today I'm going to 1st how to everyone I'm going to.",
            "Discribe Sparkle Jinx, which is an efficient distributed sparkly return.",
            "Built on top of other spark that we made with Lori Piana Bill in France.",
            "In in real."
        ],
        [
            "So first of all, let me remind you the global context.",
            "We have large amounts of RDF data, which implies to distribute them across clusters of nodes."
        ],
        [
            "And we also want to extract quickly information from them using sparkle."
        ],
        [
            "In the same time, we want, for instance, to be resilient in the case of not failure.",
            "We also want to be parsimonious on the resource construction consumer."
        ],
        [
            "And and.",
            "Early we have cluster computing frameworks which provide interfile interface with implicit data replication and parallelism and fault tolerance.",
            "And which offers set of lower level functions such as map, join, collect, reduce and among these frameworks we."
        ],
        [
            "Selected for its simplicity, Apache Spark and we use it on top of the Hadoop distributed file system called The Edge DFS.",
            "Um?"
        ],
        [
            "So first we have for instance a graph of tripod graph which represents our publication very quickly and tripod."
        ],
        [
            "So sheated through.",
            "And if we really want to ever."
        ],
        [
            "Wait, a basic graph pattern of only three triple patterns which are variable's type tool viable G type human and S contributor G. We"
        ],
        [
            "The.",
            "1st.",
            "Treat the first tribal pattern so it gives us few.",
            "So two solution.",
            "Two possible solutions for US.",
            "Sparkle Jason Apache spark similarly."
        ],
        [
            "With you we have Damian Lewis Pier enable."
        ],
        [
            "And with the contributor part we have the couple possible couple SNG which are the following and we too have the complete solution.",
            "We just have to join them.",
            "The three tribal bottle."
        ],
        [
            "To have our final solution.",
            "So that's the global strategy.",
            "The strategy used by Sparkle Jakes to evaluate a basic graph pattern.",
            "But you can see that in this case we have each time too.",
            "Kroll the entire?",
            "That does it.",
            "The entire that I said.",
            "So the."
        ],
        [
            "First level of optimization we made was to 1st partition data.",
            "Using."
        ],
        [
            "The fact that these predicates.",
            "Often carry those who carry.",
            "In fact, the semantic information in RDF and thereby and that has been shown by Gallego and there there is often a limited number of distinct predicates.",
            "You know that I said, and Moreover in sparkle queries predicates are Raleigh viable so this leads us to adapt the vertical partitioning presented by a body."
        ],
        [
            "Taking back our example, we split the read the initial data set on the left in five sub files.",
            "Containing only two columns, one for the subject and one for the object and named by the predicate.",
            "So we are going to enter now the detail of the translation of the sparkle queries into scalar compliant code with Spark."
        ],
        [
            "So first of all, to translate 1 three rifle pattern, we first used the text file function provided by Spark to access the relevant file of the vertical partitioning, and then we filter to keep the matching tripodal."
        ],
        [
            "For instance, variable is type tool we are going to search in the type files and then filter the the weather when the object equals tools and finally we just map to only retain the subject column."
        ],
        [
            "The next step is obviously dealing with a conjunction of tribal patterns, so our algorithm is the following.",
            "We first translate HTTP and after we join them 1 by 1."
        ],
        [
            "So, taking back your example."
        ],
        [
            "So we already, so that translation and you can see that at the end we.",
            "Add the key by instruction in order to.",
            "Provide spark the column not key.",
            "Similarly we."
        ],
        [
            "The same thing with the second triple pattern."
        ],
        [
            "And with the server we only have to retain.",
            "In fact, the whole contributor of file, because both.",
            "Subject and object are variables in the same type of pattern.",
            "And then to join them, we first do the first wiper pattern.",
            "We join it with the second one and then with the sound one.",
            "And as you can see there is no common variable between the 1st and the 2nd wiper pattern.",
            "So.",
            "We have in fact to do the cross product of the two of the two Subs."
        ],
        [
            "It and didn't spark is it's encoded by the Cartesian Q World, so the BGP translation is GP, One Cartesian with TP do with TP two.",
            "We only keep the values.",
            "Then we came by again on the two colors and we finally joined with their last EP and as you can see.",
            "We have here the the case in our translation of an appearing Cartesian product.",
            "The idea now will be to."
        ],
        [
            "Second level of optimization that Sparkle jigs does is too.",
            "Minimize the size of intermediate results that it has to shuffle across the network, and we do that first by avoiding Cartesian product and then by exploiting statistics on the initial datasets.",
            "So prior to the translation, we rewrite the order of the tribal pattern into BGP."
        ],
        [
            "So if we use that on our running example, we just move the search tripole.",
            "At the first position and we have now the following, the following translation.",
            "Which leaders to avoid the Cartesian product here?",
            "And so, depending of the tripod and the translation can change, but obviously the final result are always the same.",
            "And.",
            "So.",
            "To recap a bit, we have.",
            "To level of optimization for specularities, first we vertical partition the data and then we can also wear order the order of tribal pattern but."
        ],
        [
            "Is it always a good solution?",
            "In fact, we see that we have a translator and prior partitioner.",
            "Need this.",
            "Provide another tool which doesn't.",
            "Partition the data called SDE for Sparkle Jakes as a direct evaluator and which directly used the initial file as a sources so it will change a bit what is returning the text file instruction on the scale aside and both have advantages for instanced Sparkle Jakes with the vertical partitioning we have a natural compression.",
            "In this light indexing, we also use statistiques on data computing during the partitioning and as there has other advantages since because it doesn't have to.",
            "Index data prayer.",
            "It can deal with dynamic data data easily an it's also a good solution if you want to only evaluate.",
            "I don't know one sparkle query under that I said during your workflow."
        ],
        [
            "And we test them.",
            "On our own cluster of 10 nodes, each node.",
            "And positive 17 gigabyte or from it.",
            "And we used to popular benchmarks Alabama and what Dave in order to test them we selected also a set of competitors coming from the literature, we focused on HDF is based.",
            "Competitors which also open source popular in quite recent and we selected then two types of favorite.",
            "All the conventional one with which mean with preprocessing.",
            "So we selected Raya Click Square an as two RDF, which are three very three recent Sparkle evaluators and we also selected two other direct evaluator.",
            "Directbuy collaborator, called.",
            "Sparkle in I5 which doesn't need to process the data.",
            "So."
        ],
        [
            "But our experimental setup and.",
            "We experience them with several datasets datasets.",
            "So what did 1K and two album 10K which are from 100 millions of trifle to more than one billion?",
            "The original file size on the HD face can going to a bit larger.",
            "And to sum up.",
            "The result we had we notice the sparkle jigs and.",
            "Insert all the queries.",
            "Unlike unlike some competitors, we notably discover that.",
            "On our cluster, some evaluators failed to load LUT game 10K.",
            "For instance, we also notice that Sparkle Jakes is the fastest among those capable to answer a week raise.",
            "So that that's the claims with Sparkle, Jakes, and considering only or Sparkle Jakes Direct evaluator, SDE will notice that it outperforms also direct evaluators and even sometimes some conventional data stores.",
            "Which means that without having a preprocessing step it sometimes.",
            "Outperformed the other one, so the detailed results are available on our website."
        ],
        [
            "And so, to conclude, we provide, we openly provide on as a resource on the on the indicator of our team both Sparkle Jason SDR and SD, which are both efficient tools to evaluate sparkle queries in in distributed context.",
            "So thank you very much for your attention."
        ],
        [
            "Thank you very much.",
            "Using.",
            "So if you look at like the Catalyst optimizer, so you could just convert to SQL and that seems like a better comparison than trying to reimplement all this stuff using RDD.",
            "In fact we we started to work with Spark SQL, Spark SQL in order to just translate the sparkle queries into SQL queries and then evaluate them on Spark SQL.",
            "But we faced some scaling problems.",
            "For very large that I said that we do that face with with our direct version, which directly translate the sparkle crazy into Scala code compliant.",
            "And do you know why that is?",
            "Because the whole feature spark right is dataframes and they're building database optimizers.",
            "I mean, I don't.",
            "I don't understand that.",
            "Be interesting to figure out why, why that was slow and you didn't go in that direction.",
            "I would say that this because when you translate to from sparkle to SQL and you don't have key constraints or anything that you are not able to optimize SQL and the resulting SQL will be really crappy.",
            "So even a relational database will not be able to handle it.",
            "I think there are various opinions on that one.",
            "I'm not going to.",
            "Thank you at one point you were talking about indexing indexing.",
            "When you are talking about SD, are you using a indexed RDD or other form of indexing?",
            "No, no, we are using the the classic Adios Park and when I talked about indexing it was during the vertical partitioning idea.",
            "Vertical partitioning part.",
            "In fact we can say that partitioning vertical either data set offers slides of slight sort of indexing because.",
            "We split the data set according to the various predicates.",
            "You cannot, you cannot.",
            "Guarantee any orders on the on the presentation of your subject and object on the weekend and also out with your first solution.",
            "Vertical one you guarantee good performances for star queries because at least the chunks of sparkle query are usually looking like stars.",
            "So when you you will have a lot of shuffle between between your machines able to try to guarantee at least to have small, smaller service abrazar that we can have.",
            "Hi, to the best of my knowledge, in recent years some work have been proposed which you spark to process RDF data.",
            "For example the S2 RDF yes and for me it's a quite optimized system.",
            "They extend the classical vertical partitioning and there's also supports our advanced features.",
            "For example, the property path and if you compare your system too, there's an what I mean.",
            "How can you distinguish your advantage?",
            "For instance, comparing 2 S, 2 and F. Did they propose an extension of the vertical partitioning which first is longer to compute during the preprocessing phase?",
            "So we have the advantage of being fastest just during the the loading phase and.",
            "Moreover, during the computation during evaluation.",
            "We we also had better performance than store on the Earth thanks to the fact that we are directly using.",
            "Spark and we want to build on.",
            "Box equal for instance.",
            "OK, very interesting work.",
            "I have two questions.",
            "One is do you cash R DDS before you run the queries?",
            "In fact we cache and is at the text file when we when we first read them because sometimes we have to try for patterns that are dealing with the same predicate for instance.",
            "So we catch them on the fly.",
            "Second question, yes, you mentioned that you're using Lube on right Knee High University benchmark.",
            "It carries some semantics in yet right.",
            "Do you do query rewrite or do you run some sort of reasoning?",
            "Answer so.",
            "The way we do that infer, we do not have inferencing yet.",
            "Thank you, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "I just saying today I'm going to 1st how to everyone I'm going to.",
                    "label": 0
                },
                {
                    "sent": "Discribe Sparkle Jinx, which is an efficient distributed sparkly return.",
                    "label": 1
                },
                {
                    "sent": "Built on top of other spark that we made with Lori Piana Bill in France.",
                    "label": 0
                },
                {
                    "sent": "In in real.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all, let me remind you the global context.",
                    "label": 0
                },
                {
                    "sent": "We have large amounts of RDF data, which implies to distribute them across clusters of nodes.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we also want to extract quickly information from them using sparkle.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the same time, we want, for instance, to be resilient in the case of not failure.",
                    "label": 0
                },
                {
                    "sent": "We also want to be parsimonious on the resource construction consumer.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And and.",
                    "label": 0
                },
                {
                    "sent": "Early we have cluster computing frameworks which provide interfile interface with implicit data replication and parallelism and fault tolerance.",
                    "label": 1
                },
                {
                    "sent": "And which offers set of lower level functions such as map, join, collect, reduce and among these frameworks we.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Selected for its simplicity, Apache Spark and we use it on top of the Hadoop distributed file system called The Edge DFS.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first we have for instance a graph of tripod graph which represents our publication very quickly and tripod.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So sheated through.",
                    "label": 0
                },
                {
                    "sent": "And if we really want to ever.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wait, a basic graph pattern of only three triple patterns which are variable's type tool viable G type human and S contributor G. We",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "1st.",
                    "label": 0
                },
                {
                    "sent": "Treat the first tribal pattern so it gives us few.",
                    "label": 0
                },
                {
                    "sent": "So two solution.",
                    "label": 0
                },
                {
                    "sent": "Two possible solutions for US.",
                    "label": 0
                },
                {
                    "sent": "Sparkle Jason Apache spark similarly.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With you we have Damian Lewis Pier enable.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with the contributor part we have the couple possible couple SNG which are the following and we too have the complete solution.",
                    "label": 0
                },
                {
                    "sent": "We just have to join them.",
                    "label": 0
                },
                {
                    "sent": "The three tribal bottle.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To have our final solution.",
                    "label": 0
                },
                {
                    "sent": "So that's the global strategy.",
                    "label": 0
                },
                {
                    "sent": "The strategy used by Sparkle Jakes to evaluate a basic graph pattern.",
                    "label": 0
                },
                {
                    "sent": "But you can see that in this case we have each time too.",
                    "label": 0
                },
                {
                    "sent": "Kroll the entire?",
                    "label": 0
                },
                {
                    "sent": "That does it.",
                    "label": 0
                },
                {
                    "sent": "The entire that I said.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First level of optimization we made was to 1st partition data.",
                    "label": 0
                },
                {
                    "sent": "Using.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The fact that these predicates.",
                    "label": 0
                },
                {
                    "sent": "Often carry those who carry.",
                    "label": 0
                },
                {
                    "sent": "In fact, the semantic information in RDF and thereby and that has been shown by Gallego and there there is often a limited number of distinct predicates.",
                    "label": 1
                },
                {
                    "sent": "You know that I said, and Moreover in sparkle queries predicates are Raleigh viable so this leads us to adapt the vertical partitioning presented by a body.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Taking back our example, we split the read the initial data set on the left in five sub files.",
                    "label": 0
                },
                {
                    "sent": "Containing only two columns, one for the subject and one for the object and named by the predicate.",
                    "label": 0
                },
                {
                    "sent": "So we are going to enter now the detail of the translation of the sparkle queries into scalar compliant code with Spark.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all, to translate 1 three rifle pattern, we first used the text file function provided by Spark to access the relevant file of the vertical partitioning, and then we filter to keep the matching tripodal.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For instance, variable is type tool we are going to search in the type files and then filter the the weather when the object equals tools and finally we just map to only retain the subject column.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next step is obviously dealing with a conjunction of tribal patterns, so our algorithm is the following.",
                    "label": 0
                },
                {
                    "sent": "We first translate HTTP and after we join them 1 by 1.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, taking back your example.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we already, so that translation and you can see that at the end we.",
                    "label": 0
                },
                {
                    "sent": "Add the key by instruction in order to.",
                    "label": 0
                },
                {
                    "sent": "Provide spark the column not key.",
                    "label": 0
                },
                {
                    "sent": "Similarly we.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same thing with the second triple pattern.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with the server we only have to retain.",
                    "label": 0
                },
                {
                    "sent": "In fact, the whole contributor of file, because both.",
                    "label": 0
                },
                {
                    "sent": "Subject and object are variables in the same type of pattern.",
                    "label": 0
                },
                {
                    "sent": "And then to join them, we first do the first wiper pattern.",
                    "label": 0
                },
                {
                    "sent": "We join it with the second one and then with the sound one.",
                    "label": 0
                },
                {
                    "sent": "And as you can see there is no common variable between the 1st and the 2nd wiper pattern.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We have in fact to do the cross product of the two of the two Subs.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It and didn't spark is it's encoded by the Cartesian Q World, so the BGP translation is GP, One Cartesian with TP do with TP two.",
                    "label": 0
                },
                {
                    "sent": "We only keep the values.",
                    "label": 0
                },
                {
                    "sent": "Then we came by again on the two colors and we finally joined with their last EP and as you can see.",
                    "label": 0
                },
                {
                    "sent": "We have here the the case in our translation of an appearing Cartesian product.",
                    "label": 0
                },
                {
                    "sent": "The idea now will be to.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Second level of optimization that Sparkle jigs does is too.",
                    "label": 0
                },
                {
                    "sent": "Minimize the size of intermediate results that it has to shuffle across the network, and we do that first by avoiding Cartesian product and then by exploiting statistics on the initial datasets.",
                    "label": 1
                },
                {
                    "sent": "So prior to the translation, we rewrite the order of the tribal pattern into BGP.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we use that on our running example, we just move the search tripole.",
                    "label": 0
                },
                {
                    "sent": "At the first position and we have now the following, the following translation.",
                    "label": 0
                },
                {
                    "sent": "Which leaders to avoid the Cartesian product here?",
                    "label": 0
                },
                {
                    "sent": "And so, depending of the tripod and the translation can change, but obviously the final result are always the same.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To recap a bit, we have.",
                    "label": 0
                },
                {
                    "sent": "To level of optimization for specularities, first we vertical partition the data and then we can also wear order the order of tribal pattern but.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is it always a good solution?",
                    "label": 0
                },
                {
                    "sent": "In fact, we see that we have a translator and prior partitioner.",
                    "label": 0
                },
                {
                    "sent": "Need this.",
                    "label": 0
                },
                {
                    "sent": "Provide another tool which doesn't.",
                    "label": 0
                },
                {
                    "sent": "Partition the data called SDE for Sparkle Jakes as a direct evaluator and which directly used the initial file as a sources so it will change a bit what is returning the text file instruction on the scale aside and both have advantages for instanced Sparkle Jakes with the vertical partitioning we have a natural compression.",
                    "label": 0
                },
                {
                    "sent": "In this light indexing, we also use statistiques on data computing during the partitioning and as there has other advantages since because it doesn't have to.",
                    "label": 0
                },
                {
                    "sent": "Index data prayer.",
                    "label": 0
                },
                {
                    "sent": "It can deal with dynamic data data easily an it's also a good solution if you want to only evaluate.",
                    "label": 1
                },
                {
                    "sent": "I don't know one sparkle query under that I said during your workflow.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we test them.",
                    "label": 0
                },
                {
                    "sent": "On our own cluster of 10 nodes, each node.",
                    "label": 1
                },
                {
                    "sent": "And positive 17 gigabyte or from it.",
                    "label": 0
                },
                {
                    "sent": "And we used to popular benchmarks Alabama and what Dave in order to test them we selected also a set of competitors coming from the literature, we focused on HDF is based.",
                    "label": 1
                },
                {
                    "sent": "Competitors which also open source popular in quite recent and we selected then two types of favorite.",
                    "label": 0
                },
                {
                    "sent": "All the conventional one with which mean with preprocessing.",
                    "label": 0
                },
                {
                    "sent": "So we selected Raya Click Square an as two RDF, which are three very three recent Sparkle evaluators and we also selected two other direct evaluator.",
                    "label": 0
                },
                {
                    "sent": "Directbuy collaborator, called.",
                    "label": 0
                },
                {
                    "sent": "Sparkle in I5 which doesn't need to process the data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But our experimental setup and.",
                    "label": 0
                },
                {
                    "sent": "We experience them with several datasets datasets.",
                    "label": 0
                },
                {
                    "sent": "So what did 1K and two album 10K which are from 100 millions of trifle to more than one billion?",
                    "label": 0
                },
                {
                    "sent": "The original file size on the HD face can going to a bit larger.",
                    "label": 1
                },
                {
                    "sent": "And to sum up.",
                    "label": 0
                },
                {
                    "sent": "The result we had we notice the sparkle jigs and.",
                    "label": 0
                },
                {
                    "sent": "Insert all the queries.",
                    "label": 0
                },
                {
                    "sent": "Unlike unlike some competitors, we notably discover that.",
                    "label": 0
                },
                {
                    "sent": "On our cluster, some evaluators failed to load LUT game 10K.",
                    "label": 0
                },
                {
                    "sent": "For instance, we also notice that Sparkle Jakes is the fastest among those capable to answer a week raise.",
                    "label": 1
                },
                {
                    "sent": "So that that's the claims with Sparkle, Jakes, and considering only or Sparkle Jakes Direct evaluator, SDE will notice that it outperforms also direct evaluators and even sometimes some conventional data stores.",
                    "label": 1
                },
                {
                    "sent": "Which means that without having a preprocessing step it sometimes.",
                    "label": 0
                },
                {
                    "sent": "Outperformed the other one, so the detailed results are available on our website.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so, to conclude, we provide, we openly provide on as a resource on the on the indicator of our team both Sparkle Jason SDR and SD, which are both efficient tools to evaluate sparkle queries in in distributed context.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much for your attention.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Using.",
                    "label": 0
                },
                {
                    "sent": "So if you look at like the Catalyst optimizer, so you could just convert to SQL and that seems like a better comparison than trying to reimplement all this stuff using RDD.",
                    "label": 0
                },
                {
                    "sent": "In fact we we started to work with Spark SQL, Spark SQL in order to just translate the sparkle queries into SQL queries and then evaluate them on Spark SQL.",
                    "label": 0
                },
                {
                    "sent": "But we faced some scaling problems.",
                    "label": 0
                },
                {
                    "sent": "For very large that I said that we do that face with with our direct version, which directly translate the sparkle crazy into Scala code compliant.",
                    "label": 0
                },
                {
                    "sent": "And do you know why that is?",
                    "label": 0
                },
                {
                    "sent": "Because the whole feature spark right is dataframes and they're building database optimizers.",
                    "label": 0
                },
                {
                    "sent": "I mean, I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't understand that.",
                    "label": 0
                },
                {
                    "sent": "Be interesting to figure out why, why that was slow and you didn't go in that direction.",
                    "label": 0
                },
                {
                    "sent": "I would say that this because when you translate to from sparkle to SQL and you don't have key constraints or anything that you are not able to optimize SQL and the resulting SQL will be really crappy.",
                    "label": 0
                },
                {
                    "sent": "So even a relational database will not be able to handle it.",
                    "label": 0
                },
                {
                    "sent": "I think there are various opinions on that one.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to.",
                    "label": 0
                },
                {
                    "sent": "Thank you at one point you were talking about indexing indexing.",
                    "label": 0
                },
                {
                    "sent": "When you are talking about SD, are you using a indexed RDD or other form of indexing?",
                    "label": 0
                },
                {
                    "sent": "No, no, we are using the the classic Adios Park and when I talked about indexing it was during the vertical partitioning idea.",
                    "label": 0
                },
                {
                    "sent": "Vertical partitioning part.",
                    "label": 0
                },
                {
                    "sent": "In fact we can say that partitioning vertical either data set offers slides of slight sort of indexing because.",
                    "label": 0
                },
                {
                    "sent": "We split the data set according to the various predicates.",
                    "label": 0
                },
                {
                    "sent": "You cannot, you cannot.",
                    "label": 0
                },
                {
                    "sent": "Guarantee any orders on the on the presentation of your subject and object on the weekend and also out with your first solution.",
                    "label": 0
                },
                {
                    "sent": "Vertical one you guarantee good performances for star queries because at least the chunks of sparkle query are usually looking like stars.",
                    "label": 0
                },
                {
                    "sent": "So when you you will have a lot of shuffle between between your machines able to try to guarantee at least to have small, smaller service abrazar that we can have.",
                    "label": 0
                },
                {
                    "sent": "Hi, to the best of my knowledge, in recent years some work have been proposed which you spark to process RDF data.",
                    "label": 0
                },
                {
                    "sent": "For example the S2 RDF yes and for me it's a quite optimized system.",
                    "label": 0
                },
                {
                    "sent": "They extend the classical vertical partitioning and there's also supports our advanced features.",
                    "label": 0
                },
                {
                    "sent": "For example, the property path and if you compare your system too, there's an what I mean.",
                    "label": 0
                },
                {
                    "sent": "How can you distinguish your advantage?",
                    "label": 0
                },
                {
                    "sent": "For instance, comparing 2 S, 2 and F. Did they propose an extension of the vertical partitioning which first is longer to compute during the preprocessing phase?",
                    "label": 0
                },
                {
                    "sent": "So we have the advantage of being fastest just during the the loading phase and.",
                    "label": 0
                },
                {
                    "sent": "Moreover, during the computation during evaluation.",
                    "label": 0
                },
                {
                    "sent": "We we also had better performance than store on the Earth thanks to the fact that we are directly using.",
                    "label": 0
                },
                {
                    "sent": "Spark and we want to build on.",
                    "label": 0
                },
                {
                    "sent": "Box equal for instance.",
                    "label": 0
                },
                {
                    "sent": "OK, very interesting work.",
                    "label": 0
                },
                {
                    "sent": "I have two questions.",
                    "label": 0
                },
                {
                    "sent": "One is do you cash R DDS before you run the queries?",
                    "label": 0
                },
                {
                    "sent": "In fact we cache and is at the text file when we when we first read them because sometimes we have to try for patterns that are dealing with the same predicate for instance.",
                    "label": 0
                },
                {
                    "sent": "So we catch them on the fly.",
                    "label": 0
                },
                {
                    "sent": "Second question, yes, you mentioned that you're using Lube on right Knee High University benchmark.",
                    "label": 0
                },
                {
                    "sent": "It carries some semantics in yet right.",
                    "label": 0
                },
                {
                    "sent": "Do you do query rewrite or do you run some sort of reasoning?",
                    "label": 0
                },
                {
                    "sent": "Answer so.",
                    "label": 0
                },
                {
                    "sent": "The way we do that infer, we do not have inferencing yet.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you.",
                    "label": 0
                }
            ]
        }
    }
}