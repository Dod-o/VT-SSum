{
    "id": "2xy2tfpqodbyte6yuha67fn4imheohb5",
    "title": "Optimal Distributed Online Prediction Using Mini-Batches",
    "info": {
        "author": [
            "Lin Xiao, Microsoft Research"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_xiao_odo/",
    "segmentation": [
        [
            "OK. Say it again, this joint work with my colleagues offered Echo Run, get back, or head Shamir or at Microsoft Research.",
            "All algorithms?"
        ],
        [
            "Such as the Kostic grading or online gradient descent has been quite popular in machine learning cause they're fast.",
            "They're simple and often they have very good generalization capabilities.",
            "But on the other hand, as we all know, they are sequential in nature, meaning using process data 1 by 1 update their predictors or optimizers sequentially.",
            "So there are often study in the series sighting.",
            "This one."
        ],
        [
            "OK, and as we get into this, you know age of a huge data set as young as outlined in the introduction.",
            "Some of the examples at web scale online prediction problem, for example as in country search engines.",
            "In there those kind of online problems have some have their own characteristics.",
            "For example the inputs arrives very very high rate, it could be.",
            "How many thousand per second at least?",
            "Yeah, and then not only the arrival rate they need to be precise, the instance.",
            "Instantaneously we need provide real time service.",
            "This is a little bit different than in parallel this recomputing you.",
            "Can you maybe just want to speed up your time waiting time, reduce the waiting time speed up computation, but here we really need real time service to keep up the high retail input.",
            "Therefore it's critical to use parallelism.",
            "Computer here is critical is this don't have a slow choice either.",
            "Either do it or fail.",
            "I always forgot I guess use this one anyway."
        ],
        [
            "So the question is, given the completing nature of this requirement or parallel district computing and also sequential nature of our algorithm, how well can we expect online algorithm performing distributing node only if their old ones working or we need to invent new?"
        ],
        [
            "Let's look at a specific example, particular posted online prediction.",
            "The case the data of the online data comes one by one, and so at each time step you will first predict a Wii from set.",
            "W and then you will receive ZI data drawn IID from a fixed distribution.",
            "This is the classic set up and then you suffer loss F. This function F is a predefined loss function, and then you repeat.",
            "This is just kind of like online game and of course when you predict W might use."
        ],
        [
            "Previous information, in particular the gradient of the cost functions.",
            "In this case, we'll measure the quality of predictors using regret defined like this.",
            "So because of this is the cost.",
            "If we can actually define this exist global best optimizer.",
            "When you take the expectation, but we really don't know this point and but still we can hopefully get some performance bound on the cumulative loss.",
            "Our algorithm, which varies at each time.",
            "J and then compare with the fixed best predictor.",
            "So here we assume F is convex in W for each Z and the set is closed."
        ],
        [
            "Axe OK. Another really solid plastic authorization problem.",
            "In that case we just want to minimize approximate solution for this stochastic addition problem.",
            "The success is measured by the automatic gap we just after M steps you up to vector WMC.",
            "How close to the optimal value.",
            "And it's different motivation often used to so large scale batch problems which don't usually do not need a real."
        ],
        [
            "Time requirement, but still we can ask question how well?",
            "How can parallel computing speedup solution in this case?",
            "And let's go back to the other prediction case and consider this to be set up.",
            "Suppose you have.",
            "We need at least Cano to keep up the high input rate and you have a splitter to speed or load to different key machines.",
            "Of course, Peter could be virtual application, for example in search engines the incoming data stream might come at, you know, distributed way in the geographically to different computer data center anyway, so it's really for convenience with the splitter and this Community network will assume it has limited bandwidth.",
            "It has a latency.",
            "If you trust message from one note to another, not a node.",
            "And non blocking.",
            "Essentially we see where you transmitting data and waiting for the response.",
            "You can keep predicting using your old old predictor.",
            "So that should keep up your real time service and we measure the same regret.",
            "Suppose the data processed at different processors, but we want to."
        ],
        [
            "Why are the total overall regret?",
            "Now let's first look at some limited performance.",
            "What best or worst we can do in this set of the first is ideal, but unrealistic solution is run theory algorithm on a supercomputer that is key times faster, which you know is in your dream.",
            "An optimal regret regret bound in this case is often for convex functions is bounded by the square root of M. So this is optimal because in the sense any."
        ],
        [
            "Any distributor if we come up with can be simulated on the fast enough serial computer.",
            "On the other hand, a trivial non communication solution is a let each node handle their own data stream, do not communicate to each other at all.",
            "This is my cousin coming here solution so, but in this case we can see the regret bound scale problem with network size K because for each network you process for each node process and the other back pain samples.",
            "This year around and then the independent you just.",
            "You know some of them together you get this square root K scaling which is not good.",
            "So this is the two extremes we can expect."
        ],
        [
            "Let's look like some previous work.",
            "This Russell work like Professor physically summarizing his talk.",
            "There's also work in the distributed optimization in the organization community, and recently there's lots of work.",
            "Work in the in the learning community just because of this parallel distributed computing can help in learning an."
        ],
        [
            "Look at.",
            "How this algorithm will perform our setup?",
            "I mean this algorithm not exactly invented fault solving our problem, but it's still good say, can we just use them in our setup?",
            "How good they are?",
            "Do we need new algorithms?",
            "So let's be repeated here and the trivial algorithm you have non communication.",
            "You have this regret bound and this is ideal.",
            "One translated into the Clock time.",
            "Or if you do the cast computer you want to speed up your computing.",
            "This is 1 square root T and this is.",
            "Well, square key time here."
        ],
        [
            "Equals K * T because in tee time K machine you can process M examples an.",
            "If you apply most existing algorithm, the Unfortunately most fall in this region, so it's not even perform better than the trivial non communication case.",
            "This is a notable work online for the smaller the cabbage they had work there make very good effort towards this goal there, but their algorithm kind of work in a very limited research setting.",
            "Our result will say, well.",
            "Optimally achieved."
        ],
        [
            "Ideal case.",
            "OK, next I will just.",
            "First, give the virus bound for the theory, algorithms, algorithms.",
            "Our vessel is based on converting existing algorithms to parallel setup.",
            "So we just give some new bound on the theory algorithm and then I've described the algorithm."
        ],
        [
            "Parallel stagnation setup and also experiment.",
            "So here are.",
            "Some classical online algorithms.",
            "Like program grading designer projects, degree design and some do averaging algorithms.",
            "Most of this setup, but the details are not really."
        ],
        [
            "Matter, the things that we can show way for convex functions mostly they can attend this regret bound by choosing a property step, size of square root M and here the constant before the square root M is upper bound.",
            "Expected value of the gradients.",
            "But now in our case we see we."
        ],
        [
            "We will add more assumptions.",
            "So get some variance bound to improve the bound of the constant.",
            "So this assumption is that is smooth is that for each Z the loss function is Lipschitz have delicious continuous gradient with constant L, and then for each W we expect expected gradient has a bounded variance.",
            "If this is F is expected value of the."
        ],
        [
            "F excited function.",
            "These assumptions we can show after that.",
            "With proper step size they were bound still square root N order, but.",
            "The custom before it is the variance.",
            "Various obstacles creating this naturally.",
            "See you know.",
            "Variance if we average a bunch of gradient, we can reduce the average immediately.",
            "That's essentially the idea of mini batching and so here we have.",
            "You know theoretically sound basis for using."
        ],
        [
            "Mini batching an.",
            "For convenience, we'll see this happen."
        ],
        [
            "Moreover, depend on the M and also only the variance.",
            "Let the band divided by a function for PSI and then see the variance reduction techniques.",
            "Say we just predict something simple using some predictor.",
            "Be the batch size and you update predictor based on average gradients.",
            "This is not a new idea has been analyzed in both theory."
        ],
        [
            "A pair of settings, but there has been no theoretical support.",
            "Of course, with the various pound you can say.",
            "You just consider the averages average cost function and the gradient is that is average gradients and you can see the various here is divided by factor B.",
            "But the number of batteries of course, and by the end, if you calculate the rebound.",
            "If you run this algorithm in the serial case series set up, you'll get this.",
            "Bond, which is not much different than.",
            "Then without mini batching."
        ],
        [
            "Now let's look at how do we apply it in the.",
            "Distributed setup essentially you have K machines each.",
            "Process we divided the batch total of B batches into key machine.",
            "Each one process BLK just accumulated first BLK inputs and then they will run the vector sum or map reduce algorithm to compute the average of the big gradients.",
            "Suppose this takes some time.",
            "This is the time go there and this is using the predict GG an while you're singing you're doing finishing the vector sum Operation Network to compute the average gradients.",
            "You are still using the old predictor to predict at the same time there's extra mu output input coming in.",
            "You still predicting them, but their gradients are twisted.",
            "You're not using the grid into didn't catch the train to go to the centralizer?",
            "The master to particular, and then get it back, and then after you get back, you go."
        ],
        [
            "So in this case you can say is analysis is fairly easy.",
            "That this is the variance.",
            "By using the samples and this divided by B plus meal because that's the number of batches you have an you are modified by the plus meal.",
            "But because for each batch you encounter those kind of you have to count all the."
        ],
        [
            "At.",
            "In this case, I can say that.",
            "Suppose your theory algorithm has a regret bound like this.",
            "Then you see in the parallel setup.",
            "You will have a.",
            "Regret bound this notice at this term.",
            "Is exactly show the next slide that way."
        ],
        [
            "The dominant term here is the same as in the theory bound before exactly the same.",
            "And if you choose optimally of equals computer rule, you will get this.",
            "You can see exactly what.",
            "Exponents here.",
            "They are much smaller, so it's simply optimal.",
            "The dominant term, the same, and often the mule comes only in the smaller exponent he ran off.",
            "And if you use some spanning tree to do network operation, you mu is often scales like log of K, so it's very minor dependence."
        ],
        [
            "Network size let's also visit the case for stigmatization as a set before we all this case, will you know.",
            "Under the spies success about automatic gap, this average of all your previous predictors for convex Lawson ID you can easily show this relationship holds."
        ],
        [
            "And we define this by.",
            "5 bar."
        ],
        [
            "And the same algorithm runs, but in this case, what is that?",
            "You do not need to process extra data while you are doing the networks up."
        ],
        [
            "Because you can wait.",
            "And you can get a similar bound, just probably the same thing."
        ],
        [
            "You will get.",
            "I regret a convergence theory in this case.",
            "This is the conference we will get it running on a supercomputer which came faster.",
            "That's ideal case.",
            "This case has to be there, but this time is much smaller compared with this part.",
            "So asymptomatically the same as.",
            "Theory algorithm supercomputer."
        ],
        [
            "And the parallel speedup.",
            "So more interesting, say OK this and it doesn't show the number number of processors.",
            "OK, if you look at the possibility of, say, in the theory case, our process.",
            "Suppose M examples take EM unit of time and in the parallel case you say I have M / B batches.",
            "Each has each takes time B or K samples plus some witching time.",
            "You see, the parallel speedup is like this and see if he grows with them.",
            "This speedup actually turns to chaos.",
            "Linear signals the best you can hope for.",
            "Parallel computing theory.",
            "Similar result.",
            "You can say the same speed up can be achieved by achieving the same operator gap.",
            "This is for the same number of examples but similar thing.",
            "You can do this for the similarity gap."
        ],
        [
            "OK, now let's go to large scale example.",
            "We did a large scale example with online binary prediction which you know come from some search engine log when building queries with.",
            "Just predict if the next query is highly having monetizable.",
            "Anne.",
            "So the logic we use, the simple logic logistic loss function is parent is has a ellipse, continuous gradients.",
            "And we use in the particular example we used to do every method we.",
            "Of course there's some constant steps that need to be trained, but we turned it on a separate half bill."
        ],
        [
            "Queries or performance tuning.",
            "And then first let's look at the effect of Bachelorette.",
            "This is all in serial setting.",
            "You see is in the X axis is number input in log scale.",
            "This is average loss.",
            "Average loss I do not show the total loss, but the average next convergence rate and the theory settings.",
            "The mini batch actually does much better.",
            "Remembering are seen in the series setting, is not much different from not using mini batch but in the experiment we observed is much much better behavior than not using mini batching and this is our theorem didn't capture.",
            "It was very interesting."
        ],
        [
            "Now look at the.",
            "Comparison of the distributed batch algorithm with others the same.",
            "So here the blue line is is distributed batch on.",
            "1000 computers, you know which some deley with some 1000 batch size and then compare with the red one is the non communication separate version.",
            "It's much worse and this surprisingly this line is indeed.",
            "Not communication, but each computer using a mini batch.",
            "So this is again not captured by a theorem, but it's much better but still outperformed by our algorithm and this line is the one we see you run ideal theory algorithm.",
            "You can see.",
            "In theory we are, I think tactically, catch up the performance of Idea theory, but in practice we observe.",
            "You know it's good most of the time, but the last stage."
        ],
        [
            "Yeah, the theoretical start being better.",
            "This is another different parameter setting.",
            "There are more or less the same."
        ],
        [
            "Trend and here we studied the similar stuff with the latency.",
            "As you can say that with the latency increases your performance will degrade, but we can see fairly small degrade with this huge magnitude of variation on the."
        ],
        [
            "The last time slides is an experiment is optimal batch size as we see it.",
            "Describing our theorem, ACM equals cubic root, but this is theoretical bound, but in practice you can do experiments to check what's the best batch size.",
            "We run it for different total number of examples.",
            "You know 10 to the 7th example is an abelian.",
            "We can see that empirically.",
            "Interestingly, the bottom batch size is different for different."
        ],
        [
            "You know, even though how much money did you run.",
            "OK. Let me summarize.",
            "We describe the distributed.",
            "Batch mini batch algorithm for solving stochastic online prediction problem and also stochastic optimization problems.",
            "And we can see that it can turn many existing theory algorithm just without new inventing algorithms.",
            "Just turn them into theorem, which depends on the various bound we derive for.",
            "The serial case, is asymptomatically achieves optimal regret bound for smooth loss functions of course.",
            "And for stochastic conditions are linear.",
            "Near linear speedup.",
            "We think this is the first approval demonstration that distribute computing really worse for this online prediction and speeding up."
        ],
        [
            "Stochastic optimization problems.",
            "For this from future directions at.",
            "Professor cygnus.",
            "And just talk discuss the asynchronous distributed computing and there we think this.",
            "You know all possible to generating that setup as well.",
            "After all the smoothness assumption precisely said that the grading change small if you are at nearby points.",
            "And of course, this.",
            "More wealth question is can we do this similar things for anonymous functions?",
            "Now stuck asking for these are the two critical assumptions in our work, but just fairly open question, thank you.",
            "OK, so maybe one question in the interest of time.",
            "What?",
            "Why is mini batch working worse than online as they did the optimization?",
            "My intuition isn't many matches work better at the end because it's at the end that you really just start dealing with noise and you turn the average away.",
            "The noise and mini batch should be better at average, so I was surprised by the results of you so well.",
            "Also, although I have some suspicions.",
            "Some other intuition is the conditions tell me this might be the case, but I couldn't recall right now.",
            "Time for still exactly.",
            "That's our split.",
            "Surprised we would.",
            "You know, it's not pretty power bound with some interesting questions to look into.",
            "OK, so let's thank the speaker one more time."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Say it again, this joint work with my colleagues offered Echo Run, get back, or head Shamir or at Microsoft Research.",
                    "label": 0
                },
                {
                    "sent": "All algorithms?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Such as the Kostic grading or online gradient descent has been quite popular in machine learning cause they're fast.",
                    "label": 0
                },
                {
                    "sent": "They're simple and often they have very good generalization capabilities.",
                    "label": 1
                },
                {
                    "sent": "But on the other hand, as we all know, they are sequential in nature, meaning using process data 1 by 1 update their predictors or optimizers sequentially.",
                    "label": 1
                },
                {
                    "sent": "So there are often study in the series sighting.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and as we get into this, you know age of a huge data set as young as outlined in the introduction.",
                    "label": 0
                },
                {
                    "sent": "Some of the examples at web scale online prediction problem, for example as in country search engines.",
                    "label": 1
                },
                {
                    "sent": "In there those kind of online problems have some have their own characteristics.",
                    "label": 1
                },
                {
                    "sent": "For example the inputs arrives very very high rate, it could be.",
                    "label": 0
                },
                {
                    "sent": "How many thousand per second at least?",
                    "label": 0
                },
                {
                    "sent": "Yeah, and then not only the arrival rate they need to be precise, the instance.",
                    "label": 0
                },
                {
                    "sent": "Instantaneously we need provide real time service.",
                    "label": 0
                },
                {
                    "sent": "This is a little bit different than in parallel this recomputing you.",
                    "label": 0
                },
                {
                    "sent": "Can you maybe just want to speed up your time waiting time, reduce the waiting time speed up computation, but here we really need real time service to keep up the high retail input.",
                    "label": 0
                },
                {
                    "sent": "Therefore it's critical to use parallelism.",
                    "label": 1
                },
                {
                    "sent": "Computer here is critical is this don't have a slow choice either.",
                    "label": 0
                },
                {
                    "sent": "Either do it or fail.",
                    "label": 0
                },
                {
                    "sent": "I always forgot I guess use this one anyway.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the question is, given the completing nature of this requirement or parallel district computing and also sequential nature of our algorithm, how well can we expect online algorithm performing distributing node only if their old ones working or we need to invent new?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's look at a specific example, particular posted online prediction.",
                    "label": 0
                },
                {
                    "sent": "The case the data of the online data comes one by one, and so at each time step you will first predict a Wii from set.",
                    "label": 0
                },
                {
                    "sent": "W and then you will receive ZI data drawn IID from a fixed distribution.",
                    "label": 1
                },
                {
                    "sent": "This is the classic set up and then you suffer loss F. This function F is a predefined loss function, and then you repeat.",
                    "label": 0
                },
                {
                    "sent": "This is just kind of like online game and of course when you predict W might use.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Previous information, in particular the gradient of the cost functions.",
                    "label": 0
                },
                {
                    "sent": "In this case, we'll measure the quality of predictors using regret defined like this.",
                    "label": 1
                },
                {
                    "sent": "So because of this is the cost.",
                    "label": 0
                },
                {
                    "sent": "If we can actually define this exist global best optimizer.",
                    "label": 0
                },
                {
                    "sent": "When you take the expectation, but we really don't know this point and but still we can hopefully get some performance bound on the cumulative loss.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm, which varies at each time.",
                    "label": 0
                },
                {
                    "sent": "J and then compare with the fixed best predictor.",
                    "label": 1
                },
                {
                    "sent": "So here we assume F is convex in W for each Z and the set is closed.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Axe OK. Another really solid plastic authorization problem.",
                    "label": 0
                },
                {
                    "sent": "In that case we just want to minimize approximate solution for this stochastic addition problem.",
                    "label": 1
                },
                {
                    "sent": "The success is measured by the automatic gap we just after M steps you up to vector WMC.",
                    "label": 0
                },
                {
                    "sent": "How close to the optimal value.",
                    "label": 1
                },
                {
                    "sent": "And it's different motivation often used to so large scale batch problems which don't usually do not need a real.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time requirement, but still we can ask question how well?",
                    "label": 0
                },
                {
                    "sent": "How can parallel computing speedup solution in this case?",
                    "label": 0
                },
                {
                    "sent": "And let's go back to the other prediction case and consider this to be set up.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have.",
                    "label": 0
                },
                {
                    "sent": "We need at least Cano to keep up the high input rate and you have a splitter to speed or load to different key machines.",
                    "label": 0
                },
                {
                    "sent": "Of course, Peter could be virtual application, for example in search engines the incoming data stream might come at, you know, distributed way in the geographically to different computer data center anyway, so it's really for convenience with the splitter and this Community network will assume it has limited bandwidth.",
                    "label": 0
                },
                {
                    "sent": "It has a latency.",
                    "label": 0
                },
                {
                    "sent": "If you trust message from one note to another, not a node.",
                    "label": 0
                },
                {
                    "sent": "And non blocking.",
                    "label": 0
                },
                {
                    "sent": "Essentially we see where you transmitting data and waiting for the response.",
                    "label": 0
                },
                {
                    "sent": "You can keep predicting using your old old predictor.",
                    "label": 0
                },
                {
                    "sent": "So that should keep up your real time service and we measure the same regret.",
                    "label": 0
                },
                {
                    "sent": "Suppose the data processed at different processors, but we want to.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why are the total overall regret?",
                    "label": 0
                },
                {
                    "sent": "Now let's first look at some limited performance.",
                    "label": 0
                },
                {
                    "sent": "What best or worst we can do in this set of the first is ideal, but unrealistic solution is run theory algorithm on a supercomputer that is key times faster, which you know is in your dream.",
                    "label": 1
                },
                {
                    "sent": "An optimal regret regret bound in this case is often for convex functions is bounded by the square root of M. So this is optimal because in the sense any.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Any distributor if we come up with can be simulated on the fast enough serial computer.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, a trivial non communication solution is a let each node handle their own data stream, do not communicate to each other at all.",
                    "label": 0
                },
                {
                    "sent": "This is my cousin coming here solution so, but in this case we can see the regret bound scale problem with network size K because for each network you process for each node process and the other back pain samples.",
                    "label": 1
                },
                {
                    "sent": "This year around and then the independent you just.",
                    "label": 0
                },
                {
                    "sent": "You know some of them together you get this square root K scaling which is not good.",
                    "label": 0
                },
                {
                    "sent": "So this is the two extremes we can expect.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's look like some previous work.",
                    "label": 1
                },
                {
                    "sent": "This Russell work like Professor physically summarizing his talk.",
                    "label": 1
                },
                {
                    "sent": "There's also work in the distributed optimization in the organization community, and recently there's lots of work.",
                    "label": 0
                },
                {
                    "sent": "Work in the in the learning community just because of this parallel distributed computing can help in learning an.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at.",
                    "label": 0
                },
                {
                    "sent": "How this algorithm will perform our setup?",
                    "label": 0
                },
                {
                    "sent": "I mean this algorithm not exactly invented fault solving our problem, but it's still good say, can we just use them in our setup?",
                    "label": 0
                },
                {
                    "sent": "How good they are?",
                    "label": 0
                },
                {
                    "sent": "Do we need new algorithms?",
                    "label": 0
                },
                {
                    "sent": "So let's be repeated here and the trivial algorithm you have non communication.",
                    "label": 0
                },
                {
                    "sent": "You have this regret bound and this is ideal.",
                    "label": 0
                },
                {
                    "sent": "One translated into the Clock time.",
                    "label": 0
                },
                {
                    "sent": "Or if you do the cast computer you want to speed up your computing.",
                    "label": 0
                },
                {
                    "sent": "This is 1 square root T and this is.",
                    "label": 0
                },
                {
                    "sent": "Well, square key time here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Equals K * T because in tee time K machine you can process M examples an.",
                    "label": 0
                },
                {
                    "sent": "If you apply most existing algorithm, the Unfortunately most fall in this region, so it's not even perform better than the trivial non communication case.",
                    "label": 0
                },
                {
                    "sent": "This is a notable work online for the smaller the cabbage they had work there make very good effort towards this goal there, but their algorithm kind of work in a very limited research setting.",
                    "label": 0
                },
                {
                    "sent": "Our result will say, well.",
                    "label": 0
                },
                {
                    "sent": "Optimally achieved.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ideal case.",
                    "label": 0
                },
                {
                    "sent": "OK, next I will just.",
                    "label": 0
                },
                {
                    "sent": "First, give the virus bound for the theory, algorithms, algorithms.",
                    "label": 0
                },
                {
                    "sent": "Our vessel is based on converting existing algorithms to parallel setup.",
                    "label": 0
                },
                {
                    "sent": "So we just give some new bound on the theory algorithm and then I've described the algorithm.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parallel stagnation setup and also experiment.",
                    "label": 0
                },
                {
                    "sent": "So here are.",
                    "label": 0
                },
                {
                    "sent": "Some classical online algorithms.",
                    "label": 0
                },
                {
                    "sent": "Like program grading designer projects, degree design and some do averaging algorithms.",
                    "label": 0
                },
                {
                    "sent": "Most of this setup, but the details are not really.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matter, the things that we can show way for convex functions mostly they can attend this regret bound by choosing a property step, size of square root M and here the constant before the square root M is upper bound.",
                    "label": 0
                },
                {
                    "sent": "Expected value of the gradients.",
                    "label": 0
                },
                {
                    "sent": "But now in our case we see we.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will add more assumptions.",
                    "label": 0
                },
                {
                    "sent": "So get some variance bound to improve the bound of the constant.",
                    "label": 0
                },
                {
                    "sent": "So this assumption is that is smooth is that for each Z the loss function is Lipschitz have delicious continuous gradient with constant L, and then for each W we expect expected gradient has a bounded variance.",
                    "label": 0
                },
                {
                    "sent": "If this is F is expected value of the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "F excited function.",
                    "label": 0
                },
                {
                    "sent": "These assumptions we can show after that.",
                    "label": 0
                },
                {
                    "sent": "With proper step size they were bound still square root N order, but.",
                    "label": 0
                },
                {
                    "sent": "The custom before it is the variance.",
                    "label": 0
                },
                {
                    "sent": "Various obstacles creating this naturally.",
                    "label": 0
                },
                {
                    "sent": "See you know.",
                    "label": 0
                },
                {
                    "sent": "Variance if we average a bunch of gradient, we can reduce the average immediately.",
                    "label": 0
                },
                {
                    "sent": "That's essentially the idea of mini batching and so here we have.",
                    "label": 0
                },
                {
                    "sent": "You know theoretically sound basis for using.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mini batching an.",
                    "label": 0
                },
                {
                    "sent": "For convenience, we'll see this happen.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Moreover, depend on the M and also only the variance.",
                    "label": 0
                },
                {
                    "sent": "Let the band divided by a function for PSI and then see the variance reduction techniques.",
                    "label": 0
                },
                {
                    "sent": "Say we just predict something simple using some predictor.",
                    "label": 0
                },
                {
                    "sent": "Be the batch size and you update predictor based on average gradients.",
                    "label": 1
                },
                {
                    "sent": "This is not a new idea has been analyzed in both theory.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A pair of settings, but there has been no theoretical support.",
                    "label": 1
                },
                {
                    "sent": "Of course, with the various pound you can say.",
                    "label": 1
                },
                {
                    "sent": "You just consider the averages average cost function and the gradient is that is average gradients and you can see the various here is divided by factor B.",
                    "label": 0
                },
                {
                    "sent": "But the number of batteries of course, and by the end, if you calculate the rebound.",
                    "label": 0
                },
                {
                    "sent": "If you run this algorithm in the serial case series set up, you'll get this.",
                    "label": 0
                },
                {
                    "sent": "Bond, which is not much different than.",
                    "label": 0
                },
                {
                    "sent": "Then without mini batching.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's look at how do we apply it in the.",
                    "label": 0
                },
                {
                    "sent": "Distributed setup essentially you have K machines each.",
                    "label": 0
                },
                {
                    "sent": "Process we divided the batch total of B batches into key machine.",
                    "label": 0
                },
                {
                    "sent": "Each one process BLK just accumulated first BLK inputs and then they will run the vector sum or map reduce algorithm to compute the average of the big gradients.",
                    "label": 0
                },
                {
                    "sent": "Suppose this takes some time.",
                    "label": 0
                },
                {
                    "sent": "This is the time go there and this is using the predict GG an while you're singing you're doing finishing the vector sum Operation Network to compute the average gradients.",
                    "label": 0
                },
                {
                    "sent": "You are still using the old predictor to predict at the same time there's extra mu output input coming in.",
                    "label": 0
                },
                {
                    "sent": "You still predicting them, but their gradients are twisted.",
                    "label": 0
                },
                {
                    "sent": "You're not using the grid into didn't catch the train to go to the centralizer?",
                    "label": 0
                },
                {
                    "sent": "The master to particular, and then get it back, and then after you get back, you go.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case you can say is analysis is fairly easy.",
                    "label": 0
                },
                {
                    "sent": "That this is the variance.",
                    "label": 0
                },
                {
                    "sent": "By using the samples and this divided by B plus meal because that's the number of batches you have an you are modified by the plus meal.",
                    "label": 0
                },
                {
                    "sent": "But because for each batch you encounter those kind of you have to count all the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "In this case, I can say that.",
                    "label": 0
                },
                {
                    "sent": "Suppose your theory algorithm has a regret bound like this.",
                    "label": 0
                },
                {
                    "sent": "Then you see in the parallel setup.",
                    "label": 0
                },
                {
                    "sent": "You will have a.",
                    "label": 0
                },
                {
                    "sent": "Regret bound this notice at this term.",
                    "label": 0
                },
                {
                    "sent": "Is exactly show the next slide that way.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The dominant term here is the same as in the theory bound before exactly the same.",
                    "label": 1
                },
                {
                    "sent": "And if you choose optimally of equals computer rule, you will get this.",
                    "label": 0
                },
                {
                    "sent": "You can see exactly what.",
                    "label": 0
                },
                {
                    "sent": "Exponents here.",
                    "label": 0
                },
                {
                    "sent": "They are much smaller, so it's simply optimal.",
                    "label": 0
                },
                {
                    "sent": "The dominant term, the same, and often the mule comes only in the smaller exponent he ran off.",
                    "label": 0
                },
                {
                    "sent": "And if you use some spanning tree to do network operation, you mu is often scales like log of K, so it's very minor dependence.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Network size let's also visit the case for stigmatization as a set before we all this case, will you know.",
                    "label": 0
                },
                {
                    "sent": "Under the spies success about automatic gap, this average of all your previous predictors for convex Lawson ID you can easily show this relationship holds.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we define this by.",
                    "label": 0
                },
                {
                    "sent": "5 bar.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the same algorithm runs, but in this case, what is that?",
                    "label": 0
                },
                {
                    "sent": "You do not need to process extra data while you are doing the networks up.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because you can wait.",
                    "label": 0
                },
                {
                    "sent": "And you can get a similar bound, just probably the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You will get.",
                    "label": 0
                },
                {
                    "sent": "I regret a convergence theory in this case.",
                    "label": 0
                },
                {
                    "sent": "This is the conference we will get it running on a supercomputer which came faster.",
                    "label": 0
                },
                {
                    "sent": "That's ideal case.",
                    "label": 0
                },
                {
                    "sent": "This case has to be there, but this time is much smaller compared with this part.",
                    "label": 0
                },
                {
                    "sent": "So asymptomatically the same as.",
                    "label": 0
                },
                {
                    "sent": "Theory algorithm supercomputer.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the parallel speedup.",
                    "label": 0
                },
                {
                    "sent": "So more interesting, say OK this and it doesn't show the number number of processors.",
                    "label": 0
                },
                {
                    "sent": "OK, if you look at the possibility of, say, in the theory case, our process.",
                    "label": 0
                },
                {
                    "sent": "Suppose M examples take EM unit of time and in the parallel case you say I have M / B batches.",
                    "label": 0
                },
                {
                    "sent": "Each has each takes time B or K samples plus some witching time.",
                    "label": 0
                },
                {
                    "sent": "You see, the parallel speedup is like this and see if he grows with them.",
                    "label": 0
                },
                {
                    "sent": "This speedup actually turns to chaos.",
                    "label": 0
                },
                {
                    "sent": "Linear signals the best you can hope for.",
                    "label": 0
                },
                {
                    "sent": "Parallel computing theory.",
                    "label": 0
                },
                {
                    "sent": "Similar result.",
                    "label": 0
                },
                {
                    "sent": "You can say the same speed up can be achieved by achieving the same operator gap.",
                    "label": 0
                },
                {
                    "sent": "This is for the same number of examples but similar thing.",
                    "label": 0
                },
                {
                    "sent": "You can do this for the similarity gap.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now let's go to large scale example.",
                    "label": 0
                },
                {
                    "sent": "We did a large scale example with online binary prediction which you know come from some search engine log when building queries with.",
                    "label": 1
                },
                {
                    "sent": "Just predict if the next query is highly having monetizable.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 1
                },
                {
                    "sent": "So the logic we use, the simple logic logistic loss function is parent is has a ellipse, continuous gradients.",
                    "label": 0
                },
                {
                    "sent": "And we use in the particular example we used to do every method we.",
                    "label": 0
                },
                {
                    "sent": "Of course there's some constant steps that need to be trained, but we turned it on a separate half bill.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Queries or performance tuning.",
                    "label": 0
                },
                {
                    "sent": "And then first let's look at the effect of Bachelorette.",
                    "label": 0
                },
                {
                    "sent": "This is all in serial setting.",
                    "label": 0
                },
                {
                    "sent": "You see is in the X axis is number input in log scale.",
                    "label": 0
                },
                {
                    "sent": "This is average loss.",
                    "label": 0
                },
                {
                    "sent": "Average loss I do not show the total loss, but the average next convergence rate and the theory settings.",
                    "label": 0
                },
                {
                    "sent": "The mini batch actually does much better.",
                    "label": 0
                },
                {
                    "sent": "Remembering are seen in the series setting, is not much different from not using mini batch but in the experiment we observed is much much better behavior than not using mini batching and this is our theorem didn't capture.",
                    "label": 0
                },
                {
                    "sent": "It was very interesting.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now look at the.",
                    "label": 0
                },
                {
                    "sent": "Comparison of the distributed batch algorithm with others the same.",
                    "label": 0
                },
                {
                    "sent": "So here the blue line is is distributed batch on.",
                    "label": 0
                },
                {
                    "sent": "1000 computers, you know which some deley with some 1000 batch size and then compare with the red one is the non communication separate version.",
                    "label": 0
                },
                {
                    "sent": "It's much worse and this surprisingly this line is indeed.",
                    "label": 0
                },
                {
                    "sent": "Not communication, but each computer using a mini batch.",
                    "label": 0
                },
                {
                    "sent": "So this is again not captured by a theorem, but it's much better but still outperformed by our algorithm and this line is the one we see you run ideal theory algorithm.",
                    "label": 0
                },
                {
                    "sent": "You can see.",
                    "label": 0
                },
                {
                    "sent": "In theory we are, I think tactically, catch up the performance of Idea theory, but in practice we observe.",
                    "label": 0
                },
                {
                    "sent": "You know it's good most of the time, but the last stage.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, the theoretical start being better.",
                    "label": 0
                },
                {
                    "sent": "This is another different parameter setting.",
                    "label": 0
                },
                {
                    "sent": "There are more or less the same.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trend and here we studied the similar stuff with the latency.",
                    "label": 0
                },
                {
                    "sent": "As you can say that with the latency increases your performance will degrade, but we can see fairly small degrade with this huge magnitude of variation on the.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The last time slides is an experiment is optimal batch size as we see it.",
                    "label": 1
                },
                {
                    "sent": "Describing our theorem, ACM equals cubic root, but this is theoretical bound, but in practice you can do experiments to check what's the best batch size.",
                    "label": 0
                },
                {
                    "sent": "We run it for different total number of examples.",
                    "label": 0
                },
                {
                    "sent": "You know 10 to the 7th example is an abelian.",
                    "label": 0
                },
                {
                    "sent": "We can see that empirically.",
                    "label": 1
                },
                {
                    "sent": "Interestingly, the bottom batch size is different for different.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know, even though how much money did you run.",
                    "label": 0
                },
                {
                    "sent": "OK. Let me summarize.",
                    "label": 0
                },
                {
                    "sent": "We describe the distributed.",
                    "label": 0
                },
                {
                    "sent": "Batch mini batch algorithm for solving stochastic online prediction problem and also stochastic optimization problems.",
                    "label": 1
                },
                {
                    "sent": "And we can see that it can turn many existing theory algorithm just without new inventing algorithms.",
                    "label": 0
                },
                {
                    "sent": "Just turn them into theorem, which depends on the various bound we derive for.",
                    "label": 0
                },
                {
                    "sent": "The serial case, is asymptomatically achieves optimal regret bound for smooth loss functions of course.",
                    "label": 1
                },
                {
                    "sent": "And for stochastic conditions are linear.",
                    "label": 0
                },
                {
                    "sent": "Near linear speedup.",
                    "label": 0
                },
                {
                    "sent": "We think this is the first approval demonstration that distribute computing really worse for this online prediction and speeding up.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stochastic optimization problems.",
                    "label": 0
                },
                {
                    "sent": "For this from future directions at.",
                    "label": 1
                },
                {
                    "sent": "Professor cygnus.",
                    "label": 0
                },
                {
                    "sent": "And just talk discuss the asynchronous distributed computing and there we think this.",
                    "label": 1
                },
                {
                    "sent": "You know all possible to generating that setup as well.",
                    "label": 0
                },
                {
                    "sent": "After all the smoothness assumption precisely said that the grading change small if you are at nearby points.",
                    "label": 0
                },
                {
                    "sent": "And of course, this.",
                    "label": 1
                },
                {
                    "sent": "More wealth question is can we do this similar things for anonymous functions?",
                    "label": 0
                },
                {
                    "sent": "Now stuck asking for these are the two critical assumptions in our work, but just fairly open question, thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe one question in the interest of time.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Why is mini batch working worse than online as they did the optimization?",
                    "label": 0
                },
                {
                    "sent": "My intuition isn't many matches work better at the end because it's at the end that you really just start dealing with noise and you turn the average away.",
                    "label": 0
                },
                {
                    "sent": "The noise and mini batch should be better at average, so I was surprised by the results of you so well.",
                    "label": 0
                },
                {
                    "sent": "Also, although I have some suspicions.",
                    "label": 0
                },
                {
                    "sent": "Some other intuition is the conditions tell me this might be the case, but I couldn't recall right now.",
                    "label": 0
                },
                {
                    "sent": "Time for still exactly.",
                    "label": 0
                },
                {
                    "sent": "That's our split.",
                    "label": 0
                },
                {
                    "sent": "Surprised we would.",
                    "label": 0
                },
                {
                    "sent": "You know, it's not pretty power bound with some interesting questions to look into.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's thank the speaker one more time.",
                    "label": 0
                }
            ]
        }
    }
}