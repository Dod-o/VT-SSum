{
    "id": "dosregp7ehmjuufrcbe2jdzhvusi2i6y",
    "title": "New Regularized Algorithms for Transducitve Learning",
    "info": {
        "author": [
            "Partha Pratim Talukdar, Indian Institute of Science Bangalore"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_talukdar_nratl/",
    "segmentation": [
        [
            "Hello everyone, I'm part A so in this talk I'm going to talk to you about this new set of actual new regularize algorithms for transductive learning will try to understand the meaning of each one of those terms.",
            "This is joint work with Koby Kraemer who was at the University of Pennsylvania but is now at Technion, Israel, OK, Let me try to give you some context behind this work and like why we tried to address this problem so we were actually looking at the problem of information extraction.",
            "In particular.",
            "Trying to assign semantic labels to entities on a large scale.",
            "So for example, you type in the entity, say or input the individual system.",
            "Like Toyota, we want the system to come up with labels like.",
            "It's a Japanese company, car manufacturer, automobile manufacturers, human readable descriptors of that type, and we want to cover as many entities and as many labels as possible.",
            "So we decided to use sort of a graph based framework for that and we use them.",
            "Algorithm called absorption.",
            "Will see details about it in a short while and we successfully used that use that algorithm to that problem, but it turned out that like the algorithm wasn't analyzed.",
            "It wasn't known like whether it's optimizing any sort of objective function in all, because it was set up as an algorithm and not as an optimization problem.",
            "So this in this paper.",
            "Basically we tried to see whether we could understand that algorithm, better figure out its optimization in all.",
            "And we'll see like where we ended up with that and that kind of.",
            "We have some.",
            "Basically, we figured out that like that algorithm, absorption wasn't analyze.",
            "Wasn't optimizing any particular objective, so we had to come up with our own.",
            "Proposed algorithms which we wanted to have, like a good optimization but was as close as possible to the original absorption graph and then once we have the objective we could optimize it too.",
            "Fit our problem at hand better, so that's that's roughly the context in which, like proper prompted us to look at this problem, an resulted in this paper.",
            "OK, so let's first look at graph based, semi supervised learning, where actually say if you have a set of."
        ],
        [
            "This is as in the trans as in the regular supervised learning, where some of the nodes are labeled, but in addition you have lots of other points too.",
            "For example, all the unlabeled points that you care about an all the points that you consider like.",
            "So the labeled and unlabeled points together you want to put them in a graph in igraph, basically supervised learning, where each node will represent one of your instances OK, and you can always measure similarities between the nodes.",
            "And that similarity between two nodes or two instances is represented by this weight on this edge.",
            "OK, so here, for example, in this white node is connected to this node with the similarity of .3 an to this other node, which is similarity of .2.",
            "So again, all the points that you care about, labeled or unlabeled is there in front of you and there is no out of sample extension.",
            "There will be no new point that you will be asked to classify in the future.",
            "So that basically defines the transductive setting.",
            "As many of you are already probably familiar, and then we induce a graph or in many cases the graph maybe already present.",
            "Then some of these nodes are labeled.",
            "So we have like labels for them.",
            "In this case for these two nodes.",
            "So this is a red node, the red label."
        ],
        [
            "And this is the blue label OK, and the white nodes are basically like the unlabeled nodes, and we want to determine the labels for these unlabeled nodes.",
            "This white nodes, using this graph structure and."
        ],
        [
            "The initial label notes OK, so basically graph based semi supervised learning algorithms.",
            "Try to solve this problem by using the graph structure and unlabeled nodes.",
            "Trying to assign labels to the unlabeled nodes.",
            "OK, so if we use a one of those algorithms then we'll end up with something like this.",
            "I don't know."
        ],
        [
            "Can you see this thing from the back?",
            "OK great so I mean so initially we had these two nodes labeled.",
            "Now we use the graph based semi supervised learning algorithm to assign labels to all the other nodes.",
            "So in this case you see that these two nodes are kind of at the boundary between these two regions.",
            "Say if we call it like the red region and the blue reason so they have like multiple labels assigned.",
            "So it's like partially red, partially blue and these are like this node."
        ],
        [
            "Is very far apart from the blue, so it gets like the red label OK and those these algorithms are tend to be very flexible.",
            "Here I have shown you the binary case, but you can easily extend it to the multi label setting or when you have multiple labels per node.",
            "So over the years a few algorithms have been proposed starting from this label propagation or method called zarazua his authors starting from 2003 to some very recent methods called this absorption, which will be the basis on which will.",
            "Develop our methods and this is the algorithm.",
            "Essentially we used for our.",
            "Label assignment to entities.",
            "Problem that I had mentioned first.",
            "Also, since we build on the adsorption algorithm, let's try to look at its characteristics."
        ],
        [
            "The features first.",
            "This algorithm has been proposed by the baluja at all in World Wide Web 2008 where they use it to recommend videos to YouTube videos to users.",
            "OK, that was like the first application.",
            "Then we use in our semantic label assignment to entities.",
            "Problem in this paper from MLP.",
            "But as I mentioned, it has been it hasn't been analyzed so far, so it's not clear whether it's optimizing any objective and that essentially is the motive."
        ],
        [
            "Nation for our work.",
            "So to understand algorithm better and determine the details.",
            "OK, so let's look."
        ],
        [
            "How the algorithm proceeds?",
            "So again say we have like 4 nodes here, which are three of which are labeled.",
            "Two of them are colored."
        ],
        [
            "Are assigned one particular label, Red in this case and the other one has a blue label and we're trying to figure out the label for this white node, which is currently unlabeled.",
            "Then this,"
        ],
        [
            "That you saw was basically like the update performed by the algorithm and you will be."
        ],
        [
            "May be surprised to see that we have another label here, which is the green label, so the red labels are coming from these two nodes.",
            "The blue is coming from this other node.",
            "Then we have an additional label called Dummy Label.",
            "OK so dummy label or is a special label which is in a way internal to the algorithm and that you can use that the meal able to express your confidence in a way for the assignment of labels that you're doing to a particular done.",
            "So for example if you assign.",
            "High value to this dummy label node.",
            "Then maybe you are expressing the fact that you're not very certain what label should be assigned to that particular node, and we'll see shortly how we could use this.",
            "Adamy label the special label for a particular problem.",
            "So it turns out this kind of."
        ],
        [
            "Updates that the algorithm performs.",
            "Basically you just need to look at the labels of you."
        ],
        [
            "Neighbors and using that you can determine the labels for the current node.",
            "So it turns out that's very highly scalable an iterative, so you can essentially paralyze the algorithm and that helps you in working with massive amounts of data.",
            "So when you're working with the transductive setting, you have both labeled and unlabeled.",
            "We assume that the unlabeled data is is widely available, so having this kind of an iterative algorithm which can be paralyzed which can process large amounts of data efficiently.",
            "Is a desirable property.",
            "Also the main diff."
        ],
        [
            "Prince, between this between absorption and previously proposed graph based algorithm, is that it doesn't treat all nodes equally OK, so in particular, it turns out that it discounts high degree nodes.",
            "OK, so basically if you are trying to.",
            "Sorry, so when you are like propagating the labels through the graph then it won't trust the high degree nodes as much as a node with a lesser degree.",
            "So the assumption is that like the high degree node is connected to too many points or too many nodes in the graph, so it's not as reliable as the other nodes, so we won't trust it so much.",
            "So it's it's in a way slightly undemocratic view, but we'll see that that's actually useful thing.",
            "So the algorithm has two equivalent views, so.",
            "We have two nodes, so one is the label diffusion view.",
            "So you have a labeled node.",
            "You have an unlabeled node, so you can think."
        ],
        [
            "It as a label diffusion process where the labels basically propagate or diffuse."
        ],
        [
            "From the label node, the Blue note here, to the unlabeled node here, which is in white.",
            "And this they need not be connected by an edge directly.",
            "You can use the structure of the graph.",
            "The transitive property to propagate the labels from this label to the unlabeled node and equivalent view is that often random walk, so that view is again you have this unlabeled node.",
            "This label node you're trying to ascertain what's what label I should assign to this node.",
            "So you can start a random walk from this unlabeled node and using the graph structure you make transitions.",
            "Then whenever you get a label node, you can assign this unlabeled node the label of this label node with the probability with which you arrive at that node.",
            "So if this node is very hard to reach from this.",
            "Our unlabeled note you then you it will end up with a lower score for the corresponding label of this label node.",
            "OK, so it so let's look at this random walk view with in some detail because they will be useful.",
            "So again, start with some unlabel."
        ],
        [
            "You know, do you hear you make a path?",
            "This need not be an edge, a path so you reach at some intermediate vertex.",
            "Now the question is what steps you could perform.",
            "So it turns out you can do."
        ],
        [
            "Two things.",
            "One is with some probability PV can't, so that's like a continuation probability.",
            "You can continue the random walk to neighboring nodes of this, no?"
        ],
        [
            "OK, one other thing that you can do is if this V node where one of the labeled nodes right the seed nodes that we saw in the first slide, you can assign it to this node U with the probability.",
            "With some injection probability which is specific to this node.",
            "OK, so the first step is you continue the random walk with some probability.",
            "The second thing is you assign this node U the node you the label of this node V if there are any with some particular injection probability which is specific to this node V and the third thing you can do is abandon the random walk and assign this node U that dummy label that we saw before with some probability of.",
            "Abandonment.",
            "OK, so these three things are not specific, so they are specific to this node V and they will sum to one Now if.",
            "OK yeah, so so is the notion of how the assignment, how we can assign labels to this starting node while doing the random walk clear.",
            "That's OK. Um?",
            "So I again repeat that like this dummy label is coming from this abandonment probability, right?",
            "Because I mean if you reach at a node, you can determine whether you want to continue or stop the random walk.",
            "OK, so let's see how this framework can be."
        ],
        [
            "Use now to discount how high degree nodes are reduced their importance during the learning phase.",
            "So as I have."
        ],
        [
            "Already mentioned, we think that the high degree nodes are unreliable, so we don't want the random walk to follow through them or allow label propagation to happen through those nodes.",
            "So the solution is that we increase the probability on these high degree nodes, so we."
        ],
        [
            "Take the high degree or the abandonment probability on a particular node proportional to its degree.",
            "OK, so here the degree I have simplified a bit, but you can think of like if it's connected."
        ],
        [
            "A lot of neighbors.",
            "Maybe you want to consider the weighted degree of that node and you want to make that proportional.",
            "So if it's connected to a lot of nodes, don't don't trust that no Dan don't let any random walk or propagation to happen to that node.",
            "OK, so now I told you that like one of our initial motive."
        ],
        [
            "Oceans was weather.",
            "We wanted to see whether at."
        ],
        [
            "Option was optimizing any particular objective, right?",
            "So it turns out that it's actually not an, though that's kind of unsatisfactory, because we wanted to show that it's actually doing something meaningful that we could write down concisely, but we have a theorem in the paper saying that it's not doing anything so, but we also saw that it has some desirable properties that it's like highly scaleable, iterative and you can control basically these kind of propagation.",
            "Through the notes so you have like finer grained control.",
            "So we wanted to retain those properties in whatever algorithm we propose, and we also at the same time."
        ],
        [
            "Want to have well defined, objective, right?",
            "Which we know is being optimized as part of the algorithm.",
            "So that basically made us to propose."
        ],
        [
            "New algorithm called modified absorption, which we call for short mad.",
            "OK, so let's look at the modified absorption guard."
        ],
        [
            "Um so.",
            "I don't know whether you can see, but there is a.",
            "Our objective here, but."
        ],
        [
            "We don't want to look into the details, so just remove that and come up with a higher level view.",
            "OK, so the mad."
        ],
        [
            "And objective that's been minimized has actually three terms.",
            "So for each node, thank you.",
            "Thanks.",
            "So it has three terms, so the first term is it wants to minimize any loss.",
            "Or maybe we can do that animation once more.",
            "OK.",
            "So."
        ],
        [
            "In the first term, if for any node you have any seed label, if it's labeled up front, then finally you want to match that label as close as possible.",
            "The second term is like if if two nodes are connected by a high weighted edge, then you want to assign them across an edge.",
            "Similar labels and a third term is if you based on that abandonment probabilities that we had seen.",
            "If you have any prior over that node that like that node should get high dummy label score then you can impose that using that third term.",
            "OK so.",
            "As I mentioned, high degree, no discounting is imposed using the third term, but the good thing is that when you solve this you get an."
        ],
        [
            "Absorption like iterative update.",
            "It's not exactly the same because both are not the same algorithm, but it's very close and this is like as far as close as we could get to get the absorption like update but have a well defined objective.",
            "Now that we have an objective."
        ],
        [
            "With us we can extend that for to match or to handle the data that we have better.",
            "So one thing that's important is that the important recognizes that the labels are not always mutually exclusive, right?",
            "So for example, I have a example here from the."
        ],
        [
            "Beer domain, so I mean I just came up with these numbers.",
            "They may not reflect your tastes well, but so each one of these is a label, right?",
            "So say we have like a set of beer cans in front of us.",
            "An algorithm is trying to determine the label for each one of them right?",
            "And we want to say that like Scotch, we want to provide this information to the algorithm that Scottsdale and Yale are kind of similar, with the similarity score of 1 and then like white beer an like top formented beer.",
            "Are similar with a score of pointing, so each node in this graph."
        ],
        [
            "Is a label and this are weights basically represent the similarity between labels, so this is this we call a label graph.",
            "This is different from the initial graph where each node was an instance right?",
            "So here we are trying to capture the dependencies between labels and we want to input this thing into the algorithm and make sure that the similar labels are assigned similar scores on each one of the nodes.",
            "OK, so how we can do that in a principled way that we have this origonal obj."
        ],
        [
            "50 from Mad, the modified absorption objective.",
            "Now we can add an additional term right?",
            "So here what we can do is if a node."
        ],
        [
            "Is assigned different scores for for a similar label as determined by the label graph.",
            "The algorithm will be penalized so.",
            "So basically it will try to assign similar labels as determined by the label graph.",
            "Similar scores on each one of the nodes, and that's imposed by this new term.",
            "New fourth term.",
            "And this this modified this modified absorption with dependent labels.",
            "We call this algorithm medal.",
            "OK, I have already gone through this.",
            "It will penalize if similar labels are assigned different scores where this SIM."
        ],
        [
            "Party is given by these labeled graph, which could be either automatically induced or it could be a way."
        ],
        [
            "Need to introduce prior knowledge into the system.",
            "And it turns out."
        ],
        [
            "By adding this thing, you don't increase the complexity of the model.",
            "You still get in scalable iterate."
        ],
        [
            "If update with convergence guarantees so every both of them, like both Mad and medal objective has convergence guarantees.",
            "So let's look at.",
            "Now some experimental results.",
            "And we'll see.",
            "We'll see."
        ],
        [
            "Experiments for both Madden Medal and in particular for medal, will try to see how we could induce this dependence between labels."
        ],
        [
            "So we do two sets of experiments.",
            "In the first we do a set of classification experiments on two datasets.",
            "So this is web based standard text classification data set that we got from Subramanian builds.",
            "So it has four classes around 4200 instances, so the graph will have 4200 nodes and the similarity between the nodes is basically computed using a cosine similarity.",
            "Anna K nearest neighbor is nearest neighbor.",
            "Graph is constructed similarly.",
            "The other data set is this sentiment classification, where the objective is given a document algorithm is asked to specify what's the polarity, whether a user likes, whether a user on which that algorithm is strained would like that document or not OK, so it's again a four class, so maybe you have already seen, like in Amazon you provide these stars, right?",
            "So four star will be the user likes that document One star and B.",
            "He hates that document.",
            "Our product or whatever it is, and then in the second set of experiments we'll try to see whether how we can use this medal."
        ],
        [
            "Object evanetz constraints to generate smoother ranking for the sentiment classification problem.",
            "OK, so these are results from the web text."
        ],
        [
            "Application is a problem on the X axis you have increasing amounts of supervision, so in this case you have 4 total 448.",
            "Table instances, so in the graph of 4200 nodes you have only 48 nodes which are labeled and you're trying to figure out the label for all the other nodes and the Y axis you have this.",
            "Measure of quality, which is again standard for this data.",
            "Set an for text classification is called this precision recall, break even point.",
            "So basically if you look at the precision recall curve, this PRVP is the point at which precision and recall are similar are the same.",
            "So here we compare it against LP, which is a standard graph based transductive method.",
            "An adsorption, they are starting algorithm in our proposal mad.",
            "So we see that in all the settings.",
            "Mad basically outperforms both of them, so our goal here is not to compare against Madden absorption because we wanted to 1st find an objective for absorption itself, right?",
            "And we're in a force too.",
            "Suggest mad because we couldn't find one for adoption.",
            "OK, so let's look at now at."
        ],
        [
            "Sentiment classification data set.",
            "Here we use a precision as the evaluation measure.",
            "These are the number of label instances, essentially a number of labeled nodes, and here we see that Madden absorption are competitive to each other, but both of them outperform this label propagation algorithm due to zoo at all.",
            "OK, so essentially we we basically determined that on real world datasets these these proposed.",
            "This proposed algorithm is not doing is doing something sensible.",
            "And in fact, on this web KB data set for this particular setting, it achieves state of the art performance, and for this other settings achieve the best second best solution from a separate algorithm and details regarding those are in the paper.",
            "OK, so now let's look at."
        ],
        [
            "The problem of smooth sentiment ranking and how we could use this medal algorithm.",
            "The extended version to achieve this now say given a document in algorithm proposes these for ranked results.",
            "OK so at the top at rank one it proposes this four star.",
            "That means it thinks the user really likes it and then it gradually decreases at the second prediction it says it predicts three star, two star in one star.",
            "So we call these kind of things smooth predictions.",
            "Compared to that we have this sort of nonsmooth predictions.",
            "So here at the first rank you have the prediction of four, so remember that both of these are for the same document, right?",
            "We have 4.",
            "Then it's one, so it's not clear what the what really is being predicted right?",
            "Because first you said 4 star."
        ],
        [
            "In the second slot you are saying single stock, but So what we want to do is we want to prefer these kind of smooth rankings over this."
        ],
        [
            "Non smooth rankings.",
            "So the question is how we could do using the metal garthim.",
            "It turns out it's very simple so."
        ],
        [
            "We can do is that we can impose the constraint that the one star and two star are kind of similar, so they they capture similar semantics on part of the user.",
            "Sometimes they might be assigning two star instead of one star and vice versa and similarly trestar enforced are also similar.",
            "So and this this edge weight of 1 in this.",
            "Label graph basically represents how similar they are, so we just assign them one.",
            "So let's look at how we can now use this medal with these constraints to generate smoother sentiment ranking."
        ],
        [
            "So in this case, what we do is we get the predictions from mad and then using also from metal.",
            "Then we look at the top 2 predictions.",
            "So here we are looking at label one and Label 2.",
            "So we want to see how many jumps are there, how many non smooth or smooth jumps are there between those two labels?",
            "The label that's predicted in the first slot and the second slot.",
            "And then we ignored the order.",
            "So we don't care like which one was proposed at what ranking.",
            "But we just want to make sure that.",
            "Are we just want to see like with how many smooth transitions are there?",
            "So in this case you see that mad is generating lots of four to one jumps in the 1st and 2nd slot right?",
            "Because of this tall structures there, but this is the model output after you impose the constraints, so all you can you see that all these things have vanished.",
            "Basically those were the non smooth transitions that we had in their prediction.",
            "An all you have is this 423.",
            "And two to one predictions right?",
            "So to start a one star or four star to three star so you don't have any any of those four star, Two star or four star to one star transitions in the top 2 predictions.",
            "So this basically tells us that metal is able to generate smoother."
        ],
        [
            "Ranking, but at the same time preserving quality of your predictions.",
            "OK, so that essentially brings me to the."
        ],
        [
            "Of the talk.",
            "So in this we tried to analyze absorption."
        ],
        [
            "And figure out that it's not optimizing any particular.",
            "Object, if so, that prompted us to propose this modified absorption, which has similar update as adoption, but has a well defined optimization.",
            "So we extended mad to meddle to handle these kind of dependent dependent labels where we know upfront similarities between labels and we want to impose that.",
            "And we also provided results on real world data."
        ],
        [
            "As part of future work, we plan to apply this medal algorithm on information extraction from where we essentially."
        ],
        [
            "Started because there you have lots and lots of labels which are dependent synonymous and we want."
        ],
        [
            "Capture that.",
            "Thanks for time."
        ],
        [
            "OK, maybe one or two quick questions.",
            "And maybe the next speaker can come and get set up as well.",
            "Yep.",
            "No artia.",
            "You said you have a theorem.",
            "It shows that it's not optimizing your objective, yeah?",
            "She senses how that yeah, so I mean.",
            "Yeah, so I've been proving that was slightly tricky because what we had was like the final update and then we wanted to like reverse engineer an go back to the objective right?",
            "So we I mean there are like some assumptions on the class of functions that we consider there.",
            "But what we show is that if it were optimizing an objective, well defined function, then the algorithm, the update that's being optimized are like used by absorption.",
            "It wouldn't converge.",
            "So that's totally yes.",
            "In practice at least.",
            "OK, for normal questions, this next speaker again, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everyone, I'm part A so in this talk I'm going to talk to you about this new set of actual new regularize algorithms for transductive learning will try to understand the meaning of each one of those terms.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with Koby Kraemer who was at the University of Pennsylvania but is now at Technion, Israel, OK, Let me try to give you some context behind this work and like why we tried to address this problem so we were actually looking at the problem of information extraction.",
                    "label": 0
                },
                {
                    "sent": "In particular.",
                    "label": 0
                },
                {
                    "sent": "Trying to assign semantic labels to entities on a large scale.",
                    "label": 0
                },
                {
                    "sent": "So for example, you type in the entity, say or input the individual system.",
                    "label": 0
                },
                {
                    "sent": "Like Toyota, we want the system to come up with labels like.",
                    "label": 0
                },
                {
                    "sent": "It's a Japanese company, car manufacturer, automobile manufacturers, human readable descriptors of that type, and we want to cover as many entities and as many labels as possible.",
                    "label": 0
                },
                {
                    "sent": "So we decided to use sort of a graph based framework for that and we use them.",
                    "label": 0
                },
                {
                    "sent": "Algorithm called absorption.",
                    "label": 0
                },
                {
                    "sent": "Will see details about it in a short while and we successfully used that use that algorithm to that problem, but it turned out that like the algorithm wasn't analyzed.",
                    "label": 0
                },
                {
                    "sent": "It wasn't known like whether it's optimizing any sort of objective function in all, because it was set up as an algorithm and not as an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So this in this paper.",
                    "label": 0
                },
                {
                    "sent": "Basically we tried to see whether we could understand that algorithm, better figure out its optimization in all.",
                    "label": 0
                },
                {
                    "sent": "And we'll see like where we ended up with that and that kind of.",
                    "label": 0
                },
                {
                    "sent": "We have some.",
                    "label": 0
                },
                {
                    "sent": "Basically, we figured out that like that algorithm, absorption wasn't analyze.",
                    "label": 0
                },
                {
                    "sent": "Wasn't optimizing any particular objective, so we had to come up with our own.",
                    "label": 0
                },
                {
                    "sent": "Proposed algorithms which we wanted to have, like a good optimization but was as close as possible to the original absorption graph and then once we have the objective we could optimize it too.",
                    "label": 0
                },
                {
                    "sent": "Fit our problem at hand better, so that's that's roughly the context in which, like proper prompted us to look at this problem, an resulted in this paper.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's first look at graph based, semi supervised learning, where actually say if you have a set of.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is as in the trans as in the regular supervised learning, where some of the nodes are labeled, but in addition you have lots of other points too.",
                    "label": 0
                },
                {
                    "sent": "For example, all the unlabeled points that you care about an all the points that you consider like.",
                    "label": 0
                },
                {
                    "sent": "So the labeled and unlabeled points together you want to put them in a graph in igraph, basically supervised learning, where each node will represent one of your instances OK, and you can always measure similarities between the nodes.",
                    "label": 0
                },
                {
                    "sent": "And that similarity between two nodes or two instances is represented by this weight on this edge.",
                    "label": 0
                },
                {
                    "sent": "OK, so here, for example, in this white node is connected to this node with the similarity of .3 an to this other node, which is similarity of .2.",
                    "label": 0
                },
                {
                    "sent": "So again, all the points that you care about, labeled or unlabeled is there in front of you and there is no out of sample extension.",
                    "label": 0
                },
                {
                    "sent": "There will be no new point that you will be asked to classify in the future.",
                    "label": 0
                },
                {
                    "sent": "So that basically defines the transductive setting.",
                    "label": 0
                },
                {
                    "sent": "As many of you are already probably familiar, and then we induce a graph or in many cases the graph maybe already present.",
                    "label": 0
                },
                {
                    "sent": "Then some of these nodes are labeled.",
                    "label": 0
                },
                {
                    "sent": "So we have like labels for them.",
                    "label": 0
                },
                {
                    "sent": "In this case for these two nodes.",
                    "label": 0
                },
                {
                    "sent": "So this is a red node, the red label.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the blue label OK, and the white nodes are basically like the unlabeled nodes, and we want to determine the labels for these unlabeled nodes.",
                    "label": 0
                },
                {
                    "sent": "This white nodes, using this graph structure and.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The initial label notes OK, so basically graph based semi supervised learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "Try to solve this problem by using the graph structure and unlabeled nodes.",
                    "label": 0
                },
                {
                    "sent": "Trying to assign labels to the unlabeled nodes.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we use a one of those algorithms then we'll end up with something like this.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can you see this thing from the back?",
                    "label": 0
                },
                {
                    "sent": "OK great so I mean so initially we had these two nodes labeled.",
                    "label": 0
                },
                {
                    "sent": "Now we use the graph based semi supervised learning algorithm to assign labels to all the other nodes.",
                    "label": 0
                },
                {
                    "sent": "So in this case you see that these two nodes are kind of at the boundary between these two regions.",
                    "label": 0
                },
                {
                    "sent": "Say if we call it like the red region and the blue reason so they have like multiple labels assigned.",
                    "label": 0
                },
                {
                    "sent": "So it's like partially red, partially blue and these are like this node.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is very far apart from the blue, so it gets like the red label OK and those these algorithms are tend to be very flexible.",
                    "label": 0
                },
                {
                    "sent": "Here I have shown you the binary case, but you can easily extend it to the multi label setting or when you have multiple labels per node.",
                    "label": 0
                },
                {
                    "sent": "So over the years a few algorithms have been proposed starting from this label propagation or method called zarazua his authors starting from 2003 to some very recent methods called this absorption, which will be the basis on which will.",
                    "label": 0
                },
                {
                    "sent": "Develop our methods and this is the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Essentially we used for our.",
                    "label": 0
                },
                {
                    "sent": "Label assignment to entities.",
                    "label": 0
                },
                {
                    "sent": "Problem that I had mentioned first.",
                    "label": 0
                },
                {
                    "sent": "Also, since we build on the adsorption algorithm, let's try to look at its characteristics.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The features first.",
                    "label": 0
                },
                {
                    "sent": "This algorithm has been proposed by the baluja at all in World Wide Web 2008 where they use it to recommend videos to YouTube videos to users.",
                    "label": 0
                },
                {
                    "sent": "OK, that was like the first application.",
                    "label": 0
                },
                {
                    "sent": "Then we use in our semantic label assignment to entities.",
                    "label": 0
                },
                {
                    "sent": "Problem in this paper from MLP.",
                    "label": 0
                },
                {
                    "sent": "But as I mentioned, it has been it hasn't been analyzed so far, so it's not clear whether it's optimizing any objective and that essentially is the motive.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation for our work.",
                    "label": 0
                },
                {
                    "sent": "So to understand algorithm better and determine the details.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's look.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How the algorithm proceeds?",
                    "label": 0
                },
                {
                    "sent": "So again say we have like 4 nodes here, which are three of which are labeled.",
                    "label": 0
                },
                {
                    "sent": "Two of them are colored.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are assigned one particular label, Red in this case and the other one has a blue label and we're trying to figure out the label for this white node, which is currently unlabeled.",
                    "label": 0
                },
                {
                    "sent": "Then this,",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That you saw was basically like the update performed by the algorithm and you will be.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "May be surprised to see that we have another label here, which is the green label, so the red labels are coming from these two nodes.",
                    "label": 0
                },
                {
                    "sent": "The blue is coming from this other node.",
                    "label": 0
                },
                {
                    "sent": "Then we have an additional label called Dummy Label.",
                    "label": 0
                },
                {
                    "sent": "OK so dummy label or is a special label which is in a way internal to the algorithm and that you can use that the meal able to express your confidence in a way for the assignment of labels that you're doing to a particular done.",
                    "label": 0
                },
                {
                    "sent": "So for example if you assign.",
                    "label": 0
                },
                {
                    "sent": "High value to this dummy label node.",
                    "label": 1
                },
                {
                    "sent": "Then maybe you are expressing the fact that you're not very certain what label should be assigned to that particular node, and we'll see shortly how we could use this.",
                    "label": 0
                },
                {
                    "sent": "Adamy label the special label for a particular problem.",
                    "label": 0
                },
                {
                    "sent": "So it turns out this kind of.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Updates that the algorithm performs.",
                    "label": 0
                },
                {
                    "sent": "Basically you just need to look at the labels of you.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Neighbors and using that you can determine the labels for the current node.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that's very highly scalable an iterative, so you can essentially paralyze the algorithm and that helps you in working with massive amounts of data.",
                    "label": 1
                },
                {
                    "sent": "So when you're working with the transductive setting, you have both labeled and unlabeled.",
                    "label": 0
                },
                {
                    "sent": "We assume that the unlabeled data is is widely available, so having this kind of an iterative algorithm which can be paralyzed which can process large amounts of data efficiently.",
                    "label": 0
                },
                {
                    "sent": "Is a desirable property.",
                    "label": 0
                },
                {
                    "sent": "Also the main diff.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prince, between this between absorption and previously proposed graph based algorithm, is that it doesn't treat all nodes equally OK, so in particular, it turns out that it discounts high degree nodes.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically if you are trying to.",
                    "label": 0
                },
                {
                    "sent": "Sorry, so when you are like propagating the labels through the graph then it won't trust the high degree nodes as much as a node with a lesser degree.",
                    "label": 0
                },
                {
                    "sent": "So the assumption is that like the high degree node is connected to too many points or too many nodes in the graph, so it's not as reliable as the other nodes, so we won't trust it so much.",
                    "label": 0
                },
                {
                    "sent": "So it's it's in a way slightly undemocratic view, but we'll see that that's actually useful thing.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm has two equivalent views, so.",
                    "label": 0
                },
                {
                    "sent": "We have two nodes, so one is the label diffusion view.",
                    "label": 0
                },
                {
                    "sent": "So you have a labeled node.",
                    "label": 0
                },
                {
                    "sent": "You have an unlabeled node, so you can think.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It as a label diffusion process where the labels basically propagate or diffuse.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the label node, the Blue note here, to the unlabeled node here, which is in white.",
                    "label": 0
                },
                {
                    "sent": "And this they need not be connected by an edge directly.",
                    "label": 0
                },
                {
                    "sent": "You can use the structure of the graph.",
                    "label": 0
                },
                {
                    "sent": "The transitive property to propagate the labels from this label to the unlabeled node and equivalent view is that often random walk, so that view is again you have this unlabeled node.",
                    "label": 0
                },
                {
                    "sent": "This label node you're trying to ascertain what's what label I should assign to this node.",
                    "label": 0
                },
                {
                    "sent": "So you can start a random walk from this unlabeled node and using the graph structure you make transitions.",
                    "label": 0
                },
                {
                    "sent": "Then whenever you get a label node, you can assign this unlabeled node the label of this label node with the probability with which you arrive at that node.",
                    "label": 0
                },
                {
                    "sent": "So if this node is very hard to reach from this.",
                    "label": 0
                },
                {
                    "sent": "Our unlabeled note you then you it will end up with a lower score for the corresponding label of this label node.",
                    "label": 0
                },
                {
                    "sent": "OK, so it so let's look at this random walk view with in some detail because they will be useful.",
                    "label": 0
                },
                {
                    "sent": "So again, start with some unlabel.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know, do you hear you make a path?",
                    "label": 0
                },
                {
                    "sent": "This need not be an edge, a path so you reach at some intermediate vertex.",
                    "label": 0
                },
                {
                    "sent": "Now the question is what steps you could perform.",
                    "label": 0
                },
                {
                    "sent": "So it turns out you can do.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two things.",
                    "label": 0
                },
                {
                    "sent": "One is with some probability PV can't, so that's like a continuation probability.",
                    "label": 0
                },
                {
                    "sent": "You can continue the random walk to neighboring nodes of this, no?",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, one other thing that you can do is if this V node where one of the labeled nodes right the seed nodes that we saw in the first slide, you can assign it to this node U with the probability.",
                    "label": 0
                },
                {
                    "sent": "With some injection probability which is specific to this node.",
                    "label": 1
                },
                {
                    "sent": "OK, so the first step is you continue the random walk with some probability.",
                    "label": 1
                },
                {
                    "sent": "The second thing is you assign this node U the node you the label of this node V if there are any with some particular injection probability which is specific to this node V and the third thing you can do is abandon the random walk and assign this node U that dummy label that we saw before with some probability of.",
                    "label": 0
                },
                {
                    "sent": "Abandonment.",
                    "label": 0
                },
                {
                    "sent": "OK, so these three things are not specific, so they are specific to this node V and they will sum to one Now if.",
                    "label": 0
                },
                {
                    "sent": "OK yeah, so so is the notion of how the assignment, how we can assign labels to this starting node while doing the random walk clear.",
                    "label": 0
                },
                {
                    "sent": "That's OK. Um?",
                    "label": 1
                },
                {
                    "sent": "So I again repeat that like this dummy label is coming from this abandonment probability, right?",
                    "label": 0
                },
                {
                    "sent": "Because I mean if you reach at a node, you can determine whether you want to continue or stop the random walk.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see how this framework can be.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use now to discount how high degree nodes are reduced their importance during the learning phase.",
                    "label": 0
                },
                {
                    "sent": "So as I have.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Already mentioned, we think that the high degree nodes are unreliable, so we don't want the random walk to follow through them or allow label propagation to happen through those nodes.",
                    "label": 0
                },
                {
                    "sent": "So the solution is that we increase the probability on these high degree nodes, so we.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take the high degree or the abandonment probability on a particular node proportional to its degree.",
                    "label": 0
                },
                {
                    "sent": "OK, so here the degree I have simplified a bit, but you can think of like if it's connected.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A lot of neighbors.",
                    "label": 0
                },
                {
                    "sent": "Maybe you want to consider the weighted degree of that node and you want to make that proportional.",
                    "label": 0
                },
                {
                    "sent": "So if it's connected to a lot of nodes, don't don't trust that no Dan don't let any random walk or propagation to happen to that node.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I told you that like one of our initial motive.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oceans was weather.",
                    "label": 0
                },
                {
                    "sent": "We wanted to see whether at.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Option was optimizing any particular objective, right?",
                    "label": 0
                },
                {
                    "sent": "So it turns out that it's actually not an, though that's kind of unsatisfactory, because we wanted to show that it's actually doing something meaningful that we could write down concisely, but we have a theorem in the paper saying that it's not doing anything so, but we also saw that it has some desirable properties that it's like highly scaleable, iterative and you can control basically these kind of propagation.",
                    "label": 1
                },
                {
                    "sent": "Through the notes so you have like finer grained control.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to retain those properties in whatever algorithm we propose, and we also at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Want to have well defined, objective, right?",
                    "label": 0
                },
                {
                    "sent": "Which we know is being optimized as part of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So that basically made us to propose.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "New algorithm called modified absorption, which we call for short mad.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's look at the modified absorption guard.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um so.",
                    "label": 0
                },
                {
                    "sent": "I don't know whether you can see, but there is a.",
                    "label": 0
                },
                {
                    "sent": "Our objective here, but.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We don't want to look into the details, so just remove that and come up with a higher level view.",
                    "label": 0
                },
                {
                    "sent": "OK, so the mad.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And objective that's been minimized has actually three terms.",
                    "label": 0
                },
                {
                    "sent": "So for each node, thank you.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "So it has three terms, so the first term is it wants to minimize any loss.",
                    "label": 0
                },
                {
                    "sent": "Or maybe we can do that animation once more.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the first term, if for any node you have any seed label, if it's labeled up front, then finally you want to match that label as close as possible.",
                    "label": 0
                },
                {
                    "sent": "The second term is like if if two nodes are connected by a high weighted edge, then you want to assign them across an edge.",
                    "label": 0
                },
                {
                    "sent": "Similar labels and a third term is if you based on that abandonment probabilities that we had seen.",
                    "label": 0
                },
                {
                    "sent": "If you have any prior over that node that like that node should get high dummy label score then you can impose that using that third term.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, high degree, no discounting is imposed using the third term, but the good thing is that when you solve this you get an.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Absorption like iterative update.",
                    "label": 0
                },
                {
                    "sent": "It's not exactly the same because both are not the same algorithm, but it's very close and this is like as far as close as we could get to get the absorption like update but have a well defined objective.",
                    "label": 0
                },
                {
                    "sent": "Now that we have an objective.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With us we can extend that for to match or to handle the data that we have better.",
                    "label": 0
                },
                {
                    "sent": "So one thing that's important is that the important recognizes that the labels are not always mutually exclusive, right?",
                    "label": 1
                },
                {
                    "sent": "So for example, I have a example here from the.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Beer domain, so I mean I just came up with these numbers.",
                    "label": 0
                },
                {
                    "sent": "They may not reflect your tastes well, but so each one of these is a label, right?",
                    "label": 0
                },
                {
                    "sent": "So say we have like a set of beer cans in front of us.",
                    "label": 0
                },
                {
                    "sent": "An algorithm is trying to determine the label for each one of them right?",
                    "label": 0
                },
                {
                    "sent": "And we want to say that like Scotch, we want to provide this information to the algorithm that Scottsdale and Yale are kind of similar, with the similarity score of 1 and then like white beer an like top formented beer.",
                    "label": 0
                },
                {
                    "sent": "Are similar with a score of pointing, so each node in this graph.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a label and this are weights basically represent the similarity between labels, so this is this we call a label graph.",
                    "label": 0
                },
                {
                    "sent": "This is different from the initial graph where each node was an instance right?",
                    "label": 0
                },
                {
                    "sent": "So here we are trying to capture the dependencies between labels and we want to input this thing into the algorithm and make sure that the similar labels are assigned similar scores on each one of the nodes.",
                    "label": 0
                },
                {
                    "sent": "OK, so how we can do that in a principled way that we have this origonal obj.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "50 from Mad, the modified absorption objective.",
                    "label": 0
                },
                {
                    "sent": "Now we can add an additional term right?",
                    "label": 0
                },
                {
                    "sent": "So here what we can do is if a node.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is assigned different scores for for a similar label as determined by the label graph.",
                    "label": 0
                },
                {
                    "sent": "The algorithm will be penalized so.",
                    "label": 0
                },
                {
                    "sent": "So basically it will try to assign similar labels as determined by the label graph.",
                    "label": 0
                },
                {
                    "sent": "Similar scores on each one of the nodes, and that's imposed by this new term.",
                    "label": 0
                },
                {
                    "sent": "New fourth term.",
                    "label": 0
                },
                {
                    "sent": "And this this modified this modified absorption with dependent labels.",
                    "label": 1
                },
                {
                    "sent": "We call this algorithm medal.",
                    "label": 0
                },
                {
                    "sent": "OK, I have already gone through this.",
                    "label": 0
                },
                {
                    "sent": "It will penalize if similar labels are assigned different scores where this SIM.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Party is given by these labeled graph, which could be either automatically induced or it could be a way.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Need to introduce prior knowledge into the system.",
                    "label": 0
                },
                {
                    "sent": "And it turns out.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By adding this thing, you don't increase the complexity of the model.",
                    "label": 0
                },
                {
                    "sent": "You still get in scalable iterate.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If update with convergence guarantees so every both of them, like both Mad and medal objective has convergence guarantees.",
                    "label": 1
                },
                {
                    "sent": "So let's look at.",
                    "label": 0
                },
                {
                    "sent": "Now some experimental results.",
                    "label": 0
                },
                {
                    "sent": "And we'll see.",
                    "label": 0
                },
                {
                    "sent": "We'll see.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experiments for both Madden Medal and in particular for medal, will try to see how we could induce this dependence between labels.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we do two sets of experiments.",
                    "label": 0
                },
                {
                    "sent": "In the first we do a set of classification experiments on two datasets.",
                    "label": 1
                },
                {
                    "sent": "So this is web based standard text classification data set that we got from Subramanian builds.",
                    "label": 0
                },
                {
                    "sent": "So it has four classes around 4200 instances, so the graph will have 4200 nodes and the similarity between the nodes is basically computed using a cosine similarity.",
                    "label": 0
                },
                {
                    "sent": "Anna K nearest neighbor is nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "Graph is constructed similarly.",
                    "label": 0
                },
                {
                    "sent": "The other data set is this sentiment classification, where the objective is given a document algorithm is asked to specify what's the polarity, whether a user likes, whether a user on which that algorithm is strained would like that document or not OK, so it's again a four class, so maybe you have already seen, like in Amazon you provide these stars, right?",
                    "label": 0
                },
                {
                    "sent": "So four star will be the user likes that document One star and B.",
                    "label": 0
                },
                {
                    "sent": "He hates that document.",
                    "label": 0
                },
                {
                    "sent": "Our product or whatever it is, and then in the second set of experiments we'll try to see whether how we can use this medal.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Object evanetz constraints to generate smoother ranking for the sentiment classification problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are results from the web text.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Application is a problem on the X axis you have increasing amounts of supervision, so in this case you have 4 total 448.",
                    "label": 0
                },
                {
                    "sent": "Table instances, so in the graph of 4200 nodes you have only 48 nodes which are labeled and you're trying to figure out the label for all the other nodes and the Y axis you have this.",
                    "label": 0
                },
                {
                    "sent": "Measure of quality, which is again standard for this data.",
                    "label": 0
                },
                {
                    "sent": "Set an for text classification is called this precision recall, break even point.",
                    "label": 0
                },
                {
                    "sent": "So basically if you look at the precision recall curve, this PRVP is the point at which precision and recall are similar are the same.",
                    "label": 0
                },
                {
                    "sent": "So here we compare it against LP, which is a standard graph based transductive method.",
                    "label": 0
                },
                {
                    "sent": "An adsorption, they are starting algorithm in our proposal mad.",
                    "label": 0
                },
                {
                    "sent": "So we see that in all the settings.",
                    "label": 0
                },
                {
                    "sent": "Mad basically outperforms both of them, so our goal here is not to compare against Madden absorption because we wanted to 1st find an objective for absorption itself, right?",
                    "label": 0
                },
                {
                    "sent": "And we're in a force too.",
                    "label": 0
                },
                {
                    "sent": "Suggest mad because we couldn't find one for adoption.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's look at now at.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sentiment classification data set.",
                    "label": 0
                },
                {
                    "sent": "Here we use a precision as the evaluation measure.",
                    "label": 0
                },
                {
                    "sent": "These are the number of label instances, essentially a number of labeled nodes, and here we see that Madden absorption are competitive to each other, but both of them outperform this label propagation algorithm due to zoo at all.",
                    "label": 0
                },
                {
                    "sent": "OK, so essentially we we basically determined that on real world datasets these these proposed.",
                    "label": 0
                },
                {
                    "sent": "This proposed algorithm is not doing is doing something sensible.",
                    "label": 0
                },
                {
                    "sent": "And in fact, on this web KB data set for this particular setting, it achieves state of the art performance, and for this other settings achieve the best second best solution from a separate algorithm and details regarding those are in the paper.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let's look at.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem of smooth sentiment ranking and how we could use this medal algorithm.",
                    "label": 1
                },
                {
                    "sent": "The extended version to achieve this now say given a document in algorithm proposes these for ranked results.",
                    "label": 0
                },
                {
                    "sent": "OK so at the top at rank one it proposes this four star.",
                    "label": 0
                },
                {
                    "sent": "That means it thinks the user really likes it and then it gradually decreases at the second prediction it says it predicts three star, two star in one star.",
                    "label": 1
                },
                {
                    "sent": "So we call these kind of things smooth predictions.",
                    "label": 0
                },
                {
                    "sent": "Compared to that we have this sort of nonsmooth predictions.",
                    "label": 0
                },
                {
                    "sent": "So here at the first rank you have the prediction of four, so remember that both of these are for the same document, right?",
                    "label": 0
                },
                {
                    "sent": "We have 4.",
                    "label": 0
                },
                {
                    "sent": "Then it's one, so it's not clear what the what really is being predicted right?",
                    "label": 0
                },
                {
                    "sent": "Because first you said 4 star.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the second slot you are saying single stock, but So what we want to do is we want to prefer these kind of smooth rankings over this.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Non smooth rankings.",
                    "label": 0
                },
                {
                    "sent": "So the question is how we could do using the metal garthim.",
                    "label": 0
                },
                {
                    "sent": "It turns out it's very simple so.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can do is that we can impose the constraint that the one star and two star are kind of similar, so they they capture similar semantics on part of the user.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they might be assigning two star instead of one star and vice versa and similarly trestar enforced are also similar.",
                    "label": 0
                },
                {
                    "sent": "So and this this edge weight of 1 in this.",
                    "label": 0
                },
                {
                    "sent": "Label graph basically represents how similar they are, so we just assign them one.",
                    "label": 0
                },
                {
                    "sent": "So let's look at how we can now use this medal with these constraints to generate smoother sentiment ranking.",
                    "label": 1
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case, what we do is we get the predictions from mad and then using also from metal.",
                    "label": 0
                },
                {
                    "sent": "Then we look at the top 2 predictions.",
                    "label": 0
                },
                {
                    "sent": "So here we are looking at label one and Label 2.",
                    "label": 0
                },
                {
                    "sent": "So we want to see how many jumps are there, how many non smooth or smooth jumps are there between those two labels?",
                    "label": 0
                },
                {
                    "sent": "The label that's predicted in the first slot and the second slot.",
                    "label": 0
                },
                {
                    "sent": "And then we ignored the order.",
                    "label": 0
                },
                {
                    "sent": "So we don't care like which one was proposed at what ranking.",
                    "label": 0
                },
                {
                    "sent": "But we just want to make sure that.",
                    "label": 0
                },
                {
                    "sent": "Are we just want to see like with how many smooth transitions are there?",
                    "label": 0
                },
                {
                    "sent": "So in this case you see that mad is generating lots of four to one jumps in the 1st and 2nd slot right?",
                    "label": 0
                },
                {
                    "sent": "Because of this tall structures there, but this is the model output after you impose the constraints, so all you can you see that all these things have vanished.",
                    "label": 0
                },
                {
                    "sent": "Basically those were the non smooth transitions that we had in their prediction.",
                    "label": 0
                },
                {
                    "sent": "An all you have is this 423.",
                    "label": 0
                },
                {
                    "sent": "And two to one predictions right?",
                    "label": 0
                },
                {
                    "sent": "So to start a one star or four star to three star so you don't have any any of those four star, Two star or four star to one star transitions in the top 2 predictions.",
                    "label": 0
                },
                {
                    "sent": "So this basically tells us that metal is able to generate smoother.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ranking, but at the same time preserving quality of your predictions.",
                    "label": 0
                },
                {
                    "sent": "OK, so that essentially brings me to the.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the talk.",
                    "label": 0
                },
                {
                    "sent": "So in this we tried to analyze absorption.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And figure out that it's not optimizing any particular.",
                    "label": 0
                },
                {
                    "sent": "Object, if so, that prompted us to propose this modified absorption, which has similar update as adoption, but has a well defined optimization.",
                    "label": 1
                },
                {
                    "sent": "So we extended mad to meddle to handle these kind of dependent dependent labels where we know upfront similarities between labels and we want to impose that.",
                    "label": 0
                },
                {
                    "sent": "And we also provided results on real world data.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As part of future work, we plan to apply this medal algorithm on information extraction from where we essentially.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Started because there you have lots and lots of labels which are dependent synonymous and we want.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Capture that.",
                    "label": 0
                },
                {
                    "sent": "Thanks for time.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, maybe one or two quick questions.",
                    "label": 0
                },
                {
                    "sent": "And maybe the next speaker can come and get set up as well.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "No artia.",
                    "label": 0
                },
                {
                    "sent": "You said you have a theorem.",
                    "label": 0
                },
                {
                    "sent": "It shows that it's not optimizing your objective, yeah?",
                    "label": 0
                },
                {
                    "sent": "She senses how that yeah, so I mean.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I've been proving that was slightly tricky because what we had was like the final update and then we wanted to like reverse engineer an go back to the objective right?",
                    "label": 0
                },
                {
                    "sent": "So we I mean there are like some assumptions on the class of functions that we consider there.",
                    "label": 0
                },
                {
                    "sent": "But what we show is that if it were optimizing an objective, well defined function, then the algorithm, the update that's being optimized are like used by absorption.",
                    "label": 0
                },
                {
                    "sent": "It wouldn't converge.",
                    "label": 0
                },
                {
                    "sent": "So that's totally yes.",
                    "label": 0
                },
                {
                    "sent": "In practice at least.",
                    "label": 0
                },
                {
                    "sent": "OK, for normal questions, this next speaker again, thank you.",
                    "label": 0
                }
            ]
        }
    }
}