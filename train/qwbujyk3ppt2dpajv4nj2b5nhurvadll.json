{
    "id": "qwbujyk3ppt2dpajv4nj2b5nhurvadll",
    "title": "Semisupervised Learning Approaches",
    "info": {
        "author": [
            "Tom Mitchell, Machine Learning Department, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "September 2006",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/mlas06_mitchell_sla/",
    "segmentation": [
        [
            "Not easy to do really.",
            "Nor that.",
            "Flirting.",
            "I have to tell you that I'm new to pentop computers and so.",
            "This is partly a grand experiment to extend my experience with Pentop computers.",
            "Anyway, you've been through a couple lectures already, so you've talked about.",
            "Text classification, document classification and also about information extraction and.",
            "Primarily, you've been looking at that is really a function approximation problem in most cases where there's some function to be learned from, say, in the case of document classification of function from some document to a label like this email is spam, or this email isn't spam and use.",
            "So far we've been talking about that is primarily supervised learning problem, which of course means that we train the system by showing it labeled pairs.",
            "Here's an email, and this one is spam.",
            "Here's an email This one is not spam and.",
            "If you've got labeled data like that, that's by far the best way to train your system.",
            "But the point of this hour is to ask the following question."
        ],
        [
            "Given that statistical learning methods really do require substantial amount of training data.",
            "Is there any way that we can make use of all that unlabeled text that's out there?",
            "And as you know, there's a lot of it, right?",
            "There's a lot of unlabeled email in your inbox, I bet, and there's even more unlabeled text down on the web.",
            "Billions and billions of pages.",
            "So there's a lot out there, and if we could somehow figure out a way to get our learning algorithms to.",
            "Guarantee albelli take advantage of that unlabeled test.",
            "Then there's a huge resource that we could tap into.",
            "So that's the question.",
            "Now if you think about it.",
            "Just in common sense terms, if I tell you that I'm trying to learn some function from X to Y.",
            "Like documents to labels about whether it's spam or not.",
            "And then I tell you, I have some labeled examples like I'll give you a.",
            "Particular value of X and the corresponding value of Y.",
            "It's easy for you to see why that should help you learn.",
            "The spam classifier, for example, the question I'm asking now is, well, what if I give you another set of things?",
            "Which where I just give you F size and I don't give you the labels.",
            "If you think about this in common sense terms, it's obvious that the answer has to be.",
            "In general, that's not going to help me learn anything.",
            "Right, it's the.",
            "Imagine trying to learn a spam classifier where I just say, oh here, 100,000 messages.",
            "Just doesn't seem like it's reasonable to expect that we'll be able to learn better from.",
            "That additional data so that the wonderful thing about this, and the reason that we're going to spend a whole hour on it, is that it does turn out that in certain circumstances, not all.",
            "An if our problem has the right structure and we'll see what that means.",
            "At least two cases of what that can mean.",
            "Then indeed we can get a lot of benefit out of unlabeled data as well.",
            "So what you should?",
            "So that's what we're up to in this hour.",
            "By the end of the hour.",
            "Hopefully you will agree with me that there are at least 2 interesting classes of problems which have an interesting structure to them, such that if you get lucky enough to have a problem in that class, then you'll be able to use unlabeled data to help learn an Fortunately will turn to get out to cover some.",
            "Document classification problems and some information extraction problems.",
            "OK, so."
        ],
        [
            "That's what we're going to do, so let's look at the first of those settings and.",
            "The idea here is, let's say you talked about Naive Bayes classifier.",
            "Did you talk about Naive Bayes classifiers for documents during the first lecture good?",
            "And so, of course, when you talked about."
        ],
        [
            "You talked about it as a problem where we would.",
            "Classify text documents by first representing them as some feature vector where, for example we might use the words in the document's features, the number of occurrences of those words as the feature values.",
            "And then convert the problem of classifying a text document into the problem of classifying vectors like this."
        ],
        [
            "And before you talked about doing that, is the supervised learning problem and it works pretty well, for example.",
            "If you're interested, you can grab a set of 20,000 labeled training examples off this website and apply a some code there for Naive Bayes classification to that.",
            "And if you do, you'll see that.",
            "Those 20,000 articles are labeled by which of these 20 newsgroups they came from, and if you were to train on a subset of these examples.",
            "And you have a one out of 20 prediction, so I get you know the classification task is given a new article.",
            "Which of these 20 does it come from?",
            "Then you'll see with supervised learning you can actually reach pretty good accuracies if you train on say, 10,000 of those articles.",
            "Which would be what 500 from each of these twenty categories?",
            "Then with a naive Bayes classifier.",
            "For example, you can get pretty close to 90% accuracy in that one out of 20 predictions, so it's pretty reasonable.",
            "But that's taking 10,000 labeled examples.",
            "And that costs a lot of time to produce.",
            "And also you can see the curve is kind of still going up, so we'd like to know what if we had, let's say, a billion more unlabeled examples, could we get that even higher?",
            "So as I mentioned, we're going to talk about 2 problems settings where.",
            "People have found that indeed we could use unlabeled data to improve.",
            "Learning and the."
        ],
        [
            "First of those, is this kind of naive Bayes classifier for text document classification and so one way to think about this problem is.",
            "I may have some documents for which I have labels.",
            "Suppose I'm trying to again learn this function from.",
            "X to Y.",
            "And here's my training data.",
            "And notice for some of these examples I have known class.",
            "I have known class values and for others I don't.",
            "These are my unlabeled examples.",
            "They have the question marks there.",
            "So the generic.",
            "Issue here is how would we modify our naive Bayes classifier if we had data like this?",
            "And of course if we ended the first hour of today, you would have said, well the thing to do here is to use these three training examples and ignore these other two.",
            "But in fact you don't need to, and so I don't know who has the solution.",
            "Who hasn't seen this before?",
            "I'm not letting changing hands.",
            "Anybody have an idea what you do?",
            "OK well.",
            "Simple as it may sound.",
            "It's surprisingly effective to do the following very simple thing.",
            "Let's train a naive Bayes classifier using the label data.",
            "Those three.",
            "And then let's use it to predict the values for these other two examples for which we don't have labels.",
            "And then let's train a second naive Bayes classifier.",
            "That uses all five of these examples.",
            "But this time of course member Naive Bayes classifier which is predicting this label will actually predict.",
            "A probability distribution, so it might say, well, the probability that y = 1.",
            "Given that this is example 5.",
            "Is .7 in the probability that it's 0?",
            "Might be .3.",
            "So when we train our naive Bayes classifier on.",
            "These labeled data and then apply it to the unlabeled data will get these probabilistic.",
            "Predictions?",
            "And so then, on the second iteration, what we do is we train a second naive Bayes classifier.",
            "But now we use these probabilistic labels.",
            "For the examples for which we had no known labels, and we use the certain labels that we had for the original lines.",
            "Now, of course, we'll get a different naive Bayes classifier, and we can iterate that procedure.",
            "OK, now it does sound a little odd because it's a little bit like.",
            "Hallucinate ING your own world and then believing it and then hallucinating more deeply and sort of running off into some self reinforcement reinforcement set of assumptions.",
            "So it's a strange idea, but in fact.",
            "There are cases where you can show that this is a good idea, and then empirically it turns out that with many document classification problems, indeed it is a good idea.",
            "So this is a process called.",
            "It's actually an EM algorithm where M stands for expectation maximization.",
            "Some of you have seen EM algorithms before, but what they generally involve is having a set of variables.",
            "In our case, these five variables and we're trying to learn some probability distribution over them.",
            "This friggs this, by the way, is just a Bayes net that corresponds to the naive Bayes assumptions.",
            "Aninion what we generally do is.",
            "We iterate on a 2 step process where in the first step we actually usually make this the first step.",
            "We assign probability distributions to the unlabeled variables.",
            "In our case, that means the variable Y, the label itself, and then on the second step of the EM process we re estimate the parameters in our probability distribution.",
            "In this case that's the parameters of the Naive Bayes classifier and we iterate this process.",
            "And that process in general is guaranteed.",
            "2.",
            "Converge toward parameter estimates for our Naive Bayes classifier, they have the following nice property.",
            "They are they locali maximize the likelihood of the data.",
            "And when I say locali, I mean if you were to wiggle those parameters in any.",
            "Particular small way you'd find the likelihood of the data decrease likelihood of the data simply means the probability of the data that we have.",
            "According to the learned probabilistic model.",
            "And so that's a nice property.",
            "It converges to a set of parameter estimates that maximize the probability of the data.",
            "A little bit more technically, emanci Mises, the expected data likelihood where the expectation is taken over the distribution.",
            "On these unknown variables.",
            "OK, so there is a simple procedure."
        ],
        [
            "In fact, we can try this.",
            "On some real data, which is what Carmel Nigam who was here just moments ago.",
            "Giving a talk he was your second speaker today.",
            "In fact, his PhD thesis was on this very topic and here is from his work the algorithm and this is just what we said it would be.",
            "That is, given a set of labeled documents and some other unlabeled documents.",
            "The first step is to build an initial classifier.",
            "From the labeled documents only.",
            "Then iterate.",
            "This EM process where on the East step we use the current trained classifier.",
            "We're going to stay to hear to indicate the set of parameters that we've learned for the Naive Bayes classifier to estimate the membership of each unlabeled document in each class.",
            "That is the probability.",
            "That the document will.",
            "I belong to that class, so we're signing probabilistic labels to each document and then on the M Step re estimate the classifier, that is, retrain it using those probabilistically applied labels.",
            "OK, makes sense.",
            "Alright, so we do that.",
            "How long will we do it until the Theta stop changing from one iteration to the next or the change becomes very small?",
            "And that we know that's going to happen eventually, because this procedure is guaranteed to increase the.",
            "Expected data likelihood on each iteration, so eventually, assuming the step size is large enough, eventually it's going to reach a local optimum, and at that point it will stop changing and then will be done.",
            "OK, so now let's look at what happens in practice if we try this."
        ],
        [
            "I'm going to skip over some equations, but.",
            "If you're interested, you can look at the paper the citation to the paper is in your notes, and here are the equations.",
            "But they basically are just saying what what I said is an equation for getting the priors on the class label Y. I'm sorry, not the priors, but this is the probability of the class being the Y value being a particular class given the document I.",
            "And just like you talked already about Naive Bayes that involves looking at the individual words W. And the probability of those words showing up conditioned on the class label and then in the M step we retrain.",
            "The two main parameters from the Naive Bayes classifier, that is, the probability distribution on each word.",
            "Given the class and the prior probabilities of the classes.",
            "And I think you covered that already right?",
            "And the previous.",
            "So the equations for especially this.",
            "And step are a little bit more.",
            "Complicated, then the first version of Naive Bayes you saw WHI because now we have to modify this to deal with probabilistic labels.",
            "The labels aren't just.",
            "Yes or no, spam or not.",
            "Instead we get these probabilistic labels .8 spam .29.",
            "But other than that, it's."
        ],
        [
            "It's very straightforward, maybe more interesting Lee, given the amount of time we have, we can look at what really happens in the classifier from one iteration to the next.",
            "So this will show.",
            "This shows some results of running the CN process on a set of data, which is web pages.",
            "These web pages were being classified either as.",
            "Web pages home pages of academic course.",
            "Or not.",
            "And in this particular set of data that I remember, there are very few.",
            "Oh, here it is 1 labeled example per class.",
            "So there is one example, just one document, which is the course homepage.",
            "And.",
            "I recall there were many, but I don't remember how many unlabeled documents, but was on the order of 1000.",
            "So now what do we do with the analysis?",
            "We apply the CN procedure.",
            "What we do is what's the first step we take that one.",
            "Labeled document that we have, we trained a naive Bayes classifier on it.",
            "And doing that, what we're going to do is we're going to learn these parameters of the classifier.",
            "That is, what's the probability that the word shoe will occur, given that it is a course page, what's the probability that the word shoe will occur, given that it's not a course page?",
            "And similarly for the other 50,000 words of English?",
            "OK, now having trained at.",
            "Just on their first iteration, suppose we stop and now we have the program print out the top 20 words.",
            "That his learned are the most.",
            "Are the best discriminators.",
            "Of course, pages from not course pages?",
            "That is, we could do is we could print out each.",
            "We could rank order the words by the probability, the odds ratio, the probability of the word will occur given that it's part of the course page divided by the probability of the word occurring, given that it's not a course page.",
            "And if we do that, these are the words we get on the left.",
            "So now if you look at those words you see you can probably guess what that single training example course page was about.",
            "This clearly was about an artificial intelligence class.",
            "Uh.",
            "I happen to know that our US is the name of Daniela Roos at Dartmouth who is teaching the course at the time so you can see very idiosyncratic words showing up there, which is exactly what you would expect if you had one labeled example.",
            "OK. Now let's take the first step of the EM process.",
            "We now take that label, that classifier that produce.",
            "You know this based on these is the most significant words.",
            "And of course there are other thousands of words down below here that we're not looking at.",
            "And now we apply it and we label all the unlabeled pages.",
            "Then we take those probabilistic labels and retrain.",
            "Now let's print out again.",
            "The top words.",
            "This time we get this list, which is actually quite interesting here, by the way, the D stands for digit is just the way of encoding so that 13 and 72 become the same DD token.",
            "Now what you see is after just one iteration of this M process, danielo ruse is gone, Dartmouth is gone.",
            "And the top words are now things that you can recognize.",
            "This more generically associated with the course.",
            "Like course numbers, words like lecture handout, do problem set.",
            "You still do get some.",
            "Individual names that aren't very generic.",
            "If we do that one more step, then we get this set of words.",
            "So you can see what's happening is in just a couple iterations we suddenly get very representative words about courses showing up emerging from this strange process.",
            "So in practice it's sometimes quite.",
            "Effective as it is here."
        ],
        [
            "So in fact, if we continue that process.",
            "Until it converges ATM process letting it run, a number of iterations.",
            "By the way, in these experiments it only took about a dozen iterations to converge.",
            "It was rare that it would take more than 30.",
            "Usually it was somewhere between 10:00 or closer to 10 than 30.",
            "And then if we test that the accuracy of that final classifier which was trained on both labeled and unlabeled data.",
            "Here's a plot of the accuracy that we get.",
            "For the.",
            "EM based classifier.",
            "This is the one that's using labeled and unlabeled.",
            "As a function of the number of labeled documents that we gave it.",
            "And this curve below it is the accuracy that we get if we train our usual supervised learning.",
            "Only naive Bayes classifier on the same labeled data set.",
            "So what you see is that the accuracy is substantially higher.",
            "It's not perfect, but this is essentially the difference between labeled only.",
            "Naive Bayes versus this EM process that iteratively retrains the Naive Bayes classifier using labeled and unlabeled data, and so this difference is the value of the unlabeled data.",
            "Now here's an interesting phenomena that you see.",
            "You get a big benefit from unlabeled data.",
            "If you have only 20 labeled documents.",
            "Remember there were twenty groups, so 20.",
            "This is the newsgroup data by the way.",
            "So in this case we had one example for each of the one labeled example for each of the 20 newsgroups.",
            "As the number of labeled documents increases, of course the accuracy improves for both classifiers, but the leverage that we get from the unlabeled data decreases.",
            "Why would that be?",
            "One way to see that we ought to expect that is to think of what the curve would look like over there.",
            "Asymptotically, if I increase the number of labeled data to Infinity.",
            "Then we're going to go ahead.",
            "Ah, that's right, there actually wasn't what I was thinking, but that's good.",
            "You're right that if we're not increasing the unlabeled data, get swamped out by the labeled data.",
            "That's true.",
            "I was thinking was a slightly different explanation, although I don't disagree with what you said.",
            "Which is that if I have infinite labeled data, I get perfect estimates of my naive Bayes parameters.",
            "It's having infinite labeled data.",
            "Is just like somebody telling me, oh, here are the exact probabilities for each word conditioned on the class.",
            "And so if I have infinite labeled data, I've already got the perfect the best optimal parameter estimates I can get.",
            "And of course having additional information won't make me any better than that.",
            "But when labeled data is sparse, then you can see there is a significant benefit.",
            "Alright, so there is meth."
        ],
        [
            "At 1:00"
        ],
        [
            "I said that we would."
        ],
        [
            "Look at two methods, but before we leave method one have a couple of questions for you.",
            "So I showed you a couple of examples where it works quite well.",
            "Training that course classifier.",
            "Training the.",
            "Newsgroup Document classifier works pretty well.",
            "In fact, the truth is.",
            ", had a hard time finding a set of documents where it did not work.",
            "Very did not work well.",
            "So it did seem like this procedure.",
            "Applied to test documents works quite well and there might be something about text.",
            "That makes that the case.",
            "Um?",
            "So let's spend a minute just thinking about what would be the best case in the worst case.",
            "So I think the easiest way I know to think about this is to imagine that we instead of having.",
            "A bag of 50,000 different words and 50,000 features.",
            "Imagine we just had one feature.",
            "Suppose that we only look at.",
            "We're trying to tell if it's spam.",
            "We only look for the word Nigeria.",
            "No other word.",
            "We count how many times Nigeria occurs in the email.",
            "OK, so here's that could be our feature.",
            "Number of Nigeria occurrences.",
            "And now suppose that I look just among positive examples of spam.",
            "And I drew a histogram or.",
            "The probability that a randomly drawn word will be.",
            "Will be in Nigeria.",
            "Given that, well, let's try it.",
            "Given that it is spam.",
            "So I might have some kind of distribution like this.",
            "For.",
            "Spam equals yes.",
            "And it might have a different distribution, maybe fewer occurrences of the word, for example.",
            "In the case where spam equals no.",
            "And if that's the case.",
            "Uh.",
            "You know?",
            "If that's the case.",
            "Then the naive Bayes modeling assumption.",
            "Is going to make this kind of unlabeled data be wonderful for us?",
            "Because, and I don't know whether you discuss this particular fact about Naive Bayes when you discuss it the first time.",
            "So you guys were here, you gotta help me.",
            "If.",
            "I just trained in Naive Bayes classifier with no labels at all.",
            "Suppose I run the TM process, but I have zero initial labels and I just make up random parameters to get started and I have those random parameters, sign labels and retrain and re estimate and reach.",
            "You know, just go back and forth ATM process.",
            "That's a very good and very commonly used clustering algorithm.",
            "What it tends to do is find.",
            "The two finds two groups of data that are easily distinguished.",
            "It's called mixture model clustering.",
            "It's a very common method of doing clustering.",
            "If you want to document clustering, that's a great algorithm for doing it.",
            "Well, if we have data like this and we were to run that clustering algorithm with zero labeled data.",
            "We would expect to find these two clusters.",
            "They would just kind of happen.",
            "Especially, we only had this one feature.",
            "So the data is very nicely clustered even with zero labels into these two clusters.",
            "And now suppose I have O2 labels, one positive and one negative.",
            "Like maybe I have a document right over here which is positive in one right over here, which is negative.",
            "Then it's very easy once we know what the clusters are to say.",
            "Oh, this cluster is negative.",
            "This cluster is positive, and that's essentially with the EM process is going to do.",
            "Alright, so as long as we have a data set which would be clustered anyway into groups that happen to correspond to the classes we're interested in classifying.",
            "Like in this case, if this cluster is into spam and not spam, we're Golden.",
            "OK, however, that same data set.",
            "If I were interested in some other classification function like.",
            "Was this authored on Tuesday or Wednesday?",
            "Then the clusters I'm going to get out of course will still be these two clusters that if I give unlabeled data.",
            "But that those two clusters are simply probably irrelevant to the classification of whether it was Tuesday or Wednesday.",
            "Alright, so my point is this.",
            "This kind of use of unlabeled data can be very helpful.",
            "And you can kind of see with the Nigeria spam example why.",
            "Right, if I just have this one word, the clusters that will come out from the unlabeled data will be just the grouping that I want, and the only thing that I'll need labeled data for is to label which cluster is positive and which is negative, and I don't need many labels to do that.",
            "On the other hand, if the naturally occurring clusters in the data set don't happen to correspond to the labels that I'm interested in.",
            "Assigning by my classifier.",
            "Then this unlabeled stuff and the M stuff will be downright misleading.",
            "So to oversimplify a little bit.",
            "That's actually the structure of the data that this.",
            "Method is leveraging off of.",
            "And so I think 1 interesting.",
            "So how can we test given some documents, that whether it's got this kind of structure?",
            "I think the best test is to run unsupervised clustering on the document set.",
            "Take the few labels that we have and see if the clusters that come out actually.",
            "Capture.",
            "Different clusters capture different labels, and if so, then we know that the naturally occurring clustering structure of that data is at least.",
            "Consistent with this naive Bayes modeling assumption.",
            "And things things have a chance of working.",
            "In fact, the situation is a little more pleasant than I'm suggesting because.",
            "I kind of made the extreme argument that well, what if we have zero labels and we just do clustering, then we better see that the cluster is aligned with the classes.",
            "It's actually a little better than that, because as you recall, the EM process actually begins with some labels, not with zero labels, and so the.",
            "Clusters that get formed initially.",
            "Are initialized from the labeled examples, and so if there are several different clusterings that might come out from different random initializations of the clustering algorithm, those supervised labeled examples will assure that.",
            "We at least pick one that's.",
            "Pretty consistent with these labels, so then that becomes the initialization part, but I think you get the intuition here, right?",
            "OK, so any comments, suggestions, clarifications on this?",
            "How's this match up to?",
            "Public servants in mobile command upgrade as anything.",
            "Wow.",
            "I don't I.",
            "Short answer is I don't think anybody yet knows how the brain works, but there is quite a. Chipping away at it.",
            "The question was how does this correspond with what we might know about what happens in the brain?",
            "Of course, we don't know what happens exactly in the brain, but it is interesting that there's a.",
            "Of.",
            "Pretty active.",
            "Sort of research group in.",
            "I. Cognitive science and also in developmental psychology.",
            "That's.",
            "Very interested in how is it that humans can learn so much.",
            "So rapidly is children from apparently not mostly unlabeled example.",
            "Mostly unlabeled examples.",
            "And.",
            "So it is a question that is.",
            "Considerable interest there.",
            "My own hunch is that, well, I don't know if this CM stuff has any is an appropriate model for that or not.",
            "My guess is that the second method we're going to talk about is a much more appropriate model for.",
            "How humans get away with using unlabeled data.",
            "Well, the grass.",
            "Yeah.",
            "His eyes were number of people talking, so I'm wondering is it is the natural limit to this can still be cause something just generally hard to classify even for humans.",
            "Yeah, I think there is.",
            "It does top out somewhere below 100% for a couple reasons.",
            "One is that in some senses the classes are inherently ambiguous and so for example even something apparently obvious like spam.",
            "What's spam to you might not overlap perfectly with what spam to me, although probably it overlaps 90% I would guess.",
            "But even if we had two independent people label spam documents, they don't agree completely.",
            "So there is.",
            "That kind of ambiguity in the labels to begin with, but a second is that there are some really severe assumptions in the naive Bayes model for text classification, right?",
            "It's a bag of words model, so it's like saying the sentence machine learning is great is the same as saying the sentence.",
            "Learning great is machine.",
            "And to people they don't seem like I'm saying the same thing, but to this program they do.",
            "This is jumbling them altogether and ignoring.",
            "The content so there are some modeling problems here too that hurt.",
            "OK, so let's move on to the."
        ],
        [
            "2nd.",
            "So I thought hard about how to use my limited 60 minutes here and what you should know is that there's actually quite a bit.",
            "This is a fairly hot topic.",
            "In statistical language analysis right now, how do we use unlabeled data to improve on our learning?",
            "Um, in 60 minutes.",
            "I thought the most useful thing we could do was cover what seemed to me to be the two key types of problems structure that people are leveraging with these algorithms.",
            "And so I can show you a lot more algorithms for leveraging.",
            "That first type of information, the first type of problem structure.",
            "But I'm not going to 'cause I think that the most important thing to get out of this talk is from that first part is oh, I see if documents naturally cluster into groups based on the words that occur in them.",
            "And if those groups that they naturally cluster into happened to align with the class labels that I want my classifier to assign, then it's probably a good idea for me to be using unlabeled data to augment my classifier.",
            "And Furthermore, I could probably test for that by running an unsupervised clustering algorithm and then seeing to what degree those natural clusters do align with class labels.",
            "And if you get that idea, then I'm happy and you could probably yourself.",
            "Go off and think of better algorithms than the one that we just talked about.",
            "But when we talked about is pretty easy to grok and it follows right on Naive Bayes classifier."
        ],
        [
            "We just talked about.",
            "So in that same spirit, let's look at a very different problem structure that also justifies the use of unlabeled data.",
            "And here, let me illustrate this just with an example.",
            "On the example, is classifying web pages.",
            "And I would say I would assert an.",
            "I hope you'll agree with me in 10 minutes that the problem of classifying webpages happens to have a wonderful structure that lets us use unlabeled data to boost the classification accuracy and the structure that it has is very different from that M naturally occurring clusters of documents structure that we just saw.",
            "If you ask me for a name for this structure, that webpage classification has, I'd say the key thing that it has is.",
            "It's a classification problem where the features that describe the examples are redundantly predictive.",
            "So what do I mean by that?",
            "Suppose you had to classify this web page that you were looking at as either a faculty homepage or a course homepage.",
            "As a human, I think you could do that.",
            "OK, and by the way, these little features up here in the corner.",
            "These are the hyperlinks that are pointing to that web page.",
            "So somewhere else there is a web page that contains a hyperlink saying my advisor and that's pointing to this page.",
            "So I can think of the features that describe this example as containing both words on the page and those words on the hyperlinks that point to the page.",
            "Now when I say that this.",
            "Problem has this nice structure that the features are redundantly predictive.",
            "What I mean precisely is this.",
            "If I ask you to classify this web page, you can still do it.",
            "If I only show you this subset of the features.",
            "Pretty obvious that's a faculty homepage.",
            "So you can do it just with this subset of the features.",
            "Similarly, if I only show you those, you can also do it.",
            "So the property that this has, assuming that you were able to classify those two examples, is that if you know the truth, if you know the classification function.",
            "Once you've learned it, then it suffices to see either.",
            "The first feature set or the second feature set either one is sufficient.",
            "To do the classification, once you know the function.",
            "And that's the structure that web page classification.",
            "Almost has.",
            "That's the structure that we need to have, and that if we have, we can prove actually, that unlabeled data will be useful for us.",
            "OK."
        ],
        [
            "So let's see how.",
            "Why should that be relevant?",
            "It's actually a very simple story.",
            "Once you see it, here's why should be relevant.",
            "So what you just agreed with me, I think, is that if you knew the classification function.",
            "You could classify that webpage either by looking at this set of features or this other set of features.",
            "And so they are redundantly predictive of the class label.",
            "And so we can develop an algorithm that uses labeled and unlabeled data.",
            "In the following way, instead of training one classifier, will train 2.",
            "One of them will get this feature set.",
            "One of them will get this feature set.",
            "And then the two criteria that we're going to use.",
            "To estimate the parameters of our classifier to train the classifier, our first of course that both of these two classifiers should be trained so that they correctly classified whatever labeled examples we have.",
            "That's the easy part, but then actually, if you think about it, if I have an unlabeled example, suppose this is an unlabeled example.",
            "Then the program doesn't know whether that's a faculty homepage or a course homepage, but it does know.",
            "That whatever classifier one gives us the answer ought to agree.",
            "With classifier too.",
            "Right and so, for example, the fees are to naive Bayes classifiers.",
            "Or two logistic regression classifiers or two?",
            "Your favorite classifier to support vector machines.",
            "Then each labeled example is a constraint on the parameters of the classifier.",
            "Of each of the classifiers independently, but each unlabeled example is a wonderful constraint on the joint choice of parameters in the two classifiers.",
            "And the more of these unlabeled examples we can show to the system, the more constraints were piling up on the joint choice of the parameters for classifier one, Classifier 2.",
            "That's the simple idea.",
            "It's actually pretty straightforward.",
            "But of course we can only do this if.",
            "We really happen to have a learning problem where we can divide the features into one set and another set, both of which are sufficient to get the right answer, and in that case we'll be in good shape.",
            "So that's the idea."
        ],
        [
            "That problem setting is called Co training.",
            "You can think of a lot of algorithms for leveraging off that problem structure, and in fact I guess the first one or one of the first ones was.",
            "Was this very simple classified?",
            "This very simple Co training algorithm that says given some labeled data L and unlabeled data you will train.",
            "A hyperlink classifier Anna Page words on the page classifier.",
            "And then we'll use only the labeled data for that, by the way.",
            "Then we'll allow both of those classifiers to go through the unlabeled data and pick just the small number of highly confident.",
            "Examples where it's highly confident of its prediction.",
            "And then these will get added to the self labeled examples and will iterate this process.",
            "And intuitively, the reason this works well is that.",
            "Suppose this is our."
        ],
        [
            "One of our first examples.",
            "We have, let's say only 100 million unlabeled webpages.",
            "Then even after this one example are hyperlinked classifier could become very confident that if it sees the hyperlink that says exactly my advisor.",
            "Then it's going to want to label the page that it points to as a faculty page.",
            "And now when we search through the unlabeled data, if the system happens to find one other my advisor link.",
            "Then that link is probably going to point to a webpage that looks nothing like this.",
            "And will be a web page where the word base where the page classifier will be very uncertain.",
            "So part of it works in our favor here is that.",
            "The hyperlinks that we've seen before.",
            "In the labeled set happened to Co occur with web pages that we haven't seen before, and vice versa.",
            "And so the things that classifier A is very confident about tend to be things that class for her B is.",
            "Not.",
            "And so they can both help each other in this kind of bootstrapping fashion.",
            "So that's the idea of Co training and.",
            "This actually work."
        ],
        [
            "Books.",
            "For example, here's the simple case where we had 12 labeled web Pages, 1000 unlabeled pages.",
            "And as we iterate that simple code training learning algorithm, the error on held out test set classification.",
            "Decreases from on the order of 10 to 15% down to five to 10%.",
            "So that's the kind of practical advantage we get.",
            "In fact.",
            "Since then, I.",
            "People have found.",
            "Stronger results, but even here if you think about percentage wise improvement in the air, it's pretty substantial."
        ],
        [
            "But let's spend a minute.",
            "Talking a little bit more.",
            "Looking in a little bit more detail at the.",
            "Real structure of this problem.",
            "Because I think that will let you actually see.",
            "That that.",
            "Even in the case of this kind of page classification, there will be problems where this works better and works worse depending on some subtleties of the structure.",
            "OK, so what's really going on there, actually?"
        ],
        [
            "Let me let me talk about it this way.",
            "Here's the best way I know of explaining what's really going on with Co training.",
            "So think of.",
            "Are document classification problem or the web?",
            "If you like as a very very large bipartite graph, bipartite graph just means there are two kinds of nodes in the connection.",
            "Go from one kind to the other kind.",
            "In our case, these nodes are web pages with words on them.",
            "These nodes are the hyperlinks.",
            "Like my advisor.",
            "You could think of drawing the think of representing the web this way, or any collection of web pages that you have.",
            "And will put in edge between.",
            "A particular hyperlink in a particular webpage, just in case those Co occur as an example.",
            "So each edge here is an example.",
            "Of course, some of those edges will have labels for.",
            "Somebody might tell us this is a positive example.",
            "That webpage in that hyperlink pair other ones will be unlabeled.",
            "Those are just the edges that have no labels on them.",
            "Others will be labeled as negative, and so forth.",
            "OK, so you see the notation I'm using here.",
            "Alright.",
            "Now let's think of how we how the Co training process would work.",
            "If we use the world's simplest learning algorithm, let's do.",
            "We're going to Train 2 rote learners, one rote learner, for the hyperlinks.",
            "One wrote learner for the web pages.",
            "And what happens?",
            "Well, these unlabeled edges I'll say, or my unlabeled examples.",
            "And the labels that have pluses and minus on or the initially labeled subset of the data.",
            "And what's going to happen?",
            "So on the 1st iteration, my rote learner of pages will learn that this page is a positive example.",
            "And this one is negative and this one is negative.",
            "And then it gets to look for other examples that it can confidently label.",
            "So here's one.",
            "Here's another hyperlink.",
            "Page pair, which involves this same page, so it's going to label that pair a positive example.",
            "Similarly, this hyperlink labeler will label this.",
            "Example is positive.",
            "Right?",
            "Now on the next iteration, the Rote learner will learn that this page is also positive.",
            "Because.",
            "It's been told that here and then it will label this one.",
            "Which will lead to the program knowing that that's positive.",
            "Right, so if I were to execute this over many iterations, what I'd see was oh, the positive labels are going to propagate throughout this connected component in the Die Partite graph.",
            "The negative labels will similarly propagate throughout this connected components that I have.",
            "Negatives everywhere in here.",
            "And the program will, if assign those labels.",
            "So it's it's destined to do this.",
            "If the initial data that.",
            "Well, that's that's how the Co training algorithm works.",
            "So now if we ask what would make a data set, what kind of structure in a data set would be a best case for Co training?",
            "Would lead to a good outcome.",
            "What kind of data set would lead to would be a worst case would lead to a horrible outcome.",
            "What would lead to an ineffective outcome?",
            "You want to be able to answer those questions and you want to be able to answer them in the form of telling me what kind of bipartite graph I could draw that would cause the algorithm to behave nicely.",
            "Horribly or just not?",
            "Behave any differently than if it had no one labeled data.",
            "So let's take those in turn.",
            "I would say if you ask what can make this work well, I'd say well, actually works pretty well on this graph.",
            "Why?",
            "Because this graph has the nice property that.",
            "In fact, it's two disconnected, two different connected components, each of which happens to correspond to one class label.",
            "It's kind of reminiscent of the EMH thing that we were saying to clusters of documents that have that correspond to class labels.",
            "Question how would I calculate how many iterations this will take before it converges?",
            "Exactly right, it's the diameter of the largest connected component.",
            "Or the radius?",
            "Yeah, it's right.",
            "'cause those labels can propagate only one edge per iteration.",
            "And eventually if we want them to propagate throughout the connected component, it will depend on how many edges you have to traverse in order to get to everywhere in your connected component.",
            "Given that.",
            "What's this?",
            "If you could design a data set?",
            "Can you design me a data set where in one iteration this thing converges?",
            "How about this one?",
            "But if I modify this graph and just make an edge between?",
            "Every pair.",
            "You get it right.",
            "In the positive connected component, I added more edges so that everything is connected in.",
            "Distance one.",
            "Which is another another way of saying it is that the my advisor Hyperlink will Co occur with every faculty homepage.",
            "Right, if I have that kind of.",
            "Structure then just in one iteration this thing will converge.",
            "In fact, that kind of structure.",
            "Is consistent with.",
            "Let's call these two feature sets X1 and X2 in general.",
            "Instead of hyperlinks in pages.",
            "If I'm trying to learn some function.",
            "From"
        ],
        [
            "X, which is just the pair X one X2 to Y.",
            "Then it turns out there's of the best property you can have.",
            "Is that X one is conditionally independent?",
            "Of X2.",
            "Given why?",
            "Put another way.",
            "X the probability that X one that a particular value for X1 will occur.",
            "Given why is the same as the probability that value of X one is not influenced by X2?",
            "It's the same conditional independence assumption we make about words.",
            "The bags of words in naive Bayes, but it turns out if you have a problem like this where those are conditionally independent."
        ],
        [
            "Then if they are conditionally independent given Y.",
            "And the function you're trying to learn as pack learnable, which roughly means that given enough labeled data, you could learn it.",
            "Enough noisy label data.",
            "You could learn it.",
            "Then it's also pack learnable.",
            "From enough labels that give you a better than random initial classifier.",
            "And then polynomial amount of unlabeled data.",
            "So that's an example of a kind of theoretical statement.",
            "That we can make this.",
            "True only in the case where we had that very nice structure."
        ],
        [
            "Well, if you think about conditional independence.",
            "In this.",
            "In this connected component, X1 and X2 are not conditionally independent.",
            "Given why, how do I know that because Y equals minus for all these?",
            "And you can see that the probability that I'll get a particular X2 like this one really does depend on the value of X1.",
            "For example, if this is X1, then I might get this X2, but if this is X1 then I'm not going to get that X2.",
            "And I want them to be conditionally independent.",
            "What would this graph have to look like?",
            "This.",
            "So if they are conditionally independent effects one and X2 are conditionally independent given Y.",
            "Then my graph will look just like this.",
            "It will be the completely connected graph.",
            "So you see what's going on here?",
            "I think this gives you a nice insight into the algorithm that iterative Co training thing where one classifier is labeling things for the other.",
            "And then back the other way.",
            "Corresponds, at least in the simple case of rote learning classifiers to this kind of propagation of labels through the graph.",
            "The graph itself represents the data set.",
            "And we can talk about this structure of a data set, useful structure of the data set in terms of properties of this bipartite graph.",
            "So we get.",
            "For example, that conditional independence.",
            "Of the X1 and X2 features given the label is a very.",
            "Good case for algorithm.",
            "A case that would be a problem would be if there was an edge going like this.",
            "Connecting up our components.",
            "That would cause problems with our simple rote learning algorithm.",
            "As you can see, because these pluses would crawl into the negative component and propagate through there and cause.",
            "Grief.",
            "The good news is that there are algorithms that are more probabilistic in nature for allowing this kind of back and forth Co training that.",
            "Don't fall apart when you have that kind of a structure.",
            "They allow the negative labels here to outvote that positive propagation.",
            "That's trying to happen across.",
            "The bridge.",
            "And so there are other algorithms that are more robust to this kind of problem and that still work quite well.",
            "Alright, so.",
            "I think you get the idea of.",
            "That problem structure that lets us use unlabeled data.",
            "I want to ramp up because I want Jamie to start his talk, but let's at least review what we saw so.",
            "We really saw two different kinds of problems structure.",
            "Characteristics of the data and the classification function that you're trying to learn over that data.",
            "That in fact they are theoretical."
        ],
        [
            "We where we can get theoretical guarantees that unlabeled data will be helpful to us.",
            "The first one was the case where of REM extension of Naive Bayes.",
            "Which is going where unlabeled data is going to help us improve classification accuracy if.",
            "The data naturally clusters into groups that align with the class labels.",
            "And you can see that there are simple ways of testing whether that's likely to be true, even if you only have a little bit of labeled data and a lot of unlabeled, did he just do the clustering and then see whether that's happening?",
            "So that can be practically useful.",
            "The second thing we saw, the Co training setting.",
            "Involves instead of a different kind of assumption that we could take our feature vector that we're trying to classify and break it into two pieces.",
            "The hyperlink words and the words on the page, such that either one alone is sufficient to make the prediction.",
            "If you knew the classification function and then we saw that in that case, there are.",
            "Well, we didn't cover two.",
            "We covered one theoretical result, but there are others that show how the structure of this kind of graph influences how well that kind of Co training is going to work."
        ],
        [
            "So I would just end by saying I put a number of additional slides in here in case you want them for reference.",
            "The last couple also have bibliography of readings in this area and there is a."
        ],
        [
            "Very nice book, this just coming out or just did come out which is an overview of semi supervised learning and I recommend that book.",
            "It's pretty good.",
            "If you want to really dig into this area, that's a good place to look.",
            "So let me end there and I'll turn things over to Jamie, who I see is.",
            "Ready and waiting in the wings."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not easy to do really.",
                    "label": 0
                },
                {
                    "sent": "Nor that.",
                    "label": 0
                },
                {
                    "sent": "Flirting.",
                    "label": 0
                },
                {
                    "sent": "I have to tell you that I'm new to pentop computers and so.",
                    "label": 0
                },
                {
                    "sent": "This is partly a grand experiment to extend my experience with Pentop computers.",
                    "label": 0
                },
                {
                    "sent": "Anyway, you've been through a couple lectures already, so you've talked about.",
                    "label": 0
                },
                {
                    "sent": "Text classification, document classification and also about information extraction and.",
                    "label": 0
                },
                {
                    "sent": "Primarily, you've been looking at that is really a function approximation problem in most cases where there's some function to be learned from, say, in the case of document classification of function from some document to a label like this email is spam, or this email isn't spam and use.",
                    "label": 0
                },
                {
                    "sent": "So far we've been talking about that is primarily supervised learning problem, which of course means that we train the system by showing it labeled pairs.",
                    "label": 0
                },
                {
                    "sent": "Here's an email, and this one is spam.",
                    "label": 0
                },
                {
                    "sent": "Here's an email This one is not spam and.",
                    "label": 0
                },
                {
                    "sent": "If you've got labeled data like that, that's by far the best way to train your system.",
                    "label": 0
                },
                {
                    "sent": "But the point of this hour is to ask the following question.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Given that statistical learning methods really do require substantial amount of training data.",
                    "label": 1
                },
                {
                    "sent": "Is there any way that we can make use of all that unlabeled text that's out there?",
                    "label": 0
                },
                {
                    "sent": "And as you know, there's a lot of it, right?",
                    "label": 0
                },
                {
                    "sent": "There's a lot of unlabeled email in your inbox, I bet, and there's even more unlabeled text down on the web.",
                    "label": 0
                },
                {
                    "sent": "Billions and billions of pages.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot out there, and if we could somehow figure out a way to get our learning algorithms to.",
                    "label": 0
                },
                {
                    "sent": "Guarantee albelli take advantage of that unlabeled test.",
                    "label": 0
                },
                {
                    "sent": "Then there's a huge resource that we could tap into.",
                    "label": 0
                },
                {
                    "sent": "So that's the question.",
                    "label": 0
                },
                {
                    "sent": "Now if you think about it.",
                    "label": 0
                },
                {
                    "sent": "Just in common sense terms, if I tell you that I'm trying to learn some function from X to Y.",
                    "label": 0
                },
                {
                    "sent": "Like documents to labels about whether it's spam or not.",
                    "label": 0
                },
                {
                    "sent": "And then I tell you, I have some labeled examples like I'll give you a.",
                    "label": 0
                },
                {
                    "sent": "Particular value of X and the corresponding value of Y.",
                    "label": 0
                },
                {
                    "sent": "It's easy for you to see why that should help you learn.",
                    "label": 0
                },
                {
                    "sent": "The spam classifier, for example, the question I'm asking now is, well, what if I give you another set of things?",
                    "label": 0
                },
                {
                    "sent": "Which where I just give you F size and I don't give you the labels.",
                    "label": 0
                },
                {
                    "sent": "If you think about this in common sense terms, it's obvious that the answer has to be.",
                    "label": 0
                },
                {
                    "sent": "In general, that's not going to help me learn anything.",
                    "label": 0
                },
                {
                    "sent": "Right, it's the.",
                    "label": 0
                },
                {
                    "sent": "Imagine trying to learn a spam classifier where I just say, oh here, 100,000 messages.",
                    "label": 0
                },
                {
                    "sent": "Just doesn't seem like it's reasonable to expect that we'll be able to learn better from.",
                    "label": 0
                },
                {
                    "sent": "That additional data so that the wonderful thing about this, and the reason that we're going to spend a whole hour on it, is that it does turn out that in certain circumstances, not all.",
                    "label": 0
                },
                {
                    "sent": "An if our problem has the right structure and we'll see what that means.",
                    "label": 0
                },
                {
                    "sent": "At least two cases of what that can mean.",
                    "label": 0
                },
                {
                    "sent": "Then indeed we can get a lot of benefit out of unlabeled data as well.",
                    "label": 0
                },
                {
                    "sent": "So what you should?",
                    "label": 0
                },
                {
                    "sent": "So that's what we're up to in this hour.",
                    "label": 0
                },
                {
                    "sent": "By the end of the hour.",
                    "label": 0
                },
                {
                    "sent": "Hopefully you will agree with me that there are at least 2 interesting classes of problems which have an interesting structure to them, such that if you get lucky enough to have a problem in that class, then you'll be able to use unlabeled data to help learn an Fortunately will turn to get out to cover some.",
                    "label": 0
                },
                {
                    "sent": "Document classification problems and some information extraction problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's what we're going to do, so let's look at the first of those settings and.",
                    "label": 0
                },
                {
                    "sent": "The idea here is, let's say you talked about Naive Bayes classifier.",
                    "label": 0
                },
                {
                    "sent": "Did you talk about Naive Bayes classifiers for documents during the first lecture good?",
                    "label": 0
                },
                {
                    "sent": "And so, of course, when you talked about.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You talked about it as a problem where we would.",
                    "label": 0
                },
                {
                    "sent": "Classify text documents by first representing them as some feature vector where, for example we might use the words in the document's features, the number of occurrences of those words as the feature values.",
                    "label": 0
                },
                {
                    "sent": "And then convert the problem of classifying a text document into the problem of classifying vectors like this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And before you talked about doing that, is the supervised learning problem and it works pretty well, for example.",
                    "label": 0
                },
                {
                    "sent": "If you're interested, you can grab a set of 20,000 labeled training examples off this website and apply a some code there for Naive Bayes classification to that.",
                    "label": 0
                },
                {
                    "sent": "And if you do, you'll see that.",
                    "label": 0
                },
                {
                    "sent": "Those 20,000 articles are labeled by which of these 20 newsgroups they came from, and if you were to train on a subset of these examples.",
                    "label": 0
                },
                {
                    "sent": "And you have a one out of 20 prediction, so I get you know the classification task is given a new article.",
                    "label": 0
                },
                {
                    "sent": "Which of these 20 does it come from?",
                    "label": 0
                },
                {
                    "sent": "Then you'll see with supervised learning you can actually reach pretty good accuracies if you train on say, 10,000 of those articles.",
                    "label": 0
                },
                {
                    "sent": "Which would be what 500 from each of these twenty categories?",
                    "label": 0
                },
                {
                    "sent": "Then with a naive Bayes classifier.",
                    "label": 0
                },
                {
                    "sent": "For example, you can get pretty close to 90% accuracy in that one out of 20 predictions, so it's pretty reasonable.",
                    "label": 0
                },
                {
                    "sent": "But that's taking 10,000 labeled examples.",
                    "label": 0
                },
                {
                    "sent": "And that costs a lot of time to produce.",
                    "label": 0
                },
                {
                    "sent": "And also you can see the curve is kind of still going up, so we'd like to know what if we had, let's say, a billion more unlabeled examples, could we get that even higher?",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned, we're going to talk about 2 problems settings where.",
                    "label": 0
                },
                {
                    "sent": "People have found that indeed we could use unlabeled data to improve.",
                    "label": 0
                },
                {
                    "sent": "Learning and the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of those, is this kind of naive Bayes classifier for text document classification and so one way to think about this problem is.",
                    "label": 0
                },
                {
                    "sent": "I may have some documents for which I have labels.",
                    "label": 1
                },
                {
                    "sent": "Suppose I'm trying to again learn this function from.",
                    "label": 0
                },
                {
                    "sent": "X to Y.",
                    "label": 0
                },
                {
                    "sent": "And here's my training data.",
                    "label": 0
                },
                {
                    "sent": "And notice for some of these examples I have known class.",
                    "label": 0
                },
                {
                    "sent": "I have known class values and for others I don't.",
                    "label": 0
                },
                {
                    "sent": "These are my unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "They have the question marks there.",
                    "label": 0
                },
                {
                    "sent": "So the generic.",
                    "label": 1
                },
                {
                    "sent": "Issue here is how would we modify our naive Bayes classifier if we had data like this?",
                    "label": 0
                },
                {
                    "sent": "And of course if we ended the first hour of today, you would have said, well the thing to do here is to use these three training examples and ignore these other two.",
                    "label": 0
                },
                {
                    "sent": "But in fact you don't need to, and so I don't know who has the solution.",
                    "label": 0
                },
                {
                    "sent": "Who hasn't seen this before?",
                    "label": 0
                },
                {
                    "sent": "I'm not letting changing hands.",
                    "label": 0
                },
                {
                    "sent": "Anybody have an idea what you do?",
                    "label": 0
                },
                {
                    "sent": "OK well.",
                    "label": 0
                },
                {
                    "sent": "Simple as it may sound.",
                    "label": 0
                },
                {
                    "sent": "It's surprisingly effective to do the following very simple thing.",
                    "label": 0
                },
                {
                    "sent": "Let's train a naive Bayes classifier using the label data.",
                    "label": 0
                },
                {
                    "sent": "Those three.",
                    "label": 0
                },
                {
                    "sent": "And then let's use it to predict the values for these other two examples for which we don't have labels.",
                    "label": 0
                },
                {
                    "sent": "And then let's train a second naive Bayes classifier.",
                    "label": 0
                },
                {
                    "sent": "That uses all five of these examples.",
                    "label": 0
                },
                {
                    "sent": "But this time of course member Naive Bayes classifier which is predicting this label will actually predict.",
                    "label": 0
                },
                {
                    "sent": "A probability distribution, so it might say, well, the probability that y = 1.",
                    "label": 0
                },
                {
                    "sent": "Given that this is example 5.",
                    "label": 0
                },
                {
                    "sent": "Is .7 in the probability that it's 0?",
                    "label": 0
                },
                {
                    "sent": "Might be .3.",
                    "label": 0
                },
                {
                    "sent": "So when we train our naive Bayes classifier on.",
                    "label": 1
                },
                {
                    "sent": "These labeled data and then apply it to the unlabeled data will get these probabilistic.",
                    "label": 0
                },
                {
                    "sent": "Predictions?",
                    "label": 0
                },
                {
                    "sent": "And so then, on the second iteration, what we do is we train a second naive Bayes classifier.",
                    "label": 1
                },
                {
                    "sent": "But now we use these probabilistic labels.",
                    "label": 0
                },
                {
                    "sent": "For the examples for which we had no known labels, and we use the certain labels that we had for the original lines.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, we'll get a different naive Bayes classifier, and we can iterate that procedure.",
                    "label": 0
                },
                {
                    "sent": "OK, now it does sound a little odd because it's a little bit like.",
                    "label": 0
                },
                {
                    "sent": "Hallucinate ING your own world and then believing it and then hallucinating more deeply and sort of running off into some self reinforcement reinforcement set of assumptions.",
                    "label": 0
                },
                {
                    "sent": "So it's a strange idea, but in fact.",
                    "label": 0
                },
                {
                    "sent": "There are cases where you can show that this is a good idea, and then empirically it turns out that with many document classification problems, indeed it is a good idea.",
                    "label": 0
                },
                {
                    "sent": "So this is a process called.",
                    "label": 0
                },
                {
                    "sent": "It's actually an EM algorithm where M stands for expectation maximization.",
                    "label": 0
                },
                {
                    "sent": "Some of you have seen EM algorithms before, but what they generally involve is having a set of variables.",
                    "label": 0
                },
                {
                    "sent": "In our case, these five variables and we're trying to learn some probability distribution over them.",
                    "label": 0
                },
                {
                    "sent": "This friggs this, by the way, is just a Bayes net that corresponds to the naive Bayes assumptions.",
                    "label": 0
                },
                {
                    "sent": "Aninion what we generally do is.",
                    "label": 0
                },
                {
                    "sent": "We iterate on a 2 step process where in the first step we actually usually make this the first step.",
                    "label": 0
                },
                {
                    "sent": "We assign probability distributions to the unlabeled variables.",
                    "label": 0
                },
                {
                    "sent": "In our case, that means the variable Y, the label itself, and then on the second step of the EM process we re estimate the parameters in our probability distribution.",
                    "label": 0
                },
                {
                    "sent": "In this case that's the parameters of the Naive Bayes classifier and we iterate this process.",
                    "label": 0
                },
                {
                    "sent": "And that process in general is guaranteed.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Converge toward parameter estimates for our Naive Bayes classifier, they have the following nice property.",
                    "label": 0
                },
                {
                    "sent": "They are they locali maximize the likelihood of the data.",
                    "label": 0
                },
                {
                    "sent": "And when I say locali, I mean if you were to wiggle those parameters in any.",
                    "label": 0
                },
                {
                    "sent": "Particular small way you'd find the likelihood of the data decrease likelihood of the data simply means the probability of the data that we have.",
                    "label": 0
                },
                {
                    "sent": "According to the learned probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "And so that's a nice property.",
                    "label": 0
                },
                {
                    "sent": "It converges to a set of parameter estimates that maximize the probability of the data.",
                    "label": 0
                },
                {
                    "sent": "A little bit more technically, emanci Mises, the expected data likelihood where the expectation is taken over the distribution.",
                    "label": 0
                },
                {
                    "sent": "On these unknown variables.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is a simple procedure.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, we can try this.",
                    "label": 0
                },
                {
                    "sent": "On some real data, which is what Carmel Nigam who was here just moments ago.",
                    "label": 0
                },
                {
                    "sent": "Giving a talk he was your second speaker today.",
                    "label": 0
                },
                {
                    "sent": "In fact, his PhD thesis was on this very topic and here is from his work the algorithm and this is just what we said it would be.",
                    "label": 0
                },
                {
                    "sent": "That is, given a set of labeled documents and some other unlabeled documents.",
                    "label": 0
                },
                {
                    "sent": "The first step is to build an initial classifier.",
                    "label": 0
                },
                {
                    "sent": "From the labeled documents only.",
                    "label": 0
                },
                {
                    "sent": "Then iterate.",
                    "label": 0
                },
                {
                    "sent": "This EM process where on the East step we use the current trained classifier.",
                    "label": 0
                },
                {
                    "sent": "We're going to stay to hear to indicate the set of parameters that we've learned for the Naive Bayes classifier to estimate the membership of each unlabeled document in each class.",
                    "label": 0
                },
                {
                    "sent": "That is the probability.",
                    "label": 0
                },
                {
                    "sent": "That the document will.",
                    "label": 0
                },
                {
                    "sent": "I belong to that class, so we're signing probabilistic labels to each document and then on the M Step re estimate the classifier, that is, retrain it using those probabilistically applied labels.",
                    "label": 0
                },
                {
                    "sent": "OK, makes sense.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we do that.",
                    "label": 0
                },
                {
                    "sent": "How long will we do it until the Theta stop changing from one iteration to the next or the change becomes very small?",
                    "label": 0
                },
                {
                    "sent": "And that we know that's going to happen eventually, because this procedure is guaranteed to increase the.",
                    "label": 0
                },
                {
                    "sent": "Expected data likelihood on each iteration, so eventually, assuming the step size is large enough, eventually it's going to reach a local optimum, and at that point it will stop changing and then will be done.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let's look at what happens in practice if we try this.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to skip over some equations, but.",
                    "label": 0
                },
                {
                    "sent": "If you're interested, you can look at the paper the citation to the paper is in your notes, and here are the equations.",
                    "label": 0
                },
                {
                    "sent": "But they basically are just saying what what I said is an equation for getting the priors on the class label Y. I'm sorry, not the priors, but this is the probability of the class being the Y value being a particular class given the document I.",
                    "label": 0
                },
                {
                    "sent": "And just like you talked already about Naive Bayes that involves looking at the individual words W. And the probability of those words showing up conditioned on the class label and then in the M step we retrain.",
                    "label": 0
                },
                {
                    "sent": "The two main parameters from the Naive Bayes classifier, that is, the probability distribution on each word.",
                    "label": 0
                },
                {
                    "sent": "Given the class and the prior probabilities of the classes.",
                    "label": 0
                },
                {
                    "sent": "And I think you covered that already right?",
                    "label": 0
                },
                {
                    "sent": "And the previous.",
                    "label": 0
                },
                {
                    "sent": "So the equations for especially this.",
                    "label": 0
                },
                {
                    "sent": "And step are a little bit more.",
                    "label": 0
                },
                {
                    "sent": "Complicated, then the first version of Naive Bayes you saw WHI because now we have to modify this to deal with probabilistic labels.",
                    "label": 0
                },
                {
                    "sent": "The labels aren't just.",
                    "label": 0
                },
                {
                    "sent": "Yes or no, spam or not.",
                    "label": 0
                },
                {
                    "sent": "Instead we get these probabilistic labels .8 spam .29.",
                    "label": 0
                },
                {
                    "sent": "But other than that, it's.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's very straightforward, maybe more interesting Lee, given the amount of time we have, we can look at what really happens in the classifier from one iteration to the next.",
                    "label": 0
                },
                {
                    "sent": "So this will show.",
                    "label": 0
                },
                {
                    "sent": "This shows some results of running the CN process on a set of data, which is web pages.",
                    "label": 0
                },
                {
                    "sent": "These web pages were being classified either as.",
                    "label": 0
                },
                {
                    "sent": "Web pages home pages of academic course.",
                    "label": 0
                },
                {
                    "sent": "Or not.",
                    "label": 0
                },
                {
                    "sent": "And in this particular set of data that I remember, there are very few.",
                    "label": 0
                },
                {
                    "sent": "Oh, here it is 1 labeled example per class.",
                    "label": 1
                },
                {
                    "sent": "So there is one example, just one document, which is the course homepage.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I recall there were many, but I don't remember how many unlabeled documents, but was on the order of 1000.",
                    "label": 0
                },
                {
                    "sent": "So now what do we do with the analysis?",
                    "label": 0
                },
                {
                    "sent": "We apply the CN procedure.",
                    "label": 0
                },
                {
                    "sent": "What we do is what's the first step we take that one.",
                    "label": 0
                },
                {
                    "sent": "Labeled document that we have, we trained a naive Bayes classifier on it.",
                    "label": 0
                },
                {
                    "sent": "And doing that, what we're going to do is we're going to learn these parameters of the classifier.",
                    "label": 0
                },
                {
                    "sent": "That is, what's the probability that the word shoe will occur, given that it is a course page, what's the probability that the word shoe will occur, given that it's not a course page?",
                    "label": 0
                },
                {
                    "sent": "And similarly for the other 50,000 words of English?",
                    "label": 0
                },
                {
                    "sent": "OK, now having trained at.",
                    "label": 0
                },
                {
                    "sent": "Just on their first iteration, suppose we stop and now we have the program print out the top 20 words.",
                    "label": 0
                },
                {
                    "sent": "That his learned are the most.",
                    "label": 0
                },
                {
                    "sent": "Are the best discriminators.",
                    "label": 0
                },
                {
                    "sent": "Of course, pages from not course pages?",
                    "label": 0
                },
                {
                    "sent": "That is, we could do is we could print out each.",
                    "label": 0
                },
                {
                    "sent": "We could rank order the words by the probability, the odds ratio, the probability of the word will occur given that it's part of the course page divided by the probability of the word occurring, given that it's not a course page.",
                    "label": 0
                },
                {
                    "sent": "And if we do that, these are the words we get on the left.",
                    "label": 0
                },
                {
                    "sent": "So now if you look at those words you see you can probably guess what that single training example course page was about.",
                    "label": 0
                },
                {
                    "sent": "This clearly was about an artificial intelligence class.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "I happen to know that our US is the name of Daniela Roos at Dartmouth who is teaching the course at the time so you can see very idiosyncratic words showing up there, which is exactly what you would expect if you had one labeled example.",
                    "label": 0
                },
                {
                    "sent": "OK. Now let's take the first step of the EM process.",
                    "label": 0
                },
                {
                    "sent": "We now take that label, that classifier that produce.",
                    "label": 0
                },
                {
                    "sent": "You know this based on these is the most significant words.",
                    "label": 0
                },
                {
                    "sent": "And of course there are other thousands of words down below here that we're not looking at.",
                    "label": 0
                },
                {
                    "sent": "And now we apply it and we label all the unlabeled pages.",
                    "label": 0
                },
                {
                    "sent": "Then we take those probabilistic labels and retrain.",
                    "label": 0
                },
                {
                    "sent": "Now let's print out again.",
                    "label": 0
                },
                {
                    "sent": "The top words.",
                    "label": 0
                },
                {
                    "sent": "This time we get this list, which is actually quite interesting here, by the way, the D stands for digit is just the way of encoding so that 13 and 72 become the same DD token.",
                    "label": 0
                },
                {
                    "sent": "Now what you see is after just one iteration of this M process, danielo ruse is gone, Dartmouth is gone.",
                    "label": 0
                },
                {
                    "sent": "And the top words are now things that you can recognize.",
                    "label": 0
                },
                {
                    "sent": "This more generically associated with the course.",
                    "label": 0
                },
                {
                    "sent": "Like course numbers, words like lecture handout, do problem set.",
                    "label": 0
                },
                {
                    "sent": "You still do get some.",
                    "label": 0
                },
                {
                    "sent": "Individual names that aren't very generic.",
                    "label": 0
                },
                {
                    "sent": "If we do that one more step, then we get this set of words.",
                    "label": 0
                },
                {
                    "sent": "So you can see what's happening is in just a couple iterations we suddenly get very representative words about courses showing up emerging from this strange process.",
                    "label": 0
                },
                {
                    "sent": "So in practice it's sometimes quite.",
                    "label": 0
                },
                {
                    "sent": "Effective as it is here.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in fact, if we continue that process.",
                    "label": 0
                },
                {
                    "sent": "Until it converges ATM process letting it run, a number of iterations.",
                    "label": 0
                },
                {
                    "sent": "By the way, in these experiments it only took about a dozen iterations to converge.",
                    "label": 0
                },
                {
                    "sent": "It was rare that it would take more than 30.",
                    "label": 0
                },
                {
                    "sent": "Usually it was somewhere between 10:00 or closer to 10 than 30.",
                    "label": 0
                },
                {
                    "sent": "And then if we test that the accuracy of that final classifier which was trained on both labeled and unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Here's a plot of the accuracy that we get.",
                    "label": 0
                },
                {
                    "sent": "For the.",
                    "label": 0
                },
                {
                    "sent": "EM based classifier.",
                    "label": 0
                },
                {
                    "sent": "This is the one that's using labeled and unlabeled.",
                    "label": 0
                },
                {
                    "sent": "As a function of the number of labeled documents that we gave it.",
                    "label": 0
                },
                {
                    "sent": "And this curve below it is the accuracy that we get if we train our usual supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Only naive Bayes classifier on the same labeled data set.",
                    "label": 0
                },
                {
                    "sent": "So what you see is that the accuracy is substantially higher.",
                    "label": 0
                },
                {
                    "sent": "It's not perfect, but this is essentially the difference between labeled only.",
                    "label": 0
                },
                {
                    "sent": "Naive Bayes versus this EM process that iteratively retrains the Naive Bayes classifier using labeled and unlabeled data, and so this difference is the value of the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Now here's an interesting phenomena that you see.",
                    "label": 0
                },
                {
                    "sent": "You get a big benefit from unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "If you have only 20 labeled documents.",
                    "label": 0
                },
                {
                    "sent": "Remember there were twenty groups, so 20.",
                    "label": 0
                },
                {
                    "sent": "This is the newsgroup data by the way.",
                    "label": 0
                },
                {
                    "sent": "So in this case we had one example for each of the one labeled example for each of the 20 newsgroups.",
                    "label": 0
                },
                {
                    "sent": "As the number of labeled documents increases, of course the accuracy improves for both classifiers, but the leverage that we get from the unlabeled data decreases.",
                    "label": 0
                },
                {
                    "sent": "Why would that be?",
                    "label": 0
                },
                {
                    "sent": "One way to see that we ought to expect that is to think of what the curve would look like over there.",
                    "label": 0
                },
                {
                    "sent": "Asymptotically, if I increase the number of labeled data to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to go ahead.",
                    "label": 0
                },
                {
                    "sent": "Ah, that's right, there actually wasn't what I was thinking, but that's good.",
                    "label": 0
                },
                {
                    "sent": "You're right that if we're not increasing the unlabeled data, get swamped out by the labeled data.",
                    "label": 0
                },
                {
                    "sent": "That's true.",
                    "label": 0
                },
                {
                    "sent": "I was thinking was a slightly different explanation, although I don't disagree with what you said.",
                    "label": 0
                },
                {
                    "sent": "Which is that if I have infinite labeled data, I get perfect estimates of my naive Bayes parameters.",
                    "label": 0
                },
                {
                    "sent": "It's having infinite labeled data.",
                    "label": 0
                },
                {
                    "sent": "Is just like somebody telling me, oh, here are the exact probabilities for each word conditioned on the class.",
                    "label": 0
                },
                {
                    "sent": "And so if I have infinite labeled data, I've already got the perfect the best optimal parameter estimates I can get.",
                    "label": 0
                },
                {
                    "sent": "And of course having additional information won't make me any better than that.",
                    "label": 0
                },
                {
                    "sent": "But when labeled data is sparse, then you can see there is a significant benefit.",
                    "label": 0
                },
                {
                    "sent": "Alright, so there is meth.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At 1:00",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I said that we would.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look at two methods, but before we leave method one have a couple of questions for you.",
                    "label": 0
                },
                {
                    "sent": "So I showed you a couple of examples where it works quite well.",
                    "label": 0
                },
                {
                    "sent": "Training that course classifier.",
                    "label": 0
                },
                {
                    "sent": "Training the.",
                    "label": 0
                },
                {
                    "sent": "Newsgroup Document classifier works pretty well.",
                    "label": 0
                },
                {
                    "sent": "In fact, the truth is.",
                    "label": 0
                },
                {
                    "sent": ", had a hard time finding a set of documents where it did not work.",
                    "label": 0
                },
                {
                    "sent": "Very did not work well.",
                    "label": 0
                },
                {
                    "sent": "So it did seem like this procedure.",
                    "label": 0
                },
                {
                    "sent": "Applied to test documents works quite well and there might be something about text.",
                    "label": 0
                },
                {
                    "sent": "That makes that the case.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So let's spend a minute just thinking about what would be the best case in the worst case.",
                    "label": 1
                },
                {
                    "sent": "So I think the easiest way I know to think about this is to imagine that we instead of having.",
                    "label": 0
                },
                {
                    "sent": "A bag of 50,000 different words and 50,000 features.",
                    "label": 0
                },
                {
                    "sent": "Imagine we just had one feature.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we only look at.",
                    "label": 0
                },
                {
                    "sent": "We're trying to tell if it's spam.",
                    "label": 0
                },
                {
                    "sent": "We only look for the word Nigeria.",
                    "label": 0
                },
                {
                    "sent": "No other word.",
                    "label": 0
                },
                {
                    "sent": "We count how many times Nigeria occurs in the email.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's that could be our feature.",
                    "label": 0
                },
                {
                    "sent": "Number of Nigeria occurrences.",
                    "label": 0
                },
                {
                    "sent": "And now suppose that I look just among positive examples of spam.",
                    "label": 0
                },
                {
                    "sent": "And I drew a histogram or.",
                    "label": 0
                },
                {
                    "sent": "The probability that a randomly drawn word will be.",
                    "label": 0
                },
                {
                    "sent": "Will be in Nigeria.",
                    "label": 0
                },
                {
                    "sent": "Given that, well, let's try it.",
                    "label": 0
                },
                {
                    "sent": "Given that it is spam.",
                    "label": 0
                },
                {
                    "sent": "So I might have some kind of distribution like this.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "Spam equals yes.",
                    "label": 0
                },
                {
                    "sent": "And it might have a different distribution, maybe fewer occurrences of the word, for example.",
                    "label": 0
                },
                {
                    "sent": "In the case where spam equals no.",
                    "label": 0
                },
                {
                    "sent": "And if that's the case.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "If that's the case.",
                    "label": 0
                },
                {
                    "sent": "Then the naive Bayes modeling assumption.",
                    "label": 0
                },
                {
                    "sent": "Is going to make this kind of unlabeled data be wonderful for us?",
                    "label": 0
                },
                {
                    "sent": "Because, and I don't know whether you discuss this particular fact about Naive Bayes when you discuss it the first time.",
                    "label": 0
                },
                {
                    "sent": "So you guys were here, you gotta help me.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "I just trained in Naive Bayes classifier with no labels at all.",
                    "label": 0
                },
                {
                    "sent": "Suppose I run the TM process, but I have zero initial labels and I just make up random parameters to get started and I have those random parameters, sign labels and retrain and re estimate and reach.",
                    "label": 0
                },
                {
                    "sent": "You know, just go back and forth ATM process.",
                    "label": 0
                },
                {
                    "sent": "That's a very good and very commonly used clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "What it tends to do is find.",
                    "label": 0
                },
                {
                    "sent": "The two finds two groups of data that are easily distinguished.",
                    "label": 0
                },
                {
                    "sent": "It's called mixture model clustering.",
                    "label": 0
                },
                {
                    "sent": "It's a very common method of doing clustering.",
                    "label": 0
                },
                {
                    "sent": "If you want to document clustering, that's a great algorithm for doing it.",
                    "label": 0
                },
                {
                    "sent": "Well, if we have data like this and we were to run that clustering algorithm with zero labeled data.",
                    "label": 0
                },
                {
                    "sent": "We would expect to find these two clusters.",
                    "label": 0
                },
                {
                    "sent": "They would just kind of happen.",
                    "label": 0
                },
                {
                    "sent": "Especially, we only had this one feature.",
                    "label": 0
                },
                {
                    "sent": "So the data is very nicely clustered even with zero labels into these two clusters.",
                    "label": 0
                },
                {
                    "sent": "And now suppose I have O2 labels, one positive and one negative.",
                    "label": 0
                },
                {
                    "sent": "Like maybe I have a document right over here which is positive in one right over here, which is negative.",
                    "label": 0
                },
                {
                    "sent": "Then it's very easy once we know what the clusters are to say.",
                    "label": 0
                },
                {
                    "sent": "Oh, this cluster is negative.",
                    "label": 0
                },
                {
                    "sent": "This cluster is positive, and that's essentially with the EM process is going to do.",
                    "label": 0
                },
                {
                    "sent": "Alright, so as long as we have a data set which would be clustered anyway into groups that happen to correspond to the classes we're interested in classifying.",
                    "label": 0
                },
                {
                    "sent": "Like in this case, if this cluster is into spam and not spam, we're Golden.",
                    "label": 0
                },
                {
                    "sent": "OK, however, that same data set.",
                    "label": 0
                },
                {
                    "sent": "If I were interested in some other classification function like.",
                    "label": 0
                },
                {
                    "sent": "Was this authored on Tuesday or Wednesday?",
                    "label": 0
                },
                {
                    "sent": "Then the clusters I'm going to get out of course will still be these two clusters that if I give unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "But that those two clusters are simply probably irrelevant to the classification of whether it was Tuesday or Wednesday.",
                    "label": 0
                },
                {
                    "sent": "Alright, so my point is this.",
                    "label": 0
                },
                {
                    "sent": "This kind of use of unlabeled data can be very helpful.",
                    "label": 0
                },
                {
                    "sent": "And you can kind of see with the Nigeria spam example why.",
                    "label": 0
                },
                {
                    "sent": "Right, if I just have this one word, the clusters that will come out from the unlabeled data will be just the grouping that I want, and the only thing that I'll need labeled data for is to label which cluster is positive and which is negative, and I don't need many labels to do that.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if the naturally occurring clusters in the data set don't happen to correspond to the labels that I'm interested in.",
                    "label": 0
                },
                {
                    "sent": "Assigning by my classifier.",
                    "label": 0
                },
                {
                    "sent": "Then this unlabeled stuff and the M stuff will be downright misleading.",
                    "label": 0
                },
                {
                    "sent": "So to oversimplify a little bit.",
                    "label": 0
                },
                {
                    "sent": "That's actually the structure of the data that this.",
                    "label": 0
                },
                {
                    "sent": "Method is leveraging off of.",
                    "label": 0
                },
                {
                    "sent": "And so I think 1 interesting.",
                    "label": 0
                },
                {
                    "sent": "So how can we test given some documents, that whether it's got this kind of structure?",
                    "label": 1
                },
                {
                    "sent": "I think the best test is to run unsupervised clustering on the document set.",
                    "label": 0
                },
                {
                    "sent": "Take the few labels that we have and see if the clusters that come out actually.",
                    "label": 0
                },
                {
                    "sent": "Capture.",
                    "label": 0
                },
                {
                    "sent": "Different clusters capture different labels, and if so, then we know that the naturally occurring clustering structure of that data is at least.",
                    "label": 0
                },
                {
                    "sent": "Consistent with this naive Bayes modeling assumption.",
                    "label": 0
                },
                {
                    "sent": "And things things have a chance of working.",
                    "label": 0
                },
                {
                    "sent": "In fact, the situation is a little more pleasant than I'm suggesting because.",
                    "label": 0
                },
                {
                    "sent": "I kind of made the extreme argument that well, what if we have zero labels and we just do clustering, then we better see that the cluster is aligned with the classes.",
                    "label": 0
                },
                {
                    "sent": "It's actually a little better than that, because as you recall, the EM process actually begins with some labels, not with zero labels, and so the.",
                    "label": 0
                },
                {
                    "sent": "Clusters that get formed initially.",
                    "label": 0
                },
                {
                    "sent": "Are initialized from the labeled examples, and so if there are several different clusterings that might come out from different random initializations of the clustering algorithm, those supervised labeled examples will assure that.",
                    "label": 0
                },
                {
                    "sent": "We at least pick one that's.",
                    "label": 0
                },
                {
                    "sent": "Pretty consistent with these labels, so then that becomes the initialization part, but I think you get the intuition here, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so any comments, suggestions, clarifications on this?",
                    "label": 0
                },
                {
                    "sent": "How's this match up to?",
                    "label": 0
                },
                {
                    "sent": "Public servants in mobile command upgrade as anything.",
                    "label": 0
                },
                {
                    "sent": "Wow.",
                    "label": 0
                },
                {
                    "sent": "I don't I.",
                    "label": 0
                },
                {
                    "sent": "Short answer is I don't think anybody yet knows how the brain works, but there is quite a. Chipping away at it.",
                    "label": 0
                },
                {
                    "sent": "The question was how does this correspond with what we might know about what happens in the brain?",
                    "label": 0
                },
                {
                    "sent": "Of course, we don't know what happens exactly in the brain, but it is interesting that there's a.",
                    "label": 0
                },
                {
                    "sent": "Of.",
                    "label": 0
                },
                {
                    "sent": "Pretty active.",
                    "label": 0
                },
                {
                    "sent": "Sort of research group in.",
                    "label": 0
                },
                {
                    "sent": "I. Cognitive science and also in developmental psychology.",
                    "label": 0
                },
                {
                    "sent": "That's.",
                    "label": 0
                },
                {
                    "sent": "Very interested in how is it that humans can learn so much.",
                    "label": 0
                },
                {
                    "sent": "So rapidly is children from apparently not mostly unlabeled example.",
                    "label": 0
                },
                {
                    "sent": "Mostly unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So it is a question that is.",
                    "label": 0
                },
                {
                    "sent": "Considerable interest there.",
                    "label": 0
                },
                {
                    "sent": "My own hunch is that, well, I don't know if this CM stuff has any is an appropriate model for that or not.",
                    "label": 1
                },
                {
                    "sent": "My guess is that the second method we're going to talk about is a much more appropriate model for.",
                    "label": 0
                },
                {
                    "sent": "How humans get away with using unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Well, the grass.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "His eyes were number of people talking, so I'm wondering is it is the natural limit to this can still be cause something just generally hard to classify even for humans.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think there is.",
                    "label": 0
                },
                {
                    "sent": "It does top out somewhere below 100% for a couple reasons.",
                    "label": 0
                },
                {
                    "sent": "One is that in some senses the classes are inherently ambiguous and so for example even something apparently obvious like spam.",
                    "label": 0
                },
                {
                    "sent": "What's spam to you might not overlap perfectly with what spam to me, although probably it overlaps 90% I would guess.",
                    "label": 0
                },
                {
                    "sent": "But even if we had two independent people label spam documents, they don't agree completely.",
                    "label": 0
                },
                {
                    "sent": "So there is.",
                    "label": 0
                },
                {
                    "sent": "That kind of ambiguity in the labels to begin with, but a second is that there are some really severe assumptions in the naive Bayes model for text classification, right?",
                    "label": 0
                },
                {
                    "sent": "It's a bag of words model, so it's like saying the sentence machine learning is great is the same as saying the sentence.",
                    "label": 0
                },
                {
                    "sent": "Learning great is machine.",
                    "label": 0
                },
                {
                    "sent": "And to people they don't seem like I'm saying the same thing, but to this program they do.",
                    "label": 0
                },
                {
                    "sent": "This is jumbling them altogether and ignoring.",
                    "label": 0
                },
                {
                    "sent": "The content so there are some modeling problems here too that hurt.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's move on to the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2nd.",
                    "label": 0
                },
                {
                    "sent": "So I thought hard about how to use my limited 60 minutes here and what you should know is that there's actually quite a bit.",
                    "label": 0
                },
                {
                    "sent": "This is a fairly hot topic.",
                    "label": 0
                },
                {
                    "sent": "In statistical language analysis right now, how do we use unlabeled data to improve on our learning?",
                    "label": 0
                },
                {
                    "sent": "Um, in 60 minutes.",
                    "label": 0
                },
                {
                    "sent": "I thought the most useful thing we could do was cover what seemed to me to be the two key types of problems structure that people are leveraging with these algorithms.",
                    "label": 0
                },
                {
                    "sent": "And so I can show you a lot more algorithms for leveraging.",
                    "label": 0
                },
                {
                    "sent": "That first type of information, the first type of problem structure.",
                    "label": 0
                },
                {
                    "sent": "But I'm not going to 'cause I think that the most important thing to get out of this talk is from that first part is oh, I see if documents naturally cluster into groups based on the words that occur in them.",
                    "label": 0
                },
                {
                    "sent": "And if those groups that they naturally cluster into happened to align with the class labels that I want my classifier to assign, then it's probably a good idea for me to be using unlabeled data to augment my classifier.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, I could probably test for that by running an unsupervised clustering algorithm and then seeing to what degree those natural clusters do align with class labels.",
                    "label": 0
                },
                {
                    "sent": "And if you get that idea, then I'm happy and you could probably yourself.",
                    "label": 0
                },
                {
                    "sent": "Go off and think of better algorithms than the one that we just talked about.",
                    "label": 0
                },
                {
                    "sent": "But when we talked about is pretty easy to grok and it follows right on Naive Bayes classifier.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We just talked about.",
                    "label": 0
                },
                {
                    "sent": "So in that same spirit, let's look at a very different problem structure that also justifies the use of unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "And here, let me illustrate this just with an example.",
                    "label": 0
                },
                {
                    "sent": "On the example, is classifying web pages.",
                    "label": 0
                },
                {
                    "sent": "And I would say I would assert an.",
                    "label": 0
                },
                {
                    "sent": "I hope you'll agree with me in 10 minutes that the problem of classifying webpages happens to have a wonderful structure that lets us use unlabeled data to boost the classification accuracy and the structure that it has is very different from that M naturally occurring clusters of documents structure that we just saw.",
                    "label": 0
                },
                {
                    "sent": "If you ask me for a name for this structure, that webpage classification has, I'd say the key thing that it has is.",
                    "label": 0
                },
                {
                    "sent": "It's a classification problem where the features that describe the examples are redundantly predictive.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean by that?",
                    "label": 0
                },
                {
                    "sent": "Suppose you had to classify this web page that you were looking at as either a faculty homepage or a course homepage.",
                    "label": 0
                },
                {
                    "sent": "As a human, I think you could do that.",
                    "label": 0
                },
                {
                    "sent": "OK, and by the way, these little features up here in the corner.",
                    "label": 0
                },
                {
                    "sent": "These are the hyperlinks that are pointing to that web page.",
                    "label": 0
                },
                {
                    "sent": "So somewhere else there is a web page that contains a hyperlink saying my advisor and that's pointing to this page.",
                    "label": 0
                },
                {
                    "sent": "So I can think of the features that describe this example as containing both words on the page and those words on the hyperlinks that point to the page.",
                    "label": 0
                },
                {
                    "sent": "Now when I say that this.",
                    "label": 0
                },
                {
                    "sent": "Problem has this nice structure that the features are redundantly predictive.",
                    "label": 1
                },
                {
                    "sent": "What I mean precisely is this.",
                    "label": 0
                },
                {
                    "sent": "If I ask you to classify this web page, you can still do it.",
                    "label": 0
                },
                {
                    "sent": "If I only show you this subset of the features.",
                    "label": 0
                },
                {
                    "sent": "Pretty obvious that's a faculty homepage.",
                    "label": 0
                },
                {
                    "sent": "So you can do it just with this subset of the features.",
                    "label": 0
                },
                {
                    "sent": "Similarly, if I only show you those, you can also do it.",
                    "label": 0
                },
                {
                    "sent": "So the property that this has, assuming that you were able to classify those two examples, is that if you know the truth, if you know the classification function.",
                    "label": 0
                },
                {
                    "sent": "Once you've learned it, then it suffices to see either.",
                    "label": 0
                },
                {
                    "sent": "The first feature set or the second feature set either one is sufficient.",
                    "label": 0
                },
                {
                    "sent": "To do the classification, once you know the function.",
                    "label": 0
                },
                {
                    "sent": "And that's the structure that web page classification.",
                    "label": 0
                },
                {
                    "sent": "Almost has.",
                    "label": 0
                },
                {
                    "sent": "That's the structure that we need to have, and that if we have, we can prove actually, that unlabeled data will be useful for us.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see how.",
                    "label": 0
                },
                {
                    "sent": "Why should that be relevant?",
                    "label": 0
                },
                {
                    "sent": "It's actually a very simple story.",
                    "label": 0
                },
                {
                    "sent": "Once you see it, here's why should be relevant.",
                    "label": 0
                },
                {
                    "sent": "So what you just agreed with me, I think, is that if you knew the classification function.",
                    "label": 0
                },
                {
                    "sent": "You could classify that webpage either by looking at this set of features or this other set of features.",
                    "label": 0
                },
                {
                    "sent": "And so they are redundantly predictive of the class label.",
                    "label": 0
                },
                {
                    "sent": "And so we can develop an algorithm that uses labeled and unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "In the following way, instead of training one classifier, will train 2.",
                    "label": 0
                },
                {
                    "sent": "One of them will get this feature set.",
                    "label": 0
                },
                {
                    "sent": "One of them will get this feature set.",
                    "label": 0
                },
                {
                    "sent": "And then the two criteria that we're going to use.",
                    "label": 0
                },
                {
                    "sent": "To estimate the parameters of our classifier to train the classifier, our first of course that both of these two classifiers should be trained so that they correctly classified whatever labeled examples we have.",
                    "label": 0
                },
                {
                    "sent": "That's the easy part, but then actually, if you think about it, if I have an unlabeled example, suppose this is an unlabeled example.",
                    "label": 0
                },
                {
                    "sent": "Then the program doesn't know whether that's a faculty homepage or a course homepage, but it does know.",
                    "label": 0
                },
                {
                    "sent": "That whatever classifier one gives us the answer ought to agree.",
                    "label": 0
                },
                {
                    "sent": "With classifier too.",
                    "label": 0
                },
                {
                    "sent": "Right and so, for example, the fees are to naive Bayes classifiers.",
                    "label": 0
                },
                {
                    "sent": "Or two logistic regression classifiers or two?",
                    "label": 0
                },
                {
                    "sent": "Your favorite classifier to support vector machines.",
                    "label": 0
                },
                {
                    "sent": "Then each labeled example is a constraint on the parameters of the classifier.",
                    "label": 0
                },
                {
                    "sent": "Of each of the classifiers independently, but each unlabeled example is a wonderful constraint on the joint choice of parameters in the two classifiers.",
                    "label": 0
                },
                {
                    "sent": "And the more of these unlabeled examples we can show to the system, the more constraints were piling up on the joint choice of the parameters for classifier one, Classifier 2.",
                    "label": 0
                },
                {
                    "sent": "That's the simple idea.",
                    "label": 0
                },
                {
                    "sent": "It's actually pretty straightforward.",
                    "label": 0
                },
                {
                    "sent": "But of course we can only do this if.",
                    "label": 0
                },
                {
                    "sent": "We really happen to have a learning problem where we can divide the features into one set and another set, both of which are sufficient to get the right answer, and in that case we'll be in good shape.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That problem setting is called Co training.",
                    "label": 0
                },
                {
                    "sent": "You can think of a lot of algorithms for leveraging off that problem structure, and in fact I guess the first one or one of the first ones was.",
                    "label": 0
                },
                {
                    "sent": "Was this very simple classified?",
                    "label": 0
                },
                {
                    "sent": "This very simple Co training algorithm that says given some labeled data L and unlabeled data you will train.",
                    "label": 1
                },
                {
                    "sent": "A hyperlink classifier Anna Page words on the page classifier.",
                    "label": 0
                },
                {
                    "sent": "And then we'll use only the labeled data for that, by the way.",
                    "label": 0
                },
                {
                    "sent": "Then we'll allow both of those classifiers to go through the unlabeled data and pick just the small number of highly confident.",
                    "label": 0
                },
                {
                    "sent": "Examples where it's highly confident of its prediction.",
                    "label": 0
                },
                {
                    "sent": "And then these will get added to the self labeled examples and will iterate this process.",
                    "label": 0
                },
                {
                    "sent": "And intuitively, the reason this works well is that.",
                    "label": 0
                },
                {
                    "sent": "Suppose this is our.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of our first examples.",
                    "label": 0
                },
                {
                    "sent": "We have, let's say only 100 million unlabeled webpages.",
                    "label": 0
                },
                {
                    "sent": "Then even after this one example are hyperlinked classifier could become very confident that if it sees the hyperlink that says exactly my advisor.",
                    "label": 0
                },
                {
                    "sent": "Then it's going to want to label the page that it points to as a faculty page.",
                    "label": 0
                },
                {
                    "sent": "And now when we search through the unlabeled data, if the system happens to find one other my advisor link.",
                    "label": 1
                },
                {
                    "sent": "Then that link is probably going to point to a webpage that looks nothing like this.",
                    "label": 0
                },
                {
                    "sent": "And will be a web page where the word base where the page classifier will be very uncertain.",
                    "label": 0
                },
                {
                    "sent": "So part of it works in our favor here is that.",
                    "label": 0
                },
                {
                    "sent": "The hyperlinks that we've seen before.",
                    "label": 0
                },
                {
                    "sent": "In the labeled set happened to Co occur with web pages that we haven't seen before, and vice versa.",
                    "label": 0
                },
                {
                    "sent": "And so the things that classifier A is very confident about tend to be things that class for her B is.",
                    "label": 0
                },
                {
                    "sent": "Not.",
                    "label": 0
                },
                {
                    "sent": "And so they can both help each other in this kind of bootstrapping fashion.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea of Co training and.",
                    "label": 0
                },
                {
                    "sent": "This actually work.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Books.",
                    "label": 0
                },
                {
                    "sent": "For example, here's the simple case where we had 12 labeled web Pages, 1000 unlabeled pages.",
                    "label": 1
                },
                {
                    "sent": "And as we iterate that simple code training learning algorithm, the error on held out test set classification.",
                    "label": 0
                },
                {
                    "sent": "Decreases from on the order of 10 to 15% down to five to 10%.",
                    "label": 0
                },
                {
                    "sent": "So that's the kind of practical advantage we get.",
                    "label": 0
                },
                {
                    "sent": "In fact.",
                    "label": 0
                },
                {
                    "sent": "Since then, I.",
                    "label": 0
                },
                {
                    "sent": "People have found.",
                    "label": 0
                },
                {
                    "sent": "Stronger results, but even here if you think about percentage wise improvement in the air, it's pretty substantial.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But let's spend a minute.",
                    "label": 0
                },
                {
                    "sent": "Talking a little bit more.",
                    "label": 0
                },
                {
                    "sent": "Looking in a little bit more detail at the.",
                    "label": 0
                },
                {
                    "sent": "Real structure of this problem.",
                    "label": 0
                },
                {
                    "sent": "Because I think that will let you actually see.",
                    "label": 0
                },
                {
                    "sent": "That that.",
                    "label": 0
                },
                {
                    "sent": "Even in the case of this kind of page classification, there will be problems where this works better and works worse depending on some subtleties of the structure.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's really going on there, actually?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me let me talk about it this way.",
                    "label": 0
                },
                {
                    "sent": "Here's the best way I know of explaining what's really going on with Co training.",
                    "label": 0
                },
                {
                    "sent": "So think of.",
                    "label": 0
                },
                {
                    "sent": "Are document classification problem or the web?",
                    "label": 0
                },
                {
                    "sent": "If you like as a very very large bipartite graph, bipartite graph just means there are two kinds of nodes in the connection.",
                    "label": 0
                },
                {
                    "sent": "Go from one kind to the other kind.",
                    "label": 0
                },
                {
                    "sent": "In our case, these nodes are web pages with words on them.",
                    "label": 0
                },
                {
                    "sent": "These nodes are the hyperlinks.",
                    "label": 0
                },
                {
                    "sent": "Like my advisor.",
                    "label": 0
                },
                {
                    "sent": "You could think of drawing the think of representing the web this way, or any collection of web pages that you have.",
                    "label": 0
                },
                {
                    "sent": "And will put in edge between.",
                    "label": 0
                },
                {
                    "sent": "A particular hyperlink in a particular webpage, just in case those Co occur as an example.",
                    "label": 0
                },
                {
                    "sent": "So each edge here is an example.",
                    "label": 0
                },
                {
                    "sent": "Of course, some of those edges will have labels for.",
                    "label": 0
                },
                {
                    "sent": "Somebody might tell us this is a positive example.",
                    "label": 0
                },
                {
                    "sent": "That webpage in that hyperlink pair other ones will be unlabeled.",
                    "label": 0
                },
                {
                    "sent": "Those are just the edges that have no labels on them.",
                    "label": 0
                },
                {
                    "sent": "Others will be labeled as negative, and so forth.",
                    "label": 0
                },
                {
                    "sent": "OK, so you see the notation I'm using here.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Now let's think of how we how the Co training process would work.",
                    "label": 0
                },
                {
                    "sent": "If we use the world's simplest learning algorithm, let's do.",
                    "label": 0
                },
                {
                    "sent": "We're going to Train 2 rote learners, one rote learner, for the hyperlinks.",
                    "label": 1
                },
                {
                    "sent": "One wrote learner for the web pages.",
                    "label": 0
                },
                {
                    "sent": "And what happens?",
                    "label": 0
                },
                {
                    "sent": "Well, these unlabeled edges I'll say, or my unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "And the labels that have pluses and minus on or the initially labeled subset of the data.",
                    "label": 0
                },
                {
                    "sent": "And what's going to happen?",
                    "label": 0
                },
                {
                    "sent": "So on the 1st iteration, my rote learner of pages will learn that this page is a positive example.",
                    "label": 0
                },
                {
                    "sent": "And this one is negative and this one is negative.",
                    "label": 0
                },
                {
                    "sent": "And then it gets to look for other examples that it can confidently label.",
                    "label": 0
                },
                {
                    "sent": "So here's one.",
                    "label": 0
                },
                {
                    "sent": "Here's another hyperlink.",
                    "label": 0
                },
                {
                    "sent": "Page pair, which involves this same page, so it's going to label that pair a positive example.",
                    "label": 0
                },
                {
                    "sent": "Similarly, this hyperlink labeler will label this.",
                    "label": 0
                },
                {
                    "sent": "Example is positive.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Now on the next iteration, the Rote learner will learn that this page is also positive.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "It's been told that here and then it will label this one.",
                    "label": 0
                },
                {
                    "sent": "Which will lead to the program knowing that that's positive.",
                    "label": 0
                },
                {
                    "sent": "Right, so if I were to execute this over many iterations, what I'd see was oh, the positive labels are going to propagate throughout this connected component in the Die Partite graph.",
                    "label": 0
                },
                {
                    "sent": "The negative labels will similarly propagate throughout this connected components that I have.",
                    "label": 0
                },
                {
                    "sent": "Negatives everywhere in here.",
                    "label": 0
                },
                {
                    "sent": "And the program will, if assign those labels.",
                    "label": 0
                },
                {
                    "sent": "So it's it's destined to do this.",
                    "label": 0
                },
                {
                    "sent": "If the initial data that.",
                    "label": 0
                },
                {
                    "sent": "Well, that's that's how the Co training algorithm works.",
                    "label": 0
                },
                {
                    "sent": "So now if we ask what would make a data set, what kind of structure in a data set would be a best case for Co training?",
                    "label": 0
                },
                {
                    "sent": "Would lead to a good outcome.",
                    "label": 0
                },
                {
                    "sent": "What kind of data set would lead to would be a worst case would lead to a horrible outcome.",
                    "label": 0
                },
                {
                    "sent": "What would lead to an ineffective outcome?",
                    "label": 0
                },
                {
                    "sent": "You want to be able to answer those questions and you want to be able to answer them in the form of telling me what kind of bipartite graph I could draw that would cause the algorithm to behave nicely.",
                    "label": 0
                },
                {
                    "sent": "Horribly or just not?",
                    "label": 0
                },
                {
                    "sent": "Behave any differently than if it had no one labeled data.",
                    "label": 0
                },
                {
                    "sent": "So let's take those in turn.",
                    "label": 0
                },
                {
                    "sent": "I would say if you ask what can make this work well, I'd say well, actually works pretty well on this graph.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because this graph has the nice property that.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's two disconnected, two different connected components, each of which happens to correspond to one class label.",
                    "label": 0
                },
                {
                    "sent": "It's kind of reminiscent of the EMH thing that we were saying to clusters of documents that have that correspond to class labels.",
                    "label": 0
                },
                {
                    "sent": "Question how would I calculate how many iterations this will take before it converges?",
                    "label": 0
                },
                {
                    "sent": "Exactly right, it's the diameter of the largest connected component.",
                    "label": 0
                },
                {
                    "sent": "Or the radius?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's right.",
                    "label": 0
                },
                {
                    "sent": "'cause those labels can propagate only one edge per iteration.",
                    "label": 0
                },
                {
                    "sent": "And eventually if we want them to propagate throughout the connected component, it will depend on how many edges you have to traverse in order to get to everywhere in your connected component.",
                    "label": 0
                },
                {
                    "sent": "Given that.",
                    "label": 0
                },
                {
                    "sent": "What's this?",
                    "label": 0
                },
                {
                    "sent": "If you could design a data set?",
                    "label": 0
                },
                {
                    "sent": "Can you design me a data set where in one iteration this thing converges?",
                    "label": 0
                },
                {
                    "sent": "How about this one?",
                    "label": 0
                },
                {
                    "sent": "But if I modify this graph and just make an edge between?",
                    "label": 0
                },
                {
                    "sent": "Every pair.",
                    "label": 0
                },
                {
                    "sent": "You get it right.",
                    "label": 0
                },
                {
                    "sent": "In the positive connected component, I added more edges so that everything is connected in.",
                    "label": 0
                },
                {
                    "sent": "Distance one.",
                    "label": 0
                },
                {
                    "sent": "Which is another another way of saying it is that the my advisor Hyperlink will Co occur with every faculty homepage.",
                    "label": 0
                },
                {
                    "sent": "Right, if I have that kind of.",
                    "label": 0
                },
                {
                    "sent": "Structure then just in one iteration this thing will converge.",
                    "label": 0
                },
                {
                    "sent": "In fact, that kind of structure.",
                    "label": 0
                },
                {
                    "sent": "Is consistent with.",
                    "label": 0
                },
                {
                    "sent": "Let's call these two feature sets X1 and X2 in general.",
                    "label": 0
                },
                {
                    "sent": "Instead of hyperlinks in pages.",
                    "label": 0
                },
                {
                    "sent": "If I'm trying to learn some function.",
                    "label": 0
                },
                {
                    "sent": "From",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "X, which is just the pair X one X2 to Y.",
                    "label": 0
                },
                {
                    "sent": "Then it turns out there's of the best property you can have.",
                    "label": 0
                },
                {
                    "sent": "Is that X one is conditionally independent?",
                    "label": 0
                },
                {
                    "sent": "Of X2.",
                    "label": 0
                },
                {
                    "sent": "Given why?",
                    "label": 0
                },
                {
                    "sent": "Put another way.",
                    "label": 0
                },
                {
                    "sent": "X the probability that X one that a particular value for X1 will occur.",
                    "label": 0
                },
                {
                    "sent": "Given why is the same as the probability that value of X one is not influenced by X2?",
                    "label": 0
                },
                {
                    "sent": "It's the same conditional independence assumption we make about words.",
                    "label": 0
                },
                {
                    "sent": "The bags of words in naive Bayes, but it turns out if you have a problem like this where those are conditionally independent.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then if they are conditionally independent given Y.",
                    "label": 1
                },
                {
                    "sent": "And the function you're trying to learn as pack learnable, which roughly means that given enough labeled data, you could learn it.",
                    "label": 0
                },
                {
                    "sent": "Enough noisy label data.",
                    "label": 0
                },
                {
                    "sent": "You could learn it.",
                    "label": 0
                },
                {
                    "sent": "Then it's also pack learnable.",
                    "label": 0
                },
                {
                    "sent": "From enough labels that give you a better than random initial classifier.",
                    "label": 0
                },
                {
                    "sent": "And then polynomial amount of unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So that's an example of a kind of theoretical statement.",
                    "label": 0
                },
                {
                    "sent": "That we can make this.",
                    "label": 0
                },
                {
                    "sent": "True only in the case where we had that very nice structure.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, if you think about conditional independence.",
                    "label": 0
                },
                {
                    "sent": "In this.",
                    "label": 0
                },
                {
                    "sent": "In this connected component, X1 and X2 are not conditionally independent.",
                    "label": 0
                },
                {
                    "sent": "Given why, how do I know that because Y equals minus for all these?",
                    "label": 0
                },
                {
                    "sent": "And you can see that the probability that I'll get a particular X2 like this one really does depend on the value of X1.",
                    "label": 0
                },
                {
                    "sent": "For example, if this is X1, then I might get this X2, but if this is X1 then I'm not going to get that X2.",
                    "label": 0
                },
                {
                    "sent": "And I want them to be conditionally independent.",
                    "label": 0
                },
                {
                    "sent": "What would this graph have to look like?",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "So if they are conditionally independent effects one and X2 are conditionally independent given Y.",
                    "label": 0
                },
                {
                    "sent": "Then my graph will look just like this.",
                    "label": 0
                },
                {
                    "sent": "It will be the completely connected graph.",
                    "label": 0
                },
                {
                    "sent": "So you see what's going on here?",
                    "label": 0
                },
                {
                    "sent": "I think this gives you a nice insight into the algorithm that iterative Co training thing where one classifier is labeling things for the other.",
                    "label": 0
                },
                {
                    "sent": "And then back the other way.",
                    "label": 0
                },
                {
                    "sent": "Corresponds, at least in the simple case of rote learning classifiers to this kind of propagation of labels through the graph.",
                    "label": 0
                },
                {
                    "sent": "The graph itself represents the data set.",
                    "label": 0
                },
                {
                    "sent": "And we can talk about this structure of a data set, useful structure of the data set in terms of properties of this bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "So we get.",
                    "label": 0
                },
                {
                    "sent": "For example, that conditional independence.",
                    "label": 0
                },
                {
                    "sent": "Of the X1 and X2 features given the label is a very.",
                    "label": 0
                },
                {
                    "sent": "Good case for algorithm.",
                    "label": 0
                },
                {
                    "sent": "A case that would be a problem would be if there was an edge going like this.",
                    "label": 0
                },
                {
                    "sent": "Connecting up our components.",
                    "label": 0
                },
                {
                    "sent": "That would cause problems with our simple rote learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "As you can see, because these pluses would crawl into the negative component and propagate through there and cause.",
                    "label": 0
                },
                {
                    "sent": "Grief.",
                    "label": 0
                },
                {
                    "sent": "The good news is that there are algorithms that are more probabilistic in nature for allowing this kind of back and forth Co training that.",
                    "label": 0
                },
                {
                    "sent": "Don't fall apart when you have that kind of a structure.",
                    "label": 0
                },
                {
                    "sent": "They allow the negative labels here to outvote that positive propagation.",
                    "label": 0
                },
                {
                    "sent": "That's trying to happen across.",
                    "label": 0
                },
                {
                    "sent": "The bridge.",
                    "label": 0
                },
                {
                    "sent": "And so there are other algorithms that are more robust to this kind of problem and that still work quite well.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "I think you get the idea of.",
                    "label": 0
                },
                {
                    "sent": "That problem structure that lets us use unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "I want to ramp up because I want Jamie to start his talk, but let's at least review what we saw so.",
                    "label": 0
                },
                {
                    "sent": "We really saw two different kinds of problems structure.",
                    "label": 0
                },
                {
                    "sent": "Characteristics of the data and the classification function that you're trying to learn over that data.",
                    "label": 0
                },
                {
                    "sent": "That in fact they are theoretical.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We where we can get theoretical guarantees that unlabeled data will be helpful to us.",
                    "label": 0
                },
                {
                    "sent": "The first one was the case where of REM extension of Naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "Which is going where unlabeled data is going to help us improve classification accuracy if.",
                    "label": 0
                },
                {
                    "sent": "The data naturally clusters into groups that align with the class labels.",
                    "label": 0
                },
                {
                    "sent": "And you can see that there are simple ways of testing whether that's likely to be true, even if you only have a little bit of labeled data and a lot of unlabeled, did he just do the clustering and then see whether that's happening?",
                    "label": 0
                },
                {
                    "sent": "So that can be practically useful.",
                    "label": 0
                },
                {
                    "sent": "The second thing we saw, the Co training setting.",
                    "label": 0
                },
                {
                    "sent": "Involves instead of a different kind of assumption that we could take our feature vector that we're trying to classify and break it into two pieces.",
                    "label": 0
                },
                {
                    "sent": "The hyperlink words and the words on the page, such that either one alone is sufficient to make the prediction.",
                    "label": 0
                },
                {
                    "sent": "If you knew the classification function and then we saw that in that case, there are.",
                    "label": 0
                },
                {
                    "sent": "Well, we didn't cover two.",
                    "label": 0
                },
                {
                    "sent": "We covered one theoretical result, but there are others that show how the structure of this kind of graph influences how well that kind of Co training is going to work.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I would just end by saying I put a number of additional slides in here in case you want them for reference.",
                    "label": 0
                },
                {
                    "sent": "The last couple also have bibliography of readings in this area and there is a.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very nice book, this just coming out or just did come out which is an overview of semi supervised learning and I recommend that book.",
                    "label": 0
                },
                {
                    "sent": "It's pretty good.",
                    "label": 0
                },
                {
                    "sent": "If you want to really dig into this area, that's a good place to look.",
                    "label": 0
                },
                {
                    "sent": "So let me end there and I'll turn things over to Jamie, who I see is.",
                    "label": 0
                },
                {
                    "sent": "Ready and waiting in the wings.",
                    "label": 0
                }
            ]
        }
    }
}