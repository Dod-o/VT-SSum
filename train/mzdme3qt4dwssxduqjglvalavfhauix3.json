{
    "id": "mzdme3qt4dwssxduqjglvalavfhauix3",
    "title": "Safe Learning: bridging the gap between Bayes, MDL and statistical learning theory via empirical convexity",
    "info": {
        "author": [
            "Peter Gr\u00fcnwald, Centrum Wiskunde & Informatica (CWI)"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/colt2011_grunwald_gap/",
    "segmentation": [
        [
            "Good afternoon everybody.",
            "So I'm going to attempt to connect a lot of different notions here."
        ],
        [
            "As you can see, so these are all connected to two problems that are seemingly different, but as we will see there are much closer related than you might think, so the first one is a topic that was a hot topic at called around 5 six years ago.",
            "This is the idea that if you do classification with 01 loss, you assume data IID according to some distribution.",
            "Then in many situations the best bounds on the regret, the expected 01 loss between the classifier you learn from the data and the best classifier in your model.",
            "Is on the order of something divided by square root of N, so it goes to 0 but not very fast and you would like to identify situations in which you can learn faster and faster rate.",
            "So for example, if the data are generated by a simple distribution satisfying some sivakov constraint condition.",
            "Then if the.",
            "Typical exponent is nice.",
            "You can get faster rates and there are other examples, so I'll have something to say about that, but that will only be at the end of this talk because first I'm going to concentrate on a different question which is about Bayesian statistics, so it's well known that if you do Bayesian statistics and you have a model Anna prior on that model and the true distribution from which data are sampled, is it the support of the prior?",
            "Then on a very weak condition based on inference converges fast to the true distribution in terms of the callback library version so.",
            "If you get more and more data than the output of your base and procedures that this can be based on maximum, a posterior basin predictive distribution, different divergences between the output of the basin learning algorithm and the true distribution goes to zero.",
            "In collaborative versions fast now it is much less well known that if the true distribution is not in your model, then this doesn't necessarily hold anymore, so if it's not in the true distribution is not in your model, you would expect that if there are still a distribution in your model which is reasonably close.",
            "To the true distribution in the callback library, sense that given enough data, you converge to debt distribution with base, but John Langford and I showed in our call 2004 paper very simple setting where you have a true distribution generating data ID.",
            "You have a model with an element that's pretty close to the true distribution in Public Library versions, but no matter how many data you get, the output of the basin learning algorithm remains far away from the best distribution in your model, so you fail to converge.",
            "You fail to learn.",
            "This is a pretty bad state of affairs if you think about the fact that nowadays about 30% of all papers in statistics journals have the word bashan in their abstract.",
            "And if you look at the models they are using, then more often than not there probably not correct, so we want to see whether we can design A generalization of base that is safe in the sense that it also works well if the model is incorrect and the same question.",
            "The same issue holds also for minimum description length inference, which is of course very similar to base an inference.",
            "Again, it can fail if the models.",
            "Incorrect can we generalize?"
        ],
        [
            "So what we do here is we find such a generalization of minimum description, length and base, so I should immediately add that it's not practical yet.",
            "It's computationally expensive and it involves constants which at least now are pretty large.",
            "But it's a step in the first.",
            "It's a good first step in this direction, and the interesting thing now is an Outback fires to question, one that if you now have this generalization of Basin MDL and you apply it to classification problems, then you suddenly get see back of optimal rates.",
            "If the true distribution allows that an you get also fast rates in several other cases.",
            "Now the main idea of the generalization is this.",
            "You realize that models can be misspecified.",
            "That's how a statistician calls model being wrong.",
            "The true distribution is not in there.",
            "In two different ways it can be harmless or harmful.",
            "If it's harmful, that means that base won't converged now.",
            "It turns out that you can see from the data whether you're in the good or the bad, the harmful case, and if you're in the harmful case, you can adjust your priors so that Bayesian inference converges after all.",
            "Essentially what you do if you're in a harmful cases you penalize complexity things with small priors more by changing the prior and then everything works again and this test somehow magically involves convexity.",
            "We will see how in a moment."
        ],
        [
            "So to make this more precise, I will actually concentrate on minimum description length.",
            "It's bit easier here.",
            "Very basic two part version, so we assume that there are some distribution and data X&Y are IID according to the distribution.",
            "It could be a regression or classification set up, but we have a probabilistic model, a set of conditional densities or probability mass functions of why is discrete.",
            "They're all of the form probability of Y given X extended to and outcomes by independence, and we have a prior on them with full support.",
            "And know that we assume the model here to be countable because we want to encode models, but otherwise it can be very large, like all computable distributions would be fine, but it has to be countable here.",
            "Now, in this context, we define the two MD estimator denoted by P2 dots.",
            "As the minimum overall distributions in your probabilistic model of the minus lock prior that you assign to this distribution minus log, the likelihood of the data according to the distribution.",
            "Why is this called minimum description length?",
            "While you can think of minus lock prior, prior minus looked for any distribution, P's number of bits you need to encode it.",
            "If you use the Shannon Fano code corresponding to pee.",
            "So what you doing here, you quote the data where you assume the X values are given.",
            "You only called the Y values.",
            "By first coding AP.",
            "Using minus log, WP Bitzan then encoding the data using P using the code corresponding to be using minus log P bits.",
            "Now I cheated a little bit because you can see there is a two there.",
            "This is a trick introduced by Andrew Barron.",
            "I have no time to go into it, but this will make everything a lot easier so I'll just do it.",
            "But formally it would be also be MD Live.",
            "There wouldn't be a tool there.",
            "And we'll call this 2:00 AM."
        ],
        [
            "DL now I told you if the model is correct then MDL converges, so I'll give you a vanilla version of a theorem saying this.",
            "So first the KL divergent between the true distribution and pee in the setup is really a conditional KL divergent, so we assume the true distribution has a conditional density and then it's this expected difference of the lock conditional densities.",
            "Now assume that Q is the closest to the truth in my model.",
            "In terms of Public Library, so it's the closest into model Curly P in terms of Public Library distance to P star.",
            "This wreck it's mean that we take the information closure of P, which is for technical reasons.",
            "Otherwise there might be no minimum in this set, but for this talk you can forget about it.",
            "It's just the closest thing in the model.",
            "Now, Theorem stated in informal way going back to burn and Cover 1991 says that if the true distributions in your model, then in this situation with probability one according to the true distribution, as you get more and more data, the P double dot MDL estimator which depends on the amount of data clearly will converge to.",
            "To kill so that callback library versions between what you learned from the data and the best thing in your model goes to 0, because the best thing in your model is the true distribution, the whole thing converges to zero you learn later, we will see how fast you learn what is already says that you learn.",
            "Now, what is much less well known again is that this also holds not just if."
        ],
        [
            "The true distribution is in the model, but also if your model is convex.",
            "I put it between quotation marks because we're working with countable models, so convexity means that if you that the model is dense in its convex closure.",
            "This was in a very unknown publication, PhD thesis of Jonathan Lee, student of Andrew Barron, 1999, and it turns out that the same thing still holds, but now of course, the distance between the best thing in your model and P star isn't necessary.",
            "0 but you still converge to it, and as it turns out also at the same type of rates.",
            "So this means that if the model is convex, we're happy we can do MDL and base base in this fashion, more like maximum posterior and everything works, but link for it.",
            "And I showed that sometimes it goes wrong.",
            "Sometimes you don't convert.",
            "So this means that when things go wrong you have models which are not convex.",
            "Can we do something if models are not conflicts?",
            "Well, the first insight when I looked at the proof of Lee's theorem was that actually your model.",
            "Being calm."
        ],
        [
            "Max is too strong, it's sufficient.",
            "If.",
            "You cannot get closer to the truth by making your model convex.",
            "If you blow up your model to its convex whole right, you take all mixtures of the distributions in there.",
            "And then if you cannot get closer to the true distributions by then, the proof still goes through.",
            "It doesn't help to take a convict closures and you convert.",
            "So this means that there are two types of misspecification."
        ],
        [
            "The this is the bad case where you have a model which is not convex.",
            "Of course I'm talking about KL divergent, not not of Euclidean diversion, so take this with a grain of salt.",
            "But it's something like this.",
            "You have a non convex set which is a model, a set of distributions, a true distribution and the closest thing to the truth is such that."
        ],
        [
            "If you take the convex Hull of the model, you can get closer.",
            "This is bad."
        ],
        [
            "This is a good situation if you would take the convex Hull of your model, you wouldn't be able to get any closer, so this is fine.",
            "In this case, if you're lucky and the true distribution compared to your model is, that is at a location where you cannot do better if you take the convex Hull, then you can just use ordinary M dealer base and you'll converge.",
            "But this is of course very theoretical because we don't know the true distribution, right?",
            "It's not even in our models we have no idea where it is.",
            "So can we tell whether we're in a good and a bad situation?",
            "That would be a first step towards doing something better.",
            "If we can see from the data that we're in a bad situation, we just give up and say don't use them deal.",
            "We will do better than that, but that's how we start.",
            "So can we tell from the data what situation we're in?",
            "Well, clearly it seems this has something to do with convexity.",
            "If you can gain by making your model convex going to the convex closure, you can get better.",
            "You can get closer to the true distribution.",
            "This should be reflected in your data."
        ],
        [
            "So the first idea is to use MDL again, but now at a meta level.",
            "So you're now saying, like, what if I take the convex closure of my model?",
            "I take some prior on that and now I compress the data by taking the best distribution of convex closure and coding the data with that with the new prior.",
            "Can I compress the data a lot more than if I don't take the convex closure if I take the ordinary MDL so it's just two code links here, and if the one is much smaller than the other, then this is an indication that it helps to go to the convex closure.",
            "So probably I'm in a bad situation.",
            "So this might work, but it turns out to be overkill, and it would be extremely impractical because you really blow up if you take this convex Hull, right?",
            "So it turns out you can do something simpler.",
            "Well, simple, it's simpler in terms of computation, but it's actually quite complicated in terms of."
        ],
        [
            "So this will be the most complicated slide, I promise.",
            "What you can do is this.",
            "You get the data you take.",
            "The MDL estimator is P double dot and now you look at all two component mixtures of the MDL estimator and every other distribution in your model which has a prior which is no smaller than the prior of the two part MDL estimator.",
            "So the two part MD estimator has a certain prior probability.",
            "There's only a finite number of distributions in your model which have a larger prior.",
            "You look at all mixtures between your estimator and those things with the larger prior for all mixture coefficients and you see whether any one of those compresses the data more.",
            "If that happens, it's also clearly an indication that it helps to go to the convex closure of your model, but you only need two component mixtures to find out, and this turns out to be what you need.",
            "Of course, I've been very vague about this smaller, smaller then we will make it precise in the moment.",
            "But it turns out that if this condition holds, this means you're in a bad situation and MDL will not work."
        ],
        [
            "But now we want to do more, right?",
            "We can detect whether we're in a bad situation, but can we somehow repair it?",
            "So now it gets interesting.",
            "Um, what we can do is we have our probability model, we can exponentiate it by taking every density in there and taking it to some power pizza and normalizing.",
            "So this set of ITA is just the sum.",
            "Overall, why outcomes given all X outcome such that the new Peter to eat as a probability again?",
            "So this gives you a new model.",
            "We basically you shrink everything to the uniform distribution, right?",
            "If you get let Y to shrink towards zero and all these distributions if Y is finite will tend to the uniform distribution and why?",
            "Given X.",
            "Now it turns out that if you do this, you start with ITA is one that's your original model and you make it smaller and smaller.",
            "Then basically your district.",
            "Your set of distributions in away becomes more and more convex in a sense that at some point you hit a point where it doesn't help anymore to go to the convex closure to get closer to the true distribution.",
            "So this is pretty.",
            "Pretty complicated, but it is crucial so you have this set of distributions as a true distribution.",
            "If you go to the convex closure of the set, you can get closer to the true distribution.",
            "Now I'm changing the set by making it more and more uniform, and at some point if this change set.",
            "It doesn't help anymore to make it convex.",
            "It doesn't give me a point which is closer to the truth distribution, the eater for which that happens is a critical eater.",
            "That's how I call it.",
            "And if you have an Oracle which tells you what the Critical Eater is, then you can actually just use."
        ],
        [
            "DL but you don't penalize by two times minus lock prior, but by 2 * 1 over Eater Lock prior.",
            "So either will be smaller than one so one over it is larger than one, so you penalize more, right?",
            "You select a distribution with a larger prior probability.",
            "And it turns out it turns out that if you do this so you penalize more by one over Eater, then the you select would be the same P you would select.",
            "If you would do the ordinary MDL, but on the shrunken model, right?",
            "So this is a convenient way to get the P from the shrunken model, so this would work if we would know the critical eater.",
            "Right now, we could just use them, deal with these larger penalties, and with these larger penalties we would know that we would convert.",
            "Well, we don't."
        ],
        [
            "No, the larger Peter.",
            "But what can we do instead?",
            "Well, I already said we can find out whether we're in a bad situation.",
            "If it is 2 component mixture of the MDL estimator and another element of your model gives a better compression of your data then standard two part MDL.",
            "So now we repeat this for we start with the test one and we keep doing this until the difference becomes approximately 0 and on the data you have.",
            "It doesn't help you anymore.",
            "To go to these two component mixtures, it doesn't help you to get more compression.",
            "Now, informally, the safe estimator will be defined as doing two part MDL.",
            "Not with this two in front of the minus log private with a higher penalty which is 2 / y to eat is between zero and one Anita is the largest value.",
            "For which you cannot compress the data more by taking two component mixtures."
        ],
        [
            "So now I'm almost ready to state the big theorem.",
            "So I start with.",
            "A pre version which is just a trivial extension of an existing result where we still assume the model is correct or convex.",
            "So this just expresses again that if you use ordinary two MDL.",
            "You converge, you converge to the best distribution in your model.",
            "Um and this was the big thing found by Baron Cover, 1991.",
            "The convergence is governed by the redundancy."
        ],
        [
            "So the redundancy."
        ],
        [
            "I make it a bit simpler here.",
            "The formula, the redundancy.",
            "That's this long formula.",
            "If you look at it, you see that's the left two terms are the number of bits you need to code the data.",
            "If you use the MDL estimator.",
            "To code it, you first called the MD estimator and data with the MD estimator.",
            "It's a two part code.",
            "The right side is the number of bits you need with the best distribution in your model, so the redundancy is the extra number of bits you need compared to.",
            "The best thing in your model on the data you have.",
            "So this is what you get on the left, and if that thing is small then the distance.",
            "Um?",
            "On the left will be smaller,"
        ],
        [
            "Merge to the true distribution and how barren cover showed that if you choose your priors cleverly, then you can make the redundancy be logarithmically nenen for parametric models and be of the form, enter the gamma for gamma smaller than one.",
            "For nonparametric models, which means that the left hand side actually goes to 0 so you converge to the best distribution an often at rates, which are minimax optimal.",
            "As we know from statistical results.",
            "So this shows that basic MDL works, same thing holds if the model is not correct but convex.",
            "So now."
        ],
        [
            "The main result.",
            "We don't have this assumption anymore.",
            "And if we don't have the assumption anymore than the same theorem still holds, but there's one extra term next to the redundancy, and that's the convexity leg, and that's this code length difference I've talked about before.",
            "So before I looked at it as a constraint, I make it a smaller and smaller until the difference between the code.",
            "If I just code, and if I take the two component mixture is about 0.",
            "Now it turns out to be easier to prove theorems to just edit to this redundancy, and then I automatically get a tradeoff between redundancy.",
            "Which is decreasing if I make it a smaller."
        ],
        [
            "And convexity lack which sort of redundancy gets larger Mikita smaller because you penalize more, so you need more bits to code your data, but the convexity let get smaller if Y to get smaller because either get smaller, your model becomes more like a convex model and this holds uniformly for all eater.",
            "So this suggests to define the safe estimator as the MDL estimator for two overeater where eater is chosen such as to minimize the sum of the redundancy in a convexity leg.",
            "Now note that every term in the sum of redundancy and convexity lack, which depends if you want to minimize it, you can just look at the data.",
            "You don't need to know the true distribution, right?",
            "So you can effectively minimize this.",
            "This is a real."
        ],
        [
            "Algorithm.",
            "So this means that we've generalized Baron covers results, but is it useful?",
            "Well, you can show that if you have an ether which is smaller than the critical eater, then actually this convexity, like term is smaller than a constant times the redundancy.",
            "So then the whole thing becomes smaller than a constant times redundancy divided by N. This means that you get the same rates of convergence is with original MDL.",
            "If you have the true distribution in your model, you're very lucky you get the same rates of convergence as before, but with the worst constant.",
            "This may be the price you have to pay for using a procedure which still works if your model is incorrect, even though it turns out to be correct."
        ],
        [
            "So now the punchline.",
            "We get back to classification.",
            "What if you do classification with this?",
            "So we do what they do in the PAC Basin literature and in the prediction with expert advice literature we have a set of classifiers.",
            "Why is zero or one we met them to distributions by the E to the minus beta last transformation and we normalized by asset beta.",
            "So now we get a set of distributions right instead of classifiers becomes a set of distributions and we use minimum description length here.",
            "So then this generalized KL Live versions, the dystar becomes minus log E to the minus loss.",
            "Divided by each new."
        ],
        [
            "Minus loss, so it's actually now equal to beta, which we can choose ourselves.",
            "So let's choose it to be one times.",
            "The expected regret in 01 loss difference.",
            "It's now the difference in 01 loss between what you get if you do two part MDL estimation with the chosen Eater and the best thing in your model which minimizes the risk of the expected 01 lesson you see, you combine it in terms of the same quantities, right?",
            "The model we constructed may be completely wrong, right?",
            "It's E to the minus beta.",
            "Leos who knows where the data are generated in that way when we don't care because this thing doesn't need the model to be."
        ],
        [
            "Correct, no matter what, P star or H we use the critical eater in this way.",
            "If we do this will always be bounded by 1 / sqrt N. If we plug in 1 / sqrt N, here we get.",
            "Regret bound of order minus log prior of the thing you chose divided by square root of N. If you choose eat a slightly larger, you actually get the worst case optimal thing.",
            "Square root of minus lock prior.",
            "So the square root is now over the minus log divided by it, and that's the worst thing.",
            "The best thing you can do in the worst case.",
            "This is known from a structural risk minimization like analysis.",
            "Now, so this means that you can capture the worst case classification with this form of MDM base.",
            "Now if I had time I would also show you that if P star satisfies at the back of condition of the form where you don't need to have the base classifier in your model, but just that the expected your wants risk is bounded by some expected of power of 01 loss.",
            "Then you can actually get larger equated ecrit so critical eaters.",
            "They still depend on N but not as 1 / sqrt N, but they go down to zero much slower and you get the back of optimal rates."
        ],
        [
            "So All in all, I've done a lot of things here, so I've given a generalization of MDL and two part based when the model is wrong.",
            "I found a kind of connection between sybok, often convexity, so there's only one similar work I know of, which is Cody Bears, PhD thesis, and actually I want to study that better in future work, but there is no explicit connection to convexity.",
            "Um, finally there's a theorem I didn't talk about which connects this to pick basin type of bounce, and then you can really get something new.",
            "Um?",
            "Then in the future I the one thing I would like to do is make this more practical.",
            "So then you can look at online versions or full Bashan.",
            "Versions were used predictive rather than map distributions.",
            "And note that the impromptu talk by Tim from their phone was actually about something similar because their what we tried to do was learn the ether parameter in the hedge algorithm, which is again in a way a very similar problem as this.",
            "We want to determine it by the data, but in an online fashion.",
            "Thank you.",
            "Questions.",
            "Other special special cases, I mean binomial models or things like that, where computations are actually possible or simplify, just as in information theory.",
            "To be honest, I haven't looked at that yet, so I don't know I.",
            "Uh.",
            "I'm just not sure, so the first thing that you mentioned before you got into the rates was convergence to the optimal model in your class.",
            "What sort of you assume had an IID assumption?",
            "Does that make that much difference, or can you still get convergence even without IID?",
            "Maybe with organic or something like that, you probably can.",
            "So I think the original result is rather straightforward to generalize to that, but I would have to think about what I'm doing, whether that's again I didn't.",
            "It's too early.",
            "I haven't thought about yet, but the original thing can be generalized, yes.",
            "Any other questions?",
            "Alright, let's thank Peter again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good afternoon everybody.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to attempt to connect a lot of different notions here.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As you can see, so these are all connected to two problems that are seemingly different, but as we will see there are much closer related than you might think, so the first one is a topic that was a hot topic at called around 5 six years ago.",
                    "label": 0
                },
                {
                    "sent": "This is the idea that if you do classification with 01 loss, you assume data IID according to some distribution.",
                    "label": 0
                },
                {
                    "sent": "Then in many situations the best bounds on the regret, the expected 01 loss between the classifier you learn from the data and the best classifier in your model.",
                    "label": 0
                },
                {
                    "sent": "Is on the order of something divided by square root of N, so it goes to 0 but not very fast and you would like to identify situations in which you can learn faster and faster rate.",
                    "label": 0
                },
                {
                    "sent": "So for example, if the data are generated by a simple distribution satisfying some sivakov constraint condition.",
                    "label": 0
                },
                {
                    "sent": "Then if the.",
                    "label": 0
                },
                {
                    "sent": "Typical exponent is nice.",
                    "label": 0
                },
                {
                    "sent": "You can get faster rates and there are other examples, so I'll have something to say about that, but that will only be at the end of this talk because first I'm going to concentrate on a different question which is about Bayesian statistics, so it's well known that if you do Bayesian statistics and you have a model Anna prior on that model and the true distribution from which data are sampled, is it the support of the prior?",
                    "label": 0
                },
                {
                    "sent": "Then on a very weak condition based on inference converges fast to the true distribution in terms of the callback library version so.",
                    "label": 0
                },
                {
                    "sent": "If you get more and more data than the output of your base and procedures that this can be based on maximum, a posterior basin predictive distribution, different divergences between the output of the basin learning algorithm and the true distribution goes to zero.",
                    "label": 0
                },
                {
                    "sent": "In collaborative versions fast now it is much less well known that if the true distribution is not in your model, then this doesn't necessarily hold anymore, so if it's not in the true distribution is not in your model, you would expect that if there are still a distribution in your model which is reasonably close.",
                    "label": 0
                },
                {
                    "sent": "To the true distribution in the callback library, sense that given enough data, you converge to debt distribution with base, but John Langford and I showed in our call 2004 paper very simple setting where you have a true distribution generating data ID.",
                    "label": 0
                },
                {
                    "sent": "You have a model with an element that's pretty close to the true distribution in Public Library versions, but no matter how many data you get, the output of the basin learning algorithm remains far away from the best distribution in your model, so you fail to converge.",
                    "label": 0
                },
                {
                    "sent": "You fail to learn.",
                    "label": 0
                },
                {
                    "sent": "This is a pretty bad state of affairs if you think about the fact that nowadays about 30% of all papers in statistics journals have the word bashan in their abstract.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the models they are using, then more often than not there probably not correct, so we want to see whether we can design A generalization of base that is safe in the sense that it also works well if the model is incorrect and the same question.",
                    "label": 1
                },
                {
                    "sent": "The same issue holds also for minimum description length inference, which is of course very similar to base an inference.",
                    "label": 0
                },
                {
                    "sent": "Again, it can fail if the models.",
                    "label": 0
                },
                {
                    "sent": "Incorrect can we generalize?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we do here is we find such a generalization of minimum description, length and base, so I should immediately add that it's not practical yet.",
                    "label": 1
                },
                {
                    "sent": "It's computationally expensive and it involves constants which at least now are pretty large.",
                    "label": 0
                },
                {
                    "sent": "But it's a step in the first.",
                    "label": 1
                },
                {
                    "sent": "It's a good first step in this direction, and the interesting thing now is an Outback fires to question, one that if you now have this generalization of Basin MDL and you apply it to classification problems, then you suddenly get see back of optimal rates.",
                    "label": 0
                },
                {
                    "sent": "If the true distribution allows that an you get also fast rates in several other cases.",
                    "label": 1
                },
                {
                    "sent": "Now the main idea of the generalization is this.",
                    "label": 1
                },
                {
                    "sent": "You realize that models can be misspecified.",
                    "label": 0
                },
                {
                    "sent": "That's how a statistician calls model being wrong.",
                    "label": 0
                },
                {
                    "sent": "The true distribution is not in there.",
                    "label": 0
                },
                {
                    "sent": "In two different ways it can be harmless or harmful.",
                    "label": 0
                },
                {
                    "sent": "If it's harmful, that means that base won't converged now.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you can see from the data whether you're in the good or the bad, the harmful case, and if you're in the harmful case, you can adjust your priors so that Bayesian inference converges after all.",
                    "label": 1
                },
                {
                    "sent": "Essentially what you do if you're in a harmful cases you penalize complexity things with small priors more by changing the prior and then everything works again and this test somehow magically involves convexity.",
                    "label": 0
                },
                {
                    "sent": "We will see how in a moment.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to make this more precise, I will actually concentrate on minimum description length.",
                    "label": 0
                },
                {
                    "sent": "It's bit easier here.",
                    "label": 0
                },
                {
                    "sent": "Very basic two part version, so we assume that there are some distribution and data X&Y are IID according to the distribution.",
                    "label": 0
                },
                {
                    "sent": "It could be a regression or classification set up, but we have a probabilistic model, a set of conditional densities or probability mass functions of why is discrete.",
                    "label": 1
                },
                {
                    "sent": "They're all of the form probability of Y given X extended to and outcomes by independence, and we have a prior on them with full support.",
                    "label": 0
                },
                {
                    "sent": "And know that we assume the model here to be countable because we want to encode models, but otherwise it can be very large, like all computable distributions would be fine, but it has to be countable here.",
                    "label": 1
                },
                {
                    "sent": "Now, in this context, we define the two MD estimator denoted by P2 dots.",
                    "label": 0
                },
                {
                    "sent": "As the minimum overall distributions in your probabilistic model of the minus lock prior that you assign to this distribution minus log, the likelihood of the data according to the distribution.",
                    "label": 0
                },
                {
                    "sent": "Why is this called minimum description length?",
                    "label": 0
                },
                {
                    "sent": "While you can think of minus lock prior, prior minus looked for any distribution, P's number of bits you need to encode it.",
                    "label": 0
                },
                {
                    "sent": "If you use the Shannon Fano code corresponding to pee.",
                    "label": 0
                },
                {
                    "sent": "So what you doing here, you quote the data where you assume the X values are given.",
                    "label": 0
                },
                {
                    "sent": "You only called the Y values.",
                    "label": 0
                },
                {
                    "sent": "By first coding AP.",
                    "label": 0
                },
                {
                    "sent": "Using minus log, WP Bitzan then encoding the data using P using the code corresponding to be using minus log P bits.",
                    "label": 0
                },
                {
                    "sent": "Now I cheated a little bit because you can see there is a two there.",
                    "label": 0
                },
                {
                    "sent": "This is a trick introduced by Andrew Barron.",
                    "label": 0
                },
                {
                    "sent": "I have no time to go into it, but this will make everything a lot easier so I'll just do it.",
                    "label": 0
                },
                {
                    "sent": "But formally it would be also be MD Live.",
                    "label": 0
                },
                {
                    "sent": "There wouldn't be a tool there.",
                    "label": 0
                },
                {
                    "sent": "And we'll call this 2:00 AM.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "DL now I told you if the model is correct then MDL converges, so I'll give you a vanilla version of a theorem saying this.",
                    "label": 0
                },
                {
                    "sent": "So first the KL divergent between the true distribution and pee in the setup is really a conditional KL divergent, so we assume the true distribution has a conditional density and then it's this expected difference of the lock conditional densities.",
                    "label": 0
                },
                {
                    "sent": "Now assume that Q is the closest to the truth in my model.",
                    "label": 0
                },
                {
                    "sent": "In terms of Public Library, so it's the closest into model Curly P in terms of Public Library distance to P star.",
                    "label": 0
                },
                {
                    "sent": "This wreck it's mean that we take the information closure of P, which is for technical reasons.",
                    "label": 0
                },
                {
                    "sent": "Otherwise there might be no minimum in this set, but for this talk you can forget about it.",
                    "label": 0
                },
                {
                    "sent": "It's just the closest thing in the model.",
                    "label": 0
                },
                {
                    "sent": "Now, Theorem stated in informal way going back to burn and Cover 1991 says that if the true distributions in your model, then in this situation with probability one according to the true distribution, as you get more and more data, the P double dot MDL estimator which depends on the amount of data clearly will converge to.",
                    "label": 0
                },
                {
                    "sent": "To kill so that callback library versions between what you learned from the data and the best thing in your model goes to 0, because the best thing in your model is the true distribution, the whole thing converges to zero you learn later, we will see how fast you learn what is already says that you learn.",
                    "label": 0
                },
                {
                    "sent": "Now, what is much less well known again is that this also holds not just if.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The true distribution is in the model, but also if your model is convex.",
                    "label": 0
                },
                {
                    "sent": "I put it between quotation marks because we're working with countable models, so convexity means that if you that the model is dense in its convex closure.",
                    "label": 0
                },
                {
                    "sent": "This was in a very unknown publication, PhD thesis of Jonathan Lee, student of Andrew Barron, 1999, and it turns out that the same thing still holds, but now of course, the distance between the best thing in your model and P star isn't necessary.",
                    "label": 0
                },
                {
                    "sent": "0 but you still converge to it, and as it turns out also at the same type of rates.",
                    "label": 0
                },
                {
                    "sent": "So this means that if the model is convex, we're happy we can do MDL and base base in this fashion, more like maximum posterior and everything works, but link for it.",
                    "label": 0
                },
                {
                    "sent": "And I showed that sometimes it goes wrong.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you don't convert.",
                    "label": 0
                },
                {
                    "sent": "So this means that when things go wrong you have models which are not convex.",
                    "label": 0
                },
                {
                    "sent": "Can we do something if models are not conflicts?",
                    "label": 0
                },
                {
                    "sent": "Well, the first insight when I looked at the proof of Lee's theorem was that actually your model.",
                    "label": 0
                },
                {
                    "sent": "Being calm.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Max is too strong, it's sufficient.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "You cannot get closer to the truth by making your model convex.",
                    "label": 0
                },
                {
                    "sent": "If you blow up your model to its convex whole right, you take all mixtures of the distributions in there.",
                    "label": 0
                },
                {
                    "sent": "And then if you cannot get closer to the true distributions by then, the proof still goes through.",
                    "label": 0
                },
                {
                    "sent": "It doesn't help to take a convict closures and you convert.",
                    "label": 0
                },
                {
                    "sent": "So this means that there are two types of misspecification.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The this is the bad case where you have a model which is not convex.",
                    "label": 0
                },
                {
                    "sent": "Of course I'm talking about KL divergent, not not of Euclidean diversion, so take this with a grain of salt.",
                    "label": 0
                },
                {
                    "sent": "But it's something like this.",
                    "label": 0
                },
                {
                    "sent": "You have a non convex set which is a model, a set of distributions, a true distribution and the closest thing to the truth is such that.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you take the convex Hull of the model, you can get closer.",
                    "label": 0
                },
                {
                    "sent": "This is bad.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a good situation if you would take the convex Hull of your model, you wouldn't be able to get any closer, so this is fine.",
                    "label": 0
                },
                {
                    "sent": "In this case, if you're lucky and the true distribution compared to your model is, that is at a location where you cannot do better if you take the convex Hull, then you can just use ordinary M dealer base and you'll converge.",
                    "label": 0
                },
                {
                    "sent": "But this is of course very theoretical because we don't know the true distribution, right?",
                    "label": 0
                },
                {
                    "sent": "It's not even in our models we have no idea where it is.",
                    "label": 0
                },
                {
                    "sent": "So can we tell whether we're in a good and a bad situation?",
                    "label": 0
                },
                {
                    "sent": "That would be a first step towards doing something better.",
                    "label": 0
                },
                {
                    "sent": "If we can see from the data that we're in a bad situation, we just give up and say don't use them deal.",
                    "label": 0
                },
                {
                    "sent": "We will do better than that, but that's how we start.",
                    "label": 0
                },
                {
                    "sent": "So can we tell from the data what situation we're in?",
                    "label": 0
                },
                {
                    "sent": "Well, clearly it seems this has something to do with convexity.",
                    "label": 0
                },
                {
                    "sent": "If you can gain by making your model convex going to the convex closure, you can get better.",
                    "label": 0
                },
                {
                    "sent": "You can get closer to the true distribution.",
                    "label": 0
                },
                {
                    "sent": "This should be reflected in your data.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first idea is to use MDL again, but now at a meta level.",
                    "label": 0
                },
                {
                    "sent": "So you're now saying, like, what if I take the convex closure of my model?",
                    "label": 0
                },
                {
                    "sent": "I take some prior on that and now I compress the data by taking the best distribution of convex closure and coding the data with that with the new prior.",
                    "label": 1
                },
                {
                    "sent": "Can I compress the data a lot more than if I don't take the convex closure if I take the ordinary MDL so it's just two code links here, and if the one is much smaller than the other, then this is an indication that it helps to go to the convex closure.",
                    "label": 1
                },
                {
                    "sent": "So probably I'm in a bad situation.",
                    "label": 0
                },
                {
                    "sent": "So this might work, but it turns out to be overkill, and it would be extremely impractical because you really blow up if you take this convex Hull, right?",
                    "label": 1
                },
                {
                    "sent": "So it turns out you can do something simpler.",
                    "label": 0
                },
                {
                    "sent": "Well, simple, it's simpler in terms of computation, but it's actually quite complicated in terms of.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this will be the most complicated slide, I promise.",
                    "label": 0
                },
                {
                    "sent": "What you can do is this.",
                    "label": 0
                },
                {
                    "sent": "You get the data you take.",
                    "label": 0
                },
                {
                    "sent": "The MDL estimator is P double dot and now you look at all two component mixtures of the MDL estimator and every other distribution in your model which has a prior which is no smaller than the prior of the two part MDL estimator.",
                    "label": 0
                },
                {
                    "sent": "So the two part MD estimator has a certain prior probability.",
                    "label": 0
                },
                {
                    "sent": "There's only a finite number of distributions in your model which have a larger prior.",
                    "label": 0
                },
                {
                    "sent": "You look at all mixtures between your estimator and those things with the larger prior for all mixture coefficients and you see whether any one of those compresses the data more.",
                    "label": 1
                },
                {
                    "sent": "If that happens, it's also clearly an indication that it helps to go to the convex closure of your model, but you only need two component mixtures to find out, and this turns out to be what you need.",
                    "label": 0
                },
                {
                    "sent": "Of course, I've been very vague about this smaller, smaller then we will make it precise in the moment.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that if this condition holds, this means you're in a bad situation and MDL will not work.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But now we want to do more, right?",
                    "label": 0
                },
                {
                    "sent": "We can detect whether we're in a bad situation, but can we somehow repair it?",
                    "label": 0
                },
                {
                    "sent": "So now it gets interesting.",
                    "label": 0
                },
                {
                    "sent": "Um, what we can do is we have our probability model, we can exponentiate it by taking every density in there and taking it to some power pizza and normalizing.",
                    "label": 0
                },
                {
                    "sent": "So this set of ITA is just the sum.",
                    "label": 0
                },
                {
                    "sent": "Overall, why outcomes given all X outcome such that the new Peter to eat as a probability again?",
                    "label": 0
                },
                {
                    "sent": "So this gives you a new model.",
                    "label": 0
                },
                {
                    "sent": "We basically you shrink everything to the uniform distribution, right?",
                    "label": 0
                },
                {
                    "sent": "If you get let Y to shrink towards zero and all these distributions if Y is finite will tend to the uniform distribution and why?",
                    "label": 0
                },
                {
                    "sent": "Given X.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that if you do this, you start with ITA is one that's your original model and you make it smaller and smaller.",
                    "label": 0
                },
                {
                    "sent": "Then basically your district.",
                    "label": 0
                },
                {
                    "sent": "Your set of distributions in away becomes more and more convex in a sense that at some point you hit a point where it doesn't help anymore to go to the convex closure to get closer to the true distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is pretty.",
                    "label": 0
                },
                {
                    "sent": "Pretty complicated, but it is crucial so you have this set of distributions as a true distribution.",
                    "label": 0
                },
                {
                    "sent": "If you go to the convex closure of the set, you can get closer to the true distribution.",
                    "label": 0
                },
                {
                    "sent": "Now I'm changing the set by making it more and more uniform, and at some point if this change set.",
                    "label": 0
                },
                {
                    "sent": "It doesn't help anymore to make it convex.",
                    "label": 0
                },
                {
                    "sent": "It doesn't give me a point which is closer to the truth distribution, the eater for which that happens is a critical eater.",
                    "label": 0
                },
                {
                    "sent": "That's how I call it.",
                    "label": 0
                },
                {
                    "sent": "And if you have an Oracle which tells you what the Critical Eater is, then you can actually just use.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "DL but you don't penalize by two times minus lock prior, but by 2 * 1 over Eater Lock prior.",
                    "label": 0
                },
                {
                    "sent": "So either will be smaller than one so one over it is larger than one, so you penalize more, right?",
                    "label": 0
                },
                {
                    "sent": "You select a distribution with a larger prior probability.",
                    "label": 0
                },
                {
                    "sent": "And it turns out it turns out that if you do this so you penalize more by one over Eater, then the you select would be the same P you would select.",
                    "label": 0
                },
                {
                    "sent": "If you would do the ordinary MDL, but on the shrunken model, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a convenient way to get the P from the shrunken model, so this would work if we would know the critical eater.",
                    "label": 0
                },
                {
                    "sent": "Right now, we could just use them, deal with these larger penalties, and with these larger penalties we would know that we would convert.",
                    "label": 0
                },
                {
                    "sent": "Well, we don't.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, the larger Peter.",
                    "label": 0
                },
                {
                    "sent": "But what can we do instead?",
                    "label": 0
                },
                {
                    "sent": "Well, I already said we can find out whether we're in a bad situation.",
                    "label": 1
                },
                {
                    "sent": "If it is 2 component mixture of the MDL estimator and another element of your model gives a better compression of your data then standard two part MDL.",
                    "label": 0
                },
                {
                    "sent": "So now we repeat this for we start with the test one and we keep doing this until the difference becomes approximately 0 and on the data you have.",
                    "label": 0
                },
                {
                    "sent": "It doesn't help you anymore.",
                    "label": 0
                },
                {
                    "sent": "To go to these two component mixtures, it doesn't help you to get more compression.",
                    "label": 1
                },
                {
                    "sent": "Now, informally, the safe estimator will be defined as doing two part MDL.",
                    "label": 0
                },
                {
                    "sent": "Not with this two in front of the minus log private with a higher penalty which is 2 / y to eat is between zero and one Anita is the largest value.",
                    "label": 0
                },
                {
                    "sent": "For which you cannot compress the data more by taking two component mixtures.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I'm almost ready to state the big theorem.",
                    "label": 0
                },
                {
                    "sent": "So I start with.",
                    "label": 0
                },
                {
                    "sent": "A pre version which is just a trivial extension of an existing result where we still assume the model is correct or convex.",
                    "label": 1
                },
                {
                    "sent": "So this just expresses again that if you use ordinary two MDL.",
                    "label": 0
                },
                {
                    "sent": "You converge, you converge to the best distribution in your model.",
                    "label": 0
                },
                {
                    "sent": "Um and this was the big thing found by Baron Cover, 1991.",
                    "label": 0
                },
                {
                    "sent": "The convergence is governed by the redundancy.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the redundancy.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I make it a bit simpler here.",
                    "label": 0
                },
                {
                    "sent": "The formula, the redundancy.",
                    "label": 0
                },
                {
                    "sent": "That's this long formula.",
                    "label": 0
                },
                {
                    "sent": "If you look at it, you see that's the left two terms are the number of bits you need to code the data.",
                    "label": 0
                },
                {
                    "sent": "If you use the MDL estimator.",
                    "label": 0
                },
                {
                    "sent": "To code it, you first called the MD estimator and data with the MD estimator.",
                    "label": 0
                },
                {
                    "sent": "It's a two part code.",
                    "label": 0
                },
                {
                    "sent": "The right side is the number of bits you need with the best distribution in your model, so the redundancy is the extra number of bits you need compared to.",
                    "label": 0
                },
                {
                    "sent": "The best thing in your model on the data you have.",
                    "label": 0
                },
                {
                    "sent": "So this is what you get on the left, and if that thing is small then the distance.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "On the left will be smaller,",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Merge to the true distribution and how barren cover showed that if you choose your priors cleverly, then you can make the redundancy be logarithmically nenen for parametric models and be of the form, enter the gamma for gamma smaller than one.",
                    "label": 0
                },
                {
                    "sent": "For nonparametric models, which means that the left hand side actually goes to 0 so you converge to the best distribution an often at rates, which are minimax optimal.",
                    "label": 0
                },
                {
                    "sent": "As we know from statistical results.",
                    "label": 0
                },
                {
                    "sent": "So this shows that basic MDL works, same thing holds if the model is not correct but convex.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The main result.",
                    "label": 0
                },
                {
                    "sent": "We don't have this assumption anymore.",
                    "label": 0
                },
                {
                    "sent": "And if we don't have the assumption anymore than the same theorem still holds, but there's one extra term next to the redundancy, and that's the convexity leg, and that's this code length difference I've talked about before.",
                    "label": 0
                },
                {
                    "sent": "So before I looked at it as a constraint, I make it a smaller and smaller until the difference between the code.",
                    "label": 0
                },
                {
                    "sent": "If I just code, and if I take the two component mixture is about 0.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out to be easier to prove theorems to just edit to this redundancy, and then I automatically get a tradeoff between redundancy.",
                    "label": 0
                },
                {
                    "sent": "Which is decreasing if I make it a smaller.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And convexity lack which sort of redundancy gets larger Mikita smaller because you penalize more, so you need more bits to code your data, but the convexity let get smaller if Y to get smaller because either get smaller, your model becomes more like a convex model and this holds uniformly for all eater.",
                    "label": 0
                },
                {
                    "sent": "So this suggests to define the safe estimator as the MDL estimator for two overeater where eater is chosen such as to minimize the sum of the redundancy in a convexity leg.",
                    "label": 1
                },
                {
                    "sent": "Now note that every term in the sum of redundancy and convexity lack, which depends if you want to minimize it, you can just look at the data.",
                    "label": 0
                },
                {
                    "sent": "You don't need to know the true distribution, right?",
                    "label": 0
                },
                {
                    "sent": "So you can effectively minimize this.",
                    "label": 0
                },
                {
                    "sent": "This is a real.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this means that we've generalized Baron covers results, but is it useful?",
                    "label": 0
                },
                {
                    "sent": "Well, you can show that if you have an ether which is smaller than the critical eater, then actually this convexity, like term is smaller than a constant times the redundancy.",
                    "label": 0
                },
                {
                    "sent": "So then the whole thing becomes smaller than a constant times redundancy divided by N. This means that you get the same rates of convergence is with original MDL.",
                    "label": 0
                },
                {
                    "sent": "If you have the true distribution in your model, you're very lucky you get the same rates of convergence as before, but with the worst constant.",
                    "label": 0
                },
                {
                    "sent": "This may be the price you have to pay for using a procedure which still works if your model is incorrect, even though it turns out to be correct.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now the punchline.",
                    "label": 0
                },
                {
                    "sent": "We get back to classification.",
                    "label": 0
                },
                {
                    "sent": "What if you do classification with this?",
                    "label": 1
                },
                {
                    "sent": "So we do what they do in the PAC Basin literature and in the prediction with expert advice literature we have a set of classifiers.",
                    "label": 0
                },
                {
                    "sent": "Why is zero or one we met them to distributions by the E to the minus beta last transformation and we normalized by asset beta.",
                    "label": 0
                },
                {
                    "sent": "So now we get a set of distributions right instead of classifiers becomes a set of distributions and we use minimum description length here.",
                    "label": 1
                },
                {
                    "sent": "So then this generalized KL Live versions, the dystar becomes minus log E to the minus loss.",
                    "label": 0
                },
                {
                    "sent": "Divided by each new.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Minus loss, so it's actually now equal to beta, which we can choose ourselves.",
                    "label": 0
                },
                {
                    "sent": "So let's choose it to be one times.",
                    "label": 0
                },
                {
                    "sent": "The expected regret in 01 loss difference.",
                    "label": 0
                },
                {
                    "sent": "It's now the difference in 01 loss between what you get if you do two part MDL estimation with the chosen Eater and the best thing in your model which minimizes the risk of the expected 01 lesson you see, you combine it in terms of the same quantities, right?",
                    "label": 0
                },
                {
                    "sent": "The model we constructed may be completely wrong, right?",
                    "label": 1
                },
                {
                    "sent": "It's E to the minus beta.",
                    "label": 0
                },
                {
                    "sent": "Leos who knows where the data are generated in that way when we don't care because this thing doesn't need the model to be.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Correct, no matter what, P star or H we use the critical eater in this way.",
                    "label": 0
                },
                {
                    "sent": "If we do this will always be bounded by 1 / sqrt N. If we plug in 1 / sqrt N, here we get.",
                    "label": 0
                },
                {
                    "sent": "Regret bound of order minus log prior of the thing you chose divided by square root of N. If you choose eat a slightly larger, you actually get the worst case optimal thing.",
                    "label": 0
                },
                {
                    "sent": "Square root of minus lock prior.",
                    "label": 0
                },
                {
                    "sent": "So the square root is now over the minus log divided by it, and that's the worst thing.",
                    "label": 0
                },
                {
                    "sent": "The best thing you can do in the worst case.",
                    "label": 0
                },
                {
                    "sent": "This is known from a structural risk minimization like analysis.",
                    "label": 0
                },
                {
                    "sent": "Now, so this means that you can capture the worst case classification with this form of MDM base.",
                    "label": 0
                },
                {
                    "sent": "Now if I had time I would also show you that if P star satisfies at the back of condition of the form where you don't need to have the base classifier in your model, but just that the expected your wants risk is bounded by some expected of power of 01 loss.",
                    "label": 0
                },
                {
                    "sent": "Then you can actually get larger equated ecrit so critical eaters.",
                    "label": 0
                },
                {
                    "sent": "They still depend on N but not as 1 / sqrt N, but they go down to zero much slower and you get the back of optimal rates.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So All in all, I've done a lot of things here, so I've given a generalization of MDL and two part based when the model is wrong.",
                    "label": 1
                },
                {
                    "sent": "I found a kind of connection between sybok, often convexity, so there's only one similar work I know of, which is Cody Bears, PhD thesis, and actually I want to study that better in future work, but there is no explicit connection to convexity.",
                    "label": 0
                },
                {
                    "sent": "Um, finally there's a theorem I didn't talk about which connects this to pick basin type of bounce, and then you can really get something new.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Then in the future I the one thing I would like to do is make this more practical.",
                    "label": 0
                },
                {
                    "sent": "So then you can look at online versions or full Bashan.",
                    "label": 0
                },
                {
                    "sent": "Versions were used predictive rather than map distributions.",
                    "label": 0
                },
                {
                    "sent": "And note that the impromptu talk by Tim from their phone was actually about something similar because their what we tried to do was learn the ether parameter in the hedge algorithm, which is again in a way a very similar problem as this.",
                    "label": 0
                },
                {
                    "sent": "We want to determine it by the data, but in an online fashion.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Other special special cases, I mean binomial models or things like that, where computations are actually possible or simplify, just as in information theory.",
                    "label": 0
                },
                {
                    "sent": "To be honest, I haven't looked at that yet, so I don't know I.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "I'm just not sure, so the first thing that you mentioned before you got into the rates was convergence to the optimal model in your class.",
                    "label": 0
                },
                {
                    "sent": "What sort of you assume had an IID assumption?",
                    "label": 0
                },
                {
                    "sent": "Does that make that much difference, or can you still get convergence even without IID?",
                    "label": 0
                },
                {
                    "sent": "Maybe with organic or something like that, you probably can.",
                    "label": 0
                },
                {
                    "sent": "So I think the original result is rather straightforward to generalize to that, but I would have to think about what I'm doing, whether that's again I didn't.",
                    "label": 0
                },
                {
                    "sent": "It's too early.",
                    "label": 0
                },
                {
                    "sent": "I haven't thought about yet, but the original thing can be generalized, yes.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Alright, let's thank Peter again.",
                    "label": 0
                }
            ]
        }
    }
}