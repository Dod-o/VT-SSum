{
    "id": "notjdi7w3kptylnm2emy47yxaybl5ujz",
    "title": "Sparse Reinforcement Learning in High Dimensions",
    "info": {
        "author": [
            "Mohammad Ghavamzadeh, INRIA Lille - Nord Europe"
        ],
        "published": "April 25, 2012",
        "recorded": "March 2012",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/workshops2012_ghavamzadeh_sparse_learning/",
    "segmentation": [
        [
            "OK, so this is a joint project with Team SQL at INRIA Lille and Technion."
        ],
        [
            "So the part the title of the project is a sparse reinforcement.",
            "Learning in high dimensions and the participants are or group my colleague Romano Sinai and from Technion is Shimon or and this is the website of the project."
        ],
        [
            "And the project is started at the beginning of 2010 and we still continue because I have had collaboration with try before even start beginning of this project and we still continue so and we're going until the end of summer.",
            "Before so far for this Project V higher the PhD candidate from Universal British Columbia in Canada's and six months in turn or worked with us, and he participated in some of our publications and shy hired a postdoc duty caster Decastro from the technical.",
            "We didn't have any visitor of our sites, but we had four meetings at NIPS and ICML and called, so.",
            "We sat down and work together for 2, three hours each time.",
            "And of course Skype, and the same way that drivers present here this morning so."
        ],
        [
            "These are the publications that we've had from these projects related to this project we have had in 2012 and 11 papers that I, CML, and."
        ],
        [
            "22,010 nips and ECM Ellen CDC this paper is actually before the project started, but we were already thinking about this project so that the very first one, which is 2009, but it's quite related, actually to the to the project."
        ],
        [
            "So because I like to have also some formulas in the slides, and since it's 30 minutes to 45 minutes presentation, so I start with some.",
            "Description of the problem and assume that most of the audience don't know that much about reinforcement learning.",
            "So if you already know, apologize.",
            "So the problem that we study is sequential decision-making on their uncertainty, and this problem appears in human life a lot when we are moving around in the physical world, we play and try to win a game.",
            "We try to retrieve information under web, so we have a goal and we.",
            "Take some sequence of actions.",
            "We do medical diagnosis or maximize the throughput of a factory or performance of a rescue team in an area heated by a natural disaster.",
            "These are all examples of sequential decision making under."
        ],
        [
            "Uncertainty and the reinforcement learning is a class of learning problems in which an agent interacts with a dynamic stochastic and incompletely known environment, with the goal of learning a action selection strategy, or we call it policy to optimize some measure of its long term performance.",
            "And this interaction is usually modeled as Markov decision process or when the state of the system is not always completely observable by the agent as a partially observable Markov decision process.",
            "So in this research we focus on the first one, which is an easier model, which is the Markov decision process and the other thing is this term.",
            "Reinforcement learning is very much used in our community, but approximate dynamic programming is a term which is has more or less similar meaning for people in operation research and also.",
            "In control, so I may switch between these two terms in my talk.",
            "Reinforcement learning approximate dynamic programming."
        ],
        [
            "So this is the model that we are using for this interaction, which is a Markov decision process which is a 5 Cooper model.",
            "So we X is the state of the system, A is the number of actions here we can be assume it's finite, but also we can consider the infinite action set, create some difficulties, but still it's manageable.",
            "The reward function is a function over the state and actions and tells you.",
            "And gives you a real number based on the quality of your state and action related to your ultimate goal, the transition model or transition kernel is a distribution which tells you if you are state and taken action.",
            "What would be your next state.",
            "And remember this the environment is a stochastic so we have a distribution and then we have a discount factor.",
            "This is one way of defining an optimization criteria and gamma has some meaning.",
            "Some financial meaning in the sense that something in near future is more valuable.",
            "Then something in the long term, and also it's mathematically much easier to deal with, so This is why this is the more traditional way of formulation.",
            "But there are other ways and policy, which is what we are really interested in, is a mapping from the stage to actions.",
            "So this is a deterministic policy.",
            "Of course we have also stochastic policy, which is a mapping between the situation to a distribution over the actions and this is what we really want to find, and this tells us how to act in the real world.",
            "Anne."
        ],
        [
            "And of course, we're interested in good policies, but comparing the policy and say this policy is better than the other one, because policies are mapping from state to actions.",
            "So it's not possible we need to define a function based on each policy to allow us to compare to policies.",
            "And this is the value function.",
            "So this is basically shows the quality of each policy saying that if you start from a state and follow a policy, how much reward you're going to gain.",
            "At the end, in expectation because everything is stochastic, so this way we can represent if a policy which some numbers.",
            "And then with this we can compare policies and say one is better than the other and so on so forth and a little bit more technical.",
            "Is this function for a policy is the fixed point of an operator which called the Bellman operator, which basically allows us to write this function in a recursive way.",
            "So you define the value of the function at a state based on the value of its neighboring states.",
            "So this is basically the meaning of.",
            "Of this so basically, this value functions can be written on the based on the value function of the neighboring states.",
            "The states that are reachable.",
            "So the probability of reaching them is none 0.",
            "So This is why in some part of my presentation I talked about in reinforcement learning, we are dealing with potentially harder problem than then supervised learning, because sometimes you're not just dealing with regression problem, we are dealing with a fixed point problem and this is coming from here.",
            "That this function is a fixed point and an operator."
        ],
        [
            "So of course what we are interested in is not the value of any policy we are interested in the policy which has the maximum value, and that's the optimal policy.",
            "That's the goal that we would like to achieve, and if policies optimal if its value is the maximum and then we have another operator which this value function.",
            "The optimal value function is the fixed point of that operator and this operator is the bellman optimality operator which has a Max here.",
            "So the difference between this?",
            "This is a non linear operator while the."
        ],
        [
            "Other operators are linear.",
            "And this is defined based on each policy."
        ],
        [
            "One is has the optimality sense.",
            "OK, so."
        ],
        [
            "Now these two operators have very nice properties.",
            "The property is these are mono, mono, tenacity and also they have contraction with respect to the Max North.",
            "So this is the contraction property when they apply to a function then the distance gets closer and closer and the other one is the monotonicity and these are the nice properties which allows us to derive algorithms that have a convergence guarantees."
        ],
        [
            "And these algorithms are basically the basic of dynamic programming.",
            "The two algorithms value iteration which is on this slide and the value iteration is.",
            "An algorithm says that just start with an initial value function V V0 and then keep applying this nonlinear Bellman operator to this and generate a new policy.",
            "And this algorithm is going to converge because every time you apply this you make the distance.",
            "Because of the contraction property, you make the distance smaller and smaller and eventually if you continue applying this operator, you're going to converse with the optimal strategy.",
            "The second algorithm."
        ],
        [
            "Is the policy iteration algorithm the policy iteration algorithm has a I think has a better because it makes more sense than the other one in terms of artificial intelligence in the sense that it says start with the policy.",
            "With the strategy and then try to evaluate the this strategy, which means that calculate this value function.",
            "And then through a process which called gratification or policy improvement, generate a new policy with respect to the value function of the previous one.",
            "And this process guarantees that the new policy has a larger value than the previous one.",
            "So imagine that you start a new task and you start with a policy which is very naive.",
            "And then you evaluate it and you say, well, with this policy I'm going to fail, then you improve it.",
            "Then you generate a new policy again.",
            "You act in your environment, and so I'm a little better, but still too naive.",
            "And then.",
            "And eventually, because of these nice properties, both the monotonicity and the contraction of this operator.",
            "This process is also a converging process."
        ],
        [
            "So now what is the approximate dynamic programming reinforcement?",
            "Learning everything that I have told you so far is given we can calculate these operators exactly and then we can apply them to dysfunction, which is a function over the entire state space.",
            "So this application should be manageable and this function should be known.",
            "But in some scenarios the dynamic order reward function are not known, so we cannot calculate this operator exactly, we can.",
            "Only sample from this operator or the status space of the system is very large or infinite, which even if we have this operator, we cannot afford to apply it to the value function over the entire state space.",
            "And This is why we need to use approximation schemes for the value function and use generalization and so on and so forth."
        ],
        [
            "And now we can see that all the noise properties, contraction and we're not tenacity, and everything that we had for these algorithms is given.",
            "The fact that you can calculate this operator exactly and you can apply it exactly to the value function.",
            "If you cannot do it.",
            "Then you cannot guarantee.",
            "That you have this contraction and all the nice properties are going to go away.",
            "This is for value function."
        ],
        [
            "For policy iteration, you cannot guarantee that the new policy that you generate has the has a higher value than the previous one.",
            "And that's the whole property that you could guarantee the convergence of your algorithm, so you can no longer guarantee all these things and the whole research and.",
            "Exciting research and basically challenges for this problem that start right here.",
            "Then you lose all the nice properties of these operators."
        ],
        [
            "OK, and of course these algorithms approximate dynamic programming and reinforcement learning algorithms scale badly with the dimension of the state space, because if you imagine now we want to use approximation schemes and the higher the dimension of the state of space we need more features to represent the value function, and we have usually dealing with two curses of dimensionality.",
            "One is the exponential growth in the number of samples that we need in order to guarantee some sort of.",
            "Performance.",
            "And the other one is the exponential growth in the in the computation.",
            "So we have the both computational curves and the statistical curves of."
        ],
        [
            "Of dimensionality.",
            "So now what was the motivation of of this joint project?",
            "One motivation from the cognitive science was so human being also learn an act using very complex and high dimensional observations, but they are amazingly well in ignoring most of the observed data with almost no perceptual loss.",
            "So can we design algorithms to to have this sort of performance or not?",
            "This is from the cognitive point of view.",
            "The other way is now with this revolution of data we have systems that generate and work with very rich high dimensional data, and in order to have control where we have to deal with this sort of data so the data are getting bigger, more complex and reach the third motivation is.",
            "Reinforced this reinforcement learning methods are not good with the non Markovian systems and non Markovian means.",
            "When you are in a state and you take an action, knowing what will happen next to the system depends not only on the current state, but on the history of the system.",
            "And OK, so one way of.",
            "Changing a non Markovian system to a Markovian system is by adding more state variables.",
            "Two to the system, so it makes it larger in terms of dimension and the 4th one is now we have some recent advances in handling high dimensional data in a statistical machine learning in supervised learning and semi supervised learning and also in the field of compressed sensing and the goal of this project was to see whether we can bring these mathematical formulations and tools.",
            "Two sequential decision-making and develop and analyze algorithms for high dimensional.",
            "Date."
        ],
        [
            "And this is basically the main objective that I mentioned in the previous slide to find appropriate representation for this value function approximation in high dimensional problems and when they talk about high dimensional problems, we're usually talking about the setting in which the number of samples that you have is either smaller or at the order of the number of features that you consider for your problem and also to use them to develop efficient reinforcement learning algorithm.",
            "And by efficient I mean algorithm.",
            "Could sample and computational complexity do not grow rapidly in an exponential way with the dimension of the observation?",
            "Because if this happens, then your algorithm is going to end.",
            "But we want algorithms that their computation and sample complexity grows in a slower rate with respect to the dimension."
        ],
        [
            "So now we're going back again to the problem of feature selection, which is a problem known to everybody in machine learning in many areas, are machine learning, direct reinforcement learning.",
            "So we look at this problem and we divided this problem to two different approaches and the approach taken by people at Technion was to start with the feature space for the value function approximation and trying to dynamically.",
            "Adapt and change this feature space in order to perform well in that environment.",
            "When that problem so imagine that you start with some parametric feature space and then try to use some sort of loss in order to optimize these parameters.",
            "The approach that we considered it in real was.",
            "Let's imagine that we just threw as much as features that we have.",
            "And create a very huge high dimensional feature space in the hope that the good features lie in that set and then hopefully we have an algorithm which can clean up this big mess and find the the right information.",
            "Election or future construction.",
            "So this one is more in terms of construction because you start with a fixed set of features so you don't change the number of features, but you change the form of the features.",
            "So you can imagine that you change the Gaussian location or Gaussian with and these sort of things.",
            "This one is more.",
            "This one is also you create a very huge set of features and then you try to have an algorithm which deal with this huge problem but somehow find or act according to but at the end for us because our for us in enforcement learning the goal is prediction.",
            "So if you have a good predictive performance, I don't want to explain you which features and have a feature interpretation, which sometimes is important in supervised learning.",
            "For us the performance was the prediction performance, but some you are right.",
            "Sometimes we need also to explain.",
            "In terms of.",
            "Between features and states, so feature is a nonlinear function over states, so states are infinite.",
            "Now.",
            "Now we replace them with finite number of features.",
            "Let's say you define Gaussians over the state space and you place these Gaussians.",
            "In different part of the state space and you can imagine in order to cover your state space when your dimension of the space gets bigger and bigger, the number of discussions that you should put it.",
            "Yes, yes it can be a nonparametric features can be parametric features.",
            "These are so.",
            "Oh OK, so I think it meant yes.",
            "So this is basically these two approaches."
        ],
        [
            "One slide that I have here.",
            "I want to again emphasize what I've mentioned earlier.",
            "That value function approximation in reinforcement learning is more than just recovering a target function from its noisy observation, which is regression, and it's a standard problem in supervised learning in a lot of algorithms in reinforcement learning we are dealing with approximating the fixed point of this bellman operator given sample trajectories.",
            "So it's not necessarily a regression problem.",
            "It might be fixed point problem, so this creates some difficulties in applying this standard supervised learning methods to this problem.",
            "So I want you to distinguish these judges when we say bringing something from supervised learning to regression.",
            "It's not just like OK, you have a good regression algorithm, you bring it here and it works in necessarily well in decision making."
        ],
        [
            "OK, so this is the work that was done by group at Technion.",
            "They, as I mentioned this is the dynamically changing the features while interacting with the environment and they have this set of publications related to this ICM.",
            "Ellen CDC and CML."
        ],
        [
            "And I just very briefly summarized two of these publications, which is the adaptive basis for reinforcement learning.",
            "And for this well known Q learning algorithms, reinforcement learning, if you are familiar, this is one of the well known algorithm in."
        ],
        [
            "Laurel and as I mentioned here, you imagine reinforcement learning function approximation and then you have some basis functions and you want to change them.",
            "The main important thing is you change these bases according to what you want to optimize, what and these are the three errors that people there are examined.",
            "One is the approximation square error between the value function, the other one is called the Bellman residual, which is the difference between the application of the bellman operator to a function.",
            "And the function because you know if the function is the fixed point, the application of the bellman operator should be equal to the function.",
            "But since it's not the fixed point we're looking for, the first point difference between these two is called the Bellman residual answer.",
            "Very important error that is studied in reinforcement learning, which is related again to the fixed point property and the third one is the projected Bellman residual which I'm in order to explain it.",
            "I need to go a little bit more deep into the.",
            "Problem and then they implemented this in the form of an actor, critic algorithm and actor critic.",
            "Algorithm is a very cognitive inspired algorithm in the sense that you have a controller and then you have an approximate are so approximator is like a predictor and its goal is to approximate the value function of strategy.",
            "Basically tells you how well is your current strategy is and the actor is in charge of control.",
            "So basically come consult.",
            "With the predictor, get some information and then decide what to do next.",
            "So they implemented this algorithm in this way and they shown the convergence with probability one day.",
            "Synthetic convergence of these algorithms and.",
            "This is the summary of two of."
        ],
        [
            "Work done by the group attacking it.",
            "So now let's go back to what we did at INRIA.",
            "As I mentioned, we look at the problem that we put a lot of features there and then we're trying to.",
            "To design algorithms with the hope that the right features are there.",
            "So we create a huge feature space with the hope that the right features are like there and we want to have algorithms which can deal with this.",
            "So the immediately the first question is OK, we're going to have overfitting.",
            "You're going to be in trouble, but we know that in regression and people have have dealt with this problem.",
            "So two approach that we have studied, one is based on random projections, basically random.",
            "Project your high dimensional feature space to a randomly generated low dimensional space.",
            "Do your computation there with the hope that your final solution is good enough.",
            "With respect to the calculation in high dimensional space and the second one is using the irregularities in the problem and irregularities that probably you have all heard is smoothness, which is basically equivalent by using L2 penalization or rich regression in supervised learning and the other one is the sparsity which is the L1 penalized methods and the loss of problems.",
            "So these are the two types of regularity's that we can look.",
            "But there is a very subtle difference between these two irregularities which is important to look at.",
            "Smoothness is a property of the problem, so you have a value function is whether smooth or not.",
            "But the sparsity is not only.",
            "A property of the problem is a property of the problem and representation, because before you define your representation, your feature space, we cannot say that this value functions are smooth, is sparse, sparse in what?",
            "Sparse is in wavelets.",
            "Parts in what?",
            "So this there is a subtle difference between these two term types of regularity's in in our problems.",
            "Turning the representation as well here in our city is that correct?",
            "No, we start with a huge representation is very high dimensional and we hope OK, so this is too big.",
            "Probably the relevant features are only 10 of them and we want an algorithm which can deal with this huge feature.",
            "Yes yes yes yes.",
            "You are doing sparsity.",
            "Only you would do L zero kind of L1.",
            "Those both smoothness plus bar city, right?",
            "That's that's basically a pseudo.",
            "Replacement for the N 0 because it has nice properties at zero.",
            "Dealing with zero is difficult, so it's basically like.",
            "What is the right word for it its surrogate?",
            "Is there is a surrogate for?",
            "That's true, that's true.",
            "That's true, that's true.",
            "That's true."
        ],
        [
            "OK, so related to the first approach, which is related to random projections and bringing techniques from compressive sensing's.",
            "These are the publications that we've had.",
            "I try to present."
        ],
        [
            "A couple of them.",
            "The first one is compressed least square regression.",
            "So."
        ],
        [
            "Whole idea is you have a feature space of dimension Capital D and this capital D is potentially very large, so the number of samples that you have is smaller than this D, so you are doomed to overfitting.",
            "And then we want to project this large feature space to a smaller randomly generated features which with the conditions the comprehensive conditions not any random projections you know.",
            "So we have some properties here.",
            "It's not just that this can be generated from.",
            "From this, with any random projection, so it has some properties so, but these are the linear combination of these features using the random coefficients and then we want to perform our algorithm.",
            "The least squares.",
            "Basically in this space and see what happens.",
            "So in terms of theory there are, you know that every time that we try to do a deal with the regression you have two sorts of error.",
            "One is the approximation error which is related to how limited is your space.",
            "Expect your target.",
            "If your target is outside your space, the distance between your target and your space is your approximation error and estimation error is the error of finding the image of your target in your space and how close you can get to that image.",
            "Which is the best you can do so there is always like a tradeoff here that if you make your space big then this approximation error is going to go to 0.",
            "But then you're going to have a very difficult search problem 'cause you have a very huge state space, and then you need a lot of samples in order to get small estimation error.",
            "And if you do the other way, it make the space is small, then your approximation or goes up but your estimation error is going to go down.",
            "So it's basically a tradeoff between these two and here you can see that by this projection when you solve your problem here you are dealing with a smaller space or approximations is smaller, but your estimation error is going to go up.",
            "Because I'm sorry the other way.",
            "So when you're solving the problem.",
            "When you are solving the problem.",
            "When you're solving the problem in the low dimensional space, so your approximation result is larger because now you have a smaller space, but your estimation error is smaller because now you have a easier search problem to solve and the analysis in this paper basically shows that now I decompose the error of this problem in this space to an approximation error to do estimation error of this space which is smaller than the other one, plus the approximation error.",
            "Of this space plus an extra term.",
            "So we replace the approximation error here, which is bigger than approximation right there by the approximation error here.",
            "Plus on a small tear and we want to see whether this is small term can be controlled or not.",
            "If it can be controlled with respect to the estimation error, we win by doing this process, otherwise we don't.",
            "And we show in this work that for specific features that are in the form of wavelets and their specific spaces, you can always guarantee that this term can be controlled.",
            "And why control means that this capital didn't appear in it because it disappears.",
            "Then you're going to be in trouble.",
            "And you're going to gain.",
            "But if you choose any features then at least you don't have the guarantee.",
            "And by optimizing the bound we get the results actually suggested by people in compressed sensing that this D should be at the order of root square of N. So the low dimension that you choose, so it's not just any projection you can project from 1000 dimensional space is 1 dimensional space.",
            "It should be some properties for this lower dimension.",
            "Then you can get a nice rate in terms of regression.",
            "Yes, do you.",
            "Is the fact that the projection is random condition?",
            "Or you could also do a semi supervised or no no no here.",
            "You basically use this.",
            "Yes yes it is.",
            "Yes yes yes yes.",
            "So we are using the Johnson Lindenstrauss theorem here, so you have to fulfill the requirements of bacteria in order for this to go through."
        ],
        [
            "The next one is bringing this to this LCD least square temporal difference learning algorithms.",
            "Popular algorithm in reinforcement learning, and this is a fixed point problem."
        ],
        [
            "The idea is very similar to what I explained in terms of regression, but this is for a fixed point problem, so we have the reinforcement in high dimensional space.",
            "The number of features is bigger than the number of samples and then we have overfitting and poor prediction performance.",
            "So what can we do?",
            "We can use the regularization term, but here we don't look at this approach.",
            "We look at the random projection approach and then we design an algorithm which call it the LSD with random projections, which solves the LCD problem.",
            "In a randomly generated low dimensional space from the high dimension."
        ],
        [
            "And we've been in this work.",
            "We present algorithm discuss it's complete computational complexity, and we show that it's less expensive than performing the algorithm in high dimension.",
            "We provide finite sample performance bound for this algorithm for two different settings.",
            "The Markov design setting is.",
            "I don't go to the details.",
            "Imagine it's like a deterministic design.",
            "You want the performance to be good on the samples used by your algorithm.",
            "So you just evaluate the performance of our algorithm.",
            "At the points that you used used by your algorithm and then the random design is when we generalize the performance bound here to any data that can be sampled using your measure over your entire state space.",
            "And again, the the theoretical issue is the same thing that you.",
            "By doing this you have a smaller estimation error, bigger approximation error, how big.",
            "And whether this we can gain something or not.",
            "And this is the theoretical."
        ],
        [
            "Comparison the other issue that is important here is because it's a fixed point problem.",
            "Is the uniqueness of the solution in the low dimensional space.",
            "So we need to guarantee that if we assume that the LSD solution in high dimension exists, how many samples we need in the low dimension to guarantee that we have a unique solution.",
            "So, and it really relates to the gramian matrix of the problem.",
            "Basically, smallest eigenvalue of the of this problem in the high dimension.",
            "This is the model based case when you know the model and if that solution exists.",
            "For the low dimension, how many samples you need to generate.",
            "And of course, if the gram matrix here has a small eigenvalue, you need more to generate more samples.",
            "Here in order to guarantee the uniqueness of the solution.",
            "So this is.",
            "One analysis that we need to show that it's we have a unique solution and then all these reinforcement learning algorithms in addition to solving the honest problem, which is the calculating of the value function or a fixed point, this sequential problem.",
            "All those dynamic programming algorithms are iterative algorithms.",
            "So you have an error at each iteration and then you repeat this process.",
            "So in all these algorithms the analysis has two phases.",
            "One phase is very similar to supervised learning.",
            "In the sense that you have a target function or you have a fixed point and you want to see how much error you have, and then you have to repeat this process until convergence and then you have to show how this error is propagated through the iterations of the algorithm and what would be the final performance.",
            "So this is the last part of the analysis which brings everything together."
        ],
        [
            "The this is a recent work by my colleague Romanos and his student Alessandro Conti and it's."
        ],
        [
            "And it's not problem.",
            "So we the idea is you have a linear bandit problem and.",
            "You have an action set is attached to that you have a very high dimension K and which is larger than your budget.",
            "And but we know that this function is.",
            "This data is sparse and of course we assume that this is on the unit ball, because otherwise I think they don't know how to deal with this problem and the algorithm is a combination with the idea of compressed sensing with the bandit theory and this confidence ellipsoid.",
            "Algorithms for linear bandit and then they derive a regret bound or in the form of end with the number of samples or the budget and the sparsity.",
            "So the K which is the nasty term which is large, doesn't appear in the inbound.",
            "So."
        ],
        [
            "Now let's go to the second approach.",
            "The first approach, as we said, this random projection.",
            "The second approach that we studied is using sparsity in the value function approximation, and we have these two work, which one of them is an algorithmic work and the other one is more theoretical I explained."
        ],
        [
            "The theoretical work, which is the final sample analysis of lost so TD."
        ],
        [
            "So what we have done in this work is we define this LASSO TD algorithm, which is a modification of that popular STD algorithm in which you have to replace orthogonal projection with an L1 projection.",
            "So that's the change in this algorithm.",
            "So LSD becomes a LASSO TD, so we show first that this algorithm guarantees to have a unique fixed point.",
            "Then we showed that two existing algorithms for solving reinforcement learning, one designed by Andrew Ingin Zico Kolter at Stanford and the other one is by group at Duke.",
            "Are actually solving this problem because these are algorithmic paper and there are algorithms that solve this problem.",
            "So whatever analysis we have for this loss or TD is basically analysis for these two existing algorithms.",
            "And then we calculate the two performance bounds for the last 40 algorithm in the Markov design setting and Markov design setting is imaginative deterministic design setting in the sense that you evaluate your poor performance at the points that you use.",
            "That are used by your algorithm and the Markov design is coming from the fact that these points are not IID.",
            "They are generated according to a trajectory generated by Markov chain.",
            "We calculate 2 bounds and these results actually came after a very beautiful tutorial at NIPS 2010 by Peter Buhlmann, which is a statistician at IHC.",
            "If I'm not mistaken, and because if you look at this community there statistician, they developed hundreds of papers trying to find bound.",
            "For the last two algorithm and it's really hard to understand what is the assumption behind each of these results.",
            "The beauty of 1 paper published by Bullman and sort of under gear which they also talked about it in this is they basically came and sorted all these assumptions and showed which assumption is more general than the other, which implies the other an for the three important tasks that you have in high dimensional statistics, which is prediction finding the weights.",
            "And feature discovery.",
            "What is the necessary condition that you can have a reasonable bound?",
            "So this basically makes the life much simpler and gives you a road map of looking at those hundred papers generated by statistician, which is really hard to understand whether the assumptions.",
            "So we used their ideas there.",
            "We have two bonds for our algorithm.",
            "One bound is the no assumption bound, so we make no assumption but has a slow rate.",
            "And when I say slow rate is.",
            "We're talking about linear function approximation and in linear regression the rate is one over square open, but we get one over into the power of 1/4.",
            "This is why we call this slow rate.",
            "Bound and also the other problem is the normal one.",
            "Of AA is the is the weight vector of the image of your target in your space and the norm one of Alpha appears.",
            "But you want to appear is the sparsity, which is the norm zero of Alpha, the second bound which comes at the cost of a compatibility assumption, which is the most relaxed assumption exist for prediction in high dimensional statistics?",
            "I'm not going to talk about is the mathematical assumption on the gram matrix of the empirical gram matrix.",
            "So you can.",
            "You can also call it the L1 eigenvalue assumption, which is just because there is an L2 eigenvalue assumption which is mathematical, meaning he's had some similarity with that, they just call it L1 value assumption.",
            "If you want, we can talk about this assumption later.",
            "Using this assumption we get a fast rate plus we get this sparsity to appear, but you need to guarantee that your representation has that assumption.",
            "What?",
            "Spectrum in so things will I do you get anything on this one?",
            "So.",
            "So what we have observed in analyzing."
        ],
        [
            "Sorry in analyzing.",
            "Reinforcement learning algorithms is actually weaken in terms of rate.",
            "We get what we get in regression.",
            "Right, so now the immediate question is, so is the sequential decision making as hard as hard as the regression or not?",
            "But there are terms that appear in our bound here which is related to that propagation of error and independent to exploration, and it depends on how you explore your space.",
            "There are some change of measure appears in the in this bound which depends on the measure that you want to evaluate your performance at the end and the measure that you use in your sampling.",
            "Which can be very nasty are if there is a bad mismatch between these two.",
            "This is the related directly.",
            "The problem of exploration.",
            "And the other one which is the iteration problem is related to control because in control you do something you get in the feedback.",
            "Then you change it.",
            "So these are the things that if you look at the rate in terms of the number of samples, you may say that OK.",
            "The problem is as easy as regression because you get the optimal rates in most of the analysis.",
            "But there are other terms which are related to the control aspect and the exploration aspect which makes the problem in my opinion extremely harder."
        ],
        [
            "OK, and so just let me just summarize what I think as a partner in this project was our achievement.",
            "So we developed and analyzed because, OK, the goal of priming was if you have a crazy idea and you don't know whether it works or not.",
            "Just write a proposal.",
            "They give you some money, play with it for a year and a half, and if you think it's worth wide, continue it and apply it to the higher scale projects, whether in your country or or at.",
            "European level so for us was important to at least create a mathematical backbone and some algorithms for this kind of problems, because you may say OK, these are bounds that you show are in the form of hyper upper bounds, and we don't know how tight are these ones.",
            "These are the if I look at the reviews that I get from my papers at conferences.",
            "That's at least one of the reviewers is going to write something like this.",
            "I mean, I agree these are not the bounds that I look at this bound and I can develop an algorithm, but imagine that you have an algorithm and you want to tune this algorithm to work well and you have a couple of parameters to deal with.",
            "So one way is OK if you don't have anything you should blindly just play with these parameters.",
            "But if you have a bound by looking at the interaction between these parameters in the form of your upper bound, at least it gives you some idea how to tune your algorithm.",
            "And or and also gives you a better understanding of what's happening in these algorithms in terms of choosing your function space and so on so forth.",
            "So for us it was important to develop and analyze value function approximation methods for this setting, which is the high dimensional reinforcement learning problem when you have samples smaller than the features.",
            "So we took two approaches, one by Technion, which is a dynamic changing of the feature space, one with a switch just blow up the feature space in the hope to.",
            "Two for your right features to lie there, and then we look at two different ways, one inspired by the compress compressive sensing community and progress and the other one is related to a sparsity.",
            "And now we have a set of purity confounded algorithms, which is, in my opinion is good."
        ],
        [
            "And then what we can do next?",
            "So what we would like to do is we would like to have adaptive algorithms and by adaptive algorithm I mean an algorithm that can automatically takes advantage of the regularity's in the problem.",
            "So the two regularity's that have been studied one is Delta regularity.",
            "The smoothness which is has been studied both algorithmically and theoretically in the concept of control and reinforcement learning.",
            "So this is the work that I started with my colleague Farahmand.",
            "So this is the beginning of our work, which is back when we were all in Canada.",
            "So we started this work there and we developed and analyzed the L2 version of three important reinforcement learning algorithm and had an analysis for them.",
            "Then we came to the L2 station or sparsity, and what existed before this project was just a couple of algorithms, which was this LSD by people at Stanford and this one by people at Duke.",
            "And this one was what people are in our group, which was a TD type of algorithm.",
            "So now we add the theoretical results.",
            "Here we have the final sample analysis of these two algorithms.",
            "We have also some algorithm which call it into L1L STD's more algorithmic, and we have some new results which are in pipe.",
            "And this is another class of algorithms that we haven't studied.",
            "This algorithmic is a linear programming approach with L1 regularization, which I haven't studied it, it's by.",
            "This right now is at IBM, but."
        ],
        [
            "So now studying other form of regularity is what we can do next.",
            "Studying other form of regularity in sequential decision making.",
            "One of them is the manifold activity.",
            "If we know that the value function that we are looking for is.",
            "Is basically sitting on a low dimensional manifold.",
            "How we can take advantage of this?",
            "Basically just the dimension of that manifold appears in our in our bound instead of, so this is one thing that I haven't still worked on it.",
            "Then the new one that we have recently started looking at, it is called the action gap regularity and it's related to the margin because in reinforcement learning what you would like to do is you have a policy and you want to find the action.",
            "That at each state you want to find the action that has the highest value function with respect to the current policy, and then design a new policy which takes that a specific action in that space.",
            "So this is like a classification problem in the sense that you have.",
            "Let's say you have three actions.",
            "For each action you have.",
            "You have some estimate of this action value function, and you want the output to gives you the action that maximizes the true action value function.",
            "So now the question is if you have two actions which have very value functions that are very close to each other.",
            "Then the margin is very small.",
            "The gap is small and discriminating between these two requires a lot of budget.",
            "But if you have a problem that can guarantee that there is a specific margin is.",
            "I'm not an expert in classification, but if it has a very nice connection with what happens in classification that you have margin, you have a similar classification problem, so it's a regularity of the problem and whether we can take advantage of it or not.",
            "This is a sort of regularity that we recently started started looking at it, and I'm also happy to say that, OK, at least I can include.",
            "I could include a part of this idea of.",
            "Studying regularity in in sequential decision making in two European project that is complex and it got funded last year.",
            "So in one of the work packages which is related to reinforcement learning, one of the tasks that we are doing is studying the regularity in sequential decision making problem.",
            "So I think we have fulfilled a little bit the goal of pump priming to to do something next."
        ],
        [
            "The future work is, again develop algorithms based on the existing and new principle.",
            "For high dimensional methods.",
            "Study other form of regularity's that I mentioned.",
            "The other question is when we have approximated regularity's, so we are not exactly in a low dimensional manifold, but your coastal dimensional manifold can.",
            "We can be design algorithms that can give us something, so this is an interesting problem which might be even related to robust control and stuff that we haven't.",
            "We haven't touched it and the real world application people are taking on.",
            "They have a very beautiful dogfight simulator is a flight simulator that they want to run some.",
            "The algorithm and work on it, so I know that she has started looking at it.",
            "He has a PhD student working on this simulator, so it is a very good.",
            "Platform for studying high dimensional complex system for us.",
            "We are trying to look at problem in network optimization and traffic control.",
            "Depends on if the funding is going to be provided.",
            "We are going to work on this problem and it was another problem that we attempted to work on.",
            "It wasn't refinery optimization which debuted the beautiful aspect in this problem was you have a bunch of can control systems that are cascaded.",
            "The output of one is the input of the other one and so on and so forth.",
            "So it's a very interesting problem, but unfortunately again funded.",
            "So if it gets funded, maybe we're going to work on it, so these are some future."
        ],
        [
            "Application I hope I didn't go out of time.",
            "Yes, thank you.",
            "You mentioned MVP at the beginning of those applications.",
            "Would you think that the user is a form of MVP?",
            "The application you can you can you can make it more Markovian and use MVP methods, yes.",
            "Depends on the task that you want to define in the flight simulator.",
            "This plan for this pump priming project.",
            "I mean you you mentioned the MVP's as one of the framework of the method that you want to make to generalize to higher dimension of the state space, right?",
            "But have you actually run an MVP following this or you have the enabling tools?",
            "I mean so every as I mentioned at the beginning, the MDP and palm DP or just the mathematical model for this decision making and all the analysis that we have here an all the?",
            "Experiments that we have run so far is based on the MDP model, when we know the environment is Markov and the state is observable.",
            "But if you remember, sorry, but one more thing if you remember one of the motivation is when you have a non Markovian problem we can make it more cost by adding more state variables and making the dimension bigger, but we have turned into an MVP problem.",
            "Understand his MVP is what people do.",
            "People in the application side use like the dialogue people when they do reinforcement learning the language.",
            "Actually they always talk about on DP right palm DP or MDP mostly so palm DP is much harder to control and deal with.",
            "Yes so it's.",
            "I was trying to visualize if there is an example of some people doing MDP and thanks to your techniques they could kind of expand the space state space and we got actually are kind of going to get actual results on that.",
            "Again, MVP or Pompey.",
            "So MDP to everything here was MVP.",
            "All these algorithms is based on when we have the Markovian assumption.",
            "When the state is observable we have all these nice theoretical results and algorithm.",
            "Now the question is if we go to Palm DP and the state is not observable, then we don't have that much.",
            "That much theory actually for palm DP.",
            "And if you look at the current results in Palm DP level or the size of the problems that they are dealing with are very small.",
            "So this difficult problem.",
            "And I wouldn't use the value function approximation for those methods.",
            "I definitely go for the policy search things that I don't know if the young Peters talked about it this morning or not, but these are the type of things that are more suited for partial observable methods.",
            "And in terms of, uh, we're currently continue working on this, with which I have had collaboration with try since 2008, and we keep going and.",
            "Send thanks to the pump priming.",
            "We can still continue until the end of September, so I think whatever is left we're going to use.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is a joint project with Team SQL at INRIA Lille and Technion.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the part the title of the project is a sparse reinforcement.",
                    "label": 0
                },
                {
                    "sent": "Learning in high dimensions and the participants are or group my colleague Romano Sinai and from Technion is Shimon or and this is the website of the project.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the project is started at the beginning of 2010 and we still continue because I have had collaboration with try before even start beginning of this project and we still continue so and we're going until the end of summer.",
                    "label": 0
                },
                {
                    "sent": "Before so far for this Project V higher the PhD candidate from Universal British Columbia in Canada's and six months in turn or worked with us, and he participated in some of our publications and shy hired a postdoc duty caster Decastro from the technical.",
                    "label": 0
                },
                {
                    "sent": "We didn't have any visitor of our sites, but we had four meetings at NIPS and ICML and called, so.",
                    "label": 1
                },
                {
                    "sent": "We sat down and work together for 2, three hours each time.",
                    "label": 0
                },
                {
                    "sent": "And of course Skype, and the same way that drivers present here this morning so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are the publications that we've had from these projects related to this project we have had in 2012 and 11 papers that I, CML, and.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "22,010 nips and ECM Ellen CDC this paper is actually before the project started, but we were already thinking about this project so that the very first one, which is 2009, but it's quite related, actually to the to the project.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So because I like to have also some formulas in the slides, and since it's 30 minutes to 45 minutes presentation, so I start with some.",
                    "label": 0
                },
                {
                    "sent": "Description of the problem and assume that most of the audience don't know that much about reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So if you already know, apologize.",
                    "label": 0
                },
                {
                    "sent": "So the problem that we study is sequential decision-making on their uncertainty, and this problem appears in human life a lot when we are moving around in the physical world, we play and try to win a game.",
                    "label": 1
                },
                {
                    "sent": "We try to retrieve information under web, so we have a goal and we.",
                    "label": 0
                },
                {
                    "sent": "Take some sequence of actions.",
                    "label": 0
                },
                {
                    "sent": "We do medical diagnosis or maximize the throughput of a factory or performance of a rescue team in an area heated by a natural disaster.",
                    "label": 1
                },
                {
                    "sent": "These are all examples of sequential decision making under.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Uncertainty and the reinforcement learning is a class of learning problems in which an agent interacts with a dynamic stochastic and incompletely known environment, with the goal of learning a action selection strategy, or we call it policy to optimize some measure of its long term performance.",
                    "label": 1
                },
                {
                    "sent": "And this interaction is usually modeled as Markov decision process or when the state of the system is not always completely observable by the agent as a partially observable Markov decision process.",
                    "label": 0
                },
                {
                    "sent": "So in this research we focus on the first one, which is an easier model, which is the Markov decision process and the other thing is this term.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning is very much used in our community, but approximate dynamic programming is a term which is has more or less similar meaning for people in operation research and also.",
                    "label": 0
                },
                {
                    "sent": "In control, so I may switch between these two terms in my talk.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning approximate dynamic programming.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the model that we are using for this interaction, which is a Markov decision process which is a 5 Cooper model.",
                    "label": 1
                },
                {
                    "sent": "So we X is the state of the system, A is the number of actions here we can be assume it's finite, but also we can consider the infinite action set, create some difficulties, but still it's manageable.",
                    "label": 0
                },
                {
                    "sent": "The reward function is a function over the state and actions and tells you.",
                    "label": 1
                },
                {
                    "sent": "And gives you a real number based on the quality of your state and action related to your ultimate goal, the transition model or transition kernel is a distribution which tells you if you are state and taken action.",
                    "label": 0
                },
                {
                    "sent": "What would be your next state.",
                    "label": 1
                },
                {
                    "sent": "And remember this the environment is a stochastic so we have a distribution and then we have a discount factor.",
                    "label": 0
                },
                {
                    "sent": "This is one way of defining an optimization criteria and gamma has some meaning.",
                    "label": 0
                },
                {
                    "sent": "Some financial meaning in the sense that something in near future is more valuable.",
                    "label": 1
                },
                {
                    "sent": "Then something in the long term, and also it's mathematically much easier to deal with, so This is why this is the more traditional way of formulation.",
                    "label": 0
                },
                {
                    "sent": "But there are other ways and policy, which is what we are really interested in, is a mapping from the stage to actions.",
                    "label": 0
                },
                {
                    "sent": "So this is a deterministic policy.",
                    "label": 0
                },
                {
                    "sent": "Of course we have also stochastic policy, which is a mapping between the situation to a distribution over the actions and this is what we really want to find, and this tells us how to act in the real world.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of course, we're interested in good policies, but comparing the policy and say this policy is better than the other one, because policies are mapping from state to actions.",
                    "label": 0
                },
                {
                    "sent": "So it's not possible we need to define a function based on each policy to allow us to compare to policies.",
                    "label": 0
                },
                {
                    "sent": "And this is the value function.",
                    "label": 0
                },
                {
                    "sent": "So this is basically shows the quality of each policy saying that if you start from a state and follow a policy, how much reward you're going to gain.",
                    "label": 0
                },
                {
                    "sent": "At the end, in expectation because everything is stochastic, so this way we can represent if a policy which some numbers.",
                    "label": 0
                },
                {
                    "sent": "And then with this we can compare policies and say one is better than the other and so on so forth and a little bit more technical.",
                    "label": 0
                },
                {
                    "sent": "Is this function for a policy is the fixed point of an operator which called the Bellman operator, which basically allows us to write this function in a recursive way.",
                    "label": 1
                },
                {
                    "sent": "So you define the value of the function at a state based on the value of its neighboring states.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the meaning of.",
                    "label": 0
                },
                {
                    "sent": "Of this so basically, this value functions can be written on the based on the value function of the neighboring states.",
                    "label": 0
                },
                {
                    "sent": "The states that are reachable.",
                    "label": 0
                },
                {
                    "sent": "So the probability of reaching them is none 0.",
                    "label": 0
                },
                {
                    "sent": "So This is why in some part of my presentation I talked about in reinforcement learning, we are dealing with potentially harder problem than then supervised learning, because sometimes you're not just dealing with regression problem, we are dealing with a fixed point problem and this is coming from here.",
                    "label": 0
                },
                {
                    "sent": "That this function is a fixed point and an operator.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So of course what we are interested in is not the value of any policy we are interested in the policy which has the maximum value, and that's the optimal policy.",
                    "label": 0
                },
                {
                    "sent": "That's the goal that we would like to achieve, and if policies optimal if its value is the maximum and then we have another operator which this value function.",
                    "label": 0
                },
                {
                    "sent": "The optimal value function is the fixed point of that operator and this operator is the bellman optimality operator which has a Max here.",
                    "label": 1
                },
                {
                    "sent": "So the difference between this?",
                    "label": 0
                },
                {
                    "sent": "This is a non linear operator while the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other operators are linear.",
                    "label": 0
                },
                {
                    "sent": "And this is defined based on each policy.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One is has the optimality sense.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now these two operators have very nice properties.",
                    "label": 0
                },
                {
                    "sent": "The property is these are mono, mono, tenacity and also they have contraction with respect to the Max North.",
                    "label": 0
                },
                {
                    "sent": "So this is the contraction property when they apply to a function then the distance gets closer and closer and the other one is the monotonicity and these are the nice properties which allows us to derive algorithms that have a convergence guarantees.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these algorithms are basically the basic of dynamic programming.",
                    "label": 1
                },
                {
                    "sent": "The two algorithms value iteration which is on this slide and the value iteration is.",
                    "label": 1
                },
                {
                    "sent": "An algorithm says that just start with an initial value function V V0 and then keep applying this nonlinear Bellman operator to this and generate a new policy.",
                    "label": 0
                },
                {
                    "sent": "And this algorithm is going to converge because every time you apply this you make the distance.",
                    "label": 0
                },
                {
                    "sent": "Because of the contraction property, you make the distance smaller and smaller and eventually if you continue applying this operator, you're going to converse with the optimal strategy.",
                    "label": 0
                },
                {
                    "sent": "The second algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is the policy iteration algorithm the policy iteration algorithm has a I think has a better because it makes more sense than the other one in terms of artificial intelligence in the sense that it says start with the policy.",
                    "label": 1
                },
                {
                    "sent": "With the strategy and then try to evaluate the this strategy, which means that calculate this value function.",
                    "label": 1
                },
                {
                    "sent": "And then through a process which called gratification or policy improvement, generate a new policy with respect to the value function of the previous one.",
                    "label": 0
                },
                {
                    "sent": "And this process guarantees that the new policy has a larger value than the previous one.",
                    "label": 0
                },
                {
                    "sent": "So imagine that you start a new task and you start with a policy which is very naive.",
                    "label": 0
                },
                {
                    "sent": "And then you evaluate it and you say, well, with this policy I'm going to fail, then you improve it.",
                    "label": 0
                },
                {
                    "sent": "Then you generate a new policy again.",
                    "label": 0
                },
                {
                    "sent": "You act in your environment, and so I'm a little better, but still too naive.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "And eventually, because of these nice properties, both the monotonicity and the contraction of this operator.",
                    "label": 0
                },
                {
                    "sent": "This process is also a converging process.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now what is the approximate dynamic programming reinforcement?",
                    "label": 1
                },
                {
                    "sent": "Learning everything that I have told you so far is given we can calculate these operators exactly and then we can apply them to dysfunction, which is a function over the entire state space.",
                    "label": 0
                },
                {
                    "sent": "So this application should be manageable and this function should be known.",
                    "label": 0
                },
                {
                    "sent": "But in some scenarios the dynamic order reward function are not known, so we cannot calculate this operator exactly, we can.",
                    "label": 1
                },
                {
                    "sent": "Only sample from this operator or the status space of the system is very large or infinite, which even if we have this operator, we cannot afford to apply it to the value function over the entire state space.",
                    "label": 1
                },
                {
                    "sent": "And This is why we need to use approximation schemes for the value function and use generalization and so on and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we can see that all the noise properties, contraction and we're not tenacity, and everything that we had for these algorithms is given.",
                    "label": 0
                },
                {
                    "sent": "The fact that you can calculate this operator exactly and you can apply it exactly to the value function.",
                    "label": 0
                },
                {
                    "sent": "If you cannot do it.",
                    "label": 0
                },
                {
                    "sent": "Then you cannot guarantee.",
                    "label": 0
                },
                {
                    "sent": "That you have this contraction and all the nice properties are going to go away.",
                    "label": 0
                },
                {
                    "sent": "This is for value function.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For policy iteration, you cannot guarantee that the new policy that you generate has the has a higher value than the previous one.",
                    "label": 0
                },
                {
                    "sent": "And that's the whole property that you could guarantee the convergence of your algorithm, so you can no longer guarantee all these things and the whole research and.",
                    "label": 0
                },
                {
                    "sent": "Exciting research and basically challenges for this problem that start right here.",
                    "label": 0
                },
                {
                    "sent": "Then you lose all the nice properties of these operators.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and of course these algorithms approximate dynamic programming and reinforcement learning algorithms scale badly with the dimension of the state space, because if you imagine now we want to use approximation schemes and the higher the dimension of the state of space we need more features to represent the value function, and we have usually dealing with two curses of dimensionality.",
                    "label": 1
                },
                {
                    "sent": "One is the exponential growth in the number of samples that we need in order to guarantee some sort of.",
                    "label": 1
                },
                {
                    "sent": "Performance.",
                    "label": 0
                },
                {
                    "sent": "And the other one is the exponential growth in the in the computation.",
                    "label": 0
                },
                {
                    "sent": "So we have the both computational curves and the statistical curves of.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of dimensionality.",
                    "label": 0
                },
                {
                    "sent": "So now what was the motivation of of this joint project?",
                    "label": 0
                },
                {
                    "sent": "One motivation from the cognitive science was so human being also learn an act using very complex and high dimensional observations, but they are amazingly well in ignoring most of the observed data with almost no perceptual loss.",
                    "label": 1
                },
                {
                    "sent": "So can we design algorithms to to have this sort of performance or not?",
                    "label": 0
                },
                {
                    "sent": "This is from the cognitive point of view.",
                    "label": 0
                },
                {
                    "sent": "The other way is now with this revolution of data we have systems that generate and work with very rich high dimensional data, and in order to have control where we have to deal with this sort of data so the data are getting bigger, more complex and reach the third motivation is.",
                    "label": 0
                },
                {
                    "sent": "Reinforced this reinforcement learning methods are not good with the non Markovian systems and non Markovian means.",
                    "label": 0
                },
                {
                    "sent": "When you are in a state and you take an action, knowing what will happen next to the system depends not only on the current state, but on the history of the system.",
                    "label": 0
                },
                {
                    "sent": "And OK, so one way of.",
                    "label": 1
                },
                {
                    "sent": "Changing a non Markovian system to a Markovian system is by adding more state variables.",
                    "label": 0
                },
                {
                    "sent": "Two to the system, so it makes it larger in terms of dimension and the 4th one is now we have some recent advances in handling high dimensional data in a statistical machine learning in supervised learning and semi supervised learning and also in the field of compressed sensing and the goal of this project was to see whether we can bring these mathematical formulations and tools.",
                    "label": 0
                },
                {
                    "sent": "Two sequential decision-making and develop and analyze algorithms for high dimensional.",
                    "label": 0
                },
                {
                    "sent": "Date.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is basically the main objective that I mentioned in the previous slide to find appropriate representation for this value function approximation in high dimensional problems and when they talk about high dimensional problems, we're usually talking about the setting in which the number of samples that you have is either smaller or at the order of the number of features that you consider for your problem and also to use them to develop efficient reinforcement learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "And by efficient I mean algorithm.",
                    "label": 0
                },
                {
                    "sent": "Could sample and computational complexity do not grow rapidly in an exponential way with the dimension of the observation?",
                    "label": 1
                },
                {
                    "sent": "Because if this happens, then your algorithm is going to end.",
                    "label": 0
                },
                {
                    "sent": "But we want algorithms that their computation and sample complexity grows in a slower rate with respect to the dimension.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we're going back again to the problem of feature selection, which is a problem known to everybody in machine learning in many areas, are machine learning, direct reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "So we look at this problem and we divided this problem to two different approaches and the approach taken by people at Technion was to start with the feature space for the value function approximation and trying to dynamically.",
                    "label": 0
                },
                {
                    "sent": "Adapt and change this feature space in order to perform well in that environment.",
                    "label": 0
                },
                {
                    "sent": "When that problem so imagine that you start with some parametric feature space and then try to use some sort of loss in order to optimize these parameters.",
                    "label": 0
                },
                {
                    "sent": "The approach that we considered it in real was.",
                    "label": 0
                },
                {
                    "sent": "Let's imagine that we just threw as much as features that we have.",
                    "label": 0
                },
                {
                    "sent": "And create a very huge high dimensional feature space in the hope that the good features lie in that set and then hopefully we have an algorithm which can clean up this big mess and find the the right information.",
                    "label": 1
                },
                {
                    "sent": "Election or future construction.",
                    "label": 0
                },
                {
                    "sent": "So this one is more in terms of construction because you start with a fixed set of features so you don't change the number of features, but you change the form of the features.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine that you change the Gaussian location or Gaussian with and these sort of things.",
                    "label": 0
                },
                {
                    "sent": "This one is more.",
                    "label": 0
                },
                {
                    "sent": "This one is also you create a very huge set of features and then you try to have an algorithm which deal with this huge problem but somehow find or act according to but at the end for us because our for us in enforcement learning the goal is prediction.",
                    "label": 0
                },
                {
                    "sent": "So if you have a good predictive performance, I don't want to explain you which features and have a feature interpretation, which sometimes is important in supervised learning.",
                    "label": 0
                },
                {
                    "sent": "For us the performance was the prediction performance, but some you are right.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we need also to explain.",
                    "label": 0
                },
                {
                    "sent": "In terms of.",
                    "label": 0
                },
                {
                    "sent": "Between features and states, so feature is a nonlinear function over states, so states are infinite.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Now we replace them with finite number of features.",
                    "label": 0
                },
                {
                    "sent": "Let's say you define Gaussians over the state space and you place these Gaussians.",
                    "label": 0
                },
                {
                    "sent": "In different part of the state space and you can imagine in order to cover your state space when your dimension of the space gets bigger and bigger, the number of discussions that you should put it.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes it can be a nonparametric features can be parametric features.",
                    "label": 0
                },
                {
                    "sent": "These are so.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, so I think it meant yes.",
                    "label": 0
                },
                {
                    "sent": "So this is basically these two approaches.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One slide that I have here.",
                    "label": 0
                },
                {
                    "sent": "I want to again emphasize what I've mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "That value function approximation in reinforcement learning is more than just recovering a target function from its noisy observation, which is regression, and it's a standard problem in supervised learning in a lot of algorithms in reinforcement learning we are dealing with approximating the fixed point of this bellman operator given sample trajectories.",
                    "label": 1
                },
                {
                    "sent": "So it's not necessarily a regression problem.",
                    "label": 0
                },
                {
                    "sent": "It might be fixed point problem, so this creates some difficulties in applying this standard supervised learning methods to this problem.",
                    "label": 1
                },
                {
                    "sent": "So I want you to distinguish these judges when we say bringing something from supervised learning to regression.",
                    "label": 0
                },
                {
                    "sent": "It's not just like OK, you have a good regression algorithm, you bring it here and it works in necessarily well in decision making.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the work that was done by group at Technion.",
                    "label": 0
                },
                {
                    "sent": "They, as I mentioned this is the dynamically changing the features while interacting with the environment and they have this set of publications related to this ICM.",
                    "label": 1
                },
                {
                    "sent": "Ellen CDC and CML.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I just very briefly summarized two of these publications, which is the adaptive basis for reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And for this well known Q learning algorithms, reinforcement learning, if you are familiar, this is one of the well known algorithm in.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Laurel and as I mentioned here, you imagine reinforcement learning function approximation and then you have some basis functions and you want to change them.",
                    "label": 0
                },
                {
                    "sent": "The main important thing is you change these bases according to what you want to optimize, what and these are the three errors that people there are examined.",
                    "label": 0
                },
                {
                    "sent": "One is the approximation square error between the value function, the other one is called the Bellman residual, which is the difference between the application of the bellman operator to a function.",
                    "label": 1
                },
                {
                    "sent": "And the function because you know if the function is the fixed point, the application of the bellman operator should be equal to the function.",
                    "label": 0
                },
                {
                    "sent": "But since it's not the fixed point we're looking for, the first point difference between these two is called the Bellman residual answer.",
                    "label": 1
                },
                {
                    "sent": "Very important error that is studied in reinforcement learning, which is related again to the fixed point property and the third one is the projected Bellman residual which I'm in order to explain it.",
                    "label": 0
                },
                {
                    "sent": "I need to go a little bit more deep into the.",
                    "label": 0
                },
                {
                    "sent": "Problem and then they implemented this in the form of an actor, critic algorithm and actor critic.",
                    "label": 0
                },
                {
                    "sent": "Algorithm is a very cognitive inspired algorithm in the sense that you have a controller and then you have an approximate are so approximator is like a predictor and its goal is to approximate the value function of strategy.",
                    "label": 0
                },
                {
                    "sent": "Basically tells you how well is your current strategy is and the actor is in charge of control.",
                    "label": 0
                },
                {
                    "sent": "So basically come consult.",
                    "label": 0
                },
                {
                    "sent": "With the predictor, get some information and then decide what to do next.",
                    "label": 0
                },
                {
                    "sent": "So they implemented this algorithm in this way and they shown the convergence with probability one day.",
                    "label": 0
                },
                {
                    "sent": "Synthetic convergence of these algorithms and.",
                    "label": 0
                },
                {
                    "sent": "This is the summary of two of.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work done by the group attacking it.",
                    "label": 0
                },
                {
                    "sent": "So now let's go back to what we did at INRIA.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, we look at the problem that we put a lot of features there and then we're trying to.",
                    "label": 0
                },
                {
                    "sent": "To design algorithms with the hope that the right features are there.",
                    "label": 0
                },
                {
                    "sent": "So we create a huge feature space with the hope that the right features are like there and we want to have algorithms which can deal with this.",
                    "label": 1
                },
                {
                    "sent": "So the immediately the first question is OK, we're going to have overfitting.",
                    "label": 0
                },
                {
                    "sent": "You're going to be in trouble, but we know that in regression and people have have dealt with this problem.",
                    "label": 1
                },
                {
                    "sent": "So two approach that we have studied, one is based on random projections, basically random.",
                    "label": 0
                },
                {
                    "sent": "Project your high dimensional feature space to a randomly generated low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Do your computation there with the hope that your final solution is good enough.",
                    "label": 0
                },
                {
                    "sent": "With respect to the calculation in high dimensional space and the second one is using the irregularities in the problem and irregularities that probably you have all heard is smoothness, which is basically equivalent by using L2 penalization or rich regression in supervised learning and the other one is the sparsity which is the L1 penalized methods and the loss of problems.",
                    "label": 0
                },
                {
                    "sent": "So these are the two types of regularity's that we can look.",
                    "label": 0
                },
                {
                    "sent": "But there is a very subtle difference between these two irregularities which is important to look at.",
                    "label": 0
                },
                {
                    "sent": "Smoothness is a property of the problem, so you have a value function is whether smooth or not.",
                    "label": 0
                },
                {
                    "sent": "But the sparsity is not only.",
                    "label": 1
                },
                {
                    "sent": "A property of the problem is a property of the problem and representation, because before you define your representation, your feature space, we cannot say that this value functions are smooth, is sparse, sparse in what?",
                    "label": 0
                },
                {
                    "sent": "Sparse is in wavelets.",
                    "label": 0
                },
                {
                    "sent": "Parts in what?",
                    "label": 0
                },
                {
                    "sent": "So this there is a subtle difference between these two term types of regularity's in in our problems.",
                    "label": 0
                },
                {
                    "sent": "Turning the representation as well here in our city is that correct?",
                    "label": 0
                },
                {
                    "sent": "No, we start with a huge representation is very high dimensional and we hope OK, so this is too big.",
                    "label": 0
                },
                {
                    "sent": "Probably the relevant features are only 10 of them and we want an algorithm which can deal with this huge feature.",
                    "label": 0
                },
                {
                    "sent": "Yes yes yes yes.",
                    "label": 0
                },
                {
                    "sent": "You are doing sparsity.",
                    "label": 0
                },
                {
                    "sent": "Only you would do L zero kind of L1.",
                    "label": 0
                },
                {
                    "sent": "Those both smoothness plus bar city, right?",
                    "label": 0
                },
                {
                    "sent": "That's that's basically a pseudo.",
                    "label": 0
                },
                {
                    "sent": "Replacement for the N 0 because it has nice properties at zero.",
                    "label": 0
                },
                {
                    "sent": "Dealing with zero is difficult, so it's basically like.",
                    "label": 0
                },
                {
                    "sent": "What is the right word for it its surrogate?",
                    "label": 0
                },
                {
                    "sent": "Is there is a surrogate for?",
                    "label": 0
                },
                {
                    "sent": "That's true, that's true.",
                    "label": 0
                },
                {
                    "sent": "That's true, that's true.",
                    "label": 0
                },
                {
                    "sent": "That's true.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so related to the first approach, which is related to random projections and bringing techniques from compressive sensing's.",
                    "label": 0
                },
                {
                    "sent": "These are the publications that we've had.",
                    "label": 0
                },
                {
                    "sent": "I try to present.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A couple of them.",
                    "label": 0
                },
                {
                    "sent": "The first one is compressed least square regression.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Whole idea is you have a feature space of dimension Capital D and this capital D is potentially very large, so the number of samples that you have is smaller than this D, so you are doomed to overfitting.",
                    "label": 0
                },
                {
                    "sent": "And then we want to project this large feature space to a smaller randomly generated features which with the conditions the comprehensive conditions not any random projections you know.",
                    "label": 1
                },
                {
                    "sent": "So we have some properties here.",
                    "label": 0
                },
                {
                    "sent": "It's not just that this can be generated from.",
                    "label": 0
                },
                {
                    "sent": "From this, with any random projection, so it has some properties so, but these are the linear combination of these features using the random coefficients and then we want to perform our algorithm.",
                    "label": 0
                },
                {
                    "sent": "The least squares.",
                    "label": 0
                },
                {
                    "sent": "Basically in this space and see what happens.",
                    "label": 0
                },
                {
                    "sent": "So in terms of theory there are, you know that every time that we try to do a deal with the regression you have two sorts of error.",
                    "label": 0
                },
                {
                    "sent": "One is the approximation error which is related to how limited is your space.",
                    "label": 0
                },
                {
                    "sent": "Expect your target.",
                    "label": 0
                },
                {
                    "sent": "If your target is outside your space, the distance between your target and your space is your approximation error and estimation error is the error of finding the image of your target in your space and how close you can get to that image.",
                    "label": 0
                },
                {
                    "sent": "Which is the best you can do so there is always like a tradeoff here that if you make your space big then this approximation error is going to go to 0.",
                    "label": 0
                },
                {
                    "sent": "But then you're going to have a very difficult search problem 'cause you have a very huge state space, and then you need a lot of samples in order to get small estimation error.",
                    "label": 0
                },
                {
                    "sent": "And if you do the other way, it make the space is small, then your approximation or goes up but your estimation error is going to go down.",
                    "label": 0
                },
                {
                    "sent": "So it's basically a tradeoff between these two and here you can see that by this projection when you solve your problem here you are dealing with a smaller space or approximations is smaller, but your estimation error is going to go up.",
                    "label": 0
                },
                {
                    "sent": "Because I'm sorry the other way.",
                    "label": 0
                },
                {
                    "sent": "So when you're solving the problem.",
                    "label": 0
                },
                {
                    "sent": "When you are solving the problem.",
                    "label": 0
                },
                {
                    "sent": "When you're solving the problem in the low dimensional space, so your approximation result is larger because now you have a smaller space, but your estimation error is smaller because now you have a easier search problem to solve and the analysis in this paper basically shows that now I decompose the error of this problem in this space to an approximation error to do estimation error of this space which is smaller than the other one, plus the approximation error.",
                    "label": 1
                },
                {
                    "sent": "Of this space plus an extra term.",
                    "label": 0
                },
                {
                    "sent": "So we replace the approximation error here, which is bigger than approximation right there by the approximation error here.",
                    "label": 1
                },
                {
                    "sent": "Plus on a small tear and we want to see whether this is small term can be controlled or not.",
                    "label": 0
                },
                {
                    "sent": "If it can be controlled with respect to the estimation error, we win by doing this process, otherwise we don't.",
                    "label": 0
                },
                {
                    "sent": "And we show in this work that for specific features that are in the form of wavelets and their specific spaces, you can always guarantee that this term can be controlled.",
                    "label": 0
                },
                {
                    "sent": "And why control means that this capital didn't appear in it because it disappears.",
                    "label": 0
                },
                {
                    "sent": "Then you're going to be in trouble.",
                    "label": 0
                },
                {
                    "sent": "And you're going to gain.",
                    "label": 0
                },
                {
                    "sent": "But if you choose any features then at least you don't have the guarantee.",
                    "label": 0
                },
                {
                    "sent": "And by optimizing the bound we get the results actually suggested by people in compressed sensing that this D should be at the order of root square of N. So the low dimension that you choose, so it's not just any projection you can project from 1000 dimensional space is 1 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "It should be some properties for this lower dimension.",
                    "label": 0
                },
                {
                    "sent": "Then you can get a nice rate in terms of regression.",
                    "label": 0
                },
                {
                    "sent": "Yes, do you.",
                    "label": 0
                },
                {
                    "sent": "Is the fact that the projection is random condition?",
                    "label": 0
                },
                {
                    "sent": "Or you could also do a semi supervised or no no no here.",
                    "label": 0
                },
                {
                    "sent": "You basically use this.",
                    "label": 0
                },
                {
                    "sent": "Yes yes it is.",
                    "label": 0
                },
                {
                    "sent": "Yes yes yes yes.",
                    "label": 0
                },
                {
                    "sent": "So we are using the Johnson Lindenstrauss theorem here, so you have to fulfill the requirements of bacteria in order for this to go through.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next one is bringing this to this LCD least square temporal difference learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "Popular algorithm in reinforcement learning, and this is a fixed point problem.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The idea is very similar to what I explained in terms of regression, but this is for a fixed point problem, so we have the reinforcement in high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "The number of features is bigger than the number of samples and then we have overfitting and poor prediction performance.",
                    "label": 1
                },
                {
                    "sent": "So what can we do?",
                    "label": 0
                },
                {
                    "sent": "We can use the regularization term, but here we don't look at this approach.",
                    "label": 0
                },
                {
                    "sent": "We look at the random projection approach and then we design an algorithm which call it the LSD with random projections, which solves the LCD problem.",
                    "label": 0
                },
                {
                    "sent": "In a randomly generated low dimensional space from the high dimension.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we've been in this work.",
                    "label": 0
                },
                {
                    "sent": "We present algorithm discuss it's complete computational complexity, and we show that it's less expensive than performing the algorithm in high dimension.",
                    "label": 1
                },
                {
                    "sent": "We provide finite sample performance bound for this algorithm for two different settings.",
                    "label": 1
                },
                {
                    "sent": "The Markov design setting is.",
                    "label": 0
                },
                {
                    "sent": "I don't go to the details.",
                    "label": 0
                },
                {
                    "sent": "Imagine it's like a deterministic design.",
                    "label": 0
                },
                {
                    "sent": "You want the performance to be good on the samples used by your algorithm.",
                    "label": 0
                },
                {
                    "sent": "So you just evaluate the performance of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "At the points that you used used by your algorithm and then the random design is when we generalize the performance bound here to any data that can be sampled using your measure over your entire state space.",
                    "label": 0
                },
                {
                    "sent": "And again, the the theoretical issue is the same thing that you.",
                    "label": 1
                },
                {
                    "sent": "By doing this you have a smaller estimation error, bigger approximation error, how big.",
                    "label": 0
                },
                {
                    "sent": "And whether this we can gain something or not.",
                    "label": 0
                },
                {
                    "sent": "And this is the theoretical.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Comparison the other issue that is important here is because it's a fixed point problem.",
                    "label": 0
                },
                {
                    "sent": "Is the uniqueness of the solution in the low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So we need to guarantee that if we assume that the LSD solution in high dimension exists, how many samples we need in the low dimension to guarantee that we have a unique solution.",
                    "label": 1
                },
                {
                    "sent": "So, and it really relates to the gramian matrix of the problem.",
                    "label": 0
                },
                {
                    "sent": "Basically, smallest eigenvalue of the of this problem in the high dimension.",
                    "label": 0
                },
                {
                    "sent": "This is the model based case when you know the model and if that solution exists.",
                    "label": 0
                },
                {
                    "sent": "For the low dimension, how many samples you need to generate.",
                    "label": 0
                },
                {
                    "sent": "And of course, if the gram matrix here has a small eigenvalue, you need more to generate more samples.",
                    "label": 1
                },
                {
                    "sent": "Here in order to guarantee the uniqueness of the solution.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "One analysis that we need to show that it's we have a unique solution and then all these reinforcement learning algorithms in addition to solving the honest problem, which is the calculating of the value function or a fixed point, this sequential problem.",
                    "label": 0
                },
                {
                    "sent": "All those dynamic programming algorithms are iterative algorithms.",
                    "label": 0
                },
                {
                    "sent": "So you have an error at each iteration and then you repeat this process.",
                    "label": 0
                },
                {
                    "sent": "So in all these algorithms the analysis has two phases.",
                    "label": 0
                },
                {
                    "sent": "One phase is very similar to supervised learning.",
                    "label": 0
                },
                {
                    "sent": "In the sense that you have a target function or you have a fixed point and you want to see how much error you have, and then you have to repeat this process until convergence and then you have to show how this error is propagated through the iterations of the algorithm and what would be the final performance.",
                    "label": 1
                },
                {
                    "sent": "So this is the last part of the analysis which brings everything together.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The this is a recent work by my colleague Romanos and his student Alessandro Conti and it's.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it's not problem.",
                    "label": 0
                },
                {
                    "sent": "So we the idea is you have a linear bandit problem and.",
                    "label": 0
                },
                {
                    "sent": "You have an action set is attached to that you have a very high dimension K and which is larger than your budget.",
                    "label": 1
                },
                {
                    "sent": "And but we know that this function is.",
                    "label": 0
                },
                {
                    "sent": "This data is sparse and of course we assume that this is on the unit ball, because otherwise I think they don't know how to deal with this problem and the algorithm is a combination with the idea of compressed sensing with the bandit theory and this confidence ellipsoid.",
                    "label": 1
                },
                {
                    "sent": "Algorithms for linear bandit and then they derive a regret bound or in the form of end with the number of samples or the budget and the sparsity.",
                    "label": 0
                },
                {
                    "sent": "So the K which is the nasty term which is large, doesn't appear in the inbound.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's go to the second approach.",
                    "label": 0
                },
                {
                    "sent": "The first approach, as we said, this random projection.",
                    "label": 0
                },
                {
                    "sent": "The second approach that we studied is using sparsity in the value function approximation, and we have these two work, which one of them is an algorithmic work and the other one is more theoretical I explained.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The theoretical work, which is the final sample analysis of lost so TD.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we have done in this work is we define this LASSO TD algorithm, which is a modification of that popular STD algorithm in which you have to replace orthogonal projection with an L1 projection.",
                    "label": 0
                },
                {
                    "sent": "So that's the change in this algorithm.",
                    "label": 0
                },
                {
                    "sent": "So LSD becomes a LASSO TD, so we show first that this algorithm guarantees to have a unique fixed point.",
                    "label": 1
                },
                {
                    "sent": "Then we showed that two existing algorithms for solving reinforcement learning, one designed by Andrew Ingin Zico Kolter at Stanford and the other one is by group at Duke.",
                    "label": 0
                },
                {
                    "sent": "Are actually solving this problem because these are algorithmic paper and there are algorithms that solve this problem.",
                    "label": 0
                },
                {
                    "sent": "So whatever analysis we have for this loss or TD is basically analysis for these two existing algorithms.",
                    "label": 1
                },
                {
                    "sent": "And then we calculate the two performance bounds for the last 40 algorithm in the Markov design setting and Markov design setting is imaginative deterministic design setting in the sense that you evaluate your poor performance at the points that you use.",
                    "label": 0
                },
                {
                    "sent": "That are used by your algorithm and the Markov design is coming from the fact that these points are not IID.",
                    "label": 0
                },
                {
                    "sent": "They are generated according to a trajectory generated by Markov chain.",
                    "label": 0
                },
                {
                    "sent": "We calculate 2 bounds and these results actually came after a very beautiful tutorial at NIPS 2010 by Peter Buhlmann, which is a statistician at IHC.",
                    "label": 0
                },
                {
                    "sent": "If I'm not mistaken, and because if you look at this community there statistician, they developed hundreds of papers trying to find bound.",
                    "label": 0
                },
                {
                    "sent": "For the last two algorithm and it's really hard to understand what is the assumption behind each of these results.",
                    "label": 0
                },
                {
                    "sent": "The beauty of 1 paper published by Bullman and sort of under gear which they also talked about it in this is they basically came and sorted all these assumptions and showed which assumption is more general than the other, which implies the other an for the three important tasks that you have in high dimensional statistics, which is prediction finding the weights.",
                    "label": 0
                },
                {
                    "sent": "And feature discovery.",
                    "label": 0
                },
                {
                    "sent": "What is the necessary condition that you can have a reasonable bound?",
                    "label": 0
                },
                {
                    "sent": "So this basically makes the life much simpler and gives you a road map of looking at those hundred papers generated by statistician, which is really hard to understand whether the assumptions.",
                    "label": 0
                },
                {
                    "sent": "So we used their ideas there.",
                    "label": 0
                },
                {
                    "sent": "We have two bonds for our algorithm.",
                    "label": 1
                },
                {
                    "sent": "One bound is the no assumption bound, so we make no assumption but has a slow rate.",
                    "label": 0
                },
                {
                    "sent": "And when I say slow rate is.",
                    "label": 0
                },
                {
                    "sent": "We're talking about linear function approximation and in linear regression the rate is one over square open, but we get one over into the power of 1/4.",
                    "label": 0
                },
                {
                    "sent": "This is why we call this slow rate.",
                    "label": 0
                },
                {
                    "sent": "Bound and also the other problem is the normal one.",
                    "label": 0
                },
                {
                    "sent": "Of AA is the is the weight vector of the image of your target in your space and the norm one of Alpha appears.",
                    "label": 0
                },
                {
                    "sent": "But you want to appear is the sparsity, which is the norm zero of Alpha, the second bound which comes at the cost of a compatibility assumption, which is the most relaxed assumption exist for prediction in high dimensional statistics?",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about is the mathematical assumption on the gram matrix of the empirical gram matrix.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "You can also call it the L1 eigenvalue assumption, which is just because there is an L2 eigenvalue assumption which is mathematical, meaning he's had some similarity with that, they just call it L1 value assumption.",
                    "label": 0
                },
                {
                    "sent": "If you want, we can talk about this assumption later.",
                    "label": 0
                },
                {
                    "sent": "Using this assumption we get a fast rate plus we get this sparsity to appear, but you need to guarantee that your representation has that assumption.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Spectrum in so things will I do you get anything on this one?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So what we have observed in analyzing.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry in analyzing.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning algorithms is actually weaken in terms of rate.",
                    "label": 0
                },
                {
                    "sent": "We get what we get in regression.",
                    "label": 0
                },
                {
                    "sent": "Right, so now the immediate question is, so is the sequential decision making as hard as hard as the regression or not?",
                    "label": 0
                },
                {
                    "sent": "But there are terms that appear in our bound here which is related to that propagation of error and independent to exploration, and it depends on how you explore your space.",
                    "label": 0
                },
                {
                    "sent": "There are some change of measure appears in the in this bound which depends on the measure that you want to evaluate your performance at the end and the measure that you use in your sampling.",
                    "label": 0
                },
                {
                    "sent": "Which can be very nasty are if there is a bad mismatch between these two.",
                    "label": 0
                },
                {
                    "sent": "This is the related directly.",
                    "label": 0
                },
                {
                    "sent": "The problem of exploration.",
                    "label": 0
                },
                {
                    "sent": "And the other one which is the iteration problem is related to control because in control you do something you get in the feedback.",
                    "label": 0
                },
                {
                    "sent": "Then you change it.",
                    "label": 0
                },
                {
                    "sent": "So these are the things that if you look at the rate in terms of the number of samples, you may say that OK.",
                    "label": 0
                },
                {
                    "sent": "The problem is as easy as regression because you get the optimal rates in most of the analysis.",
                    "label": 0
                },
                {
                    "sent": "But there are other terms which are related to the control aspect and the exploration aspect which makes the problem in my opinion extremely harder.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and so just let me just summarize what I think as a partner in this project was our achievement.",
                    "label": 0
                },
                {
                    "sent": "So we developed and analyzed because, OK, the goal of priming was if you have a crazy idea and you don't know whether it works or not.",
                    "label": 1
                },
                {
                    "sent": "Just write a proposal.",
                    "label": 0
                },
                {
                    "sent": "They give you some money, play with it for a year and a half, and if you think it's worth wide, continue it and apply it to the higher scale projects, whether in your country or or at.",
                    "label": 0
                },
                {
                    "sent": "European level so for us was important to at least create a mathematical backbone and some algorithms for this kind of problems, because you may say OK, these are bounds that you show are in the form of hyper upper bounds, and we don't know how tight are these ones.",
                    "label": 0
                },
                {
                    "sent": "These are the if I look at the reviews that I get from my papers at conferences.",
                    "label": 0
                },
                {
                    "sent": "That's at least one of the reviewers is going to write something like this.",
                    "label": 0
                },
                {
                    "sent": "I mean, I agree these are not the bounds that I look at this bound and I can develop an algorithm, but imagine that you have an algorithm and you want to tune this algorithm to work well and you have a couple of parameters to deal with.",
                    "label": 0
                },
                {
                    "sent": "So one way is OK if you don't have anything you should blindly just play with these parameters.",
                    "label": 0
                },
                {
                    "sent": "But if you have a bound by looking at the interaction between these parameters in the form of your upper bound, at least it gives you some idea how to tune your algorithm.",
                    "label": 0
                },
                {
                    "sent": "And or and also gives you a better understanding of what's happening in these algorithms in terms of choosing your function space and so on so forth.",
                    "label": 0
                },
                {
                    "sent": "So for us it was important to develop and analyze value function approximation methods for this setting, which is the high dimensional reinforcement learning problem when you have samples smaller than the features.",
                    "label": 1
                },
                {
                    "sent": "So we took two approaches, one by Technion, which is a dynamic changing of the feature space, one with a switch just blow up the feature space in the hope to.",
                    "label": 1
                },
                {
                    "sent": "Two for your right features to lie there, and then we look at two different ways, one inspired by the compress compressive sensing community and progress and the other one is related to a sparsity.",
                    "label": 0
                },
                {
                    "sent": "And now we have a set of purity confounded algorithms, which is, in my opinion is good.",
                    "label": 1
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then what we can do next?",
                    "label": 0
                },
                {
                    "sent": "So what we would like to do is we would like to have adaptive algorithms and by adaptive algorithm I mean an algorithm that can automatically takes advantage of the regularity's in the problem.",
                    "label": 1
                },
                {
                    "sent": "So the two regularity's that have been studied one is Delta regularity.",
                    "label": 0
                },
                {
                    "sent": "The smoothness which is has been studied both algorithmically and theoretically in the concept of control and reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So this is the work that I started with my colleague Farahmand.",
                    "label": 0
                },
                {
                    "sent": "So this is the beginning of our work, which is back when we were all in Canada.",
                    "label": 0
                },
                {
                    "sent": "So we started this work there and we developed and analyzed the L2 version of three important reinforcement learning algorithm and had an analysis for them.",
                    "label": 0
                },
                {
                    "sent": "Then we came to the L2 station or sparsity, and what existed before this project was just a couple of algorithms, which was this LSD by people at Stanford and this one by people at Duke.",
                    "label": 0
                },
                {
                    "sent": "And this one was what people are in our group, which was a TD type of algorithm.",
                    "label": 0
                },
                {
                    "sent": "So now we add the theoretical results.",
                    "label": 0
                },
                {
                    "sent": "Here we have the final sample analysis of these two algorithms.",
                    "label": 0
                },
                {
                    "sent": "We have also some algorithm which call it into L1L STD's more algorithmic, and we have some new results which are in pipe.",
                    "label": 0
                },
                {
                    "sent": "And this is another class of algorithms that we haven't studied.",
                    "label": 0
                },
                {
                    "sent": "This algorithmic is a linear programming approach with L1 regularization, which I haven't studied it, it's by.",
                    "label": 0
                },
                {
                    "sent": "This right now is at IBM, but.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now studying other form of regularity is what we can do next.",
                    "label": 0
                },
                {
                    "sent": "Studying other form of regularity in sequential decision making.",
                    "label": 1
                },
                {
                    "sent": "One of them is the manifold activity.",
                    "label": 0
                },
                {
                    "sent": "If we know that the value function that we are looking for is.",
                    "label": 0
                },
                {
                    "sent": "Is basically sitting on a low dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "How we can take advantage of this?",
                    "label": 0
                },
                {
                    "sent": "Basically just the dimension of that manifold appears in our in our bound instead of, so this is one thing that I haven't still worked on it.",
                    "label": 0
                },
                {
                    "sent": "Then the new one that we have recently started looking at, it is called the action gap regularity and it's related to the margin because in reinforcement learning what you would like to do is you have a policy and you want to find the action.",
                    "label": 0
                },
                {
                    "sent": "That at each state you want to find the action that has the highest value function with respect to the current policy, and then design a new policy which takes that a specific action in that space.",
                    "label": 0
                },
                {
                    "sent": "So this is like a classification problem in the sense that you have.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have three actions.",
                    "label": 0
                },
                {
                    "sent": "For each action you have.",
                    "label": 0
                },
                {
                    "sent": "You have some estimate of this action value function, and you want the output to gives you the action that maximizes the true action value function.",
                    "label": 0
                },
                {
                    "sent": "So now the question is if you have two actions which have very value functions that are very close to each other.",
                    "label": 0
                },
                {
                    "sent": "Then the margin is very small.",
                    "label": 0
                },
                {
                    "sent": "The gap is small and discriminating between these two requires a lot of budget.",
                    "label": 0
                },
                {
                    "sent": "But if you have a problem that can guarantee that there is a specific margin is.",
                    "label": 0
                },
                {
                    "sent": "I'm not an expert in classification, but if it has a very nice connection with what happens in classification that you have margin, you have a similar classification problem, so it's a regularity of the problem and whether we can take advantage of it or not.",
                    "label": 0
                },
                {
                    "sent": "This is a sort of regularity that we recently started started looking at it, and I'm also happy to say that, OK, at least I can include.",
                    "label": 0
                },
                {
                    "sent": "I could include a part of this idea of.",
                    "label": 0
                },
                {
                    "sent": "Studying regularity in in sequential decision making in two European project that is complex and it got funded last year.",
                    "label": 0
                },
                {
                    "sent": "So in one of the work packages which is related to reinforcement learning, one of the tasks that we are doing is studying the regularity in sequential decision making problem.",
                    "label": 0
                },
                {
                    "sent": "So I think we have fulfilled a little bit the goal of pump priming to to do something next.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The future work is, again develop algorithms based on the existing and new principle.",
                    "label": 1
                },
                {
                    "sent": "For high dimensional methods.",
                    "label": 1
                },
                {
                    "sent": "Study other form of regularity's that I mentioned.",
                    "label": 0
                },
                {
                    "sent": "The other question is when we have approximated regularity's, so we are not exactly in a low dimensional manifold, but your coastal dimensional manifold can.",
                    "label": 0
                },
                {
                    "sent": "We can be design algorithms that can give us something, so this is an interesting problem which might be even related to robust control and stuff that we haven't.",
                    "label": 0
                },
                {
                    "sent": "We haven't touched it and the real world application people are taking on.",
                    "label": 0
                },
                {
                    "sent": "They have a very beautiful dogfight simulator is a flight simulator that they want to run some.",
                    "label": 0
                },
                {
                    "sent": "The algorithm and work on it, so I know that she has started looking at it.",
                    "label": 0
                },
                {
                    "sent": "He has a PhD student working on this simulator, so it is a very good.",
                    "label": 0
                },
                {
                    "sent": "Platform for studying high dimensional complex system for us.",
                    "label": 0
                },
                {
                    "sent": "We are trying to look at problem in network optimization and traffic control.",
                    "label": 0
                },
                {
                    "sent": "Depends on if the funding is going to be provided.",
                    "label": 0
                },
                {
                    "sent": "We are going to work on this problem and it was another problem that we attempted to work on.",
                    "label": 0
                },
                {
                    "sent": "It wasn't refinery optimization which debuted the beautiful aspect in this problem was you have a bunch of can control systems that are cascaded.",
                    "label": 0
                },
                {
                    "sent": "The output of one is the input of the other one and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So it's a very interesting problem, but unfortunately again funded.",
                    "label": 0
                },
                {
                    "sent": "So if it gets funded, maybe we're going to work on it, so these are some future.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Application I hope I didn't go out of time.",
                    "label": 0
                },
                {
                    "sent": "Yes, thank you.",
                    "label": 0
                },
                {
                    "sent": "You mentioned MVP at the beginning of those applications.",
                    "label": 0
                },
                {
                    "sent": "Would you think that the user is a form of MVP?",
                    "label": 0
                },
                {
                    "sent": "The application you can you can you can make it more Markovian and use MVP methods, yes.",
                    "label": 0
                },
                {
                    "sent": "Depends on the task that you want to define in the flight simulator.",
                    "label": 0
                },
                {
                    "sent": "This plan for this pump priming project.",
                    "label": 0
                },
                {
                    "sent": "I mean you you mentioned the MVP's as one of the framework of the method that you want to make to generalize to higher dimension of the state space, right?",
                    "label": 0
                },
                {
                    "sent": "But have you actually run an MVP following this or you have the enabling tools?",
                    "label": 0
                },
                {
                    "sent": "I mean so every as I mentioned at the beginning, the MDP and palm DP or just the mathematical model for this decision making and all the analysis that we have here an all the?",
                    "label": 0
                },
                {
                    "sent": "Experiments that we have run so far is based on the MDP model, when we know the environment is Markov and the state is observable.",
                    "label": 0
                },
                {
                    "sent": "But if you remember, sorry, but one more thing if you remember one of the motivation is when you have a non Markovian problem we can make it more cost by adding more state variables and making the dimension bigger, but we have turned into an MVP problem.",
                    "label": 0
                },
                {
                    "sent": "Understand his MVP is what people do.",
                    "label": 0
                },
                {
                    "sent": "People in the application side use like the dialogue people when they do reinforcement learning the language.",
                    "label": 0
                },
                {
                    "sent": "Actually they always talk about on DP right palm DP or MDP mostly so palm DP is much harder to control and deal with.",
                    "label": 0
                },
                {
                    "sent": "Yes so it's.",
                    "label": 0
                },
                {
                    "sent": "I was trying to visualize if there is an example of some people doing MDP and thanks to your techniques they could kind of expand the space state space and we got actually are kind of going to get actual results on that.",
                    "label": 0
                },
                {
                    "sent": "Again, MVP or Pompey.",
                    "label": 0
                },
                {
                    "sent": "So MDP to everything here was MVP.",
                    "label": 0
                },
                {
                    "sent": "All these algorithms is based on when we have the Markovian assumption.",
                    "label": 0
                },
                {
                    "sent": "When the state is observable we have all these nice theoretical results and algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now the question is if we go to Palm DP and the state is not observable, then we don't have that much.",
                    "label": 0
                },
                {
                    "sent": "That much theory actually for palm DP.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the current results in Palm DP level or the size of the problems that they are dealing with are very small.",
                    "label": 0
                },
                {
                    "sent": "So this difficult problem.",
                    "label": 0
                },
                {
                    "sent": "And I wouldn't use the value function approximation for those methods.",
                    "label": 0
                },
                {
                    "sent": "I definitely go for the policy search things that I don't know if the young Peters talked about it this morning or not, but these are the type of things that are more suited for partial observable methods.",
                    "label": 0
                },
                {
                    "sent": "And in terms of, uh, we're currently continue working on this, with which I have had collaboration with try since 2008, and we keep going and.",
                    "label": 0
                },
                {
                    "sent": "Send thanks to the pump priming.",
                    "label": 0
                },
                {
                    "sent": "We can still continue until the end of September, so I think whatever is left we're going to use.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}