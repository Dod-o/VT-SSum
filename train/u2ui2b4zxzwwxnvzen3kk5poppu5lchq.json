{
    "id": "u2ui2b4zxzwwxnvzen3kk5poppu5lchq",
    "title": "Fast Flux Discriminant for Large-Scale Sparse Nonlinear Classification",
    "info": {
        "author": [
            "Wenlin Chen, Department of Computer Science and Engineering, Washington University in St. Louis"
        ],
        "published": "Oct. 8, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_chen_nonlinear_classification/",
    "segmentation": [
        [
            "Today I'm going to talk about fast flux discriminate for large scale, sparse than linear classification, and this is joint work with Asian channan kidding when burger and I'm willing to."
        ],
        [
            "So let's start off by introducing some basic settings for our talk.",
            "Suppose we are given a training set which contains a."
        ],
        [
            "And training samples an each sample has."
        ],
        [
            "Feature vector here we use the brackets and subscript to denote each feature an here we in this talk we only deal with."
        ],
        [
            "Binary classification and so the task."
        ],
        [
            "It is just a typical classification task which is predict Y given X or the probability to compute the probability of Y being one given X."
        ],
        [
            "OK, so before we introduce our model, let's first talk about what we want for four classification model.",
            "So the first one is that it has.",
            "It has nonlinear classification ability and the second one is the linear classification ability is not sufficient because for data mining data mining task you want to extract knowledge from the model you learn.",
            "So the models shouldn't be just a black box, so it should be interpretable.",
            "So this is very important for like medical prediction where the doctor wants to diagnose which which featured results to a certain disease.",
            "And so most nonlinear most linear.",
            "Nonlinear method is not a not interpretable, for example, the."
        ],
        [
            "SVM with RBF kernel it's it's not near because you project the feature to Hilbert space.",
            "An random kitchen sinks is also not interpretable because it do the feature mapping by random project."
        ],
        [
            "Ocean.",
            "And the third one is the support for mixed datatypes, so."
        ],
        [
            "Often times the data set contains categorical features and numerical feature."
        ],
        [
            "And you want so a typical method is to map the category to whole feature to a numerical feature by binary coding, and so this increased a lot of new features.",
            "And also we want to be efficient, efficient and sparsity so that in testing time if the weight of the feature is zero, you don't have to extract this feature and this is efficient in testing time.",
            "So most existing classifiers that do not satisfy all these five properties."
        ],
        [
            "So in this talk, our goal is to build an efficient linear classifier that achieves sparsity an preserve interpretability."
        ],
        [
            "And so the way we do that is first, we do."
        ],
        [
            "Given the data set, we first map the.",
            "We first met the X into a feature space where so."
        ],
        [
            "FD features in this feature space.",
            "It contains 2 parts.",
            "The first part."
        ],
        [
            "Is 1D features, which is feature wise linear mapping?"
        ],
        [
            "The second one is a pairwise feature mapping which captures the interaction between.",
            "Between 2 features."
        ],
        [
            "And after we get this FD features, we throw it into filter that does feature selection by submodular optimization."
        ],
        [
            "An so we are left with a subset of empty features.",
            "Then we just do a sparse training on those FD features and get the file."
        ],
        [
            "Model so in this talk out many address these three men parts towards the final model."
        ],
        [
            "OK, so let's first talk about that linear mapping."
        ],
        [
            "For simplicity, let's denote, am is as the set of indices selected by MSFT feature."
        ],
        [
            "For example, if AM is, is the set of two and three, then X subscript am is just feature only containing the second feature."
        ],
        [
            "Feature.",
            "And so the way we do the near mapping is to apply a logit function over the conditional probability of Y given X."
        ],
        [
            "As are we defer the discussion of why we choose this than linear mapping.",
            "So let's now focus on how to compute this than linear mapping.",
            "And so it hinges on how you compute this probability.",
            "Why being one given X am an if you just model it by logistic regression, then you will just end up with a linear mapping.",
            "So the way we do that is."
        ],
        [
            "Is by general density estimation."
        ],
        [
            "So because we have 1D features and 2D features, let's start with a simple one.",
            "So for one dimensional density estimation is quite simple.",
            "You just have to bucket eyes that feature and then you just calculate the proportion of positive samples in each bin and then that proportion is the is the is the probability of wiping one given X."
        ],
        [
            "And so it's the same case for the 2D dimensional density estimation you just have."
        ],
        [
            "The bucket hides the the 2D subspace and then you can build."
        ],
        [
            "Histogram as the one dimensional density estimation."
        ],
        [
            "An if you just do the histogram, oftentimes you will end up with a lot of bins that empty or have few training examples, so this would be on robust so."
        ],
        [
            "The way to overcome this is by typical kernel smoothing.",
            "So the idea is that the the probability or the feature the probability this conditional probability is affected by the by all other bins, and so this is just basic formulation for kernel smoothing in terms of the bin."
        ],
        [
            "Anne, here the be prime is the Bing index and.",
            "NB Prime One is the number of positive samples in."
        ],
        [
            "In be prime an MP."
        ],
        [
            "You promised the number of sample in prime."
        ],
        [
            "So the kernel is just Gaussian kernel."
        ],
        [
            "Where the L is the bin lends an edge is the band."
        ],
        [
            "And if you have sufficient amounts of."
        ],
        [
            "Of our bins, then the histogram plus this kernel smoothing would be equivalent to the exact kernel density as mentioned."
        ],
        [
            "So once we get low, get low safety features, then you want to filter out redundant FD features.",
            "So here we buy, we defer the definition of redundancy.",
            "On"
        ],
        [
            "Let's first get familiar with the notations in this slide, so as is the set of selected FD features and you is the ground set of all MMD features, and CZ is the redundancy between FD feature I update feature today and AI is the accuracy of every feature I so by accuracy here.",
            "To compute the accuracy of feature, you just have to compute the misclassified.",
            "Training samples in the corresponding bin of that feature."
        ],
        [
            "And so for feature selection, if you just want sparsity then you might just apply any L1 regularization.",
            "But here we want."
        ],
        [
            "More so, the goal of our feature selection is.",
            "The first goal is we want to maximize the redundancy of the between the selected features and unselected features, so by this.",
            "The selected features is a well representative of the whole feature set."
        ],
        [
            "And the second one is that we want to reduce the redundancy within selected features so that our selected features are still diverse."
        ],
        [
            "And the third one is that we want to maximize the accuracy of the selected feature."
        ],
        [
            "And the 4th one is the just promote sparsity, an actually the last one could be moved to the constraint, which is known as cardinality."
        ],
        [
            "String, and so it turns out this combinatorials function is.",
            "Modular function which can be solved by any submodular solver."
        ],
        [
            "And so the behavior of this feature feature selection is depends on how you define the redundancy the season."
        ],
        [
            "So if you just define it as a Pearson correlation, then it just filter out the correlated features among."
        ],
        [
            "General definition is.",
            "The it's a pairwise feature."
        ],
        [
            "Penalty so by setting a high C I'd say.",
            "FT feature iff DJ are less likely to be selected together."
        ],
        [
            "And so, for example, you might want to set high C. I'd say if AI and AJ have joint index, so meaning that the feature the FD feature inj have used the same original feature and you can see that as redundancy."
        ],
        [
            "And so the final FD model is just simply logistic regression over the selected features.",
            "An here just to remind you that the linear mapping is a logic function over the probability density over the conditional probability."
        ],
        [
            "And there are many ways to explain why we choose this nonlinear mapping, but I think the most intuitive way is that if you plot the curve of this function where the X axis is the conditional probability.",
            "Then actually, if this probability is larger than .5, then find the new feature would be positive, otherwise it will be negative."
        ],
        [
            "And so Phi M is actually a indicator of how well the original feature predicts the label, and so the file here.",
            "The new feature is supposed to have positive correlation with the label.",
            "And so it's natural to think to make an assumption."
        ],
        [
            "The weights between the feature is.",
            "Is non negative."
        ],
        [
            "And so to train them."
        ],
        [
            "Father, it's."
        ],
        [
            "We we minimize the logistic loss."
        ],
        [
            "US and.",
            "Lisa L1 regularization to promote spa."
        ],
        [
            "City and under the assumption that the way should be positive, the L1 norm could only could be expressed by the sum of all the weights."
        ],
        [
            "So this is our final optimization."
        ],
        [
            "And there are some subtle distance."
        ],
        [
            "Between the typic."
        ],
        [
            "L1 and this one.",
            "So for typical L1 norm you is non differentiable at the origin.",
            "But here we are differentiable everywhere in the visible solution and so when you were doing projected gradient descent then you can wait is just truncated at 0."
        ],
        [
            "An here are some practical issues for using adaptive models an for hyper parameters.",
            "We use Bayesian optimization for tune."
        ],
        [
            "So basically optimizing is more sophisticated way for cross validation and it's very suitable for data mining based on feature engineer."
        ],
        [
            "But it may not work well for high dimensional raw data such as image, because they might have higher order of interaction between features."
        ],
        [
            "Between pixels OK, so this is a visualization of the classification results.",
            "So as you can see, it's first.",
            "It's not linear, an as the number of bins are growth the the the separation curve becomes smoother."
        ],
        [
            "And so here, let's talk about the interpretability of this model so."
        ],
        [
            "How?",
            "The output of FD itself is a confidence of the label."
        ],
        [
            "Then as I can, as I said before, after the nonlinear mapping, the new feature captures how well the original feature predicts the label, so it's it's indicator of the.",
            "A feature."
        ],
        [
            "And the WM here is the importance of that new feature an the W here is kind of come parable with other W because for example in the typical linear model you might want the weights for pressure, but blood pressure and weight for height may not be compatible, but here it's in the same probabilistic scale.",
            "So it's kind of come parable."
        ],
        [
            "So here are some experiment."
        ],
        [
            "And for the model and which?"
        ],
        [
            "The largest one we try is the is the Cup 99 which contains 5,000,000 examples, samples and FD model can solve the disc datasets within one minute.",
            "But for typical as SVM, it's just cannot.",
            "It just cannot finish."
        ],
        [
            "OK Ann, for accuracy actually have the green bar is the FD and a green red bar is the other day and the green bar is the SVM."
        ],
        [
            "Actually, the the accuracy is quite compareable.",
            "Sometimes it's even better than than SVM in some cases.",
            "So the tech."
        ],
        [
            "Max it is.",
            "We develop a classifier that has the name."
        ],
        [
            "Classification ability."
        ],
        [
            "He an interpreter."
        ],
        [
            "He is up for mixed data type."
        ],
        [
            "And efficient."
        ],
        [
            "Sparse.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today I'm going to talk about fast flux discriminate for large scale, sparse than linear classification, and this is joint work with Asian channan kidding when burger and I'm willing to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start off by introducing some basic settings for our talk.",
                    "label": 0
                },
                {
                    "sent": "Suppose we are given a training set which contains a.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And training samples an each sample has.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Feature vector here we use the brackets and subscript to denote each feature an here we in this talk we only deal with.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Binary classification and so the task.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is just a typical classification task which is predict Y given X or the probability to compute the probability of Y being one given X.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so before we introduce our model, let's first talk about what we want for four classification model.",
                    "label": 0
                },
                {
                    "sent": "So the first one is that it has.",
                    "label": 0
                },
                {
                    "sent": "It has nonlinear classification ability and the second one is the linear classification ability is not sufficient because for data mining data mining task you want to extract knowledge from the model you learn.",
                    "label": 1
                },
                {
                    "sent": "So the models shouldn't be just a black box, so it should be interpretable.",
                    "label": 0
                },
                {
                    "sent": "So this is very important for like medical prediction where the doctor wants to diagnose which which featured results to a certain disease.",
                    "label": 0
                },
                {
                    "sent": "And so most nonlinear most linear.",
                    "label": 0
                },
                {
                    "sent": "Nonlinear method is not a not interpretable, for example, the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "SVM with RBF kernel it's it's not near because you project the feature to Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "An random kitchen sinks is also not interpretable because it do the feature mapping by random project.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ocean.",
                    "label": 0
                },
                {
                    "sent": "And the third one is the support for mixed datatypes, so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Often times the data set contains categorical features and numerical feature.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you want so a typical method is to map the category to whole feature to a numerical feature by binary coding, and so this increased a lot of new features.",
                    "label": 0
                },
                {
                    "sent": "And also we want to be efficient, efficient and sparsity so that in testing time if the weight of the feature is zero, you don't have to extract this feature and this is efficient in testing time.",
                    "label": 0
                },
                {
                    "sent": "So most existing classifiers that do not satisfy all these five properties.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this talk, our goal is to build an efficient linear classifier that achieves sparsity an preserve interpretability.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the way we do that is first, we do.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Given the data set, we first map the.",
                    "label": 0
                },
                {
                    "sent": "We first met the X into a feature space where so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "FD features in this feature space.",
                    "label": 0
                },
                {
                    "sent": "It contains 2 parts.",
                    "label": 0
                },
                {
                    "sent": "The first part.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is 1D features, which is feature wise linear mapping?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second one is a pairwise feature mapping which captures the interaction between.",
                    "label": 0
                },
                {
                    "sent": "Between 2 features.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And after we get this FD features, we throw it into filter that does feature selection by submodular optimization.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An so we are left with a subset of empty features.",
                    "label": 0
                },
                {
                    "sent": "Then we just do a sparse training on those FD features and get the file.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model so in this talk out many address these three men parts towards the final model.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's first talk about that linear mapping.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For simplicity, let's denote, am is as the set of indices selected by MSFT feature.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, if AM is, is the set of two and three, then X subscript am is just feature only containing the second feature.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Feature.",
                    "label": 0
                },
                {
                    "sent": "And so the way we do the near mapping is to apply a logit function over the conditional probability of Y given X.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As are we defer the discussion of why we choose this than linear mapping.",
                    "label": 0
                },
                {
                    "sent": "So let's now focus on how to compute this than linear mapping.",
                    "label": 0
                },
                {
                    "sent": "And so it hinges on how you compute this probability.",
                    "label": 0
                },
                {
                    "sent": "Why being one given X am an if you just model it by logistic regression, then you will just end up with a linear mapping.",
                    "label": 0
                },
                {
                    "sent": "So the way we do that is.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is by general density estimation.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So because we have 1D features and 2D features, let's start with a simple one.",
                    "label": 0
                },
                {
                    "sent": "So for one dimensional density estimation is quite simple.",
                    "label": 1
                },
                {
                    "sent": "You just have to bucket eyes that feature and then you just calculate the proportion of positive samples in each bin and then that proportion is the is the is the probability of wiping one given X.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so it's the same case for the 2D dimensional density estimation you just have.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The bucket hides the the 2D subspace and then you can build.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Histogram as the one dimensional density estimation.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An if you just do the histogram, oftentimes you will end up with a lot of bins that empty or have few training examples, so this would be on robust so.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The way to overcome this is by typical kernel smoothing.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that the the probability or the feature the probability this conditional probability is affected by the by all other bins, and so this is just basic formulation for kernel smoothing in terms of the bin.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne, here the be prime is the Bing index and.",
                    "label": 0
                },
                {
                    "sent": "NB Prime One is the number of positive samples in.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In be prime an MP.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You promised the number of sample in prime.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the kernel is just Gaussian kernel.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where the L is the bin lends an edge is the band.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you have sufficient amounts of.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of our bins, then the histogram plus this kernel smoothing would be equivalent to the exact kernel density as mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So once we get low, get low safety features, then you want to filter out redundant FD features.",
                    "label": 1
                },
                {
                    "sent": "So here we buy, we defer the definition of redundancy.",
                    "label": 0
                },
                {
                    "sent": "On",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's first get familiar with the notations in this slide, so as is the set of selected FD features and you is the ground set of all MMD features, and CZ is the redundancy between FD feature I update feature today and AI is the accuracy of every feature I so by accuracy here.",
                    "label": 1
                },
                {
                    "sent": "To compute the accuracy of feature, you just have to compute the misclassified.",
                    "label": 0
                },
                {
                    "sent": "Training samples in the corresponding bin of that feature.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so for feature selection, if you just want sparsity then you might just apply any L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "But here we want.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More so, the goal of our feature selection is.",
                    "label": 0
                },
                {
                    "sent": "The first goal is we want to maximize the redundancy of the between the selected features and unselected features, so by this.",
                    "label": 0
                },
                {
                    "sent": "The selected features is a well representative of the whole feature set.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the second one is that we want to reduce the redundancy within selected features so that our selected features are still diverse.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the third one is that we want to maximize the accuracy of the selected feature.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the 4th one is the just promote sparsity, an actually the last one could be moved to the constraint, which is known as cardinality.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "String, and so it turns out this combinatorials function is.",
                    "label": 0
                },
                {
                    "sent": "Modular function which can be solved by any submodular solver.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the behavior of this feature feature selection is depends on how you define the redundancy the season.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you just define it as a Pearson correlation, then it just filter out the correlated features among.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "General definition is.",
                    "label": 0
                },
                {
                    "sent": "The it's a pairwise feature.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Penalty so by setting a high C I'd say.",
                    "label": 0
                },
                {
                    "sent": "FT feature iff DJ are less likely to be selected together.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so, for example, you might want to set high C. I'd say if AI and AJ have joint index, so meaning that the feature the FD feature inj have used the same original feature and you can see that as redundancy.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the final FD model is just simply logistic regression over the selected features.",
                    "label": 0
                },
                {
                    "sent": "An here just to remind you that the linear mapping is a logic function over the probability density over the conditional probability.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there are many ways to explain why we choose this nonlinear mapping, but I think the most intuitive way is that if you plot the curve of this function where the X axis is the conditional probability.",
                    "label": 0
                },
                {
                    "sent": "Then actually, if this probability is larger than .5, then find the new feature would be positive, otherwise it will be negative.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so Phi M is actually a indicator of how well the original feature predicts the label, and so the file here.",
                    "label": 0
                },
                {
                    "sent": "The new feature is supposed to have positive correlation with the label.",
                    "label": 0
                },
                {
                    "sent": "And so it's natural to think to make an assumption.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The weights between the feature is.",
                    "label": 0
                },
                {
                    "sent": "Is non negative.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so to train them.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Father, it's.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We we minimize the logistic loss.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "US and.",
                    "label": 0
                },
                {
                    "sent": "Lisa L1 regularization to promote spa.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "City and under the assumption that the way should be positive, the L1 norm could only could be expressed by the sum of all the weights.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is our final optimization.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there are some subtle distance.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Between the typic.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "L1 and this one.",
                    "label": 0
                },
                {
                    "sent": "So for typical L1 norm you is non differentiable at the origin.",
                    "label": 0
                },
                {
                    "sent": "But here we are differentiable everywhere in the visible solution and so when you were doing projected gradient descent then you can wait is just truncated at 0.",
                    "label": 1
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An here are some practical issues for using adaptive models an for hyper parameters.",
                    "label": 0
                },
                {
                    "sent": "We use Bayesian optimization for tune.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically optimizing is more sophisticated way for cross validation and it's very suitable for data mining based on feature engineer.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it may not work well for high dimensional raw data such as image, because they might have higher order of interaction between features.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Between pixels OK, so this is a visualization of the classification results.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, it's first.",
                    "label": 0
                },
                {
                    "sent": "It's not linear, an as the number of bins are growth the the the separation curve becomes smoother.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so here, let's talk about the interpretability of this model so.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "The output of FD itself is a confidence of the label.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then as I can, as I said before, after the nonlinear mapping, the new feature captures how well the original feature predicts the label, so it's it's indicator of the.",
                    "label": 0
                },
                {
                    "sent": "A feature.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the WM here is the importance of that new feature an the W here is kind of come parable with other W because for example in the typical linear model you might want the weights for pressure, but blood pressure and weight for height may not be compatible, but here it's in the same probabilistic scale.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of come parable.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some experiment.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the model and which?",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The largest one we try is the is the Cup 99 which contains 5,000,000 examples, samples and FD model can solve the disc datasets within one minute.",
                    "label": 0
                },
                {
                    "sent": "But for typical as SVM, it's just cannot.",
                    "label": 0
                },
                {
                    "sent": "It just cannot finish.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK Ann, for accuracy actually have the green bar is the FD and a green red bar is the other day and the green bar is the SVM.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, the the accuracy is quite compareable.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's even better than than SVM in some cases.",
                    "label": 0
                },
                {
                    "sent": "So the tech.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Max it is.",
                    "label": 0
                },
                {
                    "sent": "We develop a classifier that has the name.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classification ability.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He an interpreter.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He is up for mixed data type.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And efficient.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sparse.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}