{
    "id": "f5vys2jgv6mcabn4gb5pquugocqitjj7",
    "title": "Nonparametric Bayesian Density Modeling with Gaussian Processes",
    "info": {
        "author": [
            "Ryan Prescott Adams, Department of Computer Science, University of Toronto"
        ],
        "published": "Aug. 4, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_adams_nbd/",
    "segmentation": [
        [
            "First, let's just to get on.",
            "Let's just get on the same page and what am I talking about with the density modeling problem?",
            "Well, I just mean we have some set of data drawn from a density that we don't know and we have some prior beliefs about the density."
        ],
        [
            "And we'd like to perform posterior inference, so this is just the I mean, this is just Bayesian stuff."
        ],
        [
            "Right, of course we're interested in nonparametric things here.",
            "The classical approach to this being kernel density estimation, like parzen windows.",
            "But we're Bayesians in this room, so the most popular thing, certainly with with Bayesian approaches, is the infinite mixture of parametric distributions, and there's been a lot of talks about that so far, and then, of course there's also additional diffusion trees, Gaussian process, latent variable, model density networks, and some other models as well.",
            "What I'm going to talk about today is density modeling.",
            "We're putting a Gaussian process prior, essentially directly on the probability."
        ],
        [
            "City function itself.",
            "Now, why is this an interesting thing to do?",
            "Well, it's because Gaussian processes are very nice priors on functions.",
            "You don't have to specify an explicit set of basis functions.",
            "You can pick your input space, which for this talk you can just assume is the D dimensional real space, and then you pick a covariance function that expresses your prior beliefs about, say, the smoothness of the of the functions in your distribution, and then a mean function which will take to be 0.",
            "And it's a very convenient thing to deal with because the predicted distribution and the marginal distributions are Gaussian.",
            "And analytics, so this is a really nice really nice way to talk about distribute."
        ],
        [
            "It's on functions, and the idea of applying that to probability density functions is neat and also not a particularly new idea.",
            "It's been around for 30 years or so, and in general the way people have preceded with this kind of thing is to consider some bounded interval, and then they arrive at a density F by taking the draw from the Gaussian process which I'm calling G here, exponentiating that to make sure that it's positive, and then dividing by its integral to make sure that integrates to one.",
            "But of course this kind of thing is a lot easier said than done because.",
            "GPS don't give you functions where they give you is sets of points and so you don't actually of course know the function everywhere and this normalization constant is essentially impossible to evaluate.",
            "Most of the literature has focused on finding finite dimensional approximations to the logistic Gaussian process so that they can so they can tractably deal with inference an approximate the normalization constant numerically."
        ],
        [
            "What this talk is about is away to a different way to specify a prior on probability density functions that is still rooted in a Gaussian process, but it allows us to do inference without making approximations along the way, and we call this method the Gaussian processes density sampler and the big trick is is that our prior allows us to generate exact and exchangeable data from a common density drawn from the prior, and we don't have to make any of these finite dimensional kind of approach."
        ],
        [
            "Nations.",
            "So First off, I'm going to tell you about the prior and I'm going to spend most of the talk talking about the prior because I feel like that's the most interesting thing.",
            "And if you understand the prior, then you know you can figure out the inference later and I'm going to get my hands a lot more in the inferences.",
            "Well, basically the idea with this prior is that we're still going from a Gaussian process.",
            "Draw G and trying to arrive at a density F, but rather than exponentiating it, we're going to do is shove it through a sigmoid essentially.",
            "So I'm representing that with this.",
            "With this capital Phi.",
            "In general, is just a sum non negative function that has an upper bound, but for all intents and purposes you can just consider this to be the logistic function.",
            "That's completely reasonable to assume that file.",
            "Here is the logistic.",
            "We also were going to introduce what I call a base density Pi, which is just some maybe parametric density that you know that you choose based on your domain and if your Gaussian processes zero mean and you have a logistic for squashing function here, then your prior on densities has pie as it's.",
            "As its mean density zed here is the normalization constant that ensures that this thing still integrates to one.",
            "And of course, as in the logistic Gaussian process, we can't actually evaluate this dead.",
            "Nonetheless, the big trick with this is that we're going to be able to generate exact and exchangeable samples from a common density drawn from this prior."
        ],
        [
            "So.",
            "To describe how we do this, let's first just imagine again.",
            "Here's the prior on top here that G is just some known parametric functions, something that we can evaluate whenever we want wherever we want, we could sample from this using rejection sampling in a fairly straightforward way.",
            "We would draw first proposal from the base density.",
            "We would then draw uniformly from beneath the bound defined by the sigmoid, and then we would accept or reject based on whether or not that uniform draw was beneath was beneath this Phi squashed curve.",
            "To give you a better idea of how this would work."
        ],
        [
            "I'm going to show you graphically here.",
            "OK, so let's imagine that Pi this this base density is just uniform between zero and one, and what I'm showing you here in the vertical coordinate is the five squash function G, so we're stuck between zero and one and we just got this function that I just pulled out of the air, but that we know everywhere."
        ],
        [
            "OK, so the way that rejection sampling would work, we would draw first uniformly in the horizontal coordinate."
        ],
        [
            "Then we would draw in the vertical coordinate and if this Blue Cross here was beneath the curve, we would accept this point, otherwise we would reach."
        ],
        [
            "So that when we accept."
        ],
        [
            "We do it again.",
            "Draw horizontally."
        ],
        [
            "Draw vertically, except this one."
        ],
        [
            "Do."
        ],
        [
            "Same thing again.",
            "This time we this time we reject and if we do this."
        ],
        [
            "A whole bunch of times then."
        ],
        [
            "We can then what will wind up with is that the marginal distribution over these horizontal coordinates over the accepted proposals will be drawn from the density implied by this curve.",
            "OK, so this is just this is just off the shelf rejection sampling."
        ],
        [
            "Alright.",
            "In our case, what we're going to do is is, say, the G is itself this random function, and So what we're going to do is augment this rejection sampling to include a drawing.",
            "The sample of the G function from the Gaussian process, and what you're going to see is that we can get away with this as long as we keep track of our function and still wind up with with samples.",
            "So before I draw that vertical coordinate, now I'm going to, I'm going to sample the GP condition and everything I've seen before."
        ],
        [
            "So looking at the same kind of picture, but now I'm representing kind of my marginal uncertainty about the about the function.",
            "So here we don't know anything about the function."
        ],
        [
            "So I'm going to draw in the horizontal coordinate and now I'm."
        ],
        [
            "In a sample from the GP, so this is starting to look just kind of like that.",
            "Links of sausage plots that you see all the time with Gaussian processes.",
            "And this is a."
        ],
        [
            "Information to accept or reject based on a vertical coordinate because we know what that point is, right?",
            "And so."
        ],
        [
            "Now we're going to accept this point.",
            "Make another proposal.",
            "Now I'm going to sample from the."
        ],
        [
            "Awesome process conditioned on the stuff already know about the function.",
            "And I can."
        ],
        [
            "Do the same thing again in the vertical coordinate."
        ],
        [
            "I'm going to reject the same thing."
        ],
        [
            "Again, and gradually I'm constraining the function."
        ],
        [
            "The event, the distribution of functions that remain."
        ],
        [
            "And if we do this a whole bunch, then what happens is we tie down, we tie down the function.",
            "A bunch of locations and at the same time we're generating, we're generating data from a from a density implied by the something left within that within that that distribution over functions.",
            "OK, so this is the basic idea where cut we're sticking a GP inside of a rejection sampler here and discovering discovering this latent function at the same time."
        ],
        [
            "As we're drawing samples.",
            "So things to notice about this procedure are the rejection.",
            "Sampling is exact, and when I say exact, I mean that the data that you get out of this is not biased by, for example, the starting state of the finite Markov chain.",
            "Also it's exchangeable, which just means that you can refer to the data and will have the same probability.",
            "You can convince yourself that this is exchange pulled by considering the fact that even though I presented it sequentially, I could have just drawn a whole bunch of data from that base density and then drawing the GP from the multivariate Gaussian distribution that was implied by all of those.",
            "All those draws in the base density.",
            "And then all of the acceptance or rejection draws were just independent Bernoulli's right?",
            "So that doesn't introduce the concept of ordering, so you can kind of convince yourself of exchangeability.",
            "That way, just something Bayesians like for definitely theorem and different things.",
            "So we were able to do this while only sampling the latent function at a finite number of locations.",
            "And also we never had to evaluate the normalization constant, and both of these properties will be really useful for inference.",
            "But first let me."
        ],
        [
            "If you kind of an idea about what you can get, what, what samples from the prior look like?",
            "I'm just going to show you some different things where the base density is A is a spherical, Gaussian are going to contours, and then some.",
            "Also some data and I'm just using your really basic squared exponential covariance function and I'm going to.",
            "I'm going to tweak the link scales and amplitude just a little bit to give you a feel for this prior.",
            "So starting off we have links unit link scales in unit amplitude and you can see that what you get out is just kind of a wobbly looking version of that underlying spherical Gaussian."
        ],
        [
            "But what we can do is turn up the amplitude and this causes the sample functions to be much more likely to saturate the sigmoid to 0 or to one, and so then you get these cliffs and plateaus where it's one the density is a lot like the underlying is a lot like the underlying spherical Gaussian and then obviously where it's zero.",
            "There's no mass at all, so you get these kind of cliffs."
        ],
        [
            "Similarly, we can turn down the link scale and then get these neat little blobby features.",
            "If the link scale is less than kind of the characteristic length scale of the underlying."
        ],
        [
            "Base density and then of course we can also use different scales in different dimensions if we want."
        ],
        [
            "Alright, so that's pretty much an overview of the way that we deal with this prior and how we generate data.",
            "And now I'm going to give you a fairly brief overview of how we how we go about doing inference with this prior.",
            "Basically, this is just this is the problem where we use this prior, we get some data and we'd like to talk about the posterior distribution on this latent function, and we're going to use is just a Markov chain Monte Carlo method, just your basic."
        ],
        [
            "Metropolis Hastings, and as I said, I'm going to gloss over a lot of the details, but first, let's have a look at why this is hard in general, and and the reason is because of this pesky normalization constant.",
            "Just like an undirected graphical models with the partition function, we can't even evaluate the likelihood except for to only within a constant an.",
            "This can make this can make inference really difficult even with Markov chain Monte Carlo, as you can see if we looked at if we tried to set up Metropolis Hastings, even the acceptance ratio of some new proposal of a latent function would would be.",
            "Having intractable ratio normalization constants, so without being able to evaluate the acceptance ratio exactly, we can't get the correct equilibrium distribution."
        ],
        [
            "Fortunately, however, there are a couple of Markov chain Monte Carlo techniques that allow one to two sidestep these problems.",
            "In the specific case where you can generate exact data from a particular parameter setting, and this has been more broadly used in undirected, undirected graphical models, like icing models, where you can use coupling from the past to get your exact data.",
            "But in this case we're going to use it with rejection sampling.",
            "In this prior I've just described, so we have two different Markov chain Monte Carlo methods that seem to work reasonably well.",
            "I'm just going to give you a really hand WAVY overview.",
            "Overview of these guys.",
            "The first is exchange sampling and the 2nd is inference of the latent histories, and both of these are discussed by Ian Murray in his PhD thesis.",
            "If you want to know a lot more detail."
        ],
        [
            "First off, exchange sampling.",
            "Is an auxiliary variable.",
            "Method is just like Metropolis Hastings, but with some extra features and the basic idea is that we're going to add auxiliary variables to our Markov state to cause the intractable constants to cancel out, but it depends on the ability to generate exact fantasies from your new parameter settings, so your basic metropolis Hastings step goes.",
            "Make your new proposals fantasize data from this, and then yes, there's a question.",
            "Proposal make that evaluated Anita points of anything and then no, no this would be some of the details I'm glossing over, but the But basically you have some knowledge of the function that you need to.",
            "You need to keep around, essentially, so if you generate that fantasy then there were rejections and acceptances along the way.",
            "So essentially anything you've ever learned about a particular function that lives in the Markov state, you have to keep around so it's the data plus plus some other things that I can tell you about offline if you'd like.",
            "Um?",
            "But but basically, let's just say that some some countable amount of information that you need to keep around for for any given any given decision in your Markov chain.",
            "OK, so we take our proposal, we generate fantasy data using that procedure I just described, and then what you do is you propose exchanging the current Markov state for your proposed Markov state, keeping the fantasy data sort of where it is, and the idea is that you're asking your current Markov state.",
            "Now to explain the fantasy data and your.",
            "And your proposal to explain the true data and vice versa, an the ratio of these joint distributions determines whether or not you should accept, and the tough bits cancel out.",
            "So that's that's a neat method, and if this is something interesting, I def."
        ],
        [
            "We recommend you checking out exchange sampling, but the other thing that we do, and I think this is this, seems to work better in practice and this little little bit easier to understand is that if we're using the Gaussian, this this Gaussian process density sampler prior to model data what we're saying is that we can data is the result of having run this run this generative procedure, and then at the end of this year procedure we got some data, and so if we just knew the whole state of of that rejection sampler, then that would tell us everything that we needed to know.",
            "About about the state of that of the.",
            "Of this kind of latent function.",
            "So the set of things that we don't know, though, are we don't know how many rejections there were along the way, and we don't know where those rejections a curd in the data space, and we don't know any of the function values anywhere, yeah?",
            "Oh sorry.",
            "So, but this is something over which we have a complete probabilistic model, so this is so we can perform inference on this, and basically because the general procedure didn't require evaluation normalization constant, and we don't need to evaluate it to do inference either.",
            "This is a little bit kind of a funny thing to talk about, because on one hand we have this Metropolis Hastings algorithm that is going to have its rejections and acceptances as you wander around.",
            "But then you're actually running this Markov chain on the state of another of another rejection sampling procedure that also has rejections and acceptances.",
            "So just kind of like measures on measures were also, our model actually is a is a sampling procedure."
        ],
        [
            "Soft.",
            "Along the way, with both these methods we can need stuff like infer the Gaussian process hyperparameters, which is nice because it's kind of like learning the bandwidth of kernel density estimation in a Bayesian way.",
            "And if we introduce parameters for our base density, we can learn those as well, and we can also easily generate samples from the predicted distribution.",
            "This is the one that arises on data space after you integrate your posterior."
        ],
        [
            "Alright, now I'm just going to go over this real quickly, but I want to make sure I got to put skulls on my slides.",
            "Basically we looked at some mckaskle data.",
            "We had 228 macaques.",
            "And we looked at linear distances or log linear distances between anatomically significant landmarks, and there were three trials of each of these, and we wanted to take a look and just see how it did empirically versus, say, parzen windows with with the bandwidth set via cross validation and then also the infinite mixture of Gaussians and the Dursley diffusion trees.",
            "Both of those we used FBM from Radford, Neal, and you can see that.",
            "Well, So what these barplots are showing or each of the three trials where the different.",
            "Of the of the measurements, and this is the improvement in log probability over parzen windows.",
            "Anyway, so basically if you can just see that it does roughly equivalently to the other nonparametric Bayesian methods.",
            "So it's not."
        ],
        [
            "Totally, totally insane.",
            "But what isn't saying potentially is the computational concerns for this thing, which I feel like I'm obligated to mention.",
            "Obviously Gaussian processes are expensive to deal with in general, typically looking at at Ncube computation in your data, and then also on top of that rejection sampling is in general quite inefficient in high dimensions you can get a very large number of rejections, and the way that manifests itself in exchange sampling is that it could be very expensive to generate fantasies for your inference and in the latent history method you may wind up modeling a very large number of latent rejections.",
            "In your space."
        ],
        [
            "Alright, so just to review, this has been a Gaussian process prior on probability density functions with the unique characteristic that we can generate exact and exchangeable data from it, and this allows us to do tractable inference without having to make intermediate approximations.",
            "And as far as we know, this is the only method that's been able to do that, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, let's just to get on.",
                    "label": 0
                },
                {
                    "sent": "Let's just get on the same page and what am I talking about with the density modeling problem?",
                    "label": 0
                },
                {
                    "sent": "Well, I just mean we have some set of data drawn from a density that we don't know and we have some prior beliefs about the density.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we'd like to perform posterior inference, so this is just the I mean, this is just Bayesian stuff.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, of course we're interested in nonparametric things here.",
                    "label": 0
                },
                {
                    "sent": "The classical approach to this being kernel density estimation, like parzen windows.",
                    "label": 0
                },
                {
                    "sent": "But we're Bayesians in this room, so the most popular thing, certainly with with Bayesian approaches, is the infinite mixture of parametric distributions, and there's been a lot of talks about that so far, and then, of course there's also additional diffusion trees, Gaussian process, latent variable, model density networks, and some other models as well.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to talk about today is density modeling.",
                    "label": 1
                },
                {
                    "sent": "We're putting a Gaussian process prior, essentially directly on the probability.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "City function itself.",
                    "label": 0
                },
                {
                    "sent": "Now, why is this an interesting thing to do?",
                    "label": 0
                },
                {
                    "sent": "Well, it's because Gaussian processes are very nice priors on functions.",
                    "label": 0
                },
                {
                    "sent": "You don't have to specify an explicit set of basis functions.",
                    "label": 0
                },
                {
                    "sent": "You can pick your input space, which for this talk you can just assume is the D dimensional real space, and then you pick a covariance function that expresses your prior beliefs about, say, the smoothness of the of the functions in your distribution, and then a mean function which will take to be 0.",
                    "label": 0
                },
                {
                    "sent": "And it's a very convenient thing to deal with because the predicted distribution and the marginal distributions are Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And analytics, so this is a really nice really nice way to talk about distribute.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's on functions, and the idea of applying that to probability density functions is neat and also not a particularly new idea.",
                    "label": 1
                },
                {
                    "sent": "It's been around for 30 years or so, and in general the way people have preceded with this kind of thing is to consider some bounded interval, and then they arrive at a density F by taking the draw from the Gaussian process which I'm calling G here, exponentiating that to make sure that it's positive, and then dividing by its integral to make sure that integrates to one.",
                    "label": 1
                },
                {
                    "sent": "But of course this kind of thing is a lot easier said than done because.",
                    "label": 0
                },
                {
                    "sent": "GPS don't give you functions where they give you is sets of points and so you don't actually of course know the function everywhere and this normalization constant is essentially impossible to evaluate.",
                    "label": 0
                },
                {
                    "sent": "Most of the literature has focused on finding finite dimensional approximations to the logistic Gaussian process so that they can so they can tractably deal with inference an approximate the normalization constant numerically.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What this talk is about is away to a different way to specify a prior on probability density functions that is still rooted in a Gaussian process, but it allows us to do inference without making approximations along the way, and we call this method the Gaussian processes density sampler and the big trick is is that our prior allows us to generate exact and exchangeable data from a common density drawn from the prior, and we don't have to make any of these finite dimensional kind of approach.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nations.",
                    "label": 0
                },
                {
                    "sent": "So First off, I'm going to tell you about the prior and I'm going to spend most of the talk talking about the prior because I feel like that's the most interesting thing.",
                    "label": 0
                },
                {
                    "sent": "And if you understand the prior, then you know you can figure out the inference later and I'm going to get my hands a lot more in the inferences.",
                    "label": 0
                },
                {
                    "sent": "Well, basically the idea with this prior is that we're still going from a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Draw G and trying to arrive at a density F, but rather than exponentiating it, we're going to do is shove it through a sigmoid essentially.",
                    "label": 0
                },
                {
                    "sent": "So I'm representing that with this.",
                    "label": 0
                },
                {
                    "sent": "With this capital Phi.",
                    "label": 0
                },
                {
                    "sent": "In general, is just a sum non negative function that has an upper bound, but for all intents and purposes you can just consider this to be the logistic function.",
                    "label": 0
                },
                {
                    "sent": "That's completely reasonable to assume that file.",
                    "label": 0
                },
                {
                    "sent": "Here is the logistic.",
                    "label": 0
                },
                {
                    "sent": "We also were going to introduce what I call a base density Pi, which is just some maybe parametric density that you know that you choose based on your domain and if your Gaussian processes zero mean and you have a logistic for squashing function here, then your prior on densities has pie as it's.",
                    "label": 0
                },
                {
                    "sent": "As its mean density zed here is the normalization constant that ensures that this thing still integrates to one.",
                    "label": 0
                },
                {
                    "sent": "And of course, as in the logistic Gaussian process, we can't actually evaluate this dead.",
                    "label": 1
                },
                {
                    "sent": "Nonetheless, the big trick with this is that we're going to be able to generate exact and exchangeable samples from a common density drawn from this prior.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To describe how we do this, let's first just imagine again.",
                    "label": 0
                },
                {
                    "sent": "Here's the prior on top here that G is just some known parametric functions, something that we can evaluate whenever we want wherever we want, we could sample from this using rejection sampling in a fairly straightforward way.",
                    "label": 1
                },
                {
                    "sent": "We would draw first proposal from the base density.",
                    "label": 0
                },
                {
                    "sent": "We would then draw uniformly from beneath the bound defined by the sigmoid, and then we would accept or reject based on whether or not that uniform draw was beneath was beneath this Phi squashed curve.",
                    "label": 0
                },
                {
                    "sent": "To give you a better idea of how this would work.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to show you graphically here.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's imagine that Pi this this base density is just uniform between zero and one, and what I'm showing you here in the vertical coordinate is the five squash function G, so we're stuck between zero and one and we just got this function that I just pulled out of the air, but that we know everywhere.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the way that rejection sampling would work, we would draw first uniformly in the horizontal coordinate.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we would draw in the vertical coordinate and if this Blue Cross here was beneath the curve, we would accept this point, otherwise we would reach.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that when we accept.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We do it again.",
                    "label": 0
                },
                {
                    "sent": "Draw horizontally.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Draw vertically, except this one.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same thing again.",
                    "label": 0
                },
                {
                    "sent": "This time we this time we reject and if we do this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A whole bunch of times then.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can then what will wind up with is that the marginal distribution over these horizontal coordinates over the accepted proposals will be drawn from the density implied by this curve.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just this is just off the shelf rejection sampling.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "In our case, what we're going to do is is, say, the G is itself this random function, and So what we're going to do is augment this rejection sampling to include a drawing.",
                    "label": 0
                },
                {
                    "sent": "The sample of the G function from the Gaussian process, and what you're going to see is that we can get away with this as long as we keep track of our function and still wind up with with samples.",
                    "label": 0
                },
                {
                    "sent": "So before I draw that vertical coordinate, now I'm going to, I'm going to sample the GP condition and everything I've seen before.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So looking at the same kind of picture, but now I'm representing kind of my marginal uncertainty about the about the function.",
                    "label": 0
                },
                {
                    "sent": "So here we don't know anything about the function.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to draw in the horizontal coordinate and now I'm.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a sample from the GP, so this is starting to look just kind of like that.",
                    "label": 0
                },
                {
                    "sent": "Links of sausage plots that you see all the time with Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "And this is a.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Information to accept or reject based on a vertical coordinate because we know what that point is, right?",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we're going to accept this point.",
                    "label": 0
                },
                {
                    "sent": "Make another proposal.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to sample from the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Awesome process conditioned on the stuff already know about the function.",
                    "label": 0
                },
                {
                    "sent": "And I can.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do the same thing again in the vertical coordinate.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to reject the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, and gradually I'm constraining the function.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The event, the distribution of functions that remain.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we do this a whole bunch, then what happens is we tie down, we tie down the function.",
                    "label": 0
                },
                {
                    "sent": "A bunch of locations and at the same time we're generating, we're generating data from a from a density implied by the something left within that within that that distribution over functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the basic idea where cut we're sticking a GP inside of a rejection sampler here and discovering discovering this latent function at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we're drawing samples.",
                    "label": 0
                },
                {
                    "sent": "So things to notice about this procedure are the rejection.",
                    "label": 0
                },
                {
                    "sent": "Sampling is exact, and when I say exact, I mean that the data that you get out of this is not biased by, for example, the starting state of the finite Markov chain.",
                    "label": 0
                },
                {
                    "sent": "Also it's exchangeable, which just means that you can refer to the data and will have the same probability.",
                    "label": 0
                },
                {
                    "sent": "You can convince yourself that this is exchange pulled by considering the fact that even though I presented it sequentially, I could have just drawn a whole bunch of data from that base density and then drawing the GP from the multivariate Gaussian distribution that was implied by all of those.",
                    "label": 0
                },
                {
                    "sent": "All those draws in the base density.",
                    "label": 0
                },
                {
                    "sent": "And then all of the acceptance or rejection draws were just independent Bernoulli's right?",
                    "label": 0
                },
                {
                    "sent": "So that doesn't introduce the concept of ordering, so you can kind of convince yourself of exchangeability.",
                    "label": 0
                },
                {
                    "sent": "That way, just something Bayesians like for definitely theorem and different things.",
                    "label": 0
                },
                {
                    "sent": "So we were able to do this while only sampling the latent function at a finite number of locations.",
                    "label": 0
                },
                {
                    "sent": "And also we never had to evaluate the normalization constant, and both of these properties will be really useful for inference.",
                    "label": 0
                },
                {
                    "sent": "But first let me.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you kind of an idea about what you can get, what, what samples from the prior look like?",
                    "label": 0
                },
                {
                    "sent": "I'm just going to show you some different things where the base density is A is a spherical, Gaussian are going to contours, and then some.",
                    "label": 0
                },
                {
                    "sent": "Also some data and I'm just using your really basic squared exponential covariance function and I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to tweak the link scales and amplitude just a little bit to give you a feel for this prior.",
                    "label": 0
                },
                {
                    "sent": "So starting off we have links unit link scales in unit amplitude and you can see that what you get out is just kind of a wobbly looking version of that underlying spherical Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But what we can do is turn up the amplitude and this causes the sample functions to be much more likely to saturate the sigmoid to 0 or to one, and so then you get these cliffs and plateaus where it's one the density is a lot like the underlying is a lot like the underlying spherical Gaussian and then obviously where it's zero.",
                    "label": 0
                },
                {
                    "sent": "There's no mass at all, so you get these kind of cliffs.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarly, we can turn down the link scale and then get these neat little blobby features.",
                    "label": 0
                },
                {
                    "sent": "If the link scale is less than kind of the characteristic length scale of the underlying.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Base density and then of course we can also use different scales in different dimensions if we want.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so that's pretty much an overview of the way that we deal with this prior and how we generate data.",
                    "label": 0
                },
                {
                    "sent": "And now I'm going to give you a fairly brief overview of how we how we go about doing inference with this prior.",
                    "label": 0
                },
                {
                    "sent": "Basically, this is just this is the problem where we use this prior, we get some data and we'd like to talk about the posterior distribution on this latent function, and we're going to use is just a Markov chain Monte Carlo method, just your basic.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Metropolis Hastings, and as I said, I'm going to gloss over a lot of the details, but first, let's have a look at why this is hard in general, and and the reason is because of this pesky normalization constant.",
                    "label": 0
                },
                {
                    "sent": "Just like an undirected graphical models with the partition function, we can't even evaluate the likelihood except for to only within a constant an.",
                    "label": 0
                },
                {
                    "sent": "This can make this can make inference really difficult even with Markov chain Monte Carlo, as you can see if we looked at if we tried to set up Metropolis Hastings, even the acceptance ratio of some new proposal of a latent function would would be.",
                    "label": 1
                },
                {
                    "sent": "Having intractable ratio normalization constants, so without being able to evaluate the acceptance ratio exactly, we can't get the correct equilibrium distribution.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fortunately, however, there are a couple of Markov chain Monte Carlo techniques that allow one to two sidestep these problems.",
                    "label": 0
                },
                {
                    "sent": "In the specific case where you can generate exact data from a particular parameter setting, and this has been more broadly used in undirected, undirected graphical models, like icing models, where you can use coupling from the past to get your exact data.",
                    "label": 1
                },
                {
                    "sent": "But in this case we're going to use it with rejection sampling.",
                    "label": 0
                },
                {
                    "sent": "In this prior I've just described, so we have two different Markov chain Monte Carlo methods that seem to work reasonably well.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to give you a really hand WAVY overview.",
                    "label": 0
                },
                {
                    "sent": "Overview of these guys.",
                    "label": 0
                },
                {
                    "sent": "The first is exchange sampling and the 2nd is inference of the latent histories, and both of these are discussed by Ian Murray in his PhD thesis.",
                    "label": 0
                },
                {
                    "sent": "If you want to know a lot more detail.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First off, exchange sampling.",
                    "label": 0
                },
                {
                    "sent": "Is an auxiliary variable.",
                    "label": 0
                },
                {
                    "sent": "Method is just like Metropolis Hastings, but with some extra features and the basic idea is that we're going to add auxiliary variables to our Markov state to cause the intractable constants to cancel out, but it depends on the ability to generate exact fantasies from your new parameter settings, so your basic metropolis Hastings step goes.",
                    "label": 0
                },
                {
                    "sent": "Make your new proposals fantasize data from this, and then yes, there's a question.",
                    "label": 0
                },
                {
                    "sent": "Proposal make that evaluated Anita points of anything and then no, no this would be some of the details I'm glossing over, but the But basically you have some knowledge of the function that you need to.",
                    "label": 0
                },
                {
                    "sent": "You need to keep around, essentially, so if you generate that fantasy then there were rejections and acceptances along the way.",
                    "label": 0
                },
                {
                    "sent": "So essentially anything you've ever learned about a particular function that lives in the Markov state, you have to keep around so it's the data plus plus some other things that I can tell you about offline if you'd like.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But but basically, let's just say that some some countable amount of information that you need to keep around for for any given any given decision in your Markov chain.",
                    "label": 0
                },
                {
                    "sent": "OK, so we take our proposal, we generate fantasy data using that procedure I just described, and then what you do is you propose exchanging the current Markov state for your proposed Markov state, keeping the fantasy data sort of where it is, and the idea is that you're asking your current Markov state.",
                    "label": 0
                },
                {
                    "sent": "Now to explain the fantasy data and your.",
                    "label": 0
                },
                {
                    "sent": "And your proposal to explain the true data and vice versa, an the ratio of these joint distributions determines whether or not you should accept, and the tough bits cancel out.",
                    "label": 0
                },
                {
                    "sent": "So that's that's a neat method, and if this is something interesting, I def.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We recommend you checking out exchange sampling, but the other thing that we do, and I think this is this, seems to work better in practice and this little little bit easier to understand is that if we're using the Gaussian, this this Gaussian process density sampler prior to model data what we're saying is that we can data is the result of having run this run this generative procedure, and then at the end of this year procedure we got some data, and so if we just knew the whole state of of that rejection sampler, then that would tell us everything that we needed to know.",
                    "label": 0
                },
                {
                    "sent": "About about the state of that of the.",
                    "label": 0
                },
                {
                    "sent": "Of this kind of latent function.",
                    "label": 0
                },
                {
                    "sent": "So the set of things that we don't know, though, are we don't know how many rejections there were along the way, and we don't know where those rejections a curd in the data space, and we don't know any of the function values anywhere, yeah?",
                    "label": 0
                },
                {
                    "sent": "Oh sorry.",
                    "label": 0
                },
                {
                    "sent": "So, but this is something over which we have a complete probabilistic model, so this is so we can perform inference on this, and basically because the general procedure didn't require evaluation normalization constant, and we don't need to evaluate it to do inference either.",
                    "label": 0
                },
                {
                    "sent": "This is a little bit kind of a funny thing to talk about, because on one hand we have this Metropolis Hastings algorithm that is going to have its rejections and acceptances as you wander around.",
                    "label": 0
                },
                {
                    "sent": "But then you're actually running this Markov chain on the state of another of another rejection sampling procedure that also has rejections and acceptances.",
                    "label": 0
                },
                {
                    "sent": "So just kind of like measures on measures were also, our model actually is a is a sampling procedure.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Soft.",
                    "label": 0
                },
                {
                    "sent": "Along the way, with both these methods we can need stuff like infer the Gaussian process hyperparameters, which is nice because it's kind of like learning the bandwidth of kernel density estimation in a Bayesian way.",
                    "label": 0
                },
                {
                    "sent": "And if we introduce parameters for our base density, we can learn those as well, and we can also easily generate samples from the predicted distribution.",
                    "label": 0
                },
                {
                    "sent": "This is the one that arises on data space after you integrate your posterior.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, now I'm just going to go over this real quickly, but I want to make sure I got to put skulls on my slides.",
                    "label": 0
                },
                {
                    "sent": "Basically we looked at some mckaskle data.",
                    "label": 0
                },
                {
                    "sent": "We had 228 macaques.",
                    "label": 0
                },
                {
                    "sent": "And we looked at linear distances or log linear distances between anatomically significant landmarks, and there were three trials of each of these, and we wanted to take a look and just see how it did empirically versus, say, parzen windows with with the bandwidth set via cross validation and then also the infinite mixture of Gaussians and the Dursley diffusion trees.",
                    "label": 0
                },
                {
                    "sent": "Both of those we used FBM from Radford, Neal, and you can see that.",
                    "label": 0
                },
                {
                    "sent": "Well, So what these barplots are showing or each of the three trials where the different.",
                    "label": 0
                },
                {
                    "sent": "Of the of the measurements, and this is the improvement in log probability over parzen windows.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so basically if you can just see that it does roughly equivalently to the other nonparametric Bayesian methods.",
                    "label": 0
                },
                {
                    "sent": "So it's not.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Totally, totally insane.",
                    "label": 0
                },
                {
                    "sent": "But what isn't saying potentially is the computational concerns for this thing, which I feel like I'm obligated to mention.",
                    "label": 0
                },
                {
                    "sent": "Obviously Gaussian processes are expensive to deal with in general, typically looking at at Ncube computation in your data, and then also on top of that rejection sampling is in general quite inefficient in high dimensions you can get a very large number of rejections, and the way that manifests itself in exchange sampling is that it could be very expensive to generate fantasies for your inference and in the latent history method you may wind up modeling a very large number of latent rejections.",
                    "label": 0
                },
                {
                    "sent": "In your space.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so just to review, this has been a Gaussian process prior on probability density functions with the unique characteristic that we can generate exact and exchangeable data from it, and this allows us to do tractable inference without having to make intermediate approximations.",
                    "label": 0
                },
                {
                    "sent": "And as far as we know, this is the only method that's been able to do that, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}