{
    "id": "lbiembcax5uhq4cdrkfckakuar2tvtjb",
    "title": "A Generalized Co-HITS Algorithm and Its Application to Bipartite Graphs",
    "info": {
        "author": [
            "Hongbo Deng, Chinese University of Hong Kong"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "July 2009",
        "category": [
            "Top->Computer Science->Web Mining"
        ]
    },
    "url": "http://videolectures.net/kdd09_deng_gchaiabg/",
    "segmentation": [
        [
            "OK. OK, thank you.",
            "This is John to work with a supervisor.",
            "My cardio and Evan came from the Chinese University of Hong Kong."
        ],
        [
            "In data, many and web search applications and many data can be modeled as a bipartite graph, such as the queries, an URLs inquiry log data, the paper, an authors in the scientific literature.",
            "So here is an example of bipartite graph.",
            "The what is this in the bipartite graph can be divided into 2 disjoint sets U and we an every age connects a vertex in U2 of X in V. For a given bipartite graph, basically we have two kinds of information, the content information and graph information, so many existing information retrieval, such as vector space, model, language model to cap to capture the which could be used to obtain the relevance score for a given query based on the content of the vertex and.",
            "Many men in analysis, Mercer had have been proposed.",
            "To capture some semantical relations within the graph so as the content and graph provides different information, it's reasonable to combine the content then graph in a unified unified framework."
        ],
        [
            "Let's take the query log data as an example.",
            "Here is an by query URL bipartite graph based on the query log data an suppose our task is to find similar queries for a given query.",
            "The query map in this case if we only consider graph information, let's start from the query map doing random work between query and URLs.",
            "Then some of it can reach some queries with a high probability based on the random Oracle.",
            "So.",
            "And finally, we can get such kind of results.",
            "So as you can see, some of the results are not likely to be similar to.",
            "The Giving query map and how can the query like Google come up for the query map as a similar queries based on the graph information the.",
            "The reason is that there are many noisy links within the graph, especially for some general portal record www.google.com which is connected with many queries and query log data, so the.",
            "So the this point, it can aggregate large number after random rock.",
            "So finally finally the score.",
            "The largest score will be propagated to its highly connected query.",
            "Google.",
            "That's the reason why.",
            "We get such a results, so we can say that.",
            "We only consider the graph information.",
            "It's sensitive to the some noise links.",
            "That a full we consider the content information that tax information from the query side using the personalized Pagerank we can get such kind of results.",
            "We can see the relevance of the Google can be reduced and some other relevant queries will be can be boots.",
            "But it's not.",
            "In average, their lack of constraints, so some more reasonable results are desired.",
            "This one to achieve this goal, we propose the Cohetes algorithm."
        ],
        [
            "Here's the alternate of this talk, and then I will start to introduce the."
        ],
        [
            "His algorithm.",
            "Here's a here.",
            "We introduce some preliminaries.",
            "We use XO&Y zero to denote the initial relevance score for each entities based on the content information of this vertex and in terms of the graph, we use WV to denote the.",
            "Translation matches from U to V An using WVU to denote the transition matrix from V to you.",
            "Besides the explicit link, we can infer some hidden links within you and we based on this equation.",
            "So we using Wu.",
            "And WBV to denote the hidden links with."
        ],
        [
            "In the graph.",
            "The basic idea of our work is to incorporate the bipartite graph with the content information from both sides and a simple way is to initialize the vertex using the relevance score X0Y0 based on the content information and propagated the score on the graph iteratively, as shown in the figure A.",
            "The score of discover we K is propagated too.",
            "The vertex UI and similar additional scores are propagated from other verticies of V2 UI and then UI is updated according the aggregated score along with its initial relevance score, so it can get new value XI in Figure B.",
            "The new value XI is propagated to VK and VK is updated along with its initial.",
            "Times relevance called.",
            "In this way, the skull is propagated on the graph and updated along with its initial relevance score.",
            "This process can be formalized.",
            "This process can be.",
            "Defined by this equation and Lambda is the parameter to balance the initial relevance score and the scope propagation over the graph."
        ],
        [
            "The above equation can be combined.",
            "This combined equation and in general it contains several algorithms special as a special case.",
            "So when Lambda UN Lambda we is equal to whites, the general its original hits algorithm.",
            "When done that we is equal to 1 then it's the personalized page rank so and."
        ],
        [
            "Some other cases.",
            "So now we have.",
            "We have introduced the iterative framework.",
            "Now I would like to.",
            "Describe the Chris bonding regularization from rock.",
            "First, let me let me consider the one special case, the personalized Pagerank.",
            "Then one single cider you, and in some sense it's equivalent to the label propagation.",
            "In the semi supervised graph based semi supervised learning.",
            "So regularization framework can be developed for the label propagation.",
            "By their cost function too.",
            "By regularizes smoothness.",
            "Over the solution is of the relevance score over the graph and regularizer to fit its initial relevance score, so that's a connection between the iterative from marker and our regularization."
        ],
        [
            "Work.",
            "So based on you we can get the custom function R1 similar based on V. We get another custom function R2.",
            "So, but we have you tonight.",
            "The Hayden links within you an we but we believe the direct link WV and WVU will provide more valuable informations.",
            "So we develop a new cost function R3 to take into the direct information between you and we and.",
            "Based on this three cost function, we develop a new joint cost function.",
            "And in this paper we simply set out."
        ],
        [
            "Equals 2 one.",
            "And now the problem is to solve the.",
            "Optimization problem.",
            "Here is the this.",
            "This cost function can be transformed to this form and then the data.",
            "the W is the combined matrix based on previous hidden links and direct links matrix.",
            "And after differentiating and simplify, will get the solution.",
            "Then the given the initial relevant score and the metrics transition matrix.",
            "We can compute the final score directly here the when Lambda is equal to 1, it only consider the hidden links within U and we we called single sided regularization.",
            "When Lambda is set between zero to 1.",
            "Required to double side The Reg."
        ],
        [
            "Are azatian so in this paper we apply the Mercer to the query.",
            "You are bipartite graph for credit suggestions and for some more details you may refer."
        ],
        [
            "To our paper.",
            "And here I came to the expense."
        ],
        [
            "I meant Potter.",
            "Here's a set top of our our experiment.",
            "We use the LL query log and in our experiment we have about 1 million queries and 1 million URLs and there are about 5 million ages between the query and URL."
        ],
        [
            "Our evaluation will utilize the ODP similarity.",
            "It's it is prove it is proposed by bears yards in KDD 2007 and it's a simple measure to measure the similarity between queries using the ODP categories and in this.",
            "In addition, we.",
            "Measure the precision at rank N. And we.",
            "Randomly selector three hand requires."
        ],
        [
            "Evaluation so here is a result of the iterative framework that dashed line is the baseline only based on the content information for query similarity measure and the black line is the precision at 5, the blue line is the precision at 10.",
            "The first figure shows the results of the personalizer.",
            "Page rank with different parameter Lambda and we can say the.",
            "The performance has only a slight increase.",
            "When compared to the baseline and with the increase of Lambda, the performance is getting worse and even underperform the baseline and the right point means the it's only considered graph information.",
            "So the reason is there is some noisy link and its lack of constraints and the second figure is second figures.",
            "Once a special case of the iterative from Rock, one step propagation and set rather set.",
            "Figure is the general careers algorithm and we can see the improvement of the OSP and general cohete algorithm over the baseline are promising when compared to the personalized page rank.",
            "This is because the initial relevance score from both sides provide valuable."
        ],
        [
            "Formations.",
            "Here is the result of the regularization from work that first consider the single side regularization from work.",
            "The single side, the regularization, the results direct point means the only consider the graph information in the in the regularization and the left point is the baseline based on the content information and we can see the single side.",
            "Regularization can improve the performance over the baseline and then we fix the Lambda mu and set the parameter.",
            "The parameter Lambda here to evaluate the double sided regularization and compare with the single sided regularization.",
            "The red point is the performance of the single side, the regularization and we can see that when we incorporate and the new cost function are three the.",
            "The performance can be boosted further so."
        ],
        [
            "Some more details are shown in this.",
            "This table and this figure in this.",
            "In this figure, we compared 6 Mercer.",
            "Including three three members of the.",
            "With the.",
            "Iterative from rocker and two 2 regularization framework and top one line, the green line is the result of double sided regularization and we can see it achieves the best performance."
        ],
        [
            "So now let me conclude this talk in this in this.",
            "In this paper we propose the Cohetes algorithm to incorporate the bipartite graph.",
            "With the content information from both side an, we investigate this method from 2.",
            "Framework of the iterative from rock and the regularization from America based on different perspectives.",
            "And Cohetes algorithm is more general which including heats and personalized page rank as best case and the regularization framework is more robust with the new developed cost function which achieved the best performance."
        ],
        [
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "This is John to work with a supervisor.",
                    "label": 0
                },
                {
                    "sent": "My cardio and Evan came from the Chinese University of Hong Kong.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In data, many and web search applications and many data can be modeled as a bipartite graph, such as the queries, an URLs inquiry log data, the paper, an authors in the scientific literature.",
                    "label": 1
                },
                {
                    "sent": "So here is an example of bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "The what is this in the bipartite graph can be divided into 2 disjoint sets U and we an every age connects a vertex in U2 of X in V. For a given bipartite graph, basically we have two kinds of information, the content information and graph information, so many existing information retrieval, such as vector space, model, language model to cap to capture the which could be used to obtain the relevance score for a given query based on the content of the vertex and.",
                    "label": 0
                },
                {
                    "sent": "Many men in analysis, Mercer had have been proposed.",
                    "label": 0
                },
                {
                    "sent": "To capture some semantical relations within the graph so as the content and graph provides different information, it's reasonable to combine the content then graph in a unified unified framework.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's take the query log data as an example.",
                    "label": 0
                },
                {
                    "sent": "Here is an by query URL bipartite graph based on the query log data an suppose our task is to find similar queries for a given query.",
                    "label": 0
                },
                {
                    "sent": "The query map in this case if we only consider graph information, let's start from the query map doing random work between query and URLs.",
                    "label": 0
                },
                {
                    "sent": "Then some of it can reach some queries with a high probability based on the random Oracle.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And finally, we can get such kind of results.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, some of the results are not likely to be similar to.",
                    "label": 0
                },
                {
                    "sent": "The Giving query map and how can the query like Google come up for the query map as a similar queries based on the graph information the.",
                    "label": 0
                },
                {
                    "sent": "The reason is that there are many noisy links within the graph, especially for some general portal record www.google.com which is connected with many queries and query log data, so the.",
                    "label": 0
                },
                {
                    "sent": "So the this point, it can aggregate large number after random rock.",
                    "label": 0
                },
                {
                    "sent": "So finally finally the score.",
                    "label": 0
                },
                {
                    "sent": "The largest score will be propagated to its highly connected query.",
                    "label": 0
                },
                {
                    "sent": "Google.",
                    "label": 0
                },
                {
                    "sent": "That's the reason why.",
                    "label": 0
                },
                {
                    "sent": "We get such a results, so we can say that.",
                    "label": 0
                },
                {
                    "sent": "We only consider the graph information.",
                    "label": 0
                },
                {
                    "sent": "It's sensitive to the some noise links.",
                    "label": 0
                },
                {
                    "sent": "That a full we consider the content information that tax information from the query side using the personalized Pagerank we can get such kind of results.",
                    "label": 0
                },
                {
                    "sent": "We can see the relevance of the Google can be reduced and some other relevant queries will be can be boots.",
                    "label": 0
                },
                {
                    "sent": "But it's not.",
                    "label": 0
                },
                {
                    "sent": "In average, their lack of constraints, so some more reasonable results are desired.",
                    "label": 0
                },
                {
                    "sent": "This one to achieve this goal, we propose the Cohetes algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's the alternate of this talk, and then I will start to introduce the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "His algorithm.",
                    "label": 0
                },
                {
                    "sent": "Here's a here.",
                    "label": 0
                },
                {
                    "sent": "We introduce some preliminaries.",
                    "label": 0
                },
                {
                    "sent": "We use XO&Y zero to denote the initial relevance score for each entities based on the content information of this vertex and in terms of the graph, we use WV to denote the.",
                    "label": 0
                },
                {
                    "sent": "Translation matches from U to V An using WVU to denote the transition matrix from V to you.",
                    "label": 0
                },
                {
                    "sent": "Besides the explicit link, we can infer some hidden links within you and we based on this equation.",
                    "label": 0
                },
                {
                    "sent": "So we using Wu.",
                    "label": 0
                },
                {
                    "sent": "And WBV to denote the hidden links with.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the graph.",
                    "label": 0
                },
                {
                    "sent": "The basic idea of our work is to incorporate the bipartite graph with the content information from both sides and a simple way is to initialize the vertex using the relevance score X0Y0 based on the content information and propagated the score on the graph iteratively, as shown in the figure A.",
                    "label": 1
                },
                {
                    "sent": "The score of discover we K is propagated too.",
                    "label": 0
                },
                {
                    "sent": "The vertex UI and similar additional scores are propagated from other verticies of V2 UI and then UI is updated according the aggregated score along with its initial relevance score, so it can get new value XI in Figure B.",
                    "label": 0
                },
                {
                    "sent": "The new value XI is propagated to VK and VK is updated along with its initial.",
                    "label": 0
                },
                {
                    "sent": "Times relevance called.",
                    "label": 0
                },
                {
                    "sent": "In this way, the skull is propagated on the graph and updated along with its initial relevance score.",
                    "label": 0
                },
                {
                    "sent": "This process can be formalized.",
                    "label": 0
                },
                {
                    "sent": "This process can be.",
                    "label": 0
                },
                {
                    "sent": "Defined by this equation and Lambda is the parameter to balance the initial relevance score and the scope propagation over the graph.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The above equation can be combined.",
                    "label": 0
                },
                {
                    "sent": "This combined equation and in general it contains several algorithms special as a special case.",
                    "label": 0
                },
                {
                    "sent": "So when Lambda UN Lambda we is equal to whites, the general its original hits algorithm.",
                    "label": 0
                },
                {
                    "sent": "When done that we is equal to 1 then it's the personalized page rank so and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some other cases.",
                    "label": 0
                },
                {
                    "sent": "So now we have.",
                    "label": 0
                },
                {
                    "sent": "We have introduced the iterative framework.",
                    "label": 0
                },
                {
                    "sent": "Now I would like to.",
                    "label": 0
                },
                {
                    "sent": "Describe the Chris bonding regularization from rock.",
                    "label": 0
                },
                {
                    "sent": "First, let me let me consider the one special case, the personalized Pagerank.",
                    "label": 1
                },
                {
                    "sent": "Then one single cider you, and in some sense it's equivalent to the label propagation.",
                    "label": 0
                },
                {
                    "sent": "In the semi supervised graph based semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So regularization framework can be developed for the label propagation.",
                    "label": 1
                },
                {
                    "sent": "By their cost function too.",
                    "label": 0
                },
                {
                    "sent": "By regularizes smoothness.",
                    "label": 0
                },
                {
                    "sent": "Over the solution is of the relevance score over the graph and regularizer to fit its initial relevance score, so that's a connection between the iterative from marker and our regularization.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work.",
                    "label": 0
                },
                {
                    "sent": "So based on you we can get the custom function R1 similar based on V. We get another custom function R2.",
                    "label": 0
                },
                {
                    "sent": "So, but we have you tonight.",
                    "label": 0
                },
                {
                    "sent": "The Hayden links within you an we but we believe the direct link WV and WVU will provide more valuable informations.",
                    "label": 0
                },
                {
                    "sent": "So we develop a new cost function R3 to take into the direct information between you and we and.",
                    "label": 0
                },
                {
                    "sent": "Based on this three cost function, we develop a new joint cost function.",
                    "label": 0
                },
                {
                    "sent": "And in this paper we simply set out.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Equals 2 one.",
                    "label": 0
                },
                {
                    "sent": "And now the problem is to solve the.",
                    "label": 0
                },
                {
                    "sent": "Optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Here is the this.",
                    "label": 0
                },
                {
                    "sent": "This cost function can be transformed to this form and then the data.",
                    "label": 0
                },
                {
                    "sent": "the W is the combined matrix based on previous hidden links and direct links matrix.",
                    "label": 0
                },
                {
                    "sent": "And after differentiating and simplify, will get the solution.",
                    "label": 0
                },
                {
                    "sent": "Then the given the initial relevant score and the metrics transition matrix.",
                    "label": 0
                },
                {
                    "sent": "We can compute the final score directly here the when Lambda is equal to 1, it only consider the hidden links within U and we we called single sided regularization.",
                    "label": 0
                },
                {
                    "sent": "When Lambda is set between zero to 1.",
                    "label": 0
                },
                {
                    "sent": "Required to double side The Reg.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are azatian so in this paper we apply the Mercer to the query.",
                    "label": 0
                },
                {
                    "sent": "You are bipartite graph for credit suggestions and for some more details you may refer.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To our paper.",
                    "label": 0
                },
                {
                    "sent": "And here I came to the expense.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I meant Potter.",
                    "label": 0
                },
                {
                    "sent": "Here's a set top of our our experiment.",
                    "label": 0
                },
                {
                    "sent": "We use the LL query log and in our experiment we have about 1 million queries and 1 million URLs and there are about 5 million ages between the query and URL.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our evaluation will utilize the ODP similarity.",
                    "label": 0
                },
                {
                    "sent": "It's it is prove it is proposed by bears yards in KDD 2007 and it's a simple measure to measure the similarity between queries using the ODP categories and in this.",
                    "label": 1
                },
                {
                    "sent": "In addition, we.",
                    "label": 1
                },
                {
                    "sent": "Measure the precision at rank N. And we.",
                    "label": 0
                },
                {
                    "sent": "Randomly selector three hand requires.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Evaluation so here is a result of the iterative framework that dashed line is the baseline only based on the content information for query similarity measure and the black line is the precision at 5, the blue line is the precision at 10.",
                    "label": 0
                },
                {
                    "sent": "The first figure shows the results of the personalizer.",
                    "label": 0
                },
                {
                    "sent": "Page rank with different parameter Lambda and we can say the.",
                    "label": 0
                },
                {
                    "sent": "The performance has only a slight increase.",
                    "label": 0
                },
                {
                    "sent": "When compared to the baseline and with the increase of Lambda, the performance is getting worse and even underperform the baseline and the right point means the it's only considered graph information.",
                    "label": 0
                },
                {
                    "sent": "So the reason is there is some noisy link and its lack of constraints and the second figure is second figures.",
                    "label": 0
                },
                {
                    "sent": "Once a special case of the iterative from Rock, one step propagation and set rather set.",
                    "label": 0
                },
                {
                    "sent": "Figure is the general careers algorithm and we can see the improvement of the OSP and general cohete algorithm over the baseline are promising when compared to the personalized page rank.",
                    "label": 1
                },
                {
                    "sent": "This is because the initial relevance score from both sides provide valuable.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formations.",
                    "label": 0
                },
                {
                    "sent": "Here is the result of the regularization from work that first consider the single side regularization from work.",
                    "label": 0
                },
                {
                    "sent": "The single side, the regularization, the results direct point means the only consider the graph information in the in the regularization and the left point is the baseline based on the content information and we can see the single side.",
                    "label": 0
                },
                {
                    "sent": "Regularization can improve the performance over the baseline and then we fix the Lambda mu and set the parameter.",
                    "label": 0
                },
                {
                    "sent": "The parameter Lambda here to evaluate the double sided regularization and compare with the single sided regularization.",
                    "label": 0
                },
                {
                    "sent": "The red point is the performance of the single side, the regularization and we can see that when we incorporate and the new cost function are three the.",
                    "label": 0
                },
                {
                    "sent": "The performance can be boosted further so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some more details are shown in this.",
                    "label": 0
                },
                {
                    "sent": "This table and this figure in this.",
                    "label": 0
                },
                {
                    "sent": "In this figure, we compared 6 Mercer.",
                    "label": 0
                },
                {
                    "sent": "Including three three members of the.",
                    "label": 0
                },
                {
                    "sent": "With the.",
                    "label": 0
                },
                {
                    "sent": "Iterative from rocker and two 2 regularization framework and top one line, the green line is the result of double sided regularization and we can see it achieves the best performance.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let me conclude this talk in this in this.",
                    "label": 0
                },
                {
                    "sent": "In this paper we propose the Cohetes algorithm to incorporate the bipartite graph.",
                    "label": 1
                },
                {
                    "sent": "With the content information from both side an, we investigate this method from 2.",
                    "label": 0
                },
                {
                    "sent": "Framework of the iterative from rock and the regularization from America based on different perspectives.",
                    "label": 0
                },
                {
                    "sent": "And Cohetes algorithm is more general which including heats and personalized page rank as best case and the regularization framework is more robust with the new developed cost function which achieved the best performance.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}