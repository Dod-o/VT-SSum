{
    "id": "len7n36q3rfr5nc7dzt3sc2jlayowevy",
    "title": "Seeking Interpretable Models for High Dimensional Data",
    "info": {
        "author": [
            "Bin Yu, Department of Statistics, UC Berkeley"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss09us_yu_simhdd/",
    "segmentation": [
        [
            "So actually, I'm going to talk about joint work with many collaborators, which I will acknowledge at the end of my talk.",
            "Um?"
        ],
        [
            "So as a statistician, I think we're in a great error right now with lots of data too.",
            "Do prediction and the talk will be more moving towards interpretation through some panelization of like lesu like and the technical problem we're facing is that we have a large number of parameters and the sample size is not increasing at the same pace and would most exciting for me is that we might be able to help to do scientific discoveries in this.",
            "In the field, which in particular neuroscience."
        ],
        [
            "So here's a road map.",
            "I will start with the motivating example of.",
            "Working with a gorilla lab at Berkeley is a vision lab and they have many different projects or different areas of Physiology lab, but also they have gotten down to F MRI, so we try to relate the natural images.",
            "2 double indirect brain signals.",
            "The FMR measurements and I'll give you a history of the path of the lab has taken and then motivate outcomes razor which they really moved from.",
            "Prediction yournet SVM and tomorrow sparsity model based and of course all comes razor is nothing new.",
            "And I will review a little bit with institution, have done in the 70s model selection and the new incarnation under the L1 penalized this crash.",
            "I'll introduce some theoretical work we have done on linear models and recently on Gaussian graphical models and then return to the neuroscience problem.",
            "To tell you that how these different sparse models.",
            "Through correlation linear models down in the model and graphical model appending up on real data is very much ongoing, so it's not going to be as conclusive or some other talks, but it's just a process we've been working with calendar for the last two or three years.",
            "I'll finish with some."
        ],
        [
            "For directions.",
            "So the goal alive is leading Vision Lab and above the Pioneer Labs to use natural stimuli to stimulate the brain before in psychophysics, people have used very synthetic stimuli.",
            "Bars white.",
            "And they were one of the first to use natural images for.",
            "In the lab animals, but also are humans.",
            "So this is a.",
            "The map the brain by Davinci about 500 years ago.",
            "At the time they already figured out the eyes like a camera and things get projected back to the back of the brain.",
            "That's where the V1, the primary Contacts area.",
            "Unfortunately, the project is not showing.",
            "What I want you to look at is that the V1 area, which I'll be looking at in this area, and this projection is not telling me the colors."
        ],
        [
            "Here's a diagram of the setup, so one subject, the human subject, will be lying there and it is faulty.",
            "Very strong, pretty strong magnetic field.",
            "For collection of FMR signals under screen, this person will be looking at.",
            "Images every one second it randomly selected from a large natural image database and which is 3 seconds repeated 3 times and three seconds.",
            "Try to erase some of the memory, another image.",
            "And ephemeres if you haven't heard about it is not directly measurable.",
            "Nurse signals is so called measuring the board signals is a change of oxygen in the bloodstream, so.",
            "It's only indirect measurement and could be measuring other things rather than your."
        ],
        [
            "Signals, and so we the lab did pre processing which were not involved with and we're looking at the pre process data.",
            "So the goal, Alameda Natural News on the web last March when they use some forward model which means using features of natural images to have a regression model predictor, FMR images and use that model to do decoding, which means you look at FMR images and try to guess which image the subjects looking at.",
            "So we cheated a little bit in the sense that they did tell the decoder.",
            "Image database like 10,000 images but didn't tell you which one, so it's not completely free viewing like from your brain.",
            "You can figure out the images so it's more controlled, but still it's quite amazing.",
            "Amazing.",
            "That's why they made the national news.",
            "So those headlines about reading the brain and all that."
        ],
        [
            "So this is 1 example of natural image.",
            "They will be looking at.",
            "It's not controlled, is collected from different means, no control resolution, only showing."
        ],
        [
            "Grayscale.",
            "So the natural images are drawn from a database about 10,000 images and set up.",
            "Try to make things more ID and the person is looking at it and you have ever signals and do some preprocessing involved to remove the Harmer dynamic function.",
            "Which may be related to your heart.",
            "I know that signals we hope we remove them, but it's always a question where they should re process and you get pretty Gaussian ized responses that would we have been working with."
        ],
        [
            "Because of the subject matter, which has been pretty much 4V1.",
            "People understand that you we want works more like wavelet filter banks so.",
            "They process the images through this cable with the pyramids with different frequencies and different locations and different rotations.",
            "OK, so this is the filter bank based video by means you just multiply this functions at different locations, different frequencies and different orientations and you base expand.",
            "128 by 100."
        ],
        [
            "The 8 images into a vector of 10,921 dimensions.",
            "That's how many future banks has been applied.",
            "Because, again, subject matter, they don't think the V1 has a face information, so that's why they have the.",
            "The phase shifted to filter banks and have the norm, so you lose where things are.",
            "And that's already the preprocessing done before.",
            "We started working with him on this pro."
        ],
        [
            "Come.",
            "So mathematically, you can think of now have a P 10,921 features image.",
            "And I have about 2000 samples, 17150 samples and because of the design smaller less ID and they also have data now on free viewing movies which we haven't looked at but this particular data we're looking at.",
            "Of course the lab is interested in understanding the human visual system.",
            "And they insisted on good prediction.",
            "So the first pass they did well was working with some started like three or four years ago.",
            "They were just going for good prediction models, and the first generation is neural Nets.",
            "Of course, we know that we cannot resist.",
            "We don't have unique solution for least squares.",
            "You would need some regularization."
        ],
        [
            "So this is just a mathematical description of some notations.",
            "My ex is a big vector of dimension P. And why are the responses approved?",
            "Process FMR signals and the lab last year when we started with why happy without boosting with small steps or is very much related to the soup and insisted on setting aside 120 samples for each voxel, right?",
            "So what you have is that they have been building?",
            "Models for each voxel FMS images have many many voxels and now we try to put a different box altogether.",
            "So now I mean for each voxel they were establishing regression model.",
            "And they use correlation as prediction performance, not mean squared error because I think this is a tradition from the Physiology data.",
            "They don't think they can get the magnitude, but they want to know the spike trains up and downs so they use correlation and it's also have the absolute scale otherwise without knowing the noise level is very hard to judge if you use means."
        ],
        [
            "Error.",
            "So here's a brief history of what's happening at the lab.",
            "So for three or four years ago they were using neural Nets and then they tried SVM.",
            "And it was interesting story about boosting.",
            "I was working without talking to them at the time already.",
            "I was working on outta boosting from most radical or methodology standpoint announced at now.",
            "Maybe you guys should try out to boosting.",
            "And these are nothing will work.",
            "We tried everything but later the opposed to discover out boosting by repeating a residual.",
            "So from that now they've been really using boosting quite a lot.",
            "And they started with prediction performance and they really like boosting becausw give them much sparser model.",
            "It doesn't have to be causal relationship, at least something you can look at.",
            "And for the neural Nets, as VM is very hard to know what's going on because it's not as interpretable and I would think that this trend in the gallon lab really represent the training statistical machine learning.",
            "So we're moving from prediction to simpler and sparser models.",
            "For this lab is for prediction of our interpretation and.",
            "If you have a simple model you can do faster computation for prediction and also for data transmission you have fewer parameters transmit and I think I want to make an argument that even when you do prediction.",
            "You have a simpler model.",
            "Even your pure goal is prediction.",
            "You still want to have some understanding.",
            "Suppose there's a next person on the job in your company.",
            "You just hand over the codes for a million predictors is not going to give that person any understanding or problem.",
            "If you say that hey, this is a model with only.",
            "Maybe 20 terms and you have meanings for these terms.",
            "That's something you're passing knowledge from.",
            "The last person to the next one, even your goal, is purely prediction.",
            "I think there's a reason to think that you might want to go for simpler models without sacrificing prediction.",
            "Otherwise, just the code you don't really understand the problem."
        ],
        [
            "So they're going really following this principle.",
            "Parsimony, and this is again Internet courtesy is, I don't know whether this is really outcome, But that's what Wikipedia listed who was a flyer from 17.",
            "Century and Simply put, principle parsimony means entities must not be multiplied beyond necessity.",
            "So necessity you can say that is for prediction.",
            "You don't want to sacrifice a lot of prediction, but if you're sacrificing only a little bit, there's a qualitative.",
            "Then I will go for a model of 10 parameters over a few hundreds, 50 to 20.",
            "That's a hard call, right?",
            "But we speak difference of sparsity.",
            "I think we all agree that we want simpler ones."
        ],
        [
            "The reasons I mentioned so back to this slide.",
            "That in the 70s if you follow maximum likehood under linear regression, you end up with the largest model and people knew now and now then that if you do prediction, you end up being very bad because you're overfitting, so there's a whole field in statistics called model selection.",
            "An if you really follow.",
            "Model selection to the.",
            "No where detailed.",
            "Then you say that will have people.",
            "Amateurs.",
            "I have two to the power P possible submodels, that's huge.",
            "And.",
            "If you want to select it."
        ],
        [
            "Very hard computational problem.",
            "People say that it's a computer problem.",
            "It's impossible to carry out even when P is 100 to the power, P is something like 10 to the power of 30.",
            "You cannot possibly go through all the models.",
            "I want to make arguments saying that you actually don't want to.",
            "For my problem, only have like 2 center 2000 data points.",
            "And I don't think anybody would.",
            "Argue with me saying that for 2000 data points I can tell apart to the power 100 models.",
            "You just possibly cannot tell them apart.",
            "It's not just computation infeasible statistically.",
            "You also don't have the information to really differentiate these different models.",
            "Alot of models kind of belong to the equivalent class because the noise in the model.",
            "And so in the 70s are Kiki.",
            "Hardcover papers and similar to Mallow CP in the regression case.",
            "To say you should penalize these squares by the dimension of the model and now in modern terminology will call.",
            "You know penalized by our zero, right?",
            "But it's really dimension for the squares.",
            "Ann Schwartz has similar L zero penalized with a different.",
            "We called smoothing parameter which depends on the sample site login, but so penalized least squares with the number of parameters and this from coding theaters, minimum description lines which can unify many different approaches but not a SMB S so."
        ],
        [
            "Let's just said already that.",
            "If you follow this model selection approach, you might want to look at to the power of P. For my case it become 10 to 3000.",
            "Too expensive and overnight not necessary when you only have 2000 data points because you just don't have the precision in your data to tell them apart.",
            "So it's wasteful even you want to search over this this number member.",
            "Um models?",
            "But to put on record, that solution would never did this comment or search unless for very small model people did a lot of the stepwise selection, which is not unrelated to matching pursuit or forward selection.",
            "People did stage why forward or backward selection, but there's a difference between what we did then and now.",
            "When you do forward selection, people usually took a lot more greedy approach to feed the model.",
            "You do this crash nowadays we usually take a lot more cautious approach like.",
            "Boosting, but we shrinked step sizes so you don't do the line search to minimize their direction, you shrink back, but very much you do stage wise for large problems that ish and we're doing that already.",
            "So but without the shrinkage and the recent popular alternative is lesu which is penalized least squares and I would say it's a third generation statistics or machine learning method because it combines computational consideration with Cisco.",
            "Consideration the 1st generation I would say is where there is a separation of those using close for math to do your computation.",
            "You have closed form second generation.",
            "You do your statistical studies without worrying about computation.",
            "Worry about that later.",
            "Usually you see theorems like.",
            "Suppose the empirical optimization has a unique minimizer that I know the statistical property of that, but there's a separation and the third generation, I think the old machine learning in Beijing stuff more the third generation because you worry about computation when you design your criteria."
        ],
        [
            "Sola Sue, who doesn't know Lasu OK, so I'll do a quick one.",
            "So one penalized least squares and there are lot of precursors to the soup, and it's really if the circle or the ellipsoid means the counter of the L2.",
            "The squares function and my diamond is really the constraint because the duality, well, because you can rewrite.",
            "The L1 penalty squares as minimizing.",
            "The least squares subject to our one constraint OK, and most of the time think about throwing like that ellipsoid towards that diamond you land on the vertex, and that's where you have sparsity because one of them will be 0.",
            "But it doesn't mean that you always have that luck.",
            "Sometimes you end up on the edge.",
            "You won't have sparsity.",
            "And people love sparsity becausw its regularization.",
            "And because the edge.",
            "And vertex you have variable selection and it's convex, so you have the good computational side and you have the regularization.",
            "We know in high dimensional space."
        ],
        [
            "You have to do.",
            "And I want to say a little bit about one particular property or lesu, which has been heavily studied there.",
            "There's some reference which I won't get into, so there has been a lot of analysis on these squares, and actually there's a parallel talk now.",
            "By year wise.",
            "I think it's on compressed sensing and compressed sensing.",
            "It's really much lawsuit, but the difference that they can design the X much more becauses engineering you can choose your predictors.",
            "And for us are predictors are the natural images.",
            "You can do a little bit control by pulling from a big database, but you cannot really make them orthogonal.",
            "And to understand the issue you can do prediction error analysis.",
            "You can do parameter estimation analysis or you can do model selection consistency.",
            "So concentrate from now on the model selection consistency because in terms of interpretation I think this is the most relevant criteria.",
            "Nevertheless very idealized because in real problems I don't have the best linear model, but I think among the three.",
            "Models like consistency is the most relevant, supposing the ideal case you have a true model which is sparse.",
            "Can you get there is kind of a benchmark to understand the practically empirically successful method by itself, it doesn't mean that you have motorcycling consistency.",
            "In practice will work.",
            "However, you have successfully working method with empirical evidence.",
            "I think this is a great criterion to understand what's going on."
        ],
        [
            "So the simplest model we take is a linear model, where you assume you have P predictors only.",
            "A small number of them as number of them.",
            "I take as the first S. To be relevant or have betas now, zero right as I try to mimic the fact there's some relevant ones, some irrelevant ones.",
            "And the assumption, basically you want the design matrix to be regular and things not grown too fast.",
            "And those conditions also needed for what we call classical asymptotic statistics means that you think you assume the total number of parameters is fixed P and you letter sample size grows.",
            "That's more the classical setup and night and foods are very nice paper in annual statistics to study such estimator lusu.",
            "And we saw some tatic normality an with prefixed.",
            "OK."
        ],
        [
            "An yeah paper 9206 we decide to look at because I came from a background working MD also want to see whether Lesu people think that it's a way to do model selection.",
            "We sell, let's understand it by looking at whether has the model selection consistency.",
            "We discovered that.",
            "Is not always model selection consistent, so I'll show you why in a simple example, the proof is pretty straightforward algebraically from a KKT condition and just looking at central limit theorem and with some concentration inequality is not.",
            "But geometrically I'll just show you a simple case to get, give you some understanding why it's not always consistent so.",
            "For the inconsistency, you can go to the population version in the sense that you can assume that you noise space is zero.",
            "You still might not be consistent.",
            "OK, it's not the noise problem."
        ],
        [
            "So let me take a simple example with three predictors.",
            "And the predictors are correlated.",
            "So that the 1st and 2nd are actually independent and the 1st and the 2nd 1st and the third are correlated with the same correlation.",
            "R become.",
            "Make things easy to draw.",
            "And then you reported condition is necessary and sufficient for this case says that I need my correlation time, the sum of the signs of the true parameters.",
            "Absolute value to be less than one.",
            "OK, that just falls out from Katie.",
            "And.",
            "Here I have two examples for cases.",
            "So in the 1st.",
            "Panel.",
            "My betas are one and one, and the third 10, so you want.",
            "Now I have a like a cigar kind of, you know, allocated ball.",
            "Have on that to touch.",
            "At the pyramid constraint, be exactly at 1, one they will be consistent.",
            "Now I'm removing noise.",
            "And for my second example over there, everything being the same except that my truth will be one N -- 1.",
            "So you look at my condition, the two sides canceled out.",
            "So in this case.",
            "Doesn't matter how strong the correlation is, my necessary and sufficient condition will be satisfied.",
            "Then the two upper and lower panels with the same betas are increasing.",
            "Correlation from .4 two point 6.",
            "So my condition says that for the first 2 upper panels I will have consistency means I should be touching in the right place and for Article 2.6 for this case, because science I add up to be too, I need an artery less than .5.",
            "Point 6 is too big, I will lose it.",
            "And on the other hand when I move my true model from one 1 to 1 -- 1, suddenly I'm OK.",
            "So you can see that it matters where the cigar like counter or the ball where it is.",
            "So what's going on is that if you think about a pyramid an I have this cigar like thing.",
            "If the two correlated will bump somewhere, 'cause if it's orthogonal they will be strict.",
            "You won't have problem bumping if you bump into it, not at 00 at the first coordinate like XY plane.",
            "Now you have inconsistency because my beta three is 0.",
            "So the first 2 says that if the bumping.",
            "If your cigars not 2 two did, it just won't bump.",
            "It doesn't matter where you are.",
            "The angle won't be strong enough, but however when ours .6.",
            "In one case, it bumps into the pyramid.",
            "In the other case it doesn't.",
            "So what's going on instead?",
            "You have a pyramid.",
            "If you tilting direction is alongside the edge of the.",
            "Pyramid.",
            "You won't be bumping into the pyramid.",
            "You piece pumping in the orthogonal direction, so you OK.",
            "In one case, when you add 11, the bumping direction is directly into the pyramid and you have a problem.",
            "So that's why the location of the truth matters.",
            "You always tilt in the same way, but becausw where you are the truth.",
            "Then at one case you bump directly into the pyramid.",
            "In the other case you bumping along the side of the edge so you won't bump into it.",
            "That's why you still have consistency, so that's the second case.",
            "OK, because think about you bump this way.",
            "If you moved from the first coordinate to the second, you bumping along the side, you won't be touching.",
            "The pyramid for beta not equal to zero.",
            "OK, so completely this represented condition is a geometric condition about the relationship between how to did, how correlated.",
            "Your predictors are and, where things are the truth.",
            "OK, that's why the signs of beta 1 beta two are there."
        ],
        [
            "So mathematically, you don't know the sign.",
            "You don't know the beta, therefore you make the Maxim just put strong enough condition to make sure that.",
            "You don't bump it, doesn't matter where you are.",
            "OK, so Mathematica this is X2 is all the relevant variable into a matrix.",
            "An XY is all the relevant variables.",
            "It says that.",
            "The irrelevant ones cannot be too correlated with the relevant ones.",
            "Qualitatively, we all agree right in the worst case wise, identical.",
            "Of course you cannot tell them apart, but quantitatively, you need Katie condition to see that.",
            "Otherwise you just say well now strong.",
            "But why not cut it half?",
            "And that's because the KKT condition so interesting Lee all the designs we often do in simulations actually satisfy that you represented condition means Atlas will be consistent.",
            "You have constant correlation.",
            "You have power decay, correlation or you have abundant correlation.",
            "But it's interesting you look at the last condition which you see similar stuff in compressed sensing literature.",
            "He said your correlation if he's only upper bounded, has now.",
            "The sparsity comes in the sparser.",
            "The the X then you have bigger the upper bounds you can tolerate higher correlation, which makes sense right?",
            "If the correlation is very strong, you might still have a chance.",
            "If your model is really, really sparse, so there's a tension there."
        ],
        [
            "So at the high level, what we showed is that you need a repressive condition at the first conditions you need anyway.",
            "If you do assume the smallest coefficient to be bounded away from zero would be mixed up with the noise.",
            "You cannot ask for consistency, right?",
            "You don't know what you're asking.",
            "And in the Moonlight paper for the Goshen.",
            "Examples in the droppage on paper we did, we have a fixed design depend on the precise.",
            "Scaling between anti SNP.",
            "However, as we pointed out in the paper that this magic log PS log P scaling is very much associated with caution tail.",
            "So again, in compressed sensing or sub Gaussian tail, people might be able to control that by statistical application.",
            "Usually you cannot control the tails and you have to worry that.",
            "Not just older linear model, satisfy the decay rate of your noise.",
            "Your model will effect how well you can separate the relevant ones with relevant ones.",
            "So when you have.",
            "Heavier tails you don't have this nice log P scaling.",
            "You have algebraic scaling.",
            "So the lock key is not always there.",
            "For compressed sensing, people might say that I can choose my exit to make that whole.",
            "That's true, but instead his complications over and you don't, and we have done some recent work.",
            "Most motivated from medical imaging.",
            "And we showed there.",
            "The advantage to pursuing like model, which is very much in medical imaging, your variance and you're mean will be coupled.",
            "And in that case, your beta cannot be too big, because what happens if beta speak your noise level so high and then it got mixed up with your small betas.",
            "So the heat was like homeless capacity of the errors is also very crucial for this as log P scaling.",
            "So there's a lot of need to understand the suit under nonstandard conditions, and if you break some conditions you might not have so lucky of log P. So it seems that even you have P thousands lucky you just pretty much smaller.",
            "Sample size would be OK if you Rs is small.",
            "If it's really sparse, if it's not sparse, you see as well also hurt you if, as is the same order of an than any peoples Infinity, you might have problem.",
            "So there are many players."
        ],
        [
            "In the game.",
            "So for the.",
            "The more recent work we have done is to look at Gaussian graphical models.",
            "Gaussian graphical models has a nice representation if you do the inverse correlation, inverse covariance or precision matrix, so the edges.",
            "On the graphical presentation, if the non edge will correspond to a zero in the inverse correlation matrix.",
            "So we can now discover the conditional independence.",
            "Through estimating zeros in the inverse correlation."
        ],
        [
            "So suppose I have a vector.",
            "Mean zero and I use it for the inverse covariance matrix and you and then or the one who first proposed well one penalized minus log likelihood.",
            "And you can when you don't have a Gaussian assumption, you can look at this as L1, penalized Bregman divergent so it's still meaningful is a surrogate loss function, but won't be alike with anymore and there was a work by Banerjee Idle and later followed by Freeman for fast algorithms and what we did with Ravi Kumar will ride and Rescue D. Is to understand the sufficient condition we couldn't do necessary and sufficient for model selection, consistency for sub Gaussian and also."
        ],
        [
            "Heavy tailed distribution you basically.",
            "They are the same scaling where you have Gaussian random variable.",
            "You have the same N over log and log P scaling.",
            "So the that's a graph.",
            "Well, an overload P become big.",
            "The probability go to zero.",
            "There's a transition phase transition effect.",
            "Here if you scale with N, they don't stack up.",
            "That's assigned to say, an overlock P really works as an effective sample size.",
            "When things are nicely sub caution."
        ],
        [
            "We also tried very hard in the paper to identify relevant constants.",
            "There's also a little bit not emphasize enough in order theoretical work.",
            "People talk about MPNS, but that's not enough, right?",
            "You need your parameter to be well behaved, which is you don't know for things to work.",
            "So in this paper.",
            "We tried to flush out all the different constants.",
            "We define something called complexity.",
            "Which case, do that?",
            "I won't show you, but I'll just give you a graph to show that K involves like the smallest.",
            "Like data in the regression model so that.",
            "Unknown true coefficient will have to be large for your models.",
            "Liking work intuitive makes sense right?",
            "Because it's very close to zero.",
            "It's going to be very hard to tell apart.",
            "You cannot ask for model selection consistency, so this is a paper we try very hard to bring out all the constant to make explicit and show that whatever we define using the upper bound does show that have captured something.",
            "I'm sure not capturing everything you can see when the complexity goes up, it takes bigger and bigger.",
            "The sample size for user get consistency."
        ],
        [
            "Now I come back so to the neuroscience modeling problem.",
            "This is a reminder slide, so they were doing.",
            "L1 boosting?",
            "No, I won't go to boosting with small steps that's very closely related.",
            "Lawsuit and use cross validation to stop and use correlation which they don't touch for validation.",
            "So of course if you only have one voxel, you keep trying different methods.",
            "Eventually you overfit even you do cross validation.",
            "You proper validation set.",
            "But what we have is that we have 13113 hundred voxels in the Viva area.",
            "Most time we don't touch.",
            "We only look at a few walks or santri the methodology and just apply.",
            "So there we have some protection against over fitting.",
            "Eventually of course we do enough that won't be enough, but now I think we're feeling OK."
        ],
        [
            "So.",
            "This is a project study about year and half when we first start working on the problem, we tried linear, right?",
            "That's what they were doing and we really took advantage of the V1 understanding.",
            "By the way, this type of transform.",
            "And.",
            "What we did.",
            "Is really doing something very classical?",
            "We use correlation selection too.",
            "Do dimensionality reduction?",
            "Remember we had 10,000 features and we just did correlation selection and we end up with 500."
        ],
        [
            "Willing to Sammy supervised learning, which we didn't quite make it work for this case, so the idea was at the time seems so feasible.",
            "Was that we have only about 2000 images.",
            "We have FMR signals right.",
            "Called labeled right regression, just responses labeled and then we have an image database of 11,000 which we didn't use.",
            "So there's some population information.",
            "Can we use that to help?",
            "The prediction, so we're really going around the semi supervised learning that and then what we discovered.",
            "So the Sigma is the population covariance, which we can estimate based database and see to like Sigma Hat is just a sample covariance based on the 2000 image features.",
            "And what we discovered that the good Old Ridge works really well, which doesn't use any population covariance writes is recovering things.",
            "So basically we matched the performance of the popular or more modern measured small step outta boosting with pre selection by correlation and do Ridge regression.",
            "So remember that not always all measures won't be useful."
        ],
        [
            "And quite useful.",
            "So that's for the top ten voxels.",
            "So orderly squares because we did pre selection can do all this crash.",
            "Otherwise it's your post.",
            "It just doesn't workout too, boosting that the leading method and rich.",
            "Almost match or a tiny bit better than auto boosting with small steps.",
            "And then what we call Sammy all else inverse is using the population covariance estimated from the large database.",
            "It really got quite the same as rich.",
            "So we."
        ],
        [
            "However.",
            "If you look at remember each my features has a meaning, is a wavelet with the location she will look at all the locations selected by the pre selection.",
            "On this side we see a lot more coherence where they are on the image.",
            "We know we want is very localized.",
            "Vauxhalls it's 1000 thousands of.",
            "Sells itself is very localized, so the logs are also kept some locality and this is and that's boosting on 10,000 features is more spread out because the noise and correlation so scientifically we feel like with prior knowledge.",
            "This seems to be more reasonable, but on the other hand, for a new problem this could be true to this to do all equally well for prediction.",
            "But we kind of like this photo is coherent and all that, but it could be wrong, right?",
            "But for V1 we feel like this more likely to be the story.",
            "Then that one is all over the place.",
            "Because the preselection this is nothing to do with rich.",
            "Now we just do the correlation selection and this is auto boosting by search grading like decent kind of thing or matching pursuit with shrink."
        ],
        [
            "Step.",
            "OK, so that's why we had that and we felt I wasn't publishable, right?",
            "We got same prediction error.",
            "Good understanding, reduce coverage.",
            "But nothing to show and then Ravikumar came as a postdoc from Carnegie Mellon and came with him he.",
            "Did US spam for his thesis with John Lafferty at CMU?",
            "So what they did was they build on the additive models and make sure that some of the functions are zero by thresholding.",
            "So the editor model is like linear regression but now you don't have a better J to multiply XIJ right?",
            "That's the linear model.",
            "You have a nonlinear transform which only acting on each feature at a time.",
            "This is related work to Lee and John who are doing similar stuff, but they end up with a non convex problem and the spam.",
            "Has your comics ultimate like a framework so I wouldn't say that, so we said that maybe we should give a try because our linear approach didn't seem to improve."
        ],
        [
            "Thanks.",
            "So the pre selection became handy because spam cannot handle 10,000 features because there's a nonlinear fitting step.",
            "What they do is that they do backfeeding feed each feature at a time through something like kernel density estimator or something and just too heavy.",
            "And then if the L2 norm is not big enough, threshold that nonlinear function to be 0.",
            "That's where the sparsity comes.",
            "So the preselection became handy and we discovered that time like 100 is now much worse than 500 and in terms of computation was a lot faster.",
            "So we went for 100.",
            "And we discovered that for one single cell is understood that the orientation really matters.",
            "But because each walk so is hundreds and thousand neurons, we kind of lost their orientations like selectivity.",
            "So that's neuroscience.",
            "And then we pulled, we find important locations and then we have some pulled direction for the direction selected.",
            "We made them average to making new features.",
            "Now based on understanding and selection from nonlinear.",
            "Basic sparse models and then we feed it again with the old features selected and the new features and I'm very happy that we end up using AIC because cross validation was too heavy so we went back to very classical stuff in the 70s and use the AIC which you don't need to cross validation.",
            "One computation you have some measure of complexity of the model, we just did nonlinear.",
            "Non zero terms and we stop by AIC with minimum.",
            "That's really that really.",
            "Speed it up the calculate."
        ],
        [
            "Asian.",
            "So then we saw some improvement.",
            "So based on the nonlinear model with pooling.",
            "We improved cross board, the Vauxhall prediction performance by 12%, so the lab was quite happy because this is across the board.",
            "Something got worse but mostly got better.",
            "So we fixed the methodology and tried all 1300 voxels.",
            "Remember I'm building each voxel with one linear model now."
        ],
        [
            "John Deere model.",
            "And we look at announced linearity found nonparametrically right?",
            "This is through kernel density, kernel, smooth or not, as the kernel regression smoother and they all have this compressive effect.",
            "So you can see that mostly linear.",
            "That's where most of data is far.",
            "Linear model works so well, and then it tapers off.",
            "And this biological makes sense because there's a energy constraint, right?",
            "The signal even the stimuli gets higher and higher.",
            "A real system doesn't have the energy to go up with it, so it tapered off and we thought well.",
            "We want to compare across voxels and we thought maybe if we constrain the nonlinearity for each box.",
            "So to be the same then we have one in linear algebra, equal show and then we can compare across voxels so that."
        ],
        [
            "This motivated us to constrain the spam model to the identical spam model, so we now say all the nonlinear.",
            "Function F cannot depend on J anymore, has to be the same.",
            "So we start with spam.",
            "And then we do some basic functional normal equation to constrain that.",
            "It's all in the paper.",
            "I won't give you the details."
        ],
        [
            "To fit it.",
            "And now we bring up another four 5% improvement.",
            "First we thought were loose in term prediction because we put in the constraint.",
            "Instead, we improve the prediction.",
            "So that means that our.",
            "Non constrained nonlinear model was still overfitting.",
            "There's not that much.",
            "None of the other two just still too many uncertainties like dimensions in the model.",
            "So this is about the sparsity.",
            "Remember, before we improved 1412%."
        ],
        [
            "16%."
        ],
        [
            "OK.",
            "So this is a case that we basically after the first stage of pre selection anrich went back to old statistical methods.",
            "We basically throw some of the shell machine learning method.",
            "To the problem and we thought about it and we constrained to tailor the problem to tailor the message.",
            "The problem by constraining the nonlinearity.",
            "And I was teaching applies to this course and the final exam of my course was on this data and one student who is also hanging out with my group.",
            "I had a very nice idea."
        ],
        [
            "So he heard about the nonlinearity.",
            "I still think that if he was not hanging out with my group, he probably wouldn't think about that now, neither transform.",
            "So what he did for the final exam was that again went back.",
            "Very classical statistics.",
            "He looked at the features.",
            "And plotted the response with feature right?",
            "That was the first year PhD course, so we do a lot of scatter plots and then things are not nice in Goshen like a football, if you take an interest, it is of course you know the best way to do linear regression.",
            "You have a football data.",
            "Starting skewed so we also talk about power transform.",
            "That old statistical trick that to make things into linear.",
            "Regression when you have Gaussian data.",
            "So he did fall through transform and you suddenly have this nice football shape.",
            "So he basically was cleaning up what we did with high power machine learning techniques to discover the nonlinearity in the match."
        ],
        [
            "Nicer way.",
            "He just looked at all the feature histograms and tried the power transform.",
            "He this is very human, interactive and decided the square root and false root.",
            "For different levels of the wavelet worked pretty well, so he did two fixed transforms, remember for.",
            "First try with the high power of spam model.",
            "We allowed every feature to have any linear nonlinear transform it can have, and then we went to the other extreme, make all the transforms the same and it did get better.",
            "So one transform was better than 100 transforms.",
            "What he did is 2 transforms, cube root, false root and square root and he looked at the data and look at histograms."
        ],
        [
            "And.",
            "For some of the features depends on the level he did a square root transform.",
            "And then for the other levels of wavelets he did a false route.",
            "And compared with the sparse linear model, remember the best we could do was 16% improvement.",
            "He was about to do 21%.",
            "So this is very classical statistics right?",
            "It's power transform?",
            "We teach in the first year course.",
            "PhD is now master.",
            "I like to think that the machine learning methods are still had a role to play to point to the nonlinear transform because at the time when we couldn't beat them with linear models, we basically didn't know where to go where.",
            "We could have thought about that, just we didn't feel there's so many things you could try.",
            "And the one free kind of machine learning powerful machine did point us.",
            "Nonlinear transform is so consistently compressive, right?",
            "That was a surprise and then.",
            "But this guy really cleaned it up with very classical stuff.",
            "I'd like to think.",
            "I think he agrees that without sitting through meetings in this previous work, he wouldn't have thought about nonlinear transform.",
            "But in the end we went to the lab.",
            "That's what they like most.",
            "I like most two.",
            "This is simple cube root, source, root and square."
        ],
        [
            "But with a role to play.",
            "Soap.",
            "In terms of prediction, is simple, so the left now says we can do this fall through the square root transform.",
            "That's what they're going to do.",
            "They can take it more like a protocol now.",
            "If you look at the voxels, remember I said the box was very localized, the first Vauxhall is just different resolution for the feature features, because each wavelet has a location so clearly.",
            "All.",
            "Malta one.",
            "Cares almost like 2 areas now so strongly, but voxels two also two areas."
        ],
        [
            "And.",
            "The student who really doesn't like this.",
            "He like to have more interval model and he want to make things localized so he had tuned some criteria which you don't have to worry about to design the locality and this way you are 10 times faster.",
            "So this is where subject measure the problem or enter to speed up general purpose machine learning technique."
        ],
        [
            "And.",
            "To prove that the.",
            "This localization has a biological meaning.",
            "Is that remember now each voxel?",
            "We give it a location depends on the response.",
            "And then he went into data and look at.",
            "This location for this vocal voxel and look at if the response signal is very high now will call strong and see what's the image and there is if the response.",
            "I've had my signal is very low with the image you can see that for the weak signals just not much to see the work so is not seeing any interesting sort of.",
            "Also is not interested to fire.",
            "For this you see there are things going on for the edge stuff.",
            "Now making it like oh location important, but the center of the localization is more important than the French because we have to have area.",
            "So this is an indirect proof that the localization captures something meaningful.",
            "Because of course this is a problem.",
            "We can see things, I know what's meaningful or not, so a lot of biometric problem will be loud harder.",
            "We do have advantage that its vision and images.",
            "We all make sense of image."
        ],
        [
            "However, this localization doesn't work for all voxels.",
            "For this you cannot see anything I have read here for this vauxhalls.",
            "The global measure does worse, so that's better, so the localization, because we give each box or one location, and we know for the tool box will show you some voxels have two locations.",
            "Remember, each box was a hundreds and thousands of neuron cells, so they could be just.",
            "Chris Brown in two different locations.",
            "Single cell usually only cares about my local."
        ],
        [
            "So the question becomes.",
            "Who really drive the cells were slowly tried to answer the question of causation, but with caution, because this is also Association analysis.",
            "But we tried to build in some subject measures so that it will be more credible than just come up from a model so it could be that.",
            "You select around locations because things are correlated between different locations and the different levels of wavelets also correlated an.",
            "The different neighboring voxels may be conecting relating to the same visual effects.",
            "So can we distinguish between features?",
            "Or causation works response."
        ],
        [
            "So that's where we thought about using sparsity sparse graphical models.",
            "Remember this transform, false root and square root.",
            "Make things very fast right?",
            "Because just do the transform before have to fit.",
            "It's very labor intensive and for higher vision area that's where eventually want to do this will be can be done very fast.",
            "And if we want to build a sparse graphical model Gaussian mode, I couldn't think of anything better than this example after the transform margin of sparse and we care about causation, so that's what I suggest that we should look at sparse graphical models for these two log so you can see that they seem both care about two areas."
        ],
        [
            "And if you feed the sparse graphical models.",
            "For voxel one.",
            "I have features Box 2.",
            "I have features, so if I feed them separately features Box 1 one models are graphical models and we see both.",
            "These are the non zero coefficient in the sparse graphical models.",
            "And you look at work so too.",
            "Seem to care about both, not as strongly.",
            "But the last panel I now for two boxes together with the other features.",
            "So if it a larger sparse graphical model.",
            "And suddenly it's a nice separation of.",
            "This works all seem to only have feature relation between the voxels with area one and the other one only error two.",
            "Of course the two boxers now have a link, so they are not conditional independent.",
            "OK, so this is seemed to be captured like.",
            "I'll see.",
            "Decoupling things in a way seem to make sense."
        ],
        [
            "Don't we realize that?",
            "You know, maybe the neighboring voxels are connected and we look at the two log shows they actually next to each other.",
            "We have a way to find out where they are on the brain map.",
            "So now we build a bigger graphical model by looking at one voxel and six neighbors.",
            "And then we build individual models for each box with teachers and with the predicted values as the new feature to a."
        ],
        [
            "To put them together.",
            "OK, so that's the neighbor model.",
            "So for each voxel you have six neighbors for each of the neighbor you build a model with the features and you have a predicted value with a linear combination of other features, and I use the six predicted values with the original features and build another model.",
            "Again, sparse graphical model for the middle lock.",
            "So and that's the.",
            "As a result, so we basically do as well as original global model.",
            "Remember, local model doesn't work as well, but what's interesting is that this is where things don't work well.",
            "The most improvement comes from low signal areas.",
            "So this is a constraint we we really think it's the right one because it really can lift.",
            "Improve the signal for the low signal noise regime.",
            "So now we have a model much simpler than the global model, but do as well if not better than the global model."
        ],
        [
            "So that's where we are and.",
            "For the first part of the talk, we basically talked this radical results and if you want to go home, I want you to remember the geometric picture about what's going on with the Sue, the pyramid constraint an the cigar type stuff, and the correlation.",
            "And if you hear about the log N log P as kind of the right way to calculate complexity for the search, you should wear.",
            "Be aware of that, that's to do with caution.",
            "Subtitles, shotguns and tails and.",
            "I think it's important to understand that all these properties rely on values of the model, which you don't know, not just MPNS."
        ],
        [
            "And for the neuroscience problem, we through high power machine learning, we discover some nonlinear compressive effect.",
            "But really, the going back to the classical statistic group like false root and square root really makes things a lot cleaner.",
            "Maybe I can touch this algorithmically Eda EDA institution means explore data analysis and for huge amounts of data is very hard to plot and see things so.",
            "In statistical.",
            "Eda, like Bill Cleveland, has been writing about EDA should really include also a model fitting part.",
            "I think they spend model or the constraints by more than really serve as modeling techniques control on large dimension data and point out things later.",
            "You can look at more detail so this interaction between visualization and model fitting.",
            "And of course we rely a lot on the problem with vision.",
            "We can look at it and talk."
        ],
        [
            "Into the collaborators.",
            "So where we are now, we really need we moving towards decoding.",
            "So one thing is remember the nature paper they were doing nonlinear models and indirect validation of nonlinear model local nonlocal is to do decoding.",
            "Whether we can do better decoding.",
            "That's indirect proof that this is needed direction and scientifically.",
            "The most challenging is higher vision Area V4, where attention and memory of.",
            "Play a role and whether we can make a go at the lab right now is in the middle doing some very beginning work on that, and possibly the nonlinear presentation can be useful.",
            "This nonlinear model can be useful.",
            "Image presentation thank."
        ],
        [
            "And the most important slide.",
            "So I work with many people.",
            "The first 2 lines on the theoretical work and the next line are the people I work with for the neuroscience problem.",
            "This call and let people who have been working with thank you.",
            "Time for one of the quick questions.",
            "Chris Chris.",
            "Sorry about that.",
            "Represent about.",
            "Early data.",
            "Directions.",
            "Well, 460 sign if your access are still holds.",
            "Yeah, there's there's you can generalize too.",
            "Yeah, it's perfect design as long as that holds, I think it will all go.",
            "It's not distributional.",
            "And if you do a random example setting for finale is OK too.",
            "But however, if you do classification problems there, there is a basic you have similar responsible condition, but even for dental condition will be on the hashing out the loss function.",
            "So so if you not have L2 loss, you have like logistic regression loss.",
            "You have a similar representable condition.",
            "We're finishing a paper on that that you basically have similar step but X prime X will be replaced by the Haitian of the loss function.",
            "Yeah, so it's really quadratic.",
            "You do a quadratic approximation and you backed away the squares and then things goes.",
            "Thanks.",
            "My questions.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So actually, I'm going to talk about joint work with many collaborators, which I will acknowledge at the end of my talk.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as a statistician, I think we're in a great error right now with lots of data too.",
                    "label": 0
                },
                {
                    "sent": "Do prediction and the talk will be more moving towards interpretation through some panelization of like lesu like and the technical problem we're facing is that we have a large number of parameters and the sample size is not increasing at the same pace and would most exciting for me is that we might be able to help to do scientific discoveries in this.",
                    "label": 0
                },
                {
                    "sent": "In the field, which in particular neuroscience.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a road map.",
                    "label": 0
                },
                {
                    "sent": "I will start with the motivating example of.",
                    "label": 0
                },
                {
                    "sent": "Working with a gorilla lab at Berkeley is a vision lab and they have many different projects or different areas of Physiology lab, but also they have gotten down to F MRI, so we try to relate the natural images.",
                    "label": 0
                },
                {
                    "sent": "2 double indirect brain signals.",
                    "label": 0
                },
                {
                    "sent": "The FMR measurements and I'll give you a history of the path of the lab has taken and then motivate outcomes razor which they really moved from.",
                    "label": 0
                },
                {
                    "sent": "Prediction yournet SVM and tomorrow sparsity model based and of course all comes razor is nothing new.",
                    "label": 0
                },
                {
                    "sent": "And I will review a little bit with institution, have done in the 70s model selection and the new incarnation under the L1 penalized this crash.",
                    "label": 0
                },
                {
                    "sent": "I'll introduce some theoretical work we have done on linear models and recently on Gaussian graphical models and then return to the neuroscience problem.",
                    "label": 1
                },
                {
                    "sent": "To tell you that how these different sparse models.",
                    "label": 0
                },
                {
                    "sent": "Through correlation linear models down in the model and graphical model appending up on real data is very much ongoing, so it's not going to be as conclusive or some other talks, but it's just a process we've been working with calendar for the last two or three years.",
                    "label": 0
                },
                {
                    "sent": "I'll finish with some.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For directions.",
                    "label": 0
                },
                {
                    "sent": "So the goal alive is leading Vision Lab and above the Pioneer Labs to use natural stimuli to stimulate the brain before in psychophysics, people have used very synthetic stimuli.",
                    "label": 1
                },
                {
                    "sent": "Bars white.",
                    "label": 0
                },
                {
                    "sent": "And they were one of the first to use natural images for.",
                    "label": 0
                },
                {
                    "sent": "In the lab animals, but also are humans.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "The map the brain by Davinci about 500 years ago.",
                    "label": 0
                },
                {
                    "sent": "At the time they already figured out the eyes like a camera and things get projected back to the back of the brain.",
                    "label": 0
                },
                {
                    "sent": "That's where the V1, the primary Contacts area.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, the project is not showing.",
                    "label": 0
                },
                {
                    "sent": "What I want you to look at is that the V1 area, which I'll be looking at in this area, and this projection is not telling me the colors.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's a diagram of the setup, so one subject, the human subject, will be lying there and it is faulty.",
                    "label": 0
                },
                {
                    "sent": "Very strong, pretty strong magnetic field.",
                    "label": 0
                },
                {
                    "sent": "For collection of FMR signals under screen, this person will be looking at.",
                    "label": 0
                },
                {
                    "sent": "Images every one second it randomly selected from a large natural image database and which is 3 seconds repeated 3 times and three seconds.",
                    "label": 0
                },
                {
                    "sent": "Try to erase some of the memory, another image.",
                    "label": 0
                },
                {
                    "sent": "And ephemeres if you haven't heard about it is not directly measurable.",
                    "label": 0
                },
                {
                    "sent": "Nurse signals is so called measuring the board signals is a change of oxygen in the bloodstream, so.",
                    "label": 0
                },
                {
                    "sent": "It's only indirect measurement and could be measuring other things rather than your.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Signals, and so we the lab did pre processing which were not involved with and we're looking at the pre process data.",
                    "label": 0
                },
                {
                    "sent": "So the goal, Alameda Natural News on the web last March when they use some forward model which means using features of natural images to have a regression model predictor, FMR images and use that model to do decoding, which means you look at FMR images and try to guess which image the subjects looking at.",
                    "label": 0
                },
                {
                    "sent": "So we cheated a little bit in the sense that they did tell the decoder.",
                    "label": 0
                },
                {
                    "sent": "Image database like 10,000 images but didn't tell you which one, so it's not completely free viewing like from your brain.",
                    "label": 0
                },
                {
                    "sent": "You can figure out the images so it's more controlled, but still it's quite amazing.",
                    "label": 0
                },
                {
                    "sent": "Amazing.",
                    "label": 0
                },
                {
                    "sent": "That's why they made the national news.",
                    "label": 0
                },
                {
                    "sent": "So those headlines about reading the brain and all that.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is 1 example of natural image.",
                    "label": 1
                },
                {
                    "sent": "They will be looking at.",
                    "label": 0
                },
                {
                    "sent": "It's not controlled, is collected from different means, no control resolution, only showing.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Grayscale.",
                    "label": 0
                },
                {
                    "sent": "So the natural images are drawn from a database about 10,000 images and set up.",
                    "label": 1
                },
                {
                    "sent": "Try to make things more ID and the person is looking at it and you have ever signals and do some preprocessing involved to remove the Harmer dynamic function.",
                    "label": 0
                },
                {
                    "sent": "Which may be related to your heart.",
                    "label": 0
                },
                {
                    "sent": "I know that signals we hope we remove them, but it's always a question where they should re process and you get pretty Gaussian ized responses that would we have been working with.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because of the subject matter, which has been pretty much 4V1.",
                    "label": 0
                },
                {
                    "sent": "People understand that you we want works more like wavelet filter banks so.",
                    "label": 0
                },
                {
                    "sent": "They process the images through this cable with the pyramids with different frequencies and different locations and different rotations.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the filter bank based video by means you just multiply this functions at different locations, different frequencies and different orientations and you base expand.",
                    "label": 0
                },
                {
                    "sent": "128 by 100.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The 8 images into a vector of 10,921 dimensions.",
                    "label": 0
                },
                {
                    "sent": "That's how many future banks has been applied.",
                    "label": 0
                },
                {
                    "sent": "Because, again, subject matter, they don't think the V1 has a face information, so that's why they have the.",
                    "label": 0
                },
                {
                    "sent": "The phase shifted to filter banks and have the norm, so you lose where things are.",
                    "label": 0
                },
                {
                    "sent": "And that's already the preprocessing done before.",
                    "label": 0
                },
                {
                    "sent": "We started working with him on this pro.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Come.",
                    "label": 0
                },
                {
                    "sent": "So mathematically, you can think of now have a P 10,921 features image.",
                    "label": 0
                },
                {
                    "sent": "And I have about 2000 samples, 17150 samples and because of the design smaller less ID and they also have data now on free viewing movies which we haven't looked at but this particular data we're looking at.",
                    "label": 0
                },
                {
                    "sent": "Of course the lab is interested in understanding the human visual system.",
                    "label": 1
                },
                {
                    "sent": "And they insisted on good prediction.",
                    "label": 0
                },
                {
                    "sent": "So the first pass they did well was working with some started like three or four years ago.",
                    "label": 0
                },
                {
                    "sent": "They were just going for good prediction models, and the first generation is neural Nets.",
                    "label": 0
                },
                {
                    "sent": "Of course, we know that we cannot resist.",
                    "label": 0
                },
                {
                    "sent": "We don't have unique solution for least squares.",
                    "label": 0
                },
                {
                    "sent": "You would need some regularization.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is just a mathematical description of some notations.",
                    "label": 0
                },
                {
                    "sent": "My ex is a big vector of dimension P. And why are the responses approved?",
                    "label": 0
                },
                {
                    "sent": "Process FMR signals and the lab last year when we started with why happy without boosting with small steps or is very much related to the soup and insisted on setting aside 120 samples for each voxel, right?",
                    "label": 0
                },
                {
                    "sent": "So what you have is that they have been building?",
                    "label": 0
                },
                {
                    "sent": "Models for each voxel FMS images have many many voxels and now we try to put a different box altogether.",
                    "label": 0
                },
                {
                    "sent": "So now I mean for each voxel they were establishing regression model.",
                    "label": 1
                },
                {
                    "sent": "And they use correlation as prediction performance, not mean squared error because I think this is a tradition from the Physiology data.",
                    "label": 0
                },
                {
                    "sent": "They don't think they can get the magnitude, but they want to know the spike trains up and downs so they use correlation and it's also have the absolute scale otherwise without knowing the noise level is very hard to judge if you use means.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Error.",
                    "label": 0
                },
                {
                    "sent": "So here's a brief history of what's happening at the lab.",
                    "label": 0
                },
                {
                    "sent": "So for three or four years ago they were using neural Nets and then they tried SVM.",
                    "label": 0
                },
                {
                    "sent": "And it was interesting story about boosting.",
                    "label": 0
                },
                {
                    "sent": "I was working without talking to them at the time already.",
                    "label": 0
                },
                {
                    "sent": "I was working on outta boosting from most radical or methodology standpoint announced at now.",
                    "label": 0
                },
                {
                    "sent": "Maybe you guys should try out to boosting.",
                    "label": 0
                },
                {
                    "sent": "And these are nothing will work.",
                    "label": 0
                },
                {
                    "sent": "We tried everything but later the opposed to discover out boosting by repeating a residual.",
                    "label": 0
                },
                {
                    "sent": "So from that now they've been really using boosting quite a lot.",
                    "label": 0
                },
                {
                    "sent": "And they started with prediction performance and they really like boosting becausw give them much sparser model.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have to be causal relationship, at least something you can look at.",
                    "label": 0
                },
                {
                    "sent": "And for the neural Nets, as VM is very hard to know what's going on because it's not as interpretable and I would think that this trend in the gallon lab really represent the training statistical machine learning.",
                    "label": 1
                },
                {
                    "sent": "So we're moving from prediction to simpler and sparser models.",
                    "label": 1
                },
                {
                    "sent": "For this lab is for prediction of our interpretation and.",
                    "label": 0
                },
                {
                    "sent": "If you have a simple model you can do faster computation for prediction and also for data transmission you have fewer parameters transmit and I think I want to make an argument that even when you do prediction.",
                    "label": 0
                },
                {
                    "sent": "You have a simpler model.",
                    "label": 0
                },
                {
                    "sent": "Even your pure goal is prediction.",
                    "label": 0
                },
                {
                    "sent": "You still want to have some understanding.",
                    "label": 0
                },
                {
                    "sent": "Suppose there's a next person on the job in your company.",
                    "label": 0
                },
                {
                    "sent": "You just hand over the codes for a million predictors is not going to give that person any understanding or problem.",
                    "label": 0
                },
                {
                    "sent": "If you say that hey, this is a model with only.",
                    "label": 0
                },
                {
                    "sent": "Maybe 20 terms and you have meanings for these terms.",
                    "label": 0
                },
                {
                    "sent": "That's something you're passing knowledge from.",
                    "label": 0
                },
                {
                    "sent": "The last person to the next one, even your goal, is purely prediction.",
                    "label": 0
                },
                {
                    "sent": "I think there's a reason to think that you might want to go for simpler models without sacrificing prediction.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, just the code you don't really understand the problem.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So they're going really following this principle.",
                    "label": 0
                },
                {
                    "sent": "Parsimony, and this is again Internet courtesy is, I don't know whether this is really outcome, But that's what Wikipedia listed who was a flyer from 17.",
                    "label": 0
                },
                {
                    "sent": "Century and Simply put, principle parsimony means entities must not be multiplied beyond necessity.",
                    "label": 1
                },
                {
                    "sent": "So necessity you can say that is for prediction.",
                    "label": 0
                },
                {
                    "sent": "You don't want to sacrifice a lot of prediction, but if you're sacrificing only a little bit, there's a qualitative.",
                    "label": 0
                },
                {
                    "sent": "Then I will go for a model of 10 parameters over a few hundreds, 50 to 20.",
                    "label": 0
                },
                {
                    "sent": "That's a hard call, right?",
                    "label": 0
                },
                {
                    "sent": "But we speak difference of sparsity.",
                    "label": 0
                },
                {
                    "sent": "I think we all agree that we want simpler ones.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The reasons I mentioned so back to this slide.",
                    "label": 0
                },
                {
                    "sent": "That in the 70s if you follow maximum likehood under linear regression, you end up with the largest model and people knew now and now then that if you do prediction, you end up being very bad because you're overfitting, so there's a whole field in statistics called model selection.",
                    "label": 1
                },
                {
                    "sent": "An if you really follow.",
                    "label": 0
                },
                {
                    "sent": "Model selection to the.",
                    "label": 0
                },
                {
                    "sent": "No where detailed.",
                    "label": 0
                },
                {
                    "sent": "Then you say that will have people.",
                    "label": 0
                },
                {
                    "sent": "Amateurs.",
                    "label": 0
                },
                {
                    "sent": "I have two to the power P possible submodels, that's huge.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If you want to select it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very hard computational problem.",
                    "label": 0
                },
                {
                    "sent": "People say that it's a computer problem.",
                    "label": 0
                },
                {
                    "sent": "It's impossible to carry out even when P is 100 to the power, P is something like 10 to the power of 30.",
                    "label": 0
                },
                {
                    "sent": "You cannot possibly go through all the models.",
                    "label": 0
                },
                {
                    "sent": "I want to make arguments saying that you actually don't want to.",
                    "label": 0
                },
                {
                    "sent": "For my problem, only have like 2 center 2000 data points.",
                    "label": 0
                },
                {
                    "sent": "And I don't think anybody would.",
                    "label": 0
                },
                {
                    "sent": "Argue with me saying that for 2000 data points I can tell apart to the power 100 models.",
                    "label": 0
                },
                {
                    "sent": "You just possibly cannot tell them apart.",
                    "label": 0
                },
                {
                    "sent": "It's not just computation infeasible statistically.",
                    "label": 0
                },
                {
                    "sent": "You also don't have the information to really differentiate these different models.",
                    "label": 0
                },
                {
                    "sent": "Alot of models kind of belong to the equivalent class because the noise in the model.",
                    "label": 0
                },
                {
                    "sent": "And so in the 70s are Kiki.",
                    "label": 0
                },
                {
                    "sent": "Hardcover papers and similar to Mallow CP in the regression case.",
                    "label": 0
                },
                {
                    "sent": "To say you should penalize these squares by the dimension of the model and now in modern terminology will call.",
                    "label": 0
                },
                {
                    "sent": "You know penalized by our zero, right?",
                    "label": 0
                },
                {
                    "sent": "But it's really dimension for the squares.",
                    "label": 0
                },
                {
                    "sent": "Ann Schwartz has similar L zero penalized with a different.",
                    "label": 0
                },
                {
                    "sent": "We called smoothing parameter which depends on the sample site login, but so penalized least squares with the number of parameters and this from coding theaters, minimum description lines which can unify many different approaches but not a SMB S so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's just said already that.",
                    "label": 0
                },
                {
                    "sent": "If you follow this model selection approach, you might want to look at to the power of P. For my case it become 10 to 3000.",
                    "label": 0
                },
                {
                    "sent": "Too expensive and overnight not necessary when you only have 2000 data points because you just don't have the precision in your data to tell them apart.",
                    "label": 1
                },
                {
                    "sent": "So it's wasteful even you want to search over this this number member.",
                    "label": 0
                },
                {
                    "sent": "Um models?",
                    "label": 0
                },
                {
                    "sent": "But to put on record, that solution would never did this comment or search unless for very small model people did a lot of the stepwise selection, which is not unrelated to matching pursuit or forward selection.",
                    "label": 0
                },
                {
                    "sent": "People did stage why forward or backward selection, but there's a difference between what we did then and now.",
                    "label": 0
                },
                {
                    "sent": "When you do forward selection, people usually took a lot more greedy approach to feed the model.",
                    "label": 0
                },
                {
                    "sent": "You do this crash nowadays we usually take a lot more cautious approach like.",
                    "label": 0
                },
                {
                    "sent": "Boosting, but we shrinked step sizes so you don't do the line search to minimize their direction, you shrink back, but very much you do stage wise for large problems that ish and we're doing that already.",
                    "label": 0
                },
                {
                    "sent": "So but without the shrinkage and the recent popular alternative is lesu which is penalized least squares and I would say it's a third generation statistics or machine learning method because it combines computational consideration with Cisco.",
                    "label": 1
                },
                {
                    "sent": "Consideration the 1st generation I would say is where there is a separation of those using close for math to do your computation.",
                    "label": 0
                },
                {
                    "sent": "You have closed form second generation.",
                    "label": 0
                },
                {
                    "sent": "You do your statistical studies without worrying about computation.",
                    "label": 0
                },
                {
                    "sent": "Worry about that later.",
                    "label": 0
                },
                {
                    "sent": "Usually you see theorems like.",
                    "label": 0
                },
                {
                    "sent": "Suppose the empirical optimization has a unique minimizer that I know the statistical property of that, but there's a separation and the third generation, I think the old machine learning in Beijing stuff more the third generation because you worry about computation when you design your criteria.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sola Sue, who doesn't know Lasu OK, so I'll do a quick one.",
                    "label": 0
                },
                {
                    "sent": "So one penalized least squares and there are lot of precursors to the soup, and it's really if the circle or the ellipsoid means the counter of the L2.",
                    "label": 0
                },
                {
                    "sent": "The squares function and my diamond is really the constraint because the duality, well, because you can rewrite.",
                    "label": 0
                },
                {
                    "sent": "The L1 penalty squares as minimizing.",
                    "label": 1
                },
                {
                    "sent": "The least squares subject to our one constraint OK, and most of the time think about throwing like that ellipsoid towards that diamond you land on the vertex, and that's where you have sparsity because one of them will be 0.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't mean that you always have that luck.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you end up on the edge.",
                    "label": 0
                },
                {
                    "sent": "You won't have sparsity.",
                    "label": 0
                },
                {
                    "sent": "And people love sparsity becausw its regularization.",
                    "label": 0
                },
                {
                    "sent": "And because the edge.",
                    "label": 0
                },
                {
                    "sent": "And vertex you have variable selection and it's convex, so you have the good computational side and you have the regularization.",
                    "label": 0
                },
                {
                    "sent": "We know in high dimensional space.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You have to do.",
                    "label": 0
                },
                {
                    "sent": "And I want to say a little bit about one particular property or lesu, which has been heavily studied there.",
                    "label": 0
                },
                {
                    "sent": "There's some reference which I won't get into, so there has been a lot of analysis on these squares, and actually there's a parallel talk now.",
                    "label": 0
                },
                {
                    "sent": "By year wise.",
                    "label": 0
                },
                {
                    "sent": "I think it's on compressed sensing and compressed sensing.",
                    "label": 0
                },
                {
                    "sent": "It's really much lawsuit, but the difference that they can design the X much more becauses engineering you can choose your predictors.",
                    "label": 0
                },
                {
                    "sent": "And for us are predictors are the natural images.",
                    "label": 0
                },
                {
                    "sent": "You can do a little bit control by pulling from a big database, but you cannot really make them orthogonal.",
                    "label": 0
                },
                {
                    "sent": "And to understand the issue you can do prediction error analysis.",
                    "label": 1
                },
                {
                    "sent": "You can do parameter estimation analysis or you can do model selection consistency.",
                    "label": 0
                },
                {
                    "sent": "So concentrate from now on the model selection consistency because in terms of interpretation I think this is the most relevant criteria.",
                    "label": 1
                },
                {
                    "sent": "Nevertheless very idealized because in real problems I don't have the best linear model, but I think among the three.",
                    "label": 0
                },
                {
                    "sent": "Models like consistency is the most relevant, supposing the ideal case you have a true model which is sparse.",
                    "label": 0
                },
                {
                    "sent": "Can you get there is kind of a benchmark to understand the practically empirically successful method by itself, it doesn't mean that you have motorcycling consistency.",
                    "label": 0
                },
                {
                    "sent": "In practice will work.",
                    "label": 0
                },
                {
                    "sent": "However, you have successfully working method with empirical evidence.",
                    "label": 0
                },
                {
                    "sent": "I think this is a great criterion to understand what's going on.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the simplest model we take is a linear model, where you assume you have P predictors only.",
                    "label": 0
                },
                {
                    "sent": "A small number of them as number of them.",
                    "label": 0
                },
                {
                    "sent": "I take as the first S. To be relevant or have betas now, zero right as I try to mimic the fact there's some relevant ones, some irrelevant ones.",
                    "label": 0
                },
                {
                    "sent": "And the assumption, basically you want the design matrix to be regular and things not grown too fast.",
                    "label": 0
                },
                {
                    "sent": "And those conditions also needed for what we call classical asymptotic statistics means that you think you assume the total number of parameters is fixed P and you letter sample size grows.",
                    "label": 0
                },
                {
                    "sent": "That's more the classical setup and night and foods are very nice paper in annual statistics to study such estimator lusu.",
                    "label": 0
                },
                {
                    "sent": "And we saw some tatic normality an with prefixed.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An yeah paper 9206 we decide to look at because I came from a background working MD also want to see whether Lesu people think that it's a way to do model selection.",
                    "label": 0
                },
                {
                    "sent": "We sell, let's understand it by looking at whether has the model selection consistency.",
                    "label": 1
                },
                {
                    "sent": "We discovered that.",
                    "label": 0
                },
                {
                    "sent": "Is not always model selection consistent, so I'll show you why in a simple example, the proof is pretty straightforward algebraically from a KKT condition and just looking at central limit theorem and with some concentration inequality is not.",
                    "label": 0
                },
                {
                    "sent": "But geometrically I'll just show you a simple case to get, give you some understanding why it's not always consistent so.",
                    "label": 1
                },
                {
                    "sent": "For the inconsistency, you can go to the population version in the sense that you can assume that you noise space is zero.",
                    "label": 0
                },
                {
                    "sent": "You still might not be consistent.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not the noise problem.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me take a simple example with three predictors.",
                    "label": 0
                },
                {
                    "sent": "And the predictors are correlated.",
                    "label": 0
                },
                {
                    "sent": "So that the 1st and 2nd are actually independent and the 1st and the 2nd 1st and the third are correlated with the same correlation.",
                    "label": 0
                },
                {
                    "sent": "R become.",
                    "label": 0
                },
                {
                    "sent": "Make things easy to draw.",
                    "label": 0
                },
                {
                    "sent": "And then you reported condition is necessary and sufficient for this case says that I need my correlation time, the sum of the signs of the true parameters.",
                    "label": 0
                },
                {
                    "sent": "Absolute value to be less than one.",
                    "label": 0
                },
                {
                    "sent": "OK, that just falls out from Katie.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Here I have two examples for cases.",
                    "label": 0
                },
                {
                    "sent": "So in the 1st.",
                    "label": 0
                },
                {
                    "sent": "Panel.",
                    "label": 0
                },
                {
                    "sent": "My betas are one and one, and the third 10, so you want.",
                    "label": 0
                },
                {
                    "sent": "Now I have a like a cigar kind of, you know, allocated ball.",
                    "label": 0
                },
                {
                    "sent": "Have on that to touch.",
                    "label": 0
                },
                {
                    "sent": "At the pyramid constraint, be exactly at 1, one they will be consistent.",
                    "label": 0
                },
                {
                    "sent": "Now I'm removing noise.",
                    "label": 0
                },
                {
                    "sent": "And for my second example over there, everything being the same except that my truth will be one N -- 1.",
                    "label": 0
                },
                {
                    "sent": "So you look at my condition, the two sides canceled out.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "Doesn't matter how strong the correlation is, my necessary and sufficient condition will be satisfied.",
                    "label": 0
                },
                {
                    "sent": "Then the two upper and lower panels with the same betas are increasing.",
                    "label": 0
                },
                {
                    "sent": "Correlation from .4 two point 6.",
                    "label": 0
                },
                {
                    "sent": "So my condition says that for the first 2 upper panels I will have consistency means I should be touching in the right place and for Article 2.6 for this case, because science I add up to be too, I need an artery less than .5.",
                    "label": 0
                },
                {
                    "sent": "Point 6 is too big, I will lose it.",
                    "label": 0
                },
                {
                    "sent": "And on the other hand when I move my true model from one 1 to 1 -- 1, suddenly I'm OK.",
                    "label": 0
                },
                {
                    "sent": "So you can see that it matters where the cigar like counter or the ball where it is.",
                    "label": 0
                },
                {
                    "sent": "So what's going on is that if you think about a pyramid an I have this cigar like thing.",
                    "label": 0
                },
                {
                    "sent": "If the two correlated will bump somewhere, 'cause if it's orthogonal they will be strict.",
                    "label": 0
                },
                {
                    "sent": "You won't have problem bumping if you bump into it, not at 00 at the first coordinate like XY plane.",
                    "label": 0
                },
                {
                    "sent": "Now you have inconsistency because my beta three is 0.",
                    "label": 0
                },
                {
                    "sent": "So the first 2 says that if the bumping.",
                    "label": 0
                },
                {
                    "sent": "If your cigars not 2 two did, it just won't bump.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter where you are.",
                    "label": 0
                },
                {
                    "sent": "The angle won't be strong enough, but however when ours .6.",
                    "label": 0
                },
                {
                    "sent": "In one case, it bumps into the pyramid.",
                    "label": 0
                },
                {
                    "sent": "In the other case it doesn't.",
                    "label": 0
                },
                {
                    "sent": "So what's going on instead?",
                    "label": 0
                },
                {
                    "sent": "You have a pyramid.",
                    "label": 0
                },
                {
                    "sent": "If you tilting direction is alongside the edge of the.",
                    "label": 0
                },
                {
                    "sent": "Pyramid.",
                    "label": 0
                },
                {
                    "sent": "You won't be bumping into the pyramid.",
                    "label": 0
                },
                {
                    "sent": "You piece pumping in the orthogonal direction, so you OK.",
                    "label": 0
                },
                {
                    "sent": "In one case, when you add 11, the bumping direction is directly into the pyramid and you have a problem.",
                    "label": 0
                },
                {
                    "sent": "So that's why the location of the truth matters.",
                    "label": 0
                },
                {
                    "sent": "You always tilt in the same way, but becausw where you are the truth.",
                    "label": 0
                },
                {
                    "sent": "Then at one case you bump directly into the pyramid.",
                    "label": 0
                },
                {
                    "sent": "In the other case you bumping along the side of the edge so you won't bump into it.",
                    "label": 0
                },
                {
                    "sent": "That's why you still have consistency, so that's the second case.",
                    "label": 0
                },
                {
                    "sent": "OK, because think about you bump this way.",
                    "label": 0
                },
                {
                    "sent": "If you moved from the first coordinate to the second, you bumping along the side, you won't be touching.",
                    "label": 0
                },
                {
                    "sent": "The pyramid for beta not equal to zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so completely this represented condition is a geometric condition about the relationship between how to did, how correlated.",
                    "label": 0
                },
                {
                    "sent": "Your predictors are and, where things are the truth.",
                    "label": 0
                },
                {
                    "sent": "OK, that's why the signs of beta 1 beta two are there.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So mathematically, you don't know the sign.",
                    "label": 0
                },
                {
                    "sent": "You don't know the beta, therefore you make the Maxim just put strong enough condition to make sure that.",
                    "label": 0
                },
                {
                    "sent": "You don't bump it, doesn't matter where you are.",
                    "label": 0
                },
                {
                    "sent": "OK, so Mathematica this is X2 is all the relevant variable into a matrix.",
                    "label": 0
                },
                {
                    "sent": "An XY is all the relevant variables.",
                    "label": 0
                },
                {
                    "sent": "It says that.",
                    "label": 0
                },
                {
                    "sent": "The irrelevant ones cannot be too correlated with the relevant ones.",
                    "label": 1
                },
                {
                    "sent": "Qualitatively, we all agree right in the worst case wise, identical.",
                    "label": 0
                },
                {
                    "sent": "Of course you cannot tell them apart, but quantitatively, you need Katie condition to see that.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you just say well now strong.",
                    "label": 0
                },
                {
                    "sent": "But why not cut it half?",
                    "label": 0
                },
                {
                    "sent": "And that's because the KKT condition so interesting Lee all the designs we often do in simulations actually satisfy that you represented condition means Atlas will be consistent.",
                    "label": 0
                },
                {
                    "sent": "You have constant correlation.",
                    "label": 0
                },
                {
                    "sent": "You have power decay, correlation or you have abundant correlation.",
                    "label": 1
                },
                {
                    "sent": "But it's interesting you look at the last condition which you see similar stuff in compressed sensing literature.",
                    "label": 0
                },
                {
                    "sent": "He said your correlation if he's only upper bounded, has now.",
                    "label": 0
                },
                {
                    "sent": "The sparsity comes in the sparser.",
                    "label": 0
                },
                {
                    "sent": "The the X then you have bigger the upper bounds you can tolerate higher correlation, which makes sense right?",
                    "label": 0
                },
                {
                    "sent": "If the correlation is very strong, you might still have a chance.",
                    "label": 0
                },
                {
                    "sent": "If your model is really, really sparse, so there's a tension there.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So at the high level, what we showed is that you need a repressive condition at the first conditions you need anyway.",
                    "label": 0
                },
                {
                    "sent": "If you do assume the smallest coefficient to be bounded away from zero would be mixed up with the noise.",
                    "label": 1
                },
                {
                    "sent": "You cannot ask for consistency, right?",
                    "label": 0
                },
                {
                    "sent": "You don't know what you're asking.",
                    "label": 0
                },
                {
                    "sent": "And in the Moonlight paper for the Goshen.",
                    "label": 0
                },
                {
                    "sent": "Examples in the droppage on paper we did, we have a fixed design depend on the precise.",
                    "label": 0
                },
                {
                    "sent": "Scaling between anti SNP.",
                    "label": 0
                },
                {
                    "sent": "However, as we pointed out in the paper that this magic log PS log P scaling is very much associated with caution tail.",
                    "label": 0
                },
                {
                    "sent": "So again, in compressed sensing or sub Gaussian tail, people might be able to control that by statistical application.",
                    "label": 0
                },
                {
                    "sent": "Usually you cannot control the tails and you have to worry that.",
                    "label": 0
                },
                {
                    "sent": "Not just older linear model, satisfy the decay rate of your noise.",
                    "label": 0
                },
                {
                    "sent": "Your model will effect how well you can separate the relevant ones with relevant ones.",
                    "label": 0
                },
                {
                    "sent": "So when you have.",
                    "label": 0
                },
                {
                    "sent": "Heavier tails you don't have this nice log P scaling.",
                    "label": 0
                },
                {
                    "sent": "You have algebraic scaling.",
                    "label": 0
                },
                {
                    "sent": "So the lock key is not always there.",
                    "label": 0
                },
                {
                    "sent": "For compressed sensing, people might say that I can choose my exit to make that whole.",
                    "label": 0
                },
                {
                    "sent": "That's true, but instead his complications over and you don't, and we have done some recent work.",
                    "label": 0
                },
                {
                    "sent": "Most motivated from medical imaging.",
                    "label": 0
                },
                {
                    "sent": "And we showed there.",
                    "label": 0
                },
                {
                    "sent": "The advantage to pursuing like model, which is very much in medical imaging, your variance and you're mean will be coupled.",
                    "label": 0
                },
                {
                    "sent": "And in that case, your beta cannot be too big, because what happens if beta speak your noise level so high and then it got mixed up with your small betas.",
                    "label": 0
                },
                {
                    "sent": "So the heat was like homeless capacity of the errors is also very crucial for this as log P scaling.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of need to understand the suit under nonstandard conditions, and if you break some conditions you might not have so lucky of log P. So it seems that even you have P thousands lucky you just pretty much smaller.",
                    "label": 0
                },
                {
                    "sent": "Sample size would be OK if you Rs is small.",
                    "label": 0
                },
                {
                    "sent": "If it's really sparse, if it's not sparse, you see as well also hurt you if, as is the same order of an than any peoples Infinity, you might have problem.",
                    "label": 0
                },
                {
                    "sent": "So there are many players.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the game.",
                    "label": 0
                },
                {
                    "sent": "So for the.",
                    "label": 0
                },
                {
                    "sent": "The more recent work we have done is to look at Gaussian graphical models.",
                    "label": 0
                },
                {
                    "sent": "Gaussian graphical models has a nice representation if you do the inverse correlation, inverse covariance or precision matrix, so the edges.",
                    "label": 1
                },
                {
                    "sent": "On the graphical presentation, if the non edge will correspond to a zero in the inverse correlation matrix.",
                    "label": 0
                },
                {
                    "sent": "So we can now discover the conditional independence.",
                    "label": 0
                },
                {
                    "sent": "Through estimating zeros in the inverse correlation.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So suppose I have a vector.",
                    "label": 0
                },
                {
                    "sent": "Mean zero and I use it for the inverse covariance matrix and you and then or the one who first proposed well one penalized minus log likelihood.",
                    "label": 0
                },
                {
                    "sent": "And you can when you don't have a Gaussian assumption, you can look at this as L1, penalized Bregman divergent so it's still meaningful is a surrogate loss function, but won't be alike with anymore and there was a work by Banerjee Idle and later followed by Freeman for fast algorithms and what we did with Ravi Kumar will ride and Rescue D. Is to understand the sufficient condition we couldn't do necessary and sufficient for model selection, consistency for sub Gaussian and also.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Heavy tailed distribution you basically.",
                    "label": 0
                },
                {
                    "sent": "They are the same scaling where you have Gaussian random variable.",
                    "label": 0
                },
                {
                    "sent": "You have the same N over log and log P scaling.",
                    "label": 0
                },
                {
                    "sent": "So the that's a graph.",
                    "label": 0
                },
                {
                    "sent": "Well, an overload P become big.",
                    "label": 0
                },
                {
                    "sent": "The probability go to zero.",
                    "label": 0
                },
                {
                    "sent": "There's a transition phase transition effect.",
                    "label": 0
                },
                {
                    "sent": "Here if you scale with N, they don't stack up.",
                    "label": 0
                },
                {
                    "sent": "That's assigned to say, an overlock P really works as an effective sample size.",
                    "label": 0
                },
                {
                    "sent": "When things are nicely sub caution.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also tried very hard in the paper to identify relevant constants.",
                    "label": 0
                },
                {
                    "sent": "There's also a little bit not emphasize enough in order theoretical work.",
                    "label": 0
                },
                {
                    "sent": "People talk about MPNS, but that's not enough, right?",
                    "label": 0
                },
                {
                    "sent": "You need your parameter to be well behaved, which is you don't know for things to work.",
                    "label": 0
                },
                {
                    "sent": "So in this paper.",
                    "label": 0
                },
                {
                    "sent": "We tried to flush out all the different constants.",
                    "label": 0
                },
                {
                    "sent": "We define something called complexity.",
                    "label": 0
                },
                {
                    "sent": "Which case, do that?",
                    "label": 0
                },
                {
                    "sent": "I won't show you, but I'll just give you a graph to show that K involves like the smallest.",
                    "label": 0
                },
                {
                    "sent": "Like data in the regression model so that.",
                    "label": 0
                },
                {
                    "sent": "Unknown true coefficient will have to be large for your models.",
                    "label": 0
                },
                {
                    "sent": "Liking work intuitive makes sense right?",
                    "label": 0
                },
                {
                    "sent": "Because it's very close to zero.",
                    "label": 0
                },
                {
                    "sent": "It's going to be very hard to tell apart.",
                    "label": 0
                },
                {
                    "sent": "You cannot ask for model selection consistency, so this is a paper we try very hard to bring out all the constant to make explicit and show that whatever we define using the upper bound does show that have captured something.",
                    "label": 0
                },
                {
                    "sent": "I'm sure not capturing everything you can see when the complexity goes up, it takes bigger and bigger.",
                    "label": 0
                },
                {
                    "sent": "The sample size for user get consistency.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I come back so to the neuroscience modeling problem.",
                    "label": 0
                },
                {
                    "sent": "This is a reminder slide, so they were doing.",
                    "label": 0
                },
                {
                    "sent": "L1 boosting?",
                    "label": 0
                },
                {
                    "sent": "No, I won't go to boosting with small steps that's very closely related.",
                    "label": 0
                },
                {
                    "sent": "Lawsuit and use cross validation to stop and use correlation which they don't touch for validation.",
                    "label": 0
                },
                {
                    "sent": "So of course if you only have one voxel, you keep trying different methods.",
                    "label": 0
                },
                {
                    "sent": "Eventually you overfit even you do cross validation.",
                    "label": 0
                },
                {
                    "sent": "You proper validation set.",
                    "label": 0
                },
                {
                    "sent": "But what we have is that we have 13113 hundred voxels in the Viva area.",
                    "label": 0
                },
                {
                    "sent": "Most time we don't touch.",
                    "label": 0
                },
                {
                    "sent": "We only look at a few walks or santri the methodology and just apply.",
                    "label": 0
                },
                {
                    "sent": "So there we have some protection against over fitting.",
                    "label": 0
                },
                {
                    "sent": "Eventually of course we do enough that won't be enough, but now I think we're feeling OK.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is a project study about year and half when we first start working on the problem, we tried linear, right?",
                    "label": 0
                },
                {
                    "sent": "That's what they were doing and we really took advantage of the V1 understanding.",
                    "label": 0
                },
                {
                    "sent": "By the way, this type of transform.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "What we did.",
                    "label": 0
                },
                {
                    "sent": "Is really doing something very classical?",
                    "label": 0
                },
                {
                    "sent": "We use correlation selection too.",
                    "label": 0
                },
                {
                    "sent": "Do dimensionality reduction?",
                    "label": 0
                },
                {
                    "sent": "Remember we had 10,000 features and we just did correlation selection and we end up with 500.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Willing to Sammy supervised learning, which we didn't quite make it work for this case, so the idea was at the time seems so feasible.",
                    "label": 0
                },
                {
                    "sent": "Was that we have only about 2000 images.",
                    "label": 0
                },
                {
                    "sent": "We have FMR signals right.",
                    "label": 0
                },
                {
                    "sent": "Called labeled right regression, just responses labeled and then we have an image database of 11,000 which we didn't use.",
                    "label": 0
                },
                {
                    "sent": "So there's some population information.",
                    "label": 0
                },
                {
                    "sent": "Can we use that to help?",
                    "label": 0
                },
                {
                    "sent": "The prediction, so we're really going around the semi supervised learning that and then what we discovered.",
                    "label": 0
                },
                {
                    "sent": "So the Sigma is the population covariance, which we can estimate based database and see to like Sigma Hat is just a sample covariance based on the 2000 image features.",
                    "label": 0
                },
                {
                    "sent": "And what we discovered that the good Old Ridge works really well, which doesn't use any population covariance writes is recovering things.",
                    "label": 0
                },
                {
                    "sent": "So basically we matched the performance of the popular or more modern measured small step outta boosting with pre selection by correlation and do Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "So remember that not always all measures won't be useful.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And quite useful.",
                    "label": 0
                },
                {
                    "sent": "So that's for the top ten voxels.",
                    "label": 0
                },
                {
                    "sent": "So orderly squares because we did pre selection can do all this crash.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it's your post.",
                    "label": 0
                },
                {
                    "sent": "It just doesn't workout too, boosting that the leading method and rich.",
                    "label": 0
                },
                {
                    "sent": "Almost match or a tiny bit better than auto boosting with small steps.",
                    "label": 0
                },
                {
                    "sent": "And then what we call Sammy all else inverse is using the population covariance estimated from the large database.",
                    "label": 0
                },
                {
                    "sent": "It really got quite the same as rich.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "If you look at remember each my features has a meaning, is a wavelet with the location she will look at all the locations selected by the pre selection.",
                    "label": 0
                },
                {
                    "sent": "On this side we see a lot more coherence where they are on the image.",
                    "label": 0
                },
                {
                    "sent": "We know we want is very localized.",
                    "label": 0
                },
                {
                    "sent": "Vauxhalls it's 1000 thousands of.",
                    "label": 0
                },
                {
                    "sent": "Sells itself is very localized, so the logs are also kept some locality and this is and that's boosting on 10,000 features is more spread out because the noise and correlation so scientifically we feel like with prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "This seems to be more reasonable, but on the other hand, for a new problem this could be true to this to do all equally well for prediction.",
                    "label": 0
                },
                {
                    "sent": "But we kind of like this photo is coherent and all that, but it could be wrong, right?",
                    "label": 0
                },
                {
                    "sent": "But for V1 we feel like this more likely to be the story.",
                    "label": 0
                },
                {
                    "sent": "Then that one is all over the place.",
                    "label": 0
                },
                {
                    "sent": "Because the preselection this is nothing to do with rich.",
                    "label": 0
                },
                {
                    "sent": "Now we just do the correlation selection and this is auto boosting by search grading like decent kind of thing or matching pursuit with shrink.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Step.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's why we had that and we felt I wasn't publishable, right?",
                    "label": 0
                },
                {
                    "sent": "We got same prediction error.",
                    "label": 0
                },
                {
                    "sent": "Good understanding, reduce coverage.",
                    "label": 0
                },
                {
                    "sent": "But nothing to show and then Ravikumar came as a postdoc from Carnegie Mellon and came with him he.",
                    "label": 0
                },
                {
                    "sent": "Did US spam for his thesis with John Lafferty at CMU?",
                    "label": 0
                },
                {
                    "sent": "So what they did was they build on the additive models and make sure that some of the functions are zero by thresholding.",
                    "label": 0
                },
                {
                    "sent": "So the editor model is like linear regression but now you don't have a better J to multiply XIJ right?",
                    "label": 0
                },
                {
                    "sent": "That's the linear model.",
                    "label": 0
                },
                {
                    "sent": "You have a nonlinear transform which only acting on each feature at a time.",
                    "label": 0
                },
                {
                    "sent": "This is related work to Lee and John who are doing similar stuff, but they end up with a non convex problem and the spam.",
                    "label": 0
                },
                {
                    "sent": "Has your comics ultimate like a framework so I wouldn't say that, so we said that maybe we should give a try because our linear approach didn't seem to improve.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "So the pre selection became handy because spam cannot handle 10,000 features because there's a nonlinear fitting step.",
                    "label": 0
                },
                {
                    "sent": "What they do is that they do backfeeding feed each feature at a time through something like kernel density estimator or something and just too heavy.",
                    "label": 0
                },
                {
                    "sent": "And then if the L2 norm is not big enough, threshold that nonlinear function to be 0.",
                    "label": 0
                },
                {
                    "sent": "That's where the sparsity comes.",
                    "label": 0
                },
                {
                    "sent": "So the preselection became handy and we discovered that time like 100 is now much worse than 500 and in terms of computation was a lot faster.",
                    "label": 0
                },
                {
                    "sent": "So we went for 100.",
                    "label": 0
                },
                {
                    "sent": "And we discovered that for one single cell is understood that the orientation really matters.",
                    "label": 0
                },
                {
                    "sent": "But because each walk so is hundreds and thousand neurons, we kind of lost their orientations like selectivity.",
                    "label": 0
                },
                {
                    "sent": "So that's neuroscience.",
                    "label": 0
                },
                {
                    "sent": "And then we pulled, we find important locations and then we have some pulled direction for the direction selected.",
                    "label": 0
                },
                {
                    "sent": "We made them average to making new features.",
                    "label": 0
                },
                {
                    "sent": "Now based on understanding and selection from nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Basic sparse models and then we feed it again with the old features selected and the new features and I'm very happy that we end up using AIC because cross validation was too heavy so we went back to very classical stuff in the 70s and use the AIC which you don't need to cross validation.",
                    "label": 0
                },
                {
                    "sent": "One computation you have some measure of complexity of the model, we just did nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Non zero terms and we stop by AIC with minimum.",
                    "label": 0
                },
                {
                    "sent": "That's really that really.",
                    "label": 0
                },
                {
                    "sent": "Speed it up the calculate.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Asian.",
                    "label": 0
                },
                {
                    "sent": "So then we saw some improvement.",
                    "label": 0
                },
                {
                    "sent": "So based on the nonlinear model with pooling.",
                    "label": 0
                },
                {
                    "sent": "We improved cross board, the Vauxhall prediction performance by 12%, so the lab was quite happy because this is across the board.",
                    "label": 0
                },
                {
                    "sent": "Something got worse but mostly got better.",
                    "label": 0
                },
                {
                    "sent": "So we fixed the methodology and tried all 1300 voxels.",
                    "label": 0
                },
                {
                    "sent": "Remember I'm building each voxel with one linear model now.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "John Deere model.",
                    "label": 0
                },
                {
                    "sent": "And we look at announced linearity found nonparametrically right?",
                    "label": 0
                },
                {
                    "sent": "This is through kernel density, kernel, smooth or not, as the kernel regression smoother and they all have this compressive effect.",
                    "label": 1
                },
                {
                    "sent": "So you can see that mostly linear.",
                    "label": 0
                },
                {
                    "sent": "That's where most of data is far.",
                    "label": 0
                },
                {
                    "sent": "Linear model works so well, and then it tapers off.",
                    "label": 0
                },
                {
                    "sent": "And this biological makes sense because there's a energy constraint, right?",
                    "label": 0
                },
                {
                    "sent": "The signal even the stimuli gets higher and higher.",
                    "label": 0
                },
                {
                    "sent": "A real system doesn't have the energy to go up with it, so it tapered off and we thought well.",
                    "label": 0
                },
                {
                    "sent": "We want to compare across voxels and we thought maybe if we constrain the nonlinearity for each box.",
                    "label": 1
                },
                {
                    "sent": "So to be the same then we have one in linear algebra, equal show and then we can compare across voxels so that.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This motivated us to constrain the spam model to the identical spam model, so we now say all the nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Function F cannot depend on J anymore, has to be the same.",
                    "label": 0
                },
                {
                    "sent": "So we start with spam.",
                    "label": 0
                },
                {
                    "sent": "And then we do some basic functional normal equation to constrain that.",
                    "label": 0
                },
                {
                    "sent": "It's all in the paper.",
                    "label": 0
                },
                {
                    "sent": "I won't give you the details.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To fit it.",
                    "label": 0
                },
                {
                    "sent": "And now we bring up another four 5% improvement.",
                    "label": 0
                },
                {
                    "sent": "First we thought were loose in term prediction because we put in the constraint.",
                    "label": 0
                },
                {
                    "sent": "Instead, we improve the prediction.",
                    "label": 0
                },
                {
                    "sent": "So that means that our.",
                    "label": 0
                },
                {
                    "sent": "Non constrained nonlinear model was still overfitting.",
                    "label": 0
                },
                {
                    "sent": "There's not that much.",
                    "label": 0
                },
                {
                    "sent": "None of the other two just still too many uncertainties like dimensions in the model.",
                    "label": 0
                },
                {
                    "sent": "So this is about the sparsity.",
                    "label": 0
                },
                {
                    "sent": "Remember, before we improved 1412%.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "16%.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is a case that we basically after the first stage of pre selection anrich went back to old statistical methods.",
                    "label": 0
                },
                {
                    "sent": "We basically throw some of the shell machine learning method.",
                    "label": 0
                },
                {
                    "sent": "To the problem and we thought about it and we constrained to tailor the problem to tailor the message.",
                    "label": 0
                },
                {
                    "sent": "The problem by constraining the nonlinearity.",
                    "label": 0
                },
                {
                    "sent": "And I was teaching applies to this course and the final exam of my course was on this data and one student who is also hanging out with my group.",
                    "label": 0
                },
                {
                    "sent": "I had a very nice idea.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So he heard about the nonlinearity.",
                    "label": 0
                },
                {
                    "sent": "I still think that if he was not hanging out with my group, he probably wouldn't think about that now, neither transform.",
                    "label": 0
                },
                {
                    "sent": "So what he did for the final exam was that again went back.",
                    "label": 0
                },
                {
                    "sent": "Very classical statistics.",
                    "label": 0
                },
                {
                    "sent": "He looked at the features.",
                    "label": 0
                },
                {
                    "sent": "And plotted the response with feature right?",
                    "label": 0
                },
                {
                    "sent": "That was the first year PhD course, so we do a lot of scatter plots and then things are not nice in Goshen like a football, if you take an interest, it is of course you know the best way to do linear regression.",
                    "label": 0
                },
                {
                    "sent": "You have a football data.",
                    "label": 0
                },
                {
                    "sent": "Starting skewed so we also talk about power transform.",
                    "label": 0
                },
                {
                    "sent": "That old statistical trick that to make things into linear.",
                    "label": 0
                },
                {
                    "sent": "Regression when you have Gaussian data.",
                    "label": 0
                },
                {
                    "sent": "So he did fall through transform and you suddenly have this nice football shape.",
                    "label": 0
                },
                {
                    "sent": "So he basically was cleaning up what we did with high power machine learning techniques to discover the nonlinearity in the match.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nicer way.",
                    "label": 0
                },
                {
                    "sent": "He just looked at all the feature histograms and tried the power transform.",
                    "label": 0
                },
                {
                    "sent": "He this is very human, interactive and decided the square root and false root.",
                    "label": 0
                },
                {
                    "sent": "For different levels of the wavelet worked pretty well, so he did two fixed transforms, remember for.",
                    "label": 0
                },
                {
                    "sent": "First try with the high power of spam model.",
                    "label": 0
                },
                {
                    "sent": "We allowed every feature to have any linear nonlinear transform it can have, and then we went to the other extreme, make all the transforms the same and it did get better.",
                    "label": 0
                },
                {
                    "sent": "So one transform was better than 100 transforms.",
                    "label": 0
                },
                {
                    "sent": "What he did is 2 transforms, cube root, false root and square root and he looked at the data and look at histograms.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "For some of the features depends on the level he did a square root transform.",
                    "label": 0
                },
                {
                    "sent": "And then for the other levels of wavelets he did a false route.",
                    "label": 0
                },
                {
                    "sent": "And compared with the sparse linear model, remember the best we could do was 16% improvement.",
                    "label": 0
                },
                {
                    "sent": "He was about to do 21%.",
                    "label": 0
                },
                {
                    "sent": "So this is very classical statistics right?",
                    "label": 0
                },
                {
                    "sent": "It's power transform?",
                    "label": 0
                },
                {
                    "sent": "We teach in the first year course.",
                    "label": 0
                },
                {
                    "sent": "PhD is now master.",
                    "label": 0
                },
                {
                    "sent": "I like to think that the machine learning methods are still had a role to play to point to the nonlinear transform because at the time when we couldn't beat them with linear models, we basically didn't know where to go where.",
                    "label": 0
                },
                {
                    "sent": "We could have thought about that, just we didn't feel there's so many things you could try.",
                    "label": 0
                },
                {
                    "sent": "And the one free kind of machine learning powerful machine did point us.",
                    "label": 0
                },
                {
                    "sent": "Nonlinear transform is so consistently compressive, right?",
                    "label": 0
                },
                {
                    "sent": "That was a surprise and then.",
                    "label": 0
                },
                {
                    "sent": "But this guy really cleaned it up with very classical stuff.",
                    "label": 0
                },
                {
                    "sent": "I'd like to think.",
                    "label": 0
                },
                {
                    "sent": "I think he agrees that without sitting through meetings in this previous work, he wouldn't have thought about nonlinear transform.",
                    "label": 0
                },
                {
                    "sent": "But in the end we went to the lab.",
                    "label": 0
                },
                {
                    "sent": "That's what they like most.",
                    "label": 0
                },
                {
                    "sent": "I like most two.",
                    "label": 0
                },
                {
                    "sent": "This is simple cube root, source, root and square.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But with a role to play.",
                    "label": 0
                },
                {
                    "sent": "Soap.",
                    "label": 0
                },
                {
                    "sent": "In terms of prediction, is simple, so the left now says we can do this fall through the square root transform.",
                    "label": 0
                },
                {
                    "sent": "That's what they're going to do.",
                    "label": 0
                },
                {
                    "sent": "They can take it more like a protocol now.",
                    "label": 0
                },
                {
                    "sent": "If you look at the voxels, remember I said the box was very localized, the first Vauxhall is just different resolution for the feature features, because each wavelet has a location so clearly.",
                    "label": 0
                },
                {
                    "sent": "All.",
                    "label": 0
                },
                {
                    "sent": "Malta one.",
                    "label": 0
                },
                {
                    "sent": "Cares almost like 2 areas now so strongly, but voxels two also two areas.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The student who really doesn't like this.",
                    "label": 0
                },
                {
                    "sent": "He like to have more interval model and he want to make things localized so he had tuned some criteria which you don't have to worry about to design the locality and this way you are 10 times faster.",
                    "label": 0
                },
                {
                    "sent": "So this is where subject measure the problem or enter to speed up general purpose machine learning technique.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "To prove that the.",
                    "label": 0
                },
                {
                    "sent": "This localization has a biological meaning.",
                    "label": 0
                },
                {
                    "sent": "Is that remember now each voxel?",
                    "label": 0
                },
                {
                    "sent": "We give it a location depends on the response.",
                    "label": 0
                },
                {
                    "sent": "And then he went into data and look at.",
                    "label": 0
                },
                {
                    "sent": "This location for this vocal voxel and look at if the response signal is very high now will call strong and see what's the image and there is if the response.",
                    "label": 0
                },
                {
                    "sent": "I've had my signal is very low with the image you can see that for the weak signals just not much to see the work so is not seeing any interesting sort of.",
                    "label": 0
                },
                {
                    "sent": "Also is not interested to fire.",
                    "label": 0
                },
                {
                    "sent": "For this you see there are things going on for the edge stuff.",
                    "label": 0
                },
                {
                    "sent": "Now making it like oh location important, but the center of the localization is more important than the French because we have to have area.",
                    "label": 0
                },
                {
                    "sent": "So this is an indirect proof that the localization captures something meaningful.",
                    "label": 0
                },
                {
                    "sent": "Because of course this is a problem.",
                    "label": 0
                },
                {
                    "sent": "We can see things, I know what's meaningful or not, so a lot of biometric problem will be loud harder.",
                    "label": 0
                },
                {
                    "sent": "We do have advantage that its vision and images.",
                    "label": 0
                },
                {
                    "sent": "We all make sense of image.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, this localization doesn't work for all voxels.",
                    "label": 1
                },
                {
                    "sent": "For this you cannot see anything I have read here for this vauxhalls.",
                    "label": 0
                },
                {
                    "sent": "The global measure does worse, so that's better, so the localization, because we give each box or one location, and we know for the tool box will show you some voxels have two locations.",
                    "label": 0
                },
                {
                    "sent": "Remember, each box was a hundreds and thousands of neuron cells, so they could be just.",
                    "label": 0
                },
                {
                    "sent": "Chris Brown in two different locations.",
                    "label": 0
                },
                {
                    "sent": "Single cell usually only cares about my local.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question becomes.",
                    "label": 0
                },
                {
                    "sent": "Who really drive the cells were slowly tried to answer the question of causation, but with caution, because this is also Association analysis.",
                    "label": 0
                },
                {
                    "sent": "But we tried to build in some subject measures so that it will be more credible than just come up from a model so it could be that.",
                    "label": 0
                },
                {
                    "sent": "You select around locations because things are correlated between different locations and the different levels of wavelets also correlated an.",
                    "label": 0
                },
                {
                    "sent": "The different neighboring voxels may be conecting relating to the same visual effects.",
                    "label": 0
                },
                {
                    "sent": "So can we distinguish between features?",
                    "label": 1
                },
                {
                    "sent": "Or causation works response.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's where we thought about using sparsity sparse graphical models.",
                    "label": 0
                },
                {
                    "sent": "Remember this transform, false root and square root.",
                    "label": 0
                },
                {
                    "sent": "Make things very fast right?",
                    "label": 0
                },
                {
                    "sent": "Because just do the transform before have to fit.",
                    "label": 0
                },
                {
                    "sent": "It's very labor intensive and for higher vision area that's where eventually want to do this will be can be done very fast.",
                    "label": 0
                },
                {
                    "sent": "And if we want to build a sparse graphical model Gaussian mode, I couldn't think of anything better than this example after the transform margin of sparse and we care about causation, so that's what I suggest that we should look at sparse graphical models for these two log so you can see that they seem both care about two areas.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if you feed the sparse graphical models.",
                    "label": 1
                },
                {
                    "sent": "For voxel one.",
                    "label": 0
                },
                {
                    "sent": "I have features Box 2.",
                    "label": 0
                },
                {
                    "sent": "I have features, so if I feed them separately features Box 1 one models are graphical models and we see both.",
                    "label": 0
                },
                {
                    "sent": "These are the non zero coefficient in the sparse graphical models.",
                    "label": 0
                },
                {
                    "sent": "And you look at work so too.",
                    "label": 0
                },
                {
                    "sent": "Seem to care about both, not as strongly.",
                    "label": 0
                },
                {
                    "sent": "But the last panel I now for two boxes together with the other features.",
                    "label": 0
                },
                {
                    "sent": "So if it a larger sparse graphical model.",
                    "label": 0
                },
                {
                    "sent": "And suddenly it's a nice separation of.",
                    "label": 0
                },
                {
                    "sent": "This works all seem to only have feature relation between the voxels with area one and the other one only error two.",
                    "label": 0
                },
                {
                    "sent": "Of course the two boxers now have a link, so they are not conditional independent.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is seemed to be captured like.",
                    "label": 0
                },
                {
                    "sent": "I'll see.",
                    "label": 0
                },
                {
                    "sent": "Decoupling things in a way seem to make sense.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Don't we realize that?",
                    "label": 0
                },
                {
                    "sent": "You know, maybe the neighboring voxels are connected and we look at the two log shows they actually next to each other.",
                    "label": 0
                },
                {
                    "sent": "We have a way to find out where they are on the brain map.",
                    "label": 0
                },
                {
                    "sent": "So now we build a bigger graphical model by looking at one voxel and six neighbors.",
                    "label": 1
                },
                {
                    "sent": "And then we build individual models for each box with teachers and with the predicted values as the new feature to a.",
                    "label": 1
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To put them together.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the neighbor model.",
                    "label": 0
                },
                {
                    "sent": "So for each voxel you have six neighbors for each of the neighbor you build a model with the features and you have a predicted value with a linear combination of other features, and I use the six predicted values with the original features and build another model.",
                    "label": 0
                },
                {
                    "sent": "Again, sparse graphical model for the middle lock.",
                    "label": 0
                },
                {
                    "sent": "So and that's the.",
                    "label": 0
                },
                {
                    "sent": "As a result, so we basically do as well as original global model.",
                    "label": 0
                },
                {
                    "sent": "Remember, local model doesn't work as well, but what's interesting is that this is where things don't work well.",
                    "label": 0
                },
                {
                    "sent": "The most improvement comes from low signal areas.",
                    "label": 0
                },
                {
                    "sent": "So this is a constraint we we really think it's the right one because it really can lift.",
                    "label": 0
                },
                {
                    "sent": "Improve the signal for the low signal noise regime.",
                    "label": 0
                },
                {
                    "sent": "So now we have a model much simpler than the global model, but do as well if not better than the global model.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's where we are and.",
                    "label": 0
                },
                {
                    "sent": "For the first part of the talk, we basically talked this radical results and if you want to go home, I want you to remember the geometric picture about what's going on with the Sue, the pyramid constraint an the cigar type stuff, and the correlation.",
                    "label": 0
                },
                {
                    "sent": "And if you hear about the log N log P as kind of the right way to calculate complexity for the search, you should wear.",
                    "label": 0
                },
                {
                    "sent": "Be aware of that, that's to do with caution.",
                    "label": 0
                },
                {
                    "sent": "Subtitles, shotguns and tails and.",
                    "label": 0
                },
                {
                    "sent": "I think it's important to understand that all these properties rely on values of the model, which you don't know, not just MPNS.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for the neuroscience problem, we through high power machine learning, we discover some nonlinear compressive effect.",
                    "label": 1
                },
                {
                    "sent": "But really, the going back to the classical statistic group like false root and square root really makes things a lot cleaner.",
                    "label": 0
                },
                {
                    "sent": "Maybe I can touch this algorithmically Eda EDA institution means explore data analysis and for huge amounts of data is very hard to plot and see things so.",
                    "label": 0
                },
                {
                    "sent": "In statistical.",
                    "label": 0
                },
                {
                    "sent": "Eda, like Bill Cleveland, has been writing about EDA should really include also a model fitting part.",
                    "label": 1
                },
                {
                    "sent": "I think they spend model or the constraints by more than really serve as modeling techniques control on large dimension data and point out things later.",
                    "label": 0
                },
                {
                    "sent": "You can look at more detail so this interaction between visualization and model fitting.",
                    "label": 0
                },
                {
                    "sent": "And of course we rely a lot on the problem with vision.",
                    "label": 0
                },
                {
                    "sent": "We can look at it and talk.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into the collaborators.",
                    "label": 0
                },
                {
                    "sent": "So where we are now, we really need we moving towards decoding.",
                    "label": 0
                },
                {
                    "sent": "So one thing is remember the nature paper they were doing nonlinear models and indirect validation of nonlinear model local nonlocal is to do decoding.",
                    "label": 0
                },
                {
                    "sent": "Whether we can do better decoding.",
                    "label": 0
                },
                {
                    "sent": "That's indirect proof that this is needed direction and scientifically.",
                    "label": 0
                },
                {
                    "sent": "The most challenging is higher vision Area V4, where attention and memory of.",
                    "label": 0
                },
                {
                    "sent": "Play a role and whether we can make a go at the lab right now is in the middle doing some very beginning work on that, and possibly the nonlinear presentation can be useful.",
                    "label": 0
                },
                {
                    "sent": "This nonlinear model can be useful.",
                    "label": 0
                },
                {
                    "sent": "Image presentation thank.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the most important slide.",
                    "label": 0
                },
                {
                    "sent": "So I work with many people.",
                    "label": 0
                },
                {
                    "sent": "The first 2 lines on the theoretical work and the next line are the people I work with for the neuroscience problem.",
                    "label": 0
                },
                {
                    "sent": "This call and let people who have been working with thank you.",
                    "label": 0
                },
                {
                    "sent": "Time for one of the quick questions.",
                    "label": 0
                },
                {
                    "sent": "Chris Chris.",
                    "label": 0
                },
                {
                    "sent": "Sorry about that.",
                    "label": 0
                },
                {
                    "sent": "Represent about.",
                    "label": 0
                },
                {
                    "sent": "Early data.",
                    "label": 0
                },
                {
                    "sent": "Directions.",
                    "label": 0
                },
                {
                    "sent": "Well, 460 sign if your access are still holds.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's there's you can generalize too.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's perfect design as long as that holds, I think it will all go.",
                    "label": 0
                },
                {
                    "sent": "It's not distributional.",
                    "label": 0
                },
                {
                    "sent": "And if you do a random example setting for finale is OK too.",
                    "label": 0
                },
                {
                    "sent": "But however, if you do classification problems there, there is a basic you have similar responsible condition, but even for dental condition will be on the hashing out the loss function.",
                    "label": 0
                },
                {
                    "sent": "So so if you not have L2 loss, you have like logistic regression loss.",
                    "label": 0
                },
                {
                    "sent": "You have a similar representable condition.",
                    "label": 0
                },
                {
                    "sent": "We're finishing a paper on that that you basically have similar step but X prime X will be replaced by the Haitian of the loss function.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it's really quadratic.",
                    "label": 0
                },
                {
                    "sent": "You do a quadratic approximation and you backed away the squares and then things goes.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "My questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}