{
    "id": "deamowgmykqcprqxtuixj5bgixa7h3aa",
    "title": "Learning Similarity Metrics with Invariance Properties",
    "info": {
        "author": [
            "Yann LeCun, Computer Science Department, New York University (NYU)"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "December 2006",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Structured Output"
        ]
    },
    "url": "http://videolectures.net/lce06_lecun_lsmip/",
    "segmentation": [
        [
            "With the topic of this this workshop and because of interesting invariants is going to be similarity metrics within variances.",
            "So I'm going to talk mostly about some work I've been doing with two of my students where you had cell and sutra.",
            "And."
        ],
        [
            "So the first thing I want to say is if you want to learn.",
            "A similarity similarity metrics.",
            "We need to train some sort of parameterized function.",
            "In such a way that when we map input vectors to two output vectors within the mapped IP vector space, we can compare examples using a very simple metrics that corresponds to kind of the semantic distance between objects, so that objects that are semantically similar will have a small distance in that space, and objects that are semantically different will have a large distance in that space.",
            "So we're going to take a parameterized mapping GW of X, where W is the parameter we're going to.",
            "We're going to learn such that the L1 distance between the map mapping of two examples reflect kind of the semantic distance.",
            "We could use other distances in our one, but we use our one in most of those cases, or L2 sometimes.",
            "So the advantage of doing this.",
            "The reason why we got interested in this, I started working on things like this.",
            "Like 1230 years ago but.",
            "Sort of didn't do much about it in the meantime, until about two years ago, and the reason we started working on this again recently is because we're interested in solving the problem of being able to classify objects into many many categories, including categories for which we don't have training samples at the time of learning.",
            "So that's kind of that.",
            "Seems kind of a problem right?",
            "Because how are you going to recognize categories?",
            "We don't have any training samples, but if you have a trainable similarity metric.",
            "As soon as you get an example from a from a new category, then you can immediately use it as a template for other.",
            "Other samples, so nonparametric methods are appropriate for kind of large.",
            "Large class of large class problems.",
            "A large number of classes problems but but of course they don't work so well.",
            "So how do you kind of make parametric methods?",
            "Used in a way that you know, nonparametric methods could be used."
        ],
        [
            "So one of the things we want, techniques we we came up with way back when is something called Siamese architectures.",
            "So this is the idea of sort of coupling.",
            "Two of those mappings.",
            "So you take 2 identical Siamese Twins that are joined at the head, which is what we call this time is Siamese architecture.",
            "And you know you plug 2 images in it and on the output you compare the two and that gives you kind of a. Resemblance or dissimilarity between the two the two objects?",
            "Um, so here you could apply this to things like face recognition, where you might be asked to recognize people and the number of categories might be in the 10s of thousands, and Furthermore, at the time the time you turn this function you don't have all the people that you're supposed to recognize.",
            "You sort of giving them after after the fact.",
            "So, you know, feed one of the images and then sort of compare that image to all the ones that are in your in your Gallery and put the some sort of comparison between the two to figure out.",
            "Which person this is?",
            "So we could do."
        ],
        [
            "Is there some sort of minimization over a set?",
            "OK look for the person in the set that minimizes the output of your system, the energy if you want to call it this way for those of you who were met tutorial Atnip, since it's kind of derived from this idea of energy based systems also.",
            "OK, so how are we going to train this?",
            "It's very simple idea.",
            "You take 2 examples of the same person.",
            "For example, if you want to face recognition and you train the machine to produce a small distance.",
            "Four pairs of semantically similar inputs, and then if you plug 2 examples of different people, you train the machine to produce large distances.",
            "OK, so it's a little different from the traditional learning in which you have, you know, a sample and label.",
            "Here you have two samples and you also have a label that indicates whether they are the same or not, or whether the similar or not.",
            "Some similarity between them."
        ],
        [
            "So the advantage of course over you know, traditional discriminative learning is that.",
            "Again, you don't have to have all the categories at training time, because you can always add a new sample to category and hopefully your distance measure will have generalized to new samples."
        ],
        [
            "So you could view this also as a dimensionality reduction technique, because of course the output representation.",
            "Most of the time we sort of somewhat smaller than the inputs.",
            "If you start with an image you know you could map them to space or say, 50 or 100 dimensions, and so you can compare that with dimensionality reduction techniques like PCA.",
            "So PCA is used, was used in the vision community, discovered PCA with face recognition.",
            "It's kind of weird, but that's what happened.",
            "And you could use something that's a little more appropriate for this LDA, which takes into account the labels, but that doesn't quite work.",
            "And all of these things are a linear, so the problem linear mappings is that it's very difficult to kind of build invariants.",
            "Complicated invariances?",
            "What if the illumination changes?",
            "What if the person turns the head?",
            "What if you know person wears glasses?",
            "It's very unlikely that with a linear mapping to be able to take into account all those variabilities and still map them to the same vector at the end.",
            "You also have kernel PCA and kernel today, but.",
            "Again, unless you build the invariants in the kernel, you're not going to get really good interpolation and.",
            "Then you have things like alien and yes, local linear embedding and multidimensional scaling, and the problem with these guys is that they're great for visualization, but they don't actually give you a mapping that just, you know, give them a bunch of points and they give you a bunch of points out, but you don't have a mapping between points on the output point, so if you have new points, you have to kind of redo the whole thing again.",
            "And Furthermore, the new points you need to know the actual relationship between the new points on the old points to be able to do the thing again, so you can't really use this with the test set.",
            "Yes, revenge, oh and his group and various other people have proposed have proposed extensions of those things.",
            "For situations where you want to do out of sample mapping tool dimensional spaces.",
            "But it's basically consistent.",
            "Running Italy and then training parameterized function to kind of learn this mapping independently.",
            "OK.",
            "The."
        ],
        [
            "Positive would be to sort of built the invariants into the distance measure, so using things like attention distance methods or elastic matching.",
            "Or use your favorite feature set that's appropriate for the object you're trying to do, but it's kind of more fun as kind of machine learning people to try to run the whole thing right, and that's been sort of my obsession for last 20 years, so.",
            "So.",
            "So we're going to try it.",
            "So this first thing we tried, you know.",
            "Back in I had it 9090, three, 92 and now it's 94.",
            "I'm not even sure which year is correct, OK?"
        ],
        [
            "Over 94 so so this is what we use way back for doing signature verification and it worked pretty well.",
            "So we use the Siamese architecture and the G function was a A1 dimensional convolutional neural net.",
            "Time today neural net which which we could feed the sequence of the pen trajectory and then we could compare the two.",
            "The reason we're doing this, it turns out signature verification is actually very fairly easy.",
            "You don't need all that machinery.",
            "The reason we did this is because we could map the.",
            "Those things into a very, very compact vector that represented you know everything there is to know about the signature, and then this this compact vector with less than 80 variables could actually be stored in the magnetic magnetic strip of a credit card.",
            "So that was the only reason why we did this.",
            "It was never really used or diploid.",
            "It worked pretty."
        ],
        [
            "Though.",
            "The problem with this is."
        ],
        [
            "We work pretty well.",
            "We actually hadn't really figured out with the correct loss function was it was very unstable.",
            "We had to adjust with other parameters so that didn't come until recently.",
            "So again, we're going to do now."
        ],
        [
            "Talk about faces.",
            "Some more recent work.",
            "And.",
            "Again, we're going to do is change the parameters of the function such that you know we get a small distance for identical faces and large distance, otherwise OK."
        ],
        [
            "So what approach to this, which I'm sure Sam will tell you about, is to sort of take the probabilistic view so you can view why.",
            "So here why we did the label associated with the second phase, OK?",
            "And you know, you might have multiple faces that have the same label, so you might have to do something complicated.",
            "But what you need to do is if you want to approach the problem probabilistic way, you could view the energy's kind of the log of a probability and then to get a conditional probability of the label given the input you take it to the minus the energy and normalized by the sum of each of the minus energy.",
            "Overall possible labels.",
            "OK, so you get a normalized conditional distribution over.",
            "Over labels by.",
            "By doing this normalization.",
            "So training the system to maximize likelihood consist in.",
            "So this would be the conditional likelihood, which would be the product of all training samples of this conditional likelihood there OK?",
            "And you want to maximize this with respect to the parameters of this energy function in the W you take minus log of that, and that gives you a loss function that you can minimize.",
            "OK, so you get this loss function and of course your face again with the problem that this is the energy produced by the the current training pair.",
            "If you want so you have a pair here and here you have.",
            "You have one sample here and here.",
            "The second sample, or here I sort of represented by the label, the second sample.",
            "But really you want to feed the second sample.",
            "Here the parent here, whether they are identical or not.",
            "And here you have to sum over all all other images.",
            "All images in your training Gallery.",
            "OK, if you really want to be correct about this.",
            "That's really expensive, right?",
            "Because for each training sample you have to go through the entire Gallery and compute the Z to the matters.",
            "The energy and sort of figure out where the value of this thing is.",
            "So of course you're going to cut corners."
        ],
        [
            "Um?",
            "So the total solution is sampling.",
            "But then again, sampling is the solution to everything in Toronto.",
            "So.",
            "Sorry.",
            "So so salmon.",
            "Jeff and some of the students, had you know, a few ideas some time ago, one for doing embedding, another one for doing sort of similarity metrics, network component analysis.",
            "And you know, I see these guys all the time and it turns out.",
            "I figured out this, but not really in because I was addressing something else and then we came up with our paper and realize you know Sam had done something similar before and so I have to apologize to him in public for not citing this paper in mind, even though we talk to each other all the time.",
            "So this comes with things happen even between friends.",
            "But of course, you know we had to come up with a different solution.",
            "Since we don't, we didn't know about about these guys and so our solution is New York solution.",
            "Just go about the worst offenders, go after the worst offenders and forget about all the other guys.",
            "So that."
        ],
        [
            "It sort of ended up with a different loss function.",
            "And the idea is not to pull up on the energies of all the answers.",
            "All the bad answers, but just pull up on the energy of the most offending incorrect answers.",
            "We just pick the you know the samples in your.",
            "Set approximately you don't actually have to be really accurate about this that are that really other most offending.",
            "So if you show a pair of two images of the same person that you look through a subset for a small subset doesn't have to be the full set for one that.",
            "Has a low distance to one of your samples in there, but happens to be an image of a different person and then you pull that.",
            "You pull that guy away.",
            "OK, you make the energy larger for that guy.",
            "And or maybe a few of those guys, not just one, but if you are in fact we use just one.",
            "So this is notion of most offending or worst, offending incorrect answer.",
            "Do you have any statistics that show that the exponential decrease wins out, so that effect only one offenders is 1 years?",
            "Take does it?",
            "Typically, what happens is that as you go further, many, many more examples that are bigger than distance.",
            "Well, OK, so it really increased exponentially.",
            "What so I don't have?",
            "Which exponential?",
            "Issue.",
            "Numbers of neighbors know, but you only taking one, so you know only taking the worst one and pulling it away, right?",
            "That actually shows that only taking one neighbor is enough.",
            "Well, the only evidence we have is the results you can get with this.",
            "Compared to results you can get without a loss functions and have to admit that comparison you know face to face comparison with different loss functions in terms of of you know how fast does it converge with a given number of comparable amount of computation hasn't been done, so it's hard to say.",
            "That's right, I mean, my guess is that you know if the function is linear, you have a perception like.",
            "I mean, depending on the function you use, if user hinge for example, you have a perception like convergence procedure of some kind, But if.",
            "In your case, you know it's much more difficult and depending on the loss function that we actually use, I doubt there is good.",
            "Way of proving it.",
            "OK, so in any way.",
            "In any case, we're going to have two different loss functions or two different parts of the loss function.",
            "One that is the way we want to minimize when we have to.",
            "22 samples that are semantically similar and one that contains the.",
            "The energy for the pair that.",
            "Is not semantically similar.",
            "You know comparison with the most of any incorrect answer.",
            "We're going to make that energy large, and so the last."
        ],
        [
            "It has to be a decreasing function of this, so we're going to do is, you know, compute the L1 distance.",
            "Here we have plugged their own distance into these two.",
            "These two half of the loss function.",
            "So that's the first half, so we're going to pay quadratically for making the L1 distance between the two vectors large.",
            "OK, so this loss function is going to try to make the distance zero.",
            "Essentially, four pairs of identical faces, or the same person.",
            "An and this loss function is an exponentially decaying function.",
            "And is going to try to make the distance for different people between different people as large as possible, but the force is exponentially decaying, so you know after awhile it's not going to push much.",
            "And because of this choice of horse offending, incorrect answer, the one that are far away will actually not going to be pushed up, pushed out, pushed out at all OK, particularly if you combine that with a little bit of regularization.",
            "You know things can go very far.",
            "There are several types of functions like this that we've thought about going to show you another one after later in the talk for a different different use of this.",
            "Um?",
            "OK, the important thing is that you want this function to really kind of not push as hard on patterns that are far away from each other, and that's one mistake we did with the old work with signature verification.",
            "We didn't have the right loss function for that."
        ],
        [
            "So if you want to be sort of more hard nosed about this, one possibility is to use one of those.",
            "What I called the square square loss, so you pay quadratically for making the energy the first time energy large, and you pay quadratically also for making the second term small and the.",
            "Loss function vanish is above a certain margin, so here you can enforcing some sort of margin between the you know your friends and your enemies.",
            "But we're going to use the."
        ],
        [
            "Square exponential loss.",
            "With you know you need to kind of hook up some coefficients here so that things work OK."
        ],
        [
            "So the cooking up the coefficient will determine.",
            "Essentially it's not really crucial, it will determine the the tradeoff between those two functions if the first time is too important, then everything is going to lapse.",
            "If it's not going to collapse, but everything is going to be kind of really tightly.",
            "Cost around a point.",
            "If you increase the importance of the exponential decay then things are going to expand.",
            "So you want to.",
            "It's going to set the scale of the whole of the whole set of vectors.",
            "So we tried doing this on the.",
            "On the AT&T Olivetti.",
            "Data set, so this has 40 subjects but 10 images per subject.",
            "And with some you know a moderate degree of variation, imposing lighting, expression and everything.",
            "So we used images from 35 subjects for training and then use the other five for test.",
            "So it's different subjects altogether.",
            "Do figure out the test.",
            "So we kind of sub sampled the set of all possible so you know I'm making fraternal, but I'm using this inside, so we use the same kind of trick of sampling to kind of sample the set of possible pairs, because there's so many pairs, right?",
            "Um?",
            "So in fact we use something like 12,000 impostor pairs."
        ],
        [
            "We also use the fair database, which is a lot more challenging 'cause the faces of more more variation of pose in it.",
            "But we use that only for training.",
            "We didn't actually test on this.",
            "We tested on this other data set called the Purdue data set and this one is in."
        ],
        [
            "Credibly challenging's got quite a lot of subjects, and it's got images 26 images of each subject taken into two sessions several weeks apart apart and people were sort of making extreme sort of faces and extreme changes of kind of, you know.",
            "Sunglasses and you know.",
            "So it's very challenging data set."
        ],
        [
            "More examples of that of that data set."
        ],
        [
            "So the first thing we do is we.",
            "Well, she cropped and said samples and center those images, but this was done automatically with a very simple system, so we didn't do this by hand.",
            "OK, and the way this was done was by."
        ],
        [
            "Yeah, essentially kind of finding a.",
            "You know.",
            "We sort of generated a sort of average face if you want and sort of find the best match to find a good center and then sort of crap real thing around it.",
            "So there's a lot of jitter in this decentering, so the face kind of vary quite a bit in the imposition."
        ],
        [
            "And then we plug a convolutional net on it.",
            "Not surprisingly.",
            "So this."
        ],
        [
            "This is kind of a fairly large convention that's sort of similar to what we use for handwriting or image recognition is just slightly higher resolution.",
            "A little more feature Maps, so it's got six layers like most of the commercial less we use and go into the details of how this is built.",
            "You can read.",
            "Some papers going back 15 years."
        ],
        [
            "And this is sort of what we get on the output when we plug some images to it.",
            "So I apologize little hard to read, but this is kind of the internal state of the network for the corresponding image and what you have to look at is kind of the output vector, which is this 50 dimensional output vector here.",
            "OK, so it's a bit of a bit of a challenge if you're far away, but.",
            "But the thing that you can see is that so you see a bunch of white white areas and black dots, right?",
            "And if you plug a very different image of the same person here you see another array of black dots on white areas and they pretty much at the same place.",
            "OK is it goes to black dots here to black dots white, white, black black?",
            "You know with a few differences, so black indicates press one and white minus one or the other one around there, not all saturated from some are great, but you can't really see them.",
            "So again, here you get 2 vectors that are similar on the outputs when you give images 2 images, the same person.",
            "And here are two images of different person and you see the black dots are really not at the same place at all.",
            "So this guy is back out here, not here and you know it's got a completely different configuration there.",
            "So somehow these things sort of captures.",
            "You know some interesting difference between the two things.",
            "Same on the other side."
        ],
        [
            "So if you want to apply this to kind of new faces you have, you have to sort of build a model.",
            "You know, given a single or just a few examples of a person, how do you compare that person to the other one?",
            "And maybe a very simple Gaussian model in this output space?",
            "OK, so just gather whatever genuine examples you have and compute kind of regularised Gaussian model based on the samples of that persons you have and then when you person comes in you need to compare just compute the likelihood of that person under this distribution.",
            "And you know threshold or you know, don't do the exponential and it's like a quadratic distance threshold."
        ],
        [
            "So.",
            "That's interesting.",
            "OK, so on the.",
            "Produce this at.",
            "I apologize for this doubling of the characters here.",
            "I don't know what happened.",
            "The.",
            "The producer said we get something like 7% tuna, half percent false accept.",
            "For 14% false rejects.",
            "Which is not, you know, doesn't look too good, but it's really very challenging data set.",
            "Not many people get good results on that one.",
            "Um?",
            "And."
        ],
        [
            "So these are some of some of the examples.",
            "Of.",
            "Pairs, so for example, you know those two have a fairly low energy on the output.",
            "There actually images from the from the same person, so those are correctly identified as genuine pairs of pairs with images of the same person.",
            "Despite the wide variation of lighting conditions, and.",
            "And you know.",
            "Sunglasses or glasses or no glasses and covering the mouth.",
            "These are correctly identified impostors.",
            "And these are mistakes.",
            "So these two images are identified as being from different persons.",
            "But this is actually the same and these from the same person that they're actually different.",
            "So it's quite likely that we don't really know whether we haven't really kind of looked in detail about what the system ends up working.",
            "We suspect that it looks it probably looks at the hair quite a lot because there actually is a piece that doesn't change too much between one view on another.",
            "You can't really look at the eyes because the eyes are covered most of the time, so it's probably a global shape and inherent sort of general appearance."
        ],
        [
            "OK, so this is perhaps a slightly better.",
            "Picture of the kind of vector we get, so we get vectors that are mostly saturated and it's because of this loss function that kind of tries to push things away.",
            "But it turned out to work better than other things which way?",
            "OK, the next."
        ],
        [
            "I want to talk about is how you can use this technique for dimensionality reduction now, not just for classification or verification, but for kind of mapping question.",
            "I guess you're doing your distance learning.",
            "Can you say about how much you can prove you're just using the best passing card you can get without any distance?",
            "So we can't.",
            "We can't actually use any classifier here because the test phase uses a different set of people from the training phase.",
            "So.",
            "You know you can't use a traditional classifier.",
            "You wouldn't be able to try any other classes.",
            "I mean what you might be able to do would be to train the classifier on the on the classes you have, and if it's a multilayer net, like for example, you could drop off the last layer and then just render last layer with the examples you have on the test set.",
            "OK, the pair is so you know there might be a possibility.",
            "We didn't try that.",
            "He said the.",
            "You always take divorced, offended, but if you change cost function, most offended also change, no, because it's it's monotonic, so it's monotonic with distance, so it's always going to be the same.",
            "The same that you push away.",
            "I have the impression that people use Commission pasty, perfectly fits correct examples.",
            "Couldn't you put a margin on your quadratic function that tells you if that's the one for the good fit, but the genuine pair?",
            "Since you have a budget for incorrect there, so you have a bit of freedom to have correct pair with smaller, imagine well below the cost is 0 for Jimmy.",
            "OK so the question.",
            "I guess the suggestion would be to instead of using a quadratic cost for the part that brings the vectors together for your genuine pair would be to kind of have a slack in it so that if I understood correctly so that you don't bring the distance to 0 but you bring it to sort of within some sort of margin.",
            "And that would perhaps be more robust to kind of outliers or something.",
            "More than false positive right?",
            "Maybe you could lose power in technicals positive?",
            "OK, the reason we have more false vision.",
            "False positives, because that's the numbers we give on the.",
            "You know on the RC curve and the reason we give those numbers because application is generally want that you know they so security applications.",
            "For example.",
            "For face verification you get to a door and you have to kind of prove that yourself.",
            "If it's a really highly secure place then you don't want to accept someone on this.",
            "That person really is is who he or she claims.",
            "On the other hand, if it's you know your customer in the bank and you want to verify you know the right person signing your checks, then you don't want to upset too many customers.",
            "So here you want to know more false accept and full speed.",
            "You have set your customers right, so it really depends where you want, but most of the applications you kind of want more.",
            "Full speed Jackson for success.",
            "You said you were a sampling from the all possible pairs.",
            "And yeah, for Maria it's really a subset for each of the categories, so.",
            "Would use the oil you select the forgiven application.",
            "You may select the balance between similar and dissimilar pairs.",
            "You mean in terms of numbers?",
            "How many generating pairs was?",
            "Speak application while you math certain number of imposter pair.",
            "Write access right on the other side.",
            "You may select the ratio between similar and dissimilar pairs right?",
            "So this raises an interesting question.",
            "Which is you know?",
            "How do you set the ratio between the number of genuine or similar pairs and the number of non similar pairs in the training set?",
            "So depending on the application you might want to have different proportions of the two.",
            "It also depends on skin of the diversity of of different.",
            "You know the non genuine pairs you may have.",
            "So of course you know it's much more difficult to train the system to push away the bad guys just because it's a high dimension high dimensional space.",
            "So you gotta push away in other dimensions rather than attracting.",
            "You're attracting a lot of people to the same place so that they can only converge to one place.",
            "Could you say?",
            "Manage is to to include things like tangent distance or small changes.",
            "OK, so so the question is to what extent this kind of managed to do similar things.",
            "Attention distance, which is robustness to kind of small shifts or or kind of distortions.",
            "So I mean because we use this commercial net architecture which has built in the ability to sort of have you know, somewhat invariant representations to shifts and distortions.",
            "The guest, but it's really not sort of quantitative assessment, is that there's actually more invariants then we would get retention distance.",
            "I mean, you could always use non tangent distance by kind of you know explicitly moving faces around and distorting them so you can match them to their register them to the templates.",
            "But that's that's really expensive.",
            "If you have lots of faces.",
            "At the other way to take this might be to say OK will do this.",
            "Some kind of submanifold and we hear the submanifolds right?",
            "Well, that's kind of what this is doing.",
            "I mean, the manifold has any variance to it, so a lot of points in the input that correspond to similar faces will map to the same point on the manifold, and then you move to different points on the manifold.",
            "Possibly so, for example, one thing that tension distance won't do for you is that this will do is that you know this thing will attempt to map different.",
            "You know, poses or the same phase, for example to the same point.",
            "So it's going to try to make this whole manifold very small.",
            "Right Ascension Distance will still have many folders, just, you know, sort of kind of moving points on the manifold, so the points have to kind of move together to match, which is, whereas here they will be at the same place.",
            "Hopefully.",
            "OK, the next thing to go quickly on this is a technique that we called Doctor Lim."
        ],
        [
            "Which means dimensionality reduction by learning invariant mapping.",
            "So it's basically the same idea, except now we use this to kind of get manifolds instead of getting everybody to the same place if we have resemblance between them.",
            "So the basic idea is.",
            "So, so this is kind of a manifold learning technique that directly deals with out of sample.",
            "Cases because you know, it learns a mapping, so it doesn't.",
            "Lanza function that Maps from input space to output space, it doesn't just run a bunch of produce a bunch of output points.",
            "So what this tech is going to be able to do is when you show it a test sample which has nothing to do with any of the training samples, you just feed it to the function.",
            "It would produce an output point.",
            "OK, there's something sort of.",
            "I was playing Vanilla Illini location map or originality can't do.",
            "For those techniques, the other point, the relationship between all the points to be known in advance, so that's kind of a.",
            "It's going to be bad for test samples."
        ],
        [
            "So we're going to use this function again a mapping.",
            "And we're going to have to figure out, you know, a loss function that's going to build this."
        ],
        [
            "Manifold.",
            "And we'd like this function to be invariant to sort of irrelevant changes on the input, like, say, illumination.",
            "OK, so let's say we have images of airplanes from various POV's OK, and what we'd like the machine to produce is.",
            "A vector that corresponds to the point of view of the airplane, but is independent of illumination.",
            "OK, so we have examples like this and we tell the machine those two examples are similar.",
            "Those two examples are similar.",
            "But slightly different, and those two examples are exactly the same.",
            "OK.",
            "But this kind of expensive to say exactly the same or slightly different, so we just have two things the same or not alright, and we cannot rely on the structure of this neighborhood graph to kind of figure it out.",
            "So we're going to use that, for example, this airplane pose extraction thing and the other one would be kind of, you know, mapping all the digits of the end this data set on two kind of low dimensional 2 dimensional space for example.",
            "OK.",
            "So let's see.",
            "So here is."
        ],
        [
            "Recipe we're going to build a neighborhood graph and the neighborhood graph is going to be a graph that's going to link all of our training samples, and we're going to put an arc between two samples if we deem them similar OK.",
            "Exactly the same or relatively similar if you want.",
            "And we are allowed to use prior knowledge for this.",
            "So maybe paranoid coming from how the data was collected or from labeling or from you know whatever.",
            "It's OK to use prior knowledge because this is only for the training set in the test set.",
            "We don't need to provide any information, OK?",
            "So we're going to pick a price family of functions that you know from which we're going to train or mapping neural net visual basic functions.",
            "Whatever optimized parameter of this function so as to minimize loss function that makes the distance between the output vectors of neighbors small and the distance between the vectors of non neighbors large.",
            "OK, some ideas before.",
            "Um?"
        ],
        [
            "OK, so this is what this graph will look like for this airplane example.",
            "So those two guys are similar because they view from similar pose and those two guys are similar again because they viewed from similar posts I've listed.",
            "Guys are not similar because their poses really different now.",
            "OK so this is sort of non transitive similarity, if you want.",
            "And those two guys are from the same viewpoint but different illumination, so really they are the same.",
            "So again, we draw a line between them, and again we do this relationship.",
            "So this is our graph, OK, of course we're going to have many more examples in that for training, but this is an example of a graph similarity graph.",
            "And this is built with prior knowledge because, you know, we have the pose.",
            "Attractive."
        ],
        [
            "For the training samples, so we're going to map this graph if you want to the G function."
        ],
        [
            "And well, hopefully the G function is going to do is map map this to kind of a 2 dimensional space in which for each image we're going to have a point and the points are going to be arranged in such a way that kind of the cover, the space and at similar points are nearby."
        ],
        [
            "So again we use this Siamese architecture for the same reason."
        ],
        [
            "A different loss function that we used before and different slightly different training procedure.",
            "And also we use for this is the square square loss that I talked about before.",
            "So those information is used to determine whether we put a link between 2 images in this similarity graph, whether to decide whether to images are similar or not.",
            "So if the pose are our neighboring poses, then we put on our between them.",
            "If there are non neighboring poses and we don't.",
            "OK. Um?"
        ],
        [
            "So we're going to use this square square Dallas.",
            "OK, so we pay quadratically for making the output vectors different for two images that we deemed similar that our neighbors in the graph, we pay again quadratically for making the distance small for images that are.",
            "Deemed different images that don't have an arc between them in this similarity graph.",
            "OK, and we have a margin here and the margin does nothing more than sort of setting the scale for the entire."
        ],
        [
            "Thing we get so we can do some sort of spring analogy.",
            "Mechanical spring analogy so you know similar points in the output space are linked with the spring and similar dissimilar points in the output space.",
            "You know have a spring and you have to compress the spring to bring that point closer to the inside of this, OK?"
        ],
        [
            "And so it comes down to code and finding the minimum energy solution of this, but also producing those vectors by the thing.",
            "So we're going to."
        ],
        [
            "To do this on a list.",
            "So if you take EM list so you take.",
            "3000 samples of Forza Nines and.",
            "You train this system and the way you determine whether 2 images are neighbors or not is very stupid here.",
            "We wouldn't need to do this, we're just looking at your neighbors are the five nearest neighbors in Euclidean distance in pixel space?",
            "OK?",
            "So it's very stupid way of doing it, but it's a sanity check.",
            "So what do we get if we do that?",
            "If we if we do this so no prior knowledge here.",
            "Basically we use Euclidean distance to drive the drive, the system and we don't get anything particularly interesting other than the fact that you know you have some sort of this continuous manifold with sort of nicely continuously changing shapes as you move around.",
            "OK, and these are test samples.",
            "OK, so these are test samples that we run through the through the function and sort of figure out where they end up in this 2 dimensional output space.",
            "OK."
        ],
        [
            "And the function again that we use for this is a commercial network.",
            "Very small one."
        ],
        [
            "Um?"
        ],
        [
            "OK, now what about mapping something that's invented shift?",
            "So we're going to do is we're going to augment or training set with samples that are shifted versions of the original samples, so we can shift by minus 6 -- 3 + 3 or 6 pixels.",
            "So now we have all kinds of examples are shifted from each other and we apply the same algorithm and we'll get our fair."
        ],
        [
            "Processors and those are all the things that have been shifted by 6 pixels or things I visited by three pixels to the left or the original ones are the ones three pixels to the right and six weeks after the right.",
            "OK, that's so good, right?",
            "But you know, it's expected.",
            "So."
        ],
        [
            "So the next thing, so if you apply adelita that to that set using Euclidean distance again, you get nothing.",
            "Basically nothing interesting is very strange actually.",
            "Only.",
            "Only combats the collapse through very very weak constraint absolutely variance.",
            "Right, he's doing.",
            "He's going in cheating on you by collapsing almost all the data and then putting a big tail to keep the covariance constraint right?",
            "So for those of you who don't know Sam, you know he's one of the first author of the paper, so he knows what he's talking about and.",
            "Thank you very much.",
            "So and so that I mean that's the problem, which is that you know, since since it's using neighbors, you know the two shifted versions of the digits are very far away in Euclidean distance, so they can't possibly be neighbors.",
            "So here's the next thing we do.",
            "We put a little bit of prior knowledge in the in the learning algorithm.",
            "So we we.",
            "We tell the system will not only your neighbors are Euclidean nearest neighbors, but they are also the shifted version of your Euclidean nearest neighbors under shifted versions of yourself.",
            "OK, so in the graph we put an arc in the graph.",
            "If two images are similar in Euclidean distance in the register position, or if.",
            "If there are shifted version of your neighbors.",
            "OK, so that's using prior knowledge.",
            "OK, we have to know how the data was cooked to do this, but it's only on the training set and.",
            "And we run the algorithm again."
        ],
        [
            "We get this this so now we don't get those five clusters anymore.",
            "Everything is 1 big uniform, uniformly kind of filled.",
            "Surface.",
            "And you know, again, you get this nice sort of continuous variation of shape when you sort of move around the manifold.",
            "Here this surface.",
            "But what you can see is that.",
            "You can have widely varying positions, so for example here this guys is registered to the left and this guys reached out to the right, but they actually are neighbors, so the function map them to the same point because it figures well they have the same shape.",
            "Of course they have different there a different position, but they're the same shape.",
            "You know they were supposed to be neighbors in the training set, so these are actually test samples.",
            "These are not training samples.",
            "OK, that's a test set.",
            "And if you zoom in on kind of a little Patch here, you get you know all those lines which are pretty much all the same, very similar shapes, But at what is sort of different differing positions within the within the frame.",
            "So that's kind of example of learning really an invariant.",
            "You know we're mapping or dimensional manifold with invariant properties.",
            "OK, it's only digits."
        ],
        [
            "So let's go with the airplanes.",
            "So here is a different problem that we're trying to do.",
            "Again is.",
            "So we have 927 images of airplanes under 6 different combinations, 18 different estimates, or every 20 degrees this way.",
            "And 99 different elevations from 35 degrees to 70 degrees every 5 degrees.",
            "So we take out of this 900 or so images we take six 660 as training samples for this 660 training samples.",
            "We, the relationship the neighborhood relationship is such that one image and all its identical images, identical viewpoint, different limitations are neighbors.",
            "And the immediately.",
            "A consecutive if you want neighboring poses also are your neighbors, and so are the different dimensions.",
            "Illuminated versions of this.",
            "OK, so each image has a lot of neighbors in fact.",
            "OK 'cause it's got.",
            "Different locations in different poses.",
            "But only the neighboring poses.",
            "Um, actually it's the 1st and 2nd neighbor in azimuth and the first neighbor in elevation and all the illumination.",
            "OK, so it's a little more neighbors.",
            "I just told you.",
            "And we're not going to use a convolutional net here.",
            "We're going to use a fully connected neural net, and the reason is because there is really no shift in variance property.",
            "We need to learn here because all the all the planes are really very well registered, so we need to learn here is invariant to illumination only OK."
        ],
        [
            "So these are some of the examples from other cat."
        ],
        [
            "Stories that we could."
        ],
        [
            "OK, so you probably do this, and again you run into trouble because.",
            "So these are kind of two slices of Eli, so it comes up with this kind of fold.",
            "So this is viewed from the top and this is you from the side.",
            "And that's because it gives a huge distance.",
            "You know, using clean distance to measure to build the neighborhood graph with Italy and with no prior knowledge and so.",
            "So here you know, there's a huge distance between those two illuminations, because these are very dark, so all the pixels from this are very different for all the pixels from that.",
            "OK, on the other hand.",
            "Even dissimilar poses are fairly similar, so this completely dominates the entire distance measure.",
            "OK, this is kind of an unfair comparison.",
            "I mean, I sort of keep bashing Eli, but it's great, but.",
            "It's going to comparison because we could also put a little bit of prior knowledge into Italy.",
            "The main problem is then you know how do you kind of map new examples and how do you find relationships."
        ],
        [
            "So this is the result we get with this Doctor Lim algorithm.",
            "And these are different views of the 3D manifold we get.",
            "So we decided period of the dimension.",
            "OK, we say it's going to be 3D.",
            "We get this kind of.",
            "Cylinder.",
            "So this is viewed from the top.",
            "And.",
            "And along the periphery of the cylinder, basically you get azimuth.",
            "OK, so as you move around you get views of the airplanes corresponding to different azimuth.",
            "The so this guy is this adamant and this guys azimuth.",
            "Going to look at this cylinder going to rotate it this way.",
            "OK so it's kind of standing this way and you know there's a hole here.",
            "It's kind of like this.",
            "And if you.",
            "Um?",
            "Actually I screwed up.",
            "I think it's backwards.",
            "Yeah, no, it's on the side here.",
            "OK, so the whole goes like this so it's kind of like this, right?",
            "And so as you move along the main axis, basically what changes is the elevation.",
            "OK, so this guy is viewed from, you know, from a shadow angle.",
            "And this guy is right from the top, more or less.",
            "And then if you zoom in on kind of a, you see kind of little sort of clusters of points here, and those customer points generally have six points in them.",
            "123456 and those correspond to all the images at one azimuth, one elevation, and different illuminations.",
            "OK, so you get what you want."
        ],
        [
            "But you get a function that you know you give it an image, an airplane, and it gives you immediately 2 dimensional or three dimensional point vector.",
            "That is a point on this manifold indicating the pose.",
            "So that's an interesting thing.",
            "These are the, so it was a neural net with 20 hidden units and these are the weights of the hidden unit.",
            "So the amazing thing is that you plug a image of an airplane and you compute the dot product between this airplane image and those 20 vectors.",
            "And presents with sigmoid computer linear combination of those and it goes the pose would have thought so those are kind of really funny.",
            "Sort of little.",
            "You know circular Gabor filter like things.",
            "Of course you plug anything else on an airplane is not going to work.",
            "You have to train it on other examples, but it's kind of cute.",
            "OK, so after I've shown you all those pretty pictures, I'm going to stop here after I've shown you all those pretty pictures.",
            "I'm going to tell you that this last technique here is basically useless.",
            "I mean, you know you can't use it for anything useful, but it produces really nice pictures and it gets you papers published, CPR.",
            "It's really cute.",
            "Know people are really impressed by nice pictures, even though there's no user can think of for this.",
            "Maybe someone will have some ideas how you can use this.",
            "You know, for for visualization purposes, basically it's good for visualization like sneaky or things like that, where so assignment is really beautiful.",
            "Charge of like authors of NIPS where people cluster and depending on how much the site, each other and it's really cool.",
            "So it's good for for human consumption.",
            "But if you want a lot of monitoring applications, well, absolutely.",
            "It's just people who are looking at really ridiculous 1970s graphs of like RPM and pressure and stuff.",
            "That's right, they really wanted to know, right?",
            "Is this huge vector of measurements that the plant produced in the last hour, similar or not similar to right?",
            "So, for example, some of the you know that's applications ability or for you know people analyzing recorded data from neurons and stuff like that.",
            "And yeah, this is getting huge high dimensional stuff and you can figure out what it does or are.",
            "Some of my friends then why you used it for.",
            "Figuring out the robustness of function of some.",
            "Protein regulation mechanisms an you know you figure out that there's only a few things that really matter in there, and the manifold is very low dimensional space because their regulatory regulatory mechanism, so it's visualization is really.",
            "It's very important but but there is one thing I want to dispel, which is that these techniques cannot be used for anything like recognition.",
            "I mean, I don't see.",
            "This question here.",
            "If you made, that may make it difficult for ratification.",
            "Is that your similarities only binary, very similar or not?",
            "Extended OK. Function.",
            "Distance matrix right?",
            "OK, so there is no, there's nothing that prevents you from training those actually dealing with binary values is what makes it complicated, because if you do have exact similarity measures between pairs of samples then you can just train this.",
            "Right?",
            "How do you define?",
            "Well, so you don't use neighborhoods anymore, but what you do is that you have a value for the distance between two things.",
            "Then you train the output of this this Siamese system as a regressor.",
            "You just trying to produce the answer you want.",
            "OK, so it's just a regression problem.",
            "You train the thing to produce the right answer and that's it.",
            "You don't have it.",
            "You don't have to come up with a smart loss function, you know, just be squared.",
            "Error will do fine, so it's actually a much simpler problem.",
            "But now it puts the onus on whoever labels the data now, because now you have to come up with.",
            "Actual distances between every pair of object, or you know a subset of pairs of objects.",
            "So in fact you know what makes this complicated is affected.",
            "You can reduce the burden on the labeler, and you can use.",
            "Kind of, you know, very sort of week prior knowledge.",
            "Like you know they are sort of the same or not the same, and then you have to come up with those kind of energy functions.",
            "That sort of you know.",
            "So for example, for this problem we tried several energy functions and some of them would curdle.",
            "You know, I don't know if any of you sort of cook, but you know you want to make a.",
            "Come on guys or whatever, right?",
            "And you sort of build it too much and you know you get sort of Trump's right?",
            "So things happen with loss functions that are not well designed.",
            "You get console.",
            "You don't get this smooth uniform surface, so this function is defined to get kind of uniform density of points anywhere on the surface.",
            "Different functions will give you different results.",
            "We are always the question.",
            "Maybe we can talk.",
            "You can get us or the question offline with thank you.",
            "So we have a."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With the topic of this this workshop and because of interesting invariants is going to be similarity metrics within variances.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk mostly about some work I've been doing with two of my students where you had cell and sutra.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first thing I want to say is if you want to learn.",
                    "label": 0
                },
                {
                    "sent": "A similarity similarity metrics.",
                    "label": 0
                },
                {
                    "sent": "We need to train some sort of parameterized function.",
                    "label": 0
                },
                {
                    "sent": "In such a way that when we map input vectors to two output vectors within the mapped IP vector space, we can compare examples using a very simple metrics that corresponds to kind of the semantic distance between objects, so that objects that are semantically similar will have a small distance in that space, and objects that are semantically different will have a large distance in that space.",
                    "label": 0
                },
                {
                    "sent": "So we're going to take a parameterized mapping GW of X, where W is the parameter we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to learn such that the L1 distance between the map mapping of two examples reflect kind of the semantic distance.",
                    "label": 0
                },
                {
                    "sent": "We could use other distances in our one, but we use our one in most of those cases, or L2 sometimes.",
                    "label": 0
                },
                {
                    "sent": "So the advantage of doing this.",
                    "label": 0
                },
                {
                    "sent": "The reason why we got interested in this, I started working on things like this.",
                    "label": 0
                },
                {
                    "sent": "Like 1230 years ago but.",
                    "label": 0
                },
                {
                    "sent": "Sort of didn't do much about it in the meantime, until about two years ago, and the reason we started working on this again recently is because we're interested in solving the problem of being able to classify objects into many many categories, including categories for which we don't have training samples at the time of learning.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of that.",
                    "label": 0
                },
                {
                    "sent": "Seems kind of a problem right?",
                    "label": 0
                },
                {
                    "sent": "Because how are you going to recognize categories?",
                    "label": 0
                },
                {
                    "sent": "We don't have any training samples, but if you have a trainable similarity metric.",
                    "label": 0
                },
                {
                    "sent": "As soon as you get an example from a from a new category, then you can immediately use it as a template for other.",
                    "label": 0
                },
                {
                    "sent": "Other samples, so nonparametric methods are appropriate for kind of large.",
                    "label": 0
                },
                {
                    "sent": "Large class of large class problems.",
                    "label": 0
                },
                {
                    "sent": "A large number of classes problems but but of course they don't work so well.",
                    "label": 0
                },
                {
                    "sent": "So how do you kind of make parametric methods?",
                    "label": 0
                },
                {
                    "sent": "Used in a way that you know, nonparametric methods could be used.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of the things we want, techniques we we came up with way back when is something called Siamese architectures.",
                    "label": 0
                },
                {
                    "sent": "So this is the idea of sort of coupling.",
                    "label": 0
                },
                {
                    "sent": "Two of those mappings.",
                    "label": 0
                },
                {
                    "sent": "So you take 2 identical Siamese Twins that are joined at the head, which is what we call this time is Siamese architecture.",
                    "label": 0
                },
                {
                    "sent": "And you know you plug 2 images in it and on the output you compare the two and that gives you kind of a. Resemblance or dissimilarity between the two the two objects?",
                    "label": 0
                },
                {
                    "sent": "Um, so here you could apply this to things like face recognition, where you might be asked to recognize people and the number of categories might be in the 10s of thousands, and Furthermore, at the time the time you turn this function you don't have all the people that you're supposed to recognize.",
                    "label": 1
                },
                {
                    "sent": "You sort of giving them after after the fact.",
                    "label": 0
                },
                {
                    "sent": "So, you know, feed one of the images and then sort of compare that image to all the ones that are in your in your Gallery and put the some sort of comparison between the two to figure out.",
                    "label": 0
                },
                {
                    "sent": "Which person this is?",
                    "label": 0
                },
                {
                    "sent": "So we could do.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is there some sort of minimization over a set?",
                    "label": 0
                },
                {
                    "sent": "OK look for the person in the set that minimizes the output of your system, the energy if you want to call it this way for those of you who were met tutorial Atnip, since it's kind of derived from this idea of energy based systems also.",
                    "label": 0
                },
                {
                    "sent": "OK, so how are we going to train this?",
                    "label": 0
                },
                {
                    "sent": "It's very simple idea.",
                    "label": 0
                },
                {
                    "sent": "You take 2 examples of the same person.",
                    "label": 0
                },
                {
                    "sent": "For example, if you want to face recognition and you train the machine to produce a small distance.",
                    "label": 0
                },
                {
                    "sent": "Four pairs of semantically similar inputs, and then if you plug 2 examples of different people, you train the machine to produce large distances.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a little different from the traditional learning in which you have, you know, a sample and label.",
                    "label": 0
                },
                {
                    "sent": "Here you have two samples and you also have a label that indicates whether they are the same or not, or whether the similar or not.",
                    "label": 0
                },
                {
                    "sent": "Some similarity between them.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the advantage of course over you know, traditional discriminative learning is that.",
                    "label": 0
                },
                {
                    "sent": "Again, you don't have to have all the categories at training time, because you can always add a new sample to category and hopefully your distance measure will have generalized to new samples.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you could view this also as a dimensionality reduction technique, because of course the output representation.",
                    "label": 0
                },
                {
                    "sent": "Most of the time we sort of somewhat smaller than the inputs.",
                    "label": 0
                },
                {
                    "sent": "If you start with an image you know you could map them to space or say, 50 or 100 dimensions, and so you can compare that with dimensionality reduction techniques like PCA.",
                    "label": 0
                },
                {
                    "sent": "So PCA is used, was used in the vision community, discovered PCA with face recognition.",
                    "label": 0
                },
                {
                    "sent": "It's kind of weird, but that's what happened.",
                    "label": 0
                },
                {
                    "sent": "And you could use something that's a little more appropriate for this LDA, which takes into account the labels, but that doesn't quite work.",
                    "label": 0
                },
                {
                    "sent": "And all of these things are a linear, so the problem linear mappings is that it's very difficult to kind of build invariants.",
                    "label": 0
                },
                {
                    "sent": "Complicated invariances?",
                    "label": 0
                },
                {
                    "sent": "What if the illumination changes?",
                    "label": 0
                },
                {
                    "sent": "What if the person turns the head?",
                    "label": 0
                },
                {
                    "sent": "What if you know person wears glasses?",
                    "label": 0
                },
                {
                    "sent": "It's very unlikely that with a linear mapping to be able to take into account all those variabilities and still map them to the same vector at the end.",
                    "label": 0
                },
                {
                    "sent": "You also have kernel PCA and kernel today, but.",
                    "label": 0
                },
                {
                    "sent": "Again, unless you build the invariants in the kernel, you're not going to get really good interpolation and.",
                    "label": 0
                },
                {
                    "sent": "Then you have things like alien and yes, local linear embedding and multidimensional scaling, and the problem with these guys is that they're great for visualization, but they don't actually give you a mapping that just, you know, give them a bunch of points and they give you a bunch of points out, but you don't have a mapping between points on the output point, so if you have new points, you have to kind of redo the whole thing again.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, the new points you need to know the actual relationship between the new points on the old points to be able to do the thing again, so you can't really use this with the test set.",
                    "label": 0
                },
                {
                    "sent": "Yes, revenge, oh and his group and various other people have proposed have proposed extensions of those things.",
                    "label": 0
                },
                {
                    "sent": "For situations where you want to do out of sample mapping tool dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "But it's basically consistent.",
                    "label": 0
                },
                {
                    "sent": "Running Italy and then training parameterized function to kind of learn this mapping independently.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Positive would be to sort of built the invariants into the distance measure, so using things like attention distance methods or elastic matching.",
                    "label": 0
                },
                {
                    "sent": "Or use your favorite feature set that's appropriate for the object you're trying to do, but it's kind of more fun as kind of machine learning people to try to run the whole thing right, and that's been sort of my obsession for last 20 years, so.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we're going to try it.",
                    "label": 0
                },
                {
                    "sent": "So this first thing we tried, you know.",
                    "label": 0
                },
                {
                    "sent": "Back in I had it 9090, three, 92 and now it's 94.",
                    "label": 0
                },
                {
                    "sent": "I'm not even sure which year is correct, OK?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over 94 so so this is what we use way back for doing signature verification and it worked pretty well.",
                    "label": 0
                },
                {
                    "sent": "So we use the Siamese architecture and the G function was a A1 dimensional convolutional neural net.",
                    "label": 0
                },
                {
                    "sent": "Time today neural net which which we could feed the sequence of the pen trajectory and then we could compare the two.",
                    "label": 0
                },
                {
                    "sent": "The reason we're doing this, it turns out signature verification is actually very fairly easy.",
                    "label": 0
                },
                {
                    "sent": "You don't need all that machinery.",
                    "label": 0
                },
                {
                    "sent": "The reason we did this is because we could map the.",
                    "label": 0
                },
                {
                    "sent": "Those things into a very, very compact vector that represented you know everything there is to know about the signature, and then this this compact vector with less than 80 variables could actually be stored in the magnetic magnetic strip of a credit card.",
                    "label": 0
                },
                {
                    "sent": "So that was the only reason why we did this.",
                    "label": 0
                },
                {
                    "sent": "It was never really used or diploid.",
                    "label": 0
                },
                {
                    "sent": "It worked pretty.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Though.",
                    "label": 0
                },
                {
                    "sent": "The problem with this is.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We work pretty well.",
                    "label": 0
                },
                {
                    "sent": "We actually hadn't really figured out with the correct loss function was it was very unstable.",
                    "label": 0
                },
                {
                    "sent": "We had to adjust with other parameters so that didn't come until recently.",
                    "label": 0
                },
                {
                    "sent": "So again, we're going to do now.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk about faces.",
                    "label": 0
                },
                {
                    "sent": "Some more recent work.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Again, we're going to do is change the parameters of the function such that you know we get a small distance for identical faces and large distance, otherwise OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what approach to this, which I'm sure Sam will tell you about, is to sort of take the probabilistic view so you can view why.",
                    "label": 0
                },
                {
                    "sent": "So here why we did the label associated with the second phase, OK?",
                    "label": 0
                },
                {
                    "sent": "And you know, you might have multiple faces that have the same label, so you might have to do something complicated.",
                    "label": 0
                },
                {
                    "sent": "But what you need to do is if you want to approach the problem probabilistic way, you could view the energy's kind of the log of a probability and then to get a conditional probability of the label given the input you take it to the minus the energy and normalized by the sum of each of the minus energy.",
                    "label": 0
                },
                {
                    "sent": "Overall possible labels.",
                    "label": 0
                },
                {
                    "sent": "OK, so you get a normalized conditional distribution over.",
                    "label": 0
                },
                {
                    "sent": "Over labels by.",
                    "label": 0
                },
                {
                    "sent": "By doing this normalization.",
                    "label": 0
                },
                {
                    "sent": "So training the system to maximize likelihood consist in.",
                    "label": 0
                },
                {
                    "sent": "So this would be the conditional likelihood, which would be the product of all training samples of this conditional likelihood there OK?",
                    "label": 0
                },
                {
                    "sent": "And you want to maximize this with respect to the parameters of this energy function in the W you take minus log of that, and that gives you a loss function that you can minimize.",
                    "label": 0
                },
                {
                    "sent": "OK, so you get this loss function and of course your face again with the problem that this is the energy produced by the the current training pair.",
                    "label": 0
                },
                {
                    "sent": "If you want so you have a pair here and here you have.",
                    "label": 0
                },
                {
                    "sent": "You have one sample here and here.",
                    "label": 0
                },
                {
                    "sent": "The second sample, or here I sort of represented by the label, the second sample.",
                    "label": 0
                },
                {
                    "sent": "But really you want to feed the second sample.",
                    "label": 0
                },
                {
                    "sent": "Here the parent here, whether they are identical or not.",
                    "label": 0
                },
                {
                    "sent": "And here you have to sum over all all other images.",
                    "label": 0
                },
                {
                    "sent": "All images in your training Gallery.",
                    "label": 0
                },
                {
                    "sent": "OK, if you really want to be correct about this.",
                    "label": 0
                },
                {
                    "sent": "That's really expensive, right?",
                    "label": 0
                },
                {
                    "sent": "Because for each training sample you have to go through the entire Gallery and compute the Z to the matters.",
                    "label": 0
                },
                {
                    "sent": "The energy and sort of figure out where the value of this thing is.",
                    "label": 0
                },
                {
                    "sent": "So of course you're going to cut corners.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the total solution is sampling.",
                    "label": 0
                },
                {
                    "sent": "But then again, sampling is the solution to everything in Toronto.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So so salmon.",
                    "label": 0
                },
                {
                    "sent": "Jeff and some of the students, had you know, a few ideas some time ago, one for doing embedding, another one for doing sort of similarity metrics, network component analysis.",
                    "label": 0
                },
                {
                    "sent": "And you know, I see these guys all the time and it turns out.",
                    "label": 0
                },
                {
                    "sent": "I figured out this, but not really in because I was addressing something else and then we came up with our paper and realize you know Sam had done something similar before and so I have to apologize to him in public for not citing this paper in mind, even though we talk to each other all the time.",
                    "label": 0
                },
                {
                    "sent": "So this comes with things happen even between friends.",
                    "label": 0
                },
                {
                    "sent": "But of course, you know we had to come up with a different solution.",
                    "label": 0
                },
                {
                    "sent": "Since we don't, we didn't know about about these guys and so our solution is New York solution.",
                    "label": 0
                },
                {
                    "sent": "Just go about the worst offenders, go after the worst offenders and forget about all the other guys.",
                    "label": 0
                },
                {
                    "sent": "So that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It sort of ended up with a different loss function.",
                    "label": 0
                },
                {
                    "sent": "And the idea is not to pull up on the energies of all the answers.",
                    "label": 0
                },
                {
                    "sent": "All the bad answers, but just pull up on the energy of the most offending incorrect answers.",
                    "label": 0
                },
                {
                    "sent": "We just pick the you know the samples in your.",
                    "label": 0
                },
                {
                    "sent": "Set approximately you don't actually have to be really accurate about this that are that really other most offending.",
                    "label": 0
                },
                {
                    "sent": "So if you show a pair of two images of the same person that you look through a subset for a small subset doesn't have to be the full set for one that.",
                    "label": 0
                },
                {
                    "sent": "Has a low distance to one of your samples in there, but happens to be an image of a different person and then you pull that.",
                    "label": 0
                },
                {
                    "sent": "You pull that guy away.",
                    "label": 0
                },
                {
                    "sent": "OK, you make the energy larger for that guy.",
                    "label": 0
                },
                {
                    "sent": "And or maybe a few of those guys, not just one, but if you are in fact we use just one.",
                    "label": 0
                },
                {
                    "sent": "So this is notion of most offending or worst, offending incorrect answer.",
                    "label": 1
                },
                {
                    "sent": "Do you have any statistics that show that the exponential decrease wins out, so that effect only one offenders is 1 years?",
                    "label": 0
                },
                {
                    "sent": "Take does it?",
                    "label": 0
                },
                {
                    "sent": "Typically, what happens is that as you go further, many, many more examples that are bigger than distance.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, so it really increased exponentially.",
                    "label": 0
                },
                {
                    "sent": "What so I don't have?",
                    "label": 0
                },
                {
                    "sent": "Which exponential?",
                    "label": 0
                },
                {
                    "sent": "Issue.",
                    "label": 0
                },
                {
                    "sent": "Numbers of neighbors know, but you only taking one, so you know only taking the worst one and pulling it away, right?",
                    "label": 0
                },
                {
                    "sent": "That actually shows that only taking one neighbor is enough.",
                    "label": 0
                },
                {
                    "sent": "Well, the only evidence we have is the results you can get with this.",
                    "label": 0
                },
                {
                    "sent": "Compared to results you can get without a loss functions and have to admit that comparison you know face to face comparison with different loss functions in terms of of you know how fast does it converge with a given number of comparable amount of computation hasn't been done, so it's hard to say.",
                    "label": 0
                },
                {
                    "sent": "That's right, I mean, my guess is that you know if the function is linear, you have a perception like.",
                    "label": 0
                },
                {
                    "sent": "I mean, depending on the function you use, if user hinge for example, you have a perception like convergence procedure of some kind, But if.",
                    "label": 0
                },
                {
                    "sent": "In your case, you know it's much more difficult and depending on the loss function that we actually use, I doubt there is good.",
                    "label": 0
                },
                {
                    "sent": "Way of proving it.",
                    "label": 0
                },
                {
                    "sent": "OK, so in any way.",
                    "label": 0
                },
                {
                    "sent": "In any case, we're going to have two different loss functions or two different parts of the loss function.",
                    "label": 0
                },
                {
                    "sent": "One that is the way we want to minimize when we have to.",
                    "label": 0
                },
                {
                    "sent": "22 samples that are semantically similar and one that contains the.",
                    "label": 0
                },
                {
                    "sent": "The energy for the pair that.",
                    "label": 0
                },
                {
                    "sent": "Is not semantically similar.",
                    "label": 0
                },
                {
                    "sent": "You know comparison with the most of any incorrect answer.",
                    "label": 0
                },
                {
                    "sent": "We're going to make that energy large, and so the last.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It has to be a decreasing function of this, so we're going to do is, you know, compute the L1 distance.",
                    "label": 0
                },
                {
                    "sent": "Here we have plugged their own distance into these two.",
                    "label": 0
                },
                {
                    "sent": "These two half of the loss function.",
                    "label": 0
                },
                {
                    "sent": "So that's the first half, so we're going to pay quadratically for making the L1 distance between the two vectors large.",
                    "label": 1
                },
                {
                    "sent": "OK, so this loss function is going to try to make the distance zero.",
                    "label": 0
                },
                {
                    "sent": "Essentially, four pairs of identical faces, or the same person.",
                    "label": 0
                },
                {
                    "sent": "An and this loss function is an exponentially decaying function.",
                    "label": 0
                },
                {
                    "sent": "And is going to try to make the distance for different people between different people as large as possible, but the force is exponentially decaying, so you know after awhile it's not going to push much.",
                    "label": 0
                },
                {
                    "sent": "And because of this choice of horse offending, incorrect answer, the one that are far away will actually not going to be pushed up, pushed out, pushed out at all OK, particularly if you combine that with a little bit of regularization.",
                    "label": 0
                },
                {
                    "sent": "You know things can go very far.",
                    "label": 0
                },
                {
                    "sent": "There are several types of functions like this that we've thought about going to show you another one after later in the talk for a different different use of this.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, the important thing is that you want this function to really kind of not push as hard on patterns that are far away from each other, and that's one mistake we did with the old work with signature verification.",
                    "label": 0
                },
                {
                    "sent": "We didn't have the right loss function for that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you want to be sort of more hard nosed about this, one possibility is to use one of those.",
                    "label": 0
                },
                {
                    "sent": "What I called the square square loss, so you pay quadratically for making the energy the first time energy large, and you pay quadratically also for making the second term small and the.",
                    "label": 0
                },
                {
                    "sent": "Loss function vanish is above a certain margin, so here you can enforcing some sort of margin between the you know your friends and your enemies.",
                    "label": 0
                },
                {
                    "sent": "But we're going to use the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Square exponential loss.",
                    "label": 0
                },
                {
                    "sent": "With you know you need to kind of hook up some coefficients here so that things work OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the cooking up the coefficient will determine.",
                    "label": 0
                },
                {
                    "sent": "Essentially it's not really crucial, it will determine the the tradeoff between those two functions if the first time is too important, then everything is going to lapse.",
                    "label": 0
                },
                {
                    "sent": "If it's not going to collapse, but everything is going to be kind of really tightly.",
                    "label": 0
                },
                {
                    "sent": "Cost around a point.",
                    "label": 0
                },
                {
                    "sent": "If you increase the importance of the exponential decay then things are going to expand.",
                    "label": 0
                },
                {
                    "sent": "So you want to.",
                    "label": 0
                },
                {
                    "sent": "It's going to set the scale of the whole of the whole set of vectors.",
                    "label": 0
                },
                {
                    "sent": "So we tried doing this on the.",
                    "label": 0
                },
                {
                    "sent": "On the AT&T Olivetti.",
                    "label": 0
                },
                {
                    "sent": "Data set, so this has 40 subjects but 10 images per subject.",
                    "label": 0
                },
                {
                    "sent": "And with some you know a moderate degree of variation, imposing lighting, expression and everything.",
                    "label": 0
                },
                {
                    "sent": "So we used images from 35 subjects for training and then use the other five for test.",
                    "label": 0
                },
                {
                    "sent": "So it's different subjects altogether.",
                    "label": 0
                },
                {
                    "sent": "Do figure out the test.",
                    "label": 0
                },
                {
                    "sent": "So we kind of sub sampled the set of all possible so you know I'm making fraternal, but I'm using this inside, so we use the same kind of trick of sampling to kind of sample the set of possible pairs, because there's so many pairs, right?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So in fact we use something like 12,000 impostor pairs.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also use the fair database, which is a lot more challenging 'cause the faces of more more variation of pose in it.",
                    "label": 0
                },
                {
                    "sent": "But we use that only for training.",
                    "label": 0
                },
                {
                    "sent": "We didn't actually test on this.",
                    "label": 0
                },
                {
                    "sent": "We tested on this other data set called the Purdue data set and this one is in.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Credibly challenging's got quite a lot of subjects, and it's got images 26 images of each subject taken into two sessions several weeks apart apart and people were sort of making extreme sort of faces and extreme changes of kind of, you know.",
                    "label": 0
                },
                {
                    "sent": "Sunglasses and you know.",
                    "label": 0
                },
                {
                    "sent": "So it's very challenging data set.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More examples of that of that data set.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first thing we do is we.",
                    "label": 0
                },
                {
                    "sent": "Well, she cropped and said samples and center those images, but this was done automatically with a very simple system, so we didn't do this by hand.",
                    "label": 0
                },
                {
                    "sent": "OK, and the way this was done was by.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, essentially kind of finding a.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "We sort of generated a sort of average face if you want and sort of find the best match to find a good center and then sort of crap real thing around it.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of jitter in this decentering, so the face kind of vary quite a bit in the imposition.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we plug a convolutional net on it.",
                    "label": 0
                },
                {
                    "sent": "Not surprisingly.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is kind of a fairly large convention that's sort of similar to what we use for handwriting or image recognition is just slightly higher resolution.",
                    "label": 0
                },
                {
                    "sent": "A little more feature Maps, so it's got six layers like most of the commercial less we use and go into the details of how this is built.",
                    "label": 0
                },
                {
                    "sent": "You can read.",
                    "label": 0
                },
                {
                    "sent": "Some papers going back 15 years.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is sort of what we get on the output when we plug some images to it.",
                    "label": 0
                },
                {
                    "sent": "So I apologize little hard to read, but this is kind of the internal state of the network for the corresponding image and what you have to look at is kind of the output vector, which is this 50 dimensional output vector here.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a bit of a bit of a challenge if you're far away, but.",
                    "label": 0
                },
                {
                    "sent": "But the thing that you can see is that so you see a bunch of white white areas and black dots, right?",
                    "label": 0
                },
                {
                    "sent": "And if you plug a very different image of the same person here you see another array of black dots on white areas and they pretty much at the same place.",
                    "label": 0
                },
                {
                    "sent": "OK is it goes to black dots here to black dots white, white, black black?",
                    "label": 0
                },
                {
                    "sent": "You know with a few differences, so black indicates press one and white minus one or the other one around there, not all saturated from some are great, but you can't really see them.",
                    "label": 0
                },
                {
                    "sent": "So again, here you get 2 vectors that are similar on the outputs when you give images 2 images, the same person.",
                    "label": 0
                },
                {
                    "sent": "And here are two images of different person and you see the black dots are really not at the same place at all.",
                    "label": 0
                },
                {
                    "sent": "So this guy is back out here, not here and you know it's got a completely different configuration there.",
                    "label": 0
                },
                {
                    "sent": "So somehow these things sort of captures.",
                    "label": 0
                },
                {
                    "sent": "You know some interesting difference between the two things.",
                    "label": 0
                },
                {
                    "sent": "Same on the other side.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you want to apply this to kind of new faces you have, you have to sort of build a model.",
                    "label": 0
                },
                {
                    "sent": "You know, given a single or just a few examples of a person, how do you compare that person to the other one?",
                    "label": 0
                },
                {
                    "sent": "And maybe a very simple Gaussian model in this output space?",
                    "label": 0
                },
                {
                    "sent": "OK, so just gather whatever genuine examples you have and compute kind of regularised Gaussian model based on the samples of that persons you have and then when you person comes in you need to compare just compute the likelihood of that person under this distribution.",
                    "label": 0
                },
                {
                    "sent": "And you know threshold or you know, don't do the exponential and it's like a quadratic distance threshold.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's interesting.",
                    "label": 0
                },
                {
                    "sent": "OK, so on the.",
                    "label": 0
                },
                {
                    "sent": "Produce this at.",
                    "label": 0
                },
                {
                    "sent": "I apologize for this doubling of the characters here.",
                    "label": 0
                },
                {
                    "sent": "I don't know what happened.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The producer said we get something like 7% tuna, half percent false accept.",
                    "label": 0
                },
                {
                    "sent": "For 14% false rejects.",
                    "label": 0
                },
                {
                    "sent": "Which is not, you know, doesn't look too good, but it's really very challenging data set.",
                    "label": 0
                },
                {
                    "sent": "Not many people get good results on that one.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are some of some of the examples.",
                    "label": 0
                },
                {
                    "sent": "Of.",
                    "label": 0
                },
                {
                    "sent": "Pairs, so for example, you know those two have a fairly low energy on the output.",
                    "label": 0
                },
                {
                    "sent": "There actually images from the from the same person, so those are correctly identified as genuine pairs of pairs with images of the same person.",
                    "label": 0
                },
                {
                    "sent": "Despite the wide variation of lighting conditions, and.",
                    "label": 0
                },
                {
                    "sent": "And you know.",
                    "label": 0
                },
                {
                    "sent": "Sunglasses or glasses or no glasses and covering the mouth.",
                    "label": 0
                },
                {
                    "sent": "These are correctly identified impostors.",
                    "label": 0
                },
                {
                    "sent": "And these are mistakes.",
                    "label": 0
                },
                {
                    "sent": "So these two images are identified as being from different persons.",
                    "label": 0
                },
                {
                    "sent": "But this is actually the same and these from the same person that they're actually different.",
                    "label": 0
                },
                {
                    "sent": "So it's quite likely that we don't really know whether we haven't really kind of looked in detail about what the system ends up working.",
                    "label": 0
                },
                {
                    "sent": "We suspect that it looks it probably looks at the hair quite a lot because there actually is a piece that doesn't change too much between one view on another.",
                    "label": 0
                },
                {
                    "sent": "You can't really look at the eyes because the eyes are covered most of the time, so it's probably a global shape and inherent sort of general appearance.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is perhaps a slightly better.",
                    "label": 0
                },
                {
                    "sent": "Picture of the kind of vector we get, so we get vectors that are mostly saturated and it's because of this loss function that kind of tries to push things away.",
                    "label": 0
                },
                {
                    "sent": "But it turned out to work better than other things which way?",
                    "label": 0
                },
                {
                    "sent": "OK, the next.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to talk about is how you can use this technique for dimensionality reduction now, not just for classification or verification, but for kind of mapping question.",
                    "label": 0
                },
                {
                    "sent": "I guess you're doing your distance learning.",
                    "label": 0
                },
                {
                    "sent": "Can you say about how much you can prove you're just using the best passing card you can get without any distance?",
                    "label": 0
                },
                {
                    "sent": "So we can't.",
                    "label": 0
                },
                {
                    "sent": "We can't actually use any classifier here because the test phase uses a different set of people from the training phase.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You know you can't use a traditional classifier.",
                    "label": 0
                },
                {
                    "sent": "You wouldn't be able to try any other classes.",
                    "label": 0
                },
                {
                    "sent": "I mean what you might be able to do would be to train the classifier on the on the classes you have, and if it's a multilayer net, like for example, you could drop off the last layer and then just render last layer with the examples you have on the test set.",
                    "label": 0
                },
                {
                    "sent": "OK, the pair is so you know there might be a possibility.",
                    "label": 0
                },
                {
                    "sent": "We didn't try that.",
                    "label": 0
                },
                {
                    "sent": "He said the.",
                    "label": 0
                },
                {
                    "sent": "You always take divorced, offended, but if you change cost function, most offended also change, no, because it's it's monotonic, so it's monotonic with distance, so it's always going to be the same.",
                    "label": 0
                },
                {
                    "sent": "The same that you push away.",
                    "label": 0
                },
                {
                    "sent": "I have the impression that people use Commission pasty, perfectly fits correct examples.",
                    "label": 0
                },
                {
                    "sent": "Couldn't you put a margin on your quadratic function that tells you if that's the one for the good fit, but the genuine pair?",
                    "label": 0
                },
                {
                    "sent": "Since you have a budget for incorrect there, so you have a bit of freedom to have correct pair with smaller, imagine well below the cost is 0 for Jimmy.",
                    "label": 0
                },
                {
                    "sent": "OK so the question.",
                    "label": 0
                },
                {
                    "sent": "I guess the suggestion would be to instead of using a quadratic cost for the part that brings the vectors together for your genuine pair would be to kind of have a slack in it so that if I understood correctly so that you don't bring the distance to 0 but you bring it to sort of within some sort of margin.",
                    "label": 0
                },
                {
                    "sent": "And that would perhaps be more robust to kind of outliers or something.",
                    "label": 0
                },
                {
                    "sent": "More than false positive right?",
                    "label": 0
                },
                {
                    "sent": "Maybe you could lose power in technicals positive?",
                    "label": 0
                },
                {
                    "sent": "OK, the reason we have more false vision.",
                    "label": 0
                },
                {
                    "sent": "False positives, because that's the numbers we give on the.",
                    "label": 0
                },
                {
                    "sent": "You know on the RC curve and the reason we give those numbers because application is generally want that you know they so security applications.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "For face verification you get to a door and you have to kind of prove that yourself.",
                    "label": 0
                },
                {
                    "sent": "If it's a really highly secure place then you don't want to accept someone on this.",
                    "label": 0
                },
                {
                    "sent": "That person really is is who he or she claims.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if it's you know your customer in the bank and you want to verify you know the right person signing your checks, then you don't want to upset too many customers.",
                    "label": 0
                },
                {
                    "sent": "So here you want to know more false accept and full speed.",
                    "label": 0
                },
                {
                    "sent": "You have set your customers right, so it really depends where you want, but most of the applications you kind of want more.",
                    "label": 0
                },
                {
                    "sent": "Full speed Jackson for success.",
                    "label": 0
                },
                {
                    "sent": "You said you were a sampling from the all possible pairs.",
                    "label": 0
                },
                {
                    "sent": "And yeah, for Maria it's really a subset for each of the categories, so.",
                    "label": 0
                },
                {
                    "sent": "Would use the oil you select the forgiven application.",
                    "label": 0
                },
                {
                    "sent": "You may select the balance between similar and dissimilar pairs.",
                    "label": 0
                },
                {
                    "sent": "You mean in terms of numbers?",
                    "label": 0
                },
                {
                    "sent": "How many generating pairs was?",
                    "label": 0
                },
                {
                    "sent": "Speak application while you math certain number of imposter pair.",
                    "label": 0
                },
                {
                    "sent": "Write access right on the other side.",
                    "label": 0
                },
                {
                    "sent": "You may select the ratio between similar and dissimilar pairs right?",
                    "label": 0
                },
                {
                    "sent": "So this raises an interesting question.",
                    "label": 0
                },
                {
                    "sent": "Which is you know?",
                    "label": 0
                },
                {
                    "sent": "How do you set the ratio between the number of genuine or similar pairs and the number of non similar pairs in the training set?",
                    "label": 0
                },
                {
                    "sent": "So depending on the application you might want to have different proportions of the two.",
                    "label": 0
                },
                {
                    "sent": "It also depends on skin of the diversity of of different.",
                    "label": 0
                },
                {
                    "sent": "You know the non genuine pairs you may have.",
                    "label": 0
                },
                {
                    "sent": "So of course you know it's much more difficult to train the system to push away the bad guys just because it's a high dimension high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So you gotta push away in other dimensions rather than attracting.",
                    "label": 0
                },
                {
                    "sent": "You're attracting a lot of people to the same place so that they can only converge to one place.",
                    "label": 0
                },
                {
                    "sent": "Could you say?",
                    "label": 0
                },
                {
                    "sent": "Manage is to to include things like tangent distance or small changes.",
                    "label": 0
                },
                {
                    "sent": "OK, so so the question is to what extent this kind of managed to do similar things.",
                    "label": 0
                },
                {
                    "sent": "Attention distance, which is robustness to kind of small shifts or or kind of distortions.",
                    "label": 0
                },
                {
                    "sent": "So I mean because we use this commercial net architecture which has built in the ability to sort of have you know, somewhat invariant representations to shifts and distortions.",
                    "label": 0
                },
                {
                    "sent": "The guest, but it's really not sort of quantitative assessment, is that there's actually more invariants then we would get retention distance.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could always use non tangent distance by kind of you know explicitly moving faces around and distorting them so you can match them to their register them to the templates.",
                    "label": 0
                },
                {
                    "sent": "But that's that's really expensive.",
                    "label": 0
                },
                {
                    "sent": "If you have lots of faces.",
                    "label": 0
                },
                {
                    "sent": "At the other way to take this might be to say OK will do this.",
                    "label": 0
                },
                {
                    "sent": "Some kind of submanifold and we hear the submanifolds right?",
                    "label": 0
                },
                {
                    "sent": "Well, that's kind of what this is doing.",
                    "label": 0
                },
                {
                    "sent": "I mean, the manifold has any variance to it, so a lot of points in the input that correspond to similar faces will map to the same point on the manifold, and then you move to different points on the manifold.",
                    "label": 0
                },
                {
                    "sent": "Possibly so, for example, one thing that tension distance won't do for you is that this will do is that you know this thing will attempt to map different.",
                    "label": 0
                },
                {
                    "sent": "You know, poses or the same phase, for example to the same point.",
                    "label": 0
                },
                {
                    "sent": "So it's going to try to make this whole manifold very small.",
                    "label": 0
                },
                {
                    "sent": "Right Ascension Distance will still have many folders, just, you know, sort of kind of moving points on the manifold, so the points have to kind of move together to match, which is, whereas here they will be at the same place.",
                    "label": 0
                },
                {
                    "sent": "Hopefully.",
                    "label": 0
                },
                {
                    "sent": "OK, the next thing to go quickly on this is a technique that we called Doctor Lim.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which means dimensionality reduction by learning invariant mapping.",
                    "label": 0
                },
                {
                    "sent": "So it's basically the same idea, except now we use this to kind of get manifolds instead of getting everybody to the same place if we have resemblance between them.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is.",
                    "label": 0
                },
                {
                    "sent": "So, so this is kind of a manifold learning technique that directly deals with out of sample.",
                    "label": 0
                },
                {
                    "sent": "Cases because you know, it learns a mapping, so it doesn't.",
                    "label": 0
                },
                {
                    "sent": "Lanza function that Maps from input space to output space, it doesn't just run a bunch of produce a bunch of output points.",
                    "label": 0
                },
                {
                    "sent": "So what this tech is going to be able to do is when you show it a test sample which has nothing to do with any of the training samples, you just feed it to the function.",
                    "label": 0
                },
                {
                    "sent": "It would produce an output point.",
                    "label": 0
                },
                {
                    "sent": "OK, there's something sort of.",
                    "label": 0
                },
                {
                    "sent": "I was playing Vanilla Illini location map or originality can't do.",
                    "label": 0
                },
                {
                    "sent": "For those techniques, the other point, the relationship between all the points to be known in advance, so that's kind of a.",
                    "label": 0
                },
                {
                    "sent": "It's going to be bad for test samples.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to use this function again a mapping.",
                    "label": 0
                },
                {
                    "sent": "And we're going to have to figure out, you know, a loss function that's going to build this.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Manifold.",
                    "label": 0
                },
                {
                    "sent": "And we'd like this function to be invariant to sort of irrelevant changes on the input, like, say, illumination.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say we have images of airplanes from various POV's OK, and what we'd like the machine to produce is.",
                    "label": 0
                },
                {
                    "sent": "A vector that corresponds to the point of view of the airplane, but is independent of illumination.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have examples like this and we tell the machine those two examples are similar.",
                    "label": 0
                },
                {
                    "sent": "Those two examples are similar.",
                    "label": 0
                },
                {
                    "sent": "But slightly different, and those two examples are exactly the same.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But this kind of expensive to say exactly the same or slightly different, so we just have two things the same or not alright, and we cannot rely on the structure of this neighborhood graph to kind of figure it out.",
                    "label": 0
                },
                {
                    "sent": "So we're going to use that, for example, this airplane pose extraction thing and the other one would be kind of, you know, mapping all the digits of the end this data set on two kind of low dimensional 2 dimensional space for example.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let's see.",
                    "label": 0
                },
                {
                    "sent": "So here is.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recipe we're going to build a neighborhood graph and the neighborhood graph is going to be a graph that's going to link all of our training samples, and we're going to put an arc between two samples if we deem them similar OK.",
                    "label": 0
                },
                {
                    "sent": "Exactly the same or relatively similar if you want.",
                    "label": 0
                },
                {
                    "sent": "And we are allowed to use prior knowledge for this.",
                    "label": 0
                },
                {
                    "sent": "So maybe paranoid coming from how the data was collected or from labeling or from you know whatever.",
                    "label": 0
                },
                {
                    "sent": "It's OK to use prior knowledge because this is only for the training set in the test set.",
                    "label": 0
                },
                {
                    "sent": "We don't need to provide any information, OK?",
                    "label": 0
                },
                {
                    "sent": "So we're going to pick a price family of functions that you know from which we're going to train or mapping neural net visual basic functions.",
                    "label": 0
                },
                {
                    "sent": "Whatever optimized parameter of this function so as to minimize loss function that makes the distance between the output vectors of neighbors small and the distance between the vectors of non neighbors large.",
                    "label": 0
                },
                {
                    "sent": "OK, some ideas before.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is what this graph will look like for this airplane example.",
                    "label": 0
                },
                {
                    "sent": "So those two guys are similar because they view from similar pose and those two guys are similar again because they viewed from similar posts I've listed.",
                    "label": 0
                },
                {
                    "sent": "Guys are not similar because their poses really different now.",
                    "label": 0
                },
                {
                    "sent": "OK so this is sort of non transitive similarity, if you want.",
                    "label": 0
                },
                {
                    "sent": "And those two guys are from the same viewpoint but different illumination, so really they are the same.",
                    "label": 1
                },
                {
                    "sent": "So again, we draw a line between them, and again we do this relationship.",
                    "label": 1
                },
                {
                    "sent": "So this is our graph, OK, of course we're going to have many more examples in that for training, but this is an example of a graph similarity graph.",
                    "label": 0
                },
                {
                    "sent": "And this is built with prior knowledge because, you know, we have the pose.",
                    "label": 0
                },
                {
                    "sent": "Attractive.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the training samples, so we're going to map this graph if you want to the G function.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And well, hopefully the G function is going to do is map map this to kind of a 2 dimensional space in which for each image we're going to have a point and the points are going to be arranged in such a way that kind of the cover, the space and at similar points are nearby.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again we use this Siamese architecture for the same reason.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A different loss function that we used before and different slightly different training procedure.",
                    "label": 0
                },
                {
                    "sent": "And also we use for this is the square square loss that I talked about before.",
                    "label": 0
                },
                {
                    "sent": "So those information is used to determine whether we put a link between 2 images in this similarity graph, whether to decide whether to images are similar or not.",
                    "label": 0
                },
                {
                    "sent": "So if the pose are our neighboring poses, then we put on our between them.",
                    "label": 0
                },
                {
                    "sent": "If there are non neighboring poses and we don't.",
                    "label": 0
                },
                {
                    "sent": "OK. Um?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're going to use this square square Dallas.",
                    "label": 0
                },
                {
                    "sent": "OK, so we pay quadratically for making the output vectors different for two images that we deemed similar that our neighbors in the graph, we pay again quadratically for making the distance small for images that are.",
                    "label": 1
                },
                {
                    "sent": "Deemed different images that don't have an arc between them in this similarity graph.",
                    "label": 0
                },
                {
                    "sent": "OK, and we have a margin here and the margin does nothing more than sort of setting the scale for the entire.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thing we get so we can do some sort of spring analogy.",
                    "label": 0
                },
                {
                    "sent": "Mechanical spring analogy so you know similar points in the output space are linked with the spring and similar dissimilar points in the output space.",
                    "label": 1
                },
                {
                    "sent": "You know have a spring and you have to compress the spring to bring that point closer to the inside of this, OK?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so it comes down to code and finding the minimum energy solution of this, but also producing those vectors by the thing.",
                    "label": 0
                },
                {
                    "sent": "So we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do this on a list.",
                    "label": 0
                },
                {
                    "sent": "So if you take EM list so you take.",
                    "label": 0
                },
                {
                    "sent": "3000 samples of Forza Nines and.",
                    "label": 0
                },
                {
                    "sent": "You train this system and the way you determine whether 2 images are neighbors or not is very stupid here.",
                    "label": 0
                },
                {
                    "sent": "We wouldn't need to do this, we're just looking at your neighbors are the five nearest neighbors in Euclidean distance in pixel space?",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "So it's very stupid way of doing it, but it's a sanity check.",
                    "label": 0
                },
                {
                    "sent": "So what do we get if we do that?",
                    "label": 0
                },
                {
                    "sent": "If we if we do this so no prior knowledge here.",
                    "label": 0
                },
                {
                    "sent": "Basically we use Euclidean distance to drive the drive, the system and we don't get anything particularly interesting other than the fact that you know you have some sort of this continuous manifold with sort of nicely continuously changing shapes as you move around.",
                    "label": 0
                },
                {
                    "sent": "OK, and these are test samples.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are test samples that we run through the through the function and sort of figure out where they end up in this 2 dimensional output space.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the function again that we use for this is a commercial network.",
                    "label": 0
                },
                {
                    "sent": "Very small one.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now what about mapping something that's invented shift?",
                    "label": 0
                },
                {
                    "sent": "So we're going to do is we're going to augment or training set with samples that are shifted versions of the original samples, so we can shift by minus 6 -- 3 + 3 or 6 pixels.",
                    "label": 0
                },
                {
                    "sent": "So now we have all kinds of examples are shifted from each other and we apply the same algorithm and we'll get our fair.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Processors and those are all the things that have been shifted by 6 pixels or things I visited by three pixels to the left or the original ones are the ones three pixels to the right and six weeks after the right.",
                    "label": 0
                },
                {
                    "sent": "OK, that's so good, right?",
                    "label": 0
                },
                {
                    "sent": "But you know, it's expected.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the next thing, so if you apply adelita that to that set using Euclidean distance again, you get nothing.",
                    "label": 0
                },
                {
                    "sent": "Basically nothing interesting is very strange actually.",
                    "label": 0
                },
                {
                    "sent": "Only.",
                    "label": 0
                },
                {
                    "sent": "Only combats the collapse through very very weak constraint absolutely variance.",
                    "label": 0
                },
                {
                    "sent": "Right, he's doing.",
                    "label": 0
                },
                {
                    "sent": "He's going in cheating on you by collapsing almost all the data and then putting a big tail to keep the covariance constraint right?",
                    "label": 0
                },
                {
                    "sent": "So for those of you who don't know Sam, you know he's one of the first author of the paper, so he knows what he's talking about and.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So and so that I mean that's the problem, which is that you know, since since it's using neighbors, you know the two shifted versions of the digits are very far away in Euclidean distance, so they can't possibly be neighbors.",
                    "label": 0
                },
                {
                    "sent": "So here's the next thing we do.",
                    "label": 0
                },
                {
                    "sent": "We put a little bit of prior knowledge in the in the learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we we.",
                    "label": 0
                },
                {
                    "sent": "We tell the system will not only your neighbors are Euclidean nearest neighbors, but they are also the shifted version of your Euclidean nearest neighbors under shifted versions of yourself.",
                    "label": 0
                },
                {
                    "sent": "OK, so in the graph we put an arc in the graph.",
                    "label": 0
                },
                {
                    "sent": "If two images are similar in Euclidean distance in the register position, or if.",
                    "label": 0
                },
                {
                    "sent": "If there are shifted version of your neighbors.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's using prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "OK, we have to know how the data was cooked to do this, but it's only on the training set and.",
                    "label": 0
                },
                {
                    "sent": "And we run the algorithm again.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We get this this so now we don't get those five clusters anymore.",
                    "label": 0
                },
                {
                    "sent": "Everything is 1 big uniform, uniformly kind of filled.",
                    "label": 0
                },
                {
                    "sent": "Surface.",
                    "label": 0
                },
                {
                    "sent": "And you know, again, you get this nice sort of continuous variation of shape when you sort of move around the manifold.",
                    "label": 0
                },
                {
                    "sent": "Here this surface.",
                    "label": 0
                },
                {
                    "sent": "But what you can see is that.",
                    "label": 0
                },
                {
                    "sent": "You can have widely varying positions, so for example here this guys is registered to the left and this guys reached out to the right, but they actually are neighbors, so the function map them to the same point because it figures well they have the same shape.",
                    "label": 0
                },
                {
                    "sent": "Of course they have different there a different position, but they're the same shape.",
                    "label": 0
                },
                {
                    "sent": "You know they were supposed to be neighbors in the training set, so these are actually test samples.",
                    "label": 0
                },
                {
                    "sent": "These are not training samples.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a test set.",
                    "label": 0
                },
                {
                    "sent": "And if you zoom in on kind of a little Patch here, you get you know all those lines which are pretty much all the same, very similar shapes, But at what is sort of different differing positions within the within the frame.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of example of learning really an invariant.",
                    "label": 0
                },
                {
                    "sent": "You know we're mapping or dimensional manifold with invariant properties.",
                    "label": 0
                },
                {
                    "sent": "OK, it's only digits.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's go with the airplanes.",
                    "label": 0
                },
                {
                    "sent": "So here is a different problem that we're trying to do.",
                    "label": 0
                },
                {
                    "sent": "Again is.",
                    "label": 0
                },
                {
                    "sent": "So we have 927 images of airplanes under 6 different combinations, 18 different estimates, or every 20 degrees this way.",
                    "label": 0
                },
                {
                    "sent": "And 99 different elevations from 35 degrees to 70 degrees every 5 degrees.",
                    "label": 0
                },
                {
                    "sent": "So we take out of this 900 or so images we take six 660 as training samples for this 660 training samples.",
                    "label": 0
                },
                {
                    "sent": "We, the relationship the neighborhood relationship is such that one image and all its identical images, identical viewpoint, different limitations are neighbors.",
                    "label": 0
                },
                {
                    "sent": "And the immediately.",
                    "label": 0
                },
                {
                    "sent": "A consecutive if you want neighboring poses also are your neighbors, and so are the different dimensions.",
                    "label": 0
                },
                {
                    "sent": "Illuminated versions of this.",
                    "label": 0
                },
                {
                    "sent": "OK, so each image has a lot of neighbors in fact.",
                    "label": 0
                },
                {
                    "sent": "OK 'cause it's got.",
                    "label": 0
                },
                {
                    "sent": "Different locations in different poses.",
                    "label": 0
                },
                {
                    "sent": "But only the neighboring poses.",
                    "label": 0
                },
                {
                    "sent": "Um, actually it's the 1st and 2nd neighbor in azimuth and the first neighbor in elevation and all the illumination.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a little more neighbors.",
                    "label": 0
                },
                {
                    "sent": "I just told you.",
                    "label": 0
                },
                {
                    "sent": "And we're not going to use a convolutional net here.",
                    "label": 0
                },
                {
                    "sent": "We're going to use a fully connected neural net, and the reason is because there is really no shift in variance property.",
                    "label": 0
                },
                {
                    "sent": "We need to learn here because all the all the planes are really very well registered, so we need to learn here is invariant to illumination only OK.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are some of the examples from other cat.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stories that we could.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so you probably do this, and again you run into trouble because.",
                    "label": 0
                },
                {
                    "sent": "So these are kind of two slices of Eli, so it comes up with this kind of fold.",
                    "label": 0
                },
                {
                    "sent": "So this is viewed from the top and this is you from the side.",
                    "label": 0
                },
                {
                    "sent": "And that's because it gives a huge distance.",
                    "label": 0
                },
                {
                    "sent": "You know, using clean distance to measure to build the neighborhood graph with Italy and with no prior knowledge and so.",
                    "label": 0
                },
                {
                    "sent": "So here you know, there's a huge distance between those two illuminations, because these are very dark, so all the pixels from this are very different for all the pixels from that.",
                    "label": 0
                },
                {
                    "sent": "OK, on the other hand.",
                    "label": 0
                },
                {
                    "sent": "Even dissimilar poses are fairly similar, so this completely dominates the entire distance measure.",
                    "label": 0
                },
                {
                    "sent": "OK, this is kind of an unfair comparison.",
                    "label": 0
                },
                {
                    "sent": "I mean, I sort of keep bashing Eli, but it's great, but.",
                    "label": 0
                },
                {
                    "sent": "It's going to comparison because we could also put a little bit of prior knowledge into Italy.",
                    "label": 0
                },
                {
                    "sent": "The main problem is then you know how do you kind of map new examples and how do you find relationships.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the result we get with this Doctor Lim algorithm.",
                    "label": 0
                },
                {
                    "sent": "And these are different views of the 3D manifold we get.",
                    "label": 0
                },
                {
                    "sent": "So we decided period of the dimension.",
                    "label": 0
                },
                {
                    "sent": "OK, we say it's going to be 3D.",
                    "label": 0
                },
                {
                    "sent": "We get this kind of.",
                    "label": 0
                },
                {
                    "sent": "Cylinder.",
                    "label": 0
                },
                {
                    "sent": "So this is viewed from the top.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And along the periphery of the cylinder, basically you get azimuth.",
                    "label": 0
                },
                {
                    "sent": "OK, so as you move around you get views of the airplanes corresponding to different azimuth.",
                    "label": 0
                },
                {
                    "sent": "The so this guy is this adamant and this guys azimuth.",
                    "label": 0
                },
                {
                    "sent": "Going to look at this cylinder going to rotate it this way.",
                    "label": 0
                },
                {
                    "sent": "OK so it's kind of standing this way and you know there's a hole here.",
                    "label": 0
                },
                {
                    "sent": "It's kind of like this.",
                    "label": 0
                },
                {
                    "sent": "And if you.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Actually I screwed up.",
                    "label": 0
                },
                {
                    "sent": "I think it's backwards.",
                    "label": 0
                },
                {
                    "sent": "Yeah, no, it's on the side here.",
                    "label": 0
                },
                {
                    "sent": "OK, so the whole goes like this so it's kind of like this, right?",
                    "label": 0
                },
                {
                    "sent": "And so as you move along the main axis, basically what changes is the elevation.",
                    "label": 0
                },
                {
                    "sent": "OK, so this guy is viewed from, you know, from a shadow angle.",
                    "label": 0
                },
                {
                    "sent": "And this guy is right from the top, more or less.",
                    "label": 0
                },
                {
                    "sent": "And then if you zoom in on kind of a, you see kind of little sort of clusters of points here, and those customer points generally have six points in them.",
                    "label": 0
                },
                {
                    "sent": "123456 and those correspond to all the images at one azimuth, one elevation, and different illuminations.",
                    "label": 0
                },
                {
                    "sent": "OK, so you get what you want.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you get a function that you know you give it an image, an airplane, and it gives you immediately 2 dimensional or three dimensional point vector.",
                    "label": 0
                },
                {
                    "sent": "That is a point on this manifold indicating the pose.",
                    "label": 0
                },
                {
                    "sent": "So that's an interesting thing.",
                    "label": 0
                },
                {
                    "sent": "These are the, so it was a neural net with 20 hidden units and these are the weights of the hidden unit.",
                    "label": 0
                },
                {
                    "sent": "So the amazing thing is that you plug a image of an airplane and you compute the dot product between this airplane image and those 20 vectors.",
                    "label": 0
                },
                {
                    "sent": "And presents with sigmoid computer linear combination of those and it goes the pose would have thought so those are kind of really funny.",
                    "label": 0
                },
                {
                    "sent": "Sort of little.",
                    "label": 0
                },
                {
                    "sent": "You know circular Gabor filter like things.",
                    "label": 0
                },
                {
                    "sent": "Of course you plug anything else on an airplane is not going to work.",
                    "label": 0
                },
                {
                    "sent": "You have to train it on other examples, but it's kind of cute.",
                    "label": 0
                },
                {
                    "sent": "OK, so after I've shown you all those pretty pictures, I'm going to stop here after I've shown you all those pretty pictures.",
                    "label": 0
                },
                {
                    "sent": "I'm going to tell you that this last technique here is basically useless.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know you can't use it for anything useful, but it produces really nice pictures and it gets you papers published, CPR.",
                    "label": 0
                },
                {
                    "sent": "It's really cute.",
                    "label": 0
                },
                {
                    "sent": "Know people are really impressed by nice pictures, even though there's no user can think of for this.",
                    "label": 0
                },
                {
                    "sent": "Maybe someone will have some ideas how you can use this.",
                    "label": 0
                },
                {
                    "sent": "You know, for for visualization purposes, basically it's good for visualization like sneaky or things like that, where so assignment is really beautiful.",
                    "label": 0
                },
                {
                    "sent": "Charge of like authors of NIPS where people cluster and depending on how much the site, each other and it's really cool.",
                    "label": 0
                },
                {
                    "sent": "So it's good for for human consumption.",
                    "label": 0
                },
                {
                    "sent": "But if you want a lot of monitoring applications, well, absolutely.",
                    "label": 0
                },
                {
                    "sent": "It's just people who are looking at really ridiculous 1970s graphs of like RPM and pressure and stuff.",
                    "label": 0
                },
                {
                    "sent": "That's right, they really wanted to know, right?",
                    "label": 0
                },
                {
                    "sent": "Is this huge vector of measurements that the plant produced in the last hour, similar or not similar to right?",
                    "label": 0
                },
                {
                    "sent": "So, for example, some of the you know that's applications ability or for you know people analyzing recorded data from neurons and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "And yeah, this is getting huge high dimensional stuff and you can figure out what it does or are.",
                    "label": 0
                },
                {
                    "sent": "Some of my friends then why you used it for.",
                    "label": 0
                },
                {
                    "sent": "Figuring out the robustness of function of some.",
                    "label": 0
                },
                {
                    "sent": "Protein regulation mechanisms an you know you figure out that there's only a few things that really matter in there, and the manifold is very low dimensional space because their regulatory regulatory mechanism, so it's visualization is really.",
                    "label": 0
                },
                {
                    "sent": "It's very important but but there is one thing I want to dispel, which is that these techniques cannot be used for anything like recognition.",
                    "label": 0
                },
                {
                    "sent": "I mean, I don't see.",
                    "label": 0
                },
                {
                    "sent": "This question here.",
                    "label": 0
                },
                {
                    "sent": "If you made, that may make it difficult for ratification.",
                    "label": 0
                },
                {
                    "sent": "Is that your similarities only binary, very similar or not?",
                    "label": 0
                },
                {
                    "sent": "Extended OK. Function.",
                    "label": 0
                },
                {
                    "sent": "Distance matrix right?",
                    "label": 0
                },
                {
                    "sent": "OK, so there is no, there's nothing that prevents you from training those actually dealing with binary values is what makes it complicated, because if you do have exact similarity measures between pairs of samples then you can just train this.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "How do you define?",
                    "label": 0
                },
                {
                    "sent": "Well, so you don't use neighborhoods anymore, but what you do is that you have a value for the distance between two things.",
                    "label": 0
                },
                {
                    "sent": "Then you train the output of this this Siamese system as a regressor.",
                    "label": 0
                },
                {
                    "sent": "You just trying to produce the answer you want.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's just a regression problem.",
                    "label": 0
                },
                {
                    "sent": "You train the thing to produce the right answer and that's it.",
                    "label": 0
                },
                {
                    "sent": "You don't have it.",
                    "label": 0
                },
                {
                    "sent": "You don't have to come up with a smart loss function, you know, just be squared.",
                    "label": 0
                },
                {
                    "sent": "Error will do fine, so it's actually a much simpler problem.",
                    "label": 0
                },
                {
                    "sent": "But now it puts the onus on whoever labels the data now, because now you have to come up with.",
                    "label": 0
                },
                {
                    "sent": "Actual distances between every pair of object, or you know a subset of pairs of objects.",
                    "label": 0
                },
                {
                    "sent": "So in fact you know what makes this complicated is affected.",
                    "label": 0
                },
                {
                    "sent": "You can reduce the burden on the labeler, and you can use.",
                    "label": 0
                },
                {
                    "sent": "Kind of, you know, very sort of week prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "Like you know they are sort of the same or not the same, and then you have to come up with those kind of energy functions.",
                    "label": 0
                },
                {
                    "sent": "That sort of you know.",
                    "label": 0
                },
                {
                    "sent": "So for example, for this problem we tried several energy functions and some of them would curdle.",
                    "label": 0
                },
                {
                    "sent": "You know, I don't know if any of you sort of cook, but you know you want to make a.",
                    "label": 0
                },
                {
                    "sent": "Come on guys or whatever, right?",
                    "label": 0
                },
                {
                    "sent": "And you sort of build it too much and you know you get sort of Trump's right?",
                    "label": 0
                },
                {
                    "sent": "So things happen with loss functions that are not well designed.",
                    "label": 0
                },
                {
                    "sent": "You get console.",
                    "label": 0
                },
                {
                    "sent": "You don't get this smooth uniform surface, so this function is defined to get kind of uniform density of points anywhere on the surface.",
                    "label": 0
                },
                {
                    "sent": "Different functions will give you different results.",
                    "label": 0
                },
                {
                    "sent": "We are always the question.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can talk.",
                    "label": 0
                },
                {
                    "sent": "You can get us or the question offline with thank you.",
                    "label": 0
                },
                {
                    "sent": "So we have a.",
                    "label": 0
                }
            ]
        }
    }
}