{
    "id": "awc5ntsauchfhm2ayhkrtabcpeeft5ko",
    "title": "Inference in Ising models",
    "info": {
        "author": [
            "Bhaswar B. Bhattacharya, Department of Statistics, Stanford University"
        ],
        "published": "March 7, 2016",
        "recorded": "December 2015",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Physics->Statistical Physics"
        ]
    },
    "url": "http://videolectures.net/netadis2015_bhattacharya_ising_models/",
    "segmentation": [
        [
            "So I'm talking about inference in Ising model, so I'm a graduate cylinder stand for statistics and this is joint work with Summit Mukherjee, who was also a student at Stanford and now he's in Columbia, so."
        ],
        [
            "Just begin by recalling very quickly what is the Ising model?",
            "Everybody knows it here, but I'm still want to set up some notation so your data is a set of is an invector of plus and minus ones, and you model the correlation between the coordinates of Sigma by an exponential family.",
            "So beta is the inverse temperature, which I call the natural parameter.",
            "The sufficient statistic is the Hamiltonian, which is Tau Prime, J and power.",
            "Jane is a matrix which is symmetric matrix with zeros in the diagonals.",
            "And if in beta is the normalizing constant which you choose so that the whole thing sums to one, it becomes a probability.",
            "So."
        ],
        [
            "Our goal here is to estimate beta and so if you are statistician, the first thing you try to do is to try a maximum likelihood.",
            "Now that is particularly hard here.",
            "Be cause the normalizing constant is intractable, so there are some approximate ways to do this, and this is called the maximum pseudo likelihood, which is very widely used in the field of mark of random in the Markov random field literature, and it was introduced by Julian Basak in 1974, so I'm not going to go into great detail on what is the pseudo likelihood, but it's something like for every spin Sigma I you look at the conditional distribution of Sigma I given.",
            "All the other spins and you look at the product of this conditional densities and we look at this as a function of beta and you maximize this as a function of beta and that is the pseudo likelihood.",
            "Now the question is how good is a pseudo likelihood?",
            "Because this is not the likelihood, so it doesn't have properties that."
        ],
        [
            "Likelihood supposed to have but few years ago, so short of prove the following theorem that if you have if you log normalizing constant is order in at a point beta zero, then at that point the pseudo likelihood is root N consistent.",
            "OK.",
            "So that's pretty pretty nice, so it says that whenever the order of the normalizing constant is known to you and it's ordering, and then you get routine consistency.",
            "But it does not tell you anything of what happens if the log normalizing constant is not ordering, and so our main result."
        ],
        [
            "One of our main results is to extend this to show that if if, if you're given a point beta zero an if you look at a neighborhood of that point.",
            "So if at a neighborhood of a point beta zero, the log normalizing constant has order A in.",
            "Then the pseudo likelihood estimator is root over a inconsistent.",
            "So this generalizes show results to all orders of the normalizing constant, and we see some interesting interesting phase transition.",
            "So if you look at an Ising model on a random graph and where the probability of edge is depending on in, so it shows that if beta is less than one, then you should likely would estimate is 1 /, sqrt P and consistent, and it better is greater than one.",
            "It's routine consistent.",
            "And.",
            "We see sort of a similar thing.",
            "If you look at rising model on regular graphs, then if you look at but if you look at the sort of like your estimator then before one it is rude over and over Dion consistent and after one it is root N consistent.",
            "So for instance if you look at the lattice which is where these constant and for the two dimensional lattice these four it's always root inconsistent.",
            "But if you look at graphs like the hypercube or graphs where the degree grows to Infinity, you see there is a phase transition in the rate of the pseudo likelihood, which was not known before from shortest results.",
            "So here is a simulation."
        ],
        [
            "So this is a random graph with N = 2000 and peas into the minus 1/3, and we look at Ising model from beta equal to zero to two and you see that there is.",
            "So we look at the model and we simulate we calculate the pseudo likelihood many many times and you look at the error bars.",
            "So we see that at one there is a sharp phase transition.",
            "So if the theory is correct, before it says that before 1 the rate is into this one six after one through 10 and so you clearly see that there is a sharp transition in the in the in the length of the error bars, which is what we're saying.",
            "So."
        ],
        [
            "The theorem before what it tells you is that if the partition function is a of order N, then you get root N consistent estimate.",
            "But what if the partition function is of order one?",
            "So it just says that the estimate is not that super likelihood estimate is not consistent.",
            "But is there anything else which is consistent?",
            "So in this regime actually there is an impossibility result which says that if the partition function is order one at a point, then no estimate is consistent.",
            "And no estimate is consistent in that interval from zero debate or not.",
            "And this gives actually some very interesting consequence.",
            "So if you have a sequence of graphs, so I am so just think of G&P when P is fixed.",
            "If you don't know what it means that a sequence of graph converge, but essentially there is this theory of graph limits which you can use to understand limits of dense graphs.",
            "And it says that if you have a sequence of graphs, so think of G and half for instance if it converges to some grabbed a blue and if it has maximum eigen.",
            "If that that that.",
            "OK, so this is if the graph on W has maximum eigenvalue Lambda one then what it says is that before the before one over the maximum eigenvalue, the pseudo likelihood is inconsistent and after that it is root inconsistent.",
            "But it says something even more that before the phase transition point.",
            "No estimator is consistent.",
            "So think of the example of G and half what it says that at the point.",
            "2.",
            "Before the .2, no estimator is consistent.",
            "And after two, this little likelihood is routine consistent."
        ],
        [
            "So here is a picture which shows that you have a P on your X axis beta on the Y axis and I and the color is the.",
            "So we simulated Ising model from GNP many times and then look at I look at the pseudo like your estimate.",
            "So I want to say whether I can estimate properly so the yellow region is where estimation is possible, red where it is not.",
            "So in this case I would the phase transition curve predicted is beta equals to 1 / P and you clearly see that of the curve there is a.",
            "There's a clear, clear transition.",
            "OK."
        ],
        [
            "So we have some applications to study spatial correlation among colors among political colors in the different states of the US in the different elections.",
            "And so we analyze the data.",
            "And I have some details in the poster.",
            "So if you are interested please."
        ],
        [
            "Get out, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm talking about inference in Ising model, so I'm a graduate cylinder stand for statistics and this is joint work with Summit Mukherjee, who was also a student at Stanford and now he's in Columbia, so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just begin by recalling very quickly what is the Ising model?",
                    "label": 1
                },
                {
                    "sent": "Everybody knows it here, but I'm still want to set up some notation so your data is a set of is an invector of plus and minus ones, and you model the correlation between the coordinates of Sigma by an exponential family.",
                    "label": 1
                },
                {
                    "sent": "So beta is the inverse temperature, which I call the natural parameter.",
                    "label": 1
                },
                {
                    "sent": "The sufficient statistic is the Hamiltonian, which is Tau Prime, J and power.",
                    "label": 0
                },
                {
                    "sent": "Jane is a matrix which is symmetric matrix with zeros in the diagonals.",
                    "label": 0
                },
                {
                    "sent": "And if in beta is the normalizing constant which you choose so that the whole thing sums to one, it becomes a probability.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our goal here is to estimate beta and so if you are statistician, the first thing you try to do is to try a maximum likelihood.",
                    "label": 1
                },
                {
                    "sent": "Now that is particularly hard here.",
                    "label": 0
                },
                {
                    "sent": "Be cause the normalizing constant is intractable, so there are some approximate ways to do this, and this is called the maximum pseudo likelihood, which is very widely used in the field of mark of random in the Markov random field literature, and it was introduced by Julian Basak in 1974, so I'm not going to go into great detail on what is the pseudo likelihood, but it's something like for every spin Sigma I you look at the conditional distribution of Sigma I given.",
                    "label": 1
                },
                {
                    "sent": "All the other spins and you look at the product of this conditional densities and we look at this as a function of beta and you maximize this as a function of beta and that is the pseudo likelihood.",
                    "label": 1
                },
                {
                    "sent": "Now the question is how good is a pseudo likelihood?",
                    "label": 0
                },
                {
                    "sent": "Because this is not the likelihood, so it doesn't have properties that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Likelihood supposed to have but few years ago, so short of prove the following theorem that if you have if you log normalizing constant is order in at a point beta zero, then at that point the pseudo likelihood is root N consistent.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's pretty pretty nice, so it says that whenever the order of the normalizing constant is known to you and it's ordering, and then you get routine consistency.",
                    "label": 0
                },
                {
                    "sent": "But it does not tell you anything of what happens if the log normalizing constant is not ordering, and so our main result.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of our main results is to extend this to show that if if, if you're given a point beta zero an if you look at a neighborhood of that point.",
                    "label": 0
                },
                {
                    "sent": "So if at a neighborhood of a point beta zero, the log normalizing constant has order A in.",
                    "label": 0
                },
                {
                    "sent": "Then the pseudo likelihood estimator is root over a inconsistent.",
                    "label": 0
                },
                {
                    "sent": "So this generalizes show results to all orders of the normalizing constant, and we see some interesting interesting phase transition.",
                    "label": 0
                },
                {
                    "sent": "So if you look at an Ising model on a random graph and where the probability of edge is depending on in, so it shows that if beta is less than one, then you should likely would estimate is 1 /, sqrt P and consistent, and it better is greater than one.",
                    "label": 0
                },
                {
                    "sent": "It's routine consistent.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We see sort of a similar thing.",
                    "label": 0
                },
                {
                    "sent": "If you look at rising model on regular graphs, then if you look at but if you look at the sort of like your estimator then before one it is rude over and over Dion consistent and after one it is root N consistent.",
                    "label": 0
                },
                {
                    "sent": "So for instance if you look at the lattice which is where these constant and for the two dimensional lattice these four it's always root inconsistent.",
                    "label": 0
                },
                {
                    "sent": "But if you look at graphs like the hypercube or graphs where the degree grows to Infinity, you see there is a phase transition in the rate of the pseudo likelihood, which was not known before from shortest results.",
                    "label": 0
                },
                {
                    "sent": "So here is a simulation.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a random graph with N = 2000 and peas into the minus 1/3, and we look at Ising model from beta equal to zero to two and you see that there is.",
                    "label": 1
                },
                {
                    "sent": "So we look at the model and we simulate we calculate the pseudo likelihood many many times and you look at the error bars.",
                    "label": 0
                },
                {
                    "sent": "So we see that at one there is a sharp phase transition.",
                    "label": 0
                },
                {
                    "sent": "So if the theory is correct, before it says that before 1 the rate is into this one six after one through 10 and so you clearly see that there is a sharp transition in the in the in the length of the error bars, which is what we're saying.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The theorem before what it tells you is that if the partition function is a of order N, then you get root N consistent estimate.",
                    "label": 1
                },
                {
                    "sent": "But what if the partition function is of order one?",
                    "label": 0
                },
                {
                    "sent": "So it just says that the estimate is not that super likelihood estimate is not consistent.",
                    "label": 0
                },
                {
                    "sent": "But is there anything else which is consistent?",
                    "label": 0
                },
                {
                    "sent": "So in this regime actually there is an impossibility result which says that if the partition function is order one at a point, then no estimate is consistent.",
                    "label": 0
                },
                {
                    "sent": "And no estimate is consistent in that interval from zero debate or not.",
                    "label": 0
                },
                {
                    "sent": "And this gives actually some very interesting consequence.",
                    "label": 0
                },
                {
                    "sent": "So if you have a sequence of graphs, so I am so just think of G&P when P is fixed.",
                    "label": 0
                },
                {
                    "sent": "If you don't know what it means that a sequence of graph converge, but essentially there is this theory of graph limits which you can use to understand limits of dense graphs.",
                    "label": 1
                },
                {
                    "sent": "And it says that if you have a sequence of graphs, so think of G and half for instance if it converges to some grabbed a blue and if it has maximum eigen.",
                    "label": 0
                },
                {
                    "sent": "If that that that.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is if the graph on W has maximum eigenvalue Lambda one then what it says is that before the before one over the maximum eigenvalue, the pseudo likelihood is inconsistent and after that it is root inconsistent.",
                    "label": 0
                },
                {
                    "sent": "But it says something even more that before the phase transition point.",
                    "label": 0
                },
                {
                    "sent": "No estimator is consistent.",
                    "label": 0
                },
                {
                    "sent": "So think of the example of G and half what it says that at the point.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Before the .2, no estimator is consistent.",
                    "label": 0
                },
                {
                    "sent": "And after two, this little likelihood is routine consistent.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a picture which shows that you have a P on your X axis beta on the Y axis and I and the color is the.",
                    "label": 0
                },
                {
                    "sent": "So we simulated Ising model from GNP many times and then look at I look at the pseudo like your estimate.",
                    "label": 0
                },
                {
                    "sent": "So I want to say whether I can estimate properly so the yellow region is where estimation is possible, red where it is not.",
                    "label": 0
                },
                {
                    "sent": "So in this case I would the phase transition curve predicted is beta equals to 1 / P and you clearly see that of the curve there is a.",
                    "label": 1
                },
                {
                    "sent": "There's a clear, clear transition.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have some applications to study spatial correlation among colors among political colors in the different states of the US in the different elections.",
                    "label": 0
                },
                {
                    "sent": "And so we analyze the data.",
                    "label": 0
                },
                {
                    "sent": "And I have some details in the poster.",
                    "label": 0
                },
                {
                    "sent": "So if you are interested please.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get out, thank you.",
                    "label": 0
                }
            ]
        }
    }
}