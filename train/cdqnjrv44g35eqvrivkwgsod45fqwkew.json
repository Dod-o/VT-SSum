{
    "id": "cdqnjrv44g35eqvrivkwgsod45fqwkew",
    "title": "Generative Models II",
    "info": {
        "author": [
            "Aaron Courville, Department of Computer Science and Operations Research, University of Montreal"
        ],
        "published": "July 27, 2017",
        "recorded": "July 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_courville_generative_models/",
    "segmentation": [
        [
            "So I'm following up what Ian was presenting Ian, I think, presented a fairly good motivation and overall view what I thought I would do would be to present kind of a dive."
        ],
        [
            "Into various frameworks, various ways of doing generative modeling, and I'm going to take a kind of a mid level view.",
            "I'm not going to dive into any details particularly, but I want to give you a kind of a survey, but it just to level a little bit more with a bit more detail than Ian was presenting.",
            "So I'm going to cover a little bit less ground, but at a bit more depth, and I think that's probably the well.",
            "What I thought would be best served by you.",
            "So this is a slide at least to an amalgamation of slides that even presented.",
            "It's just motivating.",
            "Why again, are reminding you why.",
            "Again, we want to do generative modeling.",
            "So sometimes we want to do density estimation.",
            "Sometimes we actually care about generating samples.",
            "Yahshua again don't be fooled.",
            "These are not true samples.",
            "So right so?",
            "So what I'm going to be talking about specifically?"
        ],
        [
            "Is a few separate models that some of which were mentioned by Ian.",
            "I'm going to touch on auto regressive models and specifically I'm really going to concentrate on the Pixel CNN.",
            "My reason for choosing that one is because I feel like it has the most potential to be built upon.",
            "We've built upon it and others that have as well.",
            "It has some nice properties which I'd like to describe.",
            "Then I'm going to dive into latent variable models.",
            "I'm going to talk about the variational autoencoder and I'm going to talk about one.",
            "This is really just one improved strategy.",
            "Or for inference in those models, and I'm actually going to mention something else that's not here.",
            "Which is work we've done on improving the decoder of the VA. And finally, I'm going to touch on if time permits, generative adversarial networks will cover the basic Gan I'll talk about Wasserstein Gan some little bit of work we've done on improving the Washington gang or proving training in Washington and I'm going to touch on a model we have called Ali.",
            "Adversarially learned inference, so it's a lot of material.",
            "I'll get as far as I get.",
            "I welcome your questions.",
            "Slow me down if I'm moving too quickly and let's see how it goes."
        ],
        [
            "Alright, so autoregressive generative models.",
            "They basically you know Joshua presented these when he talked about recurrent neural Nets there and mostly he talked about in the context of sequential data, right?",
            "So, language modeling and other sorts of time series when I'm going to be mainly focusing on today in the context of the Pixel CNN is how to apply these two images.",
            "You can apply these to lots of other things, but mainly what I'll be talking about our images.",
            "So just to remind you autograph as models.",
            "Have a graphical model structure that looks a bit like this, so X is.",
            "You can decompose X in.",
            "The conditionals are basically given like.",
            "This will get.",
            "Will make this more explicit as time goes on, so a little bit of history, so this work sort of got started by Brendan Frey in the 90s using logistic regression for conditionals.",
            "The Bengio brothers generalize this using neural Nets for conditionals.",
            "And then Nate came along in 2011 by work by Yuval Ashelin, Ian Murray.",
            "Which basically incorporated a lot of the same ideas, but using weight sharing as well.",
            "And this is sort of the first model that really went for trying to model images, autoregressive models, trying to model images.",
            "As far as I know, from there we've had from since then we've had quite a bit of an explosion of models, deep made pixels, RNN pixel, CNN, Wavenet, video pixel networks.",
            "All of these have come out fairly recently.",
            "Again, I'm going to be concentrated on the Pixel CNN, but I really do encourage you guys to look at wave Nets and the.",
            "Video pixel network are pretty interesting models and their performance is very very good."
        ],
        [
            "So again, so for in the context of learning images or learning generative models of images, what we need to do if we want to build an auto regressive models, we need to pick an ordering over these pixels, right?",
            "And that can be somewhat difficult because there's no natural ordering to a set of pixels, but we need to have a probability joint probability distribution over pixels hearx that comes in this form right?",
            "And so some of the nice properties of these autographs models is that typically P of X is tractable, easy to.",
            "Easy to compute, easy to compute likelihoods for so they have that nice property, a con and this is really a question of some debate.",
            "Whether or not you think it's important to be able to extract a latent representation.",
            "So in the later part of the talk.",
            "And really where I'm going to be spending most of my time is talking about latent variable models because my eye and a lot of other people in our community believe that it's really important to be able to try to extract these latent factors of the model.",
            "You can think of these as maybe causal factors, but that might be pushing it a little far in terms of the interpretation of what these things mean.",
            "Auto regressive models do not have this interpretation, and as we go on maybe we'll see a little bit."
        ],
        [
            "But OK, so diving into the Pixel CNN, the approach here that the Pixel CNN uses to model this joint probability distribution.",
            "The ordering it's going to pick looks a bit like this, right?",
            "So imagine this is the full image.",
            "It's going to predict some pixel here based on the values of all of the subsequent pixels starting in the top left corner.",
            "And sort of scanning like this, this raster raster scan like this.",
            "And it's going to do something else that's a little bit different than most other generative models of images is.",
            "It's going to be for this particular value.",
            "The pixel value here it's actually going to predict a multinomial overall of the two 56 values that that value could take.",
            "If you are using a quantized representation of the pixel value, it's going to actually build a multinomial, so most of the time what we use is something like a Gaussian distribution, right?",
            "Or something along these lines where it's just picking a single real number.",
            "The fact that it's doing this multi nomial allows it to sort of have multi.",
            "Multimodal distributions, which is pretty interesting, and they're carrying this idea of autoregressive."
        ],
        [
            "Two pretty far extent there, even assuming that they have structure within the across the color channels, right?",
            "So this is the way it looks.",
            "They decompose for a single pixel value.",
            "Here it's going to be decomposed into a red channel, which is not conditional.",
            "Anything else.",
            "And then the Green Channel, which is conditional on the Red Channel and all of the subsequent pixels and the blue channel is conditioned on that pixels.",
            "Red value that pixels green value and all the subsequent pixels.",
            "And as I mentioned before, they have these multinomial distribution so you can get a sense this is.",
            "The plots they provide in their paper showing what kind of variability you can see in that kind of distribution, yes.",
            "There there isn't.",
            "There you're absolutely right, this is just the chain rule.",
            "At this point, they're not assuming any additional structure.",
            "I mean, they're picking and ordering, but there's no actual assumption of difference, so picking and ordering some orderings are better than others.",
            "For depending on the natural data.",
            "But yeah, this is a good point.",
            "I should mention this is a bit of a detail, but they use two different masks to implement this.",
            "The important thing to remember here is that they maintain this.",
            "This kind of factorization.",
            "Over over the inputs.",
            "So this is just a way of doing that through these these two masks."
        ],
        [
            "So what's interesting about the Pixel CNN?",
            "I guess what?",
            "What is exciting from my point of view.",
            "Is that it actually is able to learn relatively quickly?",
            "A lot of these auto regressive models.",
            "The problem with them and this also includes, by the way, that the Pixel CNN is that you basically have to learn by going pixel by pixel over them.",
            "Nate had this problem as well and that's really quite slow, specially if you want to scale this thing up to like 256 by 256 images or bigger than that, the pixel CNN.",
            "It exploits the fact that you can use mask convolutions that look a bit like this guy here.",
            "This would be a mass convolution of size 5 by 5.",
            "Typically you would use smaller than this, like 3 by three would be the typical.",
            "Receptive field size.",
            "But you use this kind of mass convolution where this is the pixel you're concentrating on right here and it's mast out itself because it has to be auto regressive.",
            "It can't depend on its own value, and then they're basically exploiting the same ideas that are in maid.",
            "I didn't have time to present made to you, but this is basically excuse me.",
            "A masked autoencoder and what they do in made as they basically build the structure of the model to ensure that subsequent pixels are only dependent well.",
            "Subsequent values only depend on previous value, so.",
            "Enforce in ordering and then they build these masks to ensure that that ordering is maintained as you go from layer to layer in the network.",
            "What pixel CNN does is build this into the context of a confident.",
            "So you've got this kind of.",
            "You're going to impose this kind of mask structure and what that allows you to do.",
            "Once you have this mask structure is trained essentially in parallel, so you don't actually have to go pixel by pixel, at least during training you can.",
            "You can use essentially teacher forcing, which is just maximum likelihood and use the ground truth values and you then just predict the actual values at the output condition on the ground.",
            "Truth values of all the subsequent pixels that went before, so you don't actually have to have computed them beforehand 'cause you're using the ground truth values.",
            "So that's the sort of the key insight I think the Pixel CNN into how they're able to train fast, and so it's able to scale.",
            "The tough part is that for generalization for sorry for generation, it's still a rather slow operation because there you don't have ground truth values, so you actually have to wait and do some pixel by pixel.",
            "Questions on that.",
            "OK.",
            "Play."
        ],
        [
            "Do stop me 'cause I'm going to.",
            "I'm going to race to the end of this.",
            "So if I'm going to let me know.",
            "OK, so this is just another view of what pixel CNN does.",
            "We have our mass convolution on the input layer.",
            "Let's say here this is just imagine this is just some layer and the important thing to note here is that you're applying this mass convolution at every layer to maintain this dependency, this autoregressive dependency across the entire network.",
            "So this is a mass convolution and this value here say for example is connected to this value.",
            "The the image size stays the same and so this pixel basically is masked out at this point, and it's not dependent on any pixels that are subsequent to it according to our ordering, which is from top left across the image and you compose multiple layers like this to get to get a bigger receptive field so that each pixel eventually in the output is actually dependent on maybe all pixels before it that occur in the ordering, right?",
            "Even though your initial receptive fields are quite small, you're getting the context.",
            "The wider context by stacking.",
            "Multiple layers of this now that would be true except."
        ],
        [
            "The problem which is in the original publication of the Pixel CNN they make note of this.",
            "In there they're following follow on work is that there's actually a blind spot that creeps in here, and that's because if you imagine having these kinds of mask operations, you see that that this guy right here.",
            "Is never actually doesn't actually get incorporated into this.",
            "This is like imagine if you have a three by three window, right?",
            "And this guy also is it doesn't see this guy here.",
            "This guy doesn't depend on here.",
            "So what you end up with is this big blind spot that occurs over here for this pixel, which is not necessarily good.",
            "So what they did."
        ],
        [
            "To fix this problem is they basically decompose this into two stacks of convolutions, so one which is unmasked, which is their vertical stack, and it's basically just a normal convolution.",
            "But here it's just looking at all the pixels in the Rose above the current role that you're in in the image.",
            "So this is essentially unmasked here, so you get the standard receptive field growth that you see as you go up in layers of a confident, and that's sort of represented in the darkening blue.",
            "Going from out to in here, so this is sort of local context and then one layer context to layer context etc.",
            "And then we have this other stack which is this horizontal stack which is maston.",
            "Here it's only dependent upon the layers that are the elements in the current row so far, and there could be a receptive field there too that you would grow again.",
            "With layers with except with exceeding subsequent layers of the CNN.",
            "Yep.",
            "And then.",
            "How?",
            "GoDaddy doesn't work.",
            "You go back for here, but this is not easy, yeah?",
            "And then, well, it wasn't working.",
            "I mean, would you do that?",
            "How did they notice this this thing?",
            "Yeah, it's a good question and the answer is I have no idea this is in our work, so I don't know if Nando you are here.",
            "Do you know how?",
            "They yeah, do you know how they notice this?",
            "Start.",
            "Yeah.",
            "I see OK, so right so this is this is from the like the Pixel RNN like work.",
            "They sort of OK that makes sense, right?",
            "The question was how do you notice?",
            "How did they notice that this actually is a blind spot to the model like that piece?",
            "And it's not actually that complicated once you see it right?",
            "It's kind of clear that that would happen, but what Nando was saying was that was that that maybe it was through looking at the Pixel C and a pixel RNN work where where you're basically explicitly building up dependencies both in both directions.",
            "So I'm not going to be talking about that in any particular detail, But yeah, so you basically in that case what you do is you run.",
            "There's different setups, but you basically can run two RNS.",
            "Won't going both ways towards that pixel, and so then it becomes a little bit, maybe more clear that you have that, yeah.",
            "I think I will skip this part, but there's many ways to do this.",
            "Yeah, yeah, yeah, certainly.",
            "Good point, yeah, so so this solution is actually pretty nice though.",
            "You could also imagine other solutions to this that stays in the same.",
            "For example, you could imagine keeping the same structure here, but then just having the other stack be something that carries on in a rasterized row like fashion.",
            "The problem with that approach would be that these pixels here, which are relatively close to this one, would have to go right to the end and then come back through.",
            "And so that's not a very good solution to that.",
            "This is much cleaner because you end up with because of the receptive fields you end up with the sort of close things having the strongest connection.",
            "So I think it's a particularly elegant solution.",
            "This is a good idea.",
            "Oh, like.",
            "So OK, right?",
            "So I think the question is why do auto regressive models over images at all?",
            "Is that fair?",
            "Yeah, it's well, I think I mean, one reason is that you end up being able to concentrate on very well on local textures, right?",
            "Because you can have the dependencies, it happens that an image is you have a strong dependence, local dependency between pixel values and this kind of model can capture that in a way that's a little bit more difficult from your standard latent variable model, where you have a bunch of latent causes and.",
            "In the image below, so that would be my stock answer to that would be that it's actually works well because it captures something about the natural structure of images.",
            "Now the ordering is somewhat arbitrary and it's kind of unfortunate that you have to pick an ordering, but that's just the way it is for an auto regressive model.",
            "What you get in return is a tractable model that you can train relatively efficiently, and you can evaluate likelihood on which is something.",
            "That's really nice because a lot of our other models like Gans, for example, we have really no good way of evaluating them, so a lot of the things I'm going to be showing you today, I'm not going to be.",
            "Showing you numbers, I'm going to be showing you pictures because we don't have great ways of getting reliable numbers from them.",
            "But with these kinds of models you can actually evaluate them.",
            "Tractably with using just likelihood so they do have strong advantages any other yeah?",
            "Change.",
            "Embarrass the order weather in the in the in the images yourself or in the in the architecture so I would would it change no, no, I don't think so.",
            "I mean, I think I think images I don't like in the sense of if you started at the bottom right corner and then just kind of went in the opposite direction.",
            "I don't think images probably have the property where that's about the same.",
            "Yeah, I don't think there's any, but you probably don't want to do is have some sort of crazy ordering like just random pixels here in there.",
            "You probably don't want to do something like that, but that's more that could also be for computational reasons.",
            "In your component.",
            "OK, I'm going to move on right?",
            "So this is another thing that they did with this paper, and it turns out originally when I saw this, I thought that maybe this was kind of what they really wanted to do was just fix this blind spot problem, but they thought they needed a little bit more in the paper so they did this."
        ],
        [
            "Colby complicated structure here.",
            "This is like they built this gated structure, but it turns out this gating actually helps a lot, so an it's reminiscent of a lot of more recent work on kind of these structures of gating computations.",
            "So what they have here is they keep these horizontal stack vertical stack in two separate lines, so you can imagine that you're kind of running two separate confidence throughout the Model 1.",
            "Doing these horizontal computations in sort of the blue here and the other doing the.",
            "Sorry vertical in blue horizontal in this purplely pink kind of color here and then.",
            "What they do is they actually have this gating structure here so they go through a 10 H. But this 10 H is essentially gating this signal.",
            "Sorry the sigmoid here is gating this 10 H structure, so you go through this 10 H, go through the sigmoid, multiply them together, and then that's the output of this thing and we also have a dependency now between the vertical stack and the horizontal stack, but it's in such a way that it doesn't.",
            "Again, it doesn't break this auto regressive.",
            "Structure of the model they have to maintain that because what happens is if you end up having some leak and this can happen.",
            "Actually if you're not.",
            "If you're not careful about how you code this thing up, you can have some leak and so that the pixel value XIJ will actually end up being in the computation to predict its own value.",
            "And So what could happen if you have that that happening is your model could look like it's doing really, really well, right?",
            "It can get great likelihood because it's essentially cheating, but when you go to actually generate from that model, it's going to be terrible because that information is just not available.",
            "Or you might.",
            "In fact, it might actually probably, depending on how you set it up, it probably just give you a bug 'cause it's not there.",
            "Alright, so."
        ],
        [
            "Some some results I mentioned earlier to the question of why do we do these things?",
            "One of the reasons why was at the time it was giving us pretty good textures compared to other models that are out there.",
            "So this is an example of the kinds of things you see with coral reefs."
        ],
        [
            "So it's not great for global structure, but it's actually really good at getting local local texture local structure well."
        ],
        [
            "These are sort condition on sorrel horses.",
            "This is another thing that they did with this.",
            "This work was they built in conditioning so the way they did that was just inject the label information throughout the network structure which is something I actually recommend if you in fact we're playing with different ways of doing this kind of conditioning now, but it seems like it's a very good idea to sort of bring that kind of label conditioning information throughout the structure of your model so you can see that we're actually seeing some reasonable structure of horses.",
            "You know, some of these look very good indeed.",
            "You could actually imagine that there's they're relatively small.",
            "These are only, I think, 64 by 64, so a lot of the seeming sort of maybe unrealism is due to that."
        ],
        [
            "We have sandbar."
        ],
        [
            "And here's what I was saying earlier.",
            "We get relatively good likelihood numbers out of this, so we can actually evaluate this model.",
            "This is on Cifar 10.",
            "This is compared to a bunch of previous models, including the original pixel CNN model.",
            "So this is the one that had the blind spot.",
            "By incorporating both the fixing of the blind spot with these two stacks, and then their gated operation, they were able to go from 3.14 test negative log likelihood down to 3.0103.",
            "So a relatively significant improvement.",
            "It's hard to interpret likelihood numbers in terms of sort of what sort of quality we would imagine out of these models, but it's it's an impressive improvement nonetheless.",
            "You see, it's still not quite as good, but roughly in the same category of the Pixel RNN here, which again it's a very good model that has the deficiency of being very slow to train.",
            "So if you're not deep mined with GPU farm, it might be it's actually relatively difficult to work with.",
            "Yeah sorry yeah.",
            "So the gating here with my backup."
        ],
        [
            "So the gating here is basically just another way of using, so I guess I would say the intuition is that you're basically using some units to essentially kind of turn off and on other units.",
            "Here I guess in this case it's a bit of a weird structure.",
            "I think I prefer to look at this as just another kind of non linearity that has extra parameters to it, right?",
            "So we did work, I don't think it's been presented yet, but we did some work on something called Max out which was a non linearity that had.",
            "Lot of parameters to it, and in some cases that actually worked out very well.",
            "I don't know if somebody would care to jump in for this, but my interpretation of this is that this is a little bit what this is because you're not conditioning on any extra information at this point, right?",
            "It's the same input coming in from below, so it's just another form of non linearity.",
            "Typically when we have this kind of gating operation, it's because you're actually using it as a means of injecting other information, but that doesn't seem to be the case here.",
            "OK.",
            "So I mentioned last time that the problem with the Pixel CNN was that the IT was actually quite slow to generate from and one question would be will can you speed that up?",
            "And there's actually been some interesting very recent work from Nando, among others on this topic."
        ],
        [
            "And what they use is basically a strategy of predicting a few points independently and then using the local context to fill them in.",
            "So there's an ordering now, but it's sort of over regions here, so we sample up these guys independently.",
            "Then we fill in the local neighborhoods and then fill them in again and fill them in again here.",
            "And if you do that, you see."
        ],
        [
            "This kind of operation, so here we have our independent samples, and then you're sort of filling them in based on the local context and you can go all the way to 256 like this and you get pretty good quality generated samples.",
            "OK, so that's going to be it for autoregressive models, so maybe I'll just take a pause now and ask if there are any questions on auto regressive models before we move to latent variable models.",
            "Yeah?",
            "How do we measure like performance, so this is."
        ],
        [
            "We're using log likelihood here, so that's essentially how we're training it as well, so we're trying to predict I mentioned before that we've sort of these pixels are decomposed into multinomial's, so every pixel value has a certain value between zero and 2:55, and you're predicting the likelihood of that particular pixel value.",
            "In fact, the Green Channel or the Red Channel, or the blue channel of that pixel value under the distribution of the model that's predicting it given all of the subsequent pixels before then.",
            "And maybe all of the other color channels.",
            "Up to that one, depending on what color channel you're using your predicting, and then you just evaluate that likelihood and so this is telling us on test images, how likely is a particular test image under this model.",
            "Yeah.",
            "There's like no ordering on the pixel density.",
            "Sorry if it uniform distribution.",
            "Sorry it's a multinomial, yeah, then it's if you assume independence across pixels.",
            "Yeah, there's no ordering, so it's basically.",
            "The values are treated.",
            "The intensity is not ordinary.",
            "That's right, that's right, that's weird, right?",
            "So you would, yeah, that's a good point.",
            "So you would think that I mean the way they did this.",
            "If I back up aways here."
        ],
        [
            "To this plot here, right there using multinomial distributions here.",
            "So there is no sort of notion that a pixel value of 255 is close to 254, right?",
            "These are basically seen as completely independent, so you would think that that would be problematic.",
            "There are distributions that you could build in, like the binomial, for one.",
            "If you normalize this, but there are value distributions that could incorporate this kind of order.",
            "This carnality, but.",
            "The, but it seems like from these results that maybe you don't need it right, and So what you gain is the ability.",
            "This flexibility of having multimodal distributions that you can see in this case, right?",
            "So this I believe it's not shown here, but I believe this is like for the first pixel and you get this strongly bimodal distribution.",
            "It's a little hard to see, but there's peaks right at 256 and at zero here and then later it becomes more like a sort of a single mode, but this would be kind of hard to capture with one of your standard.",
            "Distributions that capture or anality not impossible.",
            "You could construct something, but it doesn't seem like they lose that much by sort of losing that property of carnality.",
            "OK, so now."
        ],
        [
            "We're going to turn to.",
            "Latent variable models.",
            "So so as I mentioned before, we myself included, there's some people, and the specific latent variable model that we're going to start with is the autoencoder model that was sort of Co developed in two different groups, one at by Dirt Kingman Max Welling, who is here and one by the folks at DeepMind, roughly around the same time independently.",
            "And the idea behind this model, well, it's sort of a first.",
            "I'm just going to talk about latent variable models in general.",
            "And one of the things you would like a latent variable model to do is map you imagine a set of features, a latent feature space here that is somehow simple that incorporates information that you want in a kind of a dis entangled way.",
            "And then you've got a function that Maps that into your data space that is somehow complicated, right?",
            "Like like the natural manifold of images is going to be something quite complicated, but you'd like the features here to map to these to the actual data points here in a way that is coherent.",
            "And models like the variational autoencoder go some way towards doing this kind of thing, so here's an."
        ],
        [
            "Sample it's a bit of an update dated example at this point, but this is my body from the original via paper, so this is a case where they've taken 2 latent variables an mapped abunch of pictures of Brendan Frey and what they've learned from just this model.",
            "That's just takes 2 variables and tries to generate a face of Brendan Frey they've learned to dissent.",
            "Angle, pose information that's represented on the X axis.",
            "Here the Zed 1 axis.",
            "It's labeled an expression, right?",
            "So in expression, he's going from sort of a serious face down here to a smile and then pose.",
            "You see slight rotations of the head.",
            "And that's very nice.",
            "It's very powerful that we're able to learn to the kind of disentangle these factors would really like to be able to learn models that are able to do this in a little bit more of a complicated setting than what we have right here.",
            "But it's essentially one of the goals of what latent variables are, and to be able to potentially use these latent variables for things like semi supervised classification or something like that.",
            "This is another example for Emnace.",
            "Now Amnesty doesn't quite have these nice, because it's data that sort of naturally clusters.",
            "It doesn't quite have the same sort of factors quite as easily, but you can see.",
            "If you look the ones, for example, that you can see some compose information being decoded here along the ones and then sort of different regions seem to represent different digits."
        ],
        [
            "They even talked a little bit about these VA models, so I'm going to go fairly quickly over the intro to them.",
            "But the idea with these is that this is what we want to do.",
            "We want to maximize likelihood of X, right?",
            "We want to maximize this probability on X and we want to use these latent variable structure.",
            "So we're going to impose these latent variables.",
            "But we don't want to maximize this joint explicitly, really, we're interested in maximizing the likelihood of the X and the way we're going to decompose this in this joint distribution as sort of represented by this picture is that we're going to have some some P of X, something.",
            "Usually it's quite simple, like a Gaussian distribution on zed.",
            "Sorry that's PM said.",
            "And then we're going to have a conditional distribution P of X given zed, which just is the projection.",
            "So you sample from some P of zed here and then you project that into the data space here, and that's going to be done through this conditional P of X given zed.",
            "So like I said, people says something simple and this is our G function here.",
            "And.",
            "Yeah, and the issue here for us in our in the context that we're concerned considering this in the with the variational autoencoder is that we want to learn a deep.",
            "Neural net basically to the map from Z to X.",
            "We could use similar models.",
            "This is one of the places where this work is innovative is that we have.",
            "We've had simple models where, let's say a linear model mapping from X to Z, and we know how to do inference in that structure and how to learn in that structure.",
            "But in this work we're really interested in harnessing the power of deep neural Nets to map a complicated mapping right from something simple in zed space to something complicated in X space here.",
            "So the problem with this setting is that how do you learn such a model?",
            "If we were doing something like if we actually knew what Zeds corresponded to each X?",
            "If we knew the settings of zed that corresponded to a particular image, a particular data point that we had, this would be straightforward.",
            "We could just do supervised learning, right?",
            "And we would be done.",
            "The problem is we don't have those sets, right?",
            "These are latent, so by definition we don't have them.",
            "We actually are trying to discover them in the process of learning.",
            "So what do we do?"
        ],
        [
            "Well, and this was mentioned by by Ian.",
            "I think 2 days ago.",
            "So what we're going to do is actually use a variational approach and I think maybe Max is going to talk a little bit about this.",
            "I'm not actually sure how much he's going to talk about this, but we're going to use a variational model to define a variational lower bound on the likelihood of the data and this variational lower bound just looks like this.",
            "And we're going to invoke an approximate distribution here, so Q of zed given X is our approximate posterior distribution.",
            "And we're doing this because if."
        ],
        [
            "Go back to the original model if we can ask questions like what is P of X given zedan this model parameterized is that function, so that's fine.",
            "We know how to compute that, but if we were to ask the question, what is P of zed given AX, we should be able to ask that question.",
            "The problem is, is that that's a that's intractable to come up with.",
            "What that distribution is, you could do something like Monte Carlo sampling to determine approximation of that, but that's generally slow.",
            "So."
        ],
        [
            "So what we're going to do instead is invoke an approximate posterior distribution Q of zed given X, and we're going to do so in the context of this variational lower bound here.",
            "So if this Q of X actually ends up matching the P of X given set our true posterior, this bound becomes tight, meaning that this lower bound actually matches the log likelihood.",
            "And we know that we're actually maximizing the proper likelihood, so so this lower bound it can be decomposed into a different a few different sets of parameters, one which is essentially over this Theta here, which is over the generative model that we've talked about so far, right?",
            "So that's the models of our neural net here, and maybe the parameters of our Gaussians, or simple prior distribution.",
            "But usually we assume that to be fixed.",
            "And then we have Fi here, which is our parameters over this guy.",
            "So typically what's been done with the standard thing to do with variational models.",
            "And this is again you talked about.",
            "This is to look at is to just consider the variational parameters directly.",
            "So this is kind of a nonparametric model where you would have an optimization process for each X.",
            "So you would solve.",
            "You usually assume that there's some factorization over zed here and you would solve for these parameters of the zeds individually for each one.",
            "Given X and there's an optimization process for that, that's also relatively slow because it requires an optimization process in the inner loop.",
            "But in, but what we're going to see with the variational auto encoders, we're actually not going to do that.",
            "We're actually going to use a neural net to encode this posterior distribution as well, and we're going to train them together, and we're going to see how we can do that.",
            "But first, this is sort of a non standard representation of this.",
            "Of this.",
            "Lower likely this lower bound, and it's just interesting, because what we can do is rearrange terms like this and come up with something that looks like a traditional reconstruction term here, right?",
            "This is just.",
            "Expectation under so we sample is that we get our zed from our approximate distribution which I haven't talked about too much about how we get that approximate distribution yet.",
            "But let's imagine we can get that.",
            "And then what we have is this is just a standard log likelihood reconstruction.",
            "So if this distribution was a Gaussian distribution, we heard from Dina that this is just essentially mean squared error right on the input trying to reconstruct the original input through zed here.",
            "But what what's left over here is something that we might be able to interpret as a regularization term.",
            "And it's interesting to look at what this is doing.",
            "This is saying that we want our posterior.",
            "To become closer to the prior distribution here right now, I want to emphasize this is not the standard KL that you see when you do variational methods.",
            "The standard Cal you typically talk about when you're doing variational approximations is the KL between your approximate posterior and the true posterior.",
            "So the big difference is that that Cal you actually want to go to zero.",
            "This KL.",
            "There's really no reason that you'd want this to go to zero, because if this goes to zero, that means that your posterior encodes no information about the data point.",
            "Which is kind of this kind of means it's meaningless, right?",
            "It's not really a posterior distribution.",
            "It's there's no information there, so this is more you can think of this as as a regularization term, right?",
            "Just like when you put in, like weight decay on on your neural net, right?",
            "You don't actually want all your parameters to go to zero, even though you're encouraging them to go to zero.",
            "That's what a regularization term does.",
            "So think of this regularization term in very much the same spirit.",
            "OK so I'm."
        ],
        [
            "Ocean before that we're going to use a neural net again to approximate this posterior distribution here Q.",
            "So that's in fact what we're going to do.",
            "So this is our objective function, and we've got a neural net that's going to approximate our posterior Anna neural net.",
            "That's going to be our generative model.",
            "OK, so that's fine, but the one thing I haven't told you how we're going to train this model and what we're going to use is something called the perimeter is Asia."
        ],
        [
            "Trick, so this is really just a different way of writing the same thing.",
            "It's not particularly new, but it's it's a novel application of it, and it's actually a really important.",
            "It was a big big step in our field to be able to parameterise Ed as a as a mean plus a sigmoid, which are both a function of XN plus a noise term.",
            "Here.",
            "Now what this does, it allows us to isolate the stochastic element here into this term here that has no parameters.",
            "And so now we can think about back propping through from the.",
            "From Zed here through this model, which now now our posterior approximate posterior.",
            "Now it's outputs this meenan.",
            "This Sigma, which are both deterministic functions of X and our generative model, can once we fold in that noise.",
            "Propagate to the input, but it's still a deterministic function of this mu and Sigma here.",
            "So in this part here, like how you encode XI picked one way where we actually do the same similar trick where we encode amuen a Sigma as a function of Z2 in Codex.",
            "That's not actually necessary.",
            "You can imagine that the noise on the input is just constant.",
            "So for example Sigma is is just a sigmas value one for example or just just some constant value, not necessarily a function of said.",
            "Alright, so the questions on that yeah.",
            "Sort of estimators that risk Adas City of the model is that what that saying this is?",
            "Oh yeah, sure, sorry I should really go into so this is just we're assuming that this is a Gaussian distribution, right?",
            "So yeah, I'm sorry, I really should have mentioned this before.",
            "We actually it's written here, right?",
            "So we're assuming that we are parameterizing a Gaussian distribution.",
            "In fact, the suggested distribution with the diagonal covariance?",
            "So we have a vector here mu Anna vector here of Sigma just for all of the elements of zed.",
            "An here we're just making each of these two terms be a direct function of X through our encoder network, right?",
            "So this is an auto encoder right?",
            "Variational autoencoder?",
            "This is our encoder model.",
            "This is now our decoder model, so our inference machine is our encoder and our generative model is our decoder.",
            "And so we just take these two parameters and we can add the noise.",
            "And now that basically set that equal to Z and then we propagate that down and we evaluate the likelihood and we can back prop through."
        ],
        [
            "So learning in this model now becomes a simple matter.",
            "With this premise prioritisation trick of we started X, we forward propagate to get that Sigma and mu we add some noise.",
            "Here at Zed we forward propagate some more until we get to X.",
            "We compute the reconstruction and we back propagate and our objective function here.",
            "This is our reconstruction component and then add said here.",
            "We're actually also regularising zed so it doesn't collapse and it says we're putting pressure on it to be close to our.",
            "Prior distribution alright yeah.",
            "Well.",
            "So that's a good question that that is the."
        ],
        [
            "Whole point of the pressurization trick, because we have we've isolated the noise here, right noises now there's no parameter, so the stochastic element is now isolated, so XR reconstruction at this."
        ],
        [
            "Point here is a deterministic function of this X.",
            "There's just noise injected here so we can back propagate through this entire model.",
            "And it's done in such a way that it's actually like, unlike because of this regularization term, right?",
            "I mentioned before that in the context of the auto regressive model.",
            "If you had like the same pixel depended on the same pixel here, that would be essentially cheating.",
            "It's like you would have a bug and you would get terrible samples, but because of how we formulated this and we have this regularization term here, that doesn't happen in this case."
        ],
        [
            "So these are the kinds of samples you see when you use a standard V model.",
            "At this point in time these look not so good, but they were actually fairly impressive at the time, so it's we've been making a lot of progress very recently, but it was actually a fairly significant breakthrough, even just in terms of the quality sample.",
            "But you see that it's actually quite blurry here, right?",
            "And we're going to talk a little bit about why that might be in a bit."
        ],
        [
            "But another property that I actually really want to get across here is what happens when you trains up such a model.",
            "So this is a model with what we see what we call component collapse.",
            "So what we plotted here, this is the latent variables Ed, and these are just samples of the input samples.",
            "Well of the output of the generative model.",
            "So this is on M NIS, so these are samples from the model and these are sort of typical histograms of zed across a bunch of samples.",
            "And what we know what's being plotted here is the KL divergences for those samples.",
            "So the first thing to notice is that it's sparse, right?",
            "What that means is that for a lot of these components the KL divergences actually 0.",
            "So if we go back."
        ],
        [
            "To the KL divergent Sturm.",
            "Here, this term right here collapses to zero.",
            "There essentially not being used to encode anything about the data."
        ],
        [
            "And what's interesting is if you dig into the model, the output parameters, those weights that are associated with that dimension of zed go to zero.",
            "Also because at that point it just becomes a source of noise, right?",
            "Because it just matches the prior so that the model just says I'm just not going to use that, so it's interesting there seems like there's this property of picking the dimensionality, the sort of natural dimensionality that it needs to use in zed space to represent the data.",
            "And another interesting point.",
            "So so that's kind of cool and you think, well, that's awesome.",
            "It's finding the correct dimension of some sort of inherent dimensionality of the data.",
            "But the story of course isn't going to be quite that simple, because if you just build in a bigger model with a deeper encoder and a deeper decoder, you get some."
        ],
        [
            "Thing like this for the same data, right?",
            "It's it's putting the model.",
            "This KL divergent puts pressure on the model to use as few components as possible.",
            "So if you give it, the capacity is going to have unwrap it or put more information into fewer fewer dimensions.",
            "You can see that even though we're using fewer elements of this of the fewer elements of the zed vector, we're actually getting better likelihood.",
            "Earth well, better samples.",
            "I imagine better likelihood as well.",
            "So these are the dimensions along Z, right?",
            "So we just started with a dimension, so this is our we have a vector of Z right?",
            "Which is our Gaussian distribution.",
            "Diagonal covariance Gaussian distribution and it has 80 components.",
            "We're starting training this thing with 80 components and then as training progresses more and more of these kind of die away, which means that they just go to zero.",
            "This means that this is.",
            "This is again what we're measuring.",
            "Here is the KL divergent's, so it's the prior.",
            "The posterior distribution is just matching the prior, so it's not encoding anything about about the latent variables.",
            "Or sorry about the data, and so all of the information about the data is being compressed into these.",
            "What 7 components in this case?",
            "Other questions on that, yeah?",
            "How do you pick the dimension of said good question so so you don't?",
            "I mean, you just you can try a bunch of different dimensions.",
            "One of the points here is that it's sort of picks.",
            "As long as you're big enough, it's going to pick some subset of that.",
            "Sending them to 0.",
            "So that's actually a relatively nice property of the model.",
            "You can, but that probably wouldn't be a very good idea because just computationally.",
            "Yes, one of the things we found working with this model in particular is that if you if you sort of say like imagine you take this model and you say OK, well, it's found seven components, so I'm now going to start with seven components.",
            "You know there's optimization issues, so it doesn't always actually find as good a model with just those seven components, so it's kind of it likes having sort of space to work, let's say and to finding a small number of components.",
            "There's another question was there over here?",
            "So everything we've done here assumes its diagonal, right?",
            "It's the our posterior is diagonal, so meaning that all the elements are independent in the posterior.",
            "So the posterior so conditioned on X, the elements of better independent will come back to how we can address that in a second.",
            "But in general, no, like one of the reasons why you want that property is just scalability, right?",
            "If you're this is an M NIS, so we don't have.",
            "We can get away with relatively few components and then maybe we could build in a full covariance matrix, but we're not really ever targeting emnace, right?",
            "We want to scale these things up to larger inputs.",
            "In that case, using large covariance matrices is a lot of parameters to predict, and we typically don't see it being worth it.",
            "In that case, yeah, so, but there are interesting new work that actually touches on this problem of independence of the posterior that will get to in a second, But the first thing I want."
        ],
        [
            "She was just very briefly on how you can integrate some of these models together, so this is a model called the Pixel VA, where we've taken at the VA model that I've just talked about and we kind of graft in components of pixel CNN between so this is a hierarchical model now.",
            "So what we saw before was just one layer of Zed mapped to X, right?",
            "But there's no reason at all.",
            "We can't use a hierarchy of Zeds an here what we're going to use is actually pixel CNN to encode this distribution from one layer to the next.",
            "Right, so now we can start instead of having a simple prior here at this.",
            "At this variable, we can encode something actually quite complicated.",
            "This is actually going to be quite a long way in towards fixing this is purely in the decoder for the encoder.",
            "We use very traditional what we just talked about methods, so our posteriors are all factorial and so if we do this we can see that."
        ],
        [
            "We actually get pretty nice samples from this model, so here's a 64 by 64 L Son bedrooms and this is image net and we also may be getting something a little bit more object oriented than what you see in the sense that there's clear objects than what you see in a traditional pixel CNN.",
            "But what's really interesting about this model is, and This is why we're interested in integrating Pixel CNN's with with something like the variational."
        ],
        [
            "Variational autoencoder is that we now have control over these latent factors, right?",
            "We can actually look in and see as we manipulate these latent factors.",
            "What's being represented at these layers of the hierarchy.",
            "And that's what I'm showing right here.",
            "So at the top level of the hierarchy, what we're doing here is, we're just sort of sampling around twiddling different values at the at the top level and seeing what kinds of different samples we get.",
            "We do the same thing here, holding the top level constant, and we twiddle, we sample different values at a mid level of the hierarchy.",
            "And here we hold the top level and mid level constant and we sample.",
            "Just the very low level pixel CNN values, so this is the sort of the injection of the noise that's responsible purely by the lowest level pixel CNN and what you can see I think, is that this is actually kind of hard because it's hard to parse these images from far away, but it seems like global structure is largely being incorporated by this.",
            "So what I mean by that is these are bedroom scenes, so it's the room layout varies a little bit as you change the values of the latent variables at this stage right?",
            "So for example.",
            "Well, it's hard to say, but but otherwise color appears relatively constant here, so color seems to be coming in more at the lower level.",
            "So here what we have in the mid level, the global structure is now relatively fixed, so if you focus in on this, this one here, right?",
            "There's roughly a bed kind of in the middle of the scene in every one of these, but there's quite a bit of color variation and texture variation, right?",
            "If you look at this one here, this sort of stripes across this, this one here, and different colors.",
            "The covers on the bed and at the low level.",
            "At this point there's not a great deal of variability.",
            "Some illumination, but not a whole lot at that point.",
            "Now, if we were using just a pixel CNN, all of the variability would come in at this level, right?",
            "So just keep in mind that this is.",
            "This is because we're incorporating this variability at the top end, that there doesn't seem to be much being encoded at this level.",
            "Another way to say that is just at this point are multinomial distributions are very spiky.",
            "Yeah, middle levels, multinomial or middle level is Gaussian.",
            "Right, OK, so this is one way that we can kind of build up on the variational autoencoder by incorporating elements in the decoder.",
            "Now what I'm going to talk about."
        ],
        [
            "How we can incorporate aspects of what we just talked about?",
            "In fact, autoregressive components in the encoder side.",
            "So in this posterior inference component.",
            "So this is this picture sort of highlights the issues that we were talking about before, right?",
            "So this is our posterior prior distribution.",
            "Here, it's this Gaussian distribution, typically with a VE.",
            "We make this assumption of this prior of this factorial posterior, so these are kind of examples of four different possible posteriors that you could imagine, but.",
            "That are working together.",
            "So ideally this is sort of the marginal posterior, let's say so.",
            "Ideally these things together would look like this right?",
            "'cause this is covering all of the data, but it doesn't right?",
            "And it doesn't because it can't because all of these guys are essentially having to be independent.",
            "So what you really want is something like this right?",
            "Where this now marginally looks like the prior, and what I mean by that is if I sort of take all the data points, project them into the under the project them into our latent variable space, they look like the prior.",
            "There's a nice close match there, and we get that with something called the IAF or inverse autoregressive flow.",
            "So right now I'm just going to quickly go over what that looks like and what it is.",
            "So in order to do."
        ],
        [
            "That we're going to borrow something that Ian talked about in the context of a model called Nice or Real MVP, which is this.",
            "This transformation of random variables here, right?",
            "So if we have, zed prime is just a function of zed, an invertible function of Z such that we have this kind of relationship in both of these exist, then we can express it probability distribution on zed, zed prime to be equal to.",
            "If we do, this is the standard transformation of random variables here.",
            "But if we invoke the inverse function theorem, we actually can get it in this form.",
            "And what's nice about this is we can actually change change these things together so that we can actually have zed 0 here, and we can chain in.",
            "We can have this be a function of an invertible function F1 of an invertible function F2 all the way to FK.",
            "And now we can have our zed K be the function of this composition of all of these functions, and we can express the probability distribution on zed K as a function of the probability distribution on zed one.",
            "And this the log determinant of the gradient.",
            "Through of each element through its input.",
            "OK, so why that's interesting or why we how we can exploit this?",
            "And this was actually notice in a paper, at least as far as I know, the most first in the paper by rings, Indian, Mohammed Shakur, Mohamed a deep mind at Deep Mind in 2015.",
            "This is a nice email paper."
        ],
        [
            "Where they can exploit now the law of the unconscious statistician.",
            "So what this basically allows you to do is you can now express an expectation with respect to Q of K. So distribution with respect to zed.",
            "K here, which could be something quite complicated.",
            "Let's imagine that Zed 0 here is something simple like a Gaussian distribution that we know how to deal with.",
            "And ZK is now something quite complicated that we don't know how to deal with.",
            "We can actually express an expectation with respect to our complicated distribution as an expectation with respect to our simple distribution.",
            "Through this mapping.",
            "Here, as a function of through this function here and where we're going to use this is in our variational lower bound that we just saw in the context of the VA, right?",
            "So this is, this is what we had before we're going to just slightly change the notation here, so we're just going to drop the.",
            "Actual conditioning on X, but it's you can think of it as being there or just not exposing it here.",
            "And what we're saying is that now we're going to have some complicated distribution as our posterior.",
            "QK here and we can express this thing now.",
            "We can actually say OK, well we can actually through using what we saw here.",
            "This relationship and the unconscious, the lobby unconscious statistician.",
            "We can actually Re Express that as an expectation with respect to our simple distribution plus a bunch of terms.",
            "So the question is if we can evaluate this easilly, we now have a way of doing something pretty sophisticated on the posterior side.",
            "While keeping it relatively tractable.",
            "So that's the goal here.",
            "And the."
        ],
        [
            "Innovation brought in by the inverse autoregressive flow is to notice that if you actually have an auto regressive network and chaining these together, you can actually accomplish this.",
            "So when I say auto regressive here, I don't mean that these steps here, right?",
            "That we're changing a bunch of these together.",
            "It's inside here it's auto regressive.",
            "So this is so this said, for example, here is actually each element of this vector is a function of all the earlier elements of of the vector before it.",
            "So, so that structure allows you to actually come up with a relatively simple form of this term here.",
            "And that allows us to do in a tractable way, fairly complicated and rich distributions in the posterior.",
            "Yeah.",
            "Yes it does, that's right.",
            "Yes, I believe it is relatively difficult to train.",
            "Oh sorry, the question was doing this kind of thing significantly increases the depth of the network.",
            "Second question, does it make it harder to train?",
            "The answer is I believe it does.",
            "Yes, there's some tricks that they add in the paper to make it a bit more numerically stable.",
            "I have.",
            "I'm working with some students that are playing with it now and it seems like it actually works quite well, but it is a little bit unstable, but in the context of other models, we deal with things like Gans.",
            "It's not bad.",
            "Alright, so."
        ],
        [
            "So for this model we can see that we're if we evaluate on likelihood.",
            "So this is this is the variational lower bound here and then societal proximation of the actual log likelihood were actually with this model.",
            "We can see we actually get fairly close.",
            "This actually might be steady out.",
            "Right now.",
            "I'm not entirely sure what is the art, but it's right around here so we're able to do quite well with this model and improve significantly over over the diagonal covariance.",
            "So this is.",
            "The model that we would use in just sort of the traditional VE context, so the variational lower bound becomes quite a bit tighter and we were able to see a smaller increment in the negative log.",
            "Well, I guess increment in the log likelihood, but but it is significant.",
            "And then like I said that we've been playing with it, I don't have any results to show on that, but it does seem like it's a pretty useful tool if you what you care about, is inferring posterior distributions in these kinds of the type models.",
            "That's why I want to sort of show this to now.",
            "OK, so that's going to be it for VA style models.",
            "Is there any questions on that before we switch and talk about Gans?",
            "Yeah.",
            "It can be other windows.",
            "So can you say that again?",
            "Let America Gaussian yes, can they be other distributions, absolutely yes, so I don't.",
            "It's funny, I just people must have.",
            "We've tried things like mixtures of Gaussians because we want to capture something, something of the clustering of something like MNIST that turns out to be a little bit more complicated than you think.",
            "Part of it is the reason is that because the model, the encoder and decoder are so powerful that what we found in our initial experiments.",
            "This was some time ago, was that oftentimes it would just basically learn to just use one of the mixture components and just ignore the rest.",
            "So it could just do that because of all the flexibility it had an encoder decoder, so there's definitely.",
            "Reasons why you might want to do that and but it is something that's a little bit challenging to work with these models, because you're putting in your starting to put capacity in a lot of different places, so it's really quite you have to be very careful in how you set it up so that it behaves the way you would hope it would behave.",
            "Yeah, I mean there is.",
            "Yeah so we can do a bit of a different context here so.",
            "Yeah, so so yeah that's right.",
            "So so Julian won the students is actually working on something in a slightly different model paradigm where he's building these kinds of structures where it's actually it's a mixture of continuous units and discrete units as well.",
            "And he's finding that in the context of natural language modeling that they're encoding different pieces of information, different kinds of information.",
            "So that's an interesting direction to go.",
            "And yeah, thanks, yeah.",
            "So now we're going to switch to a."
        ],
        [
            "Different modeling paradigm which we've actually seen quite a bit of results on.",
            "I'm going to take it a bit of a different way, so hopefully this will be fairly new.",
            "So yeah, so the question is.",
            "Up till now we've seen like what we want to do is learn generative models, right?",
            "And what we saw with the context of the variational autoencoder is that we can learn use a neural net to encode an inference engine basically, or an encoder and then use that in the context of training this model through back popping through the whole thing.",
            "What we're going to talk about now is ganns, which is an entirely different way of training a generative model that doesn't require inference at all."
        ],
        [
            "And again, this is a slide that you've seen before.",
            "The way we're going to do this is through a game.",
            "It's a game between a discriminator here and a generator.",
            "Here, the discriminator's job is to try to tell the difference between real examples an fake ones generated from our generator network and the generator's job is to try to fool the discriminator.",
            "Another representation of this, a bit more of a."
        ],
        [
            "Holistic representation is shown here where we have the data coming in here from a probability distribution.",
            "P sub R and we have our generator we sample from again something simple like usually Gaussian distribution or uniform distribution.",
            "We put it through a deep neural net generator.",
            "And X in this case is typically a deterministic function of Z here, so there is no P of X given zed model typically, or at least it's degenerate in the sense it's a Delta function and we have X~ here, which is given by PFG or generator distribution and the discriminator's job is to tell them apart and the generator is trained to try to fool the discriminator.",
            "OK."
        ],
        [
            "So I'm going to go into a little bit more of the formal structure of this because it's going to motivate some of the other work I want to talk about.",
            "And the point here is that we can actually express this game formally.",
            "The Gan essentially as as a game with a mini Max objective of the following types.",
            "So really it's just you can think of this as as just essentially cross entropy from the discriminators POV, so the discriminator is trying to maximize this term, whereas the generator is trying to minimize this quantity and P here.",
            "As I have already described P of R&PMG.",
            "So all of these elements I think were described on the previous place, but this is our essentially our objective function, but it's important it's a mini Max objective function, so they're really working at cross purposes here."
        ],
        [
            "So we worked out in the original paper that the optimal discriminator in this context, so this is nonparametric in the sense that we're not worried about up specific parametric form of the discriminator has this form, which is that you have a probability density of the real distribution divided by the sum of the probability of the real plus the probability of the generator.",
            "And if you think about this for a second, it makes sense.",
            "It's basically just saying that if you have a greater probability of it being real, you're going to try to predict real, and otherwise you're going to predict.",
            "Fake, so it happens that under this ideal discriminator we can actually have actually shown that the generator minimizes the Jensen Shannon Divergent between the P of R&PFG, which is nice, right?",
            "'cause this tells us we're doing something sensible were under this metric of the Jensen Shannon Divergent.",
            "We're actually bringing these two densities closer together.",
            "That's what we're showing here, and this is just the Jensen Shannon Divergent again, just to remind you is a KL divergences between it's a sum of two KL divergences between P of our.",
            "And a mixture of the two, and PMG and a mixture of the two.",
            "And well, I'm just giving you the definition of the KL divergent here.",
            "Questions on that.",
            "Yep.",
            "I find that funny.",
            "For certain games.",
            "We are like I live there.",
            "I mean, you said you are maximizing my number.",
            "Yeah.",
            "Again, formulations adding up right, but it's with respect to different parameters, right?",
            "With respect to the parameters of the discriminator, we're maximizing that with respect to the parameters of the generator.",
            "We're minimizing it.",
            "So that's why it forms a saddle point.",
            "You can think of it as forming a saddle point because in One Direction you want to maximize your heading uphill the other direction you're heading downhill, but these are orthogonal directions because they in parameter space.",
            "These are two sets of parameters."
        ],
        [
            "Thanks for the question.",
            "So so in practice.",
            "However, Gans actually don't do this and this actually comes down to and this is something we noticed or we advocate right from the original paper.",
            "It turns out that it essentially doesn't work very well to use the original Mini Max objective and said what we do is we recommend that you sent the gradient the parameters of G are learned by this objective function.",
            "You want to maximize for the parameters of the generator.",
            "This term here rather than minimize this quantity here, right?",
            "Turns out that this is somewhat better behaved, but I still want to point out that even in this context that the model still can misbehave if the discriminator starts working.",
            "Sort of becomes more effective than the general, so if it comes if the discriminant becomes too good at telling the difference between generated sample and a true sample, it can become unstable.",
            "So in this context, what ends up happening is it's usually insisting on stable rather than rather than what happens if we were to use the original objective where.",
            "This is the gradients go to zero for the generator.",
            "Can you say that again?",
            "In the original Minimax game, is the equilibrium unique?",
            "I do well in the context of the parameters, I don't think so because it's a very non like the parameterisation of the model makes us look very non convex.",
            "So yeah, so that would not be unique.",
            "Well, at least we can't show that it's unique."
        ],
        [
            "Alright, so when we first published this work 2014, this is the kind of samples we got.",
            "We were pretty impressed with them.",
            "What I'm showing you here is actually a movie where we're tracing out sort of simple paths in zed space and then at each point in this path projecting that down into the image space.",
            "So you're sort of interpreting this as kind of movies and moving along, let's say on the image manifold that it's learned.",
            "Both for M Nestan for Cifar 10 over here.",
            "So like I said, that was two years ago.",
            "We've come a fairway."
        ],
        [
            "Since then, these are the least squares scans result from 2017 I guess.",
            "Yeah, February 2017.",
            "So one thing that we notice is that as soon as you go, if you can manage to go to large images, things look much better.",
            "Just sort of so it's a bit of a cheat.",
            "Well, it's still.",
            "It's still a very useful model.",
            "In fact, I recommend you look at it, but they were one of the first ones to go up to 128 by 128, so it had a big impact.",
            "'cause if you just compare samples 128 by 120 against samples 64 by 64, there's a dramatic difference right away, even for the same model.",
            "Yeah, so these are fairly good samples and I guess I don't.",
            "You probably know that there's been a huge kind of amount of work in this area."
        ],
        [
            "Yeah, this is sort of at some point.",
            "The number of Gan papers.",
            "This is a list that was kept track bye.",
            "Bye yeah, there's been actually far more than that since, but it's going a little bit crazy in the sense that we have two LS games.",
            "We have a vegan order vegan Tooheys 1 E. We have a lot of different variations on the theme."
        ],
        [
            "You quantitatively this is the.",
            "This is our paper here.",
            "Original paper here.",
            "Took some time before people caught on and then they kind of exploded and I'm hoping I'm actually kinda hoping we hit pecan here, but I'm not sure that's going to be true.",
            "So now what I'd like to do is."
        ],
        [
            "Just talk a little bit more sort of step back and talk a little bit more about what I think is going on with fans, how we think we can improve them, and kind of where the field is going, yeah?",
            "Scratch.",
            "Oh yeah, so OK, good question so.",
            "Alright, I don't know if I'm exactly answering your question, but the question was is it always trained by from scratch?",
            "Is it possible to find tune again so these are general these are generative models, right?",
            "So what were the target here?",
            "The published thing typically is when we're done training this model, we throw away the discriminator an we publish the generator, right?",
            "That's the product that's what we want.",
            "I'm not sure how you would intend to fine tune that generator, so it's like, for example, tell you what you might mean.",
            "An I'll tell you it doesn't work, which is something like maybe what you could do is.",
            "Pre training the discriminator for awhile and then try to train the generator on a pre trained discriminator that typically does not work very well.",
            "There for interesting reasons which will actually come to in a bit, but yeah, so so just getting back to this.",
            "So this is this is Dead Space interpolations.",
            "You saw movies.",
            "This is essentially the same thing, only laid out in time.",
            "So what we have here is just some point in said space and we're just kind of moving through Dead Space throughout this whole thing.",
            "And what's interesting about this is there all images right there?",
            "All pretty much all of them are interpretable images.",
            "What this means is that.",
            "This idea that we talked about a little earlier about moving around in zed space and then you kind of move along the image manifold.",
            "That really seems to be happening here, and it doesn't necessarily happen nearly as well in other kinds of models like things like VS. For example, because often involves you have these gaps in the prior.",
            "So if you kind of just move under your prior distribution you end up hitting some of these gaps and it ends up looking like garbage.",
            "We don't seem to see this with Gans and one of the reasons why I would like to suggest we don't see this is because what it's really doing, what these models are.",
            "Doing so."
        ],
        [
            "So for that I want to just kind of, you know, indulge me a little bit.",
            "This is my cartoon of what an image manifold looks like, right?",
            "So this is in 2D, so I imagine I've just picked two pixel values here and I'm just showing you what sort of natural images all fall on one of these lines, right?",
            "And so you can imagine what I'm really trying to represent here is that it's complicated, it's nonlinear, and there's a point here.",
            "You know you can be on the manifold of images.",
            "You can move off it.",
            "You can be on it again.",
            "You can move off it.",
            "There's there's rich structure to this.",
            "So what I think Ganz are doing is."
        ],
        [
            "Something like this picture here.",
            "So this thick blue line here represents the generative model structure.",
            "The dynamics of the Gan game are such that it's encouraged to look like real images, but it's not particularly encouraged to cover all of the diversity that shown in the input right for shown in the training data, for example, and that's kind of being represented here, right?",
            "So this is kind of really come close to this particular manifold, but it's essentially ignoring these other manifolds that are nearby.",
            "So when we move around in Dead Space where sensually moving around on this manifold.",
            "But we're not necessarily capturing all of the diversity that exists out there.",
            "If you contrast that with something like, say, a variational autoencoder, my interpretation is it's doing something that looks a little bit more like this, right?",
            "So what's going on here?",
            "Is the variational auto coder is trained through maximum likelihood, putting a lot of pressure on it to at least give some probability mass to everything in the training data, right?",
            "It's highly penalized if it's.",
            "Presented a training example for which it has very, very low likelihood.",
            "So what's that going to do?",
            "Is it's going to use its capacity between these two?",
            "My intention was to say the capacity is relatively the same, but they're using them in different ways.",
            "This one is sort of spreading probability mass as it can to cover the data distribution as it can, but the consequences if we now sort of draw a sample from this distribution.",
            "Here most of the time we're actually going to fall off the image manifold, right?",
            "And This is why we see these samples are often quite a bit more blurry, right?",
            "That's my kind of cartoon version of what's giving us blurry samples with things like maximum likelihood.",
            "But now there's something about this, which is, if we take this seriously as what gangs are doing and what we think cancer look like at this point, I think I'm pretty comfortable thinking about manifolds when we talk about games, because it's a deterministic function from some low level dimension level representation Space Z, right?",
            "Which we transform into some high dimensional concept.",
            "It has to be a manifold that's parameterized to be a manifold in that space."
        ],
        [
            "So if we take that seriously, that idea of this being a manifold, this is what we see.",
            "So we have our data manifold here.",
            "And let's imagine where some point in training.",
            "So the two manifolds don't quite match up as we'd like, and this is argan manifold.",
            "Here, right?",
            "This our generative model is what it looks like here and what I'm going to do is just zoom in on something here that I can work with.",
            "And it's simple right there."
        ],
        [
            "And I'm just going to say, OK, well, let's imagine this is our setting, right?",
            "So we're in 2D again.",
            "We have this this distribution here, which is our true data distribution.",
            "And this is argan manifold.",
            "Here, right?",
            "And what I'm going to ask is, what does the Shannon, the Jensen Shannon Divergent look like?",
            "This is the theory that we have for against this is the theory.",
            "Up till recently that was essentially the best theory we had for how these models are working."
        ],
        [
            "What does what does the Jensen Shannon say in a context like this?",
            "Well, it actually isn't particularly useful, so this is Jensen Shannon Divergent's.",
            "So again, I mean this is to be clear, this is not the setting where games are actually operating, so not the setting where we've ever claimed that they operate because the Jensen Shannon is the case where that we run the optimization of the discriminator.",
            "In fact, it's a it's a nonparametric discriminant, so it's an optimal discriminator.",
            "But this is the theory we have for it, so in that context, the dentist.",
            "Divergent actually is pretty degenerate, right?",
            "So it's zero everywhere for Theta as a function of Theta, except for when you hit this line right?",
            "The Zero line here, which gives it a value here.",
            "So this is our objective function, right?",
            "For training the generator, right?",
            "Yeah.",
            "Yeah, they're really important view, but one could ask.",
            "Well, what if you just add some Gaussian noise around the genitive model?",
            "Then you're not going to have zero derivatives, that's right.",
            "But then we're back in the setting I would are."
        ],
        [
            "Q We might be back in this setting.",
            "Is that the kind of yeah?",
            "But you what people do with these as they ignore the noise that you had at the end and they show the beans right?",
            "Yeah yeah but but I'm saying that that that's not just giving us bad, it's actually training a model that is OK with being off the manifold."
        ],
        [
            "That's a good question.",
            "Alright, so this is the context of sound version.",
            "If we think about this is from an optimization perspective, this is our objective function.",
            "Basically, if we want to generate if we think about from the perspective of the generator, this is not a particularly useful objective function, right?",
            "Gradients do not point towards the Theta here.",
            "I'm not claiming it's important to this.",
            "I'm not claiming that this is what games are doing.",
            "This is what the theory of Gans as it existed says they are doing or something like this, right?",
            "At least I'm sort of abusing the theory of games in this context.",
            "But I'm doing this for a point.",
            "So this is clearly not a very accurate picture of how we think games are actually training, because this would not train.",
            "This would not find this point right.",
            "There is no gradient information to find that point, so there would be no way of if we were to take the Jensen Shannon seriously, there would be no way for this manifold element to move forward towards the true data distribution.",
            "The true data manifold here."
        ],
        [
            "So that's the case of Jensen, Shannon, but there are other distance metrics out there, and one which has been relatively recently introduced as being interesting in the Gan context is called the Wash Machine or earthmovers distance, and that's just defined by this thing here, so it's the distance between a draw a point X&Y each from one from the real distribution, one from the data generating distribution.",
            "You measure their distance, and the washer steam distances basically infima of that.",
            "Right, so you can think of this as the minimum cost of transporting mass.",
            "Well, well match transporting mass from distribution R2, or from distribution PFG.",
            "Let's say two distribution.",
            "R. That's actually a better way of thinking about this.",
            "And what's interesting is this.",
            "This this earthmovers distance is continuous everywhere and differentiable almost everywhere under some mild assumptions on the transport."
        ],
        [
            "But so far, if we return to our simple example here, and this again is actually an example from the original Washington Gan Paper.",
            "So if you could evaluate the Wasserstein distance for this case, again, what we had was this degenerate Delta function, but in our car."
        ],
        [
            "Text here, this is what the washer steam gives you, which is actually much more tractable.",
            "Much more much more natural function, and something that we can get gradients on."
        ],
        [
            "So look just a representation of the gradients here.",
            "So now with this from a theoretical point of view, we can now say, well, now we actually have a theory or from a theoretical point of view we actually have a way to train to this model."
        ],
        [
            "And this motivated.",
            "Essentially the washer seen Gan BI Mart, now Ski and others at NYU you so the so the washer Steam gang.",
            "Now here's the difference right?",
            "So the if we look at the washer steam distance here it has this form right?",
            "But the problem with this form is it's in general, intractable, right?",
            "We can't actually work with this.",
            "But we can use something called the Kantorovich Rubinstein duality, which actually expresses it in this form, and what's interesting about this is that this is pretty close to what we've already seen in the context of Gans, something called an energy based game that was introduced also at NYU.",
            "You about a year ago or so year and a half ago that looked that basically had exactly this form, so it's essentially a critic now.",
            "So instead of this F function here, we're going to critic rather than calling it a discriminator, because it's just, it's output is real valued.",
            "Energy based game by the way, you can interpret this as being an energy function output and and you have the this term here.",
            "So this is the expectation under the probability under the generator.",
            "Here of that energy, or that critic.",
            "And it's the difference between that and the expectation under the real value of that same critic.",
            "And here the difference is that we have a soup.",
            "No, sorry, the soup is part of the energy base can what the difference is this?",
            "This thing here there's actually a constraint on the F on our critic.",
            "And that critic now is the constraint is that it's actually the supremum is over one Lipschitz functions, so that's the difference.",
            "Otherwise, if you remove this constraint, this will be already a form of Gan that we've seen in the literature.",
            "They've just added this constraint, but this constraint makes all the difference with that constraint makes this Gan game equivalent to from the from the generator's point of view, the critic training the critic ends up making this look like a washer some distance.",
            "Alright."
        ],
        [
            "So, so this basically motivates the W gamble Washington GAN objective function, which is just I'm getting minimax game in this form.",
            "Right, so now the question is.",
            "In this context, how do we enforce the Lipschitz constraint on the critic?",
            "So what they propose in the original paper was to clip the weights to some compact space between some constants, so every every weight is going to click between minus C&C, and that's actually going to be result in some subset of K Lipschitz function, so it's still smoothness.",
            "It's not one Lipschitz, but that's actually fine.",
            "It's actually OK for it to be just K Lipschitz, as long as we can bound K, which we can in this context."
        ],
        [
            "Now the problem with that is it turns out to be not a particularly great way of enforcing Lipschitz smoothness, and I'll just give you a particular.",
            "What it does.",
            "Is it under uses capacity, and it actually leads to can lead to exploding and vanishing gradients through the discriminator.",
            "So first of all, for underusing capacities, when you end up training these models and you use weight clipping, this is the weight distribution you tend to see, which means that all of the weights end up being sort of smashed up against their constraint.",
            "Which really means like you have the capacity of like something roughly like a binary weight model, which is quite a bit different than real valued weights.",
            "And Secondly you have these.",
            "The exploding and vanishing gradients depending on different values of the clipping you use, you either get exploding gradients or vanishing gradients.",
            "When you back propagate through the layers of the discriminator, neither one is obviously desirable.",
            "So what we wanted to do, so we kind of improved on this work or we."
        ],
        [
            "Introduced our own way of trying to enforce some sort of smoothness on our critic or discriminator.",
            "In other words, while avoiding some of these pitfalls and the way we did this is essentially draw some inspiration from the theory as well.",
            "So in theory what we have is the W game critic.",
            "If so, if we draw some X~ from the generator distribution that there is a point X from the real distribution such that all points in between those two on the straight line between those two.",
            "The gradient is this.",
            "Namely, the gradient has norm one, so this is what we're going to actually introduce as a penalty.",
            "So we're actually instead of using a weight clipping, we're actually going to use a penalty that encourages the gradient with respect to the input X.",
            "Here to be of Norm, one of the discriminator.",
            "And then we have some parameter here controlling the degree of the penalization.",
            "So the question is how do we pick this this this point?",
            "Here X hat here.",
            "So what we do is according to the theory, it's a particular point, right?",
            "It's according to the true transport from one point to the other.",
            "We don't have access to that.",
            "So what we pick?"
        ],
        [
            "Just some points we pick a point.",
            "We just draw a point at random from our generator.",
            "We pick a data sample at random and we pick some point uniformly between the two and we penalize that point.",
            "So on average, that's going to give us the region between our data distribution are true data manifold and our manifold of our generator model.",
            "Generative model is going to be regularised in this way the space between them through this mechanism.",
            "So if we do that."
        ],
        [
            "This is the kind of results we see.",
            "And that's basically yeah, so this is.",
            "This is our model.",
            "This is with WN clipping.",
            "This is just LLS Gannon, this DC can.",
            "So what we're showing you here, it's important to point out we're sort of doing stress tests right.",
            "Our claim with this is that we end up with an objective function that is easier to train, and one of the things that didn't mention is that both began and our model expected from the theoretical point of view, we actually expect the discriminator to be optimal, so we actually run it for a few updates for every update of the.",
            "Of the generator.",
            "So sorry the critic we expect it to be optimal so it's updated a few times so it's actually slower, but we see it's quite a bit more stable than these other forms of gangs.",
            "And again, we're stress testing it, so we're just changing the model architectures and you're trying to be fair to all the models, so we've just picked one set of hyperparameters for all of them.",
            "We picked ours in advance before doing this experiment, and we just ran them all and to see what kinds of results we get so."
        ],
        [
            "And this is just a different set.",
            "So for example this is 10 H nonlinearities everywhere, 101 layer resident.",
            "This is actually fairly challenging model to train again on 101 layer resident again, these are just.",
            "These are certainly not optimal model configurations that you would want.",
            "We're just essentially testing or training algorithm and we see that ours is doing fairly well in all of these cases."
        ],
        [
            "Of course, this is what we see.",
            "So in Inception score here is just.",
            "You can think of this as a measure of how realistic the objects are in terms of the images are in terms of expressing objects.",
            "I'm not going to go into describing exactly what it is, but it encourages diversity and encourages some notion of object nests to the images, and you can see this is our model AR.",
            "Let's see.",
            "Yeah, the two forms of gradient penalty here in.",
            "I guess that's Orange and green.",
            "Compared to DC Gan in red here and just use of weight clipping so on this score it actually you can see from here if we look at just iterations you know we're roughly the same.",
            "Maybe DC game is actually kind of outperforming on this model.",
            "This were by the way using the DC Gan architecture here, so it's not too surprising that it's doing very well compared to our model here, but if we look at Wall Clock time, there's actually a really big difference, right?",
            "And this is where that 5 updates for our critic.",
            "For every update of the of the generator shows up, so there is actually still quite an issue in terms of the capacity in terms of scaling this thing up."
        ],
        [
            "Alright, so we're pretty much near the end of my time.",
            "I actually do have something else to talk about, but I'm not going.",
            "I'm just going to skip it."
        ],
        [
            "Super super quickly, I'm just going to say that we've also built a model called Ali.",
            "I encourage you to check it out.",
            "I'm pretty excited by it.",
            "Adversarially learned inference.",
            "The idea here is that with, again, we've sort of sidestep this notion of doing inference, right?",
            "We don't need to do inference, but maybe you still want to do inference, but you want to sort of do this in the context of an adverse aerial learning setup so we can do that with Allie.",
            "So we're basically folding in, so this is the original decoder, so we re introduce an encoder into this model.",
            "And now we're expressing a game between the encoder and the decoder.",
            "So and the discriminator here is actually over X&Z.",
            "And where we have the prior distribution is sort of pinning the model down here and the data distribution is pinning the model down there.",
            "So that's the set up and I'll just show."
        ],
        [
            "In my last minute, what happens if you do this in one dimension so it's actually quite pretty complicated, but this is the.",
            "OK, so this is just a simulation of how this thing looks, so again, the context is we're just kind of playing around.",
            "Can this thing learn?",
            "And what we've asked is the encoder, so we've pinned down the true data to just be a simple Gaussian distribution and the prior to be a simple Gaussian distribution.",
            "And we initialize the weights to be 0 between the two, and we're seeing if we can actually learn a mapping between the two.",
            "And you can see it roughly does a OK job of learning this sort of identity function between the two.",
            "There's two reasonable solutions to this, there's identity, and then there's sort of the.",
            "The one minus identity function or sorry like it's minus identity function.",
            "It happens to find the identity function, but you can actually see just from looking at this that it's actually a pretty tricky thing because you have now three missed three moving pieces, right?",
            "You have an encoder with such as parameters.",
            "You have a decoder which has parameters and you have the discriminator, so the encoder and decoder are working together to try to fool the discriminator, and the discriminator is just trying to tell them apart.",
            "The encoder in the decoder.",
            "So I'll just end."
        ],
        [
            "Real quick with showing you kind of our latest kind of results with this kind of model, we built a hierarchical version of this model and over here what we have are.",
            "Wait?"
        ],
        [
            "Just for show you samples, these are the samples that we kind of get from this model on celebree.",
            "So again, we don't have great ways of evaluating this, but so sort of relying on samples, so these are 120 by 128 samples.",
            "Again, it looks a lot better than if you use 64 by 64.",
            "Anne."
        ],
        [
            "These are reconstructions, so what's interesting about this model is the hierarchical version.",
            "So while we're showing reconstructions and why this is interesting, is that this model was not trained to do reconstructions at all.",
            "It was nowhere in the objective function, unlike something like a VE that has part of the objective function to do a reconstruction.",
            "This model is only trained to try to fool the discriminator, and yet if we give it the latent variables, it reproduces things that look pretty close to the input, and these are on test examples.",
            "So that thing I was talking about earlier about we're being concerned that these gains are somehow under fitting the test distribution.",
            "This is directly probing that question.",
            "If you remember that was the case where we had like 3 lines of the manifold and my cartoon was at games were just kind of covering one of those manifolds.",
            "What this is basically showing us is that it's actually not doing as bad a job as maybe we would have feared to cover the full variability over the training data.",
            "Because these examples are essentially it's never trained on its impact test distributions, and yet it's still able to learn to be a reasonably good inverse model, which is what it's really trying to do over those samples.",
            "Alright, so I think that's my time, so I think I'll end there.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm following up what Ian was presenting Ian, I think, presented a fairly good motivation and overall view what I thought I would do would be to present kind of a dive.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into various frameworks, various ways of doing generative modeling, and I'm going to take a kind of a mid level view.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to dive into any details particularly, but I want to give you a kind of a survey, but it just to level a little bit more with a bit more detail than Ian was presenting.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to cover a little bit less ground, but at a bit more depth, and I think that's probably the well.",
                    "label": 0
                },
                {
                    "sent": "What I thought would be best served by you.",
                    "label": 0
                },
                {
                    "sent": "So this is a slide at least to an amalgamation of slides that even presented.",
                    "label": 0
                },
                {
                    "sent": "It's just motivating.",
                    "label": 0
                },
                {
                    "sent": "Why again, are reminding you why.",
                    "label": 0
                },
                {
                    "sent": "Again, we want to do generative modeling.",
                    "label": 0
                },
                {
                    "sent": "So sometimes we want to do density estimation.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we actually care about generating samples.",
                    "label": 0
                },
                {
                    "sent": "Yahshua again don't be fooled.",
                    "label": 0
                },
                {
                    "sent": "These are not true samples.",
                    "label": 0
                },
                {
                    "sent": "So right so?",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to be talking about specifically?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is a few separate models that some of which were mentioned by Ian.",
                    "label": 0
                },
                {
                    "sent": "I'm going to touch on auto regressive models and specifically I'm really going to concentrate on the Pixel CNN.",
                    "label": 0
                },
                {
                    "sent": "My reason for choosing that one is because I feel like it has the most potential to be built upon.",
                    "label": 0
                },
                {
                    "sent": "We've built upon it and others that have as well.",
                    "label": 0
                },
                {
                    "sent": "It has some nice properties which I'd like to describe.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to dive into latent variable models.",
                    "label": 1
                },
                {
                    "sent": "I'm going to talk about the variational autoencoder and I'm going to talk about one.",
                    "label": 0
                },
                {
                    "sent": "This is really just one improved strategy.",
                    "label": 0
                },
                {
                    "sent": "Or for inference in those models, and I'm actually going to mention something else that's not here.",
                    "label": 0
                },
                {
                    "sent": "Which is work we've done on improving the decoder of the VA. And finally, I'm going to touch on if time permits, generative adversarial networks will cover the basic Gan I'll talk about Wasserstein Gan some little bit of work we've done on improving the Washington gang or proving training in Washington and I'm going to touch on a model we have called Ali.",
                    "label": 0
                },
                {
                    "sent": "Adversarially learned inference, so it's a lot of material.",
                    "label": 0
                },
                {
                    "sent": "I'll get as far as I get.",
                    "label": 0
                },
                {
                    "sent": "I welcome your questions.",
                    "label": 0
                },
                {
                    "sent": "Slow me down if I'm moving too quickly and let's see how it goes.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so autoregressive generative models.",
                    "label": 1
                },
                {
                    "sent": "They basically you know Joshua presented these when he talked about recurrent neural Nets there and mostly he talked about in the context of sequential data, right?",
                    "label": 0
                },
                {
                    "sent": "So, language modeling and other sorts of time series when I'm going to be mainly focusing on today in the context of the Pixel CNN is how to apply these two images.",
                    "label": 0
                },
                {
                    "sent": "You can apply these to lots of other things, but mainly what I'll be talking about our images.",
                    "label": 0
                },
                {
                    "sent": "So just to remind you autograph as models.",
                    "label": 0
                },
                {
                    "sent": "Have a graphical model structure that looks a bit like this, so X is.",
                    "label": 0
                },
                {
                    "sent": "You can decompose X in.",
                    "label": 0
                },
                {
                    "sent": "The conditionals are basically given like.",
                    "label": 0
                },
                {
                    "sent": "This will get.",
                    "label": 0
                },
                {
                    "sent": "Will make this more explicit as time goes on, so a little bit of history, so this work sort of got started by Brendan Frey in the 90s using logistic regression for conditionals.",
                    "label": 0
                },
                {
                    "sent": "The Bengio brothers generalize this using neural Nets for conditionals.",
                    "label": 0
                },
                {
                    "sent": "And then Nate came along in 2011 by work by Yuval Ashelin, Ian Murray.",
                    "label": 0
                },
                {
                    "sent": "Which basically incorporated a lot of the same ideas, but using weight sharing as well.",
                    "label": 0
                },
                {
                    "sent": "And this is sort of the first model that really went for trying to model images, autoregressive models, trying to model images.",
                    "label": 0
                },
                {
                    "sent": "As far as I know, from there we've had from since then we've had quite a bit of an explosion of models, deep made pixels, RNN pixel, CNN, Wavenet, video pixel networks.",
                    "label": 0
                },
                {
                    "sent": "All of these have come out fairly recently.",
                    "label": 0
                },
                {
                    "sent": "Again, I'm going to be concentrated on the Pixel CNN, but I really do encourage you guys to look at wave Nets and the.",
                    "label": 1
                },
                {
                    "sent": "Video pixel network are pretty interesting models and their performance is very very good.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, so for in the context of learning images or learning generative models of images, what we need to do if we want to build an auto regressive models, we need to pick an ordering over these pixels, right?",
                    "label": 1
                },
                {
                    "sent": "And that can be somewhat difficult because there's no natural ordering to a set of pixels, but we need to have a probability joint probability distribution over pixels hearx that comes in this form right?",
                    "label": 0
                },
                {
                    "sent": "And so some of the nice properties of these autographs models is that typically P of X is tractable, easy to.",
                    "label": 1
                },
                {
                    "sent": "Easy to compute, easy to compute likelihoods for so they have that nice property, a con and this is really a question of some debate.",
                    "label": 0
                },
                {
                    "sent": "Whether or not you think it's important to be able to extract a latent representation.",
                    "label": 0
                },
                {
                    "sent": "So in the later part of the talk.",
                    "label": 0
                },
                {
                    "sent": "And really where I'm going to be spending most of my time is talking about latent variable models because my eye and a lot of other people in our community believe that it's really important to be able to try to extract these latent factors of the model.",
                    "label": 0
                },
                {
                    "sent": "You can think of these as maybe causal factors, but that might be pushing it a little far in terms of the interpretation of what these things mean.",
                    "label": 0
                },
                {
                    "sent": "Auto regressive models do not have this interpretation, and as we go on maybe we'll see a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But OK, so diving into the Pixel CNN, the approach here that the Pixel CNN uses to model this joint probability distribution.",
                    "label": 0
                },
                {
                    "sent": "The ordering it's going to pick looks a bit like this, right?",
                    "label": 0
                },
                {
                    "sent": "So imagine this is the full image.",
                    "label": 0
                },
                {
                    "sent": "It's going to predict some pixel here based on the values of all of the subsequent pixels starting in the top left corner.",
                    "label": 0
                },
                {
                    "sent": "And sort of scanning like this, this raster raster scan like this.",
                    "label": 0
                },
                {
                    "sent": "And it's going to do something else that's a little bit different than most other generative models of images is.",
                    "label": 0
                },
                {
                    "sent": "It's going to be for this particular value.",
                    "label": 0
                },
                {
                    "sent": "The pixel value here it's actually going to predict a multinomial overall of the two 56 values that that value could take.",
                    "label": 0
                },
                {
                    "sent": "If you are using a quantized representation of the pixel value, it's going to actually build a multinomial, so most of the time what we use is something like a Gaussian distribution, right?",
                    "label": 0
                },
                {
                    "sent": "Or something along these lines where it's just picking a single real number.",
                    "label": 0
                },
                {
                    "sent": "The fact that it's doing this multi nomial allows it to sort of have multi.",
                    "label": 0
                },
                {
                    "sent": "Multimodal distributions, which is pretty interesting, and they're carrying this idea of autoregressive.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two pretty far extent there, even assuming that they have structure within the across the color channels, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the way it looks.",
                    "label": 0
                },
                {
                    "sent": "They decompose for a single pixel value.",
                    "label": 0
                },
                {
                    "sent": "Here it's going to be decomposed into a red channel, which is not conditional.",
                    "label": 0
                },
                {
                    "sent": "Anything else.",
                    "label": 0
                },
                {
                    "sent": "And then the Green Channel, which is conditional on the Red Channel and all of the subsequent pixels and the blue channel is conditioned on that pixels.",
                    "label": 0
                },
                {
                    "sent": "Red value that pixels green value and all the subsequent pixels.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned before, they have these multinomial distribution so you can get a sense this is.",
                    "label": 0
                },
                {
                    "sent": "The plots they provide in their paper showing what kind of variability you can see in that kind of distribution, yes.",
                    "label": 0
                },
                {
                    "sent": "There there isn't.",
                    "label": 0
                },
                {
                    "sent": "There you're absolutely right, this is just the chain rule.",
                    "label": 0
                },
                {
                    "sent": "At this point, they're not assuming any additional structure.",
                    "label": 0
                },
                {
                    "sent": "I mean, they're picking and ordering, but there's no actual assumption of difference, so picking and ordering some orderings are better than others.",
                    "label": 0
                },
                {
                    "sent": "For depending on the natural data.",
                    "label": 0
                },
                {
                    "sent": "But yeah, this is a good point.",
                    "label": 0
                },
                {
                    "sent": "I should mention this is a bit of a detail, but they use two different masks to implement this.",
                    "label": 0
                },
                {
                    "sent": "The important thing to remember here is that they maintain this.",
                    "label": 0
                },
                {
                    "sent": "This kind of factorization.",
                    "label": 0
                },
                {
                    "sent": "Over over the inputs.",
                    "label": 0
                },
                {
                    "sent": "So this is just a way of doing that through these these two masks.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what's interesting about the Pixel CNN?",
                    "label": 0
                },
                {
                    "sent": "I guess what?",
                    "label": 0
                },
                {
                    "sent": "What is exciting from my point of view.",
                    "label": 0
                },
                {
                    "sent": "Is that it actually is able to learn relatively quickly?",
                    "label": 0
                },
                {
                    "sent": "A lot of these auto regressive models.",
                    "label": 0
                },
                {
                    "sent": "The problem with them and this also includes, by the way, that the Pixel CNN is that you basically have to learn by going pixel by pixel over them.",
                    "label": 0
                },
                {
                    "sent": "Nate had this problem as well and that's really quite slow, specially if you want to scale this thing up to like 256 by 256 images or bigger than that, the pixel CNN.",
                    "label": 0
                },
                {
                    "sent": "It exploits the fact that you can use mask convolutions that look a bit like this guy here.",
                    "label": 0
                },
                {
                    "sent": "This would be a mass convolution of size 5 by 5.",
                    "label": 0
                },
                {
                    "sent": "Typically you would use smaller than this, like 3 by three would be the typical.",
                    "label": 0
                },
                {
                    "sent": "Receptive field size.",
                    "label": 0
                },
                {
                    "sent": "But you use this kind of mass convolution where this is the pixel you're concentrating on right here and it's mast out itself because it has to be auto regressive.",
                    "label": 0
                },
                {
                    "sent": "It can't depend on its own value, and then they're basically exploiting the same ideas that are in maid.",
                    "label": 0
                },
                {
                    "sent": "I didn't have time to present made to you, but this is basically excuse me.",
                    "label": 0
                },
                {
                    "sent": "A masked autoencoder and what they do in made as they basically build the structure of the model to ensure that subsequent pixels are only dependent well.",
                    "label": 0
                },
                {
                    "sent": "Subsequent values only depend on previous value, so.",
                    "label": 0
                },
                {
                    "sent": "Enforce in ordering and then they build these masks to ensure that that ordering is maintained as you go from layer to layer in the network.",
                    "label": 0
                },
                {
                    "sent": "What pixel CNN does is build this into the context of a confident.",
                    "label": 0
                },
                {
                    "sent": "So you've got this kind of.",
                    "label": 0
                },
                {
                    "sent": "You're going to impose this kind of mask structure and what that allows you to do.",
                    "label": 0
                },
                {
                    "sent": "Once you have this mask structure is trained essentially in parallel, so you don't actually have to go pixel by pixel, at least during training you can.",
                    "label": 0
                },
                {
                    "sent": "You can use essentially teacher forcing, which is just maximum likelihood and use the ground truth values and you then just predict the actual values at the output condition on the ground.",
                    "label": 0
                },
                {
                    "sent": "Truth values of all the subsequent pixels that went before, so you don't actually have to have computed them beforehand 'cause you're using the ground truth values.",
                    "label": 0
                },
                {
                    "sent": "So that's the sort of the key insight I think the Pixel CNN into how they're able to train fast, and so it's able to scale.",
                    "label": 0
                },
                {
                    "sent": "The tough part is that for generalization for sorry for generation, it's still a rather slow operation because there you don't have ground truth values, so you actually have to wait and do some pixel by pixel.",
                    "label": 0
                },
                {
                    "sent": "Questions on that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Play.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do stop me 'cause I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to race to the end of this.",
                    "label": 1
                },
                {
                    "sent": "So if I'm going to let me know.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just another view of what pixel CNN does.",
                    "label": 0
                },
                {
                    "sent": "We have our mass convolution on the input layer.",
                    "label": 0
                },
                {
                    "sent": "Let's say here this is just imagine this is just some layer and the important thing to note here is that you're applying this mass convolution at every layer to maintain this dependency, this autoregressive dependency across the entire network.",
                    "label": 0
                },
                {
                    "sent": "So this is a mass convolution and this value here say for example is connected to this value.",
                    "label": 0
                },
                {
                    "sent": "The the image size stays the same and so this pixel basically is masked out at this point, and it's not dependent on any pixels that are subsequent to it according to our ordering, which is from top left across the image and you compose multiple layers like this to get to get a bigger receptive field so that each pixel eventually in the output is actually dependent on maybe all pixels before it that occur in the ordering, right?",
                    "label": 0
                },
                {
                    "sent": "Even though your initial receptive fields are quite small, you're getting the context.",
                    "label": 0
                },
                {
                    "sent": "The wider context by stacking.",
                    "label": 1
                },
                {
                    "sent": "Multiple layers of this now that would be true except.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem which is in the original publication of the Pixel CNN they make note of this.",
                    "label": 0
                },
                {
                    "sent": "In there they're following follow on work is that there's actually a blind spot that creeps in here, and that's because if you imagine having these kinds of mask operations, you see that that this guy right here.",
                    "label": 0
                },
                {
                    "sent": "Is never actually doesn't actually get incorporated into this.",
                    "label": 0
                },
                {
                    "sent": "This is like imagine if you have a three by three window, right?",
                    "label": 0
                },
                {
                    "sent": "And this guy also is it doesn't see this guy here.",
                    "label": 0
                },
                {
                    "sent": "This guy doesn't depend on here.",
                    "label": 0
                },
                {
                    "sent": "So what you end up with is this big blind spot that occurs over here for this pixel, which is not necessarily good.",
                    "label": 0
                },
                {
                    "sent": "So what they did.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To fix this problem is they basically decompose this into two stacks of convolutions, so one which is unmasked, which is their vertical stack, and it's basically just a normal convolution.",
                    "label": 1
                },
                {
                    "sent": "But here it's just looking at all the pixels in the Rose above the current role that you're in in the image.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially unmasked here, so you get the standard receptive field growth that you see as you go up in layers of a confident, and that's sort of represented in the darkening blue.",
                    "label": 0
                },
                {
                    "sent": "Going from out to in here, so this is sort of local context and then one layer context to layer context etc.",
                    "label": 1
                },
                {
                    "sent": "And then we have this other stack which is this horizontal stack which is maston.",
                    "label": 0
                },
                {
                    "sent": "Here it's only dependent upon the layers that are the elements in the current row so far, and there could be a receptive field there too that you would grow again.",
                    "label": 1
                },
                {
                    "sent": "With layers with except with exceeding subsequent layers of the CNN.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "GoDaddy doesn't work.",
                    "label": 0
                },
                {
                    "sent": "You go back for here, but this is not easy, yeah?",
                    "label": 0
                },
                {
                    "sent": "And then, well, it wasn't working.",
                    "label": 0
                },
                {
                    "sent": "I mean, would you do that?",
                    "label": 0
                },
                {
                    "sent": "How did they notice this this thing?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a good question and the answer is I have no idea this is in our work, so I don't know if Nando you are here.",
                    "label": 0
                },
                {
                    "sent": "Do you know how?",
                    "label": 0
                },
                {
                    "sent": "They yeah, do you know how they notice this?",
                    "label": 0
                },
                {
                    "sent": "Start.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I see OK, so right so this is this is from the like the Pixel RNN like work.",
                    "label": 1
                },
                {
                    "sent": "They sort of OK that makes sense, right?",
                    "label": 0
                },
                {
                    "sent": "The question was how do you notice?",
                    "label": 0
                },
                {
                    "sent": "How did they notice that this actually is a blind spot to the model like that piece?",
                    "label": 0
                },
                {
                    "sent": "And it's not actually that complicated once you see it right?",
                    "label": 0
                },
                {
                    "sent": "It's kind of clear that that would happen, but what Nando was saying was that was that that maybe it was through looking at the Pixel C and a pixel RNN work where where you're basically explicitly building up dependencies both in both directions.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to be talking about that in any particular detail, But yeah, so you basically in that case what you do is you run.",
                    "label": 0
                },
                {
                    "sent": "There's different setups, but you basically can run two RNS.",
                    "label": 0
                },
                {
                    "sent": "Won't going both ways towards that pixel, and so then it becomes a little bit, maybe more clear that you have that, yeah.",
                    "label": 0
                },
                {
                    "sent": "I think I will skip this part, but there's many ways to do this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, yeah, certainly.",
                    "label": 0
                },
                {
                    "sent": "Good point, yeah, so so this solution is actually pretty nice though.",
                    "label": 0
                },
                {
                    "sent": "You could also imagine other solutions to this that stays in the same.",
                    "label": 0
                },
                {
                    "sent": "For example, you could imagine keeping the same structure here, but then just having the other stack be something that carries on in a rasterized row like fashion.",
                    "label": 0
                },
                {
                    "sent": "The problem with that approach would be that these pixels here, which are relatively close to this one, would have to go right to the end and then come back through.",
                    "label": 0
                },
                {
                    "sent": "And so that's not a very good solution to that.",
                    "label": 0
                },
                {
                    "sent": "This is much cleaner because you end up with because of the receptive fields you end up with the sort of close things having the strongest connection.",
                    "label": 0
                },
                {
                    "sent": "So I think it's a particularly elegant solution.",
                    "label": 0
                },
                {
                    "sent": "This is a good idea.",
                    "label": 0
                },
                {
                    "sent": "Oh, like.",
                    "label": 0
                },
                {
                    "sent": "So OK, right?",
                    "label": 0
                },
                {
                    "sent": "So I think the question is why do auto regressive models over images at all?",
                    "label": 0
                },
                {
                    "sent": "Is that fair?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's well, I think I mean, one reason is that you end up being able to concentrate on very well on local textures, right?",
                    "label": 0
                },
                {
                    "sent": "Because you can have the dependencies, it happens that an image is you have a strong dependence, local dependency between pixel values and this kind of model can capture that in a way that's a little bit more difficult from your standard latent variable model, where you have a bunch of latent causes and.",
                    "label": 0
                },
                {
                    "sent": "In the image below, so that would be my stock answer to that would be that it's actually works well because it captures something about the natural structure of images.",
                    "label": 0
                },
                {
                    "sent": "Now the ordering is somewhat arbitrary and it's kind of unfortunate that you have to pick an ordering, but that's just the way it is for an auto regressive model.",
                    "label": 0
                },
                {
                    "sent": "What you get in return is a tractable model that you can train relatively efficiently, and you can evaluate likelihood on which is something.",
                    "label": 0
                },
                {
                    "sent": "That's really nice because a lot of our other models like Gans, for example, we have really no good way of evaluating them, so a lot of the things I'm going to be showing you today, I'm not going to be.",
                    "label": 0
                },
                {
                    "sent": "Showing you numbers, I'm going to be showing you pictures because we don't have great ways of getting reliable numbers from them.",
                    "label": 0
                },
                {
                    "sent": "But with these kinds of models you can actually evaluate them.",
                    "label": 0
                },
                {
                    "sent": "Tractably with using just likelihood so they do have strong advantages any other yeah?",
                    "label": 0
                },
                {
                    "sent": "Change.",
                    "label": 0
                },
                {
                    "sent": "Embarrass the order weather in the in the in the images yourself or in the in the architecture so I would would it change no, no, I don't think so.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think I think images I don't like in the sense of if you started at the bottom right corner and then just kind of went in the opposite direction.",
                    "label": 0
                },
                {
                    "sent": "I don't think images probably have the property where that's about the same.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't think there's any, but you probably don't want to do is have some sort of crazy ordering like just random pixels here in there.",
                    "label": 0
                },
                {
                    "sent": "You probably don't want to do something like that, but that's more that could also be for computational reasons.",
                    "label": 0
                },
                {
                    "sent": "In your component.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to move on right?",
                    "label": 0
                },
                {
                    "sent": "So this is another thing that they did with this paper, and it turns out originally when I saw this, I thought that maybe this was kind of what they really wanted to do was just fix this blind spot problem, but they thought they needed a little bit more in the paper so they did this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Colby complicated structure here.",
                    "label": 0
                },
                {
                    "sent": "This is like they built this gated structure, but it turns out this gating actually helps a lot, so an it's reminiscent of a lot of more recent work on kind of these structures of gating computations.",
                    "label": 0
                },
                {
                    "sent": "So what they have here is they keep these horizontal stack vertical stack in two separate lines, so you can imagine that you're kind of running two separate confidence throughout the Model 1.",
                    "label": 1
                },
                {
                    "sent": "Doing these horizontal computations in sort of the blue here and the other doing the.",
                    "label": 0
                },
                {
                    "sent": "Sorry vertical in blue horizontal in this purplely pink kind of color here and then.",
                    "label": 0
                },
                {
                    "sent": "What they do is they actually have this gating structure here so they go through a 10 H. But this 10 H is essentially gating this signal.",
                    "label": 1
                },
                {
                    "sent": "Sorry the sigmoid here is gating this 10 H structure, so you go through this 10 H, go through the sigmoid, multiply them together, and then that's the output of this thing and we also have a dependency now between the vertical stack and the horizontal stack, but it's in such a way that it doesn't.",
                    "label": 0
                },
                {
                    "sent": "Again, it doesn't break this auto regressive.",
                    "label": 0
                },
                {
                    "sent": "Structure of the model they have to maintain that because what happens is if you end up having some leak and this can happen.",
                    "label": 0
                },
                {
                    "sent": "Actually if you're not.",
                    "label": 0
                },
                {
                    "sent": "If you're not careful about how you code this thing up, you can have some leak and so that the pixel value XIJ will actually end up being in the computation to predict its own value.",
                    "label": 0
                },
                {
                    "sent": "And So what could happen if you have that that happening is your model could look like it's doing really, really well, right?",
                    "label": 0
                },
                {
                    "sent": "It can get great likelihood because it's essentially cheating, but when you go to actually generate from that model, it's going to be terrible because that information is just not available.",
                    "label": 0
                },
                {
                    "sent": "Or you might.",
                    "label": 0
                },
                {
                    "sent": "In fact, it might actually probably, depending on how you set it up, it probably just give you a bug 'cause it's not there.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some some results I mentioned earlier to the question of why do we do these things?",
                    "label": 0
                },
                {
                    "sent": "One of the reasons why was at the time it was giving us pretty good textures compared to other models that are out there.",
                    "label": 0
                },
                {
                    "sent": "So this is an example of the kinds of things you see with coral reefs.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's not great for global structure, but it's actually really good at getting local local texture local structure well.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are sort condition on sorrel horses.",
                    "label": 0
                },
                {
                    "sent": "This is another thing that they did with this.",
                    "label": 0
                },
                {
                    "sent": "This work was they built in conditioning so the way they did that was just inject the label information throughout the network structure which is something I actually recommend if you in fact we're playing with different ways of doing this kind of conditioning now, but it seems like it's a very good idea to sort of bring that kind of label conditioning information throughout the structure of your model so you can see that we're actually seeing some reasonable structure of horses.",
                    "label": 0
                },
                {
                    "sent": "You know, some of these look very good indeed.",
                    "label": 0
                },
                {
                    "sent": "You could actually imagine that there's they're relatively small.",
                    "label": 0
                },
                {
                    "sent": "These are only, I think, 64 by 64, so a lot of the seeming sort of maybe unrealism is due to that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have sandbar.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's what I was saying earlier.",
                    "label": 0
                },
                {
                    "sent": "We get relatively good likelihood numbers out of this, so we can actually evaluate this model.",
                    "label": 0
                },
                {
                    "sent": "This is on Cifar 10.",
                    "label": 0
                },
                {
                    "sent": "This is compared to a bunch of previous models, including the original pixel CNN model.",
                    "label": 0
                },
                {
                    "sent": "So this is the one that had the blind spot.",
                    "label": 0
                },
                {
                    "sent": "By incorporating both the fixing of the blind spot with these two stacks, and then their gated operation, they were able to go from 3.14 test negative log likelihood down to 3.0103.",
                    "label": 0
                },
                {
                    "sent": "So a relatively significant improvement.",
                    "label": 0
                },
                {
                    "sent": "It's hard to interpret likelihood numbers in terms of sort of what sort of quality we would imagine out of these models, but it's it's an impressive improvement nonetheless.",
                    "label": 0
                },
                {
                    "sent": "You see, it's still not quite as good, but roughly in the same category of the Pixel RNN here, which again it's a very good model that has the deficiency of being very slow to train.",
                    "label": 0
                },
                {
                    "sent": "So if you're not deep mined with GPU farm, it might be it's actually relatively difficult to work with.",
                    "label": 0
                },
                {
                    "sent": "Yeah sorry yeah.",
                    "label": 0
                },
                {
                    "sent": "So the gating here with my backup.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the gating here is basically just another way of using, so I guess I would say the intuition is that you're basically using some units to essentially kind of turn off and on other units.",
                    "label": 0
                },
                {
                    "sent": "Here I guess in this case it's a bit of a weird structure.",
                    "label": 0
                },
                {
                    "sent": "I think I prefer to look at this as just another kind of non linearity that has extra parameters to it, right?",
                    "label": 0
                },
                {
                    "sent": "So we did work, I don't think it's been presented yet, but we did some work on something called Max out which was a non linearity that had.",
                    "label": 0
                },
                {
                    "sent": "Lot of parameters to it, and in some cases that actually worked out very well.",
                    "label": 0
                },
                {
                    "sent": "I don't know if somebody would care to jump in for this, but my interpretation of this is that this is a little bit what this is because you're not conditioning on any extra information at this point, right?",
                    "label": 0
                },
                {
                    "sent": "It's the same input coming in from below, so it's just another form of non linearity.",
                    "label": 0
                },
                {
                    "sent": "Typically when we have this kind of gating operation, it's because you're actually using it as a means of injecting other information, but that doesn't seem to be the case here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned last time that the problem with the Pixel CNN was that the IT was actually quite slow to generate from and one question would be will can you speed that up?",
                    "label": 0
                },
                {
                    "sent": "And there's actually been some interesting very recent work from Nando, among others on this topic.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what they use is basically a strategy of predicting a few points independently and then using the local context to fill them in.",
                    "label": 0
                },
                {
                    "sent": "So there's an ordering now, but it's sort of over regions here, so we sample up these guys independently.",
                    "label": 0
                },
                {
                    "sent": "Then we fill in the local neighborhoods and then fill them in again and fill them in again here.",
                    "label": 0
                },
                {
                    "sent": "And if you do that, you see.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This kind of operation, so here we have our independent samples, and then you're sort of filling them in based on the local context and you can go all the way to 256 like this and you get pretty good quality generated samples.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's going to be it for autoregressive models, so maybe I'll just take a pause now and ask if there are any questions on auto regressive models before we move to latent variable models.",
                    "label": 0
                },
                {
                    "sent": "Yeah?",
                    "label": 0
                },
                {
                    "sent": "How do we measure like performance, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're using log likelihood here, so that's essentially how we're training it as well, so we're trying to predict I mentioned before that we've sort of these pixels are decomposed into multinomial's, so every pixel value has a certain value between zero and 2:55, and you're predicting the likelihood of that particular pixel value.",
                    "label": 0
                },
                {
                    "sent": "In fact, the Green Channel or the Red Channel, or the blue channel of that pixel value under the distribution of the model that's predicting it given all of the subsequent pixels before then.",
                    "label": 0
                },
                {
                    "sent": "And maybe all of the other color channels.",
                    "label": 0
                },
                {
                    "sent": "Up to that one, depending on what color channel you're using your predicting, and then you just evaluate that likelihood and so this is telling us on test images, how likely is a particular test image under this model.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "There's like no ordering on the pixel density.",
                    "label": 0
                },
                {
                    "sent": "Sorry if it uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "Sorry it's a multinomial, yeah, then it's if you assume independence across pixels.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's no ordering, so it's basically.",
                    "label": 0
                },
                {
                    "sent": "The values are treated.",
                    "label": 0
                },
                {
                    "sent": "The intensity is not ordinary.",
                    "label": 0
                },
                {
                    "sent": "That's right, that's right, that's weird, right?",
                    "label": 0
                },
                {
                    "sent": "So you would, yeah, that's a good point.",
                    "label": 0
                },
                {
                    "sent": "So you would think that I mean the way they did this.",
                    "label": 0
                },
                {
                    "sent": "If I back up aways here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To this plot here, right there using multinomial distributions here.",
                    "label": 0
                },
                {
                    "sent": "So there is no sort of notion that a pixel value of 255 is close to 254, right?",
                    "label": 0
                },
                {
                    "sent": "These are basically seen as completely independent, so you would think that that would be problematic.",
                    "label": 0
                },
                {
                    "sent": "There are distributions that you could build in, like the binomial, for one.",
                    "label": 0
                },
                {
                    "sent": "If you normalize this, but there are value distributions that could incorporate this kind of order.",
                    "label": 0
                },
                {
                    "sent": "This carnality, but.",
                    "label": 0
                },
                {
                    "sent": "The, but it seems like from these results that maybe you don't need it right, and So what you gain is the ability.",
                    "label": 0
                },
                {
                    "sent": "This flexibility of having multimodal distributions that you can see in this case, right?",
                    "label": 0
                },
                {
                    "sent": "So this I believe it's not shown here, but I believe this is like for the first pixel and you get this strongly bimodal distribution.",
                    "label": 0
                },
                {
                    "sent": "It's a little hard to see, but there's peaks right at 256 and at zero here and then later it becomes more like a sort of a single mode, but this would be kind of hard to capture with one of your standard.",
                    "label": 0
                },
                {
                    "sent": "Distributions that capture or anality not impossible.",
                    "label": 0
                },
                {
                    "sent": "You could construct something, but it doesn't seem like they lose that much by sort of losing that property of carnality.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to turn to.",
                    "label": 0
                },
                {
                    "sent": "Latent variable models.",
                    "label": 0
                },
                {
                    "sent": "So so as I mentioned before, we myself included, there's some people, and the specific latent variable model that we're going to start with is the autoencoder model that was sort of Co developed in two different groups, one at by Dirt Kingman Max Welling, who is here and one by the folks at DeepMind, roughly around the same time independently.",
                    "label": 0
                },
                {
                    "sent": "And the idea behind this model, well, it's sort of a first.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to talk about latent variable models in general.",
                    "label": 0
                },
                {
                    "sent": "And one of the things you would like a latent variable model to do is map you imagine a set of features, a latent feature space here that is somehow simple that incorporates information that you want in a kind of a dis entangled way.",
                    "label": 0
                },
                {
                    "sent": "And then you've got a function that Maps that into your data space that is somehow complicated, right?",
                    "label": 0
                },
                {
                    "sent": "Like like the natural manifold of images is going to be something quite complicated, but you'd like the features here to map to these to the actual data points here in a way that is coherent.",
                    "label": 0
                },
                {
                    "sent": "And models like the variational autoencoder go some way towards doing this kind of thing, so here's an.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample it's a bit of an update dated example at this point, but this is my body from the original via paper, so this is a case where they've taken 2 latent variables an mapped abunch of pictures of Brendan Frey and what they've learned from just this model.",
                    "label": 0
                },
                {
                    "sent": "That's just takes 2 variables and tries to generate a face of Brendan Frey they've learned to dissent.",
                    "label": 0
                },
                {
                    "sent": "Angle, pose information that's represented on the X axis.",
                    "label": 0
                },
                {
                    "sent": "Here the Zed 1 axis.",
                    "label": 0
                },
                {
                    "sent": "It's labeled an expression, right?",
                    "label": 0
                },
                {
                    "sent": "So in expression, he's going from sort of a serious face down here to a smile and then pose.",
                    "label": 0
                },
                {
                    "sent": "You see slight rotations of the head.",
                    "label": 0
                },
                {
                    "sent": "And that's very nice.",
                    "label": 0
                },
                {
                    "sent": "It's very powerful that we're able to learn to the kind of disentangle these factors would really like to be able to learn models that are able to do this in a little bit more of a complicated setting than what we have right here.",
                    "label": 0
                },
                {
                    "sent": "But it's essentially one of the goals of what latent variables are, and to be able to potentially use these latent variables for things like semi supervised classification or something like that.",
                    "label": 0
                },
                {
                    "sent": "This is another example for Emnace.",
                    "label": 0
                },
                {
                    "sent": "Now Amnesty doesn't quite have these nice, because it's data that sort of naturally clusters.",
                    "label": 0
                },
                {
                    "sent": "It doesn't quite have the same sort of factors quite as easily, but you can see.",
                    "label": 0
                },
                {
                    "sent": "If you look the ones, for example, that you can see some compose information being decoded here along the ones and then sort of different regions seem to represent different digits.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They even talked a little bit about these VA models, so I'm going to go fairly quickly over the intro to them.",
                    "label": 0
                },
                {
                    "sent": "But the idea with these is that this is what we want to do.",
                    "label": 0
                },
                {
                    "sent": "We want to maximize likelihood of X, right?",
                    "label": 0
                },
                {
                    "sent": "We want to maximize this probability on X and we want to use these latent variable structure.",
                    "label": 0
                },
                {
                    "sent": "So we're going to impose these latent variables.",
                    "label": 0
                },
                {
                    "sent": "But we don't want to maximize this joint explicitly, really, we're interested in maximizing the likelihood of the X and the way we're going to decompose this in this joint distribution as sort of represented by this picture is that we're going to have some some P of X, something.",
                    "label": 0
                },
                {
                    "sent": "Usually it's quite simple, like a Gaussian distribution on zed.",
                    "label": 0
                },
                {
                    "sent": "Sorry that's PM said.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to have a conditional distribution P of X given zed, which just is the projection.",
                    "label": 0
                },
                {
                    "sent": "So you sample from some P of zed here and then you project that into the data space here, and that's going to be done through this conditional P of X given zed.",
                    "label": 0
                },
                {
                    "sent": "So like I said, people says something simple and this is our G function here.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and the issue here for us in our in the context that we're concerned considering this in the with the variational autoencoder is that we want to learn a deep.",
                    "label": 0
                },
                {
                    "sent": "Neural net basically to the map from Z to X.",
                    "label": 0
                },
                {
                    "sent": "We could use similar models.",
                    "label": 0
                },
                {
                    "sent": "This is one of the places where this work is innovative is that we have.",
                    "label": 0
                },
                {
                    "sent": "We've had simple models where, let's say a linear model mapping from X to Z, and we know how to do inference in that structure and how to learn in that structure.",
                    "label": 0
                },
                {
                    "sent": "But in this work we're really interested in harnessing the power of deep neural Nets to map a complicated mapping right from something simple in zed space to something complicated in X space here.",
                    "label": 0
                },
                {
                    "sent": "So the problem with this setting is that how do you learn such a model?",
                    "label": 0
                },
                {
                    "sent": "If we were doing something like if we actually knew what Zeds corresponded to each X?",
                    "label": 0
                },
                {
                    "sent": "If we knew the settings of zed that corresponded to a particular image, a particular data point that we had, this would be straightforward.",
                    "label": 0
                },
                {
                    "sent": "We could just do supervised learning, right?",
                    "label": 0
                },
                {
                    "sent": "And we would be done.",
                    "label": 0
                },
                {
                    "sent": "The problem is we don't have those sets, right?",
                    "label": 0
                },
                {
                    "sent": "These are latent, so by definition we don't have them.",
                    "label": 0
                },
                {
                    "sent": "We actually are trying to discover them in the process of learning.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, and this was mentioned by by Ian.",
                    "label": 0
                },
                {
                    "sent": "I think 2 days ago.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is actually use a variational approach and I think maybe Max is going to talk a little bit about this.",
                    "label": 0
                },
                {
                    "sent": "I'm not actually sure how much he's going to talk about this, but we're going to use a variational model to define a variational lower bound on the likelihood of the data and this variational lower bound just looks like this.",
                    "label": 0
                },
                {
                    "sent": "And we're going to invoke an approximate distribution here, so Q of zed given X is our approximate posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "And we're doing this because if.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go back to the original model if we can ask questions like what is P of X given zedan this model parameterized is that function, so that's fine.",
                    "label": 0
                },
                {
                    "sent": "We know how to compute that, but if we were to ask the question, what is P of zed given AX, we should be able to ask that question.",
                    "label": 0
                },
                {
                    "sent": "The problem is, is that that's a that's intractable to come up with.",
                    "label": 0
                },
                {
                    "sent": "What that distribution is, you could do something like Monte Carlo sampling to determine approximation of that, but that's generally slow.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we're going to do instead is invoke an approximate posterior distribution Q of zed given X, and we're going to do so in the context of this variational lower bound here.",
                    "label": 0
                },
                {
                    "sent": "So if this Q of X actually ends up matching the P of X given set our true posterior, this bound becomes tight, meaning that this lower bound actually matches the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "And we know that we're actually maximizing the proper likelihood, so so this lower bound it can be decomposed into a different a few different sets of parameters, one which is essentially over this Theta here, which is over the generative model that we've talked about so far, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the models of our neural net here, and maybe the parameters of our Gaussians, or simple prior distribution.",
                    "label": 0
                },
                {
                    "sent": "But usually we assume that to be fixed.",
                    "label": 0
                },
                {
                    "sent": "And then we have Fi here, which is our parameters over this guy.",
                    "label": 0
                },
                {
                    "sent": "So typically what's been done with the standard thing to do with variational models.",
                    "label": 0
                },
                {
                    "sent": "And this is again you talked about.",
                    "label": 0
                },
                {
                    "sent": "This is to look at is to just consider the variational parameters directly.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a nonparametric model where you would have an optimization process for each X.",
                    "label": 0
                },
                {
                    "sent": "So you would solve.",
                    "label": 0
                },
                {
                    "sent": "You usually assume that there's some factorization over zed here and you would solve for these parameters of the zeds individually for each one.",
                    "label": 0
                },
                {
                    "sent": "Given X and there's an optimization process for that, that's also relatively slow because it requires an optimization process in the inner loop.",
                    "label": 0
                },
                {
                    "sent": "But in, but what we're going to see with the variational auto encoders, we're actually not going to do that.",
                    "label": 0
                },
                {
                    "sent": "We're actually going to use a neural net to encode this posterior distribution as well, and we're going to train them together, and we're going to see how we can do that.",
                    "label": 0
                },
                {
                    "sent": "But first, this is sort of a non standard representation of this.",
                    "label": 0
                },
                {
                    "sent": "Of this.",
                    "label": 0
                },
                {
                    "sent": "Lower likely this lower bound, and it's just interesting, because what we can do is rearrange terms like this and come up with something that looks like a traditional reconstruction term here, right?",
                    "label": 0
                },
                {
                    "sent": "This is just.",
                    "label": 0
                },
                {
                    "sent": "Expectation under so we sample is that we get our zed from our approximate distribution which I haven't talked about too much about how we get that approximate distribution yet.",
                    "label": 0
                },
                {
                    "sent": "But let's imagine we can get that.",
                    "label": 0
                },
                {
                    "sent": "And then what we have is this is just a standard log likelihood reconstruction.",
                    "label": 0
                },
                {
                    "sent": "So if this distribution was a Gaussian distribution, we heard from Dina that this is just essentially mean squared error right on the input trying to reconstruct the original input through zed here.",
                    "label": 0
                },
                {
                    "sent": "But what what's left over here is something that we might be able to interpret as a regularization term.",
                    "label": 0
                },
                {
                    "sent": "And it's interesting to look at what this is doing.",
                    "label": 0
                },
                {
                    "sent": "This is saying that we want our posterior.",
                    "label": 0
                },
                {
                    "sent": "To become closer to the prior distribution here right now, I want to emphasize this is not the standard KL that you see when you do variational methods.",
                    "label": 0
                },
                {
                    "sent": "The standard Cal you typically talk about when you're doing variational approximations is the KL between your approximate posterior and the true posterior.",
                    "label": 0
                },
                {
                    "sent": "So the big difference is that that Cal you actually want to go to zero.",
                    "label": 0
                },
                {
                    "sent": "This KL.",
                    "label": 0
                },
                {
                    "sent": "There's really no reason that you'd want this to go to zero, because if this goes to zero, that means that your posterior encodes no information about the data point.",
                    "label": 0
                },
                {
                    "sent": "Which is kind of this kind of means it's meaningless, right?",
                    "label": 0
                },
                {
                    "sent": "It's not really a posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "It's there's no information there, so this is more you can think of this as as a regularization term, right?",
                    "label": 0
                },
                {
                    "sent": "Just like when you put in, like weight decay on on your neural net, right?",
                    "label": 0
                },
                {
                    "sent": "You don't actually want all your parameters to go to zero, even though you're encouraging them to go to zero.",
                    "label": 0
                },
                {
                    "sent": "That's what a regularization term does.",
                    "label": 0
                },
                {
                    "sent": "So think of this regularization term in very much the same spirit.",
                    "label": 0
                },
                {
                    "sent": "OK so I'm.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ocean before that we're going to use a neural net again to approximate this posterior distribution here Q.",
                    "label": 0
                },
                {
                    "sent": "So that's in fact what we're going to do.",
                    "label": 0
                },
                {
                    "sent": "So this is our objective function, and we've got a neural net that's going to approximate our posterior Anna neural net.",
                    "label": 0
                },
                {
                    "sent": "That's going to be our generative model.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's fine, but the one thing I haven't told you how we're going to train this model and what we're going to use is something called the perimeter is Asia.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trick, so this is really just a different way of writing the same thing.",
                    "label": 0
                },
                {
                    "sent": "It's not particularly new, but it's it's a novel application of it, and it's actually a really important.",
                    "label": 0
                },
                {
                    "sent": "It was a big big step in our field to be able to parameterise Ed as a as a mean plus a sigmoid, which are both a function of XN plus a noise term.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Now what this does, it allows us to isolate the stochastic element here into this term here that has no parameters.",
                    "label": 0
                },
                {
                    "sent": "And so now we can think about back propping through from the.",
                    "label": 0
                },
                {
                    "sent": "From Zed here through this model, which now now our posterior approximate posterior.",
                    "label": 0
                },
                {
                    "sent": "Now it's outputs this meenan.",
                    "label": 0
                },
                {
                    "sent": "This Sigma, which are both deterministic functions of X and our generative model, can once we fold in that noise.",
                    "label": 0
                },
                {
                    "sent": "Propagate to the input, but it's still a deterministic function of this mu and Sigma here.",
                    "label": 0
                },
                {
                    "sent": "So in this part here, like how you encode XI picked one way where we actually do the same similar trick where we encode amuen a Sigma as a function of Z2 in Codex.",
                    "label": 0
                },
                {
                    "sent": "That's not actually necessary.",
                    "label": 0
                },
                {
                    "sent": "You can imagine that the noise on the input is just constant.",
                    "label": 0
                },
                {
                    "sent": "So for example Sigma is is just a sigmas value one for example or just just some constant value, not necessarily a function of said.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the questions on that yeah.",
                    "label": 0
                },
                {
                    "sent": "Sort of estimators that risk Adas City of the model is that what that saying this is?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, sure, sorry I should really go into so this is just we're assuming that this is a Gaussian distribution, right?",
                    "label": 0
                },
                {
                    "sent": "So yeah, I'm sorry, I really should have mentioned this before.",
                    "label": 0
                },
                {
                    "sent": "We actually it's written here, right?",
                    "label": 0
                },
                {
                    "sent": "So we're assuming that we are parameterizing a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "In fact, the suggested distribution with the diagonal covariance?",
                    "label": 0
                },
                {
                    "sent": "So we have a vector here mu Anna vector here of Sigma just for all of the elements of zed.",
                    "label": 0
                },
                {
                    "sent": "An here we're just making each of these two terms be a direct function of X through our encoder network, right?",
                    "label": 0
                },
                {
                    "sent": "So this is an auto encoder right?",
                    "label": 0
                },
                {
                    "sent": "Variational autoencoder?",
                    "label": 0
                },
                {
                    "sent": "This is our encoder model.",
                    "label": 0
                },
                {
                    "sent": "This is now our decoder model, so our inference machine is our encoder and our generative model is our decoder.",
                    "label": 0
                },
                {
                    "sent": "And so we just take these two parameters and we can add the noise.",
                    "label": 0
                },
                {
                    "sent": "And now that basically set that equal to Z and then we propagate that down and we evaluate the likelihood and we can back prop through.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So learning in this model now becomes a simple matter.",
                    "label": 0
                },
                {
                    "sent": "With this premise prioritisation trick of we started X, we forward propagate to get that Sigma and mu we add some noise.",
                    "label": 0
                },
                {
                    "sent": "Here at Zed we forward propagate some more until we get to X.",
                    "label": 0
                },
                {
                    "sent": "We compute the reconstruction and we back propagate and our objective function here.",
                    "label": 0
                },
                {
                    "sent": "This is our reconstruction component and then add said here.",
                    "label": 0
                },
                {
                    "sent": "We're actually also regularising zed so it doesn't collapse and it says we're putting pressure on it to be close to our.",
                    "label": 0
                },
                {
                    "sent": "Prior distribution alright yeah.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "So that's a good question that that is the.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Whole point of the pressurization trick, because we have we've isolated the noise here, right noises now there's no parameter, so the stochastic element is now isolated, so XR reconstruction at this.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Point here is a deterministic function of this X.",
                    "label": 0
                },
                {
                    "sent": "There's just noise injected here so we can back propagate through this entire model.",
                    "label": 0
                },
                {
                    "sent": "And it's done in such a way that it's actually like, unlike because of this regularization term, right?",
                    "label": 0
                },
                {
                    "sent": "I mentioned before that in the context of the auto regressive model.",
                    "label": 0
                },
                {
                    "sent": "If you had like the same pixel depended on the same pixel here, that would be essentially cheating.",
                    "label": 0
                },
                {
                    "sent": "It's like you would have a bug and you would get terrible samples, but because of how we formulated this and we have this regularization term here, that doesn't happen in this case.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are the kinds of samples you see when you use a standard V model.",
                    "label": 0
                },
                {
                    "sent": "At this point in time these look not so good, but they were actually fairly impressive at the time, so it's we've been making a lot of progress very recently, but it was actually a fairly significant breakthrough, even just in terms of the quality sample.",
                    "label": 0
                },
                {
                    "sent": "But you see that it's actually quite blurry here, right?",
                    "label": 0
                },
                {
                    "sent": "And we're going to talk a little bit about why that might be in a bit.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But another property that I actually really want to get across here is what happens when you trains up such a model.",
                    "label": 0
                },
                {
                    "sent": "So this is a model with what we see what we call component collapse.",
                    "label": 0
                },
                {
                    "sent": "So what we plotted here, this is the latent variables Ed, and these are just samples of the input samples.",
                    "label": 0
                },
                {
                    "sent": "Well of the output of the generative model.",
                    "label": 0
                },
                {
                    "sent": "So this is on M NIS, so these are samples from the model and these are sort of typical histograms of zed across a bunch of samples.",
                    "label": 0
                },
                {
                    "sent": "And what we know what's being plotted here is the KL divergences for those samples.",
                    "label": 0
                },
                {
                    "sent": "So the first thing to notice is that it's sparse, right?",
                    "label": 0
                },
                {
                    "sent": "What that means is that for a lot of these components the KL divergences actually 0.",
                    "label": 0
                },
                {
                    "sent": "So if we go back.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the KL divergent Sturm.",
                    "label": 0
                },
                {
                    "sent": "Here, this term right here collapses to zero.",
                    "label": 0
                },
                {
                    "sent": "There essentially not being used to encode anything about the data.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what's interesting is if you dig into the model, the output parameters, those weights that are associated with that dimension of zed go to zero.",
                    "label": 0
                },
                {
                    "sent": "Also because at that point it just becomes a source of noise, right?",
                    "label": 0
                },
                {
                    "sent": "Because it just matches the prior so that the model just says I'm just not going to use that, so it's interesting there seems like there's this property of picking the dimensionality, the sort of natural dimensionality that it needs to use in zed space to represent the data.",
                    "label": 0
                },
                {
                    "sent": "And another interesting point.",
                    "label": 0
                },
                {
                    "sent": "So so that's kind of cool and you think, well, that's awesome.",
                    "label": 0
                },
                {
                    "sent": "It's finding the correct dimension of some sort of inherent dimensionality of the data.",
                    "label": 0
                },
                {
                    "sent": "But the story of course isn't going to be quite that simple, because if you just build in a bigger model with a deeper encoder and a deeper decoder, you get some.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing like this for the same data, right?",
                    "label": 0
                },
                {
                    "sent": "It's it's putting the model.",
                    "label": 0
                },
                {
                    "sent": "This KL divergent puts pressure on the model to use as few components as possible.",
                    "label": 0
                },
                {
                    "sent": "So if you give it, the capacity is going to have unwrap it or put more information into fewer fewer dimensions.",
                    "label": 0
                },
                {
                    "sent": "You can see that even though we're using fewer elements of this of the fewer elements of the zed vector, we're actually getting better likelihood.",
                    "label": 0
                },
                {
                    "sent": "Earth well, better samples.",
                    "label": 0
                },
                {
                    "sent": "I imagine better likelihood as well.",
                    "label": 0
                },
                {
                    "sent": "So these are the dimensions along Z, right?",
                    "label": 0
                },
                {
                    "sent": "So we just started with a dimension, so this is our we have a vector of Z right?",
                    "label": 0
                },
                {
                    "sent": "Which is our Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "Diagonal covariance Gaussian distribution and it has 80 components.",
                    "label": 0
                },
                {
                    "sent": "We're starting training this thing with 80 components and then as training progresses more and more of these kind of die away, which means that they just go to zero.",
                    "label": 0
                },
                {
                    "sent": "This means that this is.",
                    "label": 0
                },
                {
                    "sent": "This is again what we're measuring.",
                    "label": 0
                },
                {
                    "sent": "Here is the KL divergent's, so it's the prior.",
                    "label": 0
                },
                {
                    "sent": "The posterior distribution is just matching the prior, so it's not encoding anything about about the latent variables.",
                    "label": 0
                },
                {
                    "sent": "Or sorry about the data, and so all of the information about the data is being compressed into these.",
                    "label": 0
                },
                {
                    "sent": "What 7 components in this case?",
                    "label": 0
                },
                {
                    "sent": "Other questions on that, yeah?",
                    "label": 0
                },
                {
                    "sent": "How do you pick the dimension of said good question so so you don't?",
                    "label": 0
                },
                {
                    "sent": "I mean, you just you can try a bunch of different dimensions.",
                    "label": 0
                },
                {
                    "sent": "One of the points here is that it's sort of picks.",
                    "label": 0
                },
                {
                    "sent": "As long as you're big enough, it's going to pick some subset of that.",
                    "label": 0
                },
                {
                    "sent": "Sending them to 0.",
                    "label": 0
                },
                {
                    "sent": "So that's actually a relatively nice property of the model.",
                    "label": 0
                },
                {
                    "sent": "You can, but that probably wouldn't be a very good idea because just computationally.",
                    "label": 0
                },
                {
                    "sent": "Yes, one of the things we found working with this model in particular is that if you if you sort of say like imagine you take this model and you say OK, well, it's found seven components, so I'm now going to start with seven components.",
                    "label": 0
                },
                {
                    "sent": "You know there's optimization issues, so it doesn't always actually find as good a model with just those seven components, so it's kind of it likes having sort of space to work, let's say and to finding a small number of components.",
                    "label": 0
                },
                {
                    "sent": "There's another question was there over here?",
                    "label": 0
                },
                {
                    "sent": "So everything we've done here assumes its diagonal, right?",
                    "label": 0
                },
                {
                    "sent": "It's the our posterior is diagonal, so meaning that all the elements are independent in the posterior.",
                    "label": 0
                },
                {
                    "sent": "So the posterior so conditioned on X, the elements of better independent will come back to how we can address that in a second.",
                    "label": 0
                },
                {
                    "sent": "But in general, no, like one of the reasons why you want that property is just scalability, right?",
                    "label": 0
                },
                {
                    "sent": "If you're this is an M NIS, so we don't have.",
                    "label": 0
                },
                {
                    "sent": "We can get away with relatively few components and then maybe we could build in a full covariance matrix, but we're not really ever targeting emnace, right?",
                    "label": 0
                },
                {
                    "sent": "We want to scale these things up to larger inputs.",
                    "label": 0
                },
                {
                    "sent": "In that case, using large covariance matrices is a lot of parameters to predict, and we typically don't see it being worth it.",
                    "label": 0
                },
                {
                    "sent": "In that case, yeah, so, but there are interesting new work that actually touches on this problem of independence of the posterior that will get to in a second, But the first thing I want.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "She was just very briefly on how you can integrate some of these models together, so this is a model called the Pixel VA, where we've taken at the VA model that I've just talked about and we kind of graft in components of pixel CNN between so this is a hierarchical model now.",
                    "label": 0
                },
                {
                    "sent": "So what we saw before was just one layer of Zed mapped to X, right?",
                    "label": 0
                },
                {
                    "sent": "But there's no reason at all.",
                    "label": 0
                },
                {
                    "sent": "We can't use a hierarchy of Zeds an here what we're going to use is actually pixel CNN to encode this distribution from one layer to the next.",
                    "label": 0
                },
                {
                    "sent": "Right, so now we can start instead of having a simple prior here at this.",
                    "label": 0
                },
                {
                    "sent": "At this variable, we can encode something actually quite complicated.",
                    "label": 0
                },
                {
                    "sent": "This is actually going to be quite a long way in towards fixing this is purely in the decoder for the encoder.",
                    "label": 0
                },
                {
                    "sent": "We use very traditional what we just talked about methods, so our posteriors are all factorial and so if we do this we can see that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We actually get pretty nice samples from this model, so here's a 64 by 64 L Son bedrooms and this is image net and we also may be getting something a little bit more object oriented than what you see in the sense that there's clear objects than what you see in a traditional pixel CNN.",
                    "label": 0
                },
                {
                    "sent": "But what's really interesting about this model is, and This is why we're interested in integrating Pixel CNN's with with something like the variational.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variational autoencoder is that we now have control over these latent factors, right?",
                    "label": 0
                },
                {
                    "sent": "We can actually look in and see as we manipulate these latent factors.",
                    "label": 0
                },
                {
                    "sent": "What's being represented at these layers of the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "And that's what I'm showing right here.",
                    "label": 0
                },
                {
                    "sent": "So at the top level of the hierarchy, what we're doing here is, we're just sort of sampling around twiddling different values at the at the top level and seeing what kinds of different samples we get.",
                    "label": 0
                },
                {
                    "sent": "We do the same thing here, holding the top level constant, and we twiddle, we sample different values at a mid level of the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "And here we hold the top level and mid level constant and we sample.",
                    "label": 0
                },
                {
                    "sent": "Just the very low level pixel CNN values, so this is the sort of the injection of the noise that's responsible purely by the lowest level pixel CNN and what you can see I think, is that this is actually kind of hard because it's hard to parse these images from far away, but it seems like global structure is largely being incorporated by this.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by that is these are bedroom scenes, so it's the room layout varies a little bit as you change the values of the latent variables at this stage right?",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "Well, it's hard to say, but but otherwise color appears relatively constant here, so color seems to be coming in more at the lower level.",
                    "label": 0
                },
                {
                    "sent": "So here what we have in the mid level, the global structure is now relatively fixed, so if you focus in on this, this one here, right?",
                    "label": 0
                },
                {
                    "sent": "There's roughly a bed kind of in the middle of the scene in every one of these, but there's quite a bit of color variation and texture variation, right?",
                    "label": 0
                },
                {
                    "sent": "If you look at this one here, this sort of stripes across this, this one here, and different colors.",
                    "label": 0
                },
                {
                    "sent": "The covers on the bed and at the low level.",
                    "label": 0
                },
                {
                    "sent": "At this point there's not a great deal of variability.",
                    "label": 0
                },
                {
                    "sent": "Some illumination, but not a whole lot at that point.",
                    "label": 0
                },
                {
                    "sent": "Now, if we were using just a pixel CNN, all of the variability would come in at this level, right?",
                    "label": 0
                },
                {
                    "sent": "So just keep in mind that this is.",
                    "label": 0
                },
                {
                    "sent": "This is because we're incorporating this variability at the top end, that there doesn't seem to be much being encoded at this level.",
                    "label": 0
                },
                {
                    "sent": "Another way to say that is just at this point are multinomial distributions are very spiky.",
                    "label": 0
                },
                {
                    "sent": "Yeah, middle levels, multinomial or middle level is Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Right, OK, so this is one way that we can kind of build up on the variational autoencoder by incorporating elements in the decoder.",
                    "label": 0
                },
                {
                    "sent": "Now what I'm going to talk about.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How we can incorporate aspects of what we just talked about?",
                    "label": 0
                },
                {
                    "sent": "In fact, autoregressive components in the encoder side.",
                    "label": 0
                },
                {
                    "sent": "So in this posterior inference component.",
                    "label": 0
                },
                {
                    "sent": "So this is this picture sort of highlights the issues that we were talking about before, right?",
                    "label": 0
                },
                {
                    "sent": "So this is our posterior prior distribution.",
                    "label": 0
                },
                {
                    "sent": "Here, it's this Gaussian distribution, typically with a VE.",
                    "label": 0
                },
                {
                    "sent": "We make this assumption of this prior of this factorial posterior, so these are kind of examples of four different possible posteriors that you could imagine, but.",
                    "label": 0
                },
                {
                    "sent": "That are working together.",
                    "label": 0
                },
                {
                    "sent": "So ideally this is sort of the marginal posterior, let's say so.",
                    "label": 0
                },
                {
                    "sent": "Ideally these things together would look like this right?",
                    "label": 0
                },
                {
                    "sent": "'cause this is covering all of the data, but it doesn't right?",
                    "label": 0
                },
                {
                    "sent": "And it doesn't because it can't because all of these guys are essentially having to be independent.",
                    "label": 0
                },
                {
                    "sent": "So what you really want is something like this right?",
                    "label": 0
                },
                {
                    "sent": "Where this now marginally looks like the prior, and what I mean by that is if I sort of take all the data points, project them into the under the project them into our latent variable space, they look like the prior.",
                    "label": 0
                },
                {
                    "sent": "There's a nice close match there, and we get that with something called the IAF or inverse autoregressive flow.",
                    "label": 0
                },
                {
                    "sent": "So right now I'm just going to quickly go over what that looks like and what it is.",
                    "label": 0
                },
                {
                    "sent": "So in order to do.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we're going to borrow something that Ian talked about in the context of a model called Nice or Real MVP, which is this.",
                    "label": 0
                },
                {
                    "sent": "This transformation of random variables here, right?",
                    "label": 0
                },
                {
                    "sent": "So if we have, zed prime is just a function of zed, an invertible function of Z such that we have this kind of relationship in both of these exist, then we can express it probability distribution on zed, zed prime to be equal to.",
                    "label": 0
                },
                {
                    "sent": "If we do, this is the standard transformation of random variables here.",
                    "label": 0
                },
                {
                    "sent": "But if we invoke the inverse function theorem, we actually can get it in this form.",
                    "label": 0
                },
                {
                    "sent": "And what's nice about this is we can actually change change these things together so that we can actually have zed 0 here, and we can chain in.",
                    "label": 0
                },
                {
                    "sent": "We can have this be a function of an invertible function F1 of an invertible function F2 all the way to FK.",
                    "label": 0
                },
                {
                    "sent": "And now we can have our zed K be the function of this composition of all of these functions, and we can express the probability distribution on zed K as a function of the probability distribution on zed one.",
                    "label": 0
                },
                {
                    "sent": "And this the log determinant of the gradient.",
                    "label": 0
                },
                {
                    "sent": "Through of each element through its input.",
                    "label": 0
                },
                {
                    "sent": "OK, so why that's interesting or why we how we can exploit this?",
                    "label": 0
                },
                {
                    "sent": "And this was actually notice in a paper, at least as far as I know, the most first in the paper by rings, Indian, Mohammed Shakur, Mohamed a deep mind at Deep Mind in 2015.",
                    "label": 0
                },
                {
                    "sent": "This is a nice email paper.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where they can exploit now the law of the unconscious statistician.",
                    "label": 0
                },
                {
                    "sent": "So what this basically allows you to do is you can now express an expectation with respect to Q of K. So distribution with respect to zed.",
                    "label": 0
                },
                {
                    "sent": "K here, which could be something quite complicated.",
                    "label": 0
                },
                {
                    "sent": "Let's imagine that Zed 0 here is something simple like a Gaussian distribution that we know how to deal with.",
                    "label": 0
                },
                {
                    "sent": "And ZK is now something quite complicated that we don't know how to deal with.",
                    "label": 0
                },
                {
                    "sent": "We can actually express an expectation with respect to our complicated distribution as an expectation with respect to our simple distribution.",
                    "label": 0
                },
                {
                    "sent": "Through this mapping.",
                    "label": 0
                },
                {
                    "sent": "Here, as a function of through this function here and where we're going to use this is in our variational lower bound that we just saw in the context of the VA, right?",
                    "label": 0
                },
                {
                    "sent": "So this is, this is what we had before we're going to just slightly change the notation here, so we're just going to drop the.",
                    "label": 0
                },
                {
                    "sent": "Actual conditioning on X, but it's you can think of it as being there or just not exposing it here.",
                    "label": 0
                },
                {
                    "sent": "And what we're saying is that now we're going to have some complicated distribution as our posterior.",
                    "label": 0
                },
                {
                    "sent": "QK here and we can express this thing now.",
                    "label": 0
                },
                {
                    "sent": "We can actually say OK, well we can actually through using what we saw here.",
                    "label": 0
                },
                {
                    "sent": "This relationship and the unconscious, the lobby unconscious statistician.",
                    "label": 0
                },
                {
                    "sent": "We can actually Re Express that as an expectation with respect to our simple distribution plus a bunch of terms.",
                    "label": 0
                },
                {
                    "sent": "So the question is if we can evaluate this easilly, we now have a way of doing something pretty sophisticated on the posterior side.",
                    "label": 0
                },
                {
                    "sent": "While keeping it relatively tractable.",
                    "label": 0
                },
                {
                    "sent": "So that's the goal here.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Innovation brought in by the inverse autoregressive flow is to notice that if you actually have an auto regressive network and chaining these together, you can actually accomplish this.",
                    "label": 0
                },
                {
                    "sent": "So when I say auto regressive here, I don't mean that these steps here, right?",
                    "label": 0
                },
                {
                    "sent": "That we're changing a bunch of these together.",
                    "label": 0
                },
                {
                    "sent": "It's inside here it's auto regressive.",
                    "label": 0
                },
                {
                    "sent": "So this is so this said, for example, here is actually each element of this vector is a function of all the earlier elements of of the vector before it.",
                    "label": 0
                },
                {
                    "sent": "So, so that structure allows you to actually come up with a relatively simple form of this term here.",
                    "label": 0
                },
                {
                    "sent": "And that allows us to do in a tractable way, fairly complicated and rich distributions in the posterior.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes it does, that's right.",
                    "label": 0
                },
                {
                    "sent": "Yes, I believe it is relatively difficult to train.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, the question was doing this kind of thing significantly increases the depth of the network.",
                    "label": 0
                },
                {
                    "sent": "Second question, does it make it harder to train?",
                    "label": 0
                },
                {
                    "sent": "The answer is I believe it does.",
                    "label": 0
                },
                {
                    "sent": "Yes, there's some tricks that they add in the paper to make it a bit more numerically stable.",
                    "label": 0
                },
                {
                    "sent": "I have.",
                    "label": 0
                },
                {
                    "sent": "I'm working with some students that are playing with it now and it seems like it actually works quite well, but it is a little bit unstable, but in the context of other models, we deal with things like Gans.",
                    "label": 0
                },
                {
                    "sent": "It's not bad.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for this model we can see that we're if we evaluate on likelihood.",
                    "label": 0
                },
                {
                    "sent": "So this is this is the variational lower bound here and then societal proximation of the actual log likelihood were actually with this model.",
                    "label": 0
                },
                {
                    "sent": "We can see we actually get fairly close.",
                    "label": 0
                },
                {
                    "sent": "This actually might be steady out.",
                    "label": 0
                },
                {
                    "sent": "Right now.",
                    "label": 0
                },
                {
                    "sent": "I'm not entirely sure what is the art, but it's right around here so we're able to do quite well with this model and improve significantly over over the diagonal covariance.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "The model that we would use in just sort of the traditional VE context, so the variational lower bound becomes quite a bit tighter and we were able to see a smaller increment in the negative log.",
                    "label": 0
                },
                {
                    "sent": "Well, I guess increment in the log likelihood, but but it is significant.",
                    "label": 0
                },
                {
                    "sent": "And then like I said that we've been playing with it, I don't have any results to show on that, but it does seem like it's a pretty useful tool if you what you care about, is inferring posterior distributions in these kinds of the type models.",
                    "label": 0
                },
                {
                    "sent": "That's why I want to sort of show this to now.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's going to be it for VA style models.",
                    "label": 0
                },
                {
                    "sent": "Is there any questions on that before we switch and talk about Gans?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "It can be other windows.",
                    "label": 0
                },
                {
                    "sent": "So can you say that again?",
                    "label": 0
                },
                {
                    "sent": "Let America Gaussian yes, can they be other distributions, absolutely yes, so I don't.",
                    "label": 0
                },
                {
                    "sent": "It's funny, I just people must have.",
                    "label": 0
                },
                {
                    "sent": "We've tried things like mixtures of Gaussians because we want to capture something, something of the clustering of something like MNIST that turns out to be a little bit more complicated than you think.",
                    "label": 0
                },
                {
                    "sent": "Part of it is the reason is that because the model, the encoder and decoder are so powerful that what we found in our initial experiments.",
                    "label": 0
                },
                {
                    "sent": "This was some time ago, was that oftentimes it would just basically learn to just use one of the mixture components and just ignore the rest.",
                    "label": 0
                },
                {
                    "sent": "So it could just do that because of all the flexibility it had an encoder decoder, so there's definitely.",
                    "label": 0
                },
                {
                    "sent": "Reasons why you might want to do that and but it is something that's a little bit challenging to work with these models, because you're putting in your starting to put capacity in a lot of different places, so it's really quite you have to be very careful in how you set it up so that it behaves the way you would hope it would behave.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean there is.",
                    "label": 0
                },
                {
                    "sent": "Yeah so we can do a bit of a different context here so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so yeah that's right.",
                    "label": 0
                },
                {
                    "sent": "So so Julian won the students is actually working on something in a slightly different model paradigm where he's building these kinds of structures where it's actually it's a mixture of continuous units and discrete units as well.",
                    "label": 0
                },
                {
                    "sent": "And he's finding that in the context of natural language modeling that they're encoding different pieces of information, different kinds of information.",
                    "label": 0
                },
                {
                    "sent": "So that's an interesting direction to go.",
                    "label": 0
                },
                {
                    "sent": "And yeah, thanks, yeah.",
                    "label": 0
                },
                {
                    "sent": "So now we're going to switch to a.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different modeling paradigm which we've actually seen quite a bit of results on.",
                    "label": 0
                },
                {
                    "sent": "I'm going to take it a bit of a different way, so hopefully this will be fairly new.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so the question is.",
                    "label": 0
                },
                {
                    "sent": "Up till now we've seen like what we want to do is learn generative models, right?",
                    "label": 0
                },
                {
                    "sent": "And what we saw with the context of the variational autoencoder is that we can learn use a neural net to encode an inference engine basically, or an encoder and then use that in the context of training this model through back popping through the whole thing.",
                    "label": 0
                },
                {
                    "sent": "What we're going to talk about now is ganns, which is an entirely different way of training a generative model that doesn't require inference at all.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, this is a slide that you've seen before.",
                    "label": 0
                },
                {
                    "sent": "The way we're going to do this is through a game.",
                    "label": 0
                },
                {
                    "sent": "It's a game between a discriminator here and a generator.",
                    "label": 0
                },
                {
                    "sent": "Here, the discriminator's job is to try to tell the difference between real examples an fake ones generated from our generator network and the generator's job is to try to fool the discriminator.",
                    "label": 0
                },
                {
                    "sent": "Another representation of this, a bit more of a.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Holistic representation is shown here where we have the data coming in here from a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "P sub R and we have our generator we sample from again something simple like usually Gaussian distribution or uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "We put it through a deep neural net generator.",
                    "label": 0
                },
                {
                    "sent": "And X in this case is typically a deterministic function of Z here, so there is no P of X given zed model typically, or at least it's degenerate in the sense it's a Delta function and we have X~ here, which is given by PFG or generator distribution and the discriminator's job is to tell them apart and the generator is trained to try to fool the discriminator.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to go into a little bit more of the formal structure of this because it's going to motivate some of the other work I want to talk about.",
                    "label": 0
                },
                {
                    "sent": "And the point here is that we can actually express this game formally.",
                    "label": 0
                },
                {
                    "sent": "The Gan essentially as as a game with a mini Max objective of the following types.",
                    "label": 0
                },
                {
                    "sent": "So really it's just you can think of this as as just essentially cross entropy from the discriminators POV, so the discriminator is trying to maximize this term, whereas the generator is trying to minimize this quantity and P here.",
                    "label": 0
                },
                {
                    "sent": "As I have already described P of R&PMG.",
                    "label": 0
                },
                {
                    "sent": "So all of these elements I think were described on the previous place, but this is our essentially our objective function, but it's important it's a mini Max objective function, so they're really working at cross purposes here.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we worked out in the original paper that the optimal discriminator in this context, so this is nonparametric in the sense that we're not worried about up specific parametric form of the discriminator has this form, which is that you have a probability density of the real distribution divided by the sum of the probability of the real plus the probability of the generator.",
                    "label": 0
                },
                {
                    "sent": "And if you think about this for a second, it makes sense.",
                    "label": 0
                },
                {
                    "sent": "It's basically just saying that if you have a greater probability of it being real, you're going to try to predict real, and otherwise you're going to predict.",
                    "label": 0
                },
                {
                    "sent": "Fake, so it happens that under this ideal discriminator we can actually have actually shown that the generator minimizes the Jensen Shannon Divergent between the P of R&PFG, which is nice, right?",
                    "label": 0
                },
                {
                    "sent": "'cause this tells us we're doing something sensible were under this metric of the Jensen Shannon Divergent.",
                    "label": 0
                },
                {
                    "sent": "We're actually bringing these two densities closer together.",
                    "label": 0
                },
                {
                    "sent": "That's what we're showing here, and this is just the Jensen Shannon Divergent again, just to remind you is a KL divergences between it's a sum of two KL divergences between P of our.",
                    "label": 0
                },
                {
                    "sent": "And a mixture of the two, and PMG and a mixture of the two.",
                    "label": 0
                },
                {
                    "sent": "And well, I'm just giving you the definition of the KL divergent here.",
                    "label": 0
                },
                {
                    "sent": "Questions on that.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "I find that funny.",
                    "label": 0
                },
                {
                    "sent": "For certain games.",
                    "label": 0
                },
                {
                    "sent": "We are like I live there.",
                    "label": 0
                },
                {
                    "sent": "I mean, you said you are maximizing my number.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Again, formulations adding up right, but it's with respect to different parameters, right?",
                    "label": 0
                },
                {
                    "sent": "With respect to the parameters of the discriminator, we're maximizing that with respect to the parameters of the generator.",
                    "label": 0
                },
                {
                    "sent": "We're minimizing it.",
                    "label": 0
                },
                {
                    "sent": "So that's why it forms a saddle point.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as forming a saddle point because in One Direction you want to maximize your heading uphill the other direction you're heading downhill, but these are orthogonal directions because they in parameter space.",
                    "label": 0
                },
                {
                    "sent": "These are two sets of parameters.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks for the question.",
                    "label": 0
                },
                {
                    "sent": "So so in practice.",
                    "label": 0
                },
                {
                    "sent": "However, Gans actually don't do this and this actually comes down to and this is something we noticed or we advocate right from the original paper.",
                    "label": 0
                },
                {
                    "sent": "It turns out that it essentially doesn't work very well to use the original Mini Max objective and said what we do is we recommend that you sent the gradient the parameters of G are learned by this objective function.",
                    "label": 0
                },
                {
                    "sent": "You want to maximize for the parameters of the generator.",
                    "label": 0
                },
                {
                    "sent": "This term here rather than minimize this quantity here, right?",
                    "label": 0
                },
                {
                    "sent": "Turns out that this is somewhat better behaved, but I still want to point out that even in this context that the model still can misbehave if the discriminator starts working.",
                    "label": 0
                },
                {
                    "sent": "Sort of becomes more effective than the general, so if it comes if the discriminant becomes too good at telling the difference between generated sample and a true sample, it can become unstable.",
                    "label": 0
                },
                {
                    "sent": "So in this context, what ends up happening is it's usually insisting on stable rather than rather than what happens if we were to use the original objective where.",
                    "label": 0
                },
                {
                    "sent": "This is the gradients go to zero for the generator.",
                    "label": 0
                },
                {
                    "sent": "Can you say that again?",
                    "label": 0
                },
                {
                    "sent": "In the original Minimax game, is the equilibrium unique?",
                    "label": 0
                },
                {
                    "sent": "I do well in the context of the parameters, I don't think so because it's a very non like the parameterisation of the model makes us look very non convex.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so that would not be unique.",
                    "label": 0
                },
                {
                    "sent": "Well, at least we can't show that it's unique.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so when we first published this work 2014, this is the kind of samples we got.",
                    "label": 0
                },
                {
                    "sent": "We were pretty impressed with them.",
                    "label": 0
                },
                {
                    "sent": "What I'm showing you here is actually a movie where we're tracing out sort of simple paths in zed space and then at each point in this path projecting that down into the image space.",
                    "label": 0
                },
                {
                    "sent": "So you're sort of interpreting this as kind of movies and moving along, let's say on the image manifold that it's learned.",
                    "label": 0
                },
                {
                    "sent": "Both for M Nestan for Cifar 10 over here.",
                    "label": 0
                },
                {
                    "sent": "So like I said, that was two years ago.",
                    "label": 0
                },
                {
                    "sent": "We've come a fairway.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Since then, these are the least squares scans result from 2017 I guess.",
                    "label": 0
                },
                {
                    "sent": "Yeah, February 2017.",
                    "label": 0
                },
                {
                    "sent": "So one thing that we notice is that as soon as you go, if you can manage to go to large images, things look much better.",
                    "label": 0
                },
                {
                    "sent": "Just sort of so it's a bit of a cheat.",
                    "label": 0
                },
                {
                    "sent": "Well, it's still.",
                    "label": 0
                },
                {
                    "sent": "It's still a very useful model.",
                    "label": 0
                },
                {
                    "sent": "In fact, I recommend you look at it, but they were one of the first ones to go up to 128 by 128, so it had a big impact.",
                    "label": 0
                },
                {
                    "sent": "'cause if you just compare samples 128 by 120 against samples 64 by 64, there's a dramatic difference right away, even for the same model.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so these are fairly good samples and I guess I don't.",
                    "label": 0
                },
                {
                    "sent": "You probably know that there's been a huge kind of amount of work in this area.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, this is sort of at some point.",
                    "label": 0
                },
                {
                    "sent": "The number of Gan papers.",
                    "label": 0
                },
                {
                    "sent": "This is a list that was kept track bye.",
                    "label": 0
                },
                {
                    "sent": "Bye yeah, there's been actually far more than that since, but it's going a little bit crazy in the sense that we have two LS games.",
                    "label": 0
                },
                {
                    "sent": "We have a vegan order vegan Tooheys 1 E. We have a lot of different variations on the theme.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You quantitatively this is the.",
                    "label": 0
                },
                {
                    "sent": "This is our paper here.",
                    "label": 0
                },
                {
                    "sent": "Original paper here.",
                    "label": 0
                },
                {
                    "sent": "Took some time before people caught on and then they kind of exploded and I'm hoping I'm actually kinda hoping we hit pecan here, but I'm not sure that's going to be true.",
                    "label": 0
                },
                {
                    "sent": "So now what I'd like to do is.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just talk a little bit more sort of step back and talk a little bit more about what I think is going on with fans, how we think we can improve them, and kind of where the field is going, yeah?",
                    "label": 0
                },
                {
                    "sent": "Scratch.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, so OK, good question so.",
                    "label": 0
                },
                {
                    "sent": "Alright, I don't know if I'm exactly answering your question, but the question was is it always trained by from scratch?",
                    "label": 0
                },
                {
                    "sent": "Is it possible to find tune again so these are general these are generative models, right?",
                    "label": 0
                },
                {
                    "sent": "So what were the target here?",
                    "label": 0
                },
                {
                    "sent": "The published thing typically is when we're done training this model, we throw away the discriminator an we publish the generator, right?",
                    "label": 0
                },
                {
                    "sent": "That's the product that's what we want.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure how you would intend to fine tune that generator, so it's like, for example, tell you what you might mean.",
                    "label": 0
                },
                {
                    "sent": "An I'll tell you it doesn't work, which is something like maybe what you could do is.",
                    "label": 0
                },
                {
                    "sent": "Pre training the discriminator for awhile and then try to train the generator on a pre trained discriminator that typically does not work very well.",
                    "label": 0
                },
                {
                    "sent": "There for interesting reasons which will actually come to in a bit, but yeah, so so just getting back to this.",
                    "label": 0
                },
                {
                    "sent": "So this is this is Dead Space interpolations.",
                    "label": 0
                },
                {
                    "sent": "You saw movies.",
                    "label": 0
                },
                {
                    "sent": "This is essentially the same thing, only laid out in time.",
                    "label": 0
                },
                {
                    "sent": "So what we have here is just some point in said space and we're just kind of moving through Dead Space throughout this whole thing.",
                    "label": 0
                },
                {
                    "sent": "And what's interesting about this is there all images right there?",
                    "label": 0
                },
                {
                    "sent": "All pretty much all of them are interpretable images.",
                    "label": 0
                },
                {
                    "sent": "What this means is that.",
                    "label": 0
                },
                {
                    "sent": "This idea that we talked about a little earlier about moving around in zed space and then you kind of move along the image manifold.",
                    "label": 0
                },
                {
                    "sent": "That really seems to be happening here, and it doesn't necessarily happen nearly as well in other kinds of models like things like VS. For example, because often involves you have these gaps in the prior.",
                    "label": 0
                },
                {
                    "sent": "So if you kind of just move under your prior distribution you end up hitting some of these gaps and it ends up looking like garbage.",
                    "label": 0
                },
                {
                    "sent": "We don't seem to see this with Gans and one of the reasons why I would like to suggest we don't see this is because what it's really doing, what these models are.",
                    "label": 0
                },
                {
                    "sent": "Doing so.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for that I want to just kind of, you know, indulge me a little bit.",
                    "label": 0
                },
                {
                    "sent": "This is my cartoon of what an image manifold looks like, right?",
                    "label": 0
                },
                {
                    "sent": "So this is in 2D, so I imagine I've just picked two pixel values here and I'm just showing you what sort of natural images all fall on one of these lines, right?",
                    "label": 0
                },
                {
                    "sent": "And so you can imagine what I'm really trying to represent here is that it's complicated, it's nonlinear, and there's a point here.",
                    "label": 0
                },
                {
                    "sent": "You know you can be on the manifold of images.",
                    "label": 0
                },
                {
                    "sent": "You can move off it.",
                    "label": 0
                },
                {
                    "sent": "You can be on it again.",
                    "label": 0
                },
                {
                    "sent": "You can move off it.",
                    "label": 0
                },
                {
                    "sent": "There's there's rich structure to this.",
                    "label": 0
                },
                {
                    "sent": "So what I think Ganz are doing is.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Something like this picture here.",
                    "label": 0
                },
                {
                    "sent": "So this thick blue line here represents the generative model structure.",
                    "label": 0
                },
                {
                    "sent": "The dynamics of the Gan game are such that it's encouraged to look like real images, but it's not particularly encouraged to cover all of the diversity that shown in the input right for shown in the training data, for example, and that's kind of being represented here, right?",
                    "label": 0
                },
                {
                    "sent": "So this is kind of really come close to this particular manifold, but it's essentially ignoring these other manifolds that are nearby.",
                    "label": 0
                },
                {
                    "sent": "So when we move around in Dead Space where sensually moving around on this manifold.",
                    "label": 0
                },
                {
                    "sent": "But we're not necessarily capturing all of the diversity that exists out there.",
                    "label": 0
                },
                {
                    "sent": "If you contrast that with something like, say, a variational autoencoder, my interpretation is it's doing something that looks a little bit more like this, right?",
                    "label": 0
                },
                {
                    "sent": "So what's going on here?",
                    "label": 0
                },
                {
                    "sent": "Is the variational auto coder is trained through maximum likelihood, putting a lot of pressure on it to at least give some probability mass to everything in the training data, right?",
                    "label": 0
                },
                {
                    "sent": "It's highly penalized if it's.",
                    "label": 0
                },
                {
                    "sent": "Presented a training example for which it has very, very low likelihood.",
                    "label": 0
                },
                {
                    "sent": "So what's that going to do?",
                    "label": 0
                },
                {
                    "sent": "Is it's going to use its capacity between these two?",
                    "label": 0
                },
                {
                    "sent": "My intention was to say the capacity is relatively the same, but they're using them in different ways.",
                    "label": 0
                },
                {
                    "sent": "This one is sort of spreading probability mass as it can to cover the data distribution as it can, but the consequences if we now sort of draw a sample from this distribution.",
                    "label": 0
                },
                {
                    "sent": "Here most of the time we're actually going to fall off the image manifold, right?",
                    "label": 1
                },
                {
                    "sent": "And This is why we see these samples are often quite a bit more blurry, right?",
                    "label": 0
                },
                {
                    "sent": "That's my kind of cartoon version of what's giving us blurry samples with things like maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "But now there's something about this, which is, if we take this seriously as what gangs are doing and what we think cancer look like at this point, I think I'm pretty comfortable thinking about manifolds when we talk about games, because it's a deterministic function from some low level dimension level representation Space Z, right?",
                    "label": 0
                },
                {
                    "sent": "Which we transform into some high dimensional concept.",
                    "label": 0
                },
                {
                    "sent": "It has to be a manifold that's parameterized to be a manifold in that space.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we take that seriously, that idea of this being a manifold, this is what we see.",
                    "label": 0
                },
                {
                    "sent": "So we have our data manifold here.",
                    "label": 0
                },
                {
                    "sent": "And let's imagine where some point in training.",
                    "label": 0
                },
                {
                    "sent": "So the two manifolds don't quite match up as we'd like, and this is argan manifold.",
                    "label": 0
                },
                {
                    "sent": "Here, right?",
                    "label": 0
                },
                {
                    "sent": "This our generative model is what it looks like here and what I'm going to do is just zoom in on something here that I can work with.",
                    "label": 0
                },
                {
                    "sent": "And it's simple right there.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm just going to say, OK, well, let's imagine this is our setting, right?",
                    "label": 0
                },
                {
                    "sent": "So we're in 2D again.",
                    "label": 0
                },
                {
                    "sent": "We have this this distribution here, which is our true data distribution.",
                    "label": 0
                },
                {
                    "sent": "And this is argan manifold.",
                    "label": 0
                },
                {
                    "sent": "Here, right?",
                    "label": 0
                },
                {
                    "sent": "And what I'm going to ask is, what does the Shannon, the Jensen Shannon Divergent look like?",
                    "label": 0
                },
                {
                    "sent": "This is the theory that we have for against this is the theory.",
                    "label": 0
                },
                {
                    "sent": "Up till recently that was essentially the best theory we had for how these models are working.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What does what does the Jensen Shannon say in a context like this?",
                    "label": 0
                },
                {
                    "sent": "Well, it actually isn't particularly useful, so this is Jensen Shannon Divergent's.",
                    "label": 0
                },
                {
                    "sent": "So again, I mean this is to be clear, this is not the setting where games are actually operating, so not the setting where we've ever claimed that they operate because the Jensen Shannon is the case where that we run the optimization of the discriminator.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's a it's a nonparametric discriminant, so it's an optimal discriminator.",
                    "label": 0
                },
                {
                    "sent": "But this is the theory we have for it, so in that context, the dentist.",
                    "label": 0
                },
                {
                    "sent": "Divergent actually is pretty degenerate, right?",
                    "label": 0
                },
                {
                    "sent": "So it's zero everywhere for Theta as a function of Theta, except for when you hit this line right?",
                    "label": 0
                },
                {
                    "sent": "The Zero line here, which gives it a value here.",
                    "label": 0
                },
                {
                    "sent": "So this is our objective function, right?",
                    "label": 0
                },
                {
                    "sent": "For training the generator, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, they're really important view, but one could ask.",
                    "label": 0
                },
                {
                    "sent": "Well, what if you just add some Gaussian noise around the genitive model?",
                    "label": 0
                },
                {
                    "sent": "Then you're not going to have zero derivatives, that's right.",
                    "label": 0
                },
                {
                    "sent": "But then we're back in the setting I would are.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Q We might be back in this setting.",
                    "label": 0
                },
                {
                    "sent": "Is that the kind of yeah?",
                    "label": 0
                },
                {
                    "sent": "But you what people do with these as they ignore the noise that you had at the end and they show the beans right?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah but but I'm saying that that that's not just giving us bad, it's actually training a model that is OK with being off the manifold.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's a good question.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is the context of sound version.",
                    "label": 0
                },
                {
                    "sent": "If we think about this is from an optimization perspective, this is our objective function.",
                    "label": 0
                },
                {
                    "sent": "Basically, if we want to generate if we think about from the perspective of the generator, this is not a particularly useful objective function, right?",
                    "label": 0
                },
                {
                    "sent": "Gradients do not point towards the Theta here.",
                    "label": 0
                },
                {
                    "sent": "I'm not claiming it's important to this.",
                    "label": 0
                },
                {
                    "sent": "I'm not claiming that this is what games are doing.",
                    "label": 0
                },
                {
                    "sent": "This is what the theory of Gans as it existed says they are doing or something like this, right?",
                    "label": 0
                },
                {
                    "sent": "At least I'm sort of abusing the theory of games in this context.",
                    "label": 0
                },
                {
                    "sent": "But I'm doing this for a point.",
                    "label": 0
                },
                {
                    "sent": "So this is clearly not a very accurate picture of how we think games are actually training, because this would not train.",
                    "label": 0
                },
                {
                    "sent": "This would not find this point right.",
                    "label": 0
                },
                {
                    "sent": "There is no gradient information to find that point, so there would be no way of if we were to take the Jensen Shannon seriously, there would be no way for this manifold element to move forward towards the true data distribution.",
                    "label": 0
                },
                {
                    "sent": "The true data manifold here.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the case of Jensen, Shannon, but there are other distance metrics out there, and one which has been relatively recently introduced as being interesting in the Gan context is called the Wash Machine or earthmovers distance, and that's just defined by this thing here, so it's the distance between a draw a point X&Y each from one from the real distribution, one from the data generating distribution.",
                    "label": 0
                },
                {
                    "sent": "You measure their distance, and the washer steam distances basically infima of that.",
                    "label": 0
                },
                {
                    "sent": "Right, so you can think of this as the minimum cost of transporting mass.",
                    "label": 0
                },
                {
                    "sent": "Well, well match transporting mass from distribution R2, or from distribution PFG.",
                    "label": 0
                },
                {
                    "sent": "Let's say two distribution.",
                    "label": 0
                },
                {
                    "sent": "R. That's actually a better way of thinking about this.",
                    "label": 0
                },
                {
                    "sent": "And what's interesting is this.",
                    "label": 0
                },
                {
                    "sent": "This this earthmovers distance is continuous everywhere and differentiable almost everywhere under some mild assumptions on the transport.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But so far, if we return to our simple example here, and this again is actually an example from the original Washington Gan Paper.",
                    "label": 0
                },
                {
                    "sent": "So if you could evaluate the Wasserstein distance for this case, again, what we had was this degenerate Delta function, but in our car.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Text here, this is what the washer steam gives you, which is actually much more tractable.",
                    "label": 0
                },
                {
                    "sent": "Much more much more natural function, and something that we can get gradients on.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So look just a representation of the gradients here.",
                    "label": 0
                },
                {
                    "sent": "So now with this from a theoretical point of view, we can now say, well, now we actually have a theory or from a theoretical point of view we actually have a way to train to this model.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this motivated.",
                    "label": 0
                },
                {
                    "sent": "Essentially the washer seen Gan BI Mart, now Ski and others at NYU you so the so the washer Steam gang.",
                    "label": 0
                },
                {
                    "sent": "Now here's the difference right?",
                    "label": 0
                },
                {
                    "sent": "So the if we look at the washer steam distance here it has this form right?",
                    "label": 0
                },
                {
                    "sent": "But the problem with this form is it's in general, intractable, right?",
                    "label": 0
                },
                {
                    "sent": "We can't actually work with this.",
                    "label": 0
                },
                {
                    "sent": "But we can use something called the Kantorovich Rubinstein duality, which actually expresses it in this form, and what's interesting about this is that this is pretty close to what we've already seen in the context of Gans, something called an energy based game that was introduced also at NYU.",
                    "label": 0
                },
                {
                    "sent": "You about a year ago or so year and a half ago that looked that basically had exactly this form, so it's essentially a critic now.",
                    "label": 0
                },
                {
                    "sent": "So instead of this F function here, we're going to critic rather than calling it a discriminator, because it's just, it's output is real valued.",
                    "label": 0
                },
                {
                    "sent": "Energy based game by the way, you can interpret this as being an energy function output and and you have the this term here.",
                    "label": 0
                },
                {
                    "sent": "So this is the expectation under the probability under the generator.",
                    "label": 0
                },
                {
                    "sent": "Here of that energy, or that critic.",
                    "label": 0
                },
                {
                    "sent": "And it's the difference between that and the expectation under the real value of that same critic.",
                    "label": 0
                },
                {
                    "sent": "And here the difference is that we have a soup.",
                    "label": 0
                },
                {
                    "sent": "No, sorry, the soup is part of the energy base can what the difference is this?",
                    "label": 0
                },
                {
                    "sent": "This thing here there's actually a constraint on the F on our critic.",
                    "label": 0
                },
                {
                    "sent": "And that critic now is the constraint is that it's actually the supremum is over one Lipschitz functions, so that's the difference.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, if you remove this constraint, this will be already a form of Gan that we've seen in the literature.",
                    "label": 0
                },
                {
                    "sent": "They've just added this constraint, but this constraint makes all the difference with that constraint makes this Gan game equivalent to from the from the generator's point of view, the critic training the critic ends up making this look like a washer some distance.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so this basically motivates the W gamble Washington GAN objective function, which is just I'm getting minimax game in this form.",
                    "label": 0
                },
                {
                    "sent": "Right, so now the question is.",
                    "label": 0
                },
                {
                    "sent": "In this context, how do we enforce the Lipschitz constraint on the critic?",
                    "label": 0
                },
                {
                    "sent": "So what they propose in the original paper was to clip the weights to some compact space between some constants, so every every weight is going to click between minus C&C, and that's actually going to be result in some subset of K Lipschitz function, so it's still smoothness.",
                    "label": 0
                },
                {
                    "sent": "It's not one Lipschitz, but that's actually fine.",
                    "label": 0
                },
                {
                    "sent": "It's actually OK for it to be just K Lipschitz, as long as we can bound K, which we can in this context.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the problem with that is it turns out to be not a particularly great way of enforcing Lipschitz smoothness, and I'll just give you a particular.",
                    "label": 0
                },
                {
                    "sent": "What it does.",
                    "label": 0
                },
                {
                    "sent": "Is it under uses capacity, and it actually leads to can lead to exploding and vanishing gradients through the discriminator.",
                    "label": 0
                },
                {
                    "sent": "So first of all, for underusing capacities, when you end up training these models and you use weight clipping, this is the weight distribution you tend to see, which means that all of the weights end up being sort of smashed up against their constraint.",
                    "label": 1
                },
                {
                    "sent": "Which really means like you have the capacity of like something roughly like a binary weight model, which is quite a bit different than real valued weights.",
                    "label": 0
                },
                {
                    "sent": "And Secondly you have these.",
                    "label": 0
                },
                {
                    "sent": "The exploding and vanishing gradients depending on different values of the clipping you use, you either get exploding gradients or vanishing gradients.",
                    "label": 1
                },
                {
                    "sent": "When you back propagate through the layers of the discriminator, neither one is obviously desirable.",
                    "label": 0
                },
                {
                    "sent": "So what we wanted to do, so we kind of improved on this work or we.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Introduced our own way of trying to enforce some sort of smoothness on our critic or discriminator.",
                    "label": 0
                },
                {
                    "sent": "In other words, while avoiding some of these pitfalls and the way we did this is essentially draw some inspiration from the theory as well.",
                    "label": 0
                },
                {
                    "sent": "So in theory what we have is the W game critic.",
                    "label": 0
                },
                {
                    "sent": "If so, if we draw some X~ from the generator distribution that there is a point X from the real distribution such that all points in between those two on the straight line between those two.",
                    "label": 0
                },
                {
                    "sent": "The gradient is this.",
                    "label": 0
                },
                {
                    "sent": "Namely, the gradient has norm one, so this is what we're going to actually introduce as a penalty.",
                    "label": 0
                },
                {
                    "sent": "So we're actually instead of using a weight clipping, we're actually going to use a penalty that encourages the gradient with respect to the input X.",
                    "label": 0
                },
                {
                    "sent": "Here to be of Norm, one of the discriminator.",
                    "label": 0
                },
                {
                    "sent": "And then we have some parameter here controlling the degree of the penalization.",
                    "label": 0
                },
                {
                    "sent": "So the question is how do we pick this this this point?",
                    "label": 0
                },
                {
                    "sent": "Here X hat here.",
                    "label": 0
                },
                {
                    "sent": "So what we do is according to the theory, it's a particular point, right?",
                    "label": 0
                },
                {
                    "sent": "It's according to the true transport from one point to the other.",
                    "label": 0
                },
                {
                    "sent": "We don't have access to that.",
                    "label": 0
                },
                {
                    "sent": "So what we pick?",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just some points we pick a point.",
                    "label": 0
                },
                {
                    "sent": "We just draw a point at random from our generator.",
                    "label": 0
                },
                {
                    "sent": "We pick a data sample at random and we pick some point uniformly between the two and we penalize that point.",
                    "label": 0
                },
                {
                    "sent": "So on average, that's going to give us the region between our data distribution are true data manifold and our manifold of our generator model.",
                    "label": 0
                },
                {
                    "sent": "Generative model is going to be regularised in this way the space between them through this mechanism.",
                    "label": 0
                },
                {
                    "sent": "So if we do that.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the kind of results we see.",
                    "label": 0
                },
                {
                    "sent": "And that's basically yeah, so this is.",
                    "label": 0
                },
                {
                    "sent": "This is our model.",
                    "label": 0
                },
                {
                    "sent": "This is with WN clipping.",
                    "label": 0
                },
                {
                    "sent": "This is just LLS Gannon, this DC can.",
                    "label": 0
                },
                {
                    "sent": "So what we're showing you here, it's important to point out we're sort of doing stress tests right.",
                    "label": 0
                },
                {
                    "sent": "Our claim with this is that we end up with an objective function that is easier to train, and one of the things that didn't mention is that both began and our model expected from the theoretical point of view, we actually expect the discriminator to be optimal, so we actually run it for a few updates for every update of the.",
                    "label": 0
                },
                {
                    "sent": "Of the generator.",
                    "label": 0
                },
                {
                    "sent": "So sorry the critic we expect it to be optimal so it's updated a few times so it's actually slower, but we see it's quite a bit more stable than these other forms of gangs.",
                    "label": 0
                },
                {
                    "sent": "And again, we're stress testing it, so we're just changing the model architectures and you're trying to be fair to all the models, so we've just picked one set of hyperparameters for all of them.",
                    "label": 0
                },
                {
                    "sent": "We picked ours in advance before doing this experiment, and we just ran them all and to see what kinds of results we get so.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is just a different set.",
                    "label": 0
                },
                {
                    "sent": "So for example this is 10 H nonlinearities everywhere, 101 layer resident.",
                    "label": 0
                },
                {
                    "sent": "This is actually fairly challenging model to train again on 101 layer resident again, these are just.",
                    "label": 0
                },
                {
                    "sent": "These are certainly not optimal model configurations that you would want.",
                    "label": 0
                },
                {
                    "sent": "We're just essentially testing or training algorithm and we see that ours is doing fairly well in all of these cases.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, this is what we see.",
                    "label": 0
                },
                {
                    "sent": "So in Inception score here is just.",
                    "label": 0
                },
                {
                    "sent": "You can think of this as a measure of how realistic the objects are in terms of the images are in terms of expressing objects.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into describing exactly what it is, but it encourages diversity and encourages some notion of object nests to the images, and you can see this is our model AR.",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the two forms of gradient penalty here in.",
                    "label": 0
                },
                {
                    "sent": "I guess that's Orange and green.",
                    "label": 0
                },
                {
                    "sent": "Compared to DC Gan in red here and just use of weight clipping so on this score it actually you can see from here if we look at just iterations you know we're roughly the same.",
                    "label": 0
                },
                {
                    "sent": "Maybe DC game is actually kind of outperforming on this model.",
                    "label": 0
                },
                {
                    "sent": "This were by the way using the DC Gan architecture here, so it's not too surprising that it's doing very well compared to our model here, but if we look at Wall Clock time, there's actually a really big difference, right?",
                    "label": 0
                },
                {
                    "sent": "And this is where that 5 updates for our critic.",
                    "label": 0
                },
                {
                    "sent": "For every update of the of the generator shows up, so there is actually still quite an issue in terms of the capacity in terms of scaling this thing up.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so we're pretty much near the end of my time.",
                    "label": 0
                },
                {
                    "sent": "I actually do have something else to talk about, but I'm not going.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to skip it.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Super super quickly, I'm just going to say that we've also built a model called Ali.",
                    "label": 0
                },
                {
                    "sent": "I encourage you to check it out.",
                    "label": 0
                },
                {
                    "sent": "I'm pretty excited by it.",
                    "label": 0
                },
                {
                    "sent": "Adversarially learned inference.",
                    "label": 0
                },
                {
                    "sent": "The idea here is that with, again, we've sort of sidestep this notion of doing inference, right?",
                    "label": 0
                },
                {
                    "sent": "We don't need to do inference, but maybe you still want to do inference, but you want to sort of do this in the context of an adverse aerial learning setup so we can do that with Allie.",
                    "label": 0
                },
                {
                    "sent": "So we're basically folding in, so this is the original decoder, so we re introduce an encoder into this model.",
                    "label": 0
                },
                {
                    "sent": "And now we're expressing a game between the encoder and the decoder.",
                    "label": 0
                },
                {
                    "sent": "So and the discriminator here is actually over X&Z.",
                    "label": 0
                },
                {
                    "sent": "And where we have the prior distribution is sort of pinning the model down here and the data distribution is pinning the model down there.",
                    "label": 0
                },
                {
                    "sent": "So that's the set up and I'll just show.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In my last minute, what happens if you do this in one dimension so it's actually quite pretty complicated, but this is the.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just a simulation of how this thing looks, so again, the context is we're just kind of playing around.",
                    "label": 0
                },
                {
                    "sent": "Can this thing learn?",
                    "label": 0
                },
                {
                    "sent": "And what we've asked is the encoder, so we've pinned down the true data to just be a simple Gaussian distribution and the prior to be a simple Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "And we initialize the weights to be 0 between the two, and we're seeing if we can actually learn a mapping between the two.",
                    "label": 0
                },
                {
                    "sent": "And you can see it roughly does a OK job of learning this sort of identity function between the two.",
                    "label": 0
                },
                {
                    "sent": "There's two reasonable solutions to this, there's identity, and then there's sort of the.",
                    "label": 0
                },
                {
                    "sent": "The one minus identity function or sorry like it's minus identity function.",
                    "label": 0
                },
                {
                    "sent": "It happens to find the identity function, but you can actually see just from looking at this that it's actually a pretty tricky thing because you have now three missed three moving pieces, right?",
                    "label": 0
                },
                {
                    "sent": "You have an encoder with such as parameters.",
                    "label": 0
                },
                {
                    "sent": "You have a decoder which has parameters and you have the discriminator, so the encoder and decoder are working together to try to fool the discriminator, and the discriminator is just trying to tell them apart.",
                    "label": 0
                },
                {
                    "sent": "The encoder in the decoder.",
                    "label": 0
                },
                {
                    "sent": "So I'll just end.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Real quick with showing you kind of our latest kind of results with this kind of model, we built a hierarchical version of this model and over here what we have are.",
                    "label": 0
                },
                {
                    "sent": "Wait?",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just for show you samples, these are the samples that we kind of get from this model on celebree.",
                    "label": 0
                },
                {
                    "sent": "So again, we don't have great ways of evaluating this, but so sort of relying on samples, so these are 120 by 128 samples.",
                    "label": 0
                },
                {
                    "sent": "Again, it looks a lot better than if you use 64 by 64.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are reconstructions, so what's interesting about this model is the hierarchical version.",
                    "label": 0
                },
                {
                    "sent": "So while we're showing reconstructions and why this is interesting, is that this model was not trained to do reconstructions at all.",
                    "label": 0
                },
                {
                    "sent": "It was nowhere in the objective function, unlike something like a VE that has part of the objective function to do a reconstruction.",
                    "label": 0
                },
                {
                    "sent": "This model is only trained to try to fool the discriminator, and yet if we give it the latent variables, it reproduces things that look pretty close to the input, and these are on test examples.",
                    "label": 0
                },
                {
                    "sent": "So that thing I was talking about earlier about we're being concerned that these gains are somehow under fitting the test distribution.",
                    "label": 0
                },
                {
                    "sent": "This is directly probing that question.",
                    "label": 0
                },
                {
                    "sent": "If you remember that was the case where we had like 3 lines of the manifold and my cartoon was at games were just kind of covering one of those manifolds.",
                    "label": 0
                },
                {
                    "sent": "What this is basically showing us is that it's actually not doing as bad a job as maybe we would have feared to cover the full variability over the training data.",
                    "label": 0
                },
                {
                    "sent": "Because these examples are essentially it's never trained on its impact test distributions, and yet it's still able to learn to be a reasonably good inverse model, which is what it's really trying to do over those samples.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I think that's my time, so I think I'll end there.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}