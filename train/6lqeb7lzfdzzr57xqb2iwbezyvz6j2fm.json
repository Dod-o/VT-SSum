{
    "id": "6lqeb7lzfdzzr57xqb2iwbezyvz6j2fm",
    "title": "Accelerated Adaptive Markov Chain for Partition Function Computation",
    "info": {
        "author": [
            "Stefano Ermon, Department of Computer Science, Cornell University"
        ],
        "published": "Sept. 6, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/nips2011_ermon_computation/",
    "segmentation": [
        [
            "So in this paper we consider the problem of computing the partition function that is the normalization constant for a factor.",
            "Probabilistic models such as Markov random fields an."
        ],
        [
            "And Mark of logic networks.",
            "So as you might know, computing the partition function exactly is hard, and so we consider an approximation based on sampling.",
            "Our method is based on the flat histogram idea or a one Landau algorithm, which is a fairly recent development in statistical physics.",
            "The idea is to 1st partition the space of all possible configurations or variable assignments according to the energy or the negative log likelihood, and then set up an adaptive Markov chain that will eventually visit all the subsets equally often.",
            "That is, it will spend the same amount of time at all energy levels.",
            "And notice that this is very different from what Standard Gibbs sampler we do, which samples according to the bottom and wait.",
            "So at the end, this algorithm produces an estimate of the size of the subsets.",
            "That's coordinated States and this can be used to compute the party."
        ],
        [
            "Function.",
            "So in this paper we propose we introduce two major contributions.",
            "That improve this algorithm.",
            "The first one is to use different way to partition the state space.",
            "We use a single bucket energy bucket for all the high energy configurations.",
            "This leads to upper bound on true value of the partition function, which we found to be tight in practice.",
            "And it also reduces the number of subsets that we need to visit, and therefore it makes the convergence faster.",
            "The second improvement is to introduce a clever way to select which variables to change or to flip.",
            "The idea comes from local search, SAT solvers and basically by changing more promising variables, we are able to find the low energy configurations faster.",
            "We reduce the number of non moves in the chain and therefore we achieve as you can see there and therefore we achieve faster convergence.",
            "And finally, this flat histogram idea is popular in statistical physics because the density of states not only gives the value of the partition function, but it allows us to compute the value of Z for all possible values of the temperature of the system, and in this paper we show how to generalize this idea to more complex parameterized probabilistic models.",
            "That is, with more parameters and we showed that by defining the right partition of the of the state space and computing the corresponding density of states, we are able to evaluate the partition function for all possible values of the of the parameters.",
            "So for example, for possible weights of the formulas, enough Markov logic representation, and we show that an application of this idea to wait."
        ],
        [
            "So experimentally we show that our improved algorithm is scales significantly better than the original one.",
            "As you can see in the upper figure.",
            "2nd, we showed that the provides very accurate estimates of the partition function as compared to variational methods such as three related belief propagation, standard samplings and others.",
            "And finally we show an application of the weight learning idea for a synthetic and mark of logic networks where we show that the models that we train can achieve close to optimal likelihoods.",
            "For that for the data.",
            "And so if you want to hear more, please come to my poster."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this paper we consider the problem of computing the partition function that is the normalization constant for a factor.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic models such as Markov random fields an.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And Mark of logic networks.",
                    "label": 0
                },
                {
                    "sent": "So as you might know, computing the partition function exactly is hard, and so we consider an approximation based on sampling.",
                    "label": 1
                },
                {
                    "sent": "Our method is based on the flat histogram idea or a one Landau algorithm, which is a fairly recent development in statistical physics.",
                    "label": 0
                },
                {
                    "sent": "The idea is to 1st partition the space of all possible configurations or variable assignments according to the energy or the negative log likelihood, and then set up an adaptive Markov chain that will eventually visit all the subsets equally often.",
                    "label": 1
                },
                {
                    "sent": "That is, it will spend the same amount of time at all energy levels.",
                    "label": 0
                },
                {
                    "sent": "And notice that this is very different from what Standard Gibbs sampler we do, which samples according to the bottom and wait.",
                    "label": 1
                },
                {
                    "sent": "So at the end, this algorithm produces an estimate of the size of the subsets.",
                    "label": 0
                },
                {
                    "sent": "That's coordinated States and this can be used to compute the party.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "So in this paper we propose we introduce two major contributions.",
                    "label": 0
                },
                {
                    "sent": "That improve this algorithm.",
                    "label": 0
                },
                {
                    "sent": "The first one is to use different way to partition the state space.",
                    "label": 0
                },
                {
                    "sent": "We use a single bucket energy bucket for all the high energy configurations.",
                    "label": 1
                },
                {
                    "sent": "This leads to upper bound on true value of the partition function, which we found to be tight in practice.",
                    "label": 1
                },
                {
                    "sent": "And it also reduces the number of subsets that we need to visit, and therefore it makes the convergence faster.",
                    "label": 0
                },
                {
                    "sent": "The second improvement is to introduce a clever way to select which variables to change or to flip.",
                    "label": 0
                },
                {
                    "sent": "The idea comes from local search, SAT solvers and basically by changing more promising variables, we are able to find the low energy configurations faster.",
                    "label": 0
                },
                {
                    "sent": "We reduce the number of non moves in the chain and therefore we achieve as you can see there and therefore we achieve faster convergence.",
                    "label": 0
                },
                {
                    "sent": "And finally, this flat histogram idea is popular in statistical physics because the density of states not only gives the value of the partition function, but it allows us to compute the value of Z for all possible values of the temperature of the system, and in this paper we show how to generalize this idea to more complex parameterized probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "That is, with more parameters and we showed that by defining the right partition of the of the state space and computing the corresponding density of states, we are able to evaluate the partition function for all possible values of the of the parameters.",
                    "label": 0
                },
                {
                    "sent": "So for example, for possible weights of the formulas, enough Markov logic representation, and we show that an application of this idea to wait.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So experimentally we show that our improved algorithm is scales significantly better than the original one.",
                    "label": 0
                },
                {
                    "sent": "As you can see in the upper figure.",
                    "label": 0
                },
                {
                    "sent": "2nd, we showed that the provides very accurate estimates of the partition function as compared to variational methods such as three related belief propagation, standard samplings and others.",
                    "label": 0
                },
                {
                    "sent": "And finally we show an application of the weight learning idea for a synthetic and mark of logic networks where we show that the models that we train can achieve close to optimal likelihoods.",
                    "label": 1
                },
                {
                    "sent": "For that for the data.",
                    "label": 0
                },
                {
                    "sent": "And so if you want to hear more, please come to my poster.",
                    "label": 0
                }
            ]
        }
    }
}