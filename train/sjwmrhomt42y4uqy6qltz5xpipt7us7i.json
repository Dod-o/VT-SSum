{
    "id": "sjwmrhomt42y4uqy6qltz5xpipt7us7i",
    "title": "Stability of Transductive Regression Algorithms",
    "info": {
        "author": [
            "Ashish Rastogi, Courant Institute of Mathematical Sciences, New York University (NYU)"
        ],
        "published": "Aug. 1, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Algorithmic Information Theory"
        ]
    },
    "url": "http://videolectures.net/icml08_rastogi_stra/",
    "segmentation": [
        [
            "Good afternoon everybody.",
            "Is a talk about using the notion of algorithmic stability to derive generalization bounds for transductive regression algorithms.",
            "My name is Ashish Rastogi and this is joint work with Carina Cortez who's at Google.",
            "My advisor Murray who's there and why you and Google and Dimitri pageant pageant.",
            "Who's at technion?"
        ],
        [
            "Alright, so let me introduce the transductive setting.",
            "We're all quite familiar with the setting of induction, in which the inductive learning algorithm receives a training set which consists of examples that are drawn in an IID fashion from a fixed but unknown distribution D and the objective of the learning algorithm is to produce a hypothesis that does well on an unknown point, also drawn according to this distribution.",
            "So the expected squared error is the performance measure in the trans doctors in the transductive setting.",
            "However, the entire universe of training plus test.",
            "Elements is available to you before hand and the order of the.",
            "Learning algorithm is to produce labels for the test set and so your performance measure is simply the average error on the test set.",
            "No other points are ever going to be considered by the learning algorithm, so the transductive setting is."
        ],
        [
            "Has received a lot of attention recently and has a lot of good motivation, mainly in real world problems.",
            "There's much more unlabeled data and labeling data is expensive, both in terms of time and money, so you can think of applications like you know, assigning scores to web pages you might have some kind of system that assigns to each web page it's quality, and you have a vast number of web pages, but for a small subset of them you've actually hired humans to assign some kind of a score, and you want to.",
            "Then build a system that assigns scores to other pages.",
            "So this would be an example of a transductive.",
            "Problem also there are applications in computational biology and recently the Netflix Challenge has also received a lot of attention in the machine learning community.",
            "As formulated, the Netflix Challenge is a transductive regression.",
            "And this picture shows or gives an illustration of the Netflix challenge.",
            "So on the rules you have users and on the columns you have movies and some of these users have seen some of these movies and assigned some ratings.",
            "The first user has assigned a rating of five stars to the first movie, and then you have in mind some target user who has seen some movies which is the last two in this case.",
            "And then for some other sets of movies you want to predict you know what their preference for these movies is going to be like and the key for the Netflix challenge is that the movies on which you want to make the predictions are available beforehand to the algorithm.",
            "So this is exactly the transductive setting.",
            "So in this paper in this paper we consider the transductive regression algorithms and.",
            "Use the notion of algorithmic stability to prove generalization bounds, and the advantage of stability for professionalization is that it's algorithmics algorithm specific and often gives tighter generalization bounds then, once there are based on VC dimension's, another complexity measures."
        ],
        [
            "So now I'll this is the outline of the talk I briefly mentioned some motivation for this work.",
            "Next, I'll introduce the transductive regression problem, then introduce the notion of stability and then say what we can show about stable learning algorithms and what kind of generalization properties they have.",
            "Then we'll look at various transductive regression algorithms that are actually used in practice and see what kind of stability bound they possess an.",
            "Finally, we will present.",
            "I'll present some experiments that demonstrate the.",
            "Usefulness of the bound that we prove."
        ],
        [
            "Alright, so here is the transductive regression problem.",
            "You're given the entire universe of M + U elements M for the rest of this talk will denote the number of labeled examples and you will denote the number of unlabeled examples.",
            "So in the transductive setting you're given the entire set X of N plus you elements X one through XN plus you, let's say, and the labeling information for a subset of M points is revealed to the algorithm.",
            "So by one through YM will be the training set, and the objective is to find a hypothesis.",
            "Edge that assigns scores which are real numbers 'cause we're in the regression setting two elements in X such that the error on the unlabeled points which is given by this expression.",
            "1 / U time times some of the errors on each unlabeled point is small and a little bit of standard notation.",
            "Our head our head of age will denote the training error and it has the usual formulation.",
            "An RH will denote the test error.",
            "So this is these are the kinds of algorithms that we will want to analyze using the notion of stability so."
        ],
        [
            "Now let me introduce stability.",
            "The ability is that a small change in the training set leads to a small change in the learned hypothesis.",
            "Let's illustrate this with the diagram.",
            "So let's say this big Oval is your set X and it's partitioned into labeled an unlabeled data in the following way on the left hand side you have the labeled points and the numbers on each.",
            "You can think of these as movies, and the numbers are the ratings which from the picture that we saw a few slides ago and on the right hand side you have unlabeled points because you don't know what the.",
            "Ratings are for these movies and let's say that this partition is partition one and call it.",
            "It produces a hypothesis HS."
        ],
        [
            "Swap one training and test point.",
            "You get another partition which is very close to the previous partition, so.",
            "And let's say that this produces a hypothesis edge of S prime.",
            "Then the learning algorithm is set to be bitter stable.",
            "If the error for all points X between two hypothesis hypothesis is not different by more than bitter in absolute value.",
            "So if you look at the squared.",
            "So this is the square difference.",
            "The error of point X on edges square.",
            "And this is the error of point X on a chess primed and you're just saying that for all points X this difference is no more than better.",
            "So this is the.",
            "Notion of algorithm stability.",
            "An inductive setting.",
            "Stability has been used to prove generalization bounds for regression and classification, and this was work done by Busquet analysis and we seek to do something similar for the transductive setting."
        ],
        [
            "So what can we show?",
            "We show the following.",
            "So for any transductive regression algorithm that has a stability parameter, beta and such that the hypothesis scores are bounded away from the true scores by at most some capital B, then with high probability the test error is bounded by the training error plus some term that depends upon both the stability and explicitly on the number of labeled points M and the number of unlabeled points U in the following manner.",
            "And the thing to an.",
            "There's Alpha of em, Use that for this talk, you should just think of it as a mu divided by N + U.",
            "Some arithmetic is there, but for large MU doesn't matter so.",
            "The thing to take away from this is to see that what happens as MN you increase and indeed as M and you increase if stability of the algorithm decreases then you do get the fact that a test error sort of is bounded by the training error, which is something that we like to see.",
            "So the way to show this bound is."
        ],
        [
            "While looking at the random variable, which is the difference of the test and the training error and what we want to do is, we want to use a concentration of measure bound on this random variable.",
            "And indeed in the inductive setting this is what people do and boost caneles if actually used the mcdermotts inequality and Mcdermott's inequality says that for a function of some independent random variables whose dependence on each individual random variable is small, which is captured captured by this condition, which says that.",
            "The value of fire doesn't change by more than CI if you only change the value of the random variable in position I, then the probability that the random variables by deviates from its expected value by more than epsilon is exponentially bounded in these deviations CI squared and into epsilon squared, and we want to use a similar concentration bound.",
            "However, in the setting of transduction, since random variables are not independent, but there without replacement we need.",
            "A new concept."
        ],
        [
            "Bound, and indeed we did have a new concentration.",
            "I should mention that this was independently discovered by Alien, even Pacione in 2007.",
            "However, we do provide simpler proof of this concentration bound, so this is what the concentration bounds says.",
            "If X1 through XM, and you should think of these as the labeled points out of your universe of M plus you elements.",
            "So if X1 through XM is a sequence of M, random variables that are all drawn from some universe X of N plus you elements without replacement.",
            "And you have a function Phi, whose dependence on any one of these random variables is bounded by at most C, just like in the mcdermotts inequality.",
            "Then the probability that fire deviates from its expected value by more than epsilon is exponentially bounded.",
            "In these deviations, an instead of M which you had in the McDonald's inequality, you have Alpha of MU, which is M times you divided by N plus you.",
            "So you can think that this inequality is sharper by the sampling fraction.",
            "And sanity check as you which is the number of unlabeled points, go to Infinity.",
            "You recover Mcdermotts inequality so.",
            "So the way we use this concentration bound to derive a generalization bound is, as I said, you consider the random variable, which is a difference of the test in the training error that becomes your fi.",
            "You provide abound on expected value of five and you also provide a bound on five of the the C, which is how much does 5 change when you just change one point, and that's where the notion of stability comes in.",
            "And if you plug everything in you.",
            "You get, you get the generalization."
        ],
        [
            "Andre mentioned in the previous slide.",
            "OK, now we're going to move and look at some algorithms and try and apply this bound to those algorithms.",
            "So the first algorithm that we consider is the algorithm of local transductive regression to Curtis Anne Murray from a paper in 2006.",
            "This is a quick description of the algorithm.",
            "In this algorithm we map points to a high dimensional feature space, let's say of dimension Capital N An.",
            "We consider linear hypothesis in this in this space, so it of X is.",
            "Some weight vector www.product with five, 5X and the way we leverage unlabeled points is we use local estimates for unlabeled points.",
            "So for each unlabeled point we look at a small neighborhood around the unlabeled point neighborhood of radius R, and in fact, this parameter R will be will come up again later in experiments, and then we use come some kind of weighted average of the labeled points in this neighborhood, and that average is what we call the estimate label for this unlabeled point.",
            "Can we introduce?",
            "This quantity, for the unlabeled points.",
            "So if you look at the first 2 terms, there's the regularization term and there's a training error.",
            "They look very familiar from the inductive setting, and now for the transductive setting.",
            "We introduce, for the unlabeled points, the error with respect to estimate labels, and we traded off with a constant C primed so we minimize this objective function.",
            "This objective function has many nice properties.",
            "It has a closed form solution which involves matrix inversion and it is Colonel Isable.",
            "Of the minimization is capital N cubed, where capital N is the number of features.",
            "If you do it in the primal plus N ^2 * N + U, or if you do it in the dual, it requires inversion of the gram matrix, so it's order of M plus you cubed, where M is the number of labeled points and use the number of points.",
            "So what we want to do is we want to figure out what is the stability coefficient of this transductive regression algorithm, we."
        ],
        [
            "Use convexity based analysis and this is what we are able to show.",
            "So suppose the following hold.",
            "Suppose for all points X the target values are bounded by some capital M an.",
            "Suppose that the kernel is bounded, which is to say that for all points XK of XX is less than Kappa squared and the algorithm that you used to generate estimate labels, which was some kind of a weighted average.",
            "Or you can choose whatever algorithm that you want has its own stability parameter, which is we call local stability, so we'll call it better log.",
            "Then local transductive regression has the following stability.",
            "It's this complicated expression and what we're interested in is the dependence on small M. The dependence in small U and the dependence on better lock.",
            "And So what we notice is that the small, even small you are always in the denominator, which is good because as these things go to Infinity, this algorithm gets more and more stable and we notice that better lock is in the numerator.",
            "So it also says that as the local estimator gets more and more stable.",
            "The algorithm gets more and more stable.",
            "So this is this is the stability that we're able to show for local."
        ],
        [
            "Adaptive regression algorithms.",
            "So we can put everything that we showed so far together and so we have this generalization bounds for local transductive regression algorithms, it's just."
        ],
        [
            "Previous slides.",
            "OK, so that analysis was carried out based on the convexity of the objective function.",
            "There is another an analysis that you can carry out based on closed form solutions and many transductive regression algorithms as we saw in the previous presentation, can be formulated as some combination of a fit term and a smoothness term.",
            "So the first term here is a smoothness term which is your vector vector, H is a vector of your predictions and Q is some smoothness matrix.",
            "And why is your true answers so for the.",
            "Labeled points, it's your true answers and unlabeled points.",
            "It could either be your estimates, or it could be.",
            "It's in fact, zero.",
            "A lot of the times, and C is some matrix that assigns weights to these points.",
            "It's often diagonal in paper in practice.",
            "So, and this formulation includes various algorithms that are well studied in the transductive setting."
        ],
        [
            "So we can show the following bound.",
            "So let Lambda small M of Q denotes the minimum eigenvalue of a matrix, Q an Lambda capital M of Q denote the maximum eigenvalue of matrix cube an.",
            "Let H1 and H2B.",
            "The solutions for this formulation.",
            "When training and test points are swapped in one position, which is exactly the difference that you need to bound for stability, then what we can show is that for all points XI.",
            "The difference in scores assigned by H1 and H2, which are the two hypothesis that are produced when you swap the training and test points and exactly in exactly 1 position, is bounded by this expression, and the expression depends upon the maximum eigenvalue of Q.",
            "The minimum eigenvalue of Q and the maximum eigenvalue of matrix C. And what we can do is we can look at various algorithms and plug in the various Q matrices and the various matrices that those specific algorithms use.",
            "An see what kind of stability we get and see if there if we can prove stability using this technique, which is that.",
            "Do the algorithms get more and more stable as you see more and more labeled points for unlimited?",
            "Um?"
        ],
        [
            "So in the algorithms we study various algorithms in the paper we study the consistency method, the LL regularization, which is local learning regularization and the Gaussian mean fields.",
            "And in all of these algorithms the estimate labels are zero.",
            "Recall that in the local transductive regression the estimate labels were some kind of a weighted average in a neighborhood, and C Max is the maximum entry of the matrix C which the weight that you're assigning to the points.",
            "And theme in is the minimum entry.",
            "This stability bounded were able to show for these algorithms.",
            "For consistency method we show that the stability parameters actually quite high and it's the same.",
            "It's quite high also for local learning regularization and the reason why this stability is quite high is becausw the minimum eigenvalue for each of these algorithms of the matrix Q is zero, and if it's in fact if you if you somehow ensure that the minimum eigenvalue is not zero.",
            "Then you can get a nontrivial stability bound, and we do present this in the paper.",
            "More details in the paper, but I will not get into it in this in this talk."
        ],
        [
            "Finally, let me present some quick experiments.",
            "The generalization bound that we showed can help in model selection, so recall that for the local transductive regression algorithms we had the following generalization bound where the test error was bounded by the training error plus some complexity terms that dependent explicitly on the number of labeled and unlabeled points, and also on the stability parameter beta and recall that we also showed that the stability parameter beta for local transductive regression algorithms depends on.",
            "Number of labeled points and unlabeled points, but also depends critically on beta lock, which is the stability of the local estimator and the way we were generating local estimates was we were considering a weighted average in a neighborhood and the radius of that neighborhood.",
            "Of the algorithm an what are bound can help us do is pick the right features which will minimize the error.",
            "So we verify experimentally that the optimal radius R. It minimizes the right hand side of the generalization bound, which is the training error plus the complexity terms which we can all we can compute.",
            "Actually indeed also minimizes the test error.",
            "This avoids the need for a held out cross validation we conduct."
        ],
        [
            "Experiments, one of which, so we conduct experiments in three publicly available datasets.",
            "the Boston housing data set the elevators data set, and the aisle irans data set an in brackets.",
            "You have the number of points which is M + U, and this is 1 representative plot on the X axis you have the radius R, which is a parameter of the LTR algorithm that you use for generating estimate labels an on the Y axis.",
            "We black dotted line is the RHS, which is the training error plus the slack term.",
            "Or the complexity term and the red line is the true error and the flat line is induction, so we do better than induction.",
            "But what's more important is that the radius that minimizes the training error in the slack term also in fact minimizes the true error.",
            "Alright, so."
        ],
        [
            "I'll conclude we presented a comprehensive analysis of transductive regression algorithms, provided a generalization bound in which you can come with your favorite transductive regression algorithm, plug-in stability.",
            "Derive generalization bound and see if it's stable or not.",
            "The bound can be useful for model selection.",
            "We know that stability based bounds.",
            "Our algorithm specific and therefore can be often tighter than we see dimension based bounds or bounds based on other complexity measures and we also verified empirically good performance of local transductive regression algorithms.",
            "Thank you.",
            "Yes.",
            "Can you say something how tight the bound actually is?",
            "If you use a theoretical bound for model selection and there is a factor of 10 between well, should we believe that the minimum of the bound is also the minimum of the empirical behavior?",
            "We what we are more interested in is the shape of the bound rather than what its actual value is.",
            "And yes, I understand.",
            "But what makes you sure that the shape is the same, despite the fact that the absolute estimate of the error is much larger in the theoretical bound and in the empirical observation?",
            "Will we verify this experimentally?",
            "We were not sure, but we on the experiments that we run we observe this and This is why we say that experiments confirm this intuition.",
            "Of course, we mathematically we have not shown that exactly the radius that minimizes the training error plus the complexity term will be the radius that will minimize the test error.",
            "But we confirm this intuition by conducting a number of experiments.",
            "And you're right that the bound can be quite off.",
            "But what we're more interested in is that the shape of the bound.",
            "Should reveal something about the algorithm.",
            "So in one of the earlier slides, you had a assumption like the hypothesis should not differ from the true predictions by a certain quantity, and is that only over the training set or over the unlabeled data as well.",
            "It's over the unlabeled data as well.",
            "But then it would not apply to the empirical minimizer or regularised empirical minimizer, right?",
            "Um?",
            "No, so there are two assumptions.",
            "One assumption is that you assume that the target values themselves are bounded.",
            "And the other assumption is that the difference between the target value and the hypothesis score is bounded.",
            "OK, so you look at worst case, these are equivalent.",
            "So yeah, OK. Anymore questions.",
            "OK then let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon everybody.",
                    "label": 0
                },
                {
                    "sent": "Is a talk about using the notion of algorithmic stability to derive generalization bounds for transductive regression algorithms.",
                    "label": 1
                },
                {
                    "sent": "My name is Ashish Rastogi and this is joint work with Carina Cortez who's at Google.",
                    "label": 0
                },
                {
                    "sent": "My advisor Murray who's there and why you and Google and Dimitri pageant pageant.",
                    "label": 0
                },
                {
                    "sent": "Who's at technion?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so let me introduce the transductive setting.",
                    "label": 0
                },
                {
                    "sent": "We're all quite familiar with the setting of induction, in which the inductive learning algorithm receives a training set which consists of examples that are drawn in an IID fashion from a fixed but unknown distribution D and the objective of the learning algorithm is to produce a hypothesis that does well on an unknown point, also drawn according to this distribution.",
                    "label": 1
                },
                {
                    "sent": "So the expected squared error is the performance measure in the trans doctors in the transductive setting.",
                    "label": 0
                },
                {
                    "sent": "However, the entire universe of training plus test.",
                    "label": 0
                },
                {
                    "sent": "Elements is available to you before hand and the order of the.",
                    "label": 1
                },
                {
                    "sent": "Learning algorithm is to produce labels for the test set and so your performance measure is simply the average error on the test set.",
                    "label": 0
                },
                {
                    "sent": "No other points are ever going to be considered by the learning algorithm, so the transductive setting is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Has received a lot of attention recently and has a lot of good motivation, mainly in real world problems.",
                    "label": 1
                },
                {
                    "sent": "There's much more unlabeled data and labeling data is expensive, both in terms of time and money, so you can think of applications like you know, assigning scores to web pages you might have some kind of system that assigns to each web page it's quality, and you have a vast number of web pages, but for a small subset of them you've actually hired humans to assign some kind of a score, and you want to.",
                    "label": 0
                },
                {
                    "sent": "Then build a system that assigns scores to other pages.",
                    "label": 1
                },
                {
                    "sent": "So this would be an example of a transductive.",
                    "label": 0
                },
                {
                    "sent": "Problem also there are applications in computational biology and recently the Netflix Challenge has also received a lot of attention in the machine learning community.",
                    "label": 0
                },
                {
                    "sent": "As formulated, the Netflix Challenge is a transductive regression.",
                    "label": 0
                },
                {
                    "sent": "And this picture shows or gives an illustration of the Netflix challenge.",
                    "label": 0
                },
                {
                    "sent": "So on the rules you have users and on the columns you have movies and some of these users have seen some of these movies and assigned some ratings.",
                    "label": 0
                },
                {
                    "sent": "The first user has assigned a rating of five stars to the first movie, and then you have in mind some target user who has seen some movies which is the last two in this case.",
                    "label": 0
                },
                {
                    "sent": "And then for some other sets of movies you want to predict you know what their preference for these movies is going to be like and the key for the Netflix challenge is that the movies on which you want to make the predictions are available beforehand to the algorithm.",
                    "label": 1
                },
                {
                    "sent": "So this is exactly the transductive setting.",
                    "label": 0
                },
                {
                    "sent": "So in this paper in this paper we consider the transductive regression algorithms and.",
                    "label": 0
                },
                {
                    "sent": "Use the notion of algorithmic stability to prove generalization bounds, and the advantage of stability for professionalization is that it's algorithmics algorithm specific and often gives tighter generalization bounds then, once there are based on VC dimension's, another complexity measures.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I'll this is the outline of the talk I briefly mentioned some motivation for this work.",
                    "label": 0
                },
                {
                    "sent": "Next, I'll introduce the transductive regression problem, then introduce the notion of stability and then say what we can show about stable learning algorithms and what kind of generalization properties they have.",
                    "label": 0
                },
                {
                    "sent": "Then we'll look at various transductive regression algorithms that are actually used in practice and see what kind of stability bound they possess an.",
                    "label": 1
                },
                {
                    "sent": "Finally, we will present.",
                    "label": 0
                },
                {
                    "sent": "I'll present some experiments that demonstrate the.",
                    "label": 1
                },
                {
                    "sent": "Usefulness of the bound that we prove.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so here is the transductive regression problem.",
                    "label": 1
                },
                {
                    "sent": "You're given the entire universe of M + U elements M for the rest of this talk will denote the number of labeled examples and you will denote the number of unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "So in the transductive setting you're given the entire set X of N plus you elements X one through XN plus you, let's say, and the labeling information for a subset of M points is revealed to the algorithm.",
                    "label": 1
                },
                {
                    "sent": "So by one through YM will be the training set, and the objective is to find a hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Edge that assigns scores which are real numbers 'cause we're in the regression setting two elements in X such that the error on the unlabeled points which is given by this expression.",
                    "label": 1
                },
                {
                    "sent": "1 / U time times some of the errors on each unlabeled point is small and a little bit of standard notation.",
                    "label": 0
                },
                {
                    "sent": "Our head our head of age will denote the training error and it has the usual formulation.",
                    "label": 0
                },
                {
                    "sent": "An RH will denote the test error.",
                    "label": 0
                },
                {
                    "sent": "So this is these are the kinds of algorithms that we will want to analyze using the notion of stability so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let me introduce stability.",
                    "label": 0
                },
                {
                    "sent": "The ability is that a small change in the training set leads to a small change in the learned hypothesis.",
                    "label": 1
                },
                {
                    "sent": "Let's illustrate this with the diagram.",
                    "label": 0
                },
                {
                    "sent": "So let's say this big Oval is your set X and it's partitioned into labeled an unlabeled data in the following way on the left hand side you have the labeled points and the numbers on each.",
                    "label": 0
                },
                {
                    "sent": "You can think of these as movies, and the numbers are the ratings which from the picture that we saw a few slides ago and on the right hand side you have unlabeled points because you don't know what the.",
                    "label": 0
                },
                {
                    "sent": "Ratings are for these movies and let's say that this partition is partition one and call it.",
                    "label": 1
                },
                {
                    "sent": "It produces a hypothesis HS.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Swap one training and test point.",
                    "label": 0
                },
                {
                    "sent": "You get another partition which is very close to the previous partition, so.",
                    "label": 0
                },
                {
                    "sent": "And let's say that this produces a hypothesis edge of S prime.",
                    "label": 0
                },
                {
                    "sent": "Then the learning algorithm is set to be bitter stable.",
                    "label": 1
                },
                {
                    "sent": "If the error for all points X between two hypothesis hypothesis is not different by more than bitter in absolute value.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the squared.",
                    "label": 0
                },
                {
                    "sent": "So this is the square difference.",
                    "label": 0
                },
                {
                    "sent": "The error of point X on edges square.",
                    "label": 0
                },
                {
                    "sent": "And this is the error of point X on a chess primed and you're just saying that for all points X this difference is no more than better.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "Notion of algorithm stability.",
                    "label": 0
                },
                {
                    "sent": "An inductive setting.",
                    "label": 0
                },
                {
                    "sent": "Stability has been used to prove generalization bounds for regression and classification, and this was work done by Busquet analysis and we seek to do something similar for the transductive setting.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what can we show?",
                    "label": 0
                },
                {
                    "sent": "We show the following.",
                    "label": 0
                },
                {
                    "sent": "So for any transductive regression algorithm that has a stability parameter, beta and such that the hypothesis scores are bounded away from the true scores by at most some capital B, then with high probability the test error is bounded by the training error plus some term that depends upon both the stability and explicitly on the number of labeled points M and the number of unlabeled points U in the following manner.",
                    "label": 0
                },
                {
                    "sent": "And the thing to an.",
                    "label": 0
                },
                {
                    "sent": "There's Alpha of em, Use that for this talk, you should just think of it as a mu divided by N + U.",
                    "label": 0
                },
                {
                    "sent": "Some arithmetic is there, but for large MU doesn't matter so.",
                    "label": 0
                },
                {
                    "sent": "The thing to take away from this is to see that what happens as MN you increase and indeed as M and you increase if stability of the algorithm decreases then you do get the fact that a test error sort of is bounded by the training error, which is something that we like to see.",
                    "label": 0
                },
                {
                    "sent": "So the way to show this bound is.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "While looking at the random variable, which is the difference of the test and the training error and what we want to do is, we want to use a concentration of measure bound on this random variable.",
                    "label": 1
                },
                {
                    "sent": "And indeed in the inductive setting this is what people do and boost caneles if actually used the mcdermotts inequality and Mcdermott's inequality says that for a function of some independent random variables whose dependence on each individual random variable is small, which is captured captured by this condition, which says that.",
                    "label": 0
                },
                {
                    "sent": "The value of fire doesn't change by more than CI if you only change the value of the random variable in position I, then the probability that the random variables by deviates from its expected value by more than epsilon is exponentially bounded in these deviations CI squared and into epsilon squared, and we want to use a similar concentration bound.",
                    "label": 1
                },
                {
                    "sent": "However, in the setting of transduction, since random variables are not independent, but there without replacement we need.",
                    "label": 0
                },
                {
                    "sent": "A new concept.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bound, and indeed we did have a new concentration.",
                    "label": 1
                },
                {
                    "sent": "I should mention that this was independently discovered by Alien, even Pacione in 2007.",
                    "label": 0
                },
                {
                    "sent": "However, we do provide simpler proof of this concentration bound, so this is what the concentration bounds says.",
                    "label": 0
                },
                {
                    "sent": "If X1 through XM, and you should think of these as the labeled points out of your universe of M plus you elements.",
                    "label": 0
                },
                {
                    "sent": "So if X1 through XM is a sequence of M, random variables that are all drawn from some universe X of N plus you elements without replacement.",
                    "label": 1
                },
                {
                    "sent": "And you have a function Phi, whose dependence on any one of these random variables is bounded by at most C, just like in the mcdermotts inequality.",
                    "label": 0
                },
                {
                    "sent": "Then the probability that fire deviates from its expected value by more than epsilon is exponentially bounded.",
                    "label": 0
                },
                {
                    "sent": "In these deviations, an instead of M which you had in the McDonald's inequality, you have Alpha of MU, which is M times you divided by N plus you.",
                    "label": 0
                },
                {
                    "sent": "So you can think that this inequality is sharper by the sampling fraction.",
                    "label": 1
                },
                {
                    "sent": "And sanity check as you which is the number of unlabeled points, go to Infinity.",
                    "label": 0
                },
                {
                    "sent": "You recover Mcdermotts inequality so.",
                    "label": 0
                },
                {
                    "sent": "So the way we use this concentration bound to derive a generalization bound is, as I said, you consider the random variable, which is a difference of the test in the training error that becomes your fi.",
                    "label": 0
                },
                {
                    "sent": "You provide abound on expected value of five and you also provide a bound on five of the the C, which is how much does 5 change when you just change one point, and that's where the notion of stability comes in.",
                    "label": 0
                },
                {
                    "sent": "And if you plug everything in you.",
                    "label": 0
                },
                {
                    "sent": "You get, you get the generalization.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Andre mentioned in the previous slide.",
                    "label": 1
                },
                {
                    "sent": "OK, now we're going to move and look at some algorithms and try and apply this bound to those algorithms.",
                    "label": 0
                },
                {
                    "sent": "So the first algorithm that we consider is the algorithm of local transductive regression to Curtis Anne Murray from a paper in 2006.",
                    "label": 1
                },
                {
                    "sent": "This is a quick description of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "In this algorithm we map points to a high dimensional feature space, let's say of dimension Capital N An.",
                    "label": 1
                },
                {
                    "sent": "We consider linear hypothesis in this in this space, so it of X is.",
                    "label": 1
                },
                {
                    "sent": "Some weight vector www.product with five, 5X and the way we leverage unlabeled points is we use local estimates for unlabeled points.",
                    "label": 0
                },
                {
                    "sent": "So for each unlabeled point we look at a small neighborhood around the unlabeled point neighborhood of radius R, and in fact, this parameter R will be will come up again later in experiments, and then we use come some kind of weighted average of the labeled points in this neighborhood, and that average is what we call the estimate label for this unlabeled point.",
                    "label": 0
                },
                {
                    "sent": "Can we introduce?",
                    "label": 0
                },
                {
                    "sent": "This quantity, for the unlabeled points.",
                    "label": 1
                },
                {
                    "sent": "So if you look at the first 2 terms, there's the regularization term and there's a training error.",
                    "label": 0
                },
                {
                    "sent": "They look very familiar from the inductive setting, and now for the transductive setting.",
                    "label": 0
                },
                {
                    "sent": "We introduce, for the unlabeled points, the error with respect to estimate labels, and we traded off with a constant C primed so we minimize this objective function.",
                    "label": 0
                },
                {
                    "sent": "This objective function has many nice properties.",
                    "label": 0
                },
                {
                    "sent": "It has a closed form solution which involves matrix inversion and it is Colonel Isable.",
                    "label": 1
                },
                {
                    "sent": "Of the minimization is capital N cubed, where capital N is the number of features.",
                    "label": 0
                },
                {
                    "sent": "If you do it in the primal plus N ^2 * N + U, or if you do it in the dual, it requires inversion of the gram matrix, so it's order of M plus you cubed, where M is the number of labeled points and use the number of points.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is we want to figure out what is the stability coefficient of this transductive regression algorithm, we.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use convexity based analysis and this is what we are able to show.",
                    "label": 0
                },
                {
                    "sent": "So suppose the following hold.",
                    "label": 1
                },
                {
                    "sent": "Suppose for all points X the target values are bounded by some capital M an.",
                    "label": 1
                },
                {
                    "sent": "Suppose that the kernel is bounded, which is to say that for all points XK of XX is less than Kappa squared and the algorithm that you used to generate estimate labels, which was some kind of a weighted average.",
                    "label": 0
                },
                {
                    "sent": "Or you can choose whatever algorithm that you want has its own stability parameter, which is we call local stability, so we'll call it better log.",
                    "label": 1
                },
                {
                    "sent": "Then local transductive regression has the following stability.",
                    "label": 0
                },
                {
                    "sent": "It's this complicated expression and what we're interested in is the dependence on small M. The dependence in small U and the dependence on better lock.",
                    "label": 0
                },
                {
                    "sent": "And So what we notice is that the small, even small you are always in the denominator, which is good because as these things go to Infinity, this algorithm gets more and more stable and we notice that better lock is in the numerator.",
                    "label": 1
                },
                {
                    "sent": "So it also says that as the local estimator gets more and more stable.",
                    "label": 0
                },
                {
                    "sent": "The algorithm gets more and more stable.",
                    "label": 0
                },
                {
                    "sent": "So this is this is the stability that we're able to show for local.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adaptive regression algorithms.",
                    "label": 0
                },
                {
                    "sent": "So we can put everything that we showed so far together and so we have this generalization bounds for local transductive regression algorithms, it's just.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Previous slides.",
                    "label": 0
                },
                {
                    "sent": "OK, so that analysis was carried out based on the convexity of the objective function.",
                    "label": 0
                },
                {
                    "sent": "There is another an analysis that you can carry out based on closed form solutions and many transductive regression algorithms as we saw in the previous presentation, can be formulated as some combination of a fit term and a smoothness term.",
                    "label": 1
                },
                {
                    "sent": "So the first term here is a smoothness term which is your vector vector, H is a vector of your predictions and Q is some smoothness matrix.",
                    "label": 1
                },
                {
                    "sent": "And why is your true answers so for the.",
                    "label": 0
                },
                {
                    "sent": "Labeled points, it's your true answers and unlabeled points.",
                    "label": 0
                },
                {
                    "sent": "It could either be your estimates, or it could be.",
                    "label": 0
                },
                {
                    "sent": "It's in fact, zero.",
                    "label": 1
                },
                {
                    "sent": "A lot of the times, and C is some matrix that assigns weights to these points.",
                    "label": 0
                },
                {
                    "sent": "It's often diagonal in paper in practice.",
                    "label": 0
                },
                {
                    "sent": "So, and this formulation includes various algorithms that are well studied in the transductive setting.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can show the following bound.",
                    "label": 0
                },
                {
                    "sent": "So let Lambda small M of Q denotes the minimum eigenvalue of a matrix, Q an Lambda capital M of Q denote the maximum eigenvalue of matrix cube an.",
                    "label": 0
                },
                {
                    "sent": "Let H1 and H2B.",
                    "label": 0
                },
                {
                    "sent": "The solutions for this formulation.",
                    "label": 0
                },
                {
                    "sent": "When training and test points are swapped in one position, which is exactly the difference that you need to bound for stability, then what we can show is that for all points XI.",
                    "label": 1
                },
                {
                    "sent": "The difference in scores assigned by H1 and H2, which are the two hypothesis that are produced when you swap the training and test points and exactly in exactly 1 position, is bounded by this expression, and the expression depends upon the maximum eigenvalue of Q.",
                    "label": 0
                },
                {
                    "sent": "The minimum eigenvalue of Q and the maximum eigenvalue of matrix C. And what we can do is we can look at various algorithms and plug in the various Q matrices and the various matrices that those specific algorithms use.",
                    "label": 0
                },
                {
                    "sent": "An see what kind of stability we get and see if there if we can prove stability using this technique, which is that.",
                    "label": 0
                },
                {
                    "sent": "Do the algorithms get more and more stable as you see more and more labeled points for unlimited?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the algorithms we study various algorithms in the paper we study the consistency method, the LL regularization, which is local learning regularization and the Gaussian mean fields.",
                    "label": 1
                },
                {
                    "sent": "And in all of these algorithms the estimate labels are zero.",
                    "label": 1
                },
                {
                    "sent": "Recall that in the local transductive regression the estimate labels were some kind of a weighted average in a neighborhood, and C Max is the maximum entry of the matrix C which the weight that you're assigning to the points.",
                    "label": 0
                },
                {
                    "sent": "And theme in is the minimum entry.",
                    "label": 0
                },
                {
                    "sent": "This stability bounded were able to show for these algorithms.",
                    "label": 0
                },
                {
                    "sent": "For consistency method we show that the stability parameters actually quite high and it's the same.",
                    "label": 0
                },
                {
                    "sent": "It's quite high also for local learning regularization and the reason why this stability is quite high is becausw the minimum eigenvalue for each of these algorithms of the matrix Q is zero, and if it's in fact if you if you somehow ensure that the minimum eigenvalue is not zero.",
                    "label": 0
                },
                {
                    "sent": "Then you can get a nontrivial stability bound, and we do present this in the paper.",
                    "label": 0
                },
                {
                    "sent": "More details in the paper, but I will not get into it in this in this talk.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, let me present some quick experiments.",
                    "label": 0
                },
                {
                    "sent": "The generalization bound that we showed can help in model selection, so recall that for the local transductive regression algorithms we had the following generalization bound where the test error was bounded by the training error plus some complexity terms that dependent explicitly on the number of labeled and unlabeled points, and also on the stability parameter beta and recall that we also showed that the stability parameter beta for local transductive regression algorithms depends on.",
                    "label": 0
                },
                {
                    "sent": "Number of labeled points and unlabeled points, but also depends critically on beta lock, which is the stability of the local estimator and the way we were generating local estimates was we were considering a weighted average in a neighborhood and the radius of that neighborhood.",
                    "label": 0
                },
                {
                    "sent": "Of the algorithm an what are bound can help us do is pick the right features which will minimize the error.",
                    "label": 1
                },
                {
                    "sent": "So we verify experimentally that the optimal radius R. It minimizes the right hand side of the generalization bound, which is the training error plus the complexity terms which we can all we can compute.",
                    "label": 1
                },
                {
                    "sent": "Actually indeed also minimizes the test error.",
                    "label": 1
                },
                {
                    "sent": "This avoids the need for a held out cross validation we conduct.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiments, one of which, so we conduct experiments in three publicly available datasets.",
                    "label": 1
                },
                {
                    "sent": "the Boston housing data set the elevators data set, and the aisle irans data set an in brackets.",
                    "label": 0
                },
                {
                    "sent": "You have the number of points which is M + U, and this is 1 representative plot on the X axis you have the radius R, which is a parameter of the LTR algorithm that you use for generating estimate labels an on the Y axis.",
                    "label": 1
                },
                {
                    "sent": "We black dotted line is the RHS, which is the training error plus the slack term.",
                    "label": 0
                },
                {
                    "sent": "Or the complexity term and the red line is the true error and the flat line is induction, so we do better than induction.",
                    "label": 0
                },
                {
                    "sent": "But what's more important is that the radius that minimizes the training error in the slack term also in fact minimizes the true error.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll conclude we presented a comprehensive analysis of transductive regression algorithms, provided a generalization bound in which you can come with your favorite transductive regression algorithm, plug-in stability.",
                    "label": 1
                },
                {
                    "sent": "Derive generalization bound and see if it's stable or not.",
                    "label": 1
                },
                {
                    "sent": "The bound can be useful for model selection.",
                    "label": 1
                },
                {
                    "sent": "We know that stability based bounds.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm specific and therefore can be often tighter than we see dimension based bounds or bounds based on other complexity measures and we also verified empirically good performance of local transductive regression algorithms.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Can you say something how tight the bound actually is?",
                    "label": 0
                },
                {
                    "sent": "If you use a theoretical bound for model selection and there is a factor of 10 between well, should we believe that the minimum of the bound is also the minimum of the empirical behavior?",
                    "label": 0
                },
                {
                    "sent": "We what we are more interested in is the shape of the bound rather than what its actual value is.",
                    "label": 0
                },
                {
                    "sent": "And yes, I understand.",
                    "label": 0
                },
                {
                    "sent": "But what makes you sure that the shape is the same, despite the fact that the absolute estimate of the error is much larger in the theoretical bound and in the empirical observation?",
                    "label": 0
                },
                {
                    "sent": "Will we verify this experimentally?",
                    "label": 0
                },
                {
                    "sent": "We were not sure, but we on the experiments that we run we observe this and This is why we say that experiments confirm this intuition.",
                    "label": 0
                },
                {
                    "sent": "Of course, we mathematically we have not shown that exactly the radius that minimizes the training error plus the complexity term will be the radius that will minimize the test error.",
                    "label": 0
                },
                {
                    "sent": "But we confirm this intuition by conducting a number of experiments.",
                    "label": 0
                },
                {
                    "sent": "And you're right that the bound can be quite off.",
                    "label": 0
                },
                {
                    "sent": "But what we're more interested in is that the shape of the bound.",
                    "label": 0
                },
                {
                    "sent": "Should reveal something about the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So in one of the earlier slides, you had a assumption like the hypothesis should not differ from the true predictions by a certain quantity, and is that only over the training set or over the unlabeled data as well.",
                    "label": 0
                },
                {
                    "sent": "It's over the unlabeled data as well.",
                    "label": 0
                },
                {
                    "sent": "But then it would not apply to the empirical minimizer or regularised empirical minimizer, right?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "No, so there are two assumptions.",
                    "label": 0
                },
                {
                    "sent": "One assumption is that you assume that the target values themselves are bounded.",
                    "label": 0
                },
                {
                    "sent": "And the other assumption is that the difference between the target value and the hypothesis score is bounded.",
                    "label": 0
                },
                {
                    "sent": "OK, so you look at worst case, these are equivalent.",
                    "label": 0
                },
                {
                    "sent": "So yeah, OK. Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "OK then let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}