{
    "id": "5mhe2venmmj6vaeygqonfb36cjef4i6x",
    "title": "Memory-Based models of inflectional morphology acquisition and processing",
    "info": {
        "author": [
            "Walter Daelemans, Center for Dutch Language and Speech, University of Antwerp"
        ],
        "published": "Oct. 31, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Human Language Technology"
        ]
    },
    "url": "http://videolectures.net/mlcs07_daelemans_mbm/",
    "segmentation": [
        [
            "OK, what I would like to do is to give you a few examples of older work and more recent work on modeling with memory based learning the inflectional morphology problem, which has been.",
            "Well.",
            "Pink coins the term the fruit fly of psycholinguistics.",
            "I don't know if that is true anymore because of course it's a long time ago, but it's still an interesting problem because it's sort of a model for other types of language acquisition problems and language processing problems.",
            "So I will explain a bit about the model we use.",
            "Its implementation, which is symbol for the tillberg memory based learner.",
            "Actually, learning learning is very easy in memory based approaches because you simply store information.",
            "So that's why in the title it's both acquisition and processing and not only acquisition, and I will go into a few cases, the English and the Dutch past ends, but more Interestingly from a theoretical point of view, the German plural and the Dutch plural and I will end with a number of observations about.",
            "Potential problems in the methodology of applying computational modeling, especially using machine learning.",
            "In psycholinguistic."
        ],
        [
            "Yes.",
            "OK, so the very familiar, probably with the with the problem or with the.",
            "That got me in theory formation, so you have to solve 2 problems if you want to say something interesting about human language parsing architecture, how is the linguistic knowledge that?",
            "An agent needs, how is it represented and how is it acquired in the words and rules approach, dual mechanism approach or the lexicon plus grammar approach of Pinker and others.",
            "It's a fairly easy modular system with a set of mental rules and associative memory, but there are a number of problems with that.",
            "There is fuzziness and leakage between rules and between linguistic categories or concepts.",
            "There are issues of semi regularity and irregularity that you have to handle.",
            "And what we propose as an alternative is similarity based reasoning or analogy to solve the problem.",
            "As for the question of how linguistic knowledge is acquired, the word same rules approach works from the assumption that you have innate rules or constraints or whatever in its structure.",
            "Abstractions over the data which you have already available at birth, or which you extract from the primary linguistic data.",
            "Whereas memory based approach would say that you only have to store patterns of observable linguistic items, and that's a bit of a methodological problem also.",
            "But the type of linguistic information that we use is already rather abstract, so it's worth syllables.",
            "Segmental structure of syllables.",
            "You could say that it's much too abstract, because of course the speech signal is much more concrete and much more.",
            "Specific than that, but we have seen a number of stock talks and there's a lot today and there's a lot of work showing that at least the assumptions we make about syllable structure being available or learnable using unsupervised learning techniques is not really a very far fetched assumption."
        ],
        [
            "So why do we like all of us?",
            "I think machine learning so much as a model for for cognitive science.",
            "I think it's becausw you can operationalize a lot of theoretical discussions very easily by refering to the different types of bias that the machine learning algorithm can have in solving an acquisition and processing task.",
            "This.",
            "Picture is supposed to be vaguely reminiscent of the chomskian view of having a performance component and then which does input output mappings and then you have a competence with representations, linguistic concepts, linguistic rules, and then there's a learning component which tries to zoom in using some search algorithm into the right representations or the right knowledge which is necessary for solving the performance task, and you use primary linguistic data or experience to do that.",
            "What machine learning gives us is some more insight into the possible types of bias that indexed inductive algorithms can have, both in their types of representations they take as input for learning in the types of representations that they use for representing the required knowledge, and most importantly the search method they used to arrive at right representations for the task."
        ],
        [
            "So I will limit.",
            "I will limit this talk to to supervise learning.",
            "This is perhaps something I should have put in a few slides about that because I think there's a lot of confusion about super supervised and unsupervised learning and its relevance for language learning thing.",
            "There's natural supervision and unnatural supervision if you assume that a child uses a tree bank with completely parse trees as information that is not really.",
            "The right type of supervision.",
            "If you presuppose that, for example, in learning prosodic system or stress system words, the system the chart uses information from the input it gets.",
            "It's also supervised, but it's much more natural, and it's completely normal to use words with stress patterns as input and therefore to frame the problem as a classification problem that you can solve with supervised learning, so I think it's not as simple as saying that unsupervised learning is more psychologically relevant.",
            "Then supervised learning depends on the task you're trying to solve in the way you actually actually represent the problem as a learning task.",
            "So this is a very brief overview of the state of the art.",
            "I think.",
            "As far as I know it in supervised learning at this time in machine learning of natural language, if you're learning to solve classification problem is supervised learning.",
            "You try to approximate some underlying function that you assume to be there, and there are basically two ways to do that.",
            "Either you immediately model this.",
            "Put mapping using a discriminative approach or you model the underlying distributions which are important for the domain and by modeling the domain you get a solution of the task as well, so are the generative methods not to be confused with generative in the linguistic terminology.",
            "So what the discriminative learner does, and I think most of the symbolic machine learning algorithms and also a lot of statistical learning algorithms, make the assumption that you don't stream estimate the underlying representations with directly try to estimate the input output mappings or whatever is relevant there.",
            "So that's, let's say the discriminative POV and generative models like conditional random fields in the Markov models and all the variants that you have in graphical learning models, methods you try to model the whole domain and then you derive from that.",
            "Your solution for the task there are problems with both approaches.",
            "So in a discriminative approach you don't estimate the underlying knowledge which may be important in a generative approach.",
            "You make a lot of very harsh assumptions about conditional independence.",
            "Most of the time.",
            "If you're trying to use it for language learning, especially in sequence learning."
        ],
        [
            "Tasks.",
            "OK, so within the discriminative methods, think eager learning methods dominate the field, because this idea which also came up this morning of the minimal description length principle.",
            "So actually what you try to do in eager learning approach is to find the model that fits the training data, potentially also store exceptional information.",
            "If you can't find the model that fits all the training data and then the size of these two.",
            "Combined is you Ristic that you use to juristic information that you're using finding a good model for your problem for your task?",
            "So what you try to zoom into is the core and you forget about the data that you used to learn the core, and then you only store the periphery as as much as as needed to solve the task.",
            "Now lazy learning memory based learning is a completely different as a completely different point of view and starts from the.",
            "At least in my case, from the linguistic problem that you can't really say in advance what will be regular cases and would be except will be exceptional cases in what will be noise, right?",
            "If you are a learning system or the input doesn't come labeled with, I'm a regular pattern and I'm an irregular pattern, or I'm in an exception, or I'm noise, that's exactly what you're trying to find out.",
            "So having an architecture that sort of.",
            "Tries to.",
            "A priority make a distinction between regularity's and exceptions may have the wrong type of bias, and as most eager learning algorithms cannot really distinguish between noise and exceptions, and exceptions are for a number of reasons very productive in language processing, it's a bad choice to forget your data and you should at least keep all of the training information that you have, perhaps on top of that also extract.",
            "Generalizations or obstructions, but the data may be important.",
            "Why is it important?",
            "Well, also for reasons that you are probably have hurt hurt about for a number of times for a number of large time problem in learning of small districts, pockets of exceptions, and polymorphism.",
            "It's not very easy language to find very nice regions most of the time for any problem that you would try to solve where an eager learning algorithm would give you.",
            "Small, compact model that actually is productive enough.",
            "Also, because of different distributions which do not only exist in vocabulary but also in, for example, the grammar rules that you find in corpora or in anything that we want to measure.",
            "In language you get this ception distributions where there's a small number of things that occurs.",
            "A lot of the time, but the productive and regular actually set of things that are very uncommon.",
            "Very frequent, but are nevertheless needed to make a good model.",
            "At least that's the."
        ],
        [
            "The assumption?",
            "Now, one way of visualizing that is to and this is work we did.",
            "Looking at a number of different natural language processing tasks, data for plural formation, diminutive formation, morphology analysis, morphological analysis, prepositional phrase attachment, chunking, named entity recognition.",
            "So what you see here in the graph is the cumulative percentage of training items which have a certain number of friendly nearest neighbors.",
            "So what is a friendly nearest neighbor that's?",
            "Close very similar item in your instance space that has the same category as you have now.",
            "A hostile nearest neighbor is then a very close, very similar nearest neighbor which has a different category.",
            "And what you see from this graph, for example, is that for all these tasks 10 to 20% of the cases doesn't have any friendly neighbors, so they are really isolated exceptions that would very easily be thrown away as noise in an eager learning algorithm, but nevertheless can reoccur if you have, for example, another sample of your population or a larger sample of your population.",
            "And if you look at problems like morphological analysis for example, then you see that.",
            "More than 50%.",
            "Of the data has less than 10 friendly nearest neighbors, so you really have these pockets and this is disjunctive and polymorphous."
        ],
        [
            "Classes here.",
            "So moving to inflectional morphology.",
            "So the word same rules approach to this problem is to posit a mental rule for the default cases for the regular cases and the memory to store the exceptional cases, and that would be a perfectly plausible model and also a good explanatory value, except that you have this type of.",
            "Album with nonsense words or with pseudo words or non words.",
            "Where sometimes these words get default inflection, but sometimes the words actually are drawn to similar irregular items and get the irregular inflection of their nearest neighbors.",
            "OK, so for example, if you have a pseudo word spelling.",
            "And you have to rate how good splint or sling or Splunk is as a past tense of spring, you will find all the answers in a different distribution, but participants will will also say that OK's plan is an OK past tense for this pseudo word.",
            "So one way to solve that is to posit an associative memory.",
            "How to talk to make your your lexicon or your exceptional data, not just a list of exceptions, but make it productive by using a neural network or something associative."
        ],
        [
            "But that's of course gives you the problem of how to decide when to follow the rule.",
            "I want to follow memory and there have been very many proposals for that, but I've never actually seen a good implementation of any of them for fracing models, and you have getting models.",
            "And if pre gating and post skating.",
            "But all these are really.",
            "A bit fishy, I think because you can really set boundaries in such a way in such a model that either the model is completely rule based or completely associative memory, and depending on the actual data that you are looking at, you can tune.",
            "You can tweak the parameters too so."
        ],
        [
            "The problem.",
            "So I will not go into a lot of detail for the past tense just to say that among work earlier, so it's working out at Psycho Linguistics Department in Antwerp and we are cooperating with a lot's done.",
            "Some nice simulations on the past tense, which are forthcoming.",
            "Where he?",
            "Uses this type of information only.",
            "Like for example, the features only contain the phonological information of the last two syllables split into the different parts of the syllable onset nucleus coda.",
            "And there's some right alignment, and then for the classes.",
            "These were automatically derived from the pairs of roots and past tense forms by applying a Levenshtein distance metric.",
            "So you can.",
            "It's not just saying regular or irregular, you actually learn the patterns with vowel changes and with the different types of analogical variants of the suffix suffixation.",
            "So the task he's trying one of the tasks he tried to simulate is the rate."
        ],
        [
            "The task of pesada in Pinker, so where people had to rate on a scale of 1 to 7 how well they liked the different past tenses."
        ],
        [
            "Often on words or pseudowords, and to get that information out of the memory based learning model.",
            "We used the class distribution of the different nearest neighbor."
        ],
        [
            "Yes, and there's also some work on the simulation of results by or both or Brighton, in case some more recent.",
            "Views on the on the past tense learning with similar matches, and I'm just going to show you the matches with without a lot of information, so the blue and green, blue and the purple are the.",
            "The ratings of the subjects are the participants rather and then the green and the orange are the.",
            "The ratings extracted from the memory based from the memory based simulation.",
            "Now what's most interesting I think here is not just the perfect match, it's very high correlation.",
            "But also the fact that you don't see the horizontal line in the blue dots which you would expect if there were a default rule at work.",
            "Because then the fact if no work is irregular or regular or very close to a regular or very close to an irregular should not have an effect on the way people.",
            "Assign goodness to regular past tense so prasada in Pinker.",
            "Following Prasad in pink are you would expect a horizontal blue line there."
        ],
        [
            "OK, and then the study was replicated for Dutch with exactly the same results, because in Dutch we have a very similar system."
        ],
        [
            "Also, as in the English past tense.",
            "So the algorithm is really very simple.",
            "For learning, you just store your instances in memory given some type of representation, which in this case for inflectional morphology is a number of features giving different parts of the syllable structure.",
            "For classification, if you have a new test item item, either an existing one or a new one, you compare to the memory instances you take the nearest neighbors."
        ],
        [
            "And you extrapolate from them.",
            "Of course, but you really have to if you want to use memory based learning for simulations, but you have to implement or what really is important is a similarity metric, and these are the four assumptions that we make feature relevance.",
            "We use an information theoretic measurement gain ratio which is used a lot in machine learning.",
            "For value similarity, we use the modified value distance metric.",
            "Or Stanfield in volts?",
            "I will use example relevance but not alot I will.",
            "Give you briefly more information about that in a minute.",
            "And then of course, the number of nearest neighbors and the distance to the nearest neighbors is very important, because that gives you an idea about the density and homogeneity of the local neighborhood.",
            "So, but I forgot to mention is that memory based learning is a nonparametric approach, and probably I think the only one in let's say the discriminative methods.",
            "So it doesn't make any assumptions on the distribution of the input data, it only looks at the local neighborhoods and extrapolates from local neighborhoods and therefore.",
            "Doesn't use any bias towards the type of distributions.",
            "For example, normal distributions or skewed distributions that you would have your."
        ],
        [
            "Training data.",
            "OK so um.",
            "Just very briefly about it, this modified value distance metric developed by Stanford in volts and refined by constant Salzburg.",
            "Its way of clustering or doing unsupervised learning within the context of supervised learning task.",
            "So what you do is you look for each value of each feature, what its distribution is of the different output classes that you're trying to predict, and those values that have a similar distribution will be more similar to each other than values that have more dissimilar distribution.",
            "It's nothing more than that.",
            "It's kind of clustering within the supervised."
        ],
        [
            "Learning approach.",
            "And then there's another important parameter.",
            "Is the distance weighted class voting?",
            "So.",
            "If you look at it from a natural language processing point of view where you try to make systems that handle sparse data.",
            "Increasing the value of K is the same as using a smoothing approach in probabilistic methods because you widen that, let's say the boundary of cases that you take into account, the local, you make the local context text bigger and so you get more information that you can.",
            "That you can use to make your decision, just like you can do smoothing in probabilistic models.",
            "OK. And of course, here one parameter is that how you take into account this local neighborhood the nearest neighbors in your local neighborhood, do you?",
            "Take a Democratic rule.",
            "Just saying every nearest neighbor that is in the scope of my K as an equal vote in the final decision.",
            "You can be a bit more specific and a bit more adapted to the data by using for example inverse distance or exponential decay."
        ],
        [
            "There.",
            "I don't have any examples here today about using example waiting, so I will not go into that in detail.",
            "Just saying that you can use in the table environment.",
            "You can use example waiting for example to model frequency effects."
        ],
        [
            "Or typical T effects or class prediction strength.",
            "This is a bout timbale which you can download from Tilburg site."
        ],
        [
            "The next version will be open source, but you can already use it for research.",
            "And education.",
            "So back to inflectional morphology.",
            "One illustration of how the memory based approach works in inflectional morphology is a German plural, which is actually much more interesting than the English past tense because it's sort of a reversal of the case in the English part, since we have the, say, the big frequent class is the regular and you only have a few exceptions.",
            "In German, if you follow the word same rules approach, it's actually the opposite.",
            "Well, John Lewis is learned very quickly by children, but not by foreign language or second language learners.",
            "So it's really a complex system with phonological and lexical information needed to solve it.",
            "Um?",
            "And so, in the discussion of the words and rules versus single mechanism approaches, the claim is that as the suffix, which is actually a very infrequent suffix, is the one that you use in a number of cases where in the English past tense you use the Ed suffix, right?",
            "Therefore, the suffix should be the default.",
            "It should be the rule and all the other inflections or the other suffixes cannot be anything else but irregular cases and should be stored in associative memory and can only be solved by associative memory.",
            "So."
        ],
        [
            "This is a bit of a strange way of doing it, because as you can see here, the suffix is really a minority class."
        ],
        [
            "In the German plural.",
            "Still, the bickering and I'll just make a very good point that in all these cases where you are not really, you don't really have a model to extrapolate from, or you don't really have something that is clearly visible or detectable as a noun or a verb.",
            "Now in this case, in those cases people actually use the S suffix.",
            "So the surnames, the borrowings, the acronyms, the lexicalized."
        ],
        [
            "ISIS etc.",
            "So this is the representation which we used.",
            "We use symbolic features, two syllables, again the same, segmentation in onset.",
            "Nucleus and coda of the syllables, and then the gender feature, which is also important and.",
            "Of course we use sielox's."
        ],
        [
            "The data set.",
            "And then if you look at the for example, the feature relevance computation using gain ratio or information gain or chi Square or alternatives, you see that it's indeed the last syllable, and especially the code of the last syllable, which is very informative and also."
        ],
        [
            "The gender of feature.",
            "Just to illustrate how you can get at the unsupervised Lee learned information from the Supervised learning task.",
            "This is a clustering of the VDM matrixes that you were extracted during learning.",
            "So remember that these MDM metrics have information about for each value of a feature about what their distribution is over the different out possible output classes.",
            "So in this case the five suffixes with a number of.",
            "About variance and if you cluster this information then you see for example that in German neutral masculine or much closer behave in a similar way, whereas feminine behaves in a different way in the model that has been extracted from these matrices, whereas you can also find, for example, number of interesting clusters of phonemes.",
            "For example all the nasalized vowels are together in what mapped in one class.",
            "All the shortfalls long vowels occur together, so you get kind of a linguistic theory about the logical classes that are relevant in solving the German plural formation task, almost for free.",
            "By using this modified."
        ],
        [
            "Value this metric.",
            "Of course, psychologically speaking, what is interesting is that this information is used, and so it's like there's some heuristic value that people may use that information as well.",
            "So if you look at the acquisition data that is available for the German plural because of course.",
            "In the memory based, if you want a simulation, memory based or otherwise, you want to.",
            "Reproduce the empirical data that exists.",
            "So if you look at the acquisition data, you see that children mainly over apply the E in the suffix and at the S pools are learned rather late and you also see that for novel words.",
            "So non existing words pseudowords children inflict them with the OR in most of the time and that the more irregular plural forms are produced more than the default suffix.",
            "In charge."
        ],
        [
            "Slang language acquisition data.",
            "And you see exactly the same happening in if you see the learning curves for the German plural using the memory based approach.",
            "So the two top ones are learned almost immediately.",
            "If you only need like 50 or 60 cases to learn the Ian and.",
            "Say the conversion suffix so no suffix that's immediately learned, so that's a very clear phonological context where you can boots up on and learn it from, and then you see that the other suffixes E&ER.",
            "I learned a bit more slowly, but still reach a very high accuracy, and that's indeed the S suffix, which is lagging behind.",
            "Why is this the case?",
            "Because it's for large part, I think conventional, so you really have to know that something is a name to not inflected with, say, the final logically correct suffix, but use this default suffix.",
            "So if you would have more add more information in the similarity space for.",
            "This problem then, probably this could be learned as well, but of course."
        ],
        [
            "It's for us to prove.",
            "If you look at the errors, which model makes memory based learner makes, then you also see that EN&E are the most of the suffixes that are most overgeneralized by the model and showed in the previous graph that Ian Ian are required faster than the DS went late and imperfectly.",
            "Like with the children.",
            "It was also interesting.",
            "One of the arguments for the for Pinker to be an associates to be interested in the German plural is that the connection is network is supposed not to be able to learn this type of problem because the default here is not the majority.",
            "The majority class right?",
            "And so the implicit argumentation or explicit argumentation here is that single root models.",
            "Have performed so well in the past tense becausw in the past tense.",
            "For English the default suffix is the majority suffix, but if you look at the overgeneralizations, the errors which the memory based learning simulation makes you see that it's not just an input is output effect because you don't exactly find, let's say, the most frequent input suffix as the the most overgeneralized or the most over regular suffix."
        ],
        [
            "It's actually actually a different ranking.",
            "Now another piece of empirical acquisition data that was available for the German plural is a study by Barca and others on how children age 3.6 to 6.6.",
            "Name?",
            "Name the I think it's the repetition task here named the different objects that they are confronted with with different plural suffixes.",
            "And then you see that if you divide or if you make two conditions in your experiment, one riming condition and one non rhyming condition for routes that you present to the children, then you see that the errors that the children make.",
            "I think it's sort of partition task, so you get presented with the wrong or the right suffix and then you have two children have to repeat and sometimes they repeat correctly and sometimes it correctly.",
            "Then you see that the.",
            "Uh.",
            "Let's say in the non running condition.",
            "The the S suffix is used much more right?",
            "So the claim here by Barca and others is that children are aware that unusual sounding words require the default.",
            "And that her children are aware that names require the default, but that's the graph that I didn't that I didn't show you.",
            "But it's it's normal similar graph."
        ],
        [
            "Then for the for the names.",
            "If you look at the output of the memory based simulation.",
            "They're going to look.",
            "You sort your the output of the system after the leave one out.",
            "Experiment into the rammingen rhyming conditions.",
            "Then you see exactly the same happening.",
            "So in a non rhyming condition the suddenly the end over generalization disappears in favor of this over generalization.",
            "So again, this is just an aspect of how your lexicon, let's say that the training with the properties of the training material rather than some innate bias."
        ],
        [
            "So in conclusion, for the German plural we see three different classes of plurals appearing in and conversion ER in S, and, whereas the former for suffixes at least 4.",
            "The dual mechanism method would be stored in the associative memory because they are irregular, it's only the suffix which is the default rule.",
            "We would say that it's actually the regular cases and that it's switches learned late and using a lot of conventional information like and syntactic context information.",
            "Also very nice is that we didn't see any difference in the learnability of.",
            "Words with or without an umlaut, which is also has been attested in child language acquisition, literature, and overall generalization.",
            "Accuracy is very high.",
            "If you look at.",
            "Using clustering techniques at.",
            "What is going on in the neighborhoods?",
            "And if you look at the nearest neighbors of different cases then you see that's what you get is actually the type of schemata which have been proposed by cupcake for the German plural away.",
            "See for example words with the code here which have the lexical feature masculine always get East as a suffix.",
            "Words like arms near things like that.",
            "So you don't really have to represent that explicitly in memory schemes like that, because they come out automatically in the way the memory based learner works."
        ],
        [
            "OK, onto the the Dutch plural and I will skip.",
            "Just explain how the Dutch plural, what the problem there is for the dual roots or the dual mechanism approach.",
            "So what you see is that, again, like in the German plural, you have a very funny, logically motivated distinction between two possible suffixes N or S, which are in almost completely complementary distribution.",
            "So actually it's very easy to learn it if you abstract from a few loanwords."
        ],
        [
            "Things like that.",
            "But the Dutch plural is problematic for the dual mechanism model, because what would you claim to do if you have two default rules for all of the conditions where the default rule normally applies?",
            "Both yes and the applied depending on the type of word, phonological makeup of the word.",
            "So actually the solution that Pinker came up with for the Dutch plural."
        ],
        [
            "Is this model?",
            "So where you have, let's say, the final logical domain which decides in or S and then you have two rules with their associated associative memory for the exceptions which.",
            "I can't really make a lot of sense of because I I wonder what associative memory would look like.",
            "If you have really need these two 2 subsets of associative memory to handle the exceptions to the two different."
        ],
        [
            "So default rules.",
            "Right?",
            "So I will skip this.",
            "This is a recent work.",
            "Mainly by Immanuel Curless, showing how the one problem which you have in the Dutch plural, which is the inflection of unassimilated borrowings, can be solved in a memory based."
        ],
        [
            "The approach and what this tells you about dual roots alternatives and I will use my final.",
            "5 minutes or so to to present to you a methodological problem.",
            "So what you see often in the literature is that you get some data set and then you get an accuracy and then say we have used backdrop learning algorithm with these parameters and the problem is solved that way.",
            "But what can we learn from that?",
            "From a psycholinguistic POV?",
            "So what useful information do we get from just one simulation in space?",
            "Large space of possible simulations that could have completely different?",
            "Outcomes so.",
            "What we really need is robustness in the sense that different parameters settings should not give you wildly different results for us in solving some problem, except when there's some psycho linguistically relevant going out.",
            "So the parameters that you look at me by optimization in machine learning is doing a lot of optimization these days.",
            "You can always get at some.",
            "Very strange parameter setting where, for example, the first syllable of a word is important in doing the inflection.",
            "While we know that, for example, the last syllable is most important in most inflectional systems, but nevertheless it would be possible for us also to make a model doing parameter tweaking and a lot of machine learning optimization work that would actually show you that that having the first syllable is actually a good indicator of of.",
            "The problem you are trying to solve or good information source.",
            "So you should be careful about that type of outcomes or results your.",
            "Results should hold for a number of different parameters settings, and they should also hold for different tasks for different empirical data sets.",
            "So the type of settings that you use if you say K should have a high value inflection inflectional morphology.",
            "It's not enough to show that on the English path, but past tense or in the German plural.",
            "It should also be shown in other inflection."
        ],
        [
            "Tasks right?",
            "So one step in that direction is a recent work with it in.",
            "In this direction.",
            "So for the Dutch plural, we made 23,000 simulations on three different tasks, three different datasets.",
            "So one is what we call a lexical reconstruction task.",
            "It's actually across validation on the on the training data and so that sort of mimics the situation where you suddenly forget 110th of your lexicon and then reconstruct that lexicon which we would claim is not really a psycho linguistically very relevant task.",
            "And then there's more interesting empirical data to data sets of pseudo words and participant behavior on pseudowords.",
            "One said bye.",
            "Bye and.",
            "Of ATC, other words in from 2002 and one of one and 80.",
            "Three other words.",
            "OK.",
            "Enter the way we are going to to evaluate is to look at the predictions of the model of all these different models and compare it to the majority of the decisions.",
            "Of the different."
        ],
        [
            "Disciplines.",
            "OK, I will go quickly here because it's not really darling.",
            "Important if you're not really into the the Dutch plural, there are many.",
            "There are many philological or if I mention source parameters that we that we varied like the number of syllables that you give the model to learn from.",
            "Whether it has stress or no stress.",
            "If you give information about the final grapheme or not because there's a linguistic reason why this would be a natural way of doing it, different types of alignment set."
        ],
        [
            "No.",
            "You also vary the type of class representation that can be just discrete classes, but also the type of patterns extracted with Levenshtein distance that you do an actual transformation of the input in the output.",
            "Also, a bit like neural networks do different.",
            "The parameters specially of course the K because we think that's the highest chance of being psycho linguistically relevant.",
            "This is waiting method set."
        ],
        [
            "Yeah.",
            "And then you see, for example in this case.",
            "Um?",
            "That the best models achieve really an enormous accuracy for some of the tasks of 100% and normally of course that is what you publish them, right?",
            "I want you forget about is the let's say the badly performing.",
            "Models, I'm not saying that this is according to machine learning methodology or wrong way of doing it, but it's just not very informative.",
            "If you're interested in psycholinguistic modeling and the type of parameters that play a role in psycholinguistic modeling.",
            "Of course, if I would do NLP parsing, I would also use the."
        ],
        [
            "Best strategy?",
            "So if you could look at this these many different results and then look for example where this still comes from.",
            "So you see that on average the accuracy is pretty good, but you have very long tails of very bad models which don't perform very."
        ],
        [
            "Well, and then you can systematically look.",
            "For example, is it the number of syllables?",
            "And then you see that indeed for this problem, if you throw out all the models with only one syllable as information as input information, your models become the averages and the best and the worst results get a lot better than if you take not the best results, of course, but the average models become and the worst models become much better if you.",
            "Don't if you use at least two syllables as."
        ],
        [
            "Information.",
            "So apart from this one silver model said the results from the memory based learning simulation appear to be quite robust for many different parameters settings.",
            "There are excusing there are few differences, for example, stress information we could not really show to that that adds any information using more than two syllables does not really help accuracy except on one of the pseudo word."
        ],
        [
            "Cassettes?",
            "So you're coming back to this last graph in that you could also give as information to the model.",
            "In in Dutch we have the situation that you in the phonology some information.",
            "Lexical information gets hidden like hunt with the dog is pronounced haunt with T. But in in the plural you get Honda where the D which is on the line in the stem in the root appears again same for things like Abarca where you don't pronounce normally the end.",
            "So in the phonological representation you would have a route ending in a schwa and whereas the underlying information is really that there's a nasal consonant there at the end of the world, and actually putting that information in the model helps the accuracy a lot.",
            "Overall, the others averaged over all the other parameter settings so.",
            "It's it's useful information, and it's even also accessible, potentially by people who don't know how to read like children, because there is some work by buying an estrous showing that that actually in the pronunciation already in the signal in the speech signal you see marked differences between the T coming from being reduced from the D and the T as a part of the lexical lexical route."
        ],
        [
            "And then there's a lot of stuff that didn't have any."
        ],
        [
            "Yeah, effect or not, a lot of effects.",
            "This is the influence of K. The value of K. So you see here that using.",
            "Only the one nearest neighbor is is not really a very good idea.",
            "You really need this smoothing effect of having a larger local neighborhood to extrapolate from, and then it doesn't really matter a lot if you don't use any decay or if you use inverse distance decay.",
            "Um?",
            "Right overall, the models are very."
        ],
        [
            "Robust."
        ],
        [
            "So I just said this.",
            "OK, so this is again saying the same thing.",
            "If you have your K too low.",
            "Then you're not really going to match the behavior of the participants very well.",
            "Even if you do very well in the lexical reconstruction task, so that's another indication that in the experiments using real participants an pseudowords something else is going on.",
            "Then in the lexical reconstruction task which is used a lot to compare."
        ],
        [
            "Models.",
            "OK, so my conclusions are that Emble appears to be robust model for inflectional morphology learning.",
            "If you have sufficient information to present to represent the examples, so in the case of the German plural you would need other information in the similarity space then then just from a logical information.",
            "That case up to now, the most important parameters that we found, with potential psycholinguistic relevance and that it would be very good if other people would also report ranges rather than the best simulation, because that way you could compare also the effect of different parameters on different tasks and also not use just one data set which you made yourself most of the time you do an experiment with students and then you make a model simulating.",
            "The behavior of the students.",
            "It would be good if you use all the available data sets to test your model and perhaps a little bit in the spirit of the talk we heard first."
        ],
        [
            "Weird today.",
            "And I'll stop with a brief list of two brief list of other groups using timbale memory based learning in modeling modeling work.",
            "OK, I'll stop here."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, what I would like to do is to give you a few examples of older work and more recent work on modeling with memory based learning the inflectional morphology problem, which has been.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Pink coins the term the fruit fly of psycholinguistics.",
                    "label": 1
                },
                {
                    "sent": "I don't know if that is true anymore because of course it's a long time ago, but it's still an interesting problem because it's sort of a model for other types of language acquisition problems and language processing problems.",
                    "label": 0
                },
                {
                    "sent": "So I will explain a bit about the model we use.",
                    "label": 0
                },
                {
                    "sent": "Its implementation, which is symbol for the tillberg memory based learner.",
                    "label": 0
                },
                {
                    "sent": "Actually, learning learning is very easy in memory based approaches because you simply store information.",
                    "label": 1
                },
                {
                    "sent": "So that's why in the title it's both acquisition and processing and not only acquisition, and I will go into a few cases, the English and the Dutch past ends, but more Interestingly from a theoretical point of view, the German plural and the Dutch plural and I will end with a number of observations about.",
                    "label": 0
                },
                {
                    "sent": "Potential problems in the methodology of applying computational modeling, especially using machine learning.",
                    "label": 0
                },
                {
                    "sent": "In psycholinguistic.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK, so the very familiar, probably with the with the problem or with the.",
                    "label": 0
                },
                {
                    "sent": "That got me in theory formation, so you have to solve 2 problems if you want to say something interesting about human language parsing architecture, how is the linguistic knowledge that?",
                    "label": 1
                },
                {
                    "sent": "An agent needs, how is it represented and how is it acquired in the words and rules approach, dual mechanism approach or the lexicon plus grammar approach of Pinker and others.",
                    "label": 0
                },
                {
                    "sent": "It's a fairly easy modular system with a set of mental rules and associative memory, but there are a number of problems with that.",
                    "label": 0
                },
                {
                    "sent": "There is fuzziness and leakage between rules and between linguistic categories or concepts.",
                    "label": 0
                },
                {
                    "sent": "There are issues of semi regularity and irregularity that you have to handle.",
                    "label": 0
                },
                {
                    "sent": "And what we propose as an alternative is similarity based reasoning or analogy to solve the problem.",
                    "label": 0
                },
                {
                    "sent": "As for the question of how linguistic knowledge is acquired, the word same rules approach works from the assumption that you have innate rules or constraints or whatever in its structure.",
                    "label": 0
                },
                {
                    "sent": "Abstractions over the data which you have already available at birth, or which you extract from the primary linguistic data.",
                    "label": 0
                },
                {
                    "sent": "Whereas memory based approach would say that you only have to store patterns of observable linguistic items, and that's a bit of a methodological problem also.",
                    "label": 1
                },
                {
                    "sent": "But the type of linguistic information that we use is already rather abstract, so it's worth syllables.",
                    "label": 0
                },
                {
                    "sent": "Segmental structure of syllables.",
                    "label": 0
                },
                {
                    "sent": "You could say that it's much too abstract, because of course the speech signal is much more concrete and much more.",
                    "label": 0
                },
                {
                    "sent": "Specific than that, but we have seen a number of stock talks and there's a lot today and there's a lot of work showing that at least the assumptions we make about syllable structure being available or learnable using unsupervised learning techniques is not really a very far fetched assumption.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why do we like all of us?",
                    "label": 0
                },
                {
                    "sent": "I think machine learning so much as a model for for cognitive science.",
                    "label": 1
                },
                {
                    "sent": "I think it's becausw you can operationalize a lot of theoretical discussions very easily by refering to the different types of bias that the machine learning algorithm can have in solving an acquisition and processing task.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Picture is supposed to be vaguely reminiscent of the chomskian view of having a performance component and then which does input output mappings and then you have a competence with representations, linguistic concepts, linguistic rules, and then there's a learning component which tries to zoom in using some search algorithm into the right representations or the right knowledge which is necessary for solving the performance task, and you use primary linguistic data or experience to do that.",
                    "label": 0
                },
                {
                    "sent": "What machine learning gives us is some more insight into the possible types of bias that indexed inductive algorithms can have, both in their types of representations they take as input for learning in the types of representations that they use for representing the required knowledge, and most importantly the search method they used to arrive at right representations for the task.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will limit.",
                    "label": 0
                },
                {
                    "sent": "I will limit this talk to to supervise learning.",
                    "label": 0
                },
                {
                    "sent": "This is perhaps something I should have put in a few slides about that because I think there's a lot of confusion about super supervised and unsupervised learning and its relevance for language learning thing.",
                    "label": 0
                },
                {
                    "sent": "There's natural supervision and unnatural supervision if you assume that a child uses a tree bank with completely parse trees as information that is not really.",
                    "label": 0
                },
                {
                    "sent": "The right type of supervision.",
                    "label": 0
                },
                {
                    "sent": "If you presuppose that, for example, in learning prosodic system or stress system words, the system the chart uses information from the input it gets.",
                    "label": 0
                },
                {
                    "sent": "It's also supervised, but it's much more natural, and it's completely normal to use words with stress patterns as input and therefore to frame the problem as a classification problem that you can solve with supervised learning, so I think it's not as simple as saying that unsupervised learning is more psychologically relevant.",
                    "label": 0
                },
                {
                    "sent": "Then supervised learning depends on the task you're trying to solve in the way you actually actually represent the problem as a learning task.",
                    "label": 0
                },
                {
                    "sent": "So this is a very brief overview of the state of the art.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "As far as I know it in supervised learning at this time in machine learning of natural language, if you're learning to solve classification problem is supervised learning.",
                    "label": 0
                },
                {
                    "sent": "You try to approximate some underlying function that you assume to be there, and there are basically two ways to do that.",
                    "label": 1
                },
                {
                    "sent": "Either you immediately model this.",
                    "label": 0
                },
                {
                    "sent": "Put mapping using a discriminative approach or you model the underlying distributions which are important for the domain and by modeling the domain you get a solution of the task as well, so are the generative methods not to be confused with generative in the linguistic terminology.",
                    "label": 0
                },
                {
                    "sent": "So what the discriminative learner does, and I think most of the symbolic machine learning algorithms and also a lot of statistical learning algorithms, make the assumption that you don't stream estimate the underlying representations with directly try to estimate the input output mappings or whatever is relevant there.",
                    "label": 0
                },
                {
                    "sent": "So that's, let's say the discriminative POV and generative models like conditional random fields in the Markov models and all the variants that you have in graphical learning models, methods you try to model the whole domain and then you derive from that.",
                    "label": 0
                },
                {
                    "sent": "Your solution for the task there are problems with both approaches.",
                    "label": 0
                },
                {
                    "sent": "So in a discriminative approach you don't estimate the underlying knowledge which may be important in a generative approach.",
                    "label": 0
                },
                {
                    "sent": "You make a lot of very harsh assumptions about conditional independence.",
                    "label": 1
                },
                {
                    "sent": "Most of the time.",
                    "label": 0
                },
                {
                    "sent": "If you're trying to use it for language learning, especially in sequence learning.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tasks.",
                    "label": 0
                },
                {
                    "sent": "OK, so within the discriminative methods, think eager learning methods dominate the field, because this idea which also came up this morning of the minimal description length principle.",
                    "label": 1
                },
                {
                    "sent": "So actually what you try to do in eager learning approach is to find the model that fits the training data, potentially also store exceptional information.",
                    "label": 0
                },
                {
                    "sent": "If you can't find the model that fits all the training data and then the size of these two.",
                    "label": 0
                },
                {
                    "sent": "Combined is you Ristic that you use to juristic information that you're using finding a good model for your problem for your task?",
                    "label": 0
                },
                {
                    "sent": "So what you try to zoom into is the core and you forget about the data that you used to learn the core, and then you only store the periphery as as much as as needed to solve the task.",
                    "label": 1
                },
                {
                    "sent": "Now lazy learning memory based learning is a completely different as a completely different point of view and starts from the.",
                    "label": 0
                },
                {
                    "sent": "At least in my case, from the linguistic problem that you can't really say in advance what will be regular cases and would be except will be exceptional cases in what will be noise, right?",
                    "label": 0
                },
                {
                    "sent": "If you are a learning system or the input doesn't come labeled with, I'm a regular pattern and I'm an irregular pattern, or I'm in an exception, or I'm noise, that's exactly what you're trying to find out.",
                    "label": 0
                },
                {
                    "sent": "So having an architecture that sort of.",
                    "label": 0
                },
                {
                    "sent": "Tries to.",
                    "label": 0
                },
                {
                    "sent": "A priority make a distinction between regularity's and exceptions may have the wrong type of bias, and as most eager learning algorithms cannot really distinguish between noise and exceptions, and exceptions are for a number of reasons very productive in language processing, it's a bad choice to forget your data and you should at least keep all of the training information that you have, perhaps on top of that also extract.",
                    "label": 0
                },
                {
                    "sent": "Generalizations or obstructions, but the data may be important.",
                    "label": 1
                },
                {
                    "sent": "Why is it important?",
                    "label": 0
                },
                {
                    "sent": "Well, also for reasons that you are probably have hurt hurt about for a number of times for a number of large time problem in learning of small districts, pockets of exceptions, and polymorphism.",
                    "label": 0
                },
                {
                    "sent": "It's not very easy language to find very nice regions most of the time for any problem that you would try to solve where an eager learning algorithm would give you.",
                    "label": 1
                },
                {
                    "sent": "Small, compact model that actually is productive enough.",
                    "label": 0
                },
                {
                    "sent": "Also, because of different distributions which do not only exist in vocabulary but also in, for example, the grammar rules that you find in corpora or in anything that we want to measure.",
                    "label": 0
                },
                {
                    "sent": "In language you get this ception distributions where there's a small number of things that occurs.",
                    "label": 0
                },
                {
                    "sent": "A lot of the time, but the productive and regular actually set of things that are very uncommon.",
                    "label": 0
                },
                {
                    "sent": "Very frequent, but are nevertheless needed to make a good model.",
                    "label": 0
                },
                {
                    "sent": "At least that's the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The assumption?",
                    "label": 0
                },
                {
                    "sent": "Now, one way of visualizing that is to and this is work we did.",
                    "label": 0
                },
                {
                    "sent": "Looking at a number of different natural language processing tasks, data for plural formation, diminutive formation, morphology analysis, morphological analysis, prepositional phrase attachment, chunking, named entity recognition.",
                    "label": 0
                },
                {
                    "sent": "So what you see here in the graph is the cumulative percentage of training items which have a certain number of friendly nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "So what is a friendly nearest neighbor that's?",
                    "label": 0
                },
                {
                    "sent": "Close very similar item in your instance space that has the same category as you have now.",
                    "label": 0
                },
                {
                    "sent": "A hostile nearest neighbor is then a very close, very similar nearest neighbor which has a different category.",
                    "label": 0
                },
                {
                    "sent": "And what you see from this graph, for example, is that for all these tasks 10 to 20% of the cases doesn't have any friendly neighbors, so they are really isolated exceptions that would very easily be thrown away as noise in an eager learning algorithm, but nevertheless can reoccur if you have, for example, another sample of your population or a larger sample of your population.",
                    "label": 0
                },
                {
                    "sent": "And if you look at problems like morphological analysis for example, then you see that.",
                    "label": 0
                },
                {
                    "sent": "More than 50%.",
                    "label": 0
                },
                {
                    "sent": "Of the data has less than 10 friendly nearest neighbors, so you really have these pockets and this is disjunctive and polymorphous.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classes here.",
                    "label": 0
                },
                {
                    "sent": "So moving to inflectional morphology.",
                    "label": 0
                },
                {
                    "sent": "So the word same rules approach to this problem is to posit a mental rule for the default cases for the regular cases and the memory to store the exceptional cases, and that would be a perfectly plausible model and also a good explanatory value, except that you have this type of.",
                    "label": 0
                },
                {
                    "sent": "Album with nonsense words or with pseudo words or non words.",
                    "label": 0
                },
                {
                    "sent": "Where sometimes these words get default inflection, but sometimes the words actually are drawn to similar irregular items and get the irregular inflection of their nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "OK, so for example, if you have a pseudo word spelling.",
                    "label": 0
                },
                {
                    "sent": "And you have to rate how good splint or sling or Splunk is as a past tense of spring, you will find all the answers in a different distribution, but participants will will also say that OK's plan is an OK past tense for this pseudo word.",
                    "label": 1
                },
                {
                    "sent": "So one way to solve that is to posit an associative memory.",
                    "label": 0
                },
                {
                    "sent": "How to talk to make your your lexicon or your exceptional data, not just a list of exceptions, but make it productive by using a neural network or something associative.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But that's of course gives you the problem of how to decide when to follow the rule.",
                    "label": 0
                },
                {
                    "sent": "I want to follow memory and there have been very many proposals for that, but I've never actually seen a good implementation of any of them for fracing models, and you have getting models.",
                    "label": 0
                },
                {
                    "sent": "And if pre gating and post skating.",
                    "label": 0
                },
                {
                    "sent": "But all these are really.",
                    "label": 0
                },
                {
                    "sent": "A bit fishy, I think because you can really set boundaries in such a way in such a model that either the model is completely rule based or completely associative memory, and depending on the actual data that you are looking at, you can tune.",
                    "label": 0
                },
                {
                    "sent": "You can tweak the parameters too so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem.",
                    "label": 0
                },
                {
                    "sent": "So I will not go into a lot of detail for the past tense just to say that among work earlier, so it's working out at Psycho Linguistics Department in Antwerp and we are cooperating with a lot's done.",
                    "label": 0
                },
                {
                    "sent": "Some nice simulations on the past tense, which are forthcoming.",
                    "label": 0
                },
                {
                    "sent": "Where he?",
                    "label": 0
                },
                {
                    "sent": "Uses this type of information only.",
                    "label": 0
                },
                {
                    "sent": "Like for example, the features only contain the phonological information of the last two syllables split into the different parts of the syllable onset nucleus coda.",
                    "label": 1
                },
                {
                    "sent": "And there's some right alignment, and then for the classes.",
                    "label": 0
                },
                {
                    "sent": "These were automatically derived from the pairs of roots and past tense forms by applying a Levenshtein distance metric.",
                    "label": 1
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "It's not just saying regular or irregular, you actually learn the patterns with vowel changes and with the different types of analogical variants of the suffix suffixation.",
                    "label": 0
                },
                {
                    "sent": "So the task he's trying one of the tasks he tried to simulate is the rate.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The task of pesada in Pinker, so where people had to rate on a scale of 1 to 7 how well they liked the different past tenses.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Often on words or pseudowords, and to get that information out of the memory based learning model.",
                    "label": 0
                },
                {
                    "sent": "We used the class distribution of the different nearest neighbor.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, and there's also some work on the simulation of results by or both or Brighton, in case some more recent.",
                    "label": 0
                },
                {
                    "sent": "Views on the on the past tense learning with similar matches, and I'm just going to show you the matches with without a lot of information, so the blue and green, blue and the purple are the.",
                    "label": 0
                },
                {
                    "sent": "The ratings of the subjects are the participants rather and then the green and the orange are the.",
                    "label": 0
                },
                {
                    "sent": "The ratings extracted from the memory based from the memory based simulation.",
                    "label": 0
                },
                {
                    "sent": "Now what's most interesting I think here is not just the perfect match, it's very high correlation.",
                    "label": 0
                },
                {
                    "sent": "But also the fact that you don't see the horizontal line in the blue dots which you would expect if there were a default rule at work.",
                    "label": 0
                },
                {
                    "sent": "Because then the fact if no work is irregular or regular or very close to a regular or very close to an irregular should not have an effect on the way people.",
                    "label": 0
                },
                {
                    "sent": "Assign goodness to regular past tense so prasada in Pinker.",
                    "label": 0
                },
                {
                    "sent": "Following Prasad in pink are you would expect a horizontal blue line there.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and then the study was replicated for Dutch with exactly the same results, because in Dutch we have a very similar system.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, as in the English past tense.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm is really very simple.",
                    "label": 0
                },
                {
                    "sent": "For learning, you just store your instances in memory given some type of representation, which in this case for inflectional morphology is a number of features giving different parts of the syllable structure.",
                    "label": 1
                },
                {
                    "sent": "For classification, if you have a new test item item, either an existing one or a new one, you compare to the memory instances you take the nearest neighbors.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you extrapolate from them.",
                    "label": 0
                },
                {
                    "sent": "Of course, but you really have to if you want to use memory based learning for simulations, but you have to implement or what really is important is a similarity metric, and these are the four assumptions that we make feature relevance.",
                    "label": 0
                },
                {
                    "sent": "We use an information theoretic measurement gain ratio which is used a lot in machine learning.",
                    "label": 0
                },
                {
                    "sent": "For value similarity, we use the modified value distance metric.",
                    "label": 1
                },
                {
                    "sent": "Or Stanfield in volts?",
                    "label": 0
                },
                {
                    "sent": "I will use example relevance but not alot I will.",
                    "label": 0
                },
                {
                    "sent": "Give you briefly more information about that in a minute.",
                    "label": 0
                },
                {
                    "sent": "And then of course, the number of nearest neighbors and the distance to the nearest neighbors is very important, because that gives you an idea about the density and homogeneity of the local neighborhood.",
                    "label": 1
                },
                {
                    "sent": "So, but I forgot to mention is that memory based learning is a nonparametric approach, and probably I think the only one in let's say the discriminative methods.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't make any assumptions on the distribution of the input data, it only looks at the local neighborhoods and extrapolates from local neighborhoods and therefore.",
                    "label": 0
                },
                {
                    "sent": "Doesn't use any bias towards the type of distributions.",
                    "label": 0
                },
                {
                    "sent": "For example, normal distributions or skewed distributions that you would have your.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Training data.",
                    "label": 0
                },
                {
                    "sent": "OK so um.",
                    "label": 0
                },
                {
                    "sent": "Just very briefly about it, this modified value distance metric developed by Stanford in volts and refined by constant Salzburg.",
                    "label": 0
                },
                {
                    "sent": "Its way of clustering or doing unsupervised learning within the context of supervised learning task.",
                    "label": 1
                },
                {
                    "sent": "So what you do is you look for each value of each feature, what its distribution is of the different output classes that you're trying to predict, and those values that have a similar distribution will be more similar to each other than values that have more dissimilar distribution.",
                    "label": 0
                },
                {
                    "sent": "It's nothing more than that.",
                    "label": 0
                },
                {
                    "sent": "It's kind of clustering within the supervised.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning approach.",
                    "label": 0
                },
                {
                    "sent": "And then there's another important parameter.",
                    "label": 0
                },
                {
                    "sent": "Is the distance weighted class voting?",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If you look at it from a natural language processing point of view where you try to make systems that handle sparse data.",
                    "label": 1
                },
                {
                    "sent": "Increasing the value of K is the same as using a smoothing approach in probabilistic methods because you widen that, let's say the boundary of cases that you take into account, the local, you make the local context text bigger and so you get more information that you can.",
                    "label": 0
                },
                {
                    "sent": "That you can use to make your decision, just like you can do smoothing in probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "OK. And of course, here one parameter is that how you take into account this local neighborhood the nearest neighbors in your local neighborhood, do you?",
                    "label": 0
                },
                {
                    "sent": "Take a Democratic rule.",
                    "label": 0
                },
                {
                    "sent": "Just saying every nearest neighbor that is in the scope of my K as an equal vote in the final decision.",
                    "label": 0
                },
                {
                    "sent": "You can be a bit more specific and a bit more adapted to the data by using for example inverse distance or exponential decay.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "I don't have any examples here today about using example waiting, so I will not go into that in detail.",
                    "label": 0
                },
                {
                    "sent": "Just saying that you can use in the table environment.",
                    "label": 0
                },
                {
                    "sent": "You can use example waiting for example to model frequency effects.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or typical T effects or class prediction strength.",
                    "label": 0
                },
                {
                    "sent": "This is a bout timbale which you can download from Tilburg site.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next version will be open source, but you can already use it for research.",
                    "label": 0
                },
                {
                    "sent": "And education.",
                    "label": 0
                },
                {
                    "sent": "So back to inflectional morphology.",
                    "label": 1
                },
                {
                    "sent": "One illustration of how the memory based approach works in inflectional morphology is a German plural, which is actually much more interesting than the English past tense because it's sort of a reversal of the case in the English part, since we have the, say, the big frequent class is the regular and you only have a few exceptions.",
                    "label": 0
                },
                {
                    "sent": "In German, if you follow the word same rules approach, it's actually the opposite.",
                    "label": 0
                },
                {
                    "sent": "Well, John Lewis is learned very quickly by children, but not by foreign language or second language learners.",
                    "label": 0
                },
                {
                    "sent": "So it's really a complex system with phonological and lexical information needed to solve it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "And so, in the discussion of the words and rules versus single mechanism approaches, the claim is that as the suffix, which is actually a very infrequent suffix, is the one that you use in a number of cases where in the English past tense you use the Ed suffix, right?",
                    "label": 0
                },
                {
                    "sent": "Therefore, the suffix should be the default.",
                    "label": 0
                },
                {
                    "sent": "It should be the rule and all the other inflections or the other suffixes cannot be anything else but irregular cases and should be stored in associative memory and can only be solved by associative memory.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a bit of a strange way of doing it, because as you can see here, the suffix is really a minority class.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the German plural.",
                    "label": 0
                },
                {
                    "sent": "Still, the bickering and I'll just make a very good point that in all these cases where you are not really, you don't really have a model to extrapolate from, or you don't really have something that is clearly visible or detectable as a noun or a verb.",
                    "label": 0
                },
                {
                    "sent": "Now in this case, in those cases people actually use the S suffix.",
                    "label": 0
                },
                {
                    "sent": "So the surnames, the borrowings, the acronyms, the lexicalized.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "ISIS etc.",
                    "label": 0
                },
                {
                    "sent": "So this is the representation which we used.",
                    "label": 0
                },
                {
                    "sent": "We use symbolic features, two syllables, again the same, segmentation in onset.",
                    "label": 1
                },
                {
                    "sent": "Nucleus and coda of the syllables, and then the gender feature, which is also important and.",
                    "label": 0
                },
                {
                    "sent": "Of course we use sielox's.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The data set.",
                    "label": 0
                },
                {
                    "sent": "And then if you look at the for example, the feature relevance computation using gain ratio or information gain or chi Square or alternatives, you see that it's indeed the last syllable, and especially the code of the last syllable, which is very informative and also.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The gender of feature.",
                    "label": 0
                },
                {
                    "sent": "Just to illustrate how you can get at the unsupervised Lee learned information from the Supervised learning task.",
                    "label": 0
                },
                {
                    "sent": "This is a clustering of the VDM matrixes that you were extracted during learning.",
                    "label": 0
                },
                {
                    "sent": "So remember that these MDM metrics have information about for each value of a feature about what their distribution is over the different out possible output classes.",
                    "label": 0
                },
                {
                    "sent": "So in this case the five suffixes with a number of.",
                    "label": 0
                },
                {
                    "sent": "About variance and if you cluster this information then you see for example that in German neutral masculine or much closer behave in a similar way, whereas feminine behaves in a different way in the model that has been extracted from these matrices, whereas you can also find, for example, number of interesting clusters of phonemes.",
                    "label": 0
                },
                {
                    "sent": "For example all the nasalized vowels are together in what mapped in one class.",
                    "label": 0
                },
                {
                    "sent": "All the shortfalls long vowels occur together, so you get kind of a linguistic theory about the logical classes that are relevant in solving the German plural formation task, almost for free.",
                    "label": 0
                },
                {
                    "sent": "By using this modified.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Value this metric.",
                    "label": 0
                },
                {
                    "sent": "Of course, psychologically speaking, what is interesting is that this information is used, and so it's like there's some heuristic value that people may use that information as well.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the acquisition data that is available for the German plural because of course.",
                    "label": 0
                },
                {
                    "sent": "In the memory based, if you want a simulation, memory based or otherwise, you want to.",
                    "label": 0
                },
                {
                    "sent": "Reproduce the empirical data that exists.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the acquisition data, you see that children mainly over apply the E in the suffix and at the S pools are learned rather late and you also see that for novel words.",
                    "label": 1
                },
                {
                    "sent": "So non existing words pseudowords children inflict them with the OR in most of the time and that the more irregular plural forms are produced more than the default suffix.",
                    "label": 0
                },
                {
                    "sent": "In charge.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slang language acquisition data.",
                    "label": 0
                },
                {
                    "sent": "And you see exactly the same happening in if you see the learning curves for the German plural using the memory based approach.",
                    "label": 0
                },
                {
                    "sent": "So the two top ones are learned almost immediately.",
                    "label": 0
                },
                {
                    "sent": "If you only need like 50 or 60 cases to learn the Ian and.",
                    "label": 0
                },
                {
                    "sent": "Say the conversion suffix so no suffix that's immediately learned, so that's a very clear phonological context where you can boots up on and learn it from, and then you see that the other suffixes E&ER.",
                    "label": 0
                },
                {
                    "sent": "I learned a bit more slowly, but still reach a very high accuracy, and that's indeed the S suffix, which is lagging behind.",
                    "label": 0
                },
                {
                    "sent": "Why is this the case?",
                    "label": 0
                },
                {
                    "sent": "Because it's for large part, I think conventional, so you really have to know that something is a name to not inflected with, say, the final logically correct suffix, but use this default suffix.",
                    "label": 0
                },
                {
                    "sent": "So if you would have more add more information in the similarity space for.",
                    "label": 0
                },
                {
                    "sent": "This problem then, probably this could be learned as well, but of course.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's for us to prove.",
                    "label": 0
                },
                {
                    "sent": "If you look at the errors, which model makes memory based learner makes, then you also see that EN&E are the most of the suffixes that are most overgeneralized by the model and showed in the previous graph that Ian Ian are required faster than the DS went late and imperfectly.",
                    "label": 0
                },
                {
                    "sent": "Like with the children.",
                    "label": 0
                },
                {
                    "sent": "It was also interesting.",
                    "label": 0
                },
                {
                    "sent": "One of the arguments for the for Pinker to be an associates to be interested in the German plural is that the connection is network is supposed not to be able to learn this type of problem because the default here is not the majority.",
                    "label": 0
                },
                {
                    "sent": "The majority class right?",
                    "label": 0
                },
                {
                    "sent": "And so the implicit argumentation or explicit argumentation here is that single root models.",
                    "label": 0
                },
                {
                    "sent": "Have performed so well in the past tense becausw in the past tense.",
                    "label": 0
                },
                {
                    "sent": "For English the default suffix is the majority suffix, but if you look at the overgeneralizations, the errors which the memory based learning simulation makes you see that it's not just an input is output effect because you don't exactly find, let's say, the most frequent input suffix as the the most overgeneralized or the most over regular suffix.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's actually actually a different ranking.",
                    "label": 0
                },
                {
                    "sent": "Now another piece of empirical acquisition data that was available for the German plural is a study by Barca and others on how children age 3.6 to 6.6.",
                    "label": 0
                },
                {
                    "sent": "Name?",
                    "label": 0
                },
                {
                    "sent": "Name the I think it's the repetition task here named the different objects that they are confronted with with different plural suffixes.",
                    "label": 0
                },
                {
                    "sent": "And then you see that if you divide or if you make two conditions in your experiment, one riming condition and one non rhyming condition for routes that you present to the children, then you see that the errors that the children make.",
                    "label": 0
                },
                {
                    "sent": "I think it's sort of partition task, so you get presented with the wrong or the right suffix and then you have two children have to repeat and sometimes they repeat correctly and sometimes it correctly.",
                    "label": 0
                },
                {
                    "sent": "Then you see that the.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Let's say in the non running condition.",
                    "label": 0
                },
                {
                    "sent": "The the S suffix is used much more right?",
                    "label": 0
                },
                {
                    "sent": "So the claim here by Barca and others is that children are aware that unusual sounding words require the default.",
                    "label": 1
                },
                {
                    "sent": "And that her children are aware that names require the default, but that's the graph that I didn't that I didn't show you.",
                    "label": 0
                },
                {
                    "sent": "But it's it's normal similar graph.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then for the for the names.",
                    "label": 0
                },
                {
                    "sent": "If you look at the output of the memory based simulation.",
                    "label": 0
                },
                {
                    "sent": "They're going to look.",
                    "label": 0
                },
                {
                    "sent": "You sort your the output of the system after the leave one out.",
                    "label": 0
                },
                {
                    "sent": "Experiment into the rammingen rhyming conditions.",
                    "label": 0
                },
                {
                    "sent": "Then you see exactly the same happening.",
                    "label": 0
                },
                {
                    "sent": "So in a non rhyming condition the suddenly the end over generalization disappears in favor of this over generalization.",
                    "label": 0
                },
                {
                    "sent": "So again, this is just an aspect of how your lexicon, let's say that the training with the properties of the training material rather than some innate bias.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, for the German plural we see three different classes of plurals appearing in and conversion ER in S, and, whereas the former for suffixes at least 4.",
                    "label": 1
                },
                {
                    "sent": "The dual mechanism method would be stored in the associative memory because they are irregular, it's only the suffix which is the default rule.",
                    "label": 0
                },
                {
                    "sent": "We would say that it's actually the regular cases and that it's switches learned late and using a lot of conventional information like and syntactic context information.",
                    "label": 1
                },
                {
                    "sent": "Also very nice is that we didn't see any difference in the learnability of.",
                    "label": 0
                },
                {
                    "sent": "Words with or without an umlaut, which is also has been attested in child language acquisition, literature, and overall generalization.",
                    "label": 0
                },
                {
                    "sent": "Accuracy is very high.",
                    "label": 0
                },
                {
                    "sent": "If you look at.",
                    "label": 0
                },
                {
                    "sent": "Using clustering techniques at.",
                    "label": 1
                },
                {
                    "sent": "What is going on in the neighborhoods?",
                    "label": 0
                },
                {
                    "sent": "And if you look at the nearest neighbors of different cases then you see that's what you get is actually the type of schemata which have been proposed by cupcake for the German plural away.",
                    "label": 0
                },
                {
                    "sent": "See for example words with the code here which have the lexical feature masculine always get East as a suffix.",
                    "label": 0
                },
                {
                    "sent": "Words like arms near things like that.",
                    "label": 0
                },
                {
                    "sent": "So you don't really have to represent that explicitly in memory schemes like that, because they come out automatically in the way the memory based learner works.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, onto the the Dutch plural and I will skip.",
                    "label": 1
                },
                {
                    "sent": "Just explain how the Dutch plural, what the problem there is for the dual roots or the dual mechanism approach.",
                    "label": 0
                },
                {
                    "sent": "So what you see is that, again, like in the German plural, you have a very funny, logically motivated distinction between two possible suffixes N or S, which are in almost completely complementary distribution.",
                    "label": 0
                },
                {
                    "sent": "So actually it's very easy to learn it if you abstract from a few loanwords.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things like that.",
                    "label": 0
                },
                {
                    "sent": "But the Dutch plural is problematic for the dual mechanism model, because what would you claim to do if you have two default rules for all of the conditions where the default rule normally applies?",
                    "label": 1
                },
                {
                    "sent": "Both yes and the applied depending on the type of word, phonological makeup of the word.",
                    "label": 0
                },
                {
                    "sent": "So actually the solution that Pinker came up with for the Dutch plural.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is this model?",
                    "label": 0
                },
                {
                    "sent": "So where you have, let's say, the final logical domain which decides in or S and then you have two rules with their associated associative memory for the exceptions which.",
                    "label": 0
                },
                {
                    "sent": "I can't really make a lot of sense of because I I wonder what associative memory would look like.",
                    "label": 0
                },
                {
                    "sent": "If you have really need these two 2 subsets of associative memory to handle the exceptions to the two different.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So default rules.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So I will skip this.",
                    "label": 0
                },
                {
                    "sent": "This is a recent work.",
                    "label": 0
                },
                {
                    "sent": "Mainly by Immanuel Curless, showing how the one problem which you have in the Dutch plural, which is the inflection of unassimilated borrowings, can be solved in a memory based.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The approach and what this tells you about dual roots alternatives and I will use my final.",
                    "label": 0
                },
                {
                    "sent": "5 minutes or so to to present to you a methodological problem.",
                    "label": 0
                },
                {
                    "sent": "So what you see often in the literature is that you get some data set and then you get an accuracy and then say we have used backdrop learning algorithm with these parameters and the problem is solved that way.",
                    "label": 0
                },
                {
                    "sent": "But what can we learn from that?",
                    "label": 1
                },
                {
                    "sent": "From a psycholinguistic POV?",
                    "label": 0
                },
                {
                    "sent": "So what useful information do we get from just one simulation in space?",
                    "label": 0
                },
                {
                    "sent": "Large space of possible simulations that could have completely different?",
                    "label": 0
                },
                {
                    "sent": "Outcomes so.",
                    "label": 0
                },
                {
                    "sent": "What we really need is robustness in the sense that different parameters settings should not give you wildly different results for us in solving some problem, except when there's some psycho linguistically relevant going out.",
                    "label": 0
                },
                {
                    "sent": "So the parameters that you look at me by optimization in machine learning is doing a lot of optimization these days.",
                    "label": 0
                },
                {
                    "sent": "You can always get at some.",
                    "label": 0
                },
                {
                    "sent": "Very strange parameter setting where, for example, the first syllable of a word is important in doing the inflection.",
                    "label": 0
                },
                {
                    "sent": "While we know that, for example, the last syllable is most important in most inflectional systems, but nevertheless it would be possible for us also to make a model doing parameter tweaking and a lot of machine learning optimization work that would actually show you that that having the first syllable is actually a good indicator of of.",
                    "label": 0
                },
                {
                    "sent": "The problem you are trying to solve or good information source.",
                    "label": 0
                },
                {
                    "sent": "So you should be careful about that type of outcomes or results your.",
                    "label": 0
                },
                {
                    "sent": "Results should hold for a number of different parameters settings, and they should also hold for different tasks for different empirical data sets.",
                    "label": 1
                },
                {
                    "sent": "So the type of settings that you use if you say K should have a high value inflection inflectional morphology.",
                    "label": 0
                },
                {
                    "sent": "It's not enough to show that on the English path, but past tense or in the German plural.",
                    "label": 0
                },
                {
                    "sent": "It should also be shown in other inflection.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tasks right?",
                    "label": 0
                },
                {
                    "sent": "So one step in that direction is a recent work with it in.",
                    "label": 0
                },
                {
                    "sent": "In this direction.",
                    "label": 0
                },
                {
                    "sent": "So for the Dutch plural, we made 23,000 simulations on three different tasks, three different datasets.",
                    "label": 1
                },
                {
                    "sent": "So one is what we call a lexical reconstruction task.",
                    "label": 1
                },
                {
                    "sent": "It's actually across validation on the on the training data and so that sort of mimics the situation where you suddenly forget 110th of your lexicon and then reconstruct that lexicon which we would claim is not really a psycho linguistically very relevant task.",
                    "label": 0
                },
                {
                    "sent": "And then there's more interesting empirical data to data sets of pseudo words and participant behavior on pseudowords.",
                    "label": 0
                },
                {
                    "sent": "One said bye.",
                    "label": 0
                },
                {
                    "sent": "Bye and.",
                    "label": 0
                },
                {
                    "sent": "Of ATC, other words in from 2002 and one of one and 80.",
                    "label": 0
                },
                {
                    "sent": "Three other words.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Enter the way we are going to to evaluate is to look at the predictions of the model of all these different models and compare it to the majority of the decisions.",
                    "label": 0
                },
                {
                    "sent": "Of the different.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Disciplines.",
                    "label": 0
                },
                {
                    "sent": "OK, I will go quickly here because it's not really darling.",
                    "label": 0
                },
                {
                    "sent": "Important if you're not really into the the Dutch plural, there are many.",
                    "label": 0
                },
                {
                    "sent": "There are many philological or if I mention source parameters that we that we varied like the number of syllables that you give the model to learn from.",
                    "label": 0
                },
                {
                    "sent": "Whether it has stress or no stress.",
                    "label": 0
                },
                {
                    "sent": "If you give information about the final grapheme or not because there's a linguistic reason why this would be a natural way of doing it, different types of alignment set.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "You also vary the type of class representation that can be just discrete classes, but also the type of patterns extracted with Levenshtein distance that you do an actual transformation of the input in the output.",
                    "label": 0
                },
                {
                    "sent": "Also, a bit like neural networks do different.",
                    "label": 0
                },
                {
                    "sent": "The parameters specially of course the K because we think that's the highest chance of being psycho linguistically relevant.",
                    "label": 0
                },
                {
                    "sent": "This is waiting method set.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And then you see, for example in this case.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "That the best models achieve really an enormous accuracy for some of the tasks of 100% and normally of course that is what you publish them, right?",
                    "label": 0
                },
                {
                    "sent": "I want you forget about is the let's say the badly performing.",
                    "label": 0
                },
                {
                    "sent": "Models, I'm not saying that this is according to machine learning methodology or wrong way of doing it, but it's just not very informative.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in psycholinguistic modeling and the type of parameters that play a role in psycholinguistic modeling.",
                    "label": 0
                },
                {
                    "sent": "Of course, if I would do NLP parsing, I would also use the.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Best strategy?",
                    "label": 0
                },
                {
                    "sent": "So if you could look at this these many different results and then look for example where this still comes from.",
                    "label": 0
                },
                {
                    "sent": "So you see that on average the accuracy is pretty good, but you have very long tails of very bad models which don't perform very.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, and then you can systematically look.",
                    "label": 0
                },
                {
                    "sent": "For example, is it the number of syllables?",
                    "label": 0
                },
                {
                    "sent": "And then you see that indeed for this problem, if you throw out all the models with only one syllable as information as input information, your models become the averages and the best and the worst results get a lot better than if you take not the best results, of course, but the average models become and the worst models become much better if you.",
                    "label": 0
                },
                {
                    "sent": "Don't if you use at least two syllables as.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Information.",
                    "label": 0
                },
                {
                    "sent": "So apart from this one silver model said the results from the memory based learning simulation appear to be quite robust for many different parameters settings.",
                    "label": 1
                },
                {
                    "sent": "There are excusing there are few differences, for example, stress information we could not really show to that that adds any information using more than two syllables does not really help accuracy except on one of the pseudo word.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cassettes?",
                    "label": 0
                },
                {
                    "sent": "So you're coming back to this last graph in that you could also give as information to the model.",
                    "label": 0
                },
                {
                    "sent": "In in Dutch we have the situation that you in the phonology some information.",
                    "label": 0
                },
                {
                    "sent": "Lexical information gets hidden like hunt with the dog is pronounced haunt with T. But in in the plural you get Honda where the D which is on the line in the stem in the root appears again same for things like Abarca where you don't pronounce normally the end.",
                    "label": 0
                },
                {
                    "sent": "So in the phonological representation you would have a route ending in a schwa and whereas the underlying information is really that there's a nasal consonant there at the end of the world, and actually putting that information in the model helps the accuracy a lot.",
                    "label": 1
                },
                {
                    "sent": "Overall, the others averaged over all the other parameter settings so.",
                    "label": 0
                },
                {
                    "sent": "It's it's useful information, and it's even also accessible, potentially by people who don't know how to read like children, because there is some work by buying an estrous showing that that actually in the pronunciation already in the signal in the speech signal you see marked differences between the T coming from being reduced from the D and the T as a part of the lexical lexical route.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then there's a lot of stuff that didn't have any.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, effect or not, a lot of effects.",
                    "label": 0
                },
                {
                    "sent": "This is the influence of K. The value of K. So you see here that using.",
                    "label": 0
                },
                {
                    "sent": "Only the one nearest neighbor is is not really a very good idea.",
                    "label": 0
                },
                {
                    "sent": "You really need this smoothing effect of having a larger local neighborhood to extrapolate from, and then it doesn't really matter a lot if you don't use any decay or if you use inverse distance decay.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Right overall, the models are very.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Robust.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I just said this.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is again saying the same thing.",
                    "label": 1
                },
                {
                    "sent": "If you have your K too low.",
                    "label": 0
                },
                {
                    "sent": "Then you're not really going to match the behavior of the participants very well.",
                    "label": 0
                },
                {
                    "sent": "Even if you do very well in the lexical reconstruction task, so that's another indication that in the experiments using real participants an pseudowords something else is going on.",
                    "label": 1
                },
                {
                    "sent": "Then in the lexical reconstruction task which is used a lot to compare.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Models.",
                    "label": 0
                },
                {
                    "sent": "OK, so my conclusions are that Emble appears to be robust model for inflectional morphology learning.",
                    "label": 1
                },
                {
                    "sent": "If you have sufficient information to present to represent the examples, so in the case of the German plural you would need other information in the similarity space then then just from a logical information.",
                    "label": 0
                },
                {
                    "sent": "That case up to now, the most important parameters that we found, with potential psycholinguistic relevance and that it would be very good if other people would also report ranges rather than the best simulation, because that way you could compare also the effect of different parameters on different tasks and also not use just one data set which you made yourself most of the time you do an experiment with students and then you make a model simulating.",
                    "label": 0
                },
                {
                    "sent": "The behavior of the students.",
                    "label": 0
                },
                {
                    "sent": "It would be good if you use all the available data sets to test your model and perhaps a little bit in the spirit of the talk we heard first.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Weird today.",
                    "label": 0
                },
                {
                    "sent": "And I'll stop with a brief list of two brief list of other groups using timbale memory based learning in modeling modeling work.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll stop here.",
                    "label": 0
                }
            ]
        }
    }
}