{
    "id": "2c2sv74c5ceuy74ui24f5qralz6kgynh",
    "title": "Consistency of Robust Kernel Density Estimators",
    "info": {
        "author": [
            "Robert A. Vandermeulen, Department of Electrical Engineering and Computer Science, University of Michigan"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_vandermeulen_kernel/",
    "segmentation": [
        [
            "Hello."
        ],
        [
            "Right, so we're looking at a pretty standard situation for density estimation.",
            "We have N samples from some distribution F for robust kernel density estimation, you would probably assume that F is some mixture of a nominal in a contamination distribution, but that won't be relevant for the analysis here.",
            "So F sub Sigma N bar is the kernel density estimator.",
            "The Classic One case of Sigma is some radial smoothing kernel which integrates to one.",
            "Sigma is its bandwidth or smoothing parameter.",
            "If K is positive definite, then we can imagine that kernel being in the reproducing kernel Hilbert space associated with that kernel, and then we have.",
            "The reproducing kernel Hilbert space representation of the kernel density estimator it's pretty easy to show that the kernel density estimator is the least squares solution to that equation up there, and that looks very familiar to this sort of equation.",
            "Would like to modify for M estimation, so it begs the question question what happens when you replace the square loss with a robust Los Ro and that was investigated by Kim and Scott in 2012.",
            "And it turns out they end up with a robust kernel density estimator.",
            "It is a weighted kernel density estimator an it is robust in the sense that kernels in low density regions sort of get less less weight and then curls in high density or agents get more weight and we like that because you expect contamination to manifest itself in excess of samples in the low density regions."
        ],
        [
            "That paper they they.",
            "Have quite a few interesting results, but they don't have any sort of any sort of consistency theorem, so I went ahead and proved that so there's a theorem up there.",
            "You'll notice that the rate on bandwidth is as good as the classic curl density estimator.",
            "In order for this to work, we need certain properties on row.",
            "Basically, Euro needs to be flat at zero, and then it needs to go become linear for large TNB strictly convex."
        ],
        [
            "So for this sort of thing, your normal proof strategy would be something along the lines of doing a bias variance decomposition and then taking care of the bias using analysis and taking care of the variance using empirical process theory.",
            "So it's possible to take care of the bias doing this, but unfortunately a robust kernel density estimator does not have a closed form expression, so it's hard to get even handle on that.",
            "So what you would do is you would sort and analyze the risk over the reproducing kernel Hilbert space, and it turns out as Sigma gets small.",
            "Are reproducing kernel Hilbert space or despawns lots of dimensions that become annoying for our empirical process concentrations and empirical process theory loses and you get sub optimal rate on bandwidth.",
            "So instead what we did was we investigated the iterated reweighted least squares algorithm from Kim and Scott here.",
            "That's our Sub Sigma event which I'll just call R. Here the application of that algorithm to do some element of our reproducing kernel Workspace gives us a sequence of weighted kernel density estimators which converges to the robust kernel density estimator."
        ],
        [
            "So here's 15 pages of proof boiled down to two bullet points.",
            "It's based on two key observations.",
            "If you take the iterative algorithm R and apply it to the zero vector, you get the classic kernel density estimator and the robust called estimator is a fixed point of our.",
            "So from this we can look at the distance between those two estimators.",
            "First equation there and then we can rewrite it using R and then.",
            "In my paper, that key point is that R is with high probability a contraction mapping.",
            "With this contraction Constant, C Sigma to the D / 2 and as Sigma gets small, that contraction constant gets really small.",
            "So sort of crushes the robust kernel density estimator together with the regular curl density estimator which we all know is consistent.",
            "And if you want any more details."
        ],
        [
            "Visit me at my poster."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so we're looking at a pretty standard situation for density estimation.",
                    "label": 0
                },
                {
                    "sent": "We have N samples from some distribution F for robust kernel density estimation, you would probably assume that F is some mixture of a nominal in a contamination distribution, but that won't be relevant for the analysis here.",
                    "label": 0
                },
                {
                    "sent": "So F sub Sigma N bar is the kernel density estimator.",
                    "label": 0
                },
                {
                    "sent": "The Classic One case of Sigma is some radial smoothing kernel which integrates to one.",
                    "label": 0
                },
                {
                    "sent": "Sigma is its bandwidth or smoothing parameter.",
                    "label": 0
                },
                {
                    "sent": "If K is positive definite, then we can imagine that kernel being in the reproducing kernel Hilbert space associated with that kernel, and then we have.",
                    "label": 0
                },
                {
                    "sent": "The reproducing kernel Hilbert space representation of the kernel density estimator it's pretty easy to show that the kernel density estimator is the least squares solution to that equation up there, and that looks very familiar to this sort of equation.",
                    "label": 0
                },
                {
                    "sent": "Would like to modify for M estimation, so it begs the question question what happens when you replace the square loss with a robust Los Ro and that was investigated by Kim and Scott in 2012.",
                    "label": 1
                },
                {
                    "sent": "And it turns out they end up with a robust kernel density estimator.",
                    "label": 1
                },
                {
                    "sent": "It is a weighted kernel density estimator an it is robust in the sense that kernels in low density regions sort of get less less weight and then curls in high density or agents get more weight and we like that because you expect contamination to manifest itself in excess of samples in the low density regions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That paper they they.",
                    "label": 0
                },
                {
                    "sent": "Have quite a few interesting results, but they don't have any sort of any sort of consistency theorem, so I went ahead and proved that so there's a theorem up there.",
                    "label": 0
                },
                {
                    "sent": "You'll notice that the rate on bandwidth is as good as the classic curl density estimator.",
                    "label": 0
                },
                {
                    "sent": "In order for this to work, we need certain properties on row.",
                    "label": 1
                },
                {
                    "sent": "Basically, Euro needs to be flat at zero, and then it needs to go become linear for large TNB strictly convex.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for this sort of thing, your normal proof strategy would be something along the lines of doing a bias variance decomposition and then taking care of the bias using analysis and taking care of the variance using empirical process theory.",
                    "label": 0
                },
                {
                    "sent": "So it's possible to take care of the bias doing this, but unfortunately a robust kernel density estimator does not have a closed form expression, so it's hard to get even handle on that.",
                    "label": 1
                },
                {
                    "sent": "So what you would do is you would sort and analyze the risk over the reproducing kernel Hilbert space, and it turns out as Sigma gets small.",
                    "label": 0
                },
                {
                    "sent": "Are reproducing kernel Hilbert space or despawns lots of dimensions that become annoying for our empirical process concentrations and empirical process theory loses and you get sub optimal rate on bandwidth.",
                    "label": 0
                },
                {
                    "sent": "So instead what we did was we investigated the iterated reweighted least squares algorithm from Kim and Scott here.",
                    "label": 1
                },
                {
                    "sent": "That's our Sub Sigma event which I'll just call R. Here the application of that algorithm to do some element of our reproducing kernel Workspace gives us a sequence of weighted kernel density estimators which converges to the robust kernel density estimator.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's 15 pages of proof boiled down to two bullet points.",
                    "label": 0
                },
                {
                    "sent": "It's based on two key observations.",
                    "label": 0
                },
                {
                    "sent": "If you take the iterative algorithm R and apply it to the zero vector, you get the classic kernel density estimator and the robust called estimator is a fixed point of our.",
                    "label": 1
                },
                {
                    "sent": "So from this we can look at the distance between those two estimators.",
                    "label": 0
                },
                {
                    "sent": "First equation there and then we can rewrite it using R and then.",
                    "label": 0
                },
                {
                    "sent": "In my paper, that key point is that R is with high probability a contraction mapping.",
                    "label": 0
                },
                {
                    "sent": "With this contraction Constant, C Sigma to the D / 2 and as Sigma gets small, that contraction constant gets really small.",
                    "label": 0
                },
                {
                    "sent": "So sort of crushes the robust kernel density estimator together with the regular curl density estimator which we all know is consistent.",
                    "label": 1
                },
                {
                    "sent": "And if you want any more details.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Visit me at my poster.",
                    "label": 0
                }
            ]
        }
    }
}