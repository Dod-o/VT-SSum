{
    "id": "oxn7i7dbtbm6544okztw3fafwx7xgjay",
    "title": "Neyman-Pearson classification under a strict constraint",
    "info": {
        "author": [
            "Philippe Rigollet, Department of Mathematics, Massachusetts Institute of Technology, MIT"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Mathematics"
        ]
    },
    "url": "http://videolectures.net/colt2011_rigollet_strict/",
    "segmentation": [
        [
            "OK, let's go back to the slide that's been shown Hunter about 1000 times it cold, so we're looking at binary classification.",
            "We have a feature X in Rd labels or minus one one and we want to construct a classifier that Maps a feature vector X in 2 -- 1 one and we know that the standard way to measure the performance of a given classifier is to look at the classification error, which is simply the probability it makes an error on a given yet innocent instance.",
            "And actually this is a combination of two types of error you can make.",
            "We know from introductory statistics that there's two types of error.",
            "The first one is to say, plus one.",
            "When it's minus one or two, say minus one.",
            "When it's plus one an, we're going to try in this talk to deal with them separately."
        ],
        [
            "So of course we want to control them simultaneously in some way, But the question is how do we combine them?",
            "And there's not just one way to do it.",
            "The nice thing about classification errors that it just does it for you.",
            "It's just the convex combination of those two types of errors that I've just shown.",
            "So remember, our minus is the is the Type 1 error.",
            "An R Plus is the Type 2 error and is just a weighted combinations.",
            "The weight being given by the proportion of each of the classes.",
            "So minimizing that gives you one way to minimize the two simultaneously an another way to do is if your have more statistical background, you know that Norman Pearson have tried to answer this problem.",
            "Trying to combine these two errors that typically cannot be minimized simultaneously, so Neyman Pearson is very systematic.",
            "It's a paradigm, it's a way of thinking of how to minimize those two errors, and the way we do it is, we say, let's force the Type 1 error to be below a given level.",
            "Alpha which is user specified, typically 1% or 5%, and once we've done that, we're going to cross our fingers and try to minimize the Type 2 error as much as we can, and so of course, here.",
            "Obviously we put more emphasis on the Type 1 error, so hence symmetric between the two types of error, and because we really are enforcing some level on the Type 1 error in the Type 2 error.",
            "We just hope for the best."
        ],
        [
            "So why would we want to do that where we want to introduce this symmetric and right classification error is very natural, it's simple and it combines them in a natural way, but we'll see that in some instances it might not be adapted to their classification task at hand.",
            "And we've seen yesterday that depending on the context you want to use different ways of doing it, so let's look at examples from this bestseller.",
            "By the way, I showed this book last year an I don't perceive any royalty on this book, unfortunately.",
            "Otherwise, I would also be gaining some prize money."
        ],
        [
            "So in this book you can just read this is verbatim from this book.",
            "So you have a problem where you're trying to predict for patient whether it's going to have a second heart attack and this should be based on on some features.",
            "You measure demographic clinical."
        ],
        [
            "So of course here type 1 error is the one you want to strictly contain and is the probability of predicting no second heart attack, whereas the patient is going to have one.",
            "So that's typical example."
        ],
        [
            "We give another one that I found in this book is funny in recognition, so you have versus Al based on time series of those."
        ],
        [
            "Observation, and here what is the type 1 error?",
            "Well, who cares?",
            "I mean it's not like one is more important than the other.",
            "So in this case it's not always the case that you want to privilege one of the errors."
        ],
        [
            "And another one is use systolic blood pressure to predict some heart disease."
        ],
        [
            "And here, of course, the Type 1 error, the one we want to maintain below a certain level, is to predict no coronary heart disease, whereas patient is going to have a corner hard to."
        ],
        [
            "These and finally the example I found in this book was pretty."
        ],
        [
            "Spams and here actually type 1 error depends on what the user wants, whether the user, whether you're annoyed to go in your junk folder and pull out spams or receive spam in your inbox.",
            "That really depends on you and you're not really ask, but maybe you should so.",
            "In some instances we can see that it might be meaningful to put more emphasis on the Type 1 error."
        ],
        [
            "Another case I can make for that is in a very specific case where one of the classes is under represented.",
            "One of the classes when P is extremely small.",
            "When you're going trying to minimize the classification error, you see this weighted combination of the two errors.",
            "And step away from that.",
            "So if you if you if this one, for example, extremely small what your classification rule, the one that minimizes this risk, what it's going to be doing is is that it will just not care about the Type 1 error, will try to minimize this one because it gets really high weight, and that's typically the case in animality tection where P is for example 5% maybe 1%.",
            "You have very small number in one class, so in this case you might actually run into the problem that what your classification rule is going to do is say well.",
            "Everything is not an anomaly because well in this case I'm not going to make that much error anyway, since it just does."
        ],
        [
            "It happens so much.",
            "So to do that, of course we're doing learning here, and I hope I've motivated why I want you to look at these errors.",
            "So I need some sampling schemes an I can consider two types of sampling schemes.",
            "We're going to focus on one of 'em, and it's the first one is the one that we all know we have identical independent copies of XY, an of 'em, and we put P to be the probability of, say plus one and another set up.",
            "We could consider, which is sort of natural in this case is.",
            "Well, we have we.",
            "We take two samples, one being the ones with the plus ones labels.",
            "Let's say my normal observations and one being my labels minus ones which are same I anomalies.",
            "So in the first case the main difference then first case we don't necessarily have a deterministic number of each class and in the second one we have a deterministic class.",
            "The advantage of the second one is that clear we actually don't need the two samples to be mutually independent.",
            "But but really, they're treating the same way and we will focus on case one, even though in case he's actually even simpler.",
            "Just because the sample sizes are deterministic.",
            "So in this case I will denote by X12.",
            "Oh X minus is the one that receives superscript.",
            "Minus are the one that had labeled minus one, and the one that get the plus are the ones that get labeled plus one.",
            "And of course they have random number.",
            "The sum of N plus and minus is of."
        ],
        [
            "Percent.",
            "So well, Neyman Pearson is not a new thing and and even though it took awhile, people have already thought about introducing that problem to learning theory.",
            "As you can see Scott's been doing some work on that and I think the 1st paper that deals basically with exactly the set up I'm going to be talking about is this paper by Canon Co.",
            "Authors.",
            "And as you can see all four papers that I mentioned contain the name name in Pearson and so clearly that's what people are trying to do.",
            "But so why would I do it again?",
            "Those are.",
            "People that did actually this is pretty good work, but none of them really implements them in person.",
            "When I was taught leaving Pearson, oh, said this Alpha level is extremely important.",
            "You want to maintain this Alpha level and none of them actually strictly satisfies this Alpha level.",
            "They just allow themselves to just go away."
        ],
        [
            "Lil bit from it, so I'd see in more detail, but they actually doing well, they're saying so remembering that Pearson says I want to constrain the Type 1 error to be a below certain level and minimize the Type 2 error.",
            "We don't have access to this quantities.",
            "We have.",
            "Empirical counterparts are plus hat or minus hat and we want to minimize the type one and type 2 error under the constraint that the empirical type 1 error is bounded by Alpha plus a little something plus a little epsilon, not.",
            "Of course we have some complexity.",
            "Options, let's say VC dimension finite dimension on the class of classifiers we're looking at and and this is what the problem they've been looking at in the in the main case, I will call it relax, constraint."
        ],
        [
            "So this extra term epsilon, not just is here to ensure that the solution of this problem will have type 1 error.",
            "Bounded by Alpha with with high probability right?",
            "So I'm sorry, so that's not the true solution.",
            "I should not have written that.",
            "So really what it ensures is that the solution of the original problem of the true problem actually belongs to this constraint.",
            "That's what they want to have.",
            "They want to make sure that if you remove the hats, then the solution of this thing will satisfy the constraint without the hat and you could say, well, you know I can do that.",
            "I can say, OK, I won't.",
            "I don't mind.",
            "I have this little epsilon 0.",
            "It's going to do something a little worse on my type 1 error, but I'm just going to make sure that Alpha plus epsilon zero is my 5%.",
            "For example, I could do that, and that's probably the idea had had in mind and the question I'm going to be trying to answer is, well, what happened if you actually do that like you just say, OK, I'm going to.",
            "I'm going to say that this thinks my my 5%.",
            "I have a slightly different benchmark because I really want to compare myself to the classifier that has type 1 error bounded by Alpha 5% and not this thing.",
            "So let's see."
        ],
        [
            "How it behaves?",
            "So those are basically the comments I've just given to you so.",
            "Here, of course we will potentially have some classifiers that have type 1 error which is larger than Alpha and so that goes against the name in person.",
            "Parenting been talking about.",
            "So what we're going to do is strengthen this constraint instead of adding, epsilons were going to remove epsilon zero and this will have an impact because we will still compare ourselves to the Neyman Pearson without the hats.",
            "So we have a smaller constraint and we compare ourselves to the optimum over a bigger set."
        ],
        [
            "So.",
            "We could say, OK, let's just let's just do that on whatever problem we've been looking at this ad plus epsilon zero.",
            "Let's do minus epsilon zero and see what happens.",
            "Well, first of all we have.",
            "Nonsmooth nonconvex optimization so this is, you know, beautiful mathematics.",
            "Problem is, we cannot really do empirical risk minimization.",
            "Not that I'm really a practitioner and can claim, but it actually in our case, it is very important to make the problem convex, or at least smooth, so that we can control this deviations.",
            "If the problem is not smooth, we will actually have a mathematical proof that says there's no way you can actually have consistent guarantees on the Type 2 error.",
            "If you have those.",
            "This continuity, so instead we're going to be using the usual smoothing way in classification.",
            "We've heard it this morning.",
            "We're going to be using a convex surrogate for the classification errors, so there's two of 'em."
        ],
        [
            "So.",
            "In the in the rest of the talk I will be introducing this classification procedure based on convex optimization that strictly implements the Neyman Pearson parenting.",
            "I've been talking about.",
            "I will talk, show some theoretical insights and Oracle inequality is that demonstrate the performance of this estimator that help us understand what's going on with this problem and and a few consequences on chance constrained optimization.",
            "So if I have time, this is an interesting sort of robust type optimization problem that might be of interest to you.",
            "It's not like we revolution, we make evolution in the field, but it's it's actually pretty interest."
        ],
        [
            "You might be interested in seeing that.",
            "So first of all, let's convex.",
            "If I enter convexified, we need a comic circuit.",
            "It's a function that will map the image of a classifier into our plus.",
            "So we had so one_Non negative is basically the indicators.",
            "The step function that puts one on negative numbers.",
            "We're going to enforce this convex function to take value one at zero.",
            "We were going to make it increasing continuous and convex.",
            "OK, so it's defined so continuous important causes the finally on the on the compact set.",
            "Negative one to one, so 3 examples that you can use, for example, that satisfied this quantities and the usual way to do that is to say I'm going to replace the indicator that appears in my classification error by this convex surrogate.",
            "I'm also going to be looking at a convict class of classifier because convex objective is not enough to have a convex optimization problem and the convict class of classifiers I'm looking at is just the boosting type of idea.",
            "I'm given M based classifiers and I'm looking at convex combinations of those classifiers and if you like, you can think of him as being weighted majority votes among those classifiers.",
            "OK, so everything my entire class is indexed by this parameter Lambda in the flat simplex of RM, so just the vectors that are non negative in sum up to one."
        ],
        [
            "So now what is our benchmark?",
            "What is this?",
            "We're actually trying to mimic what it is we're what is or what I like to call Oracle, which is basically if you actually had access to an infinite amount of data.",
            "What it is you would actually be doing in this problem in this problem.",
            "So we have the minimisation over, well, the set of convex combinations under the constraint that the convexified type 1 error is bounded by Alpha, because that's what we're looking at now and we want to minimize the convexified type 2 error.",
            "So those are just that, the same way we convexified the risk, which is just the conditional expectations.",
            "So what we want to do is to find an estimator, a classifier H hat which is going to satisfy those two properties.",
            "First of all, we wanted to satisfy strictly the Type 1.",
            "The constraint on the Type 1 error.",
            "We want it to be in this constraint and not Alpha plus epsilon as it's been done.",
            "And of course this is going to happen with high probability.",
            "We're working with random quantities here and also we want the Type 2 error of this classifier to be almost as good as the best we could do if we actually know that new that constraint.",
            "And actually, if we knew not only that constraint, but also that what the function of the Type 2 error is."
        ],
        [
            "So to ensure that, especially to ensure the first part, we need to, we need to strengthen the constraint and the amount by which which strengthen.",
            "The constraint is actually random.",
            "It's the number of random number of negative observations that we have, and we have this parameter Tau that we need to calibrate.",
            "And of course I say equal to argument, but any any any classifier in the army will work and we will call this classifier NP classify."
        ],
        [
            "And this is the first theorem.",
            "The theorem that gives us the control of type 1 error.",
            "So there's nothing really surprising here.",
            "It says that if we calibrate towel to be of order root log M over some Delta, any fires Lipschitz so we can apply contraction.",
            "Then we have the following bound.",
            "So the first inequality is trivial.",
            "The first inequality comes from the fact that five dominates the indicator, the second one is what we're actually proving.",
            "It says that with high probability, so it's this one minus Delta that comes from control of the empirical process.",
            "And the second term comes from the fact that we have a random number of observation.",
            "And it's actually extremely small."
        ],
        [
            "So so very very close to one sorry so.",
            "That's the control of type 1 error, and there's really nothing very interesting here is simple exercise and to control the Type 2 error.",
            "That's when we actually start to need to make a little more assumption and the assumptions are not that strong.",
            "The assumptions are saying that within this class of convex combinations of classifiers we can find one which will satisfy this.",
            "A slightly stronger constraint on the Type 1 error.",
            "So if we shrink a little bit by excellent and we only need epsilon to be strictly smaller than one if we if we can find such a classifier.",
            "Meaning that strengthen constraint makes sense.",
            "Then we will have the result we're looking at.",
            "So we need end to be large enough so we can see where epsilon matters.",
            "Epsilon cannot go to one, smaller Alpha will require larger sample size of course, and the proportion matters as well.",
            "And so Tao is tell this root log M that we had before an."
        ],
        [
            "Here's the result we have, so we have it with probability close to one for N sufficiently large that first of all the constraint on the Type 1 error is satisfied.",
            "Strictly with this Alpha and 2nd, the deficit of type 2 error that we have.",
            "So what we remove this minimum is our benchmark and the deficit of type 2 error we have is of order well root log M divided by root N minus plus RT log divided by root N plus.",
            "So we can see that the two sample sizes matter and if we were in the second sampling scheme we would have those deterministic.",
            "Sizes but in our case we can also apply an extra concentration for binomial tails and price being just extra terms in the probability and we can replace those N plus an minus by NPN 1 -- P respectively."
        ],
        [
            "So there's really.",
            "This is really what we expected, but we can see that what matters is really fight.",
            "Did matter if I think what's most interesting is in the proof and you see really what makes it work and a little bit about the proof basically tells us that well, type 1 error was trivial and the second one was basically trying to find the sensitivity of a convex.",
            "The optimal value of a convex optimization problem when you shrink or uh, or expand the.",
            "Constant.",
            "What's surprising is that only the convexity of Phi matters, not not delay."
        ],
        [
            "It's constant look, the Lipschitz constant is not in there."
        ],
        [
            "So that's actually."
        ],
        [
            "Pretty interesting and so just this is a visual idea.",
            "The orange, orange... original constraints, the one we wanted to have and we made it a little smaller to ensure that we would satisfy it."
        ],
        [
            "And for each of those sets we have, this is the optimal limit person classifier.",
            "When you actually have the population case, say and this is the same."
        ],
        [
            "Case we have a convex function and we want you know basically where how are function changes from 1."
        ],
        [
            "Point to the.",
            "So.",
            "The.",
            "The proposition on the on on which it hinges is that if you shrink a little bit by Alpha by new.",
            "OK, so I remove, knew to Alpha, so those are two same terms, except that the first one is just removing you.",
            "And that's really what matters to us then basically what the change of the objective is linear in new, which is which is somewhat surprising, but not that much.",
            "And I thought it would be more difficult than this hinges on the fact that this function that was introduced by Lehman when he was studying.",
            "Uniformly, uniformly, most powerful tests is actually so the optimum value of the convex problem is actually a convex function on the."
        ],
        [
            "One, so I think.",
            "Yeah, so I have 20 seconds so I'm not going to talk about chance constrained optimization.",
            "I'm sorry.",
            "OK so I'm just going to run through this slide quickly.",
            "This is an optimization problem.",
            "We want to minimize an objective subject to acons."
        ],
        [
            "Paint and chance constrained optimization is basically saying, well, my constraint might depend on a random verbal and I don't want it to be satisfied for all possible values of the random verbal like robust optimization.",
            "Would I just want it to be satisfied with high probability.",
            "So even though you started with the convex optimization problem, this clearly is not convex.",
            "I mean not in general there's examples where it is but not not in general and we can apply the same relaxation to replace this probability by an expectation of a convex surrogate and recast this problem exactly in the same case as we've."
        ],
        [
            "Our problem before, so this idea comes from Nemerofsky and Shapiro and basically we're saying that if we minimize our objective function, which now is deterministic, we now have no longer problem.",
            "Any problem with that under a slightly stronger constraint on on the Type 1 error on the Type 1 error?",
            "Well, there's not type 1 error here.",
            "Then we can basically control not only the fact that our solution satisfies the change constraint with high probability, but also that the deficit of well.",
            "This is exactly the quantity controlling optimization is actually is actually bounded by some order Tau over Route 10, which is, which is what we found before, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's go back to the slide that's been shown Hunter about 1000 times it cold, so we're looking at binary classification.",
                    "label": 0
                },
                {
                    "sent": "We have a feature X in Rd labels or minus one one and we want to construct a classifier that Maps a feature vector X in 2 -- 1 one and we know that the standard way to measure the performance of a given classifier is to look at the classification error, which is simply the probability it makes an error on a given yet innocent instance.",
                    "label": 0
                },
                {
                    "sent": "And actually this is a combination of two types of error you can make.",
                    "label": 0
                },
                {
                    "sent": "We know from introductory statistics that there's two types of error.",
                    "label": 0
                },
                {
                    "sent": "The first one is to say, plus one.",
                    "label": 0
                },
                {
                    "sent": "When it's minus one or two, say minus one.",
                    "label": 0
                },
                {
                    "sent": "When it's plus one an, we're going to try in this talk to deal with them separately.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So of course we want to control them simultaneously in some way, But the question is how do we combine them?",
                    "label": 0
                },
                {
                    "sent": "And there's not just one way to do it.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about classification errors that it just does it for you.",
                    "label": 0
                },
                {
                    "sent": "It's just the convex combination of those two types of errors that I've just shown.",
                    "label": 0
                },
                {
                    "sent": "So remember, our minus is the is the Type 1 error.",
                    "label": 0
                },
                {
                    "sent": "An R Plus is the Type 2 error and is just a weighted combinations.",
                    "label": 0
                },
                {
                    "sent": "The weight being given by the proportion of each of the classes.",
                    "label": 0
                },
                {
                    "sent": "So minimizing that gives you one way to minimize the two simultaneously an another way to do is if your have more statistical background, you know that Norman Pearson have tried to answer this problem.",
                    "label": 0
                },
                {
                    "sent": "Trying to combine these two errors that typically cannot be minimized simultaneously, so Neyman Pearson is very systematic.",
                    "label": 0
                },
                {
                    "sent": "It's a paradigm, it's a way of thinking of how to minimize those two errors, and the way we do it is, we say, let's force the Type 1 error to be below a given level.",
                    "label": 0
                },
                {
                    "sent": "Alpha which is user specified, typically 1% or 5%, and once we've done that, we're going to cross our fingers and try to minimize the Type 2 error as much as we can, and so of course, here.",
                    "label": 0
                },
                {
                    "sent": "Obviously we put more emphasis on the Type 1 error, so hence symmetric between the two types of error, and because we really are enforcing some level on the Type 1 error in the Type 2 error.",
                    "label": 0
                },
                {
                    "sent": "We just hope for the best.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why would we want to do that where we want to introduce this symmetric and right classification error is very natural, it's simple and it combines them in a natural way, but we'll see that in some instances it might not be adapted to their classification task at hand.",
                    "label": 0
                },
                {
                    "sent": "And we've seen yesterday that depending on the context you want to use different ways of doing it, so let's look at examples from this bestseller.",
                    "label": 0
                },
                {
                    "sent": "By the way, I showed this book last year an I don't perceive any royalty on this book, unfortunately.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, I would also be gaining some prize money.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this book you can just read this is verbatim from this book.",
                    "label": 0
                },
                {
                    "sent": "So you have a problem where you're trying to predict for patient whether it's going to have a second heart attack and this should be based on on some features.",
                    "label": 0
                },
                {
                    "sent": "You measure demographic clinical.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So of course here type 1 error is the one you want to strictly contain and is the probability of predicting no second heart attack, whereas the patient is going to have one.",
                    "label": 0
                },
                {
                    "sent": "So that's typical example.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We give another one that I found in this book is funny in recognition, so you have versus Al based on time series of those.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Observation, and here what is the type 1 error?",
                    "label": 0
                },
                {
                    "sent": "Well, who cares?",
                    "label": 0
                },
                {
                    "sent": "I mean it's not like one is more important than the other.",
                    "label": 0
                },
                {
                    "sent": "So in this case it's not always the case that you want to privilege one of the errors.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And another one is use systolic blood pressure to predict some heart disease.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here, of course, the Type 1 error, the one we want to maintain below a certain level, is to predict no coronary heart disease, whereas patient is going to have a corner hard to.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These and finally the example I found in this book was pretty.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spams and here actually type 1 error depends on what the user wants, whether the user, whether you're annoyed to go in your junk folder and pull out spams or receive spam in your inbox.",
                    "label": 0
                },
                {
                    "sent": "That really depends on you and you're not really ask, but maybe you should so.",
                    "label": 0
                },
                {
                    "sent": "In some instances we can see that it might be meaningful to put more emphasis on the Type 1 error.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another case I can make for that is in a very specific case where one of the classes is under represented.",
                    "label": 0
                },
                {
                    "sent": "One of the classes when P is extremely small.",
                    "label": 0
                },
                {
                    "sent": "When you're going trying to minimize the classification error, you see this weighted combination of the two errors.",
                    "label": 0
                },
                {
                    "sent": "And step away from that.",
                    "label": 0
                },
                {
                    "sent": "So if you if you if this one, for example, extremely small what your classification rule, the one that minimizes this risk, what it's going to be doing is is that it will just not care about the Type 1 error, will try to minimize this one because it gets really high weight, and that's typically the case in animality tection where P is for example 5% maybe 1%.",
                    "label": 0
                },
                {
                    "sent": "You have very small number in one class, so in this case you might actually run into the problem that what your classification rule is going to do is say well.",
                    "label": 0
                },
                {
                    "sent": "Everything is not an anomaly because well in this case I'm not going to make that much error anyway, since it just does.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It happens so much.",
                    "label": 0
                },
                {
                    "sent": "So to do that, of course we're doing learning here, and I hope I've motivated why I want you to look at these errors.",
                    "label": 0
                },
                {
                    "sent": "So I need some sampling schemes an I can consider two types of sampling schemes.",
                    "label": 0
                },
                {
                    "sent": "We're going to focus on one of 'em, and it's the first one is the one that we all know we have identical independent copies of XY, an of 'em, and we put P to be the probability of, say plus one and another set up.",
                    "label": 0
                },
                {
                    "sent": "We could consider, which is sort of natural in this case is.",
                    "label": 0
                },
                {
                    "sent": "Well, we have we.",
                    "label": 0
                },
                {
                    "sent": "We take two samples, one being the ones with the plus ones labels.",
                    "label": 0
                },
                {
                    "sent": "Let's say my normal observations and one being my labels minus ones which are same I anomalies.",
                    "label": 0
                },
                {
                    "sent": "So in the first case the main difference then first case we don't necessarily have a deterministic number of each class and in the second one we have a deterministic class.",
                    "label": 0
                },
                {
                    "sent": "The advantage of the second one is that clear we actually don't need the two samples to be mutually independent.",
                    "label": 0
                },
                {
                    "sent": "But but really, they're treating the same way and we will focus on case one, even though in case he's actually even simpler.",
                    "label": 0
                },
                {
                    "sent": "Just because the sample sizes are deterministic.",
                    "label": 0
                },
                {
                    "sent": "So in this case I will denote by X12.",
                    "label": 0
                },
                {
                    "sent": "Oh X minus is the one that receives superscript.",
                    "label": 0
                },
                {
                    "sent": "Minus are the one that had labeled minus one, and the one that get the plus are the ones that get labeled plus one.",
                    "label": 0
                },
                {
                    "sent": "And of course they have random number.",
                    "label": 0
                },
                {
                    "sent": "The sum of N plus and minus is of.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Percent.",
                    "label": 0
                },
                {
                    "sent": "So well, Neyman Pearson is not a new thing and and even though it took awhile, people have already thought about introducing that problem to learning theory.",
                    "label": 0
                },
                {
                    "sent": "As you can see Scott's been doing some work on that and I think the 1st paper that deals basically with exactly the set up I'm going to be talking about is this paper by Canon Co.",
                    "label": 0
                },
                {
                    "sent": "Authors.",
                    "label": 0
                },
                {
                    "sent": "And as you can see all four papers that I mentioned contain the name name in Pearson and so clearly that's what people are trying to do.",
                    "label": 0
                },
                {
                    "sent": "But so why would I do it again?",
                    "label": 0
                },
                {
                    "sent": "Those are.",
                    "label": 0
                },
                {
                    "sent": "People that did actually this is pretty good work, but none of them really implements them in person.",
                    "label": 0
                },
                {
                    "sent": "When I was taught leaving Pearson, oh, said this Alpha level is extremely important.",
                    "label": 0
                },
                {
                    "sent": "You want to maintain this Alpha level and none of them actually strictly satisfies this Alpha level.",
                    "label": 0
                },
                {
                    "sent": "They just allow themselves to just go away.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lil bit from it, so I'd see in more detail, but they actually doing well, they're saying so remembering that Pearson says I want to constrain the Type 1 error to be a below certain level and minimize the Type 2 error.",
                    "label": 0
                },
                {
                    "sent": "We don't have access to this quantities.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "Empirical counterparts are plus hat or minus hat and we want to minimize the type one and type 2 error under the constraint that the empirical type 1 error is bounded by Alpha plus a little something plus a little epsilon, not.",
                    "label": 0
                },
                {
                    "sent": "Of course we have some complexity.",
                    "label": 0
                },
                {
                    "sent": "Options, let's say VC dimension finite dimension on the class of classifiers we're looking at and and this is what the problem they've been looking at in the in the main case, I will call it relax, constraint.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this extra term epsilon, not just is here to ensure that the solution of this problem will have type 1 error.",
                    "label": 0
                },
                {
                    "sent": "Bounded by Alpha with with high probability right?",
                    "label": 0
                },
                {
                    "sent": "So I'm sorry, so that's not the true solution.",
                    "label": 0
                },
                {
                    "sent": "I should not have written that.",
                    "label": 0
                },
                {
                    "sent": "So really what it ensures is that the solution of the original problem of the true problem actually belongs to this constraint.",
                    "label": 0
                },
                {
                    "sent": "That's what they want to have.",
                    "label": 0
                },
                {
                    "sent": "They want to make sure that if you remove the hats, then the solution of this thing will satisfy the constraint without the hat and you could say, well, you know I can do that.",
                    "label": 0
                },
                {
                    "sent": "I can say, OK, I won't.",
                    "label": 0
                },
                {
                    "sent": "I don't mind.",
                    "label": 0
                },
                {
                    "sent": "I have this little epsilon 0.",
                    "label": 0
                },
                {
                    "sent": "It's going to do something a little worse on my type 1 error, but I'm just going to make sure that Alpha plus epsilon zero is my 5%.",
                    "label": 0
                },
                {
                    "sent": "For example, I could do that, and that's probably the idea had had in mind and the question I'm going to be trying to answer is, well, what happened if you actually do that like you just say, OK, I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say that this thinks my my 5%.",
                    "label": 0
                },
                {
                    "sent": "I have a slightly different benchmark because I really want to compare myself to the classifier that has type 1 error bounded by Alpha 5% and not this thing.",
                    "label": 0
                },
                {
                    "sent": "So let's see.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How it behaves?",
                    "label": 0
                },
                {
                    "sent": "So those are basically the comments I've just given to you so.",
                    "label": 0
                },
                {
                    "sent": "Here, of course we will potentially have some classifiers that have type 1 error which is larger than Alpha and so that goes against the name in person.",
                    "label": 0
                },
                {
                    "sent": "Parenting been talking about.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is strengthen this constraint instead of adding, epsilons were going to remove epsilon zero and this will have an impact because we will still compare ourselves to the Neyman Pearson without the hats.",
                    "label": 0
                },
                {
                    "sent": "So we have a smaller constraint and we compare ourselves to the optimum over a bigger set.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We could say, OK, let's just let's just do that on whatever problem we've been looking at this ad plus epsilon zero.",
                    "label": 0
                },
                {
                    "sent": "Let's do minus epsilon zero and see what happens.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all we have.",
                    "label": 0
                },
                {
                    "sent": "Nonsmooth nonconvex optimization so this is, you know, beautiful mathematics.",
                    "label": 0
                },
                {
                    "sent": "Problem is, we cannot really do empirical risk minimization.",
                    "label": 0
                },
                {
                    "sent": "Not that I'm really a practitioner and can claim, but it actually in our case, it is very important to make the problem convex, or at least smooth, so that we can control this deviations.",
                    "label": 0
                },
                {
                    "sent": "If the problem is not smooth, we will actually have a mathematical proof that says there's no way you can actually have consistent guarantees on the Type 2 error.",
                    "label": 0
                },
                {
                    "sent": "If you have those.",
                    "label": 0
                },
                {
                    "sent": "This continuity, so instead we're going to be using the usual smoothing way in classification.",
                    "label": 0
                },
                {
                    "sent": "We've heard it this morning.",
                    "label": 0
                },
                {
                    "sent": "We're going to be using a convex surrogate for the classification errors, so there's two of 'em.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In the in the rest of the talk I will be introducing this classification procedure based on convex optimization that strictly implements the Neyman Pearson parenting.",
                    "label": 0
                },
                {
                    "sent": "I've been talking about.",
                    "label": 0
                },
                {
                    "sent": "I will talk, show some theoretical insights and Oracle inequality is that demonstrate the performance of this estimator that help us understand what's going on with this problem and and a few consequences on chance constrained optimization.",
                    "label": 0
                },
                {
                    "sent": "So if I have time, this is an interesting sort of robust type optimization problem that might be of interest to you.",
                    "label": 0
                },
                {
                    "sent": "It's not like we revolution, we make evolution in the field, but it's it's actually pretty interest.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You might be interested in seeing that.",
                    "label": 0
                },
                {
                    "sent": "So first of all, let's convex.",
                    "label": 0
                },
                {
                    "sent": "If I enter convexified, we need a comic circuit.",
                    "label": 0
                },
                {
                    "sent": "It's a function that will map the image of a classifier into our plus.",
                    "label": 0
                },
                {
                    "sent": "So we had so one_Non negative is basically the indicators.",
                    "label": 0
                },
                {
                    "sent": "The step function that puts one on negative numbers.",
                    "label": 0
                },
                {
                    "sent": "We're going to enforce this convex function to take value one at zero.",
                    "label": 0
                },
                {
                    "sent": "We were going to make it increasing continuous and convex.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's defined so continuous important causes the finally on the on the compact set.",
                    "label": 0
                },
                {
                    "sent": "Negative one to one, so 3 examples that you can use, for example, that satisfied this quantities and the usual way to do that is to say I'm going to replace the indicator that appears in my classification error by this convex surrogate.",
                    "label": 0
                },
                {
                    "sent": "I'm also going to be looking at a convict class of classifier because convex objective is not enough to have a convex optimization problem and the convict class of classifiers I'm looking at is just the boosting type of idea.",
                    "label": 0
                },
                {
                    "sent": "I'm given M based classifiers and I'm looking at convex combinations of those classifiers and if you like, you can think of him as being weighted majority votes among those classifiers.",
                    "label": 0
                },
                {
                    "sent": "OK, so everything my entire class is indexed by this parameter Lambda in the flat simplex of RM, so just the vectors that are non negative in sum up to one.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now what is our benchmark?",
                    "label": 0
                },
                {
                    "sent": "What is this?",
                    "label": 0
                },
                {
                    "sent": "We're actually trying to mimic what it is we're what is or what I like to call Oracle, which is basically if you actually had access to an infinite amount of data.",
                    "label": 0
                },
                {
                    "sent": "What it is you would actually be doing in this problem in this problem.",
                    "label": 0
                },
                {
                    "sent": "So we have the minimisation over, well, the set of convex combinations under the constraint that the convexified type 1 error is bounded by Alpha, because that's what we're looking at now and we want to minimize the convexified type 2 error.",
                    "label": 0
                },
                {
                    "sent": "So those are just that, the same way we convexified the risk, which is just the conditional expectations.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is to find an estimator, a classifier H hat which is going to satisfy those two properties.",
                    "label": 0
                },
                {
                    "sent": "First of all, we wanted to satisfy strictly the Type 1.",
                    "label": 0
                },
                {
                    "sent": "The constraint on the Type 1 error.",
                    "label": 0
                },
                {
                    "sent": "We want it to be in this constraint and not Alpha plus epsilon as it's been done.",
                    "label": 0
                },
                {
                    "sent": "And of course this is going to happen with high probability.",
                    "label": 0
                },
                {
                    "sent": "We're working with random quantities here and also we want the Type 2 error of this classifier to be almost as good as the best we could do if we actually know that new that constraint.",
                    "label": 0
                },
                {
                    "sent": "And actually, if we knew not only that constraint, but also that what the function of the Type 2 error is.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to ensure that, especially to ensure the first part, we need to, we need to strengthen the constraint and the amount by which which strengthen.",
                    "label": 0
                },
                {
                    "sent": "The constraint is actually random.",
                    "label": 0
                },
                {
                    "sent": "It's the number of random number of negative observations that we have, and we have this parameter Tau that we need to calibrate.",
                    "label": 0
                },
                {
                    "sent": "And of course I say equal to argument, but any any any classifier in the army will work and we will call this classifier NP classify.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the first theorem.",
                    "label": 0
                },
                {
                    "sent": "The theorem that gives us the control of type 1 error.",
                    "label": 0
                },
                {
                    "sent": "So there's nothing really surprising here.",
                    "label": 0
                },
                {
                    "sent": "It says that if we calibrate towel to be of order root log M over some Delta, any fires Lipschitz so we can apply contraction.",
                    "label": 0
                },
                {
                    "sent": "Then we have the following bound.",
                    "label": 0
                },
                {
                    "sent": "So the first inequality is trivial.",
                    "label": 0
                },
                {
                    "sent": "The first inequality comes from the fact that five dominates the indicator, the second one is what we're actually proving.",
                    "label": 0
                },
                {
                    "sent": "It says that with high probability, so it's this one minus Delta that comes from control of the empirical process.",
                    "label": 0
                },
                {
                    "sent": "And the second term comes from the fact that we have a random number of observation.",
                    "label": 0
                },
                {
                    "sent": "And it's actually extremely small.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so very very close to one sorry so.",
                    "label": 0
                },
                {
                    "sent": "That's the control of type 1 error, and there's really nothing very interesting here is simple exercise and to control the Type 2 error.",
                    "label": 0
                },
                {
                    "sent": "That's when we actually start to need to make a little more assumption and the assumptions are not that strong.",
                    "label": 0
                },
                {
                    "sent": "The assumptions are saying that within this class of convex combinations of classifiers we can find one which will satisfy this.",
                    "label": 0
                },
                {
                    "sent": "A slightly stronger constraint on the Type 1 error.",
                    "label": 0
                },
                {
                    "sent": "So if we shrink a little bit by excellent and we only need epsilon to be strictly smaller than one if we if we can find such a classifier.",
                    "label": 0
                },
                {
                    "sent": "Meaning that strengthen constraint makes sense.",
                    "label": 0
                },
                {
                    "sent": "Then we will have the result we're looking at.",
                    "label": 0
                },
                {
                    "sent": "So we need end to be large enough so we can see where epsilon matters.",
                    "label": 0
                },
                {
                    "sent": "Epsilon cannot go to one, smaller Alpha will require larger sample size of course, and the proportion matters as well.",
                    "label": 0
                },
                {
                    "sent": "And so Tao is tell this root log M that we had before an.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's the result we have, so we have it with probability close to one for N sufficiently large that first of all the constraint on the Type 1 error is satisfied.",
                    "label": 0
                },
                {
                    "sent": "Strictly with this Alpha and 2nd, the deficit of type 2 error that we have.",
                    "label": 0
                },
                {
                    "sent": "So what we remove this minimum is our benchmark and the deficit of type 2 error we have is of order well root log M divided by root N minus plus RT log divided by root N plus.",
                    "label": 0
                },
                {
                    "sent": "So we can see that the two sample sizes matter and if we were in the second sampling scheme we would have those deterministic.",
                    "label": 0
                },
                {
                    "sent": "Sizes but in our case we can also apply an extra concentration for binomial tails and price being just extra terms in the probability and we can replace those N plus an minus by NPN 1 -- P respectively.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's really.",
                    "label": 0
                },
                {
                    "sent": "This is really what we expected, but we can see that what matters is really fight.",
                    "label": 0
                },
                {
                    "sent": "Did matter if I think what's most interesting is in the proof and you see really what makes it work and a little bit about the proof basically tells us that well, type 1 error was trivial and the second one was basically trying to find the sensitivity of a convex.",
                    "label": 0
                },
                {
                    "sent": "The optimal value of a convex optimization problem when you shrink or uh, or expand the.",
                    "label": 0
                },
                {
                    "sent": "Constant.",
                    "label": 0
                },
                {
                    "sent": "What's surprising is that only the convexity of Phi matters, not not delay.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's constant look, the Lipschitz constant is not in there.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's actually.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty interesting and so just this is a visual idea.",
                    "label": 0
                },
                {
                    "sent": "The orange, orange... original constraints, the one we wanted to have and we made it a little smaller to ensure that we would satisfy it.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for each of those sets we have, this is the optimal limit person classifier.",
                    "label": 0
                },
                {
                    "sent": "When you actually have the population case, say and this is the same.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case we have a convex function and we want you know basically where how are function changes from 1.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Point to the.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The proposition on the on on which it hinges is that if you shrink a little bit by Alpha by new.",
                    "label": 0
                },
                {
                    "sent": "OK, so I remove, knew to Alpha, so those are two same terms, except that the first one is just removing you.",
                    "label": 0
                },
                {
                    "sent": "And that's really what matters to us then basically what the change of the objective is linear in new, which is which is somewhat surprising, but not that much.",
                    "label": 0
                },
                {
                    "sent": "And I thought it would be more difficult than this hinges on the fact that this function that was introduced by Lehman when he was studying.",
                    "label": 0
                },
                {
                    "sent": "Uniformly, uniformly, most powerful tests is actually so the optimum value of the convex problem is actually a convex function on the.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One, so I think.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I have 20 seconds so I'm not going to talk about chance constrained optimization.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "OK so I'm just going to run through this slide quickly.",
                    "label": 0
                },
                {
                    "sent": "This is an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "We want to minimize an objective subject to acons.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Paint and chance constrained optimization is basically saying, well, my constraint might depend on a random verbal and I don't want it to be satisfied for all possible values of the random verbal like robust optimization.",
                    "label": 1
                },
                {
                    "sent": "Would I just want it to be satisfied with high probability.",
                    "label": 0
                },
                {
                    "sent": "So even though you started with the convex optimization problem, this clearly is not convex.",
                    "label": 0
                },
                {
                    "sent": "I mean not in general there's examples where it is but not not in general and we can apply the same relaxation to replace this probability by an expectation of a convex surrogate and recast this problem exactly in the same case as we've.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our problem before, so this idea comes from Nemerofsky and Shapiro and basically we're saying that if we minimize our objective function, which now is deterministic, we now have no longer problem.",
                    "label": 0
                },
                {
                    "sent": "Any problem with that under a slightly stronger constraint on on the Type 1 error on the Type 1 error?",
                    "label": 0
                },
                {
                    "sent": "Well, there's not type 1 error here.",
                    "label": 0
                },
                {
                    "sent": "Then we can basically control not only the fact that our solution satisfies the change constraint with high probability, but also that the deficit of well.",
                    "label": 0
                },
                {
                    "sent": "This is exactly the quantity controlling optimization is actually is actually bounded by some order Tau over Route 10, which is, which is what we found before, thank you.",
                    "label": 0
                }
            ]
        }
    }
}