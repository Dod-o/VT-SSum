{
    "id": "pvio6xcpkxoqlhy6o46ojbld2bffupk5",
    "title": "Structured Determinantal Point Processes",
    "info": {
        "author": [
            "Alex Kulesza, School of Engineering and Applied Science, University of Pennsylvania"
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips2010_kulesza_sdp/",
    "segmentation": [
        [
            "So imagine that you would like to model the positions of multiple particle."
        ],
        [
            "In a plane and you have some sensor information which I've plotted here in kind of purple so the dark areas are the places where your sensors think particles are likely to be an the light areas, or where it thinks it's not likely to be.",
            "And one way you could use this information as to say, well, I'm going to assume that the position of each particle is just drawn independently from that purple distribution, and that's going to be something like what you see on the left, and I've marked independent, so that's fine.",
            "And that might be good for your task, but in some cases you might believe that particles tend not to occupy the same positions.",
            "Maybe they're repulsive, or you have some other intuition.",
            "And in that case you might prefer a distribution that prefer samples like things on the right.",
            "So of course, machine learning whenever we need a model for something, we read physics literature and it turns out that there is a model called the determinantal point process, or DPP, which essentially achieves this kind of thing and very briefly, it's parameterized by a kernel K, and the semantics or the IG entry of the kernel K is telling you the similarity between location ion location J.",
            "So in this case, we would say the locations are similar if they're nearby, or that they're dissimilar if they're far apart.",
            "And then you basically augment the independent distribution with this determinant term.",
            "You take the determinant of case of lie, which is just the kernel restricted to this proposed set of positions.",
            "Why?",
            "And you can show that this has the desired effect.",
            "Essentially sets why consisting of similar points are discounted.",
            "An sets that have diverse points get increased probability, so we'd like to call this the prison cafeteria process because nobody wants to sit next to each other.",
            "The really."
        ],
        [
            "Thing about DP is is that there are mathematically very pretty, and even though you're modeling a distribution over an exponential number of sets Y, you can still efficiently compute marginal probabilities.",
            "Conditional probabilities.",
            "You can efficiently normalize.",
            "You can even efficiently sample.",
            "So all this is great, but what we'd really like to do is take this further where we're actually modeling distributions.",
            "None of our sets of simple points, but sets of objects that themselves have internal structure.",
            "So, for example, you might want to take a sentence and produce a set of diverse parses for that sentence.",
            "So maybe you want your N best list to have more variety.",
            "Or maybe you have an image with multiple people and you'd like to predict the locations of people in the image.",
            "But you have this assumption that people tend not to be in the same place at the same time.",
            "So there's this notion of diversity there as well.",
            "And it turns out and we show this in our work that you can still actually do many things efficiently, even though we're now modeling a distribution over a doubly exponential number of sets.",
            "So we can do sampling.",
            "We can do marginal probabilities and so on, and I'm not going to go into details here.",
            "But if you'd like to know how we combine sort of graphical models, kind of a reverse kernel trick and 2nd order message passing, you have to come to the poster and."
        ],
        [
            "Meantime, I just wanted to give you some intuition about the sampling algorithm.",
            "So what I have in the upper left is a still frame from a TV show that has three people in it and then in step one you can see I've plotted basically our sensor readings.",
            "So these are our probabilities given by a previously trained pose detector where heads are in yellow, torsos are in red and arms are in green, and you can see that two of the people are fairly well identified by kind of blurry mass, but the third person is essentially not so the sampling algorithm seeds by first choosing a person according to this distribution is in Step 2.",
            "And then it updates the distribution.",
            "Basically discounting positions that are near the one you already selected.",
            "Then in step three you can see it selected the second person and now it updates again and you can see that the third person who is kind of previously hidden and low probability has become more salient, because of course we're incorporating the sense of diversity and then in section in Step 4 you can see that we've identified all the people.",
            "So please stop by the poster if you'd like to learn more.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So imagine that you would like to model the positions of multiple particle.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a plane and you have some sensor information which I've plotted here in kind of purple so the dark areas are the places where your sensors think particles are likely to be an the light areas, or where it thinks it's not likely to be.",
                    "label": 0
                },
                {
                    "sent": "And one way you could use this information as to say, well, I'm going to assume that the position of each particle is just drawn independently from that purple distribution, and that's going to be something like what you see on the left, and I've marked independent, so that's fine.",
                    "label": 0
                },
                {
                    "sent": "And that might be good for your task, but in some cases you might believe that particles tend not to occupy the same positions.",
                    "label": 0
                },
                {
                    "sent": "Maybe they're repulsive, or you have some other intuition.",
                    "label": 0
                },
                {
                    "sent": "And in that case you might prefer a distribution that prefer samples like things on the right.",
                    "label": 0
                },
                {
                    "sent": "So of course, machine learning whenever we need a model for something, we read physics literature and it turns out that there is a model called the determinantal point process, or DPP, which essentially achieves this kind of thing and very briefly, it's parameterized by a kernel K, and the semantics or the IG entry of the kernel K is telling you the similarity between location ion location J.",
                    "label": 0
                },
                {
                    "sent": "So in this case, we would say the locations are similar if they're nearby, or that they're dissimilar if they're far apart.",
                    "label": 0
                },
                {
                    "sent": "And then you basically augment the independent distribution with this determinant term.",
                    "label": 0
                },
                {
                    "sent": "You take the determinant of case of lie, which is just the kernel restricted to this proposed set of positions.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "And you can show that this has the desired effect.",
                    "label": 0
                },
                {
                    "sent": "Essentially sets why consisting of similar points are discounted.",
                    "label": 0
                },
                {
                    "sent": "An sets that have diverse points get increased probability, so we'd like to call this the prison cafeteria process because nobody wants to sit next to each other.",
                    "label": 0
                },
                {
                    "sent": "The really.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thing about DP is is that there are mathematically very pretty, and even though you're modeling a distribution over an exponential number of sets Y, you can still efficiently compute marginal probabilities.",
                    "label": 0
                },
                {
                    "sent": "Conditional probabilities.",
                    "label": 0
                },
                {
                    "sent": "You can efficiently normalize.",
                    "label": 0
                },
                {
                    "sent": "You can even efficiently sample.",
                    "label": 0
                },
                {
                    "sent": "So all this is great, but what we'd really like to do is take this further where we're actually modeling distributions.",
                    "label": 0
                },
                {
                    "sent": "None of our sets of simple points, but sets of objects that themselves have internal structure.",
                    "label": 1
                },
                {
                    "sent": "So, for example, you might want to take a sentence and produce a set of diverse parses for that sentence.",
                    "label": 0
                },
                {
                    "sent": "So maybe you want your N best list to have more variety.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you have an image with multiple people and you'd like to predict the locations of people in the image.",
                    "label": 0
                },
                {
                    "sent": "But you have this assumption that people tend not to be in the same place at the same time.",
                    "label": 0
                },
                {
                    "sent": "So there's this notion of diversity there as well.",
                    "label": 0
                },
                {
                    "sent": "And it turns out and we show this in our work that you can still actually do many things efficiently, even though we're now modeling a distribution over a doubly exponential number of sets.",
                    "label": 0
                },
                {
                    "sent": "So we can do sampling.",
                    "label": 0
                },
                {
                    "sent": "We can do marginal probabilities and so on, and I'm not going to go into details here.",
                    "label": 0
                },
                {
                    "sent": "But if you'd like to know how we combine sort of graphical models, kind of a reverse kernel trick and 2nd order message passing, you have to come to the poster and.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Meantime, I just wanted to give you some intuition about the sampling algorithm.",
                    "label": 1
                },
                {
                    "sent": "So what I have in the upper left is a still frame from a TV show that has three people in it and then in step one you can see I've plotted basically our sensor readings.",
                    "label": 0
                },
                {
                    "sent": "So these are our probabilities given by a previously trained pose detector where heads are in yellow, torsos are in red and arms are in green, and you can see that two of the people are fairly well identified by kind of blurry mass, but the third person is essentially not so the sampling algorithm seeds by first choosing a person according to this distribution is in Step 2.",
                    "label": 0
                },
                {
                    "sent": "And then it updates the distribution.",
                    "label": 0
                },
                {
                    "sent": "Basically discounting positions that are near the one you already selected.",
                    "label": 0
                },
                {
                    "sent": "Then in step three you can see it selected the second person and now it updates again and you can see that the third person who is kind of previously hidden and low probability has become more salient, because of course we're incorporating the sense of diversity and then in section in Step 4 you can see that we've identified all the people.",
                    "label": 0
                },
                {
                    "sent": "So please stop by the poster if you'd like to learn more.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}