{
    "id": "lsbo2zczm4xwtaqv7kruq66yousl4ocx",
    "title": "Constrained Logistic Regression for Discriminative Pattern Mining",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Samir Al-Stouhi, Department of Computer Science, Wayne State University"
        ],
        "published": "Nov. 29, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Pattern Recognition",
            "Top->Computer Science->Data Mining->Frequent Patterns",
            "Top->Computer Science->Machine Learning->Regression"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_al_stouhi_mining/",
    "segmentation": [
        [
            "My name is Amira.",
            "How do you present this paper on the back of my colleagues and Chandan already from one state and this paper is entitled constraint but regression or square discriminative parents?"
        ],
        [
            "So this is an overview of the paper production per menarys framework.",
            "So."
        ],
        [
            "The introduction.",
            "The purpose of this paper is to look at different subgroups with labels.",
            "The idea is we have multivariate data and you have a problem, especially when this data has labels.",
            "You can just look at the data itself cause that the distribution might be different, but there's also the labels part of the part of the problem.",
            "So what we do is we we look at this problem in the context of the supervised training, where we using the data and the labels.",
            "In our analysis.",
            "So what would we be trying to do is we're trying to highlight the differences and do something different.",
            "Subgroups of this multivariate data and at the same time we're trying to classify it by maintaining a class discrimination."
        ],
        [
            "OK. What are overtime for example?",
            "Cancer rates.",
            "You have one table application.",
            "Americans with African Americans, but you can.",
            "You can solve this problem when we try to look at the data from different states.",
            "For example, Southern states have more exposure to the sun versus normal state, so the data itself will have different distributions for different areas and the label space will have also have different distributions for different areas so.",
            "This is an example where data is.",
            "It is not.",
            "It is not independent so.",
            "Another example is when looking for a generous communication companies, we have the same problem here because again different industries have different issues with female and males.",
            "So we have class labels and we have a different underlying attributions again, and the last example I'll give is when we're looking at for biosimilar approvals at different area codes.",
            "We have the problem that for example different area codes will have different social economic.",
            "Distribution is, you know.",
            "So again, we have the problem of data and labels and a different track compare man.",
            "You can just do a straight comparison."
        ],
        [
            "The challenge is is here we understand what are we trying to measure?",
            "What kind of what is it?",
            "What does it change?",
            "I mean you have data, you have labels, you have different.",
            "What is a change will also have to figure out mean.",
            "Let's say you know what the changes you have an idea, that is how you find it.",
            "How do you model it?",
            "And most importantly is how do you quantify?",
            "How do you get a value that is useful to you that you could actually apply and and your analysis?",
            "We have to keep in mind.",
            "The more complex our training or modeling is, the more difficult it is to come up with the quantifiable model that we could actually have a result that makes sense.",
            "We have to keep this simple."
        ],
        [
            "OK, so.",
            "We're looking at the existing approaches.",
            "Basically there's two types.",
            "One of 'em is looks at probability distributions, and this is basically they just look at the data.",
            "Independent of labels.",
            "So basically killed emergency gas tests are couple of methods of looking at this data from different solutions.",
            "Other methods are used.",
            "Support level, attribute value combinations.",
            "These methods like kinda sets.",
            "There's successful discovery and there's like emerging pattern mining, so there's different approaches to solving this problem.",
            "Those are changed action change my name but we needed with an approach that just looks at this whole thing and one and one picture.",
            "Basically look at the classification and the and the labels and we're trying to estimate the difference between 50s datasets."
        ],
        [
            "So they need so looking at the purchase we see that there is need to compare models.",
            "We don't compare the data or the actual models and then so so we have to come up with.",
            "This is what model represents the data.",
            "You could have many different models you need to pick the best model that represents the data.",
            "So we have to be careful.",
            "How do you translate what amount of means to you?",
            "And we're trying to compare the data.",
            "The problem is when you're looking at the classification, you can maximum margin classifier.",
            "Classifier is simply trying to find the best fit for its data and sold.",
            "It doesn't try.",
            "You have to figure out where you have to convince the constraining to make it or not just to classify, but to help you quantify differences in datasets.",
            "And again, this gets very complex as the date it's a datasets have nonlinear separation boundaries so.",
            "That's the problem."
        ],
        [
            "So what they're trying to do here is quantify difference between datasets in a different class distributions.",
            "We're doing this as a model based classification difference.",
            "We're not doing this data would look at the models, so we're comparing models are not preparing data, and I think that's what is it.",
            "Basically different from existing approaches.",
            "So we tried to go beyond just standard optimization techniques at the same time, we're also trying to keep the classification correct, so you can just make models that fit, give you quantifiable differences, but they don't fit the classification of their actual datasets."
        ],
        [
            "So we're going to do this by looking at a distance measure will also going to be have a constrained version of logistic regression to capture that this is measured and we're going to have experiments to show that our results actually catch it.",
            "I mean, it's kind of difficult to show that that you resolve the models.",
            "Is this?",
            "You can just look at it and say these models are similar, so you have to figure out a way of showing that making the case that your models actually work."
        ],
        [
            "So I'm gonna just talk about luminaries.",
            "Adject if function here is L we get there regularization factor.",
            "We'll talk more about that WS or the weights.",
            "Epsilon is a constraint on the week values.",
            "So like I said this is a constraint optimization problem and epsilon is the dash will constrain values set.",
            "I mean and standard deviation.",
            "One set of differential features.",
            "What are differentials?",
            "Because these are the features that's important for one data set there not important for these are allows you to distinguish these two different datasets.",
            "So we call those differential features."
        ],
        [
            "So a little background with degression.",
            "It's a it's a method for a binary classification.",
            "It's a what it does.",
            "It does maximizes the log likelihood in.",
            "And this function, and by doing so you'll be disconnected maximum margin classifier.",
            "So basically by maximum that function you are finding the best fit and since optimization method will be using this method to really solve this optimization problem and the bottom, the bottom equation shows how its error resolved."
        ],
        [
            "And you could see that.",
            "This function is objective function solved by looking at the partial derivatives with respect to the weights and now we can see this.",
            "See the regularization parameter and this changing this value.",
            "It's about people using a lot of classifiers is to allow you to lose overfitting and an control the parameters values finally come up with a WS or this or regression coefficients that are telling you how important features for classification.",
            "I will be using that to understand the.",
            "Route.",
            "What are differential feature?"
        ],
        [
            "So supervised division difference STD.",
            "So if you look at it, it's just the difference in weights.",
            "Using this.",
            "To look at how much deviation are we from the individual classes like part of the constraints that we will be imposing literalistic progression.",
            "So this is the value that we look at to see how far we are from the unfinished remodel."
        ],
        [
            "So what do we do?",
            "We basically contained combined the two datasets and if you look at the top step it's basically all we do here is trying to figure out the best, see how how do.",
            "To pick the best regularization factor, it's just stepped.",
            "Allow you to bend it the best optimization and then what we do is we train stick Russian individual datasets.",
            "Simply.",
            "We simply do that to get accuracy because we need a model that just is constrained but also captures the.",
            "It has to be good class classifier, force individual data set.",
            "So we use those depression individual datasets and then we apply our constrained logistic regression model on these datasets and we show more about that and we use this SD devalued to make sure that we are within certain limits."
        ],
        [
            "So here we show that how we enforce the constraints.",
            "If you look at the first part of this, this is logistic regression with constraint editor and the constraint is saying that the concerns are stick model.",
            "The way this constraint logistic model is within a certain epsilon value that is the within a certain box basically.",
            "And to solve this problem we used a scale moon stuff versus there's no step please before it's just it's just taking this up.",
            "In addition, problem unconstrained.",
            "The constraint and like I said, excellent additions allow and.",
            "And once you solve this problem, you basically have to satisfy the optimization you'll be solving that physician optimization problem.",
            "So basically is we pick epsilon at lower upper bound.",
            "And if you could see epsilon is absolute value, sort of us.",
            "So it has lower upper bounds.",
            "We we get the weight vector using this constraint optimization and we see this with inaccuracy.",
            "It has those models have to fit still fit the accuracy of their individual datasets and if it's within what we consider is good enough.",
            "We start if it's not we we relax the constraints and we do this iteratively until we find a final solution.",
            "And to find a smooth transition, basically this epsilon is dependent on the weights and because if you look here, this epsilon is a difference in weights.",
            "The weights are small, epsilon is smaller with larger, so it's basically a factor of the weights.",
            "Is China keep constraint?"
        ],
        [
            "So basically, what is it?",
            "I'm constrained versus constrained.",
            "So I'm constraint is what general in this aggression is and we are just.",
            "Constrain this problem.",
            "And then the convergence proof is given in.",
            "We did certain datasets, data set one has has got a distribution and basically we had 10 attributes and we need three or four more differential.",
            "We won't be generated data set.",
            "We move through the 1st.",
            "We want to see who could detect those differential features and the rest of the features were similar in distribution.",
            "We use the method and five and basically which we were keeping with changing the label space and not the data space.",
            "And this is we try to make sure that we could fit both both methods of comparison and also at 5 datasets real real boulders is because you have to make sure that this works, not just theoretically."
        ],
        [
            "So the data set the first data set we talked about.",
            "We had three features that are differential and you can see that our our method was able to fill up the absolute value.",
            "You could tell it was able to print to find these features versus just.",
            "I'm concerned this progression that was not really able to do that.",
            "So this is our ranking and if you look at this we can see that our method was getting a job truth as far as rank we rank those features and differences.",
            "Then we were able to do that to find to recover that in our method versus unconstrained speak Russian."
        ],
        [
            "The second data set has four datasets and we we applied rankings.",
            "We compared grantees method which is referenced in five to our method and.",
            "Both methods were able to recover the rankings.",
            "The only difference is we can quantify the differences versus just ranking that."
        ],
        [
            "So another thing is work that sensitivity.",
            "We basically put subsample the whole data set, so we want to see how much you need before, how much of the data you need before you can capture this, which is the idea is the more data you have, the closer you are to the actual distribution, so the."
        ],
        [
            "More data you have, your closer you are, the difference becomes 0 because you have the entire data set and we could tell that.",
            "For our method we."
        ],
        [
            "So it's going down to zero.",
            "Basically because we're looking at the love here, but it's pretty good even at low settings, subsamples were able to find a lot of the data we did this on this and do this on the real world data set.",
            "And actually it was even better.",
            "Only you only need 10% of the data center data and basically you got the whole entire distribution with the class labels."
        ],
        [
            "Conclusions futurists, so we develop their constrained logistic regression models.",
            "Using to capture data differences using labels tool in the future will look at kernel methods nonlinear methods.",
            "And it's much more difficult, but I believe because in a higher dimensional space, comparing models becomes much more difficult."
        ],
        [
            "So these are and this is visuals and darker."
        ],
        [
            "Question.",
            "So I didn't get the difference to just taking the SCM weighting of the attributes.",
            "If you just take the the weight, the weight, the unconstrained version, you will get the results.",
            "5th your data, but it doesn't fit the constraint models.",
            "The idea is not to classify, the machine is always constrained, it has the regularization.",
            "See yes.",
            "That's exactly the same constraint complexity.",
            "The difference in weight between the entire data set versus the two datasets separately there see just constrains complexity.",
            "And that's not all trying to.",
            "Yeah, but I also would like to see a comparison with XY Alpha method that was presented for.",
            "Exactly for this task of finding whether the data set has changed, or whether there is a change within two datasets over overall so, so I'm sorry.",
            "That message is being presented by tossing Europeans.",
            "It's using Dixie an Alpha values and not only the feature weights but also those values in order to stage.",
            "Whether there has been a change in the data set, OK.",
            "I'm excited about that, maybe.",
            "Discuss that.",
            "The C and Alpha.",
            "Measuring the mistakes an Alpha is measuring.",
            "The example wait at what you easily see is if you plot it and then you have a change and in the data then you really see.",
            "How much it?",
            "It changes their values so it is very good for for detecting changes, sure.",
            "Actually, one of the things they had was how to show such differences.",
            "It's kind of a.",
            "It's kind of difficult.",
            "Show the difference, but that I would make sure to create that.",
            "Thanks again, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Amira.",
                    "label": 0
                },
                {
                    "sent": "How do you present this paper on the back of my colleagues and Chandan already from one state and this paper is entitled constraint but regression or square discriminative parents?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is an overview of the paper production per menarys framework.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The introduction.",
                    "label": 0
                },
                {
                    "sent": "The purpose of this paper is to look at different subgroups with labels.",
                    "label": 1
                },
                {
                    "sent": "The idea is we have multivariate data and you have a problem, especially when this data has labels.",
                    "label": 0
                },
                {
                    "sent": "You can just look at the data itself cause that the distribution might be different, but there's also the labels part of the part of the problem.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we we look at this problem in the context of the supervised training, where we using the data and the labels.",
                    "label": 1
                },
                {
                    "sent": "In our analysis.",
                    "label": 0
                },
                {
                    "sent": "So what would we be trying to do is we're trying to highlight the differences and do something different.",
                    "label": 1
                },
                {
                    "sent": "Subgroups of this multivariate data and at the same time we're trying to classify it by maintaining a class discrimination.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. What are overtime for example?",
                    "label": 0
                },
                {
                    "sent": "Cancer rates.",
                    "label": 0
                },
                {
                    "sent": "You have one table application.",
                    "label": 0
                },
                {
                    "sent": "Americans with African Americans, but you can.",
                    "label": 0
                },
                {
                    "sent": "You can solve this problem when we try to look at the data from different states.",
                    "label": 0
                },
                {
                    "sent": "For example, Southern states have more exposure to the sun versus normal state, so the data itself will have different distributions for different areas and the label space will have also have different distributions for different areas so.",
                    "label": 0
                },
                {
                    "sent": "This is an example where data is.",
                    "label": 0
                },
                {
                    "sent": "It is not.",
                    "label": 0
                },
                {
                    "sent": "It is not independent so.",
                    "label": 0
                },
                {
                    "sent": "Another example is when looking for a generous communication companies, we have the same problem here because again different industries have different issues with female and males.",
                    "label": 0
                },
                {
                    "sent": "So we have class labels and we have a different underlying attributions again, and the last example I'll give is when we're looking at for biosimilar approvals at different area codes.",
                    "label": 0
                },
                {
                    "sent": "We have the problem that for example different area codes will have different social economic.",
                    "label": 0
                },
                {
                    "sent": "Distribution is, you know.",
                    "label": 0
                },
                {
                    "sent": "So again, we have the problem of data and labels and a different track compare man.",
                    "label": 0
                },
                {
                    "sent": "You can just do a straight comparison.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The challenge is is here we understand what are we trying to measure?",
                    "label": 0
                },
                {
                    "sent": "What kind of what is it?",
                    "label": 0
                },
                {
                    "sent": "What does it change?",
                    "label": 0
                },
                {
                    "sent": "I mean you have data, you have labels, you have different.",
                    "label": 0
                },
                {
                    "sent": "What is a change will also have to figure out mean.",
                    "label": 0
                },
                {
                    "sent": "Let's say you know what the changes you have an idea, that is how you find it.",
                    "label": 0
                },
                {
                    "sent": "How do you model it?",
                    "label": 0
                },
                {
                    "sent": "And most importantly is how do you quantify?",
                    "label": 0
                },
                {
                    "sent": "How do you get a value that is useful to you that you could actually apply and and your analysis?",
                    "label": 0
                },
                {
                    "sent": "We have to keep in mind.",
                    "label": 0
                },
                {
                    "sent": "The more complex our training or modeling is, the more difficult it is to come up with the quantifiable model that we could actually have a result that makes sense.",
                    "label": 0
                },
                {
                    "sent": "We have to keep this simple.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We're looking at the existing approaches.",
                    "label": 0
                },
                {
                    "sent": "Basically there's two types.",
                    "label": 0
                },
                {
                    "sent": "One of 'em is looks at probability distributions, and this is basically they just look at the data.",
                    "label": 0
                },
                {
                    "sent": "Independent of labels.",
                    "label": 0
                },
                {
                    "sent": "So basically killed emergency gas tests are couple of methods of looking at this data from different solutions.",
                    "label": 0
                },
                {
                    "sent": "Other methods are used.",
                    "label": 0
                },
                {
                    "sent": "Support level, attribute value combinations.",
                    "label": 1
                },
                {
                    "sent": "These methods like kinda sets.",
                    "label": 0
                },
                {
                    "sent": "There's successful discovery and there's like emerging pattern mining, so there's different approaches to solving this problem.",
                    "label": 1
                },
                {
                    "sent": "Those are changed action change my name but we needed with an approach that just looks at this whole thing and one and one picture.",
                    "label": 1
                },
                {
                    "sent": "Basically look at the classification and the and the labels and we're trying to estimate the difference between 50s datasets.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So they need so looking at the purchase we see that there is need to compare models.",
                    "label": 0
                },
                {
                    "sent": "We don't compare the data or the actual models and then so so we have to come up with.",
                    "label": 0
                },
                {
                    "sent": "This is what model represents the data.",
                    "label": 1
                },
                {
                    "sent": "You could have many different models you need to pick the best model that represents the data.",
                    "label": 0
                },
                {
                    "sent": "So we have to be careful.",
                    "label": 0
                },
                {
                    "sent": "How do you translate what amount of means to you?",
                    "label": 0
                },
                {
                    "sent": "And we're trying to compare the data.",
                    "label": 0
                },
                {
                    "sent": "The problem is when you're looking at the classification, you can maximum margin classifier.",
                    "label": 1
                },
                {
                    "sent": "Classifier is simply trying to find the best fit for its data and sold.",
                    "label": 1
                },
                {
                    "sent": "It doesn't try.",
                    "label": 0
                },
                {
                    "sent": "You have to figure out where you have to convince the constraining to make it or not just to classify, but to help you quantify differences in datasets.",
                    "label": 0
                },
                {
                    "sent": "And again, this gets very complex as the date it's a datasets have nonlinear separation boundaries so.",
                    "label": 0
                },
                {
                    "sent": "That's the problem.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what they're trying to do here is quantify difference between datasets in a different class distributions.",
                    "label": 1
                },
                {
                    "sent": "We're doing this as a model based classification difference.",
                    "label": 0
                },
                {
                    "sent": "We're not doing this data would look at the models, so we're comparing models are not preparing data, and I think that's what is it.",
                    "label": 1
                },
                {
                    "sent": "Basically different from existing approaches.",
                    "label": 0
                },
                {
                    "sent": "So we tried to go beyond just standard optimization techniques at the same time, we're also trying to keep the classification correct, so you can just make models that fit, give you quantifiable differences, but they don't fit the classification of their actual datasets.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're going to do this by looking at a distance measure will also going to be have a constrained version of logistic regression to capture that this is measured and we're going to have experiments to show that our results actually catch it.",
                    "label": 1
                },
                {
                    "sent": "I mean, it's kind of difficult to show that that you resolve the models.",
                    "label": 0
                },
                {
                    "sent": "Is this?",
                    "label": 0
                },
                {
                    "sent": "You can just look at it and say these models are similar, so you have to figure out a way of showing that making the case that your models actually work.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm gonna just talk about luminaries.",
                    "label": 0
                },
                {
                    "sent": "Adject if function here is L we get there regularization factor.",
                    "label": 1
                },
                {
                    "sent": "We'll talk more about that WS or the weights.",
                    "label": 0
                },
                {
                    "sent": "Epsilon is a constraint on the week values.",
                    "label": 1
                },
                {
                    "sent": "So like I said this is a constraint optimization problem and epsilon is the dash will constrain values set.",
                    "label": 0
                },
                {
                    "sent": "I mean and standard deviation.",
                    "label": 1
                },
                {
                    "sent": "One set of differential features.",
                    "label": 0
                },
                {
                    "sent": "What are differentials?",
                    "label": 0
                },
                {
                    "sent": "Because these are the features that's important for one data set there not important for these are allows you to distinguish these two different datasets.",
                    "label": 0
                },
                {
                    "sent": "So we call those differential features.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a little background with degression.",
                    "label": 0
                },
                {
                    "sent": "It's a it's a method for a binary classification.",
                    "label": 0
                },
                {
                    "sent": "It's a what it does.",
                    "label": 0
                },
                {
                    "sent": "It does maximizes the log likelihood in.",
                    "label": 0
                },
                {
                    "sent": "And this function, and by doing so you'll be disconnected maximum margin classifier.",
                    "label": 0
                },
                {
                    "sent": "So basically by maximum that function you are finding the best fit and since optimization method will be using this method to really solve this optimization problem and the bottom, the bottom equation shows how its error resolved.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you could see that.",
                    "label": 0
                },
                {
                    "sent": "This function is objective function solved by looking at the partial derivatives with respect to the weights and now we can see this.",
                    "label": 0
                },
                {
                    "sent": "See the regularization parameter and this changing this value.",
                    "label": 0
                },
                {
                    "sent": "It's about people using a lot of classifiers is to allow you to lose overfitting and an control the parameters values finally come up with a WS or this or regression coefficients that are telling you how important features for classification.",
                    "label": 0
                },
                {
                    "sent": "I will be using that to understand the.",
                    "label": 0
                },
                {
                    "sent": "Route.",
                    "label": 0
                },
                {
                    "sent": "What are differential feature?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So supervised division difference STD.",
                    "label": 0
                },
                {
                    "sent": "So if you look at it, it's just the difference in weights.",
                    "label": 0
                },
                {
                    "sent": "Using this.",
                    "label": 0
                },
                {
                    "sent": "To look at how much deviation are we from the individual classes like part of the constraints that we will be imposing literalistic progression.",
                    "label": 0
                },
                {
                    "sent": "So this is the value that we look at to see how far we are from the unfinished remodel.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "We basically contained combined the two datasets and if you look at the top step it's basically all we do here is trying to figure out the best, see how how do.",
                    "label": 0
                },
                {
                    "sent": "To pick the best regularization factor, it's just stepped.",
                    "label": 1
                },
                {
                    "sent": "Allow you to bend it the best optimization and then what we do is we train stick Russian individual datasets.",
                    "label": 0
                },
                {
                    "sent": "Simply.",
                    "label": 0
                },
                {
                    "sent": "We simply do that to get accuracy because we need a model that just is constrained but also captures the.",
                    "label": 0
                },
                {
                    "sent": "It has to be good class classifier, force individual data set.",
                    "label": 1
                },
                {
                    "sent": "So we use those depression individual datasets and then we apply our constrained logistic regression model on these datasets and we show more about that and we use this SD devalued to make sure that we are within certain limits.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we show that how we enforce the constraints.",
                    "label": 0
                },
                {
                    "sent": "If you look at the first part of this, this is logistic regression with constraint editor and the constraint is saying that the concerns are stick model.",
                    "label": 0
                },
                {
                    "sent": "The way this constraint logistic model is within a certain epsilon value that is the within a certain box basically.",
                    "label": 0
                },
                {
                    "sent": "And to solve this problem we used a scale moon stuff versus there's no step please before it's just it's just taking this up.",
                    "label": 0
                },
                {
                    "sent": "In addition, problem unconstrained.",
                    "label": 0
                },
                {
                    "sent": "The constraint and like I said, excellent additions allow and.",
                    "label": 0
                },
                {
                    "sent": "And once you solve this problem, you basically have to satisfy the optimization you'll be solving that physician optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So basically is we pick epsilon at lower upper bound.",
                    "label": 0
                },
                {
                    "sent": "And if you could see epsilon is absolute value, sort of us.",
                    "label": 0
                },
                {
                    "sent": "So it has lower upper bounds.",
                    "label": 0
                },
                {
                    "sent": "We we get the weight vector using this constraint optimization and we see this with inaccuracy.",
                    "label": 0
                },
                {
                    "sent": "It has those models have to fit still fit the accuracy of their individual datasets and if it's within what we consider is good enough.",
                    "label": 0
                },
                {
                    "sent": "We start if it's not we we relax the constraints and we do this iteratively until we find a final solution.",
                    "label": 0
                },
                {
                    "sent": "And to find a smooth transition, basically this epsilon is dependent on the weights and because if you look here, this epsilon is a difference in weights.",
                    "label": 0
                },
                {
                    "sent": "The weights are small, epsilon is smaller with larger, so it's basically a factor of the weights.",
                    "label": 0
                },
                {
                    "sent": "Is China keep constraint?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically, what is it?",
                    "label": 0
                },
                {
                    "sent": "I'm constrained versus constrained.",
                    "label": 0
                },
                {
                    "sent": "So I'm constraint is what general in this aggression is and we are just.",
                    "label": 0
                },
                {
                    "sent": "Constrain this problem.",
                    "label": 0
                },
                {
                    "sent": "And then the convergence proof is given in.",
                    "label": 0
                },
                {
                    "sent": "We did certain datasets, data set one has has got a distribution and basically we had 10 attributes and we need three or four more differential.",
                    "label": 0
                },
                {
                    "sent": "We won't be generated data set.",
                    "label": 0
                },
                {
                    "sent": "We move through the 1st.",
                    "label": 0
                },
                {
                    "sent": "We want to see who could detect those differential features and the rest of the features were similar in distribution.",
                    "label": 0
                },
                {
                    "sent": "We use the method and five and basically which we were keeping with changing the label space and not the data space.",
                    "label": 0
                },
                {
                    "sent": "And this is we try to make sure that we could fit both both methods of comparison and also at 5 datasets real real boulders is because you have to make sure that this works, not just theoretically.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the data set the first data set we talked about.",
                    "label": 0
                },
                {
                    "sent": "We had three features that are differential and you can see that our our method was able to fill up the absolute value.",
                    "label": 0
                },
                {
                    "sent": "You could tell it was able to print to find these features versus just.",
                    "label": 0
                },
                {
                    "sent": "I'm concerned this progression that was not really able to do that.",
                    "label": 0
                },
                {
                    "sent": "So this is our ranking and if you look at this we can see that our method was getting a job truth as far as rank we rank those features and differences.",
                    "label": 0
                },
                {
                    "sent": "Then we were able to do that to find to recover that in our method versus unconstrained speak Russian.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second data set has four datasets and we we applied rankings.",
                    "label": 0
                },
                {
                    "sent": "We compared grantees method which is referenced in five to our method and.",
                    "label": 0
                },
                {
                    "sent": "Both methods were able to recover the rankings.",
                    "label": 0
                },
                {
                    "sent": "The only difference is we can quantify the differences versus just ranking that.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So another thing is work that sensitivity.",
                    "label": 0
                },
                {
                    "sent": "We basically put subsample the whole data set, so we want to see how much you need before, how much of the data you need before you can capture this, which is the idea is the more data you have, the closer you are to the actual distribution, so the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More data you have, your closer you are, the difference becomes 0 because you have the entire data set and we could tell that.",
                    "label": 0
                },
                {
                    "sent": "For our method we.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's going down to zero.",
                    "label": 0
                },
                {
                    "sent": "Basically because we're looking at the love here, but it's pretty good even at low settings, subsamples were able to find a lot of the data we did this on this and do this on the real world data set.",
                    "label": 0
                },
                {
                    "sent": "And actually it was even better.",
                    "label": 0
                },
                {
                    "sent": "Only you only need 10% of the data center data and basically you got the whole entire distribution with the class labels.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conclusions futurists, so we develop their constrained logistic regression models.",
                    "label": 0
                },
                {
                    "sent": "Using to capture data differences using labels tool in the future will look at kernel methods nonlinear methods.",
                    "label": 0
                },
                {
                    "sent": "And it's much more difficult, but I believe because in a higher dimensional space, comparing models becomes much more difficult.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are and this is visuals and darker.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "So I didn't get the difference to just taking the SCM weighting of the attributes.",
                    "label": 0
                },
                {
                    "sent": "If you just take the the weight, the weight, the unconstrained version, you will get the results.",
                    "label": 0
                },
                {
                    "sent": "5th your data, but it doesn't fit the constraint models.",
                    "label": 0
                },
                {
                    "sent": "The idea is not to classify, the machine is always constrained, it has the regularization.",
                    "label": 0
                },
                {
                    "sent": "See yes.",
                    "label": 0
                },
                {
                    "sent": "That's exactly the same constraint complexity.",
                    "label": 0
                },
                {
                    "sent": "The difference in weight between the entire data set versus the two datasets separately there see just constrains complexity.",
                    "label": 0
                },
                {
                    "sent": "And that's not all trying to.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I also would like to see a comparison with XY Alpha method that was presented for.",
                    "label": 0
                },
                {
                    "sent": "Exactly for this task of finding whether the data set has changed, or whether there is a change within two datasets over overall so, so I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "That message is being presented by tossing Europeans.",
                    "label": 0
                },
                {
                    "sent": "It's using Dixie an Alpha values and not only the feature weights but also those values in order to stage.",
                    "label": 0
                },
                {
                    "sent": "Whether there has been a change in the data set, OK.",
                    "label": 0
                },
                {
                    "sent": "I'm excited about that, maybe.",
                    "label": 0
                },
                {
                    "sent": "Discuss that.",
                    "label": 0
                },
                {
                    "sent": "The C and Alpha.",
                    "label": 0
                },
                {
                    "sent": "Measuring the mistakes an Alpha is measuring.",
                    "label": 0
                },
                {
                    "sent": "The example wait at what you easily see is if you plot it and then you have a change and in the data then you really see.",
                    "label": 0
                },
                {
                    "sent": "How much it?",
                    "label": 0
                },
                {
                    "sent": "It changes their values so it is very good for for detecting changes, sure.",
                    "label": 0
                },
                {
                    "sent": "Actually, one of the things they had was how to show such differences.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a.",
                    "label": 0
                },
                {
                    "sent": "It's kind of difficult.",
                    "label": 0
                },
                {
                    "sent": "Show the difference, but that I would make sure to create that.",
                    "label": 0
                },
                {
                    "sent": "Thanks again, thank you.",
                    "label": 0
                }
            ]
        }
    }
}