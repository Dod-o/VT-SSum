{
    "id": "5izl2xafl3lix6c6jzmae4tbzk4zwnd6",
    "title": "Principles of Very Large Scale Modeling",
    "info": {
        "author": [
            "Pedro Domingos, Dept. of Computer Science & Engineering, University of Washington"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_domingos_scale_modeling/",
    "segmentation": [
        [
            "So I'm going to talk about.",
            "Principles of very large scale modeling.",
            "But before that.",
            "Well, first of all, I'd like to say that I'm very honored by this award and humbled.",
            "And.",
            "If by any chance my research to date falls short of this award, I promise to make up the deficit with my future research."
        ],
        [
            "And it's it's customary in these things to start by thanking your students and collaborators.",
            "I'm not going to do that for a very simple reason, which is that this award is really for them as much as it is for me.",
            "So what I would like to do is, on behalf of all of us."
        ],
        [
            "Thank everyone who made this possible this adventure that we've been on, including of course our families, our friends, our funders, and everybody else in this community.",
            "And on that note, I would like to date."
        ],
        [
            "Kate, this talk to the memory of my father, who was a distinguished scientist and died last month.",
            "So my talk is entitled principles of."
        ],
        [
            "Large scale modeling.",
            "So the first thing that I would like to do is say what I mean by that, and then essentially the talk will have three parts, each of which will talk about 1.",
            "One such principle and let me give you a preview of what they are.",
            "The first one is that you should model the whole, not just the parts.",
            "The second one is that you should tame complexity via hierarchical decomposition, and the third one is that time and space should not depend on data size.",
            "And of course these are only three principles and this is only one talk.",
            "There's much more.",
            "That we that we know and that we have to learn about building very large scale models."
        ],
        [
            "So I would like to introduce this notion of very large scale modeling by means of an analogy.",
            "An analogy between KDD and microelectronics and more specifically between models and microchips.",
            "So in VLSI right there was a pre history where there were no integrated circuits so people they had single components.",
            "And then there was the, you know, the integrated circuits were invented and for a while they were pretty small.",
            "They had 10s of components, maybe hundreds.",
            "And then there was the air of large scale integration, which was roughly the 70s where you got into the, you know hundreds, thousands, 10s of thousands.",
            "And now you know around 1980 there was a transition to the aerial view of the error VLSI very large scale integration, which is where we are today and we now have billions of components on the chip and and will have more the next decade.",
            "And there's an interesting parallel between this and what has happened in KDD.",
            "So likewise there was a pre history where what we have was descriptive statistics.",
            "You know, just histograms or you know means and variances and things like that.",
            "And then you know pre 1995 which is actually when I started my working in machine learning.",
            "You know what we were building then were small models things with you know 10s or hundreds of variables and maybe hundreds or thousands of examples and and then."
        ],
        [
            "Like I I'm dating this from the first Kitty conference in 1995, which is also the first one that I attended.",
            "You know, in the last 20 years, I think we've been in the air of building large models.",
            "Which are vastly larger than what we were doing back then, and they continue to get larger.",
            "And So what I believe is that we have now begun the transition to the era of very large scale models, which I think will last for the foreseeable future.",
            "Now the interesting thing about these transitions, both in the in the case of microchips and in the case of models, is that it's not just the scale that's going up, there's actually much more than that going on when the skill changes by that much, the change becomes qualitative.",
            "As well as quantitative in the case of VLSI in particular there was this huge transition from from large scale integration to very large configuration, which was, you know, the landmark was the publication of this book, which many of you probably know by Carver Mead and Lynn Conway, introduction to VLSI Systems which form the foundation of how people do VLSI today.",
            "And there's a huge difference between the before and after.",
            "So before what people did when they design integrated circuits was to wire gates 1 by 1.",
            "And the design was tide to the fabrication.",
            "You will read you are literally designing how things were going to be placed on the Silicon die so that it could then be manufactured, but that doesn't scale to building circuits with billions of transistors, and so that's what happens now in the vast areas that nobody wires gates anymore.",
            "What you do is you combine high level modules.",
            "Each of these modules performs often a very complex, very large function and the design is actually quite independent of fabrication.",
            "Those models that some of those modules at some point compiled down to Silicon in using what's called a Silicon compiler, and that's where the gates come out of.",
            "But the engineers don't think in terms of gates.",
            "The design is independent of the fabrication, and you know what you use for designing things like Cat Tools, computer aided design tools and hardware Description language where you describe at a high level what it is that you want to build, and then it gets executed because if we didn't do things like this, we would actually not be able to do them at all.",
            "Nobody can design A chip with a billion transistors one at a time.",
            "And what I believe is that today."
        ],
        [
            "KD D is undergoing a similar transition.",
            "Not just quantitatively in the scale of things that we're doing, but qualitatively.",
            "The kinds of things that we're starting to do, and that I think you know, we're going to do more of in the future.",
            "So for example, you know so here what I have in this table is like on the left large models of the kind that we've built for the past 20 years, and on the right I have what what I'm calling very large scale models.",
            "An for example, you know we all you know we do a lot of building models of customers, but something that we're starting to do a lot of this building models of the whole social network that the customers are part of.",
            "In biology for a long time we build models of individual genes and proteins, but now we're starting to build models of whole metabolic pathways and eventually we want we want to build models of whole cells, even a whole organisms in neuroscience, for example, we've done a lot of models of single neurons, but you know we're starting to model larger circuits with neurons and eventually want to model the whole brain.",
            "For example, in the case of cities, you know we have models of a single service like electricity or water.",
            "Or transportation, but what we want to have it in today's the model of a whole whole city works.",
            "All these services together with the people that use them.",
            "And this applies to them and what not that we can then optimize, going, for example, from single species to whole ecosystems going from the atmosphere, which is really just differential equations to all those things, influence the climate going from individual vision tasks like object recognition to building whole vision systems with all of the components working together.",
            "To actually provide vision language going from things like parsers and part of speech taggers and into resolution to whole systems that do all the parts on these parts actually work together and understand text.",
            "This is I think where we're, you know we're starting to go and you know one last example is recommender systems.",
            "So today we have very good recommended systems that for example, recommend books to you or they recommend DVD's or they recommend some range of products.",
            "But what we would really like to have.",
            "Assistance that have a 360 degree view of you.",
            "They understand who you are, what your tastes are, what you've been doing, where you're going, what you want, and they do their recommendations for all of the things that you want based on everything that they know about your life.",
            "And my basic contention is that this is one world that we know pretty well at this point, but this is the world where we're going.",
            "The biggest contribution to society that we can make is in doing these things.",
            "But doing these things requires a whole different set, sorry excuse."
        ],
        [
            "A whole different set of ideas from the ones that we have right now there are.",
            "There's a host of new problems just like in VLSI.",
            "There was a host of new problems compared to LSI, and So what I would like to do in this talk is mentioned three principles for building these very large scale models.",
            "Of course there are many more.",
            "I'm only going to touch on a few and I will illustrate those principles with examples from my research.",
            "Some of it old and some of it knew.",
            "So the first principle."
        ],
        [
            "Is a model the whole, not just the parts?",
            "And."
        ],
        [
            "And what do I mean by that?",
            "Let me illustrate with this, with with with an application that I'm going to use as a running example, which is social networks.",
            "Social networks are definitely one area where this transition from large models to very large models is well underway.",
            "And let's suppose that you have a new product that you want to sell and you have to decide what customers to market it to.",
            "The traditional way to do this is to build a model of each customer.",
            "For example, you may know."
        ],
        [
            "What she clicked on you may know some profile information that you provided demographics that you got from another source and based on this and many other things you will predict some probability that she will buy the product.",
            "And then there's one more input which is your marketing.",
            "What you want to predict is given these things.",
            "If now I market how much does that increase your probability of buying?",
            "And based on that we will decide to market to her or not but when we do this we're actually doing it for."
        ],
        [
            "Each customer in isolation.",
            "And that's missing something very important, which is that customers influence each other.",
            "People's buying decisions are not."
        ],
        [
            "Buying decisions, but decisions on what to do.",
            "What causes to contribute to what things to get engaged in, whether to smoke or not smoke.",
            "You know, all these things.",
            "Very often the single biggest influence on your decisions is your friends, your coworkers, your acquaintances, and if we treat this as a bunch of independent units were missing that picture.",
            "So we need to take these things into account.",
            "We need to model the."
        ],
        [
            "Whole of the social network, we need to model the effect of the marketing or of whatever action you're taking on the whole network, not just on the individual in isolation.",
            "'cause if we don't, we're going to miss the forest for the trees, not to mention probably waste a lot of money and not get the results that we would like to get.",
            "Now, if we believe in this principle that we want to model the whole and not just the parts, the next question is how are we going to do that?",
            "And this is where we come into a very big problem, which is also, I think, a large part of the reason why it hasn't been done before is that the traditional statistical models that we all know and love are really not applicable here.",
            "The reason they're not applicable is that they assume that samples are independent.",
            "Now we make this assumption because it makes life much easier, but it happens to be a false assumption.",
            "And now if you want to remove that assumption, the question is what can we do?",
            "Well, there's ad hoc things that we can do and that people do, But the problem is, I thought methods is that they are geared to one particular problem in one particular context, and they won't generalize.",
            "And also, even in that context, you know they tell you something to do, but you have no notion of how good it is.",
            "Or whether you could be doing better.",
            "So we would like to be able to do something more than this, and indeed we can."
        ],
        [
            "One way that we can do this is by you by using what are called Markov logic networks and the idea in Markov logic networks starts with the following observation.",
            "There's we actually know how to represent interactions between entities in the world.",
            "We have logic as the language to do that.",
            "For example, if we want to say that you know people influence each other, we could write a rule in logic of the following form buys X one and influences X.",
            "One X2 implies that buys X2.",
            "This means that if I buy something and I influence you, then you will also bite and this is a general rule right where these variables can then be replaced by specific people in the world.",
            "So for example, if I replace X1 by Anna and next to buy Bob, you know this becomes one specific instance, which is that if Anna buys and she influences Bob, then Bob buys.",
            "So representing this is actually not that hard, but there's a very big problem, which is that logical rules like this are all or none.",
            "Now, this really saying that if an advising she influences Bob and then he must buy as well, and we know that this is not showing the real world effect influences Bob.",
            "What that does is it increases the probability that Bob will buy, which is very different from water logic.",
            "Rule says logic can't model this kind of uncertainty.",
            "This kind of graded behavior.",
            "So what can we do?",
            "Well, one thing we can do is treat these logical formulas not in the usual way, but as feature templates for a log linear model, right log in your models are you know what.",
            "Most of the models we use are also known as Markov networks, which is where the name Markov logic networks comes from.",
            "So let me briefly review what they are and then how we can how we can go beyond them to something like Markov logic networks.",
            "So a log."
        ],
        [
            "Your model basically says that the X is a state of the world.",
            "Like say, some people buying a product in some, not buying it and it says that the probability of a state is a function of some features of that state, which can be whatever you think is important or can be learned from data.",
            "And let's think of the features is Boolean for the purpose of this talk, but they could be continuous and the way we compute the probability of a state is we take each feature inside the zero or one we multiplied by await so features with a higher weight are more important.",
            "And then we exponentiate that and we normalize it to turn it into a probability, right?",
            "This ensures that the awesome to one OK, and this is very well known in statistics and in machine learning.",
            "We know how to use and learn these things very well.",
            "What we want to do is we want to generalize this beyond the independent context where they usually used to something broader and the way we can do that is by turning the rules that we saw before into templates for these features.",
            "So each rule can become a set of features, one for each set of objects that the rule applies to.",
            "OK, so for example, using using the rule that we saw before, right?",
            "This is going to become one feature for each pair of people, including one feature for Anna and Bob and now and now, since N influences Bob, what this means is that if an influences Bob in both by then these features going to be true, which means that the probability of the world is going to go up OK, so when a lot of a lot of features with a high weight are true.",
            "Then the world is very probable.",
            "Otherwise, it's improbable.",
            "So now we can both represent the rich relationships among objects, but also the fact that there's statistical.",
            "They're not deterministic."
        ],
        [
            "OK.",
            "Here's a very simple example Markov logic network for viral marketing.",
            "There's a very simple rule that is nobody buys anything.",
            "OK, most people don't buy most things right.",
            "That's just life, so we priority.",
            "Nobody is going to buy the product and so this is what this rule says that you know, for any given person, they probably are not going to buy.",
            "Remember, this is not a deterministic cool, this is just going to have a weight which could be the same for everybody.",
            "Or it could be different for different people.",
            "You know.",
            "Depending on how likely we think they are a priority to buy the product and then we can have a rule that represents the effective marketing, which is that if I market to somebody then they buy and again this does not mean that they guarantee to buy.",
            "This just means that the higher the weight of this rule, the more likely they are to buy, given that are marketed to them OK.",
            "But so far this is just the classic setting.",
            "Now you know the thing that really brings in the modeling of the whole is this rule that we saw before effects one buys and she influences X2.",
            "The next 2 bytes as well, and the higher the weight of this rule, the more of an effect it will have.",
            "So if any inferences babolat then for her this will have a high weight, if she influences him a little then this will have a low weight."
        ],
        [
            "And of course, this is just a bare bones MLN.",
            "In reality, what we want to do is build something much richer using a lot more formulas in the corresponding features.",
            "We want to include things like customer and product attributes and how they affect the probability of buying things like the cost of marketing, the prices that discounts, the profits we will in general have multiple relations, so you could have friends influencing each other or coworkers influencing each other.",
            "We can have multiple types of entity, like for example if the product is a drug you want to take the patients into account, but also the doctors.",
            "The hospitals, the health insurance companies, and so forth.",
            "You may have a choice of different marketing actions.",
            "You know different things that you can do.",
            "Different amounts that you can spend.",
            "You can place an ad you can you know, give a discount, and so forth.",
            "Very important one is time right?",
            "You may want to intervene in this process as it involves overtime.",
            "I market it.",
            "Some people bought what is now the next set of people that I want to market to.",
            "You may be doing this for multiple products at the same time because you may have a choice of doing this product versus that.",
            "You may have this with multiple companies competing, and I are actually playing this game.",
            "You know, if I do this, what will they do?",
            "And all of these things we can and have written down in logic and then we learn weights for them and then this becomes a statistical model and there's of course many other things that you can imagine adding, but hopefully at this point you know you have the more less of an idea of."
        ],
        [
            "How how we can do this?",
            "Now the next question is OK, so let's say we built like this.",
            "Great, you know, beautiful model of the whole system.",
            "In this case the social network.",
            "Then how do we use it?",
            "Well, the way we would like to use a model like this is to figure out what is the best set of people to market to.",
            "Right ideally would like I would like to do is find a relatively small set of people to market this so my marketing cost is low, but they have a lot of influence in the network, both direct and indirect, such that I get a lot of people buying because of those few.",
            "So I maximize my bank for the Buck.",
            "Well, how do I do that?",
            "Well, for example I can choose an initial set of customers to market 2, and then I use my MLN to compute the expected number of buyers, right?",
            "This is probabilistic, but in expectation how many people will buy?",
            "Given those people that are marketing to and their effect on the network, and then what I do is I try variations of that.",
            "I said well if I add this person, how does that change the expected number of people who buy?",
            "What if I add that one and I keep on doing this kind of greedy search until I can't improve it anymore.",
            "OK, now this all sounds very good and in fact we did this in the paper in KDD over 10 years ago, which was actually a precursor to the paper by, you know Kleinberg and Tardos and you know, like the.",
            "Champion on on influence maximization.",
            "The big problem, however, is that this process of computing the expected number of buyers in the in the MLN is extremely intractable, right?",
            "So this sounds good in principle, but doing it in practice on a large scale is very difficult.",
            "So let's see why it's difficult, and."
        ],
        [
            "And what we can do about it?",
            "Well, let's say for example, that we have in our social network here, and we've persuaded Anet to bite.",
            "So any bites?",
            "And now we want to know how much is that going to change Zacks probability of buying right?",
            "And if we can do this for everybody, we're in good shape.",
            "That's what we want, and then we can evaluate our strategies that way.",
            "But the problem is that to evaluate the effect of an on Zach, we have to propagate her influence through the network and notice that this would still be a problem even if Anna was in a direct influence of, say, Bob here, 'cause there would be the direct influence.",
            "But there will also be many paths of indirect influence, and those also count.",
            "So one way or another, we have to propagate the influence through the network and then what happens is like for example we have to see what."
        ],
        [
            "M's effect is on these two people.",
            "Let's say there Bob and Chris, right?",
            "But the two people and they're not independent, right?",
            "And they have four possible states.",
            "You know.",
            "Like you know, Bob Bias and Chris doesn't, and so forth, right?",
            "So there's four possible states, each one of them has a different probability.",
            "And then there's the probability of the Zach bias given each of these States and then those two people in turn."
        ],
        [
            "Influence these four.",
            "So now I have two of the four States and I have to sum over all of them.",
            "You know bye bye bye, not bye.",
            "What's the probability of that times the probability that Zach then buys?",
            "Given these people, something that overall states OK?",
            "So if I have so the cost of my influence is going to be exponential in the width of the network.",
            "So if my network has 1000 people with which is actually not that much, this means that it would need two to the thousands.",
            "I have a cost of order of two to the 1000.",
            "To actually compute the influence of Anna on Zack.",
            "And obviously that's not going to happen OK."
        ],
        [
            "So the question is, what can we do?",
            "And this is where the second principle."
        ],
        [
            "It comes in, which is tame complexity by hierarchical decomposition.",
            "OK, in general in the most general case the problem is NP complete.",
            "There's nothing we can do, but the real world is not the most general case.",
            "The real world has hierarchical structure.",
            "Now when you look at a principle like this, you might think, well, sure motherhood and Apple pie, right?",
            "Why wouldn't you do hierarchical decomposition, right?",
            "And then?"
        ],
        [
            "Fact, it's where everybody does.",
            "It's what they do in VLSI.",
            "In fact, one of the main things that the meeting Conway book introduced was this notion of hierarchically.",
            "You know, having this hierarchy of modules and sub modules and submodules with this specific devices at the end.",
            "And of course it's what people do in programming.",
            "Right when we write programs, we use hierarchical decomposition, but you should remember that you know back in the day, people wrote programs using go to statements and they were big bowls of spaghetti.",
            "And then Dykstra wrote a paper saying, you know.",
            "Two statements considered harmful, and that's why we do structured programming today.",
            "Now the interesting thing is that everybody and you know, scientists you know.",
            "Engineers use hierarchical decomposition.",
            "We are probably the only ones.",
            "Who don't do it?",
            "But when you look at most models that people build, even when they're very, very large, they're not hierarchical with issue exceptions, right?",
            "For the most part, our models are not hierarchical.",
            "And this is interesting, right?",
            "So you could say, well, you know that's because there hasn't been that much need so far, right?",
            "If you're just going to predict somebody's buying decision using the logistic regression or random forest, this on 1000 attributes, you don't need hierarchal with composition for that.",
            "But I think if we want to move into this world with very large scale models where we model the whole not just the parts, we gotta start doing hierarchical decomposition now.",
            "You could also say that well, but at at heart the phenomenon, not hierarchical, and that's why we are modeling like.",
            "Think of a vision network for example.",
            "It usually doesn't have hierarchical structures, just it's a big jumble of nodes and links.",
            "Well with that I think I would have to disagree because the world."
        ],
        [
            "It has a lot of hierarchical structure, at least as we perceive it right.",
            "'cause we also have finite computational resources in our brains.",
            "The world has hierarchical structure in at least two important ways.",
            "The first is that.",
            "The world is made of parts and subparts and sub sub parts, like for example the economy has sectors like energy media, health.",
            "You know each of these sectors in turn has companies in it.",
            "Like for example Disney Viacom, wandering Media and then the companies in turn have divisions like HR, it sales all the way down to individual people.",
            "So the world decomposes into parts.",
            "Another very important type of hierarchy is class is of course the class hierarchy, right?",
            "The objects, the entities."
        ],
        [
            "The world are not a random jumble of things.",
            "They have similarities and you can group them.",
            "So for example, humans could be Asian American, European and so forth.",
            "Americans could be Californians, New Yorkers, Texans, and so on all the way down to individual people like, you know, like Bill de Blasio, Derek Jeter and you know Barbra Streisand, who are all famous New Yorkers.",
            "Another point of having these kinds of hierarchy is that we can exploit them."
        ],
        [
            "To make our inference tractable.",
            "And I, you know, even if the world is not hierarchical, I would argue that we're better off approximating.",
            "It is hierarchical.",
            "Then, for example, just assuming that everything is independent because as we saw, that's really throwing out the baby with the bathwater, or just winding up with an intractable model that sure, this isn't this is this is an accurate model, but you can't get accurate results out of it, because because you can't do the inference.",
            "OK, so let's try to approximate the world using using these."
        ],
        [
            "Kinds of hierarchies, and there's two assumptions that we can make that will that will really help us.",
            "The 1st and remember our goal here is not to just use hierarchical decomposition to you know the same way you would use in programming, but use hierarchical decomposition to make inference in one of these very large probabilistic models with very large system tractable.",
            "And there are two assumptions at least that we can make related to these hierarchies that that will get us very far.",
            "The first one is that subparts are independent.",
            "Given the parts, and in fact psychologists will tell you that this is the assumption that human beings make all the time.",
            "So for example, if I want to predict the probability that different divisions in the company will buy, for example a particular computer for the desktops of their employees.",
            "I couldn't.",
            "I could model a very big dependency between all the people on the units, or I could assume that the departments within the organization are independent.",
            "Given weather, the organization itself buys.",
            "So the probability that, for example the company and they charge 90 by is the probability that the company buys times the probability that a chart buys.",
            "Given that the company buys times the same thing for the other departments.",
            "So the simplification that I'm making here is that I'm ignoring the fact that HR and it could actually influence each other directly.",
            "And in reality they do right.",
            "And of course this is a very simplified example, but the idea is that if there most of the interaction is through the larger part that they're in, then this really reduces the complexity.",
            "The other, the other assumption that we can make, of course, is that the probability for a class.",
            "Is an average of the probabilities for the subclasses for.",
            "So, for example, the problem with that somebody buys my product given that their American, well, that's the probability that they're in New Yorker and then that they buy given that their New Yorker, plus the probability that the Californian you know the problems that they buy, given that they California plus the same thing for all 50 States and if we combine these two things together, we can actually ensure that our inferences tractable?",
            "Let's see how that happens.",
            "So let's suppose that."
        ],
        [
            "You know there's some item and you're interested in the distribution of of whether everyone will buy it or not.",
            "So each of these people will either buy or not buy it and let you know.",
            "Here's a complicated network, right that we have from before.",
            "Let me simplify this network by saying, well, there's really 2 main clusters in this network.",
            "There's the cluster of Anna's friends who influence each other a lot, and there's the cluster of ZX Friends who also influence each other a lot, but the influence between those two classes is fairly weak.",
            "OK, So what I'm going to do is like.",
            "I'm going to model the probability that some person that people in Ennis cluster will buy the product and I'm going to do the same thing for Zach and I'm going to like this.",
            "So sorry for Zach's group and I'm going to allow dependency between the groups at the group level and then within the group I'm going to have the dependency between the group and each of the individual people so I'm still more or less modeling what's going on.",
            "What I'm doing is I'm abstracting away from the details of how."
        ],
        [
            "People inside the group interact, they reach still going to have their own different probability and their own different parameters.",
            "But I've kind of summarized their interaction through the group.",
            "And of course here I just have these two levels plus the level of home network being Journal.",
            "I could have as many levels of this as I want to have.",
            "OK, now so far what we've done is not that interesting or new things get interesting when you start thinking about the following.",
            "Well, there's more than one relation involved here, right?",
            "There's friends, but there's also coworkers an maybe for some products were more, and this is true actually, for some products we are more influenced by our friends and for some parts were more influenced by our coworkers and now the circle of Anna's coworkers is different from the circle of her friends.",
            "They probably overlap, but they're not the same.",
            "And same thing for Zach, OK?",
            "So now we actually have two alternative compositions of the world going on here and there operated for different kinds of products.",
            "And now when I."
        ],
        [
            "Ave product, let's say I'm trying to sell a book.",
            "Well, a book might be more of a leisure item or more of a work item.",
            "If it's a leisure item, then this is the composition that's going to be in effect.",
            "If it's more of a work item, this is the decomposition that's going to be in effect.",
            "OK, so now if I combine all of this, I can actually have very rich interactions between going on between various groups of people without without actually having lost my tractability.",
            "Why have I not lost my tractability because he?"
        ],
        [
            "That's how this plays out, right?",
            "What I have to do is I have to sum over these two classes of products, right?",
            "And then for this class I have one decomposition of the world into parts and then my overall result in this very simple example is just the product and in this case the world decomposes differently into coworkers.",
            "So when you look at what's going on here, right?",
            "And try to generalize it, you can actually state this very nicely in the form of what's called the sum product theorem.",
            "What the sum product theorem says is the following is that.",
            "And marginal."
        ],
        [
            "Probability can be efficiently computed.",
            "If it is one of two things.",
            "If it's a weighted sum of efficiently computable marginals over the same variables.",
            "Or if it's the product of efficiently computable marginals over disjoint variables?",
            "Now let's look at each of these things in turn and see what it means and why that's the case here.",
            "What I'm saying is that as long as the two distributions are over exactly the same variables, then I can just add them with weights.",
            "And I don't lose track the ability that way, and the reason is very simple.",
            "It's just that you know addition is commutative.",
            "So if I have a sum over the values of a variable of two functions, right?",
            "Because addition is commutative, I can naturally reorder this and put this sum over the functions outside, and there's some of the variable inside.",
            "And this is still OK.",
            "So I've actually decomposed right this into these two pieces.",
            "And you know there was no blow up.",
            "The second one is that a product of efficiently computable marginals over disjoint variables is also still tractable, and this now is because of the distributivity of multiplication over addition.",
            "So if you look at this expression over here right, I can see this is this is a product of two sums, one is over the function F and one is over the function G and they're over different variables, right?",
            "If these were, if they were shared variables here, this would not work right.",
            "But since there are different variables, I can first distribute this over this and get a bunch of terms.",
            "And then I distribute each of those terms over this guy and I get a sum of all the pairs.",
            "Notice that here I have this sum over all the pairs, right?",
            "Which is quadratic, and here I just.",
            "Have you know this number plus this number right?",
            "So if I keep doing this actually get an exponential decrease in the complexity of my inference, and so if we combine these two things, we can actually and we quite sums with classes and products with parts we can actually."
        ],
        [
            "Get very far, OK. And of course the beauty of this definition is that it's recursive.",
            "I can have a sum of products of sums of products with as many levels as I want, and I can wind up with a very complex, very large network where you know the inference can still be done in linear time in the number of people in the network.",
            "And by the way, this does not just apply to probabilities.",
            "This applies essentially to any function it applies to any semiring that has distributivity and commutativity, and you know, for example, it applies to things like the relational semiring with joins and unions.",
            "It applies to the Boolean, semiring, etc.",
            "It applies to you know constraint satisfaction.",
            "It applies to logic.",
            "So this is very, very powerful and this decomposition, right?",
            "We could learn it.",
            "We could be primary.",
            "So we're only going to learn models that actually decompose this way.",
            "It could be encoded by hand write the same way you encode the class hierarchy into an object oriented program, or it could just be used as an approximation to a more complex model at inference time.",
            "In all of these cases, the decomposition is giving us tractability."
        ],
        [
            "And if we combine the Markov logic networks which we saw before with some product team which we saw just now, we can actually define a tractable subset of Markov logic networks which we call tractable Markov logic, and the guarantee that this gives you is that if you learn or write your model in tractable Markov logic, then no matter how large the model has, no matter how moving parts it has, the cost of inference will always be linear in the size of the model.",
            "Because essentially the structure of the model and the structure of the computation are isomorphic at the end of the day.",
            "This is what's really going on here.",
            "OK, so now the 3rd and final principle that."
        ],
        [
            "Going to talk about here is the idea that time and space should not depend on data size.",
            "If your learning is blowing up more than linearly with the amount of data that you're using, you want to ask yourself whether maybe you're doing something wrong and how you could fix it.",
            "So the purpose you know we live, of course, in the."
        ],
        [
            "The age of big data.",
            "The purpose of having big data is to be able to learn big models.",
            "Rich complex models, or at least many small ones, but we already saw the limitations of that.",
            "If you're using big data to learn a small model or even a medium sized model, then you're really wasting your resources right and resources are expensive.",
            "The size of data that you need to really be dictated by the size of the model that you're learning.",
            "Small model, small data, large model, large data, huge model, huge data.",
            "And then the excess that you can just ignore, right?",
            "You don't even need to bother with it.",
            "Now, of course, if you if you accept this principle, then the question becomes, well, how do you figure out just how much there is enough right?",
            "If I stop using data Now, if I use this small subsample, how do I know that I'm not missing out on some really important effects?",
            "So what I'm going to try to do now is give some amount you know try to."
        ],
        [
            "Answer That to some extent.",
            "And we're going to do this in the following setting.",
            "We call this streaming bound algorithms.",
            "And the idea is that we're going to assume that you have an infinite data stream.",
            "You have data coming at you very fast everyday, forever, that they never ends.",
            "This is how essentially every organization operates to their right.",
            "The data is never ending, so let's just assume that it's infinite.",
            "Let's go full hog and say we have infinite data.",
            "The problem, of course, is that we only have constant time and memory.",
            "You only have so much RAM in your server farm, and you can only wait so long for an answer, right?",
            "You can't wait to the end of the universe before you start taking actions.",
            "Now what we would really like to do.",
            "Is to using only this fixed as small as possible amount of time and memory learn a model that is as close as possible to the model that you would learn if you have infinite space and time to run through that infinite data stream and get your best possible result.",
            "So, well, sure, that sounds like a great goal.",
            "It also sounds impossible.",
            "What you know?",
            "What makes me think that that this is actually possible.",
            "Well, the reason that this is possible is actually."
        ],
        [
            "Very simple at heart.",
            "If you think about, for example, the problem of predicting who's going to win the next presidential election, right?",
            "We of course don't need to ask all 200 million voters to know who the winner is going to be.",
            "We need, you know, only a sample of a few 1000.",
            "And the size of sample that we need.",
            "Is independent of the size of the population.",
            "You can do the same thing in the US or in China, or in a very small country.",
            "Doesn't really matter the size of sample that you need only depends on the amount of uncertainty that you're willing to tolerate.",
            "OK, now of course, that's for estimating one thing who will be the winner of the next election?",
            "Or one mean of a variable or something like that?",
            "The whole trick is going to be to generalize this idea to models that have very complex structure that have thousands or maybe millions of parameters.",
            "We want to be able to at the end of this like I've just learned this model with millions of parameters on a huge data stream, but I can guarantee that this model is within epsilon of what I would have gotten if I had infinite space and time to to learn.",
            "And of course, there's a very important condition here, which is the data in this stream must be in random order.",
            "If there in the stream is not in random order, then you know if if there is going to be different in the future.",
            "I can't do anything correct without seeing it, but we will see a little bit later what we can do about the case where there is not in random order, but for the moment, let's assume that the data is in the random order.",
            "And so let's say for example that we want to."
        ],
        [
            "One MLN in this way.",
            "So when learning and let the data is a relational database, right?",
            "One of the fits benefits of Markov logic networks is that your data doesn't have to be a single table anymore.",
            "It can be as many tables as you want, and you know the logical rules will do the joins and you can build a model of the whole database at once.",
            "So your data is a relational database, and since this is nessun salaga model, of course we want to learn it, let's say by the standard method of maximizing likelihood.",
            "Now, unfortunately for logging in models in general, the likelihood cannot be optimized in closed form, so we have to do something like gradient descent OK. Now the good thing is that there's a.",
            "There's a global optimum and there are no local optimum, so you know so we don't have the kinds of problems that we have when learning something like, say, and you will network by back Prop, where we get stuck into into local minima lot so we can do gradient descent.",
            "So the first thing is to see you know what is the gradient, right?",
            "The gradient is the derivative, so it's going to be a vector, right?",
            "So the partial derivatives, the partial derivative of each component is going to be the partial derivative of the log likelihood.",
            "So the log of the probability.",
            "With the expression that we saw earlier on the derivative of that with respect to the weight and the derivative of the log likelihood respected the item, it actually has a very intuitive meaning.",
            "Which is going to be very useful.",
            "In what follows, it's just the difference between the number of two instances of the ruling, the data.",
            "So for example, how many pairs of people you know one was the influence or the other one was influenced, then they both bought.",
            "So the number of two instances according to the data.",
            "So this is the empirical count of the feature and the expected count according to the model.",
            "If the MLN predicts that the rule is going to be true less often than it really is, then it's weak needs to go up.",
            "If it's predicting that it's true very often, but isn't the way it needs to go down, and when they all line up, we're done.",
            "The model is making you know the most accurate predictions that it can, and we've we've maximized the likelihood.",
            "This is for learning weights.",
            "We can also and we do learn the structure.",
            "Learning this structure means trying different ethnicities for the rules and you know keeping the ones that must improve the likelihood and and so on.",
            "OK, so we can do that using radius search or various other kinds of of discrete search."
        ],
        [
            "OK, so remember what we want to do is take this notion of I only need a sample.",
            "To the problem of learning a whole MLN OK. Or a whole logging in model?",
            "And how do we do that?",
            "Well, the basic tool that we're going to use is the one that we could also already have used for these small problems, which is first of all these models, right?",
            "And this is true of all logging your models.",
            "They only depend on the data through the sufficient statistics.",
            "In this case, it's the counts how often it truly stream the data, but it could be things like the means and the covariances of your continuous variables and what not.",
            "Once you know those sufficient statistics, you can throw away the data.",
            "So the whole point is going to be to estimate to have enough data to estimate the sufficient statistics well enough that your overall model winds up being within the bounds of error that you're willing to allow, and our basic tool for doing that is going to be a very widely used one in statistics and machine learning, which is the half thing bound the offering bound says that with probability at least one minus Delta, so with high probability, because Delta is going to be a small number, the true frequency of some event Pi, like you know, like you know, the number of people who buy, for example.",
            "Is going to be within epsilon of the empirical estimate, P hat and P hat is just NI over and the size of your sample an East is bounded by the square root of this log of two over Delta divided by 2 N. OK, now there are two important things about this Formula One is that the Delta is inside the log and that is wonderful news for us because it means I can drive this probability being something very very small without too much cost by linearly increasing the amount of data that I get.",
            "I get an exponential improvement in my confidence and the other one of course, is that here I have square root of N. OK, so as I increase the size of my sample my bound gets better with the square root of that sample size, OK?",
            "So this is the basic thing that we're now going to try."
        ],
        [
            "To construct a bound for the whole model based on this and how we're going to do that well, the algorithm that we're using is gradient descent.",
            "So here's a brief review of the things that we've already seen so far.",
            "Here's the form of the logging model.",
            "Right is the normalized exponentiated some of the weighted features of this state.",
            "OK, the gradient right is just the difference between the empirical count of a feature and its expected value according to the model.",
            "And now the algorithm that we're going to use to learn these weights is actually in this line, right?",
            "This is one step of gradient descent, and we're going to do steps of gradient descent until we converge.",
            "What happens in one step of reading descent?",
            "Is that we subtract from the current weight the gradient times the learning rate OK?",
            "At one of them is probably the most widely algorithm used algorithm in the world for lending parameters right?",
            "And I'm sure all of you know it.",
            "This are.",
            "Here is the learning rate.",
            "I've divided it by N because first of all we know that the more that we have, the smaller learning rate we need to use, otherwise things would blow up.",
            "And also I'm about to take the limit of N as it goes to Infinity, right?",
            "I'm going to be interested in what happens when my N is Infinity and on my end I is also Infinity.",
            "This is why this is the green dissent that I would do if I have infinite data.",
            "What I'm interested in doing is comparing that with the case where I have finite data.",
            "Where my end and then I are finite and I want to bound the difference between the two.",
            "So how can I do that?",
            "Well I need to first of all pick some measure."
        ],
        [
            "Error, let's just say that my measure of error is the sum over all the weights over all the parameters of the difference between the weight learned with infinite data, which I'm calling WI and the weight learned with finite data, which I'm going to call W hat I OK and what I want to minimize the sum of these.",
            "Notice that this is a more stringent criterion than likelihood.",
            "I'm not just requiring that the Model B is likely as the infinite data one.",
            "I'm requiring that they actually look the same.",
            "They have pretty much the same weight, so that my interpretation.",
            "You know the actions that I think based on a model.",
            "My understanding will be the same.",
            "OK and now what I want to do is I want to bound this difference as I go through my steps of gradient descent.",
            "Well, let's start with initialization well.",
            "At initialization time, the error is 0 because you know I initialize the weights to the same value in both cases, OK, zero or whatever, but you know they're the same, so there's no error.",
            "Now let's look at what happens in the first step, right?",
            "Whoops.",
            "Now, in the first step, right, the weights from the previous iteration are the same 'cause there's the initial weights and therefore the expectations based on them are also the same, so there's no error there.",
            "The only error comes from the fact that in one case I used my finite NI over North and in the other case I use my infinite one, which is the real Pi.",
            "OK, so in the first iteration my error is just the sum over the parameters of the learning rate times the difference in the empirical estimates, the infinite in the find out one, and this is exactly where the huffing bound comes in.",
            "Right, we know that this is less than square root of blah blah blah.",
            "OK, so I know my error.",
            "I know the difference between my infinite data model and my finite data one after step one.",
            "Now it's instead it's in the 2nd and later step that things get interesting.",
            "So now what's going to happen here?",
            "Well, first of all, I'm going to have again looking back at this formula right?",
            "I need to have some error that comes from the previous iteration, the model, the weights already started out some distance apart and now they're going to drift apart some more.",
            "First of all, because of the error in the counts right, there's going to be that term again.",
            "But in addition, there's going to be an interesting new term, which is the difference in the expectations that I computed using finite and infinite data, right?",
            "Those are two different models.",
            "One is the model with W weights W and the other one is the model with weights W hat.",
            "They're different.",
            "So then we produce different expectations.",
            "So what I'm going to have is 3 terms, the previous one, the empirical one, and the expectation one.",
            "These ones we already now, So what remains is to take care of this one.",
            "Now if you look at this model right for him, it's not hard to see that if I have a bound on the some of the errors in these weights, which we call epsilon, then the infinite data expectation before we or any probability before we normalize it is going to be within a factor of epsilon.",
            "The error epsilon of the empirical one that I measured.",
            "So let me write that down."
        ],
        [
            "Right, and by the way, there should be ahead here in the head.",
            "Here I have a bound on the probabilities that I would compute from the ideal, you know, infinite data model and now based on that bond you can.",
            "You can come up with a bound for the expectation error, which is this slightly complicated expression here.",
            "And finally you have an error, an expression for the error in the in the East step in any step of gradient descent, and ignoring the details.",
            "What is what this says is that as I do one step after another.",
            "Even dissent, right?",
            "My error goes up by first of all, a constant amount which has to do with the data, and then this fraction, which because you know we're operating very close to 0, this exponential is approximately linear.",
            "And So what this means is that this whole thing is increasing roughly linearly with the number of iterations, which is pretty good.",
            "What that means is that as I'm learning my model with finite data from the stream, the moral is gradually diverging from all that I would learn from the infinite data.",
            "But only linearly OK?",
            "And now how do I use this?",
            "Well, there's actually many ways I can use this, but the simplest way."
        ],
        [
            "It's just this I can start out by learning with some optimistic sample size by, for example, assuming that you know this, this term is zero and I just have you know the data.",
            "Turn to contend with, and if that gets me to the end and the bound is still within what I want, right, I want.",
            "I want to be within 10% of the true model with 99% probability, and I know whether I am as I think separating dissent.",
            "If I reach the end that way, then fine, if not all I do is I restart with double the data size.",
            "I doubled the sample and I start again.",
            "And at the end of the day, in the worst case I will have done worse by a small factor, but in the best case, right?",
            "What's going to happen in Journal is that I used some fixed amount of data and then I turned off the faucet and I can return my model to my customer and say this is pretty much exactly the same model that I would have learned if I had learned on, you know, no matter how large that it is that we have or we going to have.",
            "So the time and space really are going to be independent of the data size, because I never even look or store or do anything with the data that is beyond what I need.",
            "OK, now those of you familiar with learning theory might be running well.",
            "We know that this thing looks a lot like you know PAC bounds and pack pounds are notoriously loose.",
            "Well, what I'd like to point out is that these bounds I'm much much tighter than PAC Bounds PAC bounds depend on the whole size of the class of models that you're considering.",
            "And so they're extremely pessimistic, to the point where they seldom practical.",
            "These bonds are very different because they're based on the actual algorithm that you're running the based on looking at step by step what the algorithm is doing, so you know there's still pessimistic, like for example, huffing bounds are more pessimistic than normal bounds, which you could use instead and speed things up that way.",
            "But bottom line, there actually tight enough to be useful in practice, and when we've we've used things like this in practice, we you know what happens is that up to some point, we don't get any gains and then we get to that point, which for example might be in the millions of examples, and then we're done.",
            "And then if you didn't have 10,000,000 examples, you've just spent things up by an order of magnitude.",
            "If you did ahead 100 million examples, you spread things up by two orders of magnitude and so on.",
            "OK, so this can potentially let you do with a single CPU?",
            "Well, that what otherwise you might need thousands of CPU's to do the same kinds of ideas can be applied to structural learning.",
            "Here I just showed how you would do this for learning the parameters, But if you want to learn the structure of the actual features then you could do the same type of treatment.",
            "Essentially what has to happen there is that you need to divide your Delta by the number of models that you've tried.",
            "The more models you try, the smaller your Delta for each one has to be such that when you add them all up over all the models that you tried and all the different steps, you still have this bond for only type bound on your on your overall error."
        ],
        [
            "Now, what happens if the data is changing overtime?",
            "Because most of the time it is changing overtime and often is the change over the time.",
            "That's the most interesting.",
            "Well in that case, what you do is you maintain a sufficient statistics over a sliding window.",
            "OK, so as your data comes through right, you keep maintaining the sufficient distance for for the most recent window of data, and you compare that.",
            "The current statistics with the ones that the model was learned on and if it some point the difference between the current statistic and the historical one exceeds abound then you re learn based on based on the new values.",
            "OK so you can have any at any point or more that makes as much use as possible of the past while changing as the new data demands.",
            "OK, so let me just mention some of the applications of this that we've done to date with of course the NAP."
        ],
        [
            "Locations to viral marketing, where we saw that indeed, you know, marketing to the influences can be orders of magnitude more effective than marketing to just anybody.",
            "We've built large and we're building extracting large knowledge bases from the web using Markov logic.",
            "We've done, you know we and others have done applications of these ideas in object recognition, web caching, semantic parsing, and many others, and the various things that I've talked about here.",
            "We actually have open source implementations for them.",
            "Which you can find there's pointers to them.",
            "From my homepage they include things like the alchemy implementation of Markov logic, sum product networks which implement the same product theorem, very fast machine learning, which is a toolkit for turning algorithms into streaming algorithms in the way that I described here and so forth.",
            "So to summarize."
        ],
        [
            "We are entering the era of very large scale modeling and what I did in this talk was scratched the surface of what we need to do to start building these very large scale models.",
            "You know, as opposed to the large scale models that we built today and we saw three principles.",
            "One is more of the whole, not just the parts and one way in which you can do that is with Markov logic networks.",
            "The second principle was tame complexity via hierarchical decomposition, and one way we can do that is with the sum product theorem.",
            "And the third principle is that time and space should not depend on data size, and one way in which you can realize this is with streaming bound algorithms.",
            "As we just saw more on this and many other things in a book that I'm writing that I will now shamelessly plug."
        ],
        [
            "It's a Popular Science book on machine learning, so the idea is to introduce things like data science and all the principles and ideas that we know and love to a broader audience.",
            "It's called the Master algorithm, machine learning and the Big Data Revolution, and it's going to come out next year.",
            "And with that I would like to thank you all for being here, and let's make the next 20 years of KDD even better than the last 20.",
            "Thank you.",
            "You assume that.",
            "Number of parameters you have in your model is always fixed.",
            "It does not increase with more data, but that is often not the case, right?",
            "I mean, we've seen that with more streaming data you get new customers and with new customers you have new parameters and so the idea that the number of parameters will remain fixed as you get more and more data is not true.",
            "Well, to clarify, in this simple example, I assume that the number of parameters was fixed, but in reality we don't have to assume that the parameters I fixed what happens is that if you introduce a new parameter into the model, you have to add that to the bonds and start treating it the same way as the others.",
            "So it's actually not, and in fact, when we do this for structure learning right, which we do as I add structure, I often also add parameters like.",
            "For example, I may learn new features and the bounds still apply.",
            "They get worse of course, because I know I have more parameters, but that's actually not a problem.",
            "I have a question regarding the last part.",
            "So you started by assuming that the constraint was on CPU time and memory, but it looked like at the end you were really constraining yourself in terms of the number of examples you can use.",
            "Yeah, it seems that if the constraint is really on CPU time and no constraint on the amount of data you can use instead of using less data, you should be using more data for the same CPU and not cycle through your examples many times.",
            "OK, so again, let me clarify this right.",
            "This is a good question.",
            "In what I said here, actually mixed up those two, so let me give it because what happens in, for example, for something like gradient descent, there is a direct correspondence between the number of data points that I have and the amount of memory that I use.",
            "A number of CPU that I use, but the more general treatment, and in some ways the more interesting and sophisticated application of this which we have done is where you do the following.",
            "What I'm going to do is I'm going to explicitly minimize the CPU time I write down a function for the CPU time as a function of the number of data points that I use at each step.",
            "And now what I do is I use LaGrange multipliers to minimize the CPU time if the CPU time is a linear function of the data, then you get this very simple case, but in general it could actually be different.",
            "I could.",
            "I could for example, for example, we've applied this to things like him and K means where we wind up using different numbers of examples at different stages in the algorithm.",
            "Because there's different tradeoffs between, for example a quadratic cost and how much that air blows up if the erables up a lot in succeeding iterations.",
            "I want to I want to have a lot of data there if in succeeding iterations there is going to die down to zero, then actually.",
            "I can be very loose there, so your question is a good one, but actually you know in truth we can actually do this deliberately for maximizing for minimizing the computational complexity.",
            "Even you can include the stochastic gradient case and your analysis this drastically and I was wondering if this is what you had in mind.",
            "Of course we can, so we haven't studied the stochastic gradient case, but let me let me tell you what I think right.",
            "We can certainly include it here, right, but the question is whether it would be useful.",
            "I think you know, here's one way to think about this, like how large should your mini batch B?",
            "Right, this is 1 very pragmatic question, right?",
            "And you could use this to set mini batch sizes, but this wouldn't.",
            "You know the mini batch size that people use are smaller than what you actually using here, so I actually don't have a very definitive answer.",
            "Also, I think that for learning things like say you know deep models, right?",
            "Just a random example.",
            "I think in the kinds of applications that people this is.",
            "Again, just my suspicion, but you know things like continents for image net, the data there is so rich, right?",
            "That I would guess that even with all of image Now we still have not reached the point where this becomes where you want to stop.",
            "So maybe there is no need for it there yet, but there probably will at some point.",
            "Sorry to be half my hand like this, but the light is very bright and I want to see this gradient thanks better.",
            "Thanks for the wonderful dog.",
            "Yeah we do.",
            "An honor to Pedro for a standing ovation as the most innovative perform."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Principles of very large scale modeling.",
                    "label": 1
                },
                {
                    "sent": "But before that.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, I'd like to say that I'm very honored by this award and humbled.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If by any chance my research to date falls short of this award, I promise to make up the deficit with my future research.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's it's customary in these things to start by thanking your students and collaborators.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to do that for a very simple reason, which is that this award is really for them as much as it is for me.",
                    "label": 0
                },
                {
                    "sent": "So what I would like to do is, on behalf of all of us.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank everyone who made this possible this adventure that we've been on, including of course our families, our friends, our funders, and everybody else in this community.",
                    "label": 0
                },
                {
                    "sent": "And on that note, I would like to date.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kate, this talk to the memory of my father, who was a distinguished scientist and died last month.",
                    "label": 0
                },
                {
                    "sent": "So my talk is entitled principles of.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Large scale modeling.",
                    "label": 0
                },
                {
                    "sent": "So the first thing that I would like to do is say what I mean by that, and then essentially the talk will have three parts, each of which will talk about 1.",
                    "label": 0
                },
                {
                    "sent": "One such principle and let me give you a preview of what they are.",
                    "label": 0
                },
                {
                    "sent": "The first one is that you should model the whole, not just the parts.",
                    "label": 1
                },
                {
                    "sent": "The second one is that you should tame complexity via hierarchical decomposition, and the third one is that time and space should not depend on data size.",
                    "label": 1
                },
                {
                    "sent": "And of course these are only three principles and this is only one talk.",
                    "label": 0
                },
                {
                    "sent": "There's much more.",
                    "label": 0
                },
                {
                    "sent": "That we that we know and that we have to learn about building very large scale models.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I would like to introduce this notion of very large scale modeling by means of an analogy.",
                    "label": 1
                },
                {
                    "sent": "An analogy between KDD and microelectronics and more specifically between models and microchips.",
                    "label": 0
                },
                {
                    "sent": "So in VLSI right there was a pre history where there were no integrated circuits so people they had single components.",
                    "label": 0
                },
                {
                    "sent": "And then there was the, you know, the integrated circuits were invented and for a while they were pretty small.",
                    "label": 0
                },
                {
                    "sent": "They had 10s of components, maybe hundreds.",
                    "label": 0
                },
                {
                    "sent": "And then there was the air of large scale integration, which was roughly the 70s where you got into the, you know hundreds, thousands, 10s of thousands.",
                    "label": 0
                },
                {
                    "sent": "And now you know around 1980 there was a transition to the aerial view of the error VLSI very large scale integration, which is where we are today and we now have billions of components on the chip and and will have more the next decade.",
                    "label": 0
                },
                {
                    "sent": "And there's an interesting parallel between this and what has happened in KDD.",
                    "label": 1
                },
                {
                    "sent": "So likewise there was a pre history where what we have was descriptive statistics.",
                    "label": 0
                },
                {
                    "sent": "You know, just histograms or you know means and variances and things like that.",
                    "label": 0
                },
                {
                    "sent": "And then you know pre 1995 which is actually when I started my working in machine learning.",
                    "label": 0
                },
                {
                    "sent": "You know what we were building then were small models things with you know 10s or hundreds of variables and maybe hundreds or thousands of examples and and then.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like I I'm dating this from the first Kitty conference in 1995, which is also the first one that I attended.",
                    "label": 0
                },
                {
                    "sent": "You know, in the last 20 years, I think we've been in the air of building large models.",
                    "label": 0
                },
                {
                    "sent": "Which are vastly larger than what we were doing back then, and they continue to get larger.",
                    "label": 0
                },
                {
                    "sent": "And So what I believe is that we have now begun the transition to the era of very large scale models, which I think will last for the foreseeable future.",
                    "label": 0
                },
                {
                    "sent": "Now the interesting thing about these transitions, both in the in the case of microchips and in the case of models, is that it's not just the scale that's going up, there's actually much more than that going on when the skill changes by that much, the change becomes qualitative.",
                    "label": 0
                },
                {
                    "sent": "As well as quantitative in the case of VLSI in particular there was this huge transition from from large scale integration to very large configuration, which was, you know, the landmark was the publication of this book, which many of you probably know by Carver Mead and Lynn Conway, introduction to VLSI Systems which form the foundation of how people do VLSI today.",
                    "label": 0
                },
                {
                    "sent": "And there's a huge difference between the before and after.",
                    "label": 0
                },
                {
                    "sent": "So before what people did when they design integrated circuits was to wire gates 1 by 1.",
                    "label": 1
                },
                {
                    "sent": "And the design was tide to the fabrication.",
                    "label": 0
                },
                {
                    "sent": "You will read you are literally designing how things were going to be placed on the Silicon die so that it could then be manufactured, but that doesn't scale to building circuits with billions of transistors, and so that's what happens now in the vast areas that nobody wires gates anymore.",
                    "label": 0
                },
                {
                    "sent": "What you do is you combine high level modules.",
                    "label": 0
                },
                {
                    "sent": "Each of these modules performs often a very complex, very large function and the design is actually quite independent of fabrication.",
                    "label": 1
                },
                {
                    "sent": "Those models that some of those modules at some point compiled down to Silicon in using what's called a Silicon compiler, and that's where the gates come out of.",
                    "label": 0
                },
                {
                    "sent": "But the engineers don't think in terms of gates.",
                    "label": 0
                },
                {
                    "sent": "The design is independent of the fabrication, and you know what you use for designing things like Cat Tools, computer aided design tools and hardware Description language where you describe at a high level what it is that you want to build, and then it gets executed because if we didn't do things like this, we would actually not be able to do them at all.",
                    "label": 0
                },
                {
                    "sent": "Nobody can design A chip with a billion transistors one at a time.",
                    "label": 0
                },
                {
                    "sent": "And what I believe is that today.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "KD D is undergoing a similar transition.",
                    "label": 1
                },
                {
                    "sent": "Not just quantitatively in the scale of things that we're doing, but qualitatively.",
                    "label": 0
                },
                {
                    "sent": "The kinds of things that we're starting to do, and that I think you know, we're going to do more of in the future.",
                    "label": 0
                },
                {
                    "sent": "So for example, you know so here what I have in this table is like on the left large models of the kind that we've built for the past 20 years, and on the right I have what what I'm calling very large scale models.",
                    "label": 0
                },
                {
                    "sent": "An for example, you know we all you know we do a lot of building models of customers, but something that we're starting to do a lot of this building models of the whole social network that the customers are part of.",
                    "label": 0
                },
                {
                    "sent": "In biology for a long time we build models of individual genes and proteins, but now we're starting to build models of whole metabolic pathways and eventually we want we want to build models of whole cells, even a whole organisms in neuroscience, for example, we've done a lot of models of single neurons, but you know we're starting to model larger circuits with neurons and eventually want to model the whole brain.",
                    "label": 0
                },
                {
                    "sent": "For example, in the case of cities, you know we have models of a single service like electricity or water.",
                    "label": 0
                },
                {
                    "sent": "Or transportation, but what we want to have it in today's the model of a whole whole city works.",
                    "label": 0
                },
                {
                    "sent": "All these services together with the people that use them.",
                    "label": 0
                },
                {
                    "sent": "And this applies to them and what not that we can then optimize, going, for example, from single species to whole ecosystems going from the atmosphere, which is really just differential equations to all those things, influence the climate going from individual vision tasks like object recognition to building whole vision systems with all of the components working together.",
                    "label": 0
                },
                {
                    "sent": "To actually provide vision language going from things like parsers and part of speech taggers and into resolution to whole systems that do all the parts on these parts actually work together and understand text.",
                    "label": 0
                },
                {
                    "sent": "This is I think where we're, you know we're starting to go and you know one last example is recommender systems.",
                    "label": 0
                },
                {
                    "sent": "So today we have very good recommended systems that for example, recommend books to you or they recommend DVD's or they recommend some range of products.",
                    "label": 0
                },
                {
                    "sent": "But what we would really like to have.",
                    "label": 0
                },
                {
                    "sent": "Assistance that have a 360 degree view of you.",
                    "label": 0
                },
                {
                    "sent": "They understand who you are, what your tastes are, what you've been doing, where you're going, what you want, and they do their recommendations for all of the things that you want based on everything that they know about your life.",
                    "label": 0
                },
                {
                    "sent": "And my basic contention is that this is one world that we know pretty well at this point, but this is the world where we're going.",
                    "label": 0
                },
                {
                    "sent": "The biggest contribution to society that we can make is in doing these things.",
                    "label": 0
                },
                {
                    "sent": "But doing these things requires a whole different set, sorry excuse.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A whole different set of ideas from the ones that we have right now there are.",
                    "label": 0
                },
                {
                    "sent": "There's a host of new problems just like in VLSI.",
                    "label": 0
                },
                {
                    "sent": "There was a host of new problems compared to LSI, and So what I would like to do in this talk is mentioned three principles for building these very large scale models.",
                    "label": 1
                },
                {
                    "sent": "Of course there are many more.",
                    "label": 1
                },
                {
                    "sent": "I'm only going to touch on a few and I will illustrate those principles with examples from my research.",
                    "label": 0
                },
                {
                    "sent": "Some of it old and some of it knew.",
                    "label": 0
                },
                {
                    "sent": "So the first principle.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a model the whole, not just the parts?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what do I mean by that?",
                    "label": 0
                },
                {
                    "sent": "Let me illustrate with this, with with with an application that I'm going to use as a running example, which is social networks.",
                    "label": 0
                },
                {
                    "sent": "Social networks are definitely one area where this transition from large models to very large models is well underway.",
                    "label": 1
                },
                {
                    "sent": "And let's suppose that you have a new product that you want to sell and you have to decide what customers to market it to.",
                    "label": 0
                },
                {
                    "sent": "The traditional way to do this is to build a model of each customer.",
                    "label": 0
                },
                {
                    "sent": "For example, you may know.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What she clicked on you may know some profile information that you provided demographics that you got from another source and based on this and many other things you will predict some probability that she will buy the product.",
                    "label": 0
                },
                {
                    "sent": "And then there's one more input which is your marketing.",
                    "label": 0
                },
                {
                    "sent": "What you want to predict is given these things.",
                    "label": 0
                },
                {
                    "sent": "If now I market how much does that increase your probability of buying?",
                    "label": 0
                },
                {
                    "sent": "And based on that we will decide to market to her or not but when we do this we're actually doing it for.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Each customer in isolation.",
                    "label": 0
                },
                {
                    "sent": "And that's missing something very important, which is that customers influence each other.",
                    "label": 1
                },
                {
                    "sent": "People's buying decisions are not.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Buying decisions, but decisions on what to do.",
                    "label": 0
                },
                {
                    "sent": "What causes to contribute to what things to get engaged in, whether to smoke or not smoke.",
                    "label": 0
                },
                {
                    "sent": "You know, all these things.",
                    "label": 0
                },
                {
                    "sent": "Very often the single biggest influence on your decisions is your friends, your coworkers, your acquaintances, and if we treat this as a bunch of independent units were missing that picture.",
                    "label": 0
                },
                {
                    "sent": "So we need to take these things into account.",
                    "label": 0
                },
                {
                    "sent": "We need to model the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Whole of the social network, we need to model the effect of the marketing or of whatever action you're taking on the whole network, not just on the individual in isolation.",
                    "label": 1
                },
                {
                    "sent": "'cause if we don't, we're going to miss the forest for the trees, not to mention probably waste a lot of money and not get the results that we would like to get.",
                    "label": 1
                },
                {
                    "sent": "Now, if we believe in this principle that we want to model the whole and not just the parts, the next question is how are we going to do that?",
                    "label": 0
                },
                {
                    "sent": "And this is where we come into a very big problem, which is also, I think, a large part of the reason why it hasn't been done before is that the traditional statistical models that we all know and love are really not applicable here.",
                    "label": 0
                },
                {
                    "sent": "The reason they're not applicable is that they assume that samples are independent.",
                    "label": 0
                },
                {
                    "sent": "Now we make this assumption because it makes life much easier, but it happens to be a false assumption.",
                    "label": 0
                },
                {
                    "sent": "And now if you want to remove that assumption, the question is what can we do?",
                    "label": 0
                },
                {
                    "sent": "Well, there's ad hoc things that we can do and that people do, But the problem is, I thought methods is that they are geared to one particular problem in one particular context, and they won't generalize.",
                    "label": 0
                },
                {
                    "sent": "And also, even in that context, you know they tell you something to do, but you have no notion of how good it is.",
                    "label": 0
                },
                {
                    "sent": "Or whether you could be doing better.",
                    "label": 0
                },
                {
                    "sent": "So we would like to be able to do something more than this, and indeed we can.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One way that we can do this is by you by using what are called Markov logic networks and the idea in Markov logic networks starts with the following observation.",
                    "label": 1
                },
                {
                    "sent": "There's we actually know how to represent interactions between entities in the world.",
                    "label": 0
                },
                {
                    "sent": "We have logic as the language to do that.",
                    "label": 0
                },
                {
                    "sent": "For example, if we want to say that you know people influence each other, we could write a rule in logic of the following form buys X one and influences X.",
                    "label": 0
                },
                {
                    "sent": "One X2 implies that buys X2.",
                    "label": 0
                },
                {
                    "sent": "This means that if I buy something and I influence you, then you will also bite and this is a general rule right where these variables can then be replaced by specific people in the world.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I replace X1 by Anna and next to buy Bob, you know this becomes one specific instance, which is that if Anna buys and she influences Bob, then Bob buys.",
                    "label": 0
                },
                {
                    "sent": "So representing this is actually not that hard, but there's a very big problem, which is that logical rules like this are all or none.",
                    "label": 0
                },
                {
                    "sent": "Now, this really saying that if an advising she influences Bob and then he must buy as well, and we know that this is not showing the real world effect influences Bob.",
                    "label": 0
                },
                {
                    "sent": "What that does is it increases the probability that Bob will buy, which is very different from water logic.",
                    "label": 0
                },
                {
                    "sent": "Rule says logic can't model this kind of uncertainty.",
                    "label": 0
                },
                {
                    "sent": "This kind of graded behavior.",
                    "label": 0
                },
                {
                    "sent": "So what can we do?",
                    "label": 0
                },
                {
                    "sent": "Well, one thing we can do is treat these logical formulas not in the usual way, but as feature templates for a log linear model, right log in your models are you know what.",
                    "label": 0
                },
                {
                    "sent": "Most of the models we use are also known as Markov networks, which is where the name Markov logic networks comes from.",
                    "label": 0
                },
                {
                    "sent": "So let me briefly review what they are and then how we can how we can go beyond them to something like Markov logic networks.",
                    "label": 0
                },
                {
                    "sent": "So a log.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your model basically says that the X is a state of the world.",
                    "label": 0
                },
                {
                    "sent": "Like say, some people buying a product in some, not buying it and it says that the probability of a state is a function of some features of that state, which can be whatever you think is important or can be learned from data.",
                    "label": 0
                },
                {
                    "sent": "And let's think of the features is Boolean for the purpose of this talk, but they could be continuous and the way we compute the probability of a state is we take each feature inside the zero or one we multiplied by await so features with a higher weight are more important.",
                    "label": 0
                },
                {
                    "sent": "And then we exponentiate that and we normalize it to turn it into a probability, right?",
                    "label": 0
                },
                {
                    "sent": "This ensures that the awesome to one OK, and this is very well known in statistics and in machine learning.",
                    "label": 0
                },
                {
                    "sent": "We know how to use and learn these things very well.",
                    "label": 0
                },
                {
                    "sent": "What we want to do is we want to generalize this beyond the independent context where they usually used to something broader and the way we can do that is by turning the rules that we saw before into templates for these features.",
                    "label": 0
                },
                {
                    "sent": "So each rule can become a set of features, one for each set of objects that the rule applies to.",
                    "label": 0
                },
                {
                    "sent": "OK, so for example, using using the rule that we saw before, right?",
                    "label": 0
                },
                {
                    "sent": "This is going to become one feature for each pair of people, including one feature for Anna and Bob and now and now, since N influences Bob, what this means is that if an influences Bob in both by then these features going to be true, which means that the probability of the world is going to go up OK, so when a lot of a lot of features with a high weight are true.",
                    "label": 0
                },
                {
                    "sent": "Then the world is very probable.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, it's improbable.",
                    "label": 0
                },
                {
                    "sent": "So now we can both represent the rich relationships among objects, but also the fact that there's statistical.",
                    "label": 0
                },
                {
                    "sent": "They're not deterministic.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Here's a very simple example Markov logic network for viral marketing.",
                    "label": 0
                },
                {
                    "sent": "There's a very simple rule that is nobody buys anything.",
                    "label": 0
                },
                {
                    "sent": "OK, most people don't buy most things right.",
                    "label": 0
                },
                {
                    "sent": "That's just life, so we priority.",
                    "label": 0
                },
                {
                    "sent": "Nobody is going to buy the product and so this is what this rule says that you know, for any given person, they probably are not going to buy.",
                    "label": 0
                },
                {
                    "sent": "Remember, this is not a deterministic cool, this is just going to have a weight which could be the same for everybody.",
                    "label": 0
                },
                {
                    "sent": "Or it could be different for different people.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "Depending on how likely we think they are a priority to buy the product and then we can have a rule that represents the effective marketing, which is that if I market to somebody then they buy and again this does not mean that they guarantee to buy.",
                    "label": 0
                },
                {
                    "sent": "This just means that the higher the weight of this rule, the more likely they are to buy, given that are marketed to them OK.",
                    "label": 0
                },
                {
                    "sent": "But so far this is just the classic setting.",
                    "label": 0
                },
                {
                    "sent": "Now you know the thing that really brings in the modeling of the whole is this rule that we saw before effects one buys and she influences X2.",
                    "label": 0
                },
                {
                    "sent": "The next 2 bytes as well, and the higher the weight of this rule, the more of an effect it will have.",
                    "label": 0
                },
                {
                    "sent": "So if any inferences babolat then for her this will have a high weight, if she influences him a little then this will have a low weight.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of course, this is just a bare bones MLN.",
                    "label": 0
                },
                {
                    "sent": "In reality, what we want to do is build something much richer using a lot more formulas in the corresponding features.",
                    "label": 0
                },
                {
                    "sent": "We want to include things like customer and product attributes and how they affect the probability of buying things like the cost of marketing, the prices that discounts, the profits we will in general have multiple relations, so you could have friends influencing each other or coworkers influencing each other.",
                    "label": 1
                },
                {
                    "sent": "We can have multiple types of entity, like for example if the product is a drug you want to take the patients into account, but also the doctors.",
                    "label": 0
                },
                {
                    "sent": "The hospitals, the health insurance companies, and so forth.",
                    "label": 1
                },
                {
                    "sent": "You may have a choice of different marketing actions.",
                    "label": 0
                },
                {
                    "sent": "You know different things that you can do.",
                    "label": 0
                },
                {
                    "sent": "Different amounts that you can spend.",
                    "label": 0
                },
                {
                    "sent": "You can place an ad you can you know, give a discount, and so forth.",
                    "label": 0
                },
                {
                    "sent": "Very important one is time right?",
                    "label": 0
                },
                {
                    "sent": "You may want to intervene in this process as it involves overtime.",
                    "label": 0
                },
                {
                    "sent": "I market it.",
                    "label": 0
                },
                {
                    "sent": "Some people bought what is now the next set of people that I want to market to.",
                    "label": 0
                },
                {
                    "sent": "You may be doing this for multiple products at the same time because you may have a choice of doing this product versus that.",
                    "label": 0
                },
                {
                    "sent": "You may have this with multiple companies competing, and I are actually playing this game.",
                    "label": 0
                },
                {
                    "sent": "You know, if I do this, what will they do?",
                    "label": 0
                },
                {
                    "sent": "And all of these things we can and have written down in logic and then we learn weights for them and then this becomes a statistical model and there's of course many other things that you can imagine adding, but hopefully at this point you know you have the more less of an idea of.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How how we can do this?",
                    "label": 0
                },
                {
                    "sent": "Now the next question is OK, so let's say we built like this.",
                    "label": 0
                },
                {
                    "sent": "Great, you know, beautiful model of the whole system.",
                    "label": 0
                },
                {
                    "sent": "In this case the social network.",
                    "label": 0
                },
                {
                    "sent": "Then how do we use it?",
                    "label": 1
                },
                {
                    "sent": "Well, the way we would like to use a model like this is to figure out what is the best set of people to market to.",
                    "label": 0
                },
                {
                    "sent": "Right ideally would like I would like to do is find a relatively small set of people to market this so my marketing cost is low, but they have a lot of influence in the network, both direct and indirect, such that I get a lot of people buying because of those few.",
                    "label": 0
                },
                {
                    "sent": "So I maximize my bank for the Buck.",
                    "label": 0
                },
                {
                    "sent": "Well, how do I do that?",
                    "label": 0
                },
                {
                    "sent": "Well, for example I can choose an initial set of customers to market 2, and then I use my MLN to compute the expected number of buyers, right?",
                    "label": 1
                },
                {
                    "sent": "This is probabilistic, but in expectation how many people will buy?",
                    "label": 0
                },
                {
                    "sent": "Given those people that are marketing to and their effect on the network, and then what I do is I try variations of that.",
                    "label": 0
                },
                {
                    "sent": "I said well if I add this person, how does that change the expected number of people who buy?",
                    "label": 0
                },
                {
                    "sent": "What if I add that one and I keep on doing this kind of greedy search until I can't improve it anymore.",
                    "label": 0
                },
                {
                    "sent": "OK, now this all sounds very good and in fact we did this in the paper in KDD over 10 years ago, which was actually a precursor to the paper by, you know Kleinberg and Tardos and you know, like the.",
                    "label": 0
                },
                {
                    "sent": "Champion on on influence maximization.",
                    "label": 0
                },
                {
                    "sent": "The big problem, however, is that this process of computing the expected number of buyers in the in the MLN is extremely intractable, right?",
                    "label": 0
                },
                {
                    "sent": "So this sounds good in principle, but doing it in practice on a large scale is very difficult.",
                    "label": 0
                },
                {
                    "sent": "So let's see why it's difficult, and.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we can do about it?",
                    "label": 0
                },
                {
                    "sent": "Well, let's say for example, that we have in our social network here, and we've persuaded Anet to bite.",
                    "label": 0
                },
                {
                    "sent": "So any bites?",
                    "label": 0
                },
                {
                    "sent": "And now we want to know how much is that going to change Zacks probability of buying right?",
                    "label": 0
                },
                {
                    "sent": "And if we can do this for everybody, we're in good shape.",
                    "label": 0
                },
                {
                    "sent": "That's what we want, and then we can evaluate our strategies that way.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that to evaluate the effect of an on Zach, we have to propagate her influence through the network and notice that this would still be a problem even if Anna was in a direct influence of, say, Bob here, 'cause there would be the direct influence.",
                    "label": 0
                },
                {
                    "sent": "But there will also be many paths of indirect influence, and those also count.",
                    "label": 0
                },
                {
                    "sent": "So one way or another, we have to propagate the influence through the network and then what happens is like for example we have to see what.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "M's effect is on these two people.",
                    "label": 0
                },
                {
                    "sent": "Let's say there Bob and Chris, right?",
                    "label": 0
                },
                {
                    "sent": "But the two people and they're not independent, right?",
                    "label": 0
                },
                {
                    "sent": "And they have four possible states.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "Like you know, Bob Bias and Chris doesn't, and so forth, right?",
                    "label": 0
                },
                {
                    "sent": "So there's four possible states, each one of them has a different probability.",
                    "label": 0
                },
                {
                    "sent": "And then there's the probability of the Zach bias given each of these States and then those two people in turn.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Influence these four.",
                    "label": 0
                },
                {
                    "sent": "So now I have two of the four States and I have to sum over all of them.",
                    "label": 0
                },
                {
                    "sent": "You know bye bye bye, not bye.",
                    "label": 0
                },
                {
                    "sent": "What's the probability of that times the probability that Zach then buys?",
                    "label": 0
                },
                {
                    "sent": "Given these people, something that overall states OK?",
                    "label": 0
                },
                {
                    "sent": "So if I have so the cost of my influence is going to be exponential in the width of the network.",
                    "label": 1
                },
                {
                    "sent": "So if my network has 1000 people with which is actually not that much, this means that it would need two to the thousands.",
                    "label": 1
                },
                {
                    "sent": "I have a cost of order of two to the 1000.",
                    "label": 0
                },
                {
                    "sent": "To actually compute the influence of Anna on Zack.",
                    "label": 0
                },
                {
                    "sent": "And obviously that's not going to happen OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the question is, what can we do?",
                    "label": 0
                },
                {
                    "sent": "And this is where the second principle.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It comes in, which is tame complexity by hierarchical decomposition.",
                    "label": 1
                },
                {
                    "sent": "OK, in general in the most general case the problem is NP complete.",
                    "label": 0
                },
                {
                    "sent": "There's nothing we can do, but the real world is not the most general case.",
                    "label": 0
                },
                {
                    "sent": "The real world has hierarchical structure.",
                    "label": 0
                },
                {
                    "sent": "Now when you look at a principle like this, you might think, well, sure motherhood and Apple pie, right?",
                    "label": 0
                },
                {
                    "sent": "Why wouldn't you do hierarchical decomposition, right?",
                    "label": 0
                },
                {
                    "sent": "And then?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fact, it's where everybody does.",
                    "label": 0
                },
                {
                    "sent": "It's what they do in VLSI.",
                    "label": 0
                },
                {
                    "sent": "In fact, one of the main things that the meeting Conway book introduced was this notion of hierarchically.",
                    "label": 0
                },
                {
                    "sent": "You know, having this hierarchy of modules and sub modules and submodules with this specific devices at the end.",
                    "label": 0
                },
                {
                    "sent": "And of course it's what people do in programming.",
                    "label": 0
                },
                {
                    "sent": "Right when we write programs, we use hierarchical decomposition, but you should remember that you know back in the day, people wrote programs using go to statements and they were big bowls of spaghetti.",
                    "label": 0
                },
                {
                    "sent": "And then Dykstra wrote a paper saying, you know.",
                    "label": 0
                },
                {
                    "sent": "Two statements considered harmful, and that's why we do structured programming today.",
                    "label": 0
                },
                {
                    "sent": "Now the interesting thing is that everybody and you know, scientists you know.",
                    "label": 0
                },
                {
                    "sent": "Engineers use hierarchical decomposition.",
                    "label": 0
                },
                {
                    "sent": "We are probably the only ones.",
                    "label": 0
                },
                {
                    "sent": "Who don't do it?",
                    "label": 0
                },
                {
                    "sent": "But when you look at most models that people build, even when they're very, very large, they're not hierarchical with issue exceptions, right?",
                    "label": 0
                },
                {
                    "sent": "For the most part, our models are not hierarchical.",
                    "label": 0
                },
                {
                    "sent": "And this is interesting, right?",
                    "label": 0
                },
                {
                    "sent": "So you could say, well, you know that's because there hasn't been that much need so far, right?",
                    "label": 0
                },
                {
                    "sent": "If you're just going to predict somebody's buying decision using the logistic regression or random forest, this on 1000 attributes, you don't need hierarchal with composition for that.",
                    "label": 0
                },
                {
                    "sent": "But I think if we want to move into this world with very large scale models where we model the whole not just the parts, we gotta start doing hierarchical decomposition now.",
                    "label": 0
                },
                {
                    "sent": "You could also say that well, but at at heart the phenomenon, not hierarchical, and that's why we are modeling like.",
                    "label": 0
                },
                {
                    "sent": "Think of a vision network for example.",
                    "label": 0
                },
                {
                    "sent": "It usually doesn't have hierarchical structures, just it's a big jumble of nodes and links.",
                    "label": 0
                },
                {
                    "sent": "Well with that I think I would have to disagree because the world.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It has a lot of hierarchical structure, at least as we perceive it right.",
                    "label": 0
                },
                {
                    "sent": "'cause we also have finite computational resources in our brains.",
                    "label": 0
                },
                {
                    "sent": "The world has hierarchical structure in at least two important ways.",
                    "label": 0
                },
                {
                    "sent": "The first is that.",
                    "label": 0
                },
                {
                    "sent": "The world is made of parts and subparts and sub sub parts, like for example the economy has sectors like energy media, health.",
                    "label": 1
                },
                {
                    "sent": "You know each of these sectors in turn has companies in it.",
                    "label": 0
                },
                {
                    "sent": "Like for example Disney Viacom, wandering Media and then the companies in turn have divisions like HR, it sales all the way down to individual people.",
                    "label": 0
                },
                {
                    "sent": "So the world decomposes into parts.",
                    "label": 0
                },
                {
                    "sent": "Another very important type of hierarchy is class is of course the class hierarchy, right?",
                    "label": 0
                },
                {
                    "sent": "The objects, the entities.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The world are not a random jumble of things.",
                    "label": 1
                },
                {
                    "sent": "They have similarities and you can group them.",
                    "label": 0
                },
                {
                    "sent": "So for example, humans could be Asian American, European and so forth.",
                    "label": 0
                },
                {
                    "sent": "Americans could be Californians, New Yorkers, Texans, and so on all the way down to individual people like, you know, like Bill de Blasio, Derek Jeter and you know Barbra Streisand, who are all famous New Yorkers.",
                    "label": 0
                },
                {
                    "sent": "Another point of having these kinds of hierarchy is that we can exploit them.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To make our inference tractable.",
                    "label": 0
                },
                {
                    "sent": "And I, you know, even if the world is not hierarchical, I would argue that we're better off approximating.",
                    "label": 1
                },
                {
                    "sent": "It is hierarchical.",
                    "label": 0
                },
                {
                    "sent": "Then, for example, just assuming that everything is independent because as we saw, that's really throwing out the baby with the bathwater, or just winding up with an intractable model that sure, this isn't this is this is an accurate model, but you can't get accurate results out of it, because because you can't do the inference.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's try to approximate the world using using these.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kinds of hierarchies, and there's two assumptions that we can make that will that will really help us.",
                    "label": 0
                },
                {
                    "sent": "The 1st and remember our goal here is not to just use hierarchical decomposition to you know the same way you would use in programming, but use hierarchical decomposition to make inference in one of these very large probabilistic models with very large system tractable.",
                    "label": 0
                },
                {
                    "sent": "And there are two assumptions at least that we can make related to these hierarchies that that will get us very far.",
                    "label": 0
                },
                {
                    "sent": "The first one is that subparts are independent.",
                    "label": 1
                },
                {
                    "sent": "Given the parts, and in fact psychologists will tell you that this is the assumption that human beings make all the time.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I want to predict the probability that different divisions in the company will buy, for example a particular computer for the desktops of their employees.",
                    "label": 0
                },
                {
                    "sent": "I couldn't.",
                    "label": 0
                },
                {
                    "sent": "I could model a very big dependency between all the people on the units, or I could assume that the departments within the organization are independent.",
                    "label": 1
                },
                {
                    "sent": "Given weather, the organization itself buys.",
                    "label": 0
                },
                {
                    "sent": "So the probability that, for example the company and they charge 90 by is the probability that the company buys times the probability that a chart buys.",
                    "label": 0
                },
                {
                    "sent": "Given that the company buys times the same thing for the other departments.",
                    "label": 0
                },
                {
                    "sent": "So the simplification that I'm making here is that I'm ignoring the fact that HR and it could actually influence each other directly.",
                    "label": 0
                },
                {
                    "sent": "And in reality they do right.",
                    "label": 0
                },
                {
                    "sent": "And of course this is a very simplified example, but the idea is that if there most of the interaction is through the larger part that they're in, then this really reduces the complexity.",
                    "label": 0
                },
                {
                    "sent": "The other, the other assumption that we can make, of course, is that the probability for a class.",
                    "label": 0
                },
                {
                    "sent": "Is an average of the probabilities for the subclasses for.",
                    "label": 0
                },
                {
                    "sent": "So, for example, the problem with that somebody buys my product given that their American, well, that's the probability that they're in New Yorker and then that they buy given that their New Yorker, plus the probability that the Californian you know the problems that they buy, given that they California plus the same thing for all 50 States and if we combine these two things together, we can actually ensure that our inferences tractable?",
                    "label": 0
                },
                {
                    "sent": "Let's see how that happens.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose that.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know there's some item and you're interested in the distribution of of whether everyone will buy it or not.",
                    "label": 0
                },
                {
                    "sent": "So each of these people will either buy or not buy it and let you know.",
                    "label": 0
                },
                {
                    "sent": "Here's a complicated network, right that we have from before.",
                    "label": 0
                },
                {
                    "sent": "Let me simplify this network by saying, well, there's really 2 main clusters in this network.",
                    "label": 0
                },
                {
                    "sent": "There's the cluster of Anna's friends who influence each other a lot, and there's the cluster of ZX Friends who also influence each other a lot, but the influence between those two classes is fairly weak.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'm going to do is like.",
                    "label": 0
                },
                {
                    "sent": "I'm going to model the probability that some person that people in Ennis cluster will buy the product and I'm going to do the same thing for Zach and I'm going to like this.",
                    "label": 0
                },
                {
                    "sent": "So sorry for Zach's group and I'm going to allow dependency between the groups at the group level and then within the group I'm going to have the dependency between the group and each of the individual people so I'm still more or less modeling what's going on.",
                    "label": 0
                },
                {
                    "sent": "What I'm doing is I'm abstracting away from the details of how.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People inside the group interact, they reach still going to have their own different probability and their own different parameters.",
                    "label": 0
                },
                {
                    "sent": "But I've kind of summarized their interaction through the group.",
                    "label": 0
                },
                {
                    "sent": "And of course here I just have these two levels plus the level of home network being Journal.",
                    "label": 0
                },
                {
                    "sent": "I could have as many levels of this as I want to have.",
                    "label": 0
                },
                {
                    "sent": "OK, now so far what we've done is not that interesting or new things get interesting when you start thinking about the following.",
                    "label": 0
                },
                {
                    "sent": "Well, there's more than one relation involved here, right?",
                    "label": 0
                },
                {
                    "sent": "There's friends, but there's also coworkers an maybe for some products were more, and this is true actually, for some products we are more influenced by our friends and for some parts were more influenced by our coworkers and now the circle of Anna's coworkers is different from the circle of her friends.",
                    "label": 0
                },
                {
                    "sent": "They probably overlap, but they're not the same.",
                    "label": 0
                },
                {
                    "sent": "And same thing for Zach, OK?",
                    "label": 0
                },
                {
                    "sent": "So now we actually have two alternative compositions of the world going on here and there operated for different kinds of products.",
                    "label": 0
                },
                {
                    "sent": "And now when I.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ave product, let's say I'm trying to sell a book.",
                    "label": 0
                },
                {
                    "sent": "Well, a book might be more of a leisure item or more of a work item.",
                    "label": 0
                },
                {
                    "sent": "If it's a leisure item, then this is the composition that's going to be in effect.",
                    "label": 0
                },
                {
                    "sent": "If it's more of a work item, this is the decomposition that's going to be in effect.",
                    "label": 0
                },
                {
                    "sent": "OK, so now if I combine all of this, I can actually have very rich interactions between going on between various groups of people without without actually having lost my tractability.",
                    "label": 0
                },
                {
                    "sent": "Why have I not lost my tractability because he?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's how this plays out, right?",
                    "label": 0
                },
                {
                    "sent": "What I have to do is I have to sum over these two classes of products, right?",
                    "label": 0
                },
                {
                    "sent": "And then for this class I have one decomposition of the world into parts and then my overall result in this very simple example is just the product and in this case the world decomposes differently into coworkers.",
                    "label": 0
                },
                {
                    "sent": "So when you look at what's going on here, right?",
                    "label": 0
                },
                {
                    "sent": "And try to generalize it, you can actually state this very nicely in the form of what's called the sum product theorem.",
                    "label": 0
                },
                {
                    "sent": "What the sum product theorem says is the following is that.",
                    "label": 0
                },
                {
                    "sent": "And marginal.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Probability can be efficiently computed.",
                    "label": 1
                },
                {
                    "sent": "If it is one of two things.",
                    "label": 0
                },
                {
                    "sent": "If it's a weighted sum of efficiently computable marginals over the same variables.",
                    "label": 0
                },
                {
                    "sent": "Or if it's the product of efficiently computable marginals over disjoint variables?",
                    "label": 0
                },
                {
                    "sent": "Now let's look at each of these things in turn and see what it means and why that's the case here.",
                    "label": 0
                },
                {
                    "sent": "What I'm saying is that as long as the two distributions are over exactly the same variables, then I can just add them with weights.",
                    "label": 0
                },
                {
                    "sent": "And I don't lose track the ability that way, and the reason is very simple.",
                    "label": 0
                },
                {
                    "sent": "It's just that you know addition is commutative.",
                    "label": 0
                },
                {
                    "sent": "So if I have a sum over the values of a variable of two functions, right?",
                    "label": 0
                },
                {
                    "sent": "Because addition is commutative, I can naturally reorder this and put this sum over the functions outside, and there's some of the variable inside.",
                    "label": 0
                },
                {
                    "sent": "And this is still OK.",
                    "label": 0
                },
                {
                    "sent": "So I've actually decomposed right this into these two pieces.",
                    "label": 0
                },
                {
                    "sent": "And you know there was no blow up.",
                    "label": 0
                },
                {
                    "sent": "The second one is that a product of efficiently computable marginals over disjoint variables is also still tractable, and this now is because of the distributivity of multiplication over addition.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this expression over here right, I can see this is this is a product of two sums, one is over the function F and one is over the function G and they're over different variables, right?",
                    "label": 0
                },
                {
                    "sent": "If these were, if they were shared variables here, this would not work right.",
                    "label": 0
                },
                {
                    "sent": "But since there are different variables, I can first distribute this over this and get a bunch of terms.",
                    "label": 0
                },
                {
                    "sent": "And then I distribute each of those terms over this guy and I get a sum of all the pairs.",
                    "label": 0
                },
                {
                    "sent": "Notice that here I have this sum over all the pairs, right?",
                    "label": 0
                },
                {
                    "sent": "Which is quadratic, and here I just.",
                    "label": 0
                },
                {
                    "sent": "Have you know this number plus this number right?",
                    "label": 0
                },
                {
                    "sent": "So if I keep doing this actually get an exponential decrease in the complexity of my inference, and so if we combine these two things, we can actually and we quite sums with classes and products with parts we can actually.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get very far, OK. And of course the beauty of this definition is that it's recursive.",
                    "label": 0
                },
                {
                    "sent": "I can have a sum of products of sums of products with as many levels as I want, and I can wind up with a very complex, very large network where you know the inference can still be done in linear time in the number of people in the network.",
                    "label": 0
                },
                {
                    "sent": "And by the way, this does not just apply to probabilities.",
                    "label": 1
                },
                {
                    "sent": "This applies essentially to any function it applies to any semiring that has distributivity and commutativity, and you know, for example, it applies to things like the relational semiring with joins and unions.",
                    "label": 0
                },
                {
                    "sent": "It applies to the Boolean, semiring, etc.",
                    "label": 0
                },
                {
                    "sent": "It applies to you know constraint satisfaction.",
                    "label": 0
                },
                {
                    "sent": "It applies to logic.",
                    "label": 0
                },
                {
                    "sent": "So this is very, very powerful and this decomposition, right?",
                    "label": 0
                },
                {
                    "sent": "We could learn it.",
                    "label": 0
                },
                {
                    "sent": "We could be primary.",
                    "label": 0
                },
                {
                    "sent": "So we're only going to learn models that actually decompose this way.",
                    "label": 0
                },
                {
                    "sent": "It could be encoded by hand write the same way you encode the class hierarchy into an object oriented program, or it could just be used as an approximation to a more complex model at inference time.",
                    "label": 0
                },
                {
                    "sent": "In all of these cases, the decomposition is giving us tractability.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if we combine the Markov logic networks which we saw before with some product team which we saw just now, we can actually define a tractable subset of Markov logic networks which we call tractable Markov logic, and the guarantee that this gives you is that if you learn or write your model in tractable Markov logic, then no matter how large the model has, no matter how moving parts it has, the cost of inference will always be linear in the size of the model.",
                    "label": 1
                },
                {
                    "sent": "Because essentially the structure of the model and the structure of the computation are isomorphic at the end of the day.",
                    "label": 0
                },
                {
                    "sent": "This is what's really going on here.",
                    "label": 0
                },
                {
                    "sent": "OK, so now the 3rd and final principle that.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going to talk about here is the idea that time and space should not depend on data size.",
                    "label": 1
                },
                {
                    "sent": "If your learning is blowing up more than linearly with the amount of data that you're using, you want to ask yourself whether maybe you're doing something wrong and how you could fix it.",
                    "label": 0
                },
                {
                    "sent": "So the purpose you know we live, of course, in the.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The age of big data.",
                    "label": 1
                },
                {
                    "sent": "The purpose of having big data is to be able to learn big models.",
                    "label": 1
                },
                {
                    "sent": "Rich complex models, or at least many small ones, but we already saw the limitations of that.",
                    "label": 1
                },
                {
                    "sent": "If you're using big data to learn a small model or even a medium sized model, then you're really wasting your resources right and resources are expensive.",
                    "label": 0
                },
                {
                    "sent": "The size of data that you need to really be dictated by the size of the model that you're learning.",
                    "label": 0
                },
                {
                    "sent": "Small model, small data, large model, large data, huge model, huge data.",
                    "label": 0
                },
                {
                    "sent": "And then the excess that you can just ignore, right?",
                    "label": 0
                },
                {
                    "sent": "You don't even need to bother with it.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, if you if you accept this principle, then the question becomes, well, how do you figure out just how much there is enough right?",
                    "label": 0
                },
                {
                    "sent": "If I stop using data Now, if I use this small subsample, how do I know that I'm not missing out on some really important effects?",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to try to do now is give some amount you know try to.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Answer That to some extent.",
                    "label": 0
                },
                {
                    "sent": "And we're going to do this in the following setting.",
                    "label": 0
                },
                {
                    "sent": "We call this streaming bound algorithms.",
                    "label": 1
                },
                {
                    "sent": "And the idea is that we're going to assume that you have an infinite data stream.",
                    "label": 0
                },
                {
                    "sent": "You have data coming at you very fast everyday, forever, that they never ends.",
                    "label": 0
                },
                {
                    "sent": "This is how essentially every organization operates to their right.",
                    "label": 0
                },
                {
                    "sent": "The data is never ending, so let's just assume that it's infinite.",
                    "label": 0
                },
                {
                    "sent": "Let's go full hog and say we have infinite data.",
                    "label": 0
                },
                {
                    "sent": "The problem, of course, is that we only have constant time and memory.",
                    "label": 1
                },
                {
                    "sent": "You only have so much RAM in your server farm, and you can only wait so long for an answer, right?",
                    "label": 0
                },
                {
                    "sent": "You can't wait to the end of the universe before you start taking actions.",
                    "label": 0
                },
                {
                    "sent": "Now what we would really like to do.",
                    "label": 0
                },
                {
                    "sent": "Is to using only this fixed as small as possible amount of time and memory learn a model that is as close as possible to the model that you would learn if you have infinite space and time to run through that infinite data stream and get your best possible result.",
                    "label": 0
                },
                {
                    "sent": "So, well, sure, that sounds like a great goal.",
                    "label": 0
                },
                {
                    "sent": "It also sounds impossible.",
                    "label": 0
                },
                {
                    "sent": "What you know?",
                    "label": 0
                },
                {
                    "sent": "What makes me think that that this is actually possible.",
                    "label": 0
                },
                {
                    "sent": "Well, the reason that this is possible is actually.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very simple at heart.",
                    "label": 0
                },
                {
                    "sent": "If you think about, for example, the problem of predicting who's going to win the next presidential election, right?",
                    "label": 0
                },
                {
                    "sent": "We of course don't need to ask all 200 million voters to know who the winner is going to be.",
                    "label": 0
                },
                {
                    "sent": "We need, you know, only a sample of a few 1000.",
                    "label": 0
                },
                {
                    "sent": "And the size of sample that we need.",
                    "label": 0
                },
                {
                    "sent": "Is independent of the size of the population.",
                    "label": 0
                },
                {
                    "sent": "You can do the same thing in the US or in China, or in a very small country.",
                    "label": 0
                },
                {
                    "sent": "Doesn't really matter the size of sample that you need only depends on the amount of uncertainty that you're willing to tolerate.",
                    "label": 0
                },
                {
                    "sent": "OK, now of course, that's for estimating one thing who will be the winner of the next election?",
                    "label": 0
                },
                {
                    "sent": "Or one mean of a variable or something like that?",
                    "label": 0
                },
                {
                    "sent": "The whole trick is going to be to generalize this idea to models that have very complex structure that have thousands or maybe millions of parameters.",
                    "label": 1
                },
                {
                    "sent": "We want to be able to at the end of this like I've just learned this model with millions of parameters on a huge data stream, but I can guarantee that this model is within epsilon of what I would have gotten if I had infinite space and time to to learn.",
                    "label": 0
                },
                {
                    "sent": "And of course, there's a very important condition here, which is the data in this stream must be in random order.",
                    "label": 1
                },
                {
                    "sent": "If there in the stream is not in random order, then you know if if there is going to be different in the future.",
                    "label": 0
                },
                {
                    "sent": "I can't do anything correct without seeing it, but we will see a little bit later what we can do about the case where there is not in random order, but for the moment, let's assume that the data is in the random order.",
                    "label": 0
                },
                {
                    "sent": "And so let's say for example that we want to.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One MLN in this way.",
                    "label": 0
                },
                {
                    "sent": "So when learning and let the data is a relational database, right?",
                    "label": 1
                },
                {
                    "sent": "One of the fits benefits of Markov logic networks is that your data doesn't have to be a single table anymore.",
                    "label": 0
                },
                {
                    "sent": "It can be as many tables as you want, and you know the logical rules will do the joins and you can build a model of the whole database at once.",
                    "label": 0
                },
                {
                    "sent": "So your data is a relational database, and since this is nessun salaga model, of course we want to learn it, let's say by the standard method of maximizing likelihood.",
                    "label": 0
                },
                {
                    "sent": "Now, unfortunately for logging in models in general, the likelihood cannot be optimized in closed form, so we have to do something like gradient descent OK. Now the good thing is that there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a global optimum and there are no local optimum, so you know so we don't have the kinds of problems that we have when learning something like, say, and you will network by back Prop, where we get stuck into into local minima lot so we can do gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So the first thing is to see you know what is the gradient, right?",
                    "label": 0
                },
                {
                    "sent": "The gradient is the derivative, so it's going to be a vector, right?",
                    "label": 0
                },
                {
                    "sent": "So the partial derivatives, the partial derivative of each component is going to be the partial derivative of the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "So the log of the probability.",
                    "label": 0
                },
                {
                    "sent": "With the expression that we saw earlier on the derivative of that with respect to the weight and the derivative of the log likelihood respected the item, it actually has a very intuitive meaning.",
                    "label": 0
                },
                {
                    "sent": "Which is going to be very useful.",
                    "label": 0
                },
                {
                    "sent": "In what follows, it's just the difference between the number of two instances of the ruling, the data.",
                    "label": 0
                },
                {
                    "sent": "So for example, how many pairs of people you know one was the influence or the other one was influenced, then they both bought.",
                    "label": 0
                },
                {
                    "sent": "So the number of two instances according to the data.",
                    "label": 1
                },
                {
                    "sent": "So this is the empirical count of the feature and the expected count according to the model.",
                    "label": 0
                },
                {
                    "sent": "If the MLN predicts that the rule is going to be true less often than it really is, then it's weak needs to go up.",
                    "label": 0
                },
                {
                    "sent": "If it's predicting that it's true very often, but isn't the way it needs to go down, and when they all line up, we're done.",
                    "label": 0
                },
                {
                    "sent": "The model is making you know the most accurate predictions that it can, and we've we've maximized the likelihood.",
                    "label": 0
                },
                {
                    "sent": "This is for learning weights.",
                    "label": 0
                },
                {
                    "sent": "We can also and we do learn the structure.",
                    "label": 0
                },
                {
                    "sent": "Learning this structure means trying different ethnicities for the rules and you know keeping the ones that must improve the likelihood and and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can do that using radius search or various other kinds of of discrete search.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so remember what we want to do is take this notion of I only need a sample.",
                    "label": 0
                },
                {
                    "sent": "To the problem of learning a whole MLN OK. Or a whole logging in model?",
                    "label": 0
                },
                {
                    "sent": "And how do we do that?",
                    "label": 0
                },
                {
                    "sent": "Well, the basic tool that we're going to use is the one that we could also already have used for these small problems, which is first of all these models, right?",
                    "label": 0
                },
                {
                    "sent": "And this is true of all logging your models.",
                    "label": 0
                },
                {
                    "sent": "They only depend on the data through the sufficient statistics.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's the counts how often it truly stream the data, but it could be things like the means and the covariances of your continuous variables and what not.",
                    "label": 0
                },
                {
                    "sent": "Once you know those sufficient statistics, you can throw away the data.",
                    "label": 0
                },
                {
                    "sent": "So the whole point is going to be to estimate to have enough data to estimate the sufficient statistics well enough that your overall model winds up being within the bounds of error that you're willing to allow, and our basic tool for doing that is going to be a very widely used one in statistics and machine learning, which is the half thing bound the offering bound says that with probability at least one minus Delta, so with high probability, because Delta is going to be a small number, the true frequency of some event Pi, like you know, like you know, the number of people who buy, for example.",
                    "label": 0
                },
                {
                    "sent": "Is going to be within epsilon of the empirical estimate, P hat and P hat is just NI over and the size of your sample an East is bounded by the square root of this log of two over Delta divided by 2 N. OK, now there are two important things about this Formula One is that the Delta is inside the log and that is wonderful news for us because it means I can drive this probability being something very very small without too much cost by linearly increasing the amount of data that I get.",
                    "label": 0
                },
                {
                    "sent": "I get an exponential improvement in my confidence and the other one of course, is that here I have square root of N. OK, so as I increase the size of my sample my bound gets better with the square root of that sample size, OK?",
                    "label": 0
                },
                {
                    "sent": "So this is the basic thing that we're now going to try.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To construct a bound for the whole model based on this and how we're going to do that well, the algorithm that we're using is gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So here's a brief review of the things that we've already seen so far.",
                    "label": 0
                },
                {
                    "sent": "Here's the form of the logging model.",
                    "label": 0
                },
                {
                    "sent": "Right is the normalized exponentiated some of the weighted features of this state.",
                    "label": 0
                },
                {
                    "sent": "OK, the gradient right is just the difference between the empirical count of a feature and its expected value according to the model.",
                    "label": 0
                },
                {
                    "sent": "And now the algorithm that we're going to use to learn these weights is actually in this line, right?",
                    "label": 0
                },
                {
                    "sent": "This is one step of gradient descent, and we're going to do steps of gradient descent until we converge.",
                    "label": 0
                },
                {
                    "sent": "What happens in one step of reading descent?",
                    "label": 0
                },
                {
                    "sent": "Is that we subtract from the current weight the gradient times the learning rate OK?",
                    "label": 0
                },
                {
                    "sent": "At one of them is probably the most widely algorithm used algorithm in the world for lending parameters right?",
                    "label": 0
                },
                {
                    "sent": "And I'm sure all of you know it.",
                    "label": 0
                },
                {
                    "sent": "This are.",
                    "label": 0
                },
                {
                    "sent": "Here is the learning rate.",
                    "label": 0
                },
                {
                    "sent": "I've divided it by N because first of all we know that the more that we have, the smaller learning rate we need to use, otherwise things would blow up.",
                    "label": 0
                },
                {
                    "sent": "And also I'm about to take the limit of N as it goes to Infinity, right?",
                    "label": 0
                },
                {
                    "sent": "I'm going to be interested in what happens when my N is Infinity and on my end I is also Infinity.",
                    "label": 0
                },
                {
                    "sent": "This is why this is the green dissent that I would do if I have infinite data.",
                    "label": 0
                },
                {
                    "sent": "What I'm interested in doing is comparing that with the case where I have finite data.",
                    "label": 0
                },
                {
                    "sent": "Where my end and then I are finite and I want to bound the difference between the two.",
                    "label": 0
                },
                {
                    "sent": "So how can I do that?",
                    "label": 0
                },
                {
                    "sent": "Well I need to first of all pick some measure.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Error, let's just say that my measure of error is the sum over all the weights over all the parameters of the difference between the weight learned with infinite data, which I'm calling WI and the weight learned with finite data, which I'm going to call W hat I OK and what I want to minimize the sum of these.",
                    "label": 0
                },
                {
                    "sent": "Notice that this is a more stringent criterion than likelihood.",
                    "label": 0
                },
                {
                    "sent": "I'm not just requiring that the Model B is likely as the infinite data one.",
                    "label": 0
                },
                {
                    "sent": "I'm requiring that they actually look the same.",
                    "label": 0
                },
                {
                    "sent": "They have pretty much the same weight, so that my interpretation.",
                    "label": 0
                },
                {
                    "sent": "You know the actions that I think based on a model.",
                    "label": 0
                },
                {
                    "sent": "My understanding will be the same.",
                    "label": 0
                },
                {
                    "sent": "OK and now what I want to do is I want to bound this difference as I go through my steps of gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Well, let's start with initialization well.",
                    "label": 0
                },
                {
                    "sent": "At initialization time, the error is 0 because you know I initialize the weights to the same value in both cases, OK, zero or whatever, but you know they're the same, so there's no error.",
                    "label": 0
                },
                {
                    "sent": "Now let's look at what happens in the first step, right?",
                    "label": 0
                },
                {
                    "sent": "Whoops.",
                    "label": 0
                },
                {
                    "sent": "Now, in the first step, right, the weights from the previous iteration are the same 'cause there's the initial weights and therefore the expectations based on them are also the same, so there's no error there.",
                    "label": 0
                },
                {
                    "sent": "The only error comes from the fact that in one case I used my finite NI over North and in the other case I use my infinite one, which is the real Pi.",
                    "label": 0
                },
                {
                    "sent": "OK, so in the first iteration my error is just the sum over the parameters of the learning rate times the difference in the empirical estimates, the infinite in the find out one, and this is exactly where the huffing bound comes in.",
                    "label": 0
                },
                {
                    "sent": "Right, we know that this is less than square root of blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "OK, so I know my error.",
                    "label": 0
                },
                {
                    "sent": "I know the difference between my infinite data model and my finite data one after step one.",
                    "label": 0
                },
                {
                    "sent": "Now it's instead it's in the 2nd and later step that things get interesting.",
                    "label": 0
                },
                {
                    "sent": "So now what's going to happen here?",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, I'm going to have again looking back at this formula right?",
                    "label": 0
                },
                {
                    "sent": "I need to have some error that comes from the previous iteration, the model, the weights already started out some distance apart and now they're going to drift apart some more.",
                    "label": 0
                },
                {
                    "sent": "First of all, because of the error in the counts right, there's going to be that term again.",
                    "label": 0
                },
                {
                    "sent": "But in addition, there's going to be an interesting new term, which is the difference in the expectations that I computed using finite and infinite data, right?",
                    "label": 0
                },
                {
                    "sent": "Those are two different models.",
                    "label": 0
                },
                {
                    "sent": "One is the model with W weights W and the other one is the model with weights W hat.",
                    "label": 0
                },
                {
                    "sent": "They're different.",
                    "label": 0
                },
                {
                    "sent": "So then we produce different expectations.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to have is 3 terms, the previous one, the empirical one, and the expectation one.",
                    "label": 0
                },
                {
                    "sent": "These ones we already now, So what remains is to take care of this one.",
                    "label": 0
                },
                {
                    "sent": "Now if you look at this model right for him, it's not hard to see that if I have a bound on the some of the errors in these weights, which we call epsilon, then the infinite data expectation before we or any probability before we normalize it is going to be within a factor of epsilon.",
                    "label": 0
                },
                {
                    "sent": "The error epsilon of the empirical one that I measured.",
                    "label": 0
                },
                {
                    "sent": "So let me write that down.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, and by the way, there should be ahead here in the head.",
                    "label": 0
                },
                {
                    "sent": "Here I have a bound on the probabilities that I would compute from the ideal, you know, infinite data model and now based on that bond you can.",
                    "label": 0
                },
                {
                    "sent": "You can come up with a bound for the expectation error, which is this slightly complicated expression here.",
                    "label": 0
                },
                {
                    "sent": "And finally you have an error, an expression for the error in the in the East step in any step of gradient descent, and ignoring the details.",
                    "label": 0
                },
                {
                    "sent": "What is what this says is that as I do one step after another.",
                    "label": 0
                },
                {
                    "sent": "Even dissent, right?",
                    "label": 0
                },
                {
                    "sent": "My error goes up by first of all, a constant amount which has to do with the data, and then this fraction, which because you know we're operating very close to 0, this exponential is approximately linear.",
                    "label": 0
                },
                {
                    "sent": "And So what this means is that this whole thing is increasing roughly linearly with the number of iterations, which is pretty good.",
                    "label": 0
                },
                {
                    "sent": "What that means is that as I'm learning my model with finite data from the stream, the moral is gradually diverging from all that I would learn from the infinite data.",
                    "label": 0
                },
                {
                    "sent": "But only linearly OK?",
                    "label": 0
                },
                {
                    "sent": "And now how do I use this?",
                    "label": 0
                },
                {
                    "sent": "Well, there's actually many ways I can use this, but the simplest way.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's just this I can start out by learning with some optimistic sample size by, for example, assuming that you know this, this term is zero and I just have you know the data.",
                    "label": 0
                },
                {
                    "sent": "Turn to contend with, and if that gets me to the end and the bound is still within what I want, right, I want.",
                    "label": 0
                },
                {
                    "sent": "I want to be within 10% of the true model with 99% probability, and I know whether I am as I think separating dissent.",
                    "label": 0
                },
                {
                    "sent": "If I reach the end that way, then fine, if not all I do is I restart with double the data size.",
                    "label": 0
                },
                {
                    "sent": "I doubled the sample and I start again.",
                    "label": 0
                },
                {
                    "sent": "And at the end of the day, in the worst case I will have done worse by a small factor, but in the best case, right?",
                    "label": 0
                },
                {
                    "sent": "What's going to happen in Journal is that I used some fixed amount of data and then I turned off the faucet and I can return my model to my customer and say this is pretty much exactly the same model that I would have learned if I had learned on, you know, no matter how large that it is that we have or we going to have.",
                    "label": 0
                },
                {
                    "sent": "So the time and space really are going to be independent of the data size, because I never even look or store or do anything with the data that is beyond what I need.",
                    "label": 0
                },
                {
                    "sent": "OK, now those of you familiar with learning theory might be running well.",
                    "label": 0
                },
                {
                    "sent": "We know that this thing looks a lot like you know PAC bounds and pack pounds are notoriously loose.",
                    "label": 0
                },
                {
                    "sent": "Well, what I'd like to point out is that these bounds I'm much much tighter than PAC Bounds PAC bounds depend on the whole size of the class of models that you're considering.",
                    "label": 0
                },
                {
                    "sent": "And so they're extremely pessimistic, to the point where they seldom practical.",
                    "label": 0
                },
                {
                    "sent": "These bonds are very different because they're based on the actual algorithm that you're running the based on looking at step by step what the algorithm is doing, so you know there's still pessimistic, like for example, huffing bounds are more pessimistic than normal bounds, which you could use instead and speed things up that way.",
                    "label": 0
                },
                {
                    "sent": "But bottom line, there actually tight enough to be useful in practice, and when we've we've used things like this in practice, we you know what happens is that up to some point, we don't get any gains and then we get to that point, which for example might be in the millions of examples, and then we're done.",
                    "label": 0
                },
                {
                    "sent": "And then if you didn't have 10,000,000 examples, you've just spent things up by an order of magnitude.",
                    "label": 0
                },
                {
                    "sent": "If you did ahead 100 million examples, you spread things up by two orders of magnitude and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so this can potentially let you do with a single CPU?",
                    "label": 0
                },
                {
                    "sent": "Well, that what otherwise you might need thousands of CPU's to do the same kinds of ideas can be applied to structural learning.",
                    "label": 0
                },
                {
                    "sent": "Here I just showed how you would do this for learning the parameters, But if you want to learn the structure of the actual features then you could do the same type of treatment.",
                    "label": 0
                },
                {
                    "sent": "Essentially what has to happen there is that you need to divide your Delta by the number of models that you've tried.",
                    "label": 0
                },
                {
                    "sent": "The more models you try, the smaller your Delta for each one has to be such that when you add them all up over all the models that you tried and all the different steps, you still have this bond for only type bound on your on your overall error.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, what happens if the data is changing overtime?",
                    "label": 0
                },
                {
                    "sent": "Because most of the time it is changing overtime and often is the change over the time.",
                    "label": 0
                },
                {
                    "sent": "That's the most interesting.",
                    "label": 0
                },
                {
                    "sent": "Well in that case, what you do is you maintain a sufficient statistics over a sliding window.",
                    "label": 1
                },
                {
                    "sent": "OK, so as your data comes through right, you keep maintaining the sufficient distance for for the most recent window of data, and you compare that.",
                    "label": 0
                },
                {
                    "sent": "The current statistics with the ones that the model was learned on and if it some point the difference between the current statistic and the historical one exceeds abound then you re learn based on based on the new values.",
                    "label": 1
                },
                {
                    "sent": "OK so you can have any at any point or more that makes as much use as possible of the past while changing as the new data demands.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me just mention some of the applications of this that we've done to date with of course the NAP.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Locations to viral marketing, where we saw that indeed, you know, marketing to the influences can be orders of magnitude more effective than marketing to just anybody.",
                    "label": 0
                },
                {
                    "sent": "We've built large and we're building extracting large knowledge bases from the web using Markov logic.",
                    "label": 0
                },
                {
                    "sent": "We've done, you know we and others have done applications of these ideas in object recognition, web caching, semantic parsing, and many others, and the various things that I've talked about here.",
                    "label": 1
                },
                {
                    "sent": "We actually have open source implementations for them.",
                    "label": 0
                },
                {
                    "sent": "Which you can find there's pointers to them.",
                    "label": 0
                },
                {
                    "sent": "From my homepage they include things like the alchemy implementation of Markov logic, sum product networks which implement the same product theorem, very fast machine learning, which is a toolkit for turning algorithms into streaming algorithms in the way that I described here and so forth.",
                    "label": 0
                },
                {
                    "sent": "So to summarize.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are entering the era of very large scale modeling and what I did in this talk was scratched the surface of what we need to do to start building these very large scale models.",
                    "label": 0
                },
                {
                    "sent": "You know, as opposed to the large scale models that we built today and we saw three principles.",
                    "label": 0
                },
                {
                    "sent": "One is more of the whole, not just the parts and one way in which you can do that is with Markov logic networks.",
                    "label": 1
                },
                {
                    "sent": "The second principle was tame complexity via hierarchical decomposition, and one way we can do that is with the sum product theorem.",
                    "label": 0
                },
                {
                    "sent": "And the third principle is that time and space should not depend on data size, and one way in which you can realize this is with streaming bound algorithms.",
                    "label": 1
                },
                {
                    "sent": "As we just saw more on this and many other things in a book that I'm writing that I will now shamelessly plug.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a Popular Science book on machine learning, so the idea is to introduce things like data science and all the principles and ideas that we know and love to a broader audience.",
                    "label": 0
                },
                {
                    "sent": "It's called the Master algorithm, machine learning and the Big Data Revolution, and it's going to come out next year.",
                    "label": 1
                },
                {
                    "sent": "And with that I would like to thank you all for being here, and let's make the next 20 years of KDD even better than the last 20.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "You assume that.",
                    "label": 0
                },
                {
                    "sent": "Number of parameters you have in your model is always fixed.",
                    "label": 0
                },
                {
                    "sent": "It does not increase with more data, but that is often not the case, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, we've seen that with more streaming data you get new customers and with new customers you have new parameters and so the idea that the number of parameters will remain fixed as you get more and more data is not true.",
                    "label": 0
                },
                {
                    "sent": "Well, to clarify, in this simple example, I assume that the number of parameters was fixed, but in reality we don't have to assume that the parameters I fixed what happens is that if you introduce a new parameter into the model, you have to add that to the bonds and start treating it the same way as the others.",
                    "label": 0
                },
                {
                    "sent": "So it's actually not, and in fact, when we do this for structure learning right, which we do as I add structure, I often also add parameters like.",
                    "label": 0
                },
                {
                    "sent": "For example, I may learn new features and the bounds still apply.",
                    "label": 0
                },
                {
                    "sent": "They get worse of course, because I know I have more parameters, but that's actually not a problem.",
                    "label": 0
                },
                {
                    "sent": "I have a question regarding the last part.",
                    "label": 0
                },
                {
                    "sent": "So you started by assuming that the constraint was on CPU time and memory, but it looked like at the end you were really constraining yourself in terms of the number of examples you can use.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it seems that if the constraint is really on CPU time and no constraint on the amount of data you can use instead of using less data, you should be using more data for the same CPU and not cycle through your examples many times.",
                    "label": 0
                },
                {
                    "sent": "OK, so again, let me clarify this right.",
                    "label": 0
                },
                {
                    "sent": "This is a good question.",
                    "label": 0
                },
                {
                    "sent": "In what I said here, actually mixed up those two, so let me give it because what happens in, for example, for something like gradient descent, there is a direct correspondence between the number of data points that I have and the amount of memory that I use.",
                    "label": 0
                },
                {
                    "sent": "A number of CPU that I use, but the more general treatment, and in some ways the more interesting and sophisticated application of this which we have done is where you do the following.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to do is I'm going to explicitly minimize the CPU time I write down a function for the CPU time as a function of the number of data points that I use at each step.",
                    "label": 0
                },
                {
                    "sent": "And now what I do is I use LaGrange multipliers to minimize the CPU time if the CPU time is a linear function of the data, then you get this very simple case, but in general it could actually be different.",
                    "label": 0
                },
                {
                    "sent": "I could.",
                    "label": 0
                },
                {
                    "sent": "I could for example, for example, we've applied this to things like him and K means where we wind up using different numbers of examples at different stages in the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Because there's different tradeoffs between, for example a quadratic cost and how much that air blows up if the erables up a lot in succeeding iterations.",
                    "label": 0
                },
                {
                    "sent": "I want to I want to have a lot of data there if in succeeding iterations there is going to die down to zero, then actually.",
                    "label": 0
                },
                {
                    "sent": "I can be very loose there, so your question is a good one, but actually you know in truth we can actually do this deliberately for maximizing for minimizing the computational complexity.",
                    "label": 0
                },
                {
                    "sent": "Even you can include the stochastic gradient case and your analysis this drastically and I was wondering if this is what you had in mind.",
                    "label": 0
                },
                {
                    "sent": "Of course we can, so we haven't studied the stochastic gradient case, but let me let me tell you what I think right.",
                    "label": 0
                },
                {
                    "sent": "We can certainly include it here, right, but the question is whether it would be useful.",
                    "label": 0
                },
                {
                    "sent": "I think you know, here's one way to think about this, like how large should your mini batch B?",
                    "label": 0
                },
                {
                    "sent": "Right, this is 1 very pragmatic question, right?",
                    "label": 0
                },
                {
                    "sent": "And you could use this to set mini batch sizes, but this wouldn't.",
                    "label": 0
                },
                {
                    "sent": "You know the mini batch size that people use are smaller than what you actually using here, so I actually don't have a very definitive answer.",
                    "label": 0
                },
                {
                    "sent": "Also, I think that for learning things like say you know deep models, right?",
                    "label": 0
                },
                {
                    "sent": "Just a random example.",
                    "label": 0
                },
                {
                    "sent": "I think in the kinds of applications that people this is.",
                    "label": 0
                },
                {
                    "sent": "Again, just my suspicion, but you know things like continents for image net, the data there is so rich, right?",
                    "label": 0
                },
                {
                    "sent": "That I would guess that even with all of image Now we still have not reached the point where this becomes where you want to stop.",
                    "label": 0
                },
                {
                    "sent": "So maybe there is no need for it there yet, but there probably will at some point.",
                    "label": 0
                },
                {
                    "sent": "Sorry to be half my hand like this, but the light is very bright and I want to see this gradient thanks better.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the wonderful dog.",
                    "label": 0
                },
                {
                    "sent": "Yeah we do.",
                    "label": 0
                },
                {
                    "sent": "An honor to Pedro for a standing ovation as the most innovative perform.",
                    "label": 0
                }
            ]
        }
    }
}