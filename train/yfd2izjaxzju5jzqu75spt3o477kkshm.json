{
    "id": "yfd2izjaxzju5jzqu75spt3o477kkshm",
    "title": "Deep learning for noise-tolerant RDFS reasoning",
    "info": {
        "author": [
            "James A. Hendler, Rensselaer Polytechnic Institute"
        ],
        "published": "Dec. 10, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2019_hendler_RDFS_reasoning/",
    "segmentation": [
        [
            "So I learned a new phrase the other day which was PowerPoint karaoke.",
            "Which is where you must give the talk from someone elses slides and I'm today going to practice what is a bit of PowerPoint karaoke because we discovered on Friday that the first author of the paper did not get his visa.",
            "After having been told repeatedly that it would be coming and so I am presenting a presentation that was basically sent to me on yesterday night.",
            "But this was one of my PhD students.",
            "I have worked on it, so I do know something about the work and so I'm going to talk about what we've been doing.",
            "I'm going to give a little bit of motivation of my own."
        ],
        [
            "Before I get into the motivation, specifically in the paper, this is a Journal track paper, so the actual paper was in the Semantic Web Journal Special issue on Semantic Deep Learning, and all of the code, the models and the datasets are available on line at GitHub.",
            "And the first author is Busted Mackney, who again is not here because of the visa issue and is the one who really should be presenting this work because he was the one responsible."
        ],
        [
            "For most of it.",
            "So we're going to talk a little bit about knowledge and learning.",
            "What kind of embedding we were doing, what sort of ground truthing?",
            "The embedding model we use, which is a little different than what you've heard in the past two talks and a little bit about."
        ],
        [
            "Also, not surprisingly, but the motivating thing for a lot of the work in my lab now is really starting to look at knowledge meets the stuff coming out of modern AI.",
            "When you start looking at the perception world, write the text world.",
            "There are now a lot of very powerful things coming along where those of us in the AI community have for many years said in the in the KR community have said, you know when the vision systems get good enough, then we'll worry about it.",
            "Till then, we're just going to use these nice logic representations.",
            "Well, guess what?",
            "The vision systems have gotten.",
            "Good enough and what they're producing don't really look like a lot of the traditional logic representations.",
            "The question is, how do we start putting these together?",
            "There's been a sequence of three or four different PhD theses from my lab that have looked at some of this Bassham was the first, but essentially so.",
            "The way I like to motivate this, and this is part of a much, much longer talk.",
            "You can find my slides online is if we look at a picture like this and said what's the relationship between this man and this woman.",
            "You get something and.",
            "I could show you if I had time.",
            "What you know, Cinegraph labelers and things like do that do.",
            "But if I."
        ],
        [
            "If I make this change right so I just open up the aperture a little and you can now see that in the background there's this woman in a white dress that actually changes the context pretty drastically.",
            "Now most people knowing the US wedding.",
            "Sort of culture would say, oh, that's a wedding.",
            "Now you would start making hypothesis.",
            "You know, maybe that's the the groom dancing with his mother.",
            "Right, in fact, that in some informal testing in classrooms tends to come up a lot OK, but I could keep adding information and so at some point sooner or later you reach the point where you could just say just increasing the size of the training set.",
            "So you said now you can be wedding pictures.",
            "Now you can be this pictures.",
            "Now you give me that picture, OK?",
            "I'm from New York State in New York State we allow gay marriage, right?",
            "So if the two people getting married are both are both women, then that's not the groom.",
            "Dancing with the pride.",
            "That might be one of the tomb vetting.",
            "Married dancing with a different relative, etc etc.",
            "So we can always keep adding complexities right?",
            "So that's been motivating a lot of the thinking in my in my lab.",
            "And Ann were actually look."
        ],
        [
            "Not a lot of it in more technical type areas.",
            "So for example, these two pictures, the kind of context switches happening in the same way it was in the previous, and the kind of exceptions are there and I won't go into details except to say that picture a most most image analysts an most deep learning systems will say there is a tumor there when it turns out there's not a tumor there, but you need the medical record to know the reason the tumor is not there is the person who's had a mastectomy, so there can't be a tumor there, right?",
            "So that's that exception case.",
            "OK, the other one is a very rare kind of cancer that's very hard to detect, or a rare kind of positioning with tumor and again in less you had a training set with lots and lots of these in it.",
            "Right, you're not going to see it, but if you took a million cancer patient, maybe one of them would have this.",
            "So again, you need knowledge of the medical situation to be able to do some of these more complicated cases.",
            "OK, which is a long way from the actual paper were on, but it just is sort of motivation."
        ],
        [
            "So.",
            "What are some proposed to me an I figured he had no way it was going to make it work was that he would be able to use some reasoning he would be able to train up a reasoner in RDF and RDF reasoner using deep learning techniques.",
            "But that wasn't that hard.",
            "The hard, I mean, it took some interesting technical tricks, but the hard part was, So what?",
            "Right, what was interesting about the knowledge representation issue because we have plenty of deep learning's.",
            "Sorry DL Deep logic, system description, logic systems that can get the correct answers.",
            "Right, but when you started looking at noisy data.",
            "So for example, if you take DB pedia or Freebase or any of those and you actually apply all the rules to get a deductive closure, you get a lot of garbage 'cause every piece, every wrong inference, every wrong assertion.",
            "Right, every inconsistent assertion gets propagated through all of the reasoning you can do about that.",
            "So the question that started to come out is in theory, these the deep learning systems are better at recognizing exception cases better.",
            "I'm sorry at recognizing categories and things like that, so the question is, can we train on inferences in such a way that the systems would be able to actually do the inferencing, but also recognize when something didn't look right so the the.",
            "The overall graph doesn't make sense with certain inferences and makes more sense with others in terms of other graphs we've seen.",
            "So this goes back to something I've been saying for many many years that what motivated a lot of the semantic web were back in the late 90s into the early 2000s was exactly that.",
            "The web is noisy.",
            "Write text extraction.",
            "Write a really, really good text extraction system.",
            "Might have a F1 metric of .9 three, which means 7% of this stuff is junk.",
            "Not exactly, but close enough, right?",
            "So in other words, there's a lot of errors in there, you just don't know what they are.",
            "Things like that.",
            "So the question is when we look at some of the things going on."
        ],
        [
            "On the web can we find?",
            "You know the deep learning system do better than a traditional reasoner at recognizing the situations where there's something wrong.",
            "So, just as an example in DB pedia.",
            "Turns out."
        ],
        [
            "There are bunch of things that are both people and places OK in fact."
        ],
        [
            "If you."
        ],
        [
            "No, sorry.",
            "And of course, if you have something that's both the person place, then the place if the place was the correct inference, the inference off the person is that that's an agent, right?",
            "But a place can't be an agent, so ascent."
        ],
        [
            "Surely you can find a lot of these are just this one example of things that are both people and places you find at the time that we did.",
            "This 1716 of them in DB pedia.",
            "Right, so again, these are things where any decent reasoner will make a inference that is incorrect because at least one of these two statements must be incorrect, and in the real."
        ],
        [
            "World.",
            "OK thanks so."
        ],
        [
            "So.",
            "So what we've somewhat blossoms thesis work was was to look at noise handling structures.",
            "How did we really look at some other things?",
            "And then how do we do graph embedding for this?",
            "Any compared in the actual Journal paper?",
            "A lot of comparisons that."
        ],
        [
            "You can read about but Ascentia Lee the question is, you know, can we find a way an adaptive way of getting rid of the noise rather than doing the cleaning by hand for these things?"
        ],
        [
            "So so the other thing is when we look at most graph embedding techniques right?",
            "Most graph to graph learners right are not good at learning a graph which either adds or deletes information from the previous graph rather than just manipulates the graph or you end up with two graphs turned into vectors that you can do vector math on.",
            "That may or may not actually have much to do with the actual inference space.",
            "So after a long.",
            "A lot of work trying to use many of the graph to graph deep learning systems eventually."
        ],
        [
            "We came up with something.",
            "Quite different because basically most of these embedding technologies were not designed for RDF S. The other thing is.",
            "One of the things that we want to be looking at is things that are higher up in the hierarchy, right?",
            "So if you're a person, there's an inference about things that you do as a person.",
            "If you replace, there's an inference about things to do as a place both up and down.",
            "So again that propagation.",
            "Is complicated."
        ],
        [
            "So the overall approach that was used in Basson's work is essentially you take a graph.",
            "Right, so this would be, you know, typically your assertion graph in RDF plus the ontology.",
            "You can run any of your favorite reasoners.",
            "He did through Jenna.",
            "So Jenna will generate a new graph with at least one new triple in it, right?",
            "Use the same encoding to create the graphs without the inference in the graph with the inference.",
            "OK, so this is an inference being made in this graph.",
            "We turned it into a set of tensors, which I'll show you very briefly because.",
            "The details are in the paper an RB in the doctoral thesis and being the advisor.",
            "I won't claim that I know all the details.",
            "That goes into a sequence to sequence learners where we can then do the actual comparison, training and evaluation.",
            "So essentially we're turning the graphs into a set of tensors using those tensors into.",
            "A sequence so it has a lot of similarity.",
            "The last paper, with the exception that we're not doing the embedding per say we're not doing any of the embedding kind of maths."
        ],
        [
            "For ground truthing, then what you can do for the cases where you actually know all of the things we can actually take a particular set of facts, we can say what should be done by a rule based system.",
            "Did we actually learn what was supposed to be done, and so we can compare to something like a generation are."
        ],
        [
            "Or something like that.",
            "So the work was done against two different datasets.",
            "In the paper it's actually been tested on more than that.",
            "One was an artificial data set, so we took plumbum.",
            "We proved we could learn love 'em up to a fairly good level of accuracy and then added noise, which I'll talk about in a minute.",
            "And you know, did the usual train validating test."
        ],
        [
            "The other thing that was running the valuation was actually to generate from DB pedia what we what was called the scientist data set which is not just the scientist.",
            "It's sort of.",
            "It's essentially what we would call the RDF S closure of all of the people who are asserted to be scientists.",
            "Comes up with a lot of weird stuff, so if a scientist was born in London and we have facts about London, it will be in there.",
            "So it's essentially propagating a lot of facts about and generates a very large.",
            "A very large graph, so again we look at in places things like that that are related to the scientists."
        ],
        [
            "Now the biggest question is how to embed this.",
            "So this was really where the doctoral thesis."
        ],
        [
            "This work that led to the.",
            "To the to the Journal Paper an.",
            "The key thing was finding a mechanism to turn an RDF graph into a a set of things which can be compared and then learn.",
            "So.",
            "So essentially the idea is that you're using a tensor representation.",
            "I don't really."
        ],
        [
            "Time to go into it.",
            "It turns out that if you use this representation, you have potentially a very large number of items.",
            "If you do the whole cross product.",
            "But if you actually look at how to lay things out as."
        ],
        [
            "Yes, what's happening is what you're really doing is you're taking all of the terms all of the.",
            "So software version, organization, advisor, etc.",
            "You have the entities in one thing, and then you have the.",
            "Related properties in the other, so I can assert in each of those.",
            "Each of those matrices there's a one in there somewhere which says this that represents this person as this advisor.",
            "Right, and then you turn those sets of those graphs those tensors into see."
        ],
        [
            "And says.",
            "And so you end up with this sequence of words.",
            "So essentially our DFS reasoning becomes figuring out which sequences are legal.",
            "Which, given a set of given the graph, you got a second graph out which is the graph plus some plus the legal inference, the plus, the inferences that the system has learned."
        ],
        [
            "Details so it uses a bidirectional recurrent neural network.",
            "Lot of tricks use dropouts and things like that that are fairly standard for these."
        ],
        [
            "Kind of learning when you look at something like love 'em you see it?",
            "Does it actually a pretty good job of learning for the scientist data set it took?"
        ],
        [
            "Longer now what was done for the actual evaluation.",
            "The primary evaluation was generate the correct libm set.",
            "Then we need some ground truth about something that's wrong, so you add some facts that are incorrect.",
            "Now incorrect things in a reasoning system can be things which propagate because they are inferences off of them and things that can't, and so the paper has some discussion of this that I'm not going to go into, but essentially things that propagator where the interesting problem is.",
            "Right, so if a student is if somebody is a grad student, they have a.",
            "Particular University so."
        ],
        [
            "So basically what you're comparing to here is how our system does to Jenna for various things.",
            "So for the noisy set, right, those things where are correct.",
            "Inferences are defined by Jenna, so Jenna gets 100%.",
            "So that's the GCC, right?",
            "The other two are cases where noise has been introduced.",
            "In one case the noise has been introduced in such a way that by definition you won't get the inferences and the other one.",
            "By definition you will so.",
            "What you see is that essentially the the important thing here is the green line.",
            "So for our ATA, which is the one we're mainly talking about about one particular inference you see, it did almost as well as Jennifer, the other one.",
            "It did much better because again, this is where Jenna was propagating the noise in this system wasn't."
        ],
        [
            "On the scientist data set, it's much harder to explain because you don't actually know what's correct and incorrect in there for everything, but just an example I mentioned 17161 errors of the actual of those in the that are in all of DB pedia 90.",
            "Four of those fall into this data set.",
            "One of the incorrect inferences is that people are agents.",
            "So in six of the 94.",
            "So I'm sorry in only in six of the cases did our influencer make that assumption.",
            "Now of course, that would be easy to say it never.",
            "You know if it never made any assumptions, then by definition we would never make any of the wrong ones.",
            "So of the looking at all the things that could be in Ferd of the of the 94, there were 38 where it got exactly the inferences you would expect if you eliminated the fact it was a person.",
            "Some of them had inferred properties that that Jenna couldn't make and that those were really the most interesting case and makes it somewhat leads to some of the future inference.",
            "So for example.",
            "This is one of my favorite one.",
            "So Big Ben shows up in there and Big Ben was both said to be a person and place the system.",
            "So Jenna of course would make it an agent.",
            "Our system did not make it an agent, but added that it was a historic place.",
            "Right, it looked like it had a lot of the same properties.",
            "A lot of other things that were historic places.",
            "Now, how do you evaluate whether that's correct or not?",
            "Because it's not directly asserted that it's a historic place, so some of the inferencing you know is.",
            "So when you do it in these very large real world sets, exactly what measures is not easy, but you see a lot."
        ],
        [
            "Have interesting things, so essentially.",
            "We've really shown that you can get out of a learning system and RDF silly reason or that is more noise tolerant than a traditional reason are still not perfect.",
            "Long way to go.",
            "There was a lot of work that I didn't talk about giving the time about what sort of noise and noise tolerance and what a lot of that means.",
            "I showed you very briefly in hand, WAVY the layer graph model and the graph words for how we encode the RDF, and again the details are all in the Journal Paper.",
            "So you can you can see it and again as I said it right at the beginning, all of the code, models, examples etc are available."
        ],
        [
            "Where we going with this?",
            "Well, so one real issue, obviously is generality.",
            "So in theory I can take a deep.",
            "I can take my.",
            "Description Logic System put in any set of.",
            "You know facts and get a set of inferences out.",
            "Without by definition, the good thing about is I don't have any domain.",
            "Changes here of course, depending on how many, how many things you have, how they do it, how you train, etc.",
            "You don't get that same kind of generality, so there's a tradeoff there.",
            "Bastion is now at IBM Research, where he's actually looking at the transfer learning solve some of these problems and then something that we're looking at together is one of the nice things about the learning actually being able to find and or eliminate the different RDF.",
            "Statement is that you have Providence with those statements, so now you can say who asserted the thing that was wrong.",
            "Right, so the system decided that Big Ben was not a person.",
            "Can it actually say hey, this is where that you know in Wikipedia who did that?",
            "Or in some other system, so you can start actually looking at some of the trust stuff and I will basically stop there.",
            "I'll answer whatever questions I can, those I can."
        ],
        [
            "Not you can send Tabassum OK, thank you very much.",
            "We have time for maybe one or two quick questions.",
            "Thank you, can you comment on whether say, sub property reasoning or domain range reasoning or subclass reasoning which one is?",
            "Better, or you know how it's off a part?",
            "Yeah, so.",
            "So the fast answer is, if you just take the general our DFS rules and just train it directly, so no noise right, it will get too.",
            "I forget the exact number, it's in the paper, but high 90s percent correctness, right?",
            "So it's essentially doing a fine job on all of the standard inferencing.",
            "Caveat reasonable size, sad alot of time.",
            "You know all the usual learning things.",
            "OK, interesting Lee when but when you add noise again the different kind of noise.",
            "So for example properties subproperties things like that propagate noise differently than class, subclass etc.",
            "So that's what that stuff I kinda hand waved about the propagation and then I'm getting what we sort of showed is that it does a good job of cutting off the ones that so given something that's asserted with multiple properties.",
            "Right, if some of those properties start not to look right, they just got cut off and again that can be on most of these were in the class hierarchy.",
            "Inferences you get the same thing in property, but for example in DB pedia you have very few prep.",
            "You don't have any long sub property in prisons like that.",
            "So again it was it was pretty carefully controlled for the actual paper an does pretty well but was not tested in all different types.",
            "OK, thanks very quick one.",
            "Just now we will talking about corruptions of the triples.",
            "You mentioned summer summer coming propagated with some cannot be.",
            "Can you talk a bit more on why certain things can be propagated in others that cannot be so.",
            "So again, it depends whether what kind of inferences come off of the noise that you add.",
            "So if you are a grad student who has an advisor, I simply assert that you are not a grad student, or I simply leave that out.",
            "There is no advisor in principle.",
            "Grad students have advisors, and you're not graduate, so that doesn't propagate right.",
            "If I say you're at a certain University, then that implies.",
            "If so, there are things.",
            "So again, if I say you you are a grad student and an undergraduate, then you end up with an advisor.",
            "And if I forget the exact details.",
            "Yep, thanks, maybe we could take the questions offline again so so it's all defined in the paper.",
            "If you look at it, but against some of these things when you make a wrong assertion, all it does is remove inferences.",
            "So nothing propagates right.",
            "Others of these, when you put them in, you end up with propagation and there's even somewhere if you take them out you can get some propagation, but that's pretty rare category, so that's all in the in the Journal Paper how this works.",
            "Or interesting work, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I learned a new phrase the other day which was PowerPoint karaoke.",
                    "label": 0
                },
                {
                    "sent": "Which is where you must give the talk from someone elses slides and I'm today going to practice what is a bit of PowerPoint karaoke because we discovered on Friday that the first author of the paper did not get his visa.",
                    "label": 0
                },
                {
                    "sent": "After having been told repeatedly that it would be coming and so I am presenting a presentation that was basically sent to me on yesterday night.",
                    "label": 0
                },
                {
                    "sent": "But this was one of my PhD students.",
                    "label": 0
                },
                {
                    "sent": "I have worked on it, so I do know something about the work and so I'm going to talk about what we've been doing.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give a little bit of motivation of my own.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before I get into the motivation, specifically in the paper, this is a Journal track paper, so the actual paper was in the Semantic Web Journal Special issue on Semantic Deep Learning, and all of the code, the models and the datasets are available on line at GitHub.",
                    "label": 0
                },
                {
                    "sent": "And the first author is Busted Mackney, who again is not here because of the visa issue and is the one who really should be presenting this work because he was the one responsible.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For most of it.",
                    "label": 0
                },
                {
                    "sent": "So we're going to talk a little bit about knowledge and learning.",
                    "label": 1
                },
                {
                    "sent": "What kind of embedding we were doing, what sort of ground truthing?",
                    "label": 0
                },
                {
                    "sent": "The embedding model we use, which is a little different than what you've heard in the past two talks and a little bit about.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, not surprisingly, but the motivating thing for a lot of the work in my lab now is really starting to look at knowledge meets the stuff coming out of modern AI.",
                    "label": 0
                },
                {
                    "sent": "When you start looking at the perception world, write the text world.",
                    "label": 0
                },
                {
                    "sent": "There are now a lot of very powerful things coming along where those of us in the AI community have for many years said in the in the KR community have said, you know when the vision systems get good enough, then we'll worry about it.",
                    "label": 0
                },
                {
                    "sent": "Till then, we're just going to use these nice logic representations.",
                    "label": 0
                },
                {
                    "sent": "Well, guess what?",
                    "label": 0
                },
                {
                    "sent": "The vision systems have gotten.",
                    "label": 0
                },
                {
                    "sent": "Good enough and what they're producing don't really look like a lot of the traditional logic representations.",
                    "label": 0
                },
                {
                    "sent": "The question is, how do we start putting these together?",
                    "label": 0
                },
                {
                    "sent": "There's been a sequence of three or four different PhD theses from my lab that have looked at some of this Bassham was the first, but essentially so.",
                    "label": 0
                },
                {
                    "sent": "The way I like to motivate this, and this is part of a much, much longer talk.",
                    "label": 0
                },
                {
                    "sent": "You can find my slides online is if we look at a picture like this and said what's the relationship between this man and this woman.",
                    "label": 1
                },
                {
                    "sent": "You get something and.",
                    "label": 0
                },
                {
                    "sent": "I could show you if I had time.",
                    "label": 0
                },
                {
                    "sent": "What you know, Cinegraph labelers and things like do that do.",
                    "label": 0
                },
                {
                    "sent": "But if I.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I make this change right so I just open up the aperture a little and you can now see that in the background there's this woman in a white dress that actually changes the context pretty drastically.",
                    "label": 0
                },
                {
                    "sent": "Now most people knowing the US wedding.",
                    "label": 0
                },
                {
                    "sent": "Sort of culture would say, oh, that's a wedding.",
                    "label": 0
                },
                {
                    "sent": "Now you would start making hypothesis.",
                    "label": 0
                },
                {
                    "sent": "You know, maybe that's the the groom dancing with his mother.",
                    "label": 0
                },
                {
                    "sent": "Right, in fact, that in some informal testing in classrooms tends to come up a lot OK, but I could keep adding information and so at some point sooner or later you reach the point where you could just say just increasing the size of the training set.",
                    "label": 0
                },
                {
                    "sent": "So you said now you can be wedding pictures.",
                    "label": 0
                },
                {
                    "sent": "Now you can be this pictures.",
                    "label": 0
                },
                {
                    "sent": "Now you give me that picture, OK?",
                    "label": 0
                },
                {
                    "sent": "I'm from New York State in New York State we allow gay marriage, right?",
                    "label": 0
                },
                {
                    "sent": "So if the two people getting married are both are both women, then that's not the groom.",
                    "label": 0
                },
                {
                    "sent": "Dancing with the pride.",
                    "label": 0
                },
                {
                    "sent": "That might be one of the tomb vetting.",
                    "label": 0
                },
                {
                    "sent": "Married dancing with a different relative, etc etc.",
                    "label": 0
                },
                {
                    "sent": "So we can always keep adding complexities right?",
                    "label": 0
                },
                {
                    "sent": "So that's been motivating a lot of the thinking in my in my lab.",
                    "label": 0
                },
                {
                    "sent": "And Ann were actually look.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not a lot of it in more technical type areas.",
                    "label": 0
                },
                {
                    "sent": "So for example, these two pictures, the kind of context switches happening in the same way it was in the previous, and the kind of exceptions are there and I won't go into details except to say that picture a most most image analysts an most deep learning systems will say there is a tumor there when it turns out there's not a tumor there, but you need the medical record to know the reason the tumor is not there is the person who's had a mastectomy, so there can't be a tumor there, right?",
                    "label": 0
                },
                {
                    "sent": "So that's that exception case.",
                    "label": 0
                },
                {
                    "sent": "OK, the other one is a very rare kind of cancer that's very hard to detect, or a rare kind of positioning with tumor and again in less you had a training set with lots and lots of these in it.",
                    "label": 0
                },
                {
                    "sent": "Right, you're not going to see it, but if you took a million cancer patient, maybe one of them would have this.",
                    "label": 0
                },
                {
                    "sent": "So again, you need knowledge of the medical situation to be able to do some of these more complicated cases.",
                    "label": 0
                },
                {
                    "sent": "OK, which is a long way from the actual paper were on, but it just is sort of motivation.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What are some proposed to me an I figured he had no way it was going to make it work was that he would be able to use some reasoning he would be able to train up a reasoner in RDF and RDF reasoner using deep learning techniques.",
                    "label": 0
                },
                {
                    "sent": "But that wasn't that hard.",
                    "label": 0
                },
                {
                    "sent": "The hard, I mean, it took some interesting technical tricks, but the hard part was, So what?",
                    "label": 0
                },
                {
                    "sent": "Right, what was interesting about the knowledge representation issue because we have plenty of deep learning's.",
                    "label": 0
                },
                {
                    "sent": "Sorry DL Deep logic, system description, logic systems that can get the correct answers.",
                    "label": 0
                },
                {
                    "sent": "Right, but when you started looking at noisy data.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you take DB pedia or Freebase or any of those and you actually apply all the rules to get a deductive closure, you get a lot of garbage 'cause every piece, every wrong inference, every wrong assertion.",
                    "label": 0
                },
                {
                    "sent": "Right, every inconsistent assertion gets propagated through all of the reasoning you can do about that.",
                    "label": 0
                },
                {
                    "sent": "So the question that started to come out is in theory, these the deep learning systems are better at recognizing exception cases better.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry at recognizing categories and things like that, so the question is, can we train on inferences in such a way that the systems would be able to actually do the inferencing, but also recognize when something didn't look right so the the.",
                    "label": 0
                },
                {
                    "sent": "The overall graph doesn't make sense with certain inferences and makes more sense with others in terms of other graphs we've seen.",
                    "label": 0
                },
                {
                    "sent": "So this goes back to something I've been saying for many many years that what motivated a lot of the semantic web were back in the late 90s into the early 2000s was exactly that.",
                    "label": 0
                },
                {
                    "sent": "The web is noisy.",
                    "label": 0
                },
                {
                    "sent": "Write text extraction.",
                    "label": 0
                },
                {
                    "sent": "Write a really, really good text extraction system.",
                    "label": 0
                },
                {
                    "sent": "Might have a F1 metric of .9 three, which means 7% of this stuff is junk.",
                    "label": 0
                },
                {
                    "sent": "Not exactly, but close enough, right?",
                    "label": 0
                },
                {
                    "sent": "So in other words, there's a lot of errors in there, you just don't know what they are.",
                    "label": 0
                },
                {
                    "sent": "Things like that.",
                    "label": 0
                },
                {
                    "sent": "So the question is when we look at some of the things going on.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the web can we find?",
                    "label": 0
                },
                {
                    "sent": "You know the deep learning system do better than a traditional reasoner at recognizing the situations where there's something wrong.",
                    "label": 0
                },
                {
                    "sent": "So, just as an example in DB pedia.",
                    "label": 0
                },
                {
                    "sent": "Turns out.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are bunch of things that are both people and places OK in fact.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, sorry.",
                    "label": 0
                },
                {
                    "sent": "And of course, if you have something that's both the person place, then the place if the place was the correct inference, the inference off the person is that that's an agent, right?",
                    "label": 0
                },
                {
                    "sent": "But a place can't be an agent, so ascent.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Surely you can find a lot of these are just this one example of things that are both people and places you find at the time that we did.",
                    "label": 0
                },
                {
                    "sent": "This 1716 of them in DB pedia.",
                    "label": 0
                },
                {
                    "sent": "Right, so again, these are things where any decent reasoner will make a inference that is incorrect because at least one of these two statements must be incorrect, and in the real.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "World.",
                    "label": 0
                },
                {
                    "sent": "OK thanks so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So what we've somewhat blossoms thesis work was was to look at noise handling structures.",
                    "label": 1
                },
                {
                    "sent": "How did we really look at some other things?",
                    "label": 0
                },
                {
                    "sent": "And then how do we do graph embedding for this?",
                    "label": 1
                },
                {
                    "sent": "Any compared in the actual Journal paper?",
                    "label": 0
                },
                {
                    "sent": "A lot of comparisons that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can read about but Ascentia Lee the question is, you know, can we find a way an adaptive way of getting rid of the noise rather than doing the cleaning by hand for these things?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so the other thing is when we look at most graph embedding techniques right?",
                    "label": 1
                },
                {
                    "sent": "Most graph to graph learners right are not good at learning a graph which either adds or deletes information from the previous graph rather than just manipulates the graph or you end up with two graphs turned into vectors that you can do vector math on.",
                    "label": 0
                },
                {
                    "sent": "That may or may not actually have much to do with the actual inference space.",
                    "label": 0
                },
                {
                    "sent": "So after a long.",
                    "label": 0
                },
                {
                    "sent": "A lot of work trying to use many of the graph to graph deep learning systems eventually.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We came up with something.",
                    "label": 0
                },
                {
                    "sent": "Quite different because basically most of these embedding technologies were not designed for RDF S. The other thing is.",
                    "label": 1
                },
                {
                    "sent": "One of the things that we want to be looking at is things that are higher up in the hierarchy, right?",
                    "label": 0
                },
                {
                    "sent": "So if you're a person, there's an inference about things that you do as a person.",
                    "label": 0
                },
                {
                    "sent": "If you replace, there's an inference about things to do as a place both up and down.",
                    "label": 0
                },
                {
                    "sent": "So again that propagation.",
                    "label": 0
                },
                {
                    "sent": "Is complicated.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the overall approach that was used in Basson's work is essentially you take a graph.",
                    "label": 0
                },
                {
                    "sent": "Right, so this would be, you know, typically your assertion graph in RDF plus the ontology.",
                    "label": 0
                },
                {
                    "sent": "You can run any of your favorite reasoners.",
                    "label": 0
                },
                {
                    "sent": "He did through Jenna.",
                    "label": 0
                },
                {
                    "sent": "So Jenna will generate a new graph with at least one new triple in it, right?",
                    "label": 0
                },
                {
                    "sent": "Use the same encoding to create the graphs without the inference in the graph with the inference.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is an inference being made in this graph.",
                    "label": 0
                },
                {
                    "sent": "We turned it into a set of tensors, which I'll show you very briefly because.",
                    "label": 0
                },
                {
                    "sent": "The details are in the paper an RB in the doctoral thesis and being the advisor.",
                    "label": 0
                },
                {
                    "sent": "I won't claim that I know all the details.",
                    "label": 0
                },
                {
                    "sent": "That goes into a sequence to sequence learners where we can then do the actual comparison, training and evaluation.",
                    "label": 0
                },
                {
                    "sent": "So essentially we're turning the graphs into a set of tensors using those tensors into.",
                    "label": 0
                },
                {
                    "sent": "A sequence so it has a lot of similarity.",
                    "label": 0
                },
                {
                    "sent": "The last paper, with the exception that we're not doing the embedding per say we're not doing any of the embedding kind of maths.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For ground truthing, then what you can do for the cases where you actually know all of the things we can actually take a particular set of facts, we can say what should be done by a rule based system.",
                    "label": 0
                },
                {
                    "sent": "Did we actually learn what was supposed to be done, and so we can compare to something like a generation are.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or something like that.",
                    "label": 0
                },
                {
                    "sent": "So the work was done against two different datasets.",
                    "label": 0
                },
                {
                    "sent": "In the paper it's actually been tested on more than that.",
                    "label": 0
                },
                {
                    "sent": "One was an artificial data set, so we took plumbum.",
                    "label": 0
                },
                {
                    "sent": "We proved we could learn love 'em up to a fairly good level of accuracy and then added noise, which I'll talk about in a minute.",
                    "label": 0
                },
                {
                    "sent": "And you know, did the usual train validating test.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other thing that was running the valuation was actually to generate from DB pedia what we what was called the scientist data set which is not just the scientist.",
                    "label": 0
                },
                {
                    "sent": "It's sort of.",
                    "label": 0
                },
                {
                    "sent": "It's essentially what we would call the RDF S closure of all of the people who are asserted to be scientists.",
                    "label": 0
                },
                {
                    "sent": "Comes up with a lot of weird stuff, so if a scientist was born in London and we have facts about London, it will be in there.",
                    "label": 0
                },
                {
                    "sent": "So it's essentially propagating a lot of facts about and generates a very large.",
                    "label": 0
                },
                {
                    "sent": "A very large graph, so again we look at in places things like that that are related to the scientists.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the biggest question is how to embed this.",
                    "label": 0
                },
                {
                    "sent": "So this was really where the doctoral thesis.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This work that led to the.",
                    "label": 0
                },
                {
                    "sent": "To the to the Journal Paper an.",
                    "label": 0
                },
                {
                    "sent": "The key thing was finding a mechanism to turn an RDF graph into a a set of things which can be compared and then learn.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So essentially the idea is that you're using a tensor representation.",
                    "label": 0
                },
                {
                    "sent": "I don't really.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Time to go into it.",
                    "label": 0
                },
                {
                    "sent": "It turns out that if you use this representation, you have potentially a very large number of items.",
                    "label": 1
                },
                {
                    "sent": "If you do the whole cross product.",
                    "label": 0
                },
                {
                    "sent": "But if you actually look at how to lay things out as.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, what's happening is what you're really doing is you're taking all of the terms all of the.",
                    "label": 0
                },
                {
                    "sent": "So software version, organization, advisor, etc.",
                    "label": 0
                },
                {
                    "sent": "You have the entities in one thing, and then you have the.",
                    "label": 0
                },
                {
                    "sent": "Related properties in the other, so I can assert in each of those.",
                    "label": 0
                },
                {
                    "sent": "Each of those matrices there's a one in there somewhere which says this that represents this person as this advisor.",
                    "label": 0
                },
                {
                    "sent": "Right, and then you turn those sets of those graphs those tensors into see.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And says.",
                    "label": 0
                },
                {
                    "sent": "And so you end up with this sequence of words.",
                    "label": 0
                },
                {
                    "sent": "So essentially our DFS reasoning becomes figuring out which sequences are legal.",
                    "label": 0
                },
                {
                    "sent": "Which, given a set of given the graph, you got a second graph out which is the graph plus some plus the legal inference, the plus, the inferences that the system has learned.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Details so it uses a bidirectional recurrent neural network.",
                    "label": 0
                },
                {
                    "sent": "Lot of tricks use dropouts and things like that that are fairly standard for these.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of learning when you look at something like love 'em you see it?",
                    "label": 0
                },
                {
                    "sent": "Does it actually a pretty good job of learning for the scientist data set it took?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Longer now what was done for the actual evaluation.",
                    "label": 0
                },
                {
                    "sent": "The primary evaluation was generate the correct libm set.",
                    "label": 0
                },
                {
                    "sent": "Then we need some ground truth about something that's wrong, so you add some facts that are incorrect.",
                    "label": 0
                },
                {
                    "sent": "Now incorrect things in a reasoning system can be things which propagate because they are inferences off of them and things that can't, and so the paper has some discussion of this that I'm not going to go into, but essentially things that propagator where the interesting problem is.",
                    "label": 0
                },
                {
                    "sent": "Right, so if a student is if somebody is a grad student, they have a.",
                    "label": 0
                },
                {
                    "sent": "Particular University so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically what you're comparing to here is how our system does to Jenna for various things.",
                    "label": 0
                },
                {
                    "sent": "So for the noisy set, right, those things where are correct.",
                    "label": 0
                },
                {
                    "sent": "Inferences are defined by Jenna, so Jenna gets 100%.",
                    "label": 0
                },
                {
                    "sent": "So that's the GCC, right?",
                    "label": 0
                },
                {
                    "sent": "The other two are cases where noise has been introduced.",
                    "label": 0
                },
                {
                    "sent": "In one case the noise has been introduced in such a way that by definition you won't get the inferences and the other one.",
                    "label": 0
                },
                {
                    "sent": "By definition you will so.",
                    "label": 0
                },
                {
                    "sent": "What you see is that essentially the the important thing here is the green line.",
                    "label": 0
                },
                {
                    "sent": "So for our ATA, which is the one we're mainly talking about about one particular inference you see, it did almost as well as Jennifer, the other one.",
                    "label": 0
                },
                {
                    "sent": "It did much better because again, this is where Jenna was propagating the noise in this system wasn't.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the scientist data set, it's much harder to explain because you don't actually know what's correct and incorrect in there for everything, but just an example I mentioned 17161 errors of the actual of those in the that are in all of DB pedia 90.",
                    "label": 0
                },
                {
                    "sent": "Four of those fall into this data set.",
                    "label": 0
                },
                {
                    "sent": "One of the incorrect inferences is that people are agents.",
                    "label": 0
                },
                {
                    "sent": "So in six of the 94.",
                    "label": 0
                },
                {
                    "sent": "So I'm sorry in only in six of the cases did our influencer make that assumption.",
                    "label": 0
                },
                {
                    "sent": "Now of course, that would be easy to say it never.",
                    "label": 0
                },
                {
                    "sent": "You know if it never made any assumptions, then by definition we would never make any of the wrong ones.",
                    "label": 0
                },
                {
                    "sent": "So of the looking at all the things that could be in Ferd of the of the 94, there were 38 where it got exactly the inferences you would expect if you eliminated the fact it was a person.",
                    "label": 0
                },
                {
                    "sent": "Some of them had inferred properties that that Jenna couldn't make and that those were really the most interesting case and makes it somewhat leads to some of the future inference.",
                    "label": 1
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "This is one of my favorite one.",
                    "label": 0
                },
                {
                    "sent": "So Big Ben shows up in there and Big Ben was both said to be a person and place the system.",
                    "label": 0
                },
                {
                    "sent": "So Jenna of course would make it an agent.",
                    "label": 0
                },
                {
                    "sent": "Our system did not make it an agent, but added that it was a historic place.",
                    "label": 0
                },
                {
                    "sent": "Right, it looked like it had a lot of the same properties.",
                    "label": 1
                },
                {
                    "sent": "A lot of other things that were historic places.",
                    "label": 0
                },
                {
                    "sent": "Now, how do you evaluate whether that's correct or not?",
                    "label": 0
                },
                {
                    "sent": "Because it's not directly asserted that it's a historic place, so some of the inferencing you know is.",
                    "label": 0
                },
                {
                    "sent": "So when you do it in these very large real world sets, exactly what measures is not easy, but you see a lot.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have interesting things, so essentially.",
                    "label": 0
                },
                {
                    "sent": "We've really shown that you can get out of a learning system and RDF silly reason or that is more noise tolerant than a traditional reason are still not perfect.",
                    "label": 0
                },
                {
                    "sent": "Long way to go.",
                    "label": 0
                },
                {
                    "sent": "There was a lot of work that I didn't talk about giving the time about what sort of noise and noise tolerance and what a lot of that means.",
                    "label": 0
                },
                {
                    "sent": "I showed you very briefly in hand, WAVY the layer graph model and the graph words for how we encode the RDF, and again the details are all in the Journal Paper.",
                    "label": 1
                },
                {
                    "sent": "So you can you can see it and again as I said it right at the beginning, all of the code, models, examples etc are available.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where we going with this?",
                    "label": 0
                },
                {
                    "sent": "Well, so one real issue, obviously is generality.",
                    "label": 0
                },
                {
                    "sent": "So in theory I can take a deep.",
                    "label": 0
                },
                {
                    "sent": "I can take my.",
                    "label": 0
                },
                {
                    "sent": "Description Logic System put in any set of.",
                    "label": 0
                },
                {
                    "sent": "You know facts and get a set of inferences out.",
                    "label": 0
                },
                {
                    "sent": "Without by definition, the good thing about is I don't have any domain.",
                    "label": 0
                },
                {
                    "sent": "Changes here of course, depending on how many, how many things you have, how they do it, how you train, etc.",
                    "label": 0
                },
                {
                    "sent": "You don't get that same kind of generality, so there's a tradeoff there.",
                    "label": 0
                },
                {
                    "sent": "Bastion is now at IBM Research, where he's actually looking at the transfer learning solve some of these problems and then something that we're looking at together is one of the nice things about the learning actually being able to find and or eliminate the different RDF.",
                    "label": 0
                },
                {
                    "sent": "Statement is that you have Providence with those statements, so now you can say who asserted the thing that was wrong.",
                    "label": 0
                },
                {
                    "sent": "Right, so the system decided that Big Ben was not a person.",
                    "label": 0
                },
                {
                    "sent": "Can it actually say hey, this is where that you know in Wikipedia who did that?",
                    "label": 0
                },
                {
                    "sent": "Or in some other system, so you can start actually looking at some of the trust stuff and I will basically stop there.",
                    "label": 0
                },
                {
                    "sent": "I'll answer whatever questions I can, those I can.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not you can send Tabassum OK, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "We have time for maybe one or two quick questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you, can you comment on whether say, sub property reasoning or domain range reasoning or subclass reasoning which one is?",
                    "label": 0
                },
                {
                    "sent": "Better, or you know how it's off a part?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so.",
                    "label": 0
                },
                {
                    "sent": "So the fast answer is, if you just take the general our DFS rules and just train it directly, so no noise right, it will get too.",
                    "label": 0
                },
                {
                    "sent": "I forget the exact number, it's in the paper, but high 90s percent correctness, right?",
                    "label": 0
                },
                {
                    "sent": "So it's essentially doing a fine job on all of the standard inferencing.",
                    "label": 0
                },
                {
                    "sent": "Caveat reasonable size, sad alot of time.",
                    "label": 0
                },
                {
                    "sent": "You know all the usual learning things.",
                    "label": 0
                },
                {
                    "sent": "OK, interesting Lee when but when you add noise again the different kind of noise.",
                    "label": 0
                },
                {
                    "sent": "So for example properties subproperties things like that propagate noise differently than class, subclass etc.",
                    "label": 0
                },
                {
                    "sent": "So that's what that stuff I kinda hand waved about the propagation and then I'm getting what we sort of showed is that it does a good job of cutting off the ones that so given something that's asserted with multiple properties.",
                    "label": 0
                },
                {
                    "sent": "Right, if some of those properties start not to look right, they just got cut off and again that can be on most of these were in the class hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Inferences you get the same thing in property, but for example in DB pedia you have very few prep.",
                    "label": 0
                },
                {
                    "sent": "You don't have any long sub property in prisons like that.",
                    "label": 0
                },
                {
                    "sent": "So again it was it was pretty carefully controlled for the actual paper an does pretty well but was not tested in all different types.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks very quick one.",
                    "label": 0
                },
                {
                    "sent": "Just now we will talking about corruptions of the triples.",
                    "label": 0
                },
                {
                    "sent": "You mentioned summer summer coming propagated with some cannot be.",
                    "label": 0
                },
                {
                    "sent": "Can you talk a bit more on why certain things can be propagated in others that cannot be so.",
                    "label": 1
                },
                {
                    "sent": "So again, it depends whether what kind of inferences come off of the noise that you add.",
                    "label": 0
                },
                {
                    "sent": "So if you are a grad student who has an advisor, I simply assert that you are not a grad student, or I simply leave that out.",
                    "label": 0
                },
                {
                    "sent": "There is no advisor in principle.",
                    "label": 0
                },
                {
                    "sent": "Grad students have advisors, and you're not graduate, so that doesn't propagate right.",
                    "label": 0
                },
                {
                    "sent": "If I say you're at a certain University, then that implies.",
                    "label": 0
                },
                {
                    "sent": "If so, there are things.",
                    "label": 0
                },
                {
                    "sent": "So again, if I say you you are a grad student and an undergraduate, then you end up with an advisor.",
                    "label": 0
                },
                {
                    "sent": "And if I forget the exact details.",
                    "label": 0
                },
                {
                    "sent": "Yep, thanks, maybe we could take the questions offline again so so it's all defined in the paper.",
                    "label": 0
                },
                {
                    "sent": "If you look at it, but against some of these things when you make a wrong assertion, all it does is remove inferences.",
                    "label": 0
                },
                {
                    "sent": "So nothing propagates right.",
                    "label": 0
                },
                {
                    "sent": "Others of these, when you put them in, you end up with propagation and there's even somewhere if you take them out you can get some propagation, but that's pretty rare category, so that's all in the in the Journal Paper how this works.",
                    "label": 0
                },
                {
                    "sent": "Or interesting work, thank you.",
                    "label": 0
                }
            ]
        }
    }
}