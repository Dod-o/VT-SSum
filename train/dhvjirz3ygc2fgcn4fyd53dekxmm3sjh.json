{
    "id": "dhvjirz3ygc2fgcn4fyd53dekxmm3sjh",
    "title": "Time-series information and unsupervised representation learning",
    "info": {
        "author": [
            "Daniil Ryabko, SequeL lab, INRIA Lille - Nord Europe"
        ],
        "published": "Aug. 6, 2013",
        "recorded": "April 2013",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/machine_ryabko_representation_learning/",
    "segmentation": [
        [
            "I'm the new Repko from in Red Hill and I'm going to talk about time series information and supervised and unsupervised learning and which is a very recent work of mine.",
            "And also my saying I must say I'm a mathematician so please bear with me."
        ],
        [
            "OK, so we have the following problem and we have a time series XX1X2 and so on and works.",
            "I come from some high dimensional space and we want to find a representation of this sequence in the form of another sequence, YY Y1Y2 and so on where this yr from some small loud dimensional space or just from a finite set.",
            "OK, and then what we want from the representation is that we preserve as much as possible of the time series dependence in the original sequence XI, and so why would we need this series?",
            "Because if then someone gives us some problem about this data, for example, someone gives us labels, or maybe someone gives us rewards in a reinforcement learning setup, then we would be able to solve this problem much more easier and faster because we have a much smaller space.",
            "OK, so this problem can be sort of a dimensionality reduction in with the focus on time series dependent."
        ],
        [
            "So OK, so let's try to formalize this problem a little bit.",
            "So in the.",
            "In an ideal situation, we would like to have a function F such that given F of XI and the the initial inputs as I become conditionally independent.",
            "So given the FX, I would like to be able to discard XI and so it can be shown that in this situation, finding the function F that achieves this property amounts to maximizing assertive functional, which is the time series.",
            "Automation here, which is a zero of F of X minus age Infinity, where H 0 is a zero order entropy of F of XI&H Infinity is the entropy rate, so a 0 is how random the sequence looks if you discard time series dependence and Asian finishes, how random it looks?",
            "If you know and learn perfectly well all the time series dependence so it can be shown that that under this conditionally independence conditional.",
            "In this condition, the function that is the best representation maximizes this functional.",
            "The time series information I Infinity of F. And so this basically allows us to formulate the problem that given a set of functions F from X to Y, we would like to find the function that maximizes this functional OK."
        ],
        [
            "And So what is very important is this functionaly Infinity of F and does not involve the value of this function.",
            "We do not need to consider model the distribution of the original time series XI.",
            "So if you want to estimate this thing, we only need to work with the representations F of XI.",
            "OK, and so I can have some some learning theory style results about this.",
            "Basically that shows that empirically you can find this quite easily if you just maximize instead of the theoretical function, the empirical version.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm the new Repko from in Red Hill and I'm going to talk about time series information and supervised and unsupervised learning and which is a very recent work of mine.",
                    "label": 0
                },
                {
                    "sent": "And also my saying I must say I'm a mathematician so please bear with me.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we have the following problem and we have a time series XX1X2 and so on and works.",
                    "label": 0
                },
                {
                    "sent": "I come from some high dimensional space and we want to find a representation of this sequence in the form of another sequence, YY Y1Y2 and so on where this yr from some small loud dimensional space or just from a finite set.",
                    "label": 1
                },
                {
                    "sent": "OK, and then what we want from the representation is that we preserve as much as possible of the time series dependence in the original sequence XI, and so why would we need this series?",
                    "label": 1
                },
                {
                    "sent": "Because if then someone gives us some problem about this data, for example, someone gives us labels, or maybe someone gives us rewards in a reinforcement learning setup, then we would be able to solve this problem much more easier and faster because we have a much smaller space.",
                    "label": 1
                },
                {
                    "sent": "OK, so this problem can be sort of a dimensionality reduction in with the focus on time series dependent.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK, so let's try to formalize this problem a little bit.",
                    "label": 0
                },
                {
                    "sent": "So in the.",
                    "label": 0
                },
                {
                    "sent": "In an ideal situation, we would like to have a function F such that given F of XI and the the initial inputs as I become conditionally independent.",
                    "label": 1
                },
                {
                    "sent": "So given the FX, I would like to be able to discard XI and so it can be shown that in this situation, finding the function F that achieves this property amounts to maximizing assertive functional, which is the time series.",
                    "label": 0
                },
                {
                    "sent": "Automation here, which is a zero of F of X minus age Infinity, where H 0 is a zero order entropy of F of XI&H Infinity is the entropy rate, so a 0 is how random the sequence looks if you discard time series dependence and Asian finishes, how random it looks?",
                    "label": 0
                },
                {
                    "sent": "If you know and learn perfectly well all the time series dependence so it can be shown that that under this conditionally independence conditional.",
                    "label": 0
                },
                {
                    "sent": "In this condition, the function that is the best representation maximizes this functional.",
                    "label": 0
                },
                {
                    "sent": "The time series information I Infinity of F. And so this basically allows us to formulate the problem that given a set of functions F from X to Y, we would like to find the function that maximizes this functional OK.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And So what is very important is this functionaly Infinity of F and does not involve the value of this function.",
                    "label": 1
                },
                {
                    "sent": "We do not need to consider model the distribution of the original time series XI.",
                    "label": 0
                },
                {
                    "sent": "So if you want to estimate this thing, we only need to work with the representations F of XI.",
                    "label": 1
                },
                {
                    "sent": "OK, and so I can have some some learning theory style results about this.",
                    "label": 0
                },
                {
                    "sent": "Basically that shows that empirically you can find this quite easily if you just maximize instead of the theoretical function, the empirical version.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}