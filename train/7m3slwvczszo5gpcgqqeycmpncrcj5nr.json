{
    "id": "7m3slwvczszo5gpcgqqeycmpncrcj5nr",
    "title": "Achieving All with No Parameters: Adaptive NormalHedge",
    "info": {
        "author": [
            "Haipeng Luo, Department of Computer Science, Princeton University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_luo_adaptive_normalhedge/",
    "segmentation": [
        [
            "Good morning I'm hyped from Princeton, so this is a joint work with my advisor option."
        ],
        [
            "So we studied the classic problem of predicting with expert advice, which I'm sure most of you are familiar with.",
            "So imagine on each day you need to make a decision of a particular problem, and you're given the advice of a bunch of experts.",
            "So what you do?"
        ],
        [
            "The former E. Or is she want you?",
            "You predict the distribution PT over all the experts and after that the adversary will reveal the loss of each expert and and you just suffer the expected loss for this round.",
            "And the goal here is to minimize the regret to the best expert, which is the difference between your total loss and the loss of the best expert in hindsight.",
            "And more generally, you may even want to consider the regret to any fixed strategy U which says if I could have stuck with using you all the time.",
            "Then how much less lost I will have suffered and the hope here is of course to obtain a sublinear regret, so that on average we're doing almost as well as the competitor.",
            "And there are a lot of algorithms that give us sublinear regret but be."
        ],
        [
            "In the classical regret, people have been studying and many more difficult objectives.",
            "For example, you might want your algorithm to achieve much smaller regret when the problem is easy in some sense, while still making sure that the algorithm is doing the right thing in the worst case.",
            "And other examples, including learning with unknown number of experts competing with the sequence of different competitors instead, instead of a fixed one, or learning with experts who provide a confidence rating, advice and all this goes will study a previously.",
            "Extent but with different algorithms, so this work is about one single parameter free algorithm that can achieve all these goals and with improved results."
        ],
        [
            "So what's the algorithm require our algorithm at a normal hash, which is a new variant of normal hatch, do Treasury front and through a few years ago, and the prediction rule can be summarized in this one line.",
            "So here PT is just the weight you put on expert I or around T&Q is a prior distribution.",
            "So just think of it as a uniform distribution.",
            "If you don't have a prior and capital R is the cumulative regret up to the current point.",
            "Capital C is the cumulative manage of the regret.",
            "Up to current one, which is the sum of the absolute values of the instantaneous regrets.",
            "So this is the main difference between our new algorithm compared to our previous version appeared and now slips.",
            "So it is making use of some sort first in first order information by using this.",
            "So finally W is a wait function, which is basically the the discrete derivative of a potential function and the potential function Phi here is just common in the normal hedge family, so I can explain more if you come to my poster.",
            "But the point here is that the algorithm is completely parameter free, so it doesn't even has an explicit dependence on the number of experts."
        ],
        [
            "So, so what's the regret guarantee?",
            "Is this out there for any competitor you simultaneously the regret of other normal hedges is of order, square root of Udacity times the relative entropy of the competitor U and the prior Q.",
            "So here you should think of Udacity as roughly the loss of the competitor.",
            "So what does this mean?"
        ],
        [
            "It has several nice implications.",
            "First, it means that you will have very small regret when the competitor also has small loss.",
            "And second, you can prove a constant regret when the losses are stochastic.",
            "So in the world, your algorithm will have a small regret when when the problem is easy.",
            "And finally, it also gives a better epsilon.",
            "Can't I'll regret the resolving an open problem proposed previously.",
            "So epsilon content regret is just the regret compares to all but the top epsilon fraction of experts.",
            "And of course you want to do it simultaneously for all epsilon, which is exactly what our algorithm does.",
            "And final."
        ],
        [
            "We extend our alchemist, the sleeping experts setting and it's very natural to do this for our algorithm because it doesn't.",
            "It does not need to know the total number of sleeping experts in advance, and based on this is extensions.",
            "We propose two applications.",
            "The first one is to compete with time variant competitors and so so instead of comparing with a fixed U, we even let you to vary overtime.",
            "So and we do this by first showing that this similar hard problem.",
            "Is actually equivalent to a very special case, which can then be reduced to a slipping expert problem, and then we can just apply our new algorithm and capital reserves and this actually resolve another open problem on optimal shifting regret proposed last NIPS and the second application is on a predicting as well as the best burning tree, so I don't have time for this, but for more details, please come to my poster, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good morning I'm hyped from Princeton, so this is a joint work with my advisor option.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we studied the classic problem of predicting with expert advice, which I'm sure most of you are familiar with.",
                    "label": 1
                },
                {
                    "sent": "So imagine on each day you need to make a decision of a particular problem, and you're given the advice of a bunch of experts.",
                    "label": 0
                },
                {
                    "sent": "So what you do?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The former E. Or is she want you?",
                    "label": 0
                },
                {
                    "sent": "You predict the distribution PT over all the experts and after that the adversary will reveal the loss of each expert and and you just suffer the expected loss for this round.",
                    "label": 1
                },
                {
                    "sent": "And the goal here is to minimize the regret to the best expert, which is the difference between your total loss and the loss of the best expert in hindsight.",
                    "label": 0
                },
                {
                    "sent": "And more generally, you may even want to consider the regret to any fixed strategy U which says if I could have stuck with using you all the time.",
                    "label": 1
                },
                {
                    "sent": "Then how much less lost I will have suffered and the hope here is of course to obtain a sublinear regret, so that on average we're doing almost as well as the competitor.",
                    "label": 0
                },
                {
                    "sent": "And there are a lot of algorithms that give us sublinear regret but be.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the classical regret, people have been studying and many more difficult objectives.",
                    "label": 0
                },
                {
                    "sent": "For example, you might want your algorithm to achieve much smaller regret when the problem is easy in some sense, while still making sure that the algorithm is doing the right thing in the worst case.",
                    "label": 1
                },
                {
                    "sent": "And other examples, including learning with unknown number of experts competing with the sequence of different competitors instead, instead of a fixed one, or learning with experts who provide a confidence rating, advice and all this goes will study a previously.",
                    "label": 1
                },
                {
                    "sent": "Extent but with different algorithms, so this work is about one single parameter free algorithm that can achieve all these goals and with improved results.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the algorithm require our algorithm at a normal hash, which is a new variant of normal hatch, do Treasury front and through a few years ago, and the prediction rule can be summarized in this one line.",
                    "label": 0
                },
                {
                    "sent": "So here PT is just the weight you put on expert I or around T&Q is a prior distribution.",
                    "label": 0
                },
                {
                    "sent": "So just think of it as a uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "If you don't have a prior and capital R is the cumulative regret up to the current point.",
                    "label": 1
                },
                {
                    "sent": "Capital C is the cumulative manage of the regret.",
                    "label": 0
                },
                {
                    "sent": "Up to current one, which is the sum of the absolute values of the instantaneous regrets.",
                    "label": 0
                },
                {
                    "sent": "So this is the main difference between our new algorithm compared to our previous version appeared and now slips.",
                    "label": 0
                },
                {
                    "sent": "So it is making use of some sort first in first order information by using this.",
                    "label": 0
                },
                {
                    "sent": "So finally W is a wait function, which is basically the the discrete derivative of a potential function and the potential function Phi here is just common in the normal hedge family, so I can explain more if you come to my poster.",
                    "label": 0
                },
                {
                    "sent": "But the point here is that the algorithm is completely parameter free, so it doesn't even has an explicit dependence on the number of experts.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so what's the regret guarantee?",
                    "label": 0
                },
                {
                    "sent": "Is this out there for any competitor you simultaneously the regret of other normal hedges is of order, square root of Udacity times the relative entropy of the competitor U and the prior Q.",
                    "label": 1
                },
                {
                    "sent": "So here you should think of Udacity as roughly the loss of the competitor.",
                    "label": 0
                },
                {
                    "sent": "So what does this mean?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It has several nice implications.",
                    "label": 0
                },
                {
                    "sent": "First, it means that you will have very small regret when the competitor also has small loss.",
                    "label": 1
                },
                {
                    "sent": "And second, you can prove a constant regret when the losses are stochastic.",
                    "label": 0
                },
                {
                    "sent": "So in the world, your algorithm will have a small regret when when the problem is easy.",
                    "label": 1
                },
                {
                    "sent": "And finally, it also gives a better epsilon.",
                    "label": 0
                },
                {
                    "sent": "Can't I'll regret the resolving an open problem proposed previously.",
                    "label": 0
                },
                {
                    "sent": "So epsilon content regret is just the regret compares to all but the top epsilon fraction of experts.",
                    "label": 0
                },
                {
                    "sent": "And of course you want to do it simultaneously for all epsilon, which is exactly what our algorithm does.",
                    "label": 0
                },
                {
                    "sent": "And final.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We extend our alchemist, the sleeping experts setting and it's very natural to do this for our algorithm because it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It does not need to know the total number of sleeping experts in advance, and based on this is extensions.",
                    "label": 1
                },
                {
                    "sent": "We propose two applications.",
                    "label": 0
                },
                {
                    "sent": "The first one is to compete with time variant competitors and so so instead of comparing with a fixed U, we even let you to vary overtime.",
                    "label": 0
                },
                {
                    "sent": "So and we do this by first showing that this similar hard problem.",
                    "label": 1
                },
                {
                    "sent": "Is actually equivalent to a very special case, which can then be reduced to a slipping expert problem, and then we can just apply our new algorithm and capital reserves and this actually resolve another open problem on optimal shifting regret proposed last NIPS and the second application is on a predicting as well as the best burning tree, so I don't have time for this, but for more details, please come to my poster, thanks.",
                    "label": 0
                }
            ]
        }
    }
}