{
    "id": "s7fm3fn32sjlbphvnv46swnx2c7ewqhk",
    "title": "Multiple Target Localisation at over 100 FPS",
    "info": {
        "author": [
            "Simon Taylor, University of Cambridge"
        ],
        "published": "Dec. 1, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/bmvc09_taylor_mtl/",
    "segmentation": [
        [
            "I'm I'm Simon.",
            "I'm PhD student at Cambridge, supervised by Tom.",
            "And my work is funded by Boeing, so thanks to them."
        ],
        [
            "The problem I'm looking at is rapid localization of known targets.",
            "In a novel view from a camera image.",
            "Such as showing up there.",
            "But we want to do it as quickly as possible because the.",
            "The application we have in mind here is mobile phone based augmented reality applications.",
            "So here we can see this 3D object that appears to be floating above the target.",
            "On the mobile there is localising in every frame the target.",
            "In about 40 milliseconds, so it's running at 15 frames a second.",
            "And the content is just pulling out these images so you can see the house that you might go in and slightly more clearly."
        ],
        [
            "OK, so we use a local feature method for this.",
            "Local features of proved successful for many applications and they usually have these two independent stages.",
            "So the first stage is interest region detection, which involves factoring out as many transformations as you can.",
            "Typically translation often scale as well with dog detection.",
            "Sometimes I find.",
            "To try and reduce the difference, the appearance difference between your features.",
            "However, after that there are still some differences remaining, either through errors in the parameter assignment from the first stages, we just been hearing about, or because you have chosen not to try and factor it out, such as if you're using a fixed scale detector.",
            "And so a region matching approach needs to handle all of these differences.",
            "And the two typical categories of matching approaches that have been studied are descriptors such as sift, mops and glow, which do some image processing.",
            "Some further image processing, often blurring some soft bend histograms to try and.",
            "Reduce the effects of those differences and alternative approaches to train a classifier on these.",
            "Different on these patches of different appearance coming out of your region detector.",
            "And that's being studied in ferns and randomized trees work.",
            "The descriptor approach is symmetric in that you do the same thing on both images you're trying to match and your matching score is given by the distance in your descriptor space, whereas the classifier approach you have a separate offline training phase.",
            "And to learn your model for the feature and then at runtime that means you can often do less work and achieve faster matching.",
            "So our approach is classification based.",
            "However, unlike the ferns and randomized trees approaches, we don't use any joint distributions, so our memory cost is much smaller and that was one of the major problems with the previous approaches."
        ],
        [
            "So the basic units if you like of our system, is these quantized patches, very simple to extract for an image, just sample every other pixel.",
            "Calculate the mean and standard deviation and then quantize heavily to 5 levels.",
            "Now, this model here is obviously not doesn't carry a lot of information about the appearance of the feature, and you wouldn't expect it.",
            "To be useful for matching, especially in the localization case, where you've got no prior information about the position and the appearance.",
            "However, This is why we have a classifier to learn a model for the.",
            "The range of patches corresponding to the same feature.",
            "So for this we have a training set for a large set of images."
        ],
        [
            "So we also officially warp.",
            "This is an offline stage on a PC.",
            "A few thousand 10,000 or so images from him from an original reference image.",
            "And we group them into these viewpoint bins.",
            "Now here you can see we have we have separate scale bins and that allows us to use.",
            "Fast fixed, fixed scale color detection.",
            "Rather than needing to do a scale space search runtime, so we trade off that cost for having more features in the database.",
            "And we'll learn separate sets of features for each of these viewpoint bins.",
            "And although here we only show top down center begins, you could extend this kind of idea in order to have more extreme affine viewpoint bins.",
            "If you want your localization to work over those kind of ranges.",
            "So now we extract."
        ],
        [
            "Something from every one of those images that we turn subfeatures and this is.",
            "These things are going to be built together to give us our features, but from a single image.",
            "Will run fast.",
            "9 interest point detection.",
            "Which is referenced in the paper and this very simple orientation.",
            "Measure just based on the sum of some pixel differences.",
            "So again, it's purely designed for speed.",
            "These aren't accurate.",
            "Measures they are likely to vary a lot over our viewpoint, but because we have this training phase, we can learn about how these measures are going to vary and that means it doesn't necessarily cost us a lot to have.",
            "Cheap metrics like this.",
            "It doesn't cost a lot in terms of performance.",
            "So from this single image.",
            "We've highlighted one feature here.",
            "And we rotate our sparse 8 by 8 grid to be aligned with that orientation.",
            "Extracts the pixels from that image.",
            "Bilinear interpolation gets used and then do the quantization.",
            "So what are some feature stores here?",
            "Is the location of the interest point.",
            "It stores the orientation.",
            "And it stores this.",
            "Quantized view open that corner.",
            "As the appearance information from this particular warped image.",
            "So we then do that on all of our walks, images in the viewpoint bin.",
            "And then we want to."
        ],
        [
            "Them and the clustering we can do because we know the warp that generated these images so we can walk back the coordinates from our interest point detection.",
            "So that's a single image if we add.",
            "All of the corners from all of the images in our viewpoint bin.",
            "We get a picture like this over here.",
            "And you can see, for example, this point on the exclamation mark is very poorly localized in terms of rotation.",
            "You can see a kind of Halo around the whole thing because there's a rotation measure is essentially random for that feature, but most of the others are all sort of pointing in the same direction, which is all we really want out of this data.",
            "So now we have all that data we should be able to select.",
            "What are good features to include in our model for the target.",
            "So we do that by setting a hard bound on the errors were willing to accept from interest point section and rotation assignment.",
            "So here we allow two pixels of error.",
            "In position and plus minus 10 degrees in terms of rotation.",
            "And then for every single one of those sub features, we consider it as the center of a cluster and count how many other sub features would fall into that cluster with those bounds.",
            "So that you can see a little blue one with this sort of yellow Halo, you probably can't sit too well and that's the yellow ones are all the sub features that would be included in the cluster if the blue one was chosen as the center, there's another one over there.",
            "So then we can pick the which sub features are the best centers in a sort of greedy manner to take the one with the most, remove those from the set and then find the next.",
            "So this I'm just passing over here, this sentence of the clusters that we've chosen to use to represent this target.",
            "Now you can see how the fact that we're using a cheap orientation assignment scheme is not too important, because here in the case like this point where the orientation isn't very well defined, we've actually assigned three separate features to model that whole space, each of which only needs to deal with that 10 degree difference.",
            "So this gives us a way of setting this hard bound on the.",
            "Interesting region detection stage.",
            "Sort of intelligently selecting the best features to use.",
            "So OK, each one of these points is going to feature in our model.",
            "A feature in our yeah in our target database.",
            "So how do we go about building the features?",
            "Right well."
        ],
        [
            "The model we use features are these histograms intensity patches?",
            "So here we can see 4 sub features that go into the most repeatable feature from that target.",
            "And if we consider just the top left pixel over there and pull it into a histogram from those four, we can see that the leftmost 2 images there are both intensity level 4.",
            "And the others give you the two other bars there.",
            "If you include all of the sub features in that cluster into our little histogram.",
            "We get this distribution for this pixel, so we can kind of see this pixel doesn't really tell us very much about whether or not this.",
            "An input Patch could be this feature or not.",
            "However, if an input Patch was white.",
            "Then the fact that there's actually no none of our training subfeatures wherever whites that gives us quite a strong idea that it's probably not a match.",
            "So OK, that's the top left pixel.",
            "What we can do is do the same thing for all the other pixels in our descriptor, so this gives us the complete histogram intensity.",
            "Patch this hip model and you can see it captures this important.",
            "Information for the feature that this error here is very consistently white in all of our training samples that there's always this dark diamond shape over in this side.",
            "So that's all captured in that model.",
            "But really, we don't need that much detail in there.",
            "As I said, the important bit so that this is."
        ],
        [
            "Right, and this is dark.",
            "So actually we can threshold these histogram values now to just a single bit each.",
            "So we use a threshold of 5% so.",
            "If during training.",
            "The histogram was less than 5%.",
            "Then we say that's a rare.",
            "That's a rare intensity level for this sample.",
            "If it's more than 5%, then it's implausible level.",
            "So now we just need 1 bits for every one of these lines on this diagram.",
            "And we choose to use one to represent the rarely seen intensities in the model.",
            "So that's things where if a runtime Patch.",
            "If Pixel four in runtime patches anything other than white, then that will count as an error, you know.",
            "Matching scheme."
        ],
        [
            "So that's our dissimilarity score.",
            "It's the account.",
            "Of the number of pixels in a runtime Patch that fall into these ribbons in a feature model.",
            "So here's a matching Patch.",
            "I took it from the set of sub features that generated all this data and we have a binary representation for this as well, so this is the first pixel which is level 4, the second pixel level 4, and the 3rd and 4th or both level 5 'cause they're both white.",
            "So if we represent it like this, we can count the number of those bits in the rabbits from the feature just by handing that with our hip representation.",
            "And then bit counting the result.",
            "And actually, we can arrange the bits such that we only need to do a 64 bits bit count.",
            "Just to speed things up.",
            "So that's much impact with the non matching Patch.",
            "This is just a random other one.",
            "Over the 1st four pixels are all black hair, so all of those look like that.",
            "And then if you end it with your feature model you can see we get an error from both the 2nd and the 3rd.",
            "Pixels.",
            "So.",
            "The overall error is 2 plus it's a lot higher than two, clearly.",
            "If we want to calculate it for all the pixels and so we can just set a threshold on this number now and it turns out that a low threshold is actually fine, anything greater than 4?",
            "Is not a match in our system and a thing greater than two is the second match, so we have this these two thresholds.",
            "So if it's 01 errors from the Patch that counts the primary match, which we then use.",
            "That information to try and figure out.",
            "Which are the best viewpoints?",
            "But we can.",
            "We can do some more than that rather than just comparing against all of our models in the database, we can actually combine them.",
            "So by ending together two of."
        ],
        [
            "These binary representations, we get another.",
            "Another hip if you like that actually will represent the common.",
            "Rabbins between those two parents.",
            "So if we then compare against that.",
            "Parent.",
            "And we have more than our error threshold of errors.",
            "Then we don't need to do the exhaustive comparison.",
            "We don't need to compare against the two children because it sets a hard lower bound just matching to the handed version.",
            "So let's have an animation that might clarify things a bit.",
            "So these two.",
            "Is obviously much.",
            "There's only nine bit long and 320 bit in rail system, but they share 41 bits in common, so this is 4.",
            "Rare bins in in the histograms.",
            "So we pull it out by ending.",
            "The feature representations together.",
            "And now we will do a comparison at runtime against this parent thing here.",
            "So if at runtime we get say 4 hours there in our thresholds too, then we can stop and we know automatically that that Patch will not match to either the red or the green, even though we've only done this one comparison.",
            "And we can actually continue using this combining the most overlapping features in order to build tree.",
            "So these are the features that haven't yet been represented in our tree.",
            "The most overlap now is between the new parent and this Purple One.",
            "So we pull out the shared 3 features there and we repeat.",
            "Until we get to the stage where there are features in the root, here have no bits in common.",
            "So now there's nothing more we can do.",
            "But at runtime this means.",
            "In the best case, we only need to make 2 comparisons in order to project a match against all of our features.",
            "In the database, and then?",
            "Because because it's a binary tree, it has nice scaling properties as well."
        ],
        [
            "So that's how we extract all of our matches at runtime.",
            "Then we can do a half transform because because of our viewpoints we have a scale.",
            "We know the rough scale of each feature.",
            "They have their own orientation as well, so we can build that into.",
            "16 or so orientation bins.",
            "To extract all the features that agree at least on the on the course scale and rotation of the target.",
            "No more than that.",
            "We can see a slightly more finer viewpoint consistency.",
            "Check from a single target, so here the blue feature represents the lowest scoring match, so the."
        ],
        [
            "As much that we think.",
            "So in the reference frame here in the viewpoint Russian frame, we can workout the vectors to the other expected matches.",
            "Rotate that, do the similarity transform on that.",
            "And we have this acceptance region to account for the fact that it might not be a top down view and there is a range of scales in this cabin.",
            "And then if the match does appear inside that acceptance region then it means it's a good match.",
            "If it doesn't, we can reject it, so this much here.",
            "There should be a green line that is OK, so this is.",
            "This match is consistent with the blue one.",
            "So once we do that, we can get a set set of features that we can be reasonably sure.",
            "Are consistent and that means we don't need to do many iterations on Prozac on robust estimation in order to get the pose in the end.",
            "OK, so some results.",
            "Wagner in 2009.",
            "How to a paper with?",
            "Making adjustments to the Sifton ferns approach is to make them suitable for mobile phone based.",
            "Augmented reality type applications.",
            "This is one of the sequences.",
            "That they were using on the box shows our results on it.",
            "On their PC they they record reported about 5 milliseconds total frame time.",
            "On these sequences, and they localized about 96% of the frames.",
            "You can see the seven sequences have.",
            "A fairly wide range in terms of density of the texture of a photo summer graphics.",
            "So our results on here we."
        ],
        [
            "In terms of time, it takes around one millisecond for all of those targets.",
            "Which is over four times faster, even accounting for the slightly faster computer we were using.",
            "Our business is increased to 99.6% on average.",
            "And also memory usage because these binary models are only 40 bytes each, we only require about 100 kilobytes.",
            "For storing our databases.",
            "I haven't talked about the indexing here, but that's in the paper."
        ],
        [
            "Right, so now because this is so quick I thought can we combine all these targets into the same database and recognize more than one thing at once?",
            "So this is the video that the results in the paper based on and it shows us looking for all seven of those targets in every frame.",
            "And then any that are found, we try and draw the box around and there's no tracking happening at all there.",
            "So it's it's just the raw output from the localization scheme.",
            "Because of the fixed scale interest point detector, using you'll see in a second that we can't handle extreme blur very well, so it tends to lose it here, but as long as things are reasonably focused, it works reasonably well.",
            "So what we're interested in here?"
        ],
        [
            "Is how does the speed scale and that's shown in the graph on the left here?",
            "So even with this binary tree approach, it seems like we're scaling sort of linearly here, so maybe that's just because we haven't got enough features still to exploit this similarity fully.",
            "Suddenly something I'm looking at in the future.",
            "But still, even with all seven targets in the database.",
            "And using a simple indexing scheme that's.",
            "In the paper.",
            "It takes about 7 1/2 milliseconds to localize, so that's.",
            "Over 100 pounds, and even with all seven of those targets, the other interesting graph is this one here so.",
            "This is processing the number of frames in which the advert the poster target was localized.",
            "And each point represents the addition of a new target to the database.",
            "So the first point is just the poster in the database.",
            "The second one answered the book, and the third one is the the cars poster, etc.",
            "And you can see that adding these additional targets does have some impact on the number of frames localized, but it's only fairly minimal.",
            "It's only a drop of about 10 frames, and that's because we're using this classification approach now, so we're not we're not limited to.",
            "Finding the nearest neighbour of a match, as in a sort of descriptor scheme, because we have a classifier that actually says yes.",
            "This is plausibly from that feature and that so we will extract exactly the same set of matches regardless of the number of targets in the database.",
            "So that begs the question, why did he get a drop at all?",
            "That's because.",
            "In our viewpoint consistency stage, we only check the best few viewpoints so that the few viewpoints with the most good matches.",
            "So then you start getting a bit of noise in that and the correct viewpoint maybe drops out of that list.",
            "But it seems after a few targets are added then it stays reasonably stable."
        ],
        [
            "OK, so using this classification based matching we can have robust fast localization suitable for mobile phone platforms.",
            "I'll hit model is computationally efficient and doesn't require much memory.",
            "Using this training phase in our two paths approach allows us to characterize the interest region detection stages.",
            "We can limit the impact of using inaccurate region detection and then the tree based look up allows us to return the exact classification results, but without having to exhaustively compare against everything in the database.",
            "OK, so I'm happy to take questions.",
            "It's 2.4 Giga Hertz Core 2.",
            "I was only using single quotes or single threaded.",
            "If you're running on a mobile phone, you know so the mobile phone.",
            "Is.",
            "It's currently 40 milliseconds per frame.",
            "For a single target in the database.",
            "That's a, it's an.",
            "It's a corsac seats.",
            "It's a new.",
            "600 megahertz on a on a friend or megahertz and 95 we get about it.",
            "Takes about 60 milliseconds, so it doesn't actually double, and that's mainly overhead in the phone in terms of getting images from the camera, converting them to the right color space.",
            "All that kind of stuff.",
            "More questions.",
            "The training.",
            "At the moment is around half an hour per target.",
            "The work I'm doing right now is about reducing that, so an early result I've found is that actually only about 19 images needed to workout the repeatability of your interests region detection.",
            "You start winning very little extra by adding more images and all of these were using about 1200 images per viewpoint bin, so.",
            "By reducing that number and then extracting more than a single histogram from every training image.",
            "By warping their patches rather than the images, I think I can probably get it down to under a minute, certainly.",
            "Hopefully around 10 seconds.",
            "But it's alright on a PC again.",
            "Will there?",
            "The difference really is in the model.",
            "There for the feature.",
            "So instead of the Patch approach, they would.",
            "They sample, I don't really know what I'm looking for, they would.",
            "Sample say 10 pairs of pixels from the image and then that gives you a binary one or zero if a is greater than B.",
            "And then they concatenate those together.",
            "So those ten will give you a 10 bit number.",
            "So so my from nought to two to the 10 not to 184.",
            "Oh right, OK so.",
            "These joint distributions mean the memory is cost is much higher.",
            "The computation, because they are then limited in scaleability.",
            "They forced to use scale space detection so that impacts frame time in terms of robustness.",
            "I haven't got their code to compare with, but.",
            "Yeah, OK, so I want something."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm I'm Simon.",
                    "label": 0
                },
                {
                    "sent": "I'm PhD student at Cambridge, supervised by Tom.",
                    "label": 0
                },
                {
                    "sent": "And my work is funded by Boeing, so thanks to them.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem I'm looking at is rapid localization of known targets.",
                    "label": 0
                },
                {
                    "sent": "In a novel view from a camera image.",
                    "label": 1
                },
                {
                    "sent": "Such as showing up there.",
                    "label": 0
                },
                {
                    "sent": "But we want to do it as quickly as possible because the.",
                    "label": 0
                },
                {
                    "sent": "The application we have in mind here is mobile phone based augmented reality applications.",
                    "label": 0
                },
                {
                    "sent": "So here we can see this 3D object that appears to be floating above the target.",
                    "label": 0
                },
                {
                    "sent": "On the mobile there is localising in every frame the target.",
                    "label": 0
                },
                {
                    "sent": "In about 40 milliseconds, so it's running at 15 frames a second.",
                    "label": 0
                },
                {
                    "sent": "And the content is just pulling out these images so you can see the house that you might go in and slightly more clearly.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we use a local feature method for this.",
                    "label": 1
                },
                {
                    "sent": "Local features of proved successful for many applications and they usually have these two independent stages.",
                    "label": 0
                },
                {
                    "sent": "So the first stage is interest region detection, which involves factoring out as many transformations as you can.",
                    "label": 1
                },
                {
                    "sent": "Typically translation often scale as well with dog detection.",
                    "label": 0
                },
                {
                    "sent": "Sometimes I find.",
                    "label": 0
                },
                {
                    "sent": "To try and reduce the difference, the appearance difference between your features.",
                    "label": 1
                },
                {
                    "sent": "However, after that there are still some differences remaining, either through errors in the parameter assignment from the first stages, we just been hearing about, or because you have chosen not to try and factor it out, such as if you're using a fixed scale detector.",
                    "label": 0
                },
                {
                    "sent": "And so a region matching approach needs to handle all of these differences.",
                    "label": 0
                },
                {
                    "sent": "And the two typical categories of matching approaches that have been studied are descriptors such as sift, mops and glow, which do some image processing.",
                    "label": 0
                },
                {
                    "sent": "Some further image processing, often blurring some soft bend histograms to try and.",
                    "label": 0
                },
                {
                    "sent": "Reduce the effects of those differences and alternative approaches to train a classifier on these.",
                    "label": 0
                },
                {
                    "sent": "Different on these patches of different appearance coming out of your region detector.",
                    "label": 0
                },
                {
                    "sent": "And that's being studied in ferns and randomized trees work.",
                    "label": 0
                },
                {
                    "sent": "The descriptor approach is symmetric in that you do the same thing on both images you're trying to match and your matching score is given by the distance in your descriptor space, whereas the classifier approach you have a separate offline training phase.",
                    "label": 0
                },
                {
                    "sent": "And to learn your model for the feature and then at runtime that means you can often do less work and achieve faster matching.",
                    "label": 0
                },
                {
                    "sent": "So our approach is classification based.",
                    "label": 0
                },
                {
                    "sent": "However, unlike the ferns and randomized trees approaches, we don't use any joint distributions, so our memory cost is much smaller and that was one of the major problems with the previous approaches.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the basic units if you like of our system, is these quantized patches, very simple to extract for an image, just sample every other pixel.",
                    "label": 0
                },
                {
                    "sent": "Calculate the mean and standard deviation and then quantize heavily to 5 levels.",
                    "label": 0
                },
                {
                    "sent": "Now, this model here is obviously not doesn't carry a lot of information about the appearance of the feature, and you wouldn't expect it.",
                    "label": 0
                },
                {
                    "sent": "To be useful for matching, especially in the localization case, where you've got no prior information about the position and the appearance.",
                    "label": 0
                },
                {
                    "sent": "However, This is why we have a classifier to learn a model for the.",
                    "label": 0
                },
                {
                    "sent": "The range of patches corresponding to the same feature.",
                    "label": 0
                },
                {
                    "sent": "So for this we have a training set for a large set of images.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we also officially warp.",
                    "label": 0
                },
                {
                    "sent": "This is an offline stage on a PC.",
                    "label": 0
                },
                {
                    "sent": "A few thousand 10,000 or so images from him from an original reference image.",
                    "label": 0
                },
                {
                    "sent": "And we group them into these viewpoint bins.",
                    "label": 0
                },
                {
                    "sent": "Now here you can see we have we have separate scale bins and that allows us to use.",
                    "label": 0
                },
                {
                    "sent": "Fast fixed, fixed scale color detection.",
                    "label": 0
                },
                {
                    "sent": "Rather than needing to do a scale space search runtime, so we trade off that cost for having more features in the database.",
                    "label": 0
                },
                {
                    "sent": "And we'll learn separate sets of features for each of these viewpoint bins.",
                    "label": 0
                },
                {
                    "sent": "And although here we only show top down center begins, you could extend this kind of idea in order to have more extreme affine viewpoint bins.",
                    "label": 0
                },
                {
                    "sent": "If you want your localization to work over those kind of ranges.",
                    "label": 0
                },
                {
                    "sent": "So now we extract.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something from every one of those images that we turn subfeatures and this is.",
                    "label": 0
                },
                {
                    "sent": "These things are going to be built together to give us our features, but from a single image.",
                    "label": 0
                },
                {
                    "sent": "Will run fast.",
                    "label": 0
                },
                {
                    "sent": "9 interest point detection.",
                    "label": 0
                },
                {
                    "sent": "Which is referenced in the paper and this very simple orientation.",
                    "label": 0
                },
                {
                    "sent": "Measure just based on the sum of some pixel differences.",
                    "label": 0
                },
                {
                    "sent": "So again, it's purely designed for speed.",
                    "label": 0
                },
                {
                    "sent": "These aren't accurate.",
                    "label": 0
                },
                {
                    "sent": "Measures they are likely to vary a lot over our viewpoint, but because we have this training phase, we can learn about how these measures are going to vary and that means it doesn't necessarily cost us a lot to have.",
                    "label": 0
                },
                {
                    "sent": "Cheap metrics like this.",
                    "label": 0
                },
                {
                    "sent": "It doesn't cost a lot in terms of performance.",
                    "label": 0
                },
                {
                    "sent": "So from this single image.",
                    "label": 0
                },
                {
                    "sent": "We've highlighted one feature here.",
                    "label": 0
                },
                {
                    "sent": "And we rotate our sparse 8 by 8 grid to be aligned with that orientation.",
                    "label": 0
                },
                {
                    "sent": "Extracts the pixels from that image.",
                    "label": 0
                },
                {
                    "sent": "Bilinear interpolation gets used and then do the quantization.",
                    "label": 0
                },
                {
                    "sent": "So what are some feature stores here?",
                    "label": 0
                },
                {
                    "sent": "Is the location of the interest point.",
                    "label": 0
                },
                {
                    "sent": "It stores the orientation.",
                    "label": 0
                },
                {
                    "sent": "And it stores this.",
                    "label": 0
                },
                {
                    "sent": "Quantized view open that corner.",
                    "label": 0
                },
                {
                    "sent": "As the appearance information from this particular warped image.",
                    "label": 0
                },
                {
                    "sent": "So we then do that on all of our walks, images in the viewpoint bin.",
                    "label": 0
                },
                {
                    "sent": "And then we want to.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Them and the clustering we can do because we know the warp that generated these images so we can walk back the coordinates from our interest point detection.",
                    "label": 0
                },
                {
                    "sent": "So that's a single image if we add.",
                    "label": 0
                },
                {
                    "sent": "All of the corners from all of the images in our viewpoint bin.",
                    "label": 0
                },
                {
                    "sent": "We get a picture like this over here.",
                    "label": 0
                },
                {
                    "sent": "And you can see, for example, this point on the exclamation mark is very poorly localized in terms of rotation.",
                    "label": 0
                },
                {
                    "sent": "You can see a kind of Halo around the whole thing because there's a rotation measure is essentially random for that feature, but most of the others are all sort of pointing in the same direction, which is all we really want out of this data.",
                    "label": 0
                },
                {
                    "sent": "So now we have all that data we should be able to select.",
                    "label": 0
                },
                {
                    "sent": "What are good features to include in our model for the target.",
                    "label": 0
                },
                {
                    "sent": "So we do that by setting a hard bound on the errors were willing to accept from interest point section and rotation assignment.",
                    "label": 0
                },
                {
                    "sent": "So here we allow two pixels of error.",
                    "label": 0
                },
                {
                    "sent": "In position and plus minus 10 degrees in terms of rotation.",
                    "label": 0
                },
                {
                    "sent": "And then for every single one of those sub features, we consider it as the center of a cluster and count how many other sub features would fall into that cluster with those bounds.",
                    "label": 0
                },
                {
                    "sent": "So that you can see a little blue one with this sort of yellow Halo, you probably can't sit too well and that's the yellow ones are all the sub features that would be included in the cluster if the blue one was chosen as the center, there's another one over there.",
                    "label": 0
                },
                {
                    "sent": "So then we can pick the which sub features are the best centers in a sort of greedy manner to take the one with the most, remove those from the set and then find the next.",
                    "label": 0
                },
                {
                    "sent": "So this I'm just passing over here, this sentence of the clusters that we've chosen to use to represent this target.",
                    "label": 0
                },
                {
                    "sent": "Now you can see how the fact that we're using a cheap orientation assignment scheme is not too important, because here in the case like this point where the orientation isn't very well defined, we've actually assigned three separate features to model that whole space, each of which only needs to deal with that 10 degree difference.",
                    "label": 0
                },
                {
                    "sent": "So this gives us a way of setting this hard bound on the.",
                    "label": 0
                },
                {
                    "sent": "Interesting region detection stage.",
                    "label": 0
                },
                {
                    "sent": "Sort of intelligently selecting the best features to use.",
                    "label": 0
                },
                {
                    "sent": "So OK, each one of these points is going to feature in our model.",
                    "label": 0
                },
                {
                    "sent": "A feature in our yeah in our target database.",
                    "label": 0
                },
                {
                    "sent": "So how do we go about building the features?",
                    "label": 0
                },
                {
                    "sent": "Right well.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The model we use features are these histograms intensity patches?",
                    "label": 1
                },
                {
                    "sent": "So here we can see 4 sub features that go into the most repeatable feature from that target.",
                    "label": 0
                },
                {
                    "sent": "And if we consider just the top left pixel over there and pull it into a histogram from those four, we can see that the leftmost 2 images there are both intensity level 4.",
                    "label": 0
                },
                {
                    "sent": "And the others give you the two other bars there.",
                    "label": 0
                },
                {
                    "sent": "If you include all of the sub features in that cluster into our little histogram.",
                    "label": 0
                },
                {
                    "sent": "We get this distribution for this pixel, so we can kind of see this pixel doesn't really tell us very much about whether or not this.",
                    "label": 0
                },
                {
                    "sent": "An input Patch could be this feature or not.",
                    "label": 0
                },
                {
                    "sent": "However, if an input Patch was white.",
                    "label": 0
                },
                {
                    "sent": "Then the fact that there's actually no none of our training subfeatures wherever whites that gives us quite a strong idea that it's probably not a match.",
                    "label": 0
                },
                {
                    "sent": "So OK, that's the top left pixel.",
                    "label": 0
                },
                {
                    "sent": "What we can do is do the same thing for all the other pixels in our descriptor, so this gives us the complete histogram intensity.",
                    "label": 0
                },
                {
                    "sent": "Patch this hip model and you can see it captures this important.",
                    "label": 0
                },
                {
                    "sent": "Information for the feature that this error here is very consistently white in all of our training samples that there's always this dark diamond shape over in this side.",
                    "label": 0
                },
                {
                    "sent": "So that's all captured in that model.",
                    "label": 0
                },
                {
                    "sent": "But really, we don't need that much detail in there.",
                    "label": 0
                },
                {
                    "sent": "As I said, the important bit so that this is.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, and this is dark.",
                    "label": 0
                },
                {
                    "sent": "So actually we can threshold these histogram values now to just a single bit each.",
                    "label": 1
                },
                {
                    "sent": "So we use a threshold of 5% so.",
                    "label": 0
                },
                {
                    "sent": "If during training.",
                    "label": 0
                },
                {
                    "sent": "The histogram was less than 5%.",
                    "label": 0
                },
                {
                    "sent": "Then we say that's a rare.",
                    "label": 0
                },
                {
                    "sent": "That's a rare intensity level for this sample.",
                    "label": 0
                },
                {
                    "sent": "If it's more than 5%, then it's implausible level.",
                    "label": 0
                },
                {
                    "sent": "So now we just need 1 bits for every one of these lines on this diagram.",
                    "label": 0
                },
                {
                    "sent": "And we choose to use one to represent the rarely seen intensities in the model.",
                    "label": 0
                },
                {
                    "sent": "So that's things where if a runtime Patch.",
                    "label": 0
                },
                {
                    "sent": "If Pixel four in runtime patches anything other than white, then that will count as an error, you know.",
                    "label": 0
                },
                {
                    "sent": "Matching scheme.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's our dissimilarity score.",
                    "label": 1
                },
                {
                    "sent": "It's the account.",
                    "label": 1
                },
                {
                    "sent": "Of the number of pixels in a runtime Patch that fall into these ribbons in a feature model.",
                    "label": 1
                },
                {
                    "sent": "So here's a matching Patch.",
                    "label": 0
                },
                {
                    "sent": "I took it from the set of sub features that generated all this data and we have a binary representation for this as well, so this is the first pixel which is level 4, the second pixel level 4, and the 3rd and 4th or both level 5 'cause they're both white.",
                    "label": 0
                },
                {
                    "sent": "So if we represent it like this, we can count the number of those bits in the rabbits from the feature just by handing that with our hip representation.",
                    "label": 0
                },
                {
                    "sent": "And then bit counting the result.",
                    "label": 0
                },
                {
                    "sent": "And actually, we can arrange the bits such that we only need to do a 64 bits bit count.",
                    "label": 0
                },
                {
                    "sent": "Just to speed things up.",
                    "label": 0
                },
                {
                    "sent": "So that's much impact with the non matching Patch.",
                    "label": 0
                },
                {
                    "sent": "This is just a random other one.",
                    "label": 0
                },
                {
                    "sent": "Over the 1st four pixels are all black hair, so all of those look like that.",
                    "label": 0
                },
                {
                    "sent": "And then if you end it with your feature model you can see we get an error from both the 2nd and the 3rd.",
                    "label": 0
                },
                {
                    "sent": "Pixels.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The overall error is 2 plus it's a lot higher than two, clearly.",
                    "label": 0
                },
                {
                    "sent": "If we want to calculate it for all the pixels and so we can just set a threshold on this number now and it turns out that a low threshold is actually fine, anything greater than 4?",
                    "label": 0
                },
                {
                    "sent": "Is not a match in our system and a thing greater than two is the second match, so we have this these two thresholds.",
                    "label": 0
                },
                {
                    "sent": "So if it's 01 errors from the Patch that counts the primary match, which we then use.",
                    "label": 0
                },
                {
                    "sent": "That information to try and figure out.",
                    "label": 0
                },
                {
                    "sent": "Which are the best viewpoints?",
                    "label": 0
                },
                {
                    "sent": "But we can.",
                    "label": 0
                },
                {
                    "sent": "We can do some more than that rather than just comparing against all of our models in the database, we can actually combine them.",
                    "label": 0
                },
                {
                    "sent": "So by ending together two of.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These binary representations, we get another.",
                    "label": 0
                },
                {
                    "sent": "Another hip if you like that actually will represent the common.",
                    "label": 0
                },
                {
                    "sent": "Rabbins between those two parents.",
                    "label": 0
                },
                {
                    "sent": "So if we then compare against that.",
                    "label": 0
                },
                {
                    "sent": "Parent.",
                    "label": 0
                },
                {
                    "sent": "And we have more than our error threshold of errors.",
                    "label": 0
                },
                {
                    "sent": "Then we don't need to do the exhaustive comparison.",
                    "label": 0
                },
                {
                    "sent": "We don't need to compare against the two children because it sets a hard lower bound just matching to the handed version.",
                    "label": 0
                },
                {
                    "sent": "So let's have an animation that might clarify things a bit.",
                    "label": 0
                },
                {
                    "sent": "So these two.",
                    "label": 0
                },
                {
                    "sent": "Is obviously much.",
                    "label": 0
                },
                {
                    "sent": "There's only nine bit long and 320 bit in rail system, but they share 41 bits in common, so this is 4.",
                    "label": 0
                },
                {
                    "sent": "Rare bins in in the histograms.",
                    "label": 0
                },
                {
                    "sent": "So we pull it out by ending.",
                    "label": 0
                },
                {
                    "sent": "The feature representations together.",
                    "label": 0
                },
                {
                    "sent": "And now we will do a comparison at runtime against this parent thing here.",
                    "label": 0
                },
                {
                    "sent": "So if at runtime we get say 4 hours there in our thresholds too, then we can stop and we know automatically that that Patch will not match to either the red or the green, even though we've only done this one comparison.",
                    "label": 0
                },
                {
                    "sent": "And we can actually continue using this combining the most overlapping features in order to build tree.",
                    "label": 0
                },
                {
                    "sent": "So these are the features that haven't yet been represented in our tree.",
                    "label": 0
                },
                {
                    "sent": "The most overlap now is between the new parent and this Purple One.",
                    "label": 0
                },
                {
                    "sent": "So we pull out the shared 3 features there and we repeat.",
                    "label": 0
                },
                {
                    "sent": "Until we get to the stage where there are features in the root, here have no bits in common.",
                    "label": 0
                },
                {
                    "sent": "So now there's nothing more we can do.",
                    "label": 0
                },
                {
                    "sent": "But at runtime this means.",
                    "label": 0
                },
                {
                    "sent": "In the best case, we only need to make 2 comparisons in order to project a match against all of our features.",
                    "label": 0
                },
                {
                    "sent": "In the database, and then?",
                    "label": 0
                },
                {
                    "sent": "Because because it's a binary tree, it has nice scaling properties as well.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's how we extract all of our matches at runtime.",
                    "label": 0
                },
                {
                    "sent": "Then we can do a half transform because because of our viewpoints we have a scale.",
                    "label": 0
                },
                {
                    "sent": "We know the rough scale of each feature.",
                    "label": 0
                },
                {
                    "sent": "They have their own orientation as well, so we can build that into.",
                    "label": 0
                },
                {
                    "sent": "16 or so orientation bins.",
                    "label": 0
                },
                {
                    "sent": "To extract all the features that agree at least on the on the course scale and rotation of the target.",
                    "label": 0
                },
                {
                    "sent": "No more than that.",
                    "label": 0
                },
                {
                    "sent": "We can see a slightly more finer viewpoint consistency.",
                    "label": 1
                },
                {
                    "sent": "Check from a single target, so here the blue feature represents the lowest scoring match, so the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As much that we think.",
                    "label": 0
                },
                {
                    "sent": "So in the reference frame here in the viewpoint Russian frame, we can workout the vectors to the other expected matches.",
                    "label": 0
                },
                {
                    "sent": "Rotate that, do the similarity transform on that.",
                    "label": 0
                },
                {
                    "sent": "And we have this acceptance region to account for the fact that it might not be a top down view and there is a range of scales in this cabin.",
                    "label": 0
                },
                {
                    "sent": "And then if the match does appear inside that acceptance region then it means it's a good match.",
                    "label": 0
                },
                {
                    "sent": "If it doesn't, we can reject it, so this much here.",
                    "label": 0
                },
                {
                    "sent": "There should be a green line that is OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "This match is consistent with the blue one.",
                    "label": 0
                },
                {
                    "sent": "So once we do that, we can get a set set of features that we can be reasonably sure.",
                    "label": 0
                },
                {
                    "sent": "Are consistent and that means we don't need to do many iterations on Prozac on robust estimation in order to get the pose in the end.",
                    "label": 0
                },
                {
                    "sent": "OK, so some results.",
                    "label": 0
                },
                {
                    "sent": "Wagner in 2009.",
                    "label": 0
                },
                {
                    "sent": "How to a paper with?",
                    "label": 0
                },
                {
                    "sent": "Making adjustments to the Sifton ferns approach is to make them suitable for mobile phone based.",
                    "label": 0
                },
                {
                    "sent": "Augmented reality type applications.",
                    "label": 0
                },
                {
                    "sent": "This is one of the sequences.",
                    "label": 0
                },
                {
                    "sent": "That they were using on the box shows our results on it.",
                    "label": 0
                },
                {
                    "sent": "On their PC they they record reported about 5 milliseconds total frame time.",
                    "label": 1
                },
                {
                    "sent": "On these sequences, and they localized about 96% of the frames.",
                    "label": 0
                },
                {
                    "sent": "You can see the seven sequences have.",
                    "label": 0
                },
                {
                    "sent": "A fairly wide range in terms of density of the texture of a photo summer graphics.",
                    "label": 0
                },
                {
                    "sent": "So our results on here we.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of time, it takes around one millisecond for all of those targets.",
                    "label": 0
                },
                {
                    "sent": "Which is over four times faster, even accounting for the slightly faster computer we were using.",
                    "label": 0
                },
                {
                    "sent": "Our business is increased to 99.6% on average.",
                    "label": 0
                },
                {
                    "sent": "And also memory usage because these binary models are only 40 bytes each, we only require about 100 kilobytes.",
                    "label": 0
                },
                {
                    "sent": "For storing our databases.",
                    "label": 0
                },
                {
                    "sent": "I haven't talked about the indexing here, but that's in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so now because this is so quick I thought can we combine all these targets into the same database and recognize more than one thing at once?",
                    "label": 0
                },
                {
                    "sent": "So this is the video that the results in the paper based on and it shows us looking for all seven of those targets in every frame.",
                    "label": 0
                },
                {
                    "sent": "And then any that are found, we try and draw the box around and there's no tracking happening at all there.",
                    "label": 0
                },
                {
                    "sent": "So it's it's just the raw output from the localization scheme.",
                    "label": 0
                },
                {
                    "sent": "Because of the fixed scale interest point detector, using you'll see in a second that we can't handle extreme blur very well, so it tends to lose it here, but as long as things are reasonably focused, it works reasonably well.",
                    "label": 0
                },
                {
                    "sent": "So what we're interested in here?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is how does the speed scale and that's shown in the graph on the left here?",
                    "label": 0
                },
                {
                    "sent": "So even with this binary tree approach, it seems like we're scaling sort of linearly here, so maybe that's just because we haven't got enough features still to exploit this similarity fully.",
                    "label": 0
                },
                {
                    "sent": "Suddenly something I'm looking at in the future.",
                    "label": 0
                },
                {
                    "sent": "But still, even with all seven targets in the database.",
                    "label": 1
                },
                {
                    "sent": "And using a simple indexing scheme that's.",
                    "label": 0
                },
                {
                    "sent": "In the paper.",
                    "label": 0
                },
                {
                    "sent": "It takes about 7 1/2 milliseconds to localize, so that's.",
                    "label": 0
                },
                {
                    "sent": "Over 100 pounds, and even with all seven of those targets, the other interesting graph is this one here so.",
                    "label": 0
                },
                {
                    "sent": "This is processing the number of frames in which the advert the poster target was localized.",
                    "label": 0
                },
                {
                    "sent": "And each point represents the addition of a new target to the database.",
                    "label": 0
                },
                {
                    "sent": "So the first point is just the poster in the database.",
                    "label": 0
                },
                {
                    "sent": "The second one answered the book, and the third one is the the cars poster, etc.",
                    "label": 0
                },
                {
                    "sent": "And you can see that adding these additional targets does have some impact on the number of frames localized, but it's only fairly minimal.",
                    "label": 0
                },
                {
                    "sent": "It's only a drop of about 10 frames, and that's because we're using this classification approach now, so we're not we're not limited to.",
                    "label": 0
                },
                {
                    "sent": "Finding the nearest neighbour of a match, as in a sort of descriptor scheme, because we have a classifier that actually says yes.",
                    "label": 0
                },
                {
                    "sent": "This is plausibly from that feature and that so we will extract exactly the same set of matches regardless of the number of targets in the database.",
                    "label": 0
                },
                {
                    "sent": "So that begs the question, why did he get a drop at all?",
                    "label": 0
                },
                {
                    "sent": "That's because.",
                    "label": 0
                },
                {
                    "sent": "In our viewpoint consistency stage, we only check the best few viewpoints so that the few viewpoints with the most good matches.",
                    "label": 0
                },
                {
                    "sent": "So then you start getting a bit of noise in that and the correct viewpoint maybe drops out of that list.",
                    "label": 0
                },
                {
                    "sent": "But it seems after a few targets are added then it stays reasonably stable.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so using this classification based matching we can have robust fast localization suitable for mobile phone platforms.",
                    "label": 0
                },
                {
                    "sent": "I'll hit model is computationally efficient and doesn't require much memory.",
                    "label": 0
                },
                {
                    "sent": "Using this training phase in our two paths approach allows us to characterize the interest region detection stages.",
                    "label": 0
                },
                {
                    "sent": "We can limit the impact of using inaccurate region detection and then the tree based look up allows us to return the exact classification results, but without having to exhaustively compare against everything in the database.",
                    "label": 1
                },
                {
                    "sent": "OK, so I'm happy to take questions.",
                    "label": 0
                },
                {
                    "sent": "It's 2.4 Giga Hertz Core 2.",
                    "label": 0
                },
                {
                    "sent": "I was only using single quotes or single threaded.",
                    "label": 0
                },
                {
                    "sent": "If you're running on a mobile phone, you know so the mobile phone.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "It's currently 40 milliseconds per frame.",
                    "label": 0
                },
                {
                    "sent": "For a single target in the database.",
                    "label": 0
                },
                {
                    "sent": "That's a, it's an.",
                    "label": 0
                },
                {
                    "sent": "It's a corsac seats.",
                    "label": 0
                },
                {
                    "sent": "It's a new.",
                    "label": 0
                },
                {
                    "sent": "600 megahertz on a on a friend or megahertz and 95 we get about it.",
                    "label": 0
                },
                {
                    "sent": "Takes about 60 milliseconds, so it doesn't actually double, and that's mainly overhead in the phone in terms of getting images from the camera, converting them to the right color space.",
                    "label": 0
                },
                {
                    "sent": "All that kind of stuff.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "The training.",
                    "label": 0
                },
                {
                    "sent": "At the moment is around half an hour per target.",
                    "label": 0
                },
                {
                    "sent": "The work I'm doing right now is about reducing that, so an early result I've found is that actually only about 19 images needed to workout the repeatability of your interests region detection.",
                    "label": 0
                },
                {
                    "sent": "You start winning very little extra by adding more images and all of these were using about 1200 images per viewpoint bin, so.",
                    "label": 0
                },
                {
                    "sent": "By reducing that number and then extracting more than a single histogram from every training image.",
                    "label": 0
                },
                {
                    "sent": "By warping their patches rather than the images, I think I can probably get it down to under a minute, certainly.",
                    "label": 0
                },
                {
                    "sent": "Hopefully around 10 seconds.",
                    "label": 0
                },
                {
                    "sent": "But it's alright on a PC again.",
                    "label": 0
                },
                {
                    "sent": "Will there?",
                    "label": 0
                },
                {
                    "sent": "The difference really is in the model.",
                    "label": 0
                },
                {
                    "sent": "There for the feature.",
                    "label": 0
                },
                {
                    "sent": "So instead of the Patch approach, they would.",
                    "label": 0
                },
                {
                    "sent": "They sample, I don't really know what I'm looking for, they would.",
                    "label": 0
                },
                {
                    "sent": "Sample say 10 pairs of pixels from the image and then that gives you a binary one or zero if a is greater than B.",
                    "label": 0
                },
                {
                    "sent": "And then they concatenate those together.",
                    "label": 0
                },
                {
                    "sent": "So those ten will give you a 10 bit number.",
                    "label": 0
                },
                {
                    "sent": "So so my from nought to two to the 10 not to 184.",
                    "label": 0
                },
                {
                    "sent": "Oh right, OK so.",
                    "label": 0
                },
                {
                    "sent": "These joint distributions mean the memory is cost is much higher.",
                    "label": 0
                },
                {
                    "sent": "The computation, because they are then limited in scaleability.",
                    "label": 0
                },
                {
                    "sent": "They forced to use scale space detection so that impacts frame time in terms of robustness.",
                    "label": 0
                },
                {
                    "sent": "I haven't got their code to compare with, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so I want something.",
                    "label": 0
                }
            ]
        }
    }
}