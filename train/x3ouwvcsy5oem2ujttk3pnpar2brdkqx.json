{
    "id": "x3ouwvcsy5oem2ujttk3pnpar2brdkqx",
    "title": "Reinforcement Learning",
    "info": {
        "author": [
            "Satinder Singh, Electrical Engineering and Computer Science Department, University of Michigan"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "February 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/mlss06au_singh_rl/",
    "segmentation": [
        [
            "I can.",
            "I can buy the summer school in directions that I'm interested in, so judicially is always been a heavy kernel influence over the basic methods influence, so I'm happy to make it more reinforcement learning influence first with the tender this weekend and Michael.",
            "Next week, but I'll let him do his.",
            "I need direction that will probably thank you.",
            "Before I start, let me just get a show of hands of how many of you are interested in reinforcement learning.",
            "Think about reinforcement learning or just curious about reinforcement learning.",
            "So it keeps substantial fraction good.",
            "OK, so as I said I'm sitting there saying I'm at the University of Michigan.",
            "At this point, the material here, the perspectives here, more the perspectives and the material was a special thanks to many many people, but particularly rich Sutton who's been a great colleague and a mentor for me for many years.",
            "And you'll see his hand if you like indirectly in much of what I'll talk about.",
            "But of course I've had a lot of colleagues over the years.",
            "Here are some of the ones that I've collaborated with or have been influenced by fairly closely.",
            "I've been working in reinforcement learning for about 15 years now.",
            "When I started my pH.",
            "D. At the University of Massachusetts, Amherst with Andy Barter Combo Switch Sutton, and I did our PHD's with there about maybe five people in the world doing reinforcement learning.",
            "When I started and I finished about 90.",
            "Three.",
            "Now of course there are thousands of people, I think.",
            "All around the world.",
            "In this area of research.",
            "And so I started fairly early on in the reinforcement learning days in this area.",
            "So you'll get a perspective that's somewhat different.",
            "On reinforcement learning, then you will get from some of my more recent colleagues in this area, so that's a plus or minus, but just be aware that you're getting a particular perspective particular slice from someone who.",
            "Has been in the field for for a fairly long time, so has a longer view of these things.",
            "The specific slides I'm going to talk about today are on this website as well.",
            "I know that you probably have them in your hand out.",
            "There are slides that I'll use tomorrow.",
            "Hang on, hang on.",
            "OK, so those of you are trying to look for it will not find them, but these are available there on the slides are used.",
            "Tomorrow will also be available on some website and.",
            "Maybe even print it out.",
            "Try it, I apologize for not being my putting back together and giving them to Doug in time, so any blame for this is on my shoulders and not on the organizers so that you know where to who complain about.",
            "So here's an outline for the talk."
        ],
        [
            "I'm going to begin by giving you a history and place a reinforcement learning.",
            "Just sort of the content.",
            "Place it in context of artificial intelligence and machine learning, but also other areas of research that have influenced reinforcement learning.",
            "And then I'm going to fairly quickly dive into the D dominant sort of framework.",
            "The most productive framework, the reinforcement learning is used, which is that a Markov decision processes, and then once I've described that framework, I'm going to talk about planning an MVP's learning and EM DPS and function approximation reinforcement.",
            "And and reinforcement learning, and I'll do all of that today.",
            "I'm not gonna talk very much about partial Markov decision processes, because Michael Littman's going to talk about that when he comes next week, but I'll I'll touch on touch on this stuff and then tomorrow I'll go beyond MVP's in Palm DP's and Palm.",
            "DP is being partially observable.",
            "Markov decision processes and and perhaps today and some tomorrow.",
            "Talk about some applications of reinforcement learning, so that's sort of the plan.",
            "I should say that this is a small enough group that let's try to make this as interactive as possible, so feel free to ask me questions at anytime you want, right?",
            "Just let's start.",
            "Make it as as productive as possible.",
            "OK, so here is the."
        ],
        [
            "Obligatory sort of very high level view of reinforcement learning, so this is a 30,000 feet view of the reinforcement learning problem.",
            "What's the reinforcement learning problem?",
            "You have an agent of some sort interacting with some environment.",
            "The agent has sensors through it.",
            "It perceives the environment.",
            "And then it takes actions on the environment and has some sort of feedback or reward or reinforcement.",
            "Or pay off all of these terms are synonymous.",
            "And the agent's task is to act in the world so as to maximize some measure of reward it gets from the environment, right?",
            "So that at a very high level view is the reinforcement learning problem.",
            "And of course it's very abstract.",
            "And at this level of abstraction, lots and lots of different problems can be formulated this way.",
            "I'll show you examples of problems that can be formulated this way, but.",
            "I tried to emphasize something and I'll repeatedly do this to distinguish it from supervised and unsupervised learning.",
            "Is that in some ways reinforcement learning is sort of the complete AI problem, right?",
            "What's the complete AI problem?",
            "It's really the problem of building agents that can interact successfully in very rich environments, right?",
            "So in that sense, of course this is a.",
            "A large problem, a problem that that encompasses much of AI, encompasses supervised and unsupervised learning, for example.",
            "So certain things that distinguish reinforcement learning, we have a complete agent.",
            "It's temporally situated, that is, it's acting.",
            "Overtime is continually learning and planning in its environment.",
            "And its object is to affect the environment, right?",
            "Subjective is to affect the environment.",
            "And of course we live in a statistical sort of framework.",
            "So the environment is stochastic and uncertain.",
            "Will do things in a probabilistic and sort of stochastic sort of way, and no one can argue.",
            "Of course about this, but in some ways, reinforcement learning is like life, right?",
            "Because what we are?",
            "Just acting or behaving in the world.",
            "So what we're doing, we're sensing, were acting occasionally.",
            "We get some sort of intrinsic or extrinsic reward, and one can argue that.",
            "That the agents task our task is to choose actions so as to optimize Mr Ward overtime.",
            "So that's the reinforcement learning problem at a very high level.",
            "Here's another slice of the reinforce."
        ],
        [
            "Learning problem, you know, here's another way to view the reinforcement learning problem, which is that life for an agent is really a sequence of observations, actions and rewards.",
            "So subscript here denotes time is action pose observation, ours reward.",
            "So here the agent is born, the beginning of time gets an observation, takes an action, gets reward observation, action, reward and life sort of proceeds linearly.",
            "If you like in a long trajectory like this.",
            "And at any given point of time.",
            "The agent has a history.",
            "Of life and I should choose an action.",
            "And his task is to choose actions so as to.",
            "Maximize the amount of reward will get in the future.",
            "Right, that's sort of the reinforcement learning problem.",
            "Life.",
            "Now an interesting thing that will come back over and over 2 is that you can think of a unit of experience.",
            "The agent has a unit of experience in the world, which is it gets an observation, takes an action, gets a reward, and observes the next observation.",
            "And that.",
            "You can think of reinforcement learning as a problem of learning how to act from these units of experience, and this will become.",
            "More meaningful as we go to the reinforcement learning problem, and again they didn't choose actions to maximize some expected cumulative reward over some time horizon and will talk about.",
            "I'll make all this very precise as we.",
            "Define the Markov decision process framework.",
            "I'm just trying to make sure you understand the larger problem that reinforcement learning is bad.",
            "Now observe, of course, that you know this looks very simple, but of course these observations can be vectors order structure, so it's arbitrary rich sensory observations.",
            "The actions can be multidimensional, right?",
            "Simultaneously moving my my my arms, my legs, my my vocal cord, and so on, right?",
            "I'm doing all kinds of motor motor affecting all kinds of motor actuators.",
            "And the one the one.",
            "Aspect the reinforcement learning assumes that is less general.",
            "If you like, then the extreme generality so far is that the we subscribe to what's called the reinforcement learning hypothesis.",
            "What's the reinforcement learning hypothesis?",
            "Is the hypothesis that an agent acts so as to maximize scalar measure of reward?",
            "So we'll assume the reward is some scalar signal.",
            "Whatever it is, and we'll see examples of where the reward might come from, but there is some scalar reward, and the agents task is to maximize some cumulative measure of this scalar reward.",
            "And of course, the reward can be arbitrarily uninformative, and we'll see examples of that.",
            "Furthermore, this is very important because it may not be apparent from what I've been saying so far, is of course the agent has some partial knowledge about its environment.",
            "The agent might have some ability to predict what will happen, as it takes actions in the world, so there is.",
            "No commitment to the amount of knowledge the agent has about its world, so you'll see if you've heard reinforcement learning talks, you'll often hear talks in which no knowledge is assumed to the other extreme, where lots of information about the environment is already known to the agent, so we'll talk about this distinction as we go through as well."
        ],
        [
            "So here's a listing of some of the key ideas in reinforcement learning that I want to present today.",
            "Except for the last one to do tomorrow.",
            "So the first sort of key contribution off the area of modern reinforcement learning is what's called a temporal differencing or temporal differences idea.",
            "And we'll talk in quite a bit of detail about that high level summary of what temporal differences is that it's really the idea of updating a guess on the basis of another guess, which makes it very different from supervised learning where you're updating a guess on the basis of teacher provided information.",
            "So there's this sort of circularity in reinforcement learning.",
            "Then I'll talk about.",
            "That's the idea of temporal difference in another key idea of reinforcement learning.",
            "That reinforcement learning has in it is the idea of eligibility traces.",
            "I'll talk about that.",
            "The idea of policy learning.",
            "So this is an outline function approximation for reinforcement learning, and then hierarchical reinforcement learning.",
            "OK, so now."
        ],
        [
            "Begin by giving you a quick demo, offer reinforcement learning problem."
        ],
        [
            "Here's a reinforcement learning problem.",
            "It's simulated robot soccer.",
            "This is the work of Stone and Sutton.",
            "This is a particular problem is called Keeper Way in which.",
            "4 red players are trying to keep the ball away from the three blue players.",
            "While staying within this square that surrounds them, the white square that surrounds them.",
            "And these robots have sensors.",
            "They can sense objects that are within a field of view.",
            "The field of view is denoted by the white straight line.",
            "On these on these circular robots and they have actions like kicking the ball and and dribbling the ball and so on.",
            "And the task again.",
            "As you can imagine, is to just keep the ball away from the blue players and what you're seeing is actually learned behavior.",
            "I'll show you in a minute.",
            "Random behavior, so here's the random B."
        ],
        [
            "Where this acting randomly, and hopefully you'll see a difference into as to how fast they lose the ball.",
            "This is they lose the ball much faster here than they did in the learned behavior, yes.",
            "The in this specific instance, the red team was learning and the blue team was not, so only the the defensive players.",
            "If you like we're learning.",
            "And I'll talk a little bit more about is 1 interesting aspect of this, of course, is that this is a multi agent problem.",
            "I think Michael Lipman's going to talk about multi agent systems.",
            "I won't spend much time talking multi agent learning.",
            "I'm really going to focus on a single agent learning, but just to give you an example of reinforcement learning problem here just to drive home the point, what might be good reward function.",
            "The reward is basically going, imagine they get one unit of reward for every time step that they're able to keep the ball away and so maximizing reward would be maximizing the amount of time they can keep the ball to themselves.",
            "And the sensors are again ability to detect objects within a certain field of view.",
            "Those are the sensors, and the actuators are things like dribbling and kicking the ball.",
            "OK.",
            "So just to complete this little demo."
        ],
        [
            "They showed that in this four versus 3 keep away task the learn players could keep the ball away for 10.2 seconds while the random player could keep the ball away from 6.3 seconds.",
            "By the way, I'll cover the idea, the algorithms, and their function approximator today, so you'll see the algorithms in the architecture used today.",
            "I just want to place making person anymore concrete by showing you a specific example and the random player.",
            "By contrast, could keep the ball away for 6.3 seconds.",
            "They also had five versus 4 keep away and they had this distinction between the learn and the random ones.",
            "And here is 5 versus 4.",
            "Keep away.",
            "I won't really show it to you."
        ],
        [
            "OK, here's another demo of reinforcement learning problem."
        ],
        [
            "Mongolia against have you think about how this is a reinforcement learning problem?",
            "And in other words, think about what the sensors are, what the actuators are, what the reward function is, because those are three basic elements of reinforcement learning problem, the sensors, actuators and reinforcement signal.",
            "So here's Tetris.",
            "We've all played this.",
            "This is my work by Drew Bagnall and Jeff Schneider, and.",
            "I guess you're not being able to see the block that's falling, but you're seeing the actions you're seeing the block, the where the block lands, and so the sensors in here are sensors that inform the agent about the shape and the height of the blocks around the floor and the shape of the object and the location of the object is falling and the actions are to move the object right or left objects falling or to push it all the way down.",
            "The usual things that you would do when you play Tetris.",
            "And.",
            "The reward function would be, for example, one unit of reward for every time step.",
            "The game is not over.",
            "So maximizing that would then lead to obvious sort of optimization.",
            "OK."
        ],
        [
            "So just to make those problems, make reinforcement learning concrete.",
            "Now let me step back and talk a little bit more about the history in place of reinforcement learning, and I will then dive into the Markov decision process or framework.",
            "So."
        ],
        [
            "In some ways, reinforcement learning has has.",
            "Has had input from an.",
            "Actually outputted ideas from multiple different disciplines.",
            "Anylist some of these here because I'll essentially touch on some of these ideas that come from these different disciplines.",
            "I won't really talk about neuroscience, so let me begin with that.",
            "I won't talk much about that, so let me say a couple of sentences about it and then move on from there.",
            "Particularly over the last five years or so, you know, great deal of excitement in the intersection of neuroscience and reinforcement learning.",
            "In particular, there has been a.",
            "Part of the brain.",
            "The basal ganglia on the dopamine system in particular, that has been shown pretty convincingly now to actually implement an algorithm.",
            "I'll tell you about today.",
            "The temporal differencing algorithm.",
            "So there is a part of the brain that fairly convincingly now shown to be implementing the TD algorithm.",
            "So as you can imagine, this is very exciting for reinforcement learning researchers, but also has been very exciting for neuroscience people because now they have a connection to all this wonderful, I think pieces of work and reinforcement learning that they can map their ideas.",
            "My math and I just want to.",
            "There's a whole bunch of other work in neuroscience that has, I guess, a reinforcement learning and it really won't talk about that at all.",
            "If you're really interested in that, sort of that direction, I can mention a name from where to follow on the name is Peter Diane DAYN.",
            "If you just do a Google search will find him an from his papers.",
            "You can make lots and lots of connections in this space.",
            "OK, a very strong intersection exists between operations research and reinforcement learning, and in fact the Markov decision process formulation.",
            "The palm DP formulation come from operations research, so it's a very strong interplay between operations research and reinforcement learning, and some of the researchers have very good researchers and reinforcement learning come from this community, and there's been some contributions back to this community from reinforcement learning.",
            "There's an obvious connection to artificial intelligence.",
            "Much of the original reinforcement learning actually started with mathematical psychology.",
            "Bush was tell her work and I'll show you a quote in a minute from mathematical psychology that has a nugget of the reinforcement learning idea in it and finally.",
            "Control theory or optimal control intersects with operations research and reinforcement learning, because in some sense to reinforcement learning view of the AI problem is that AI problem is an optimal control problem, so there's a very strong and obvious sort of intersection.",
            "Then you'll see me develop some of this.",
            "So here's the quote that."
        ],
        [
            "I wanted to share with you.",
            "This is from Thorndike's law of effect and really from way back in 1911 captures the essence of the idea of reinforcement learning.",
            "And I'll read it to you.",
            "And of course we should follow along on the screen.",
            "The basic idea of several responses made the same situation, those which are followed or accompanied closely in time.",
            "My satisfaction to the animal will.",
            "Other things being equal, be more firmly connected with that situation so that we're in the same situation.",
            "Reoccurs, the reaction is more likely to reoccur and the opposite.",
            "For the opposite case.",
            "Right, so this idea of.",
            "Temporal coincidence between a situation and effect followed by satisfaction leads to strengthening of that connection.",
            "Has been around for a long time and essentially is the foundational principle of the of reinforcement learning, and I'll, you know, and I will develop this when I talk about temporal differences.",
            "I won't really belabor much more about the history."
        ],
        [
            "But there's a whole long history of.",
            "People building reinforcement learning ideas in in the field of computer science.",
            "An artificial intelligence starting from touring really well.",
            "I do have programming computer learn by trial and error, Minsky and so on, and I won't spend more time on this, OK?",
            "Now."
        ],
        [
            "Now.",
            "I promise I'm going to get to some very formal work very soon.",
            "I want to continue building the connection between reinforcement learning an other areas for a few more slides.",
            "So here what I'm going to try and do is distinguished reinforcement learning from the other areas of machine learning that more people are familiar with, which is supervised and unsupervised learning so supervised learning, learning from examples, learning approaches to regression classification, unsupervised learning, are learning approach, dimensionality reduction, density estimation, and so on.",
            "In this context, which reinforcement learning reinforcement learning is learning approaches to sequential decision making, learning from delayed reward?",
            "So let me explain this distinction.",
            "A key key question that reinforcement learning asks that is not asked by supervisor.",
            "Unsupervised learning is the following.",
            "Imagine you play a game of chess.",
            "And you lose.",
            "You made.",
            "I don't know 50 moves and you've lost.",
            "It's possible that exactly 1 move was not the right move, and everything else is perfect.",
            "And by one move doomed you to lose.",
            "How would you figure that out?",
            "How would an agent figure out which one of its 50 moves, which one or more of his 50 moves was responsible for that failure?",
            "This problem is called a temporal credit assignment problem.",
            "And is the key problem that reinforcement learning solves and is not addressed at all by supervised and unsupervised learning.",
            "It's this problem that distinguishes reinforcement learning from the other areas of machine learning.",
            "This problem of temporary assignment.",
            "How do you figure out what actions are responsible for good outcomes in the long run?",
            "OK, here's the part."
        ],
        [
            "List of applications.",
            "Um, as you can imagine.",
            "Robotics and control theory has lots and lots of applications that can be formulated as reinforcement learning problems.",
            "Lots of games, backgammon, Chester, Taylor, Tetris, have been.",
            "Reinforcement learning is been applied to lots of fascinating work in control, including the very recent and I'll show you some videos often work by Andrew ING on Helicopter Control, which is really been quite dramatically successful at using reinforcement learning to address a problem that optimal control people have worked on for a very long time, and reinforcement came along and in fairly short order, admittedly at the hands of a very good person and ring produced a really.",
            "Really quite successful.",
            "Outstanding performance in a real helicopter.",
            "Lots of operations research problems are formulated.",
            "Reinforcement learning problems, other problems in HCI, which is one of my areas of interest and so on.",
            "So I will see applications as we as we go along."
        ],
        [
            "You need to see this.",
            "OK, I'm ready to now die."
        ],
        [
            "Into the more meaty stuff.",
            "So we're going to begin to think about how one formulates a reinforcement learning problem.",
            "So you have an agent, environment agent, environment interaction.",
            "We're going to make the following assumptions discrete time.",
            "That is, the agent acts once every time interval.",
            "Discrete observations, so the sensors give it one of N from a set of observations.",
            "Discreet action so will stay.",
            "In fact.",
            "Alter this talk, except for tomorrow a little bit tomorrow in the discrete time discrete observations discrete actions framework.",
            "If you like you can ask me questions I'll try to leave 510 minutes at the end for how deal with continuous actions and continuous observations.",
            "I won't really talk about this.",
            "With these assumptions, let's begin groups.",
            "Let's begin to talk about how one defines a model of the reinforcement learning agent environment interaction.",
            "So there are two pieces to the model.",
            "What is what I call transition probabilities?",
            "These define the probability of the observation at time T + 1 given the entire history of interaction.",
            "So far.",
            "So just say what might conditional distribution over the next observation would be given what I've seen so far.",
            "And of course, the same thing for the reward.",
            "So this is the probability density if you like over the next reward.",
            "Given my history of interaction so far.",
            "If I can define these two pieces, then in effect I have defined the agent environment agent environment interaction because agent will take a next action and then recursively we can define the next observation and the next reward and so on.",
            "So these two pieces or what's needed to define the environment agent interaction?",
            "Is that clear?",
            "Everybody, yes, OK.",
            "So now here is the MDP formula formalism."
        ],
        [
            "So the key key idea behind the formalism is that it makes a Markov assumption.",
            "What's the Markov assumption?",
            "The Markov assumption makes the following the observation at time T + 1.",
            "The conditional distribution given the entire history is equal to the conditional distribution of the next observation given just the recent observation and the recent action.",
            "The last observation in action.",
            "This is the Markov assumption, same sort of absorbed assumption for the reward function.",
            "This leads to defining a very simple compact model for.",
            "Agent around interactions which defines these transition probabilities as follows.",
            "Here's some notation.",
            "This is the conditional probability the next state.",
            "By the way, if you make the Markov assumption.",
            "Then you essentially assuming that observations are state.",
            "That is, the recent the latest observation is a sufficient statistic of the entire history of observations.",
            "You say it differently then if you have the last observation, you don't need to keep the rest of the history.",
            "Because the next observation is just determined by stochastically, determined by the last observation in the action.",
            "So once you have the last observation, the entire rest of the history is irrelevant.",
            "This Markov assumption is equivalent to saying that the last observation is just state, so we use a symbol S for state and you say PSS prime is the probability the next status prime given the current state of South and the current action is a similar award RASM.",
            "Prime is, that is the expected immediate reward if the agent transitions from State S 2 S prime, an action A.",
            "So this defines the Markov decision process model."
        ],
        [
            "And now I'm ready to define for you the reinforcement learning problem given the MDP formula.",
            "OK so X is a finite state space is a finite action space.",
            "The transition probabilities as just define R as a reward function.",
            "Another term we need is the notion of a policy policy Pi.",
            "If policy is a mapping from states to actions.",
            "So policy prescribes the behavior of the agent.",
            "It says what action the agent will do in every possible state of the world.",
            "So policy is a mapping from states to actions.",
            "In particular, will for MDP's will be talking about deterministic nonstationary policies.",
            "So the non stationary policies, because they're not a function of time, the deterministic because they're not random, that is there not mapping some states to distributions over actions.",
            "It turns out that we can consider deterministic non stationary policies only because.",
            "In a Markov decision process, I won't prove this, but can be easily proven that the optimal policy belongs to this class.",
            "That is, there always exists an optimal policy that is deterministic and non stationary.",
            "I want really prove this, but I'm happy too.",
            "If you go to any operations research book on dynamic Programming and MPs will see this proof.",
            "OK, now the quantity V super pivi.",
            "This is the return.",
            "For policy Pi, when it started in state I.",
            "So let's define the notion of return.",
            "So V of Pi I is the expected long-term discounted sum of rewards.",
            "If the agent starts in state eye and executes policy Pi, so it's a measure of how good policy Pi is.",
            "If the agent starts in state I.",
            "So we apply the value of state I under policy.",
            "Pi is just how good it is to start in state I and execute policy \u03c0.",
            "Where goodness is measured by the discounted sum of rewards.",
            "So Gamma is a scalar between zero and one.",
            "Let's say it's .9.",
            "And So what this discounted measure is saying is a dollar today.",
            "Sorry, a dollar tomorrow is worth $0.90.",
            "A dollar 2 days from now is worth $0.81.",
            "Or three days from now is worth points, 729 cents, 72.9 cents, right?",
            "So it's discounting future reward by this sort of geometric gamma.",
            "Right, sort of.",
            "The motivation for this is twofold.",
            "The basic motivation for this comes from economics.",
            "The idea is again, a dollar.",
            "Today is worth more than a dollar tomorrow because it's actually put in the bank and an interest on it.",
            "So this discount factor in effect captures that notion of interest.",
            "The other motivations for this that yes.",
            "This microphone.",
            "Actually.",
            "Coronation.",
            "I see so the Markov assumption icing the question so one way to think about the Markov assumption might be that somehow the Markov assumption decouples the long run from the short run.",
            "But that's not true.",
            "Imagine the game of chess, the board, the Chess board is.",
            "A state right in some sense, if I show you the chess board how I arrived, the chess board is irrelevant.",
            "It captures all the information there is about the history of interaction.",
            "Almost right, it doesn't model the agents opponents playing, but forget that for a minute.",
            "Imagine you're playing is a fixed opponent, always plays the same way for just a minute.",
            "So the board is then state.",
            "But still there is.",
            "Fundamental tradeoff that I should have observed before the temporal create assignment problem still remains this tradeoff between short term and long term gain still remains because it could be that I can take actions on the chess board to gain pieces in the short run.",
            "But that could hurt me in the long run.",
            "Similarly, the idea of trading off short term versus long term is still there in MDP in a Markov decision process framework, so it's not the case that the Markov assumption says I can choose actions greedily.",
            "Then I don't have to worry about the impact of my actions on the long term, because when I take an action it affects the state I'll get to.",
            "In chess, it affects the next board.",
            "And the next board.",
            "What action choices I have available will be different.",
            "Will be affected by the board that I landed.",
            "Right, so through this notion of state, I have long term consequences that I would worry about.",
            "And to this value function that I care about him and I'll define the optimization problem.",
            "It is capturing the long term.",
            "So.",
            "Another way of distinguishing reinforcement learning from machine from other machine learning methods is the fact that.",
            "Reinforcement learning.",
            "Yeah, reinforcement learning.",
            "Is interested in the problem of the trading off between short term and long term consequences of actions, right?",
            "So we can choose actions that give us high reward in the short run, but they may lead to states from which is not possible to obtain high reward in the long run.",
            "Contrast that with the case where you might choose actions which give you low toward the longer in the short run, but lead you to states from which very high rewards obtainable in the long run, right?",
            "This is a fundamental aspect of reinforcement learning, fundamental aspects of human life, animal life, and that's captured in this reinforcement learning Markov Decision process framework.",
            "Now, by the way, again, to emphasize, this is called a discounted framework.",
            "There is another framework and reinforcement that I won't talk much about is called the average reward framework and the average reward framework replaces this discounted sum of rewards by the average reward overtime.",
            "And you can choose to optimize that quantity rather than a discounted framework.",
            "OK, and in fact I say that here now is what framework will just take the expected average reward.",
            "The limit of that overtime.",
            "I should say the horizon here is Infinity, right?",
            "We're going all the way to Infinity all the way to the future in this.",
            "In this framework, OK?",
            "Step."
        ],
        [
            "Step of defining for you the optimization problem.",
            "So we have a quantity called the optimal policy which we did not \u03c0 star.",
            "The optimal policy is the policy that maximizes the value, that is, the policy that maximizes the value function.",
            "So it's the policy so that you behave if you behave according to that.",
            "There is no other policy that obtains higher reward.",
            "The optimal value function we star is of course the optimal is the value associated with the optimal policy, or if in effect the optimal value of state.",
            "I restarted.",
            "Why is the most value you can obtain from state and any way of behaving?",
            "Here's the interesting observation.",
            "It turns out in MDP.",
            "To always exist a optimal policy in the sense that a policy that optimizes the value of every state simultaneously.",
            "Or in other words, it is not the case that depending on where you start, which state you start in, the policy changes.",
            "It exists a policy that no matter where you start is the optimal policy.",
            "So that's just a fundamental result from a from an MDP point of view, and that's something we'll exploit as we develop planning and learning algorithms in reinforcement learning.",
            "Is the import of that tier right?",
            "We're going to have optimal policy that assigns an action to every state and it just doesn't depend on which state you're starting on.",
            "It's independent of that.",
            "That property holds for mark operating processes, and we're going to exploit that as we go through.",
            "Oh, let's see.",
            "So.",
            "So just again to confirm just to drive home this point via Pi is a function that assigns or a vector, sometimes with the function, sometimes a vector that assigns a real number every state similarly star.",
            "OK."
        ],
        [
            "There are two problems that one defines when bills reinforcement learning, one is called a policy evaluation problem or the prediction problem.",
            "The policy evaluation problem is a problem given a policy.",
            "Tell me how good that policy is so it's predicting how good is specific policy is.",
            "As the policy evaluation problem, and I'll tell you how to solve it in a minute, and then I'll define what's called the optimal control problem, which is the problem of finding the optimal policy.",
            "And the reason why we split things this way is because we can develop a lot of intuition and the algorithms from the policy evaluation problem with the prediction problem and then move on to the optimal control problem.",
            "So the so the policy evaluation problem is to learn via Pi given a pie.",
            "And the way we do it is, we define what's called a bellman optimality equations.",
            "Or at least the policy evaluation version of these, which looks like this.",
            "Basically you look at his first principles definition of the value for state I under policy \u03c0, and you can use the Markov assumption to derive this recursive system of equations, which says the value of state S under policy Pi is the immediate reward you get from that state if you behave according to policy \u03c0, plus the should be a discount.",
            "There should be gamma here, it discounted expected value of the next state.",
            "Policy pie.",
            "So this is relating the value of the current state.",
            "With the expected value of the states that are one time step away under policy pie.",
            "So for all states S. Immediate value of status is immediate reward plus the discount.",
            "The gamma is missing discount.",
            "Expected value of the next date and the policy \u03c0.",
            "So you set up this system of equations.",
            "One for every state.",
            "Whose solution?",
            "Is the value for policy pie.",
            "Right, this is a system of equations, one for every state, where the unknowns in the system of equations, the value function, what's known, what's defined by the Markov decision process as a reward functions and the transition probabilities.",
            "But either end states that any equations and then unknowns and you can solve this if you like for the value for the value functions of policy \u03c0.",
            "It turns out it's very useful to also define a alternative definition for value function, which is called Q values or state action values, which defines a number not for states, as here, but for state action pair.",
            "So let me define.",
            "This will come in handy when we talk about sort of the most famous contribution of reinforcement learning, which is Q learning.",
            "So here's the definition of the state action value for policy.",
            "Pi is just the expected value of the discounted sum of rewards if you start in state S and the first action you take is A and thereafter you follow policy \u03c0.",
            "So it delays policy Piper one step, and prescribes action A in the starting state and asks how good is that starting state action pair rather than the starting state, which is what we defined here.",
            "And you can similarly define recursive system of equations.",
            "The Q value for state action pair is the immediate reward you get if you take action A in state S now rather than the action pie of S. Plus the discounted gamma missing again expected value for the next state, but you have to be able to policy pie from the next day onwards.",
            "Again, you have a system of equations now, whose solution is these?",
            "Quantities of interest, which is the value of these state action pairs.",
            "But if it isn't clear why these values are interesting, will become clear in a minute.",
            "But let me anticipate that in just a second the value functions are interesting because they summarize.",
            "The long term consequences of this policy right vypyr vest is a summary statistic, a number that says what's the long term utility of being in status.",
            "If you're instead ask what's the long term benefit you will get if you behave according to \u03c0, so it captures a smooshed together.",
            "Right or a future into this one number.",
            "OK. Let me define a similar system equations for the optimal control problem, and then I think we already talk about algorithms and about to get to algorithms.",
            "If you're feeling that I'm giving you too many definitions, let me just go through one more page definitions and will do algorithms OK.",
            "So here's the optimal.",
            "Here's the optimal control problems.",
            "Automated equation so the applicant."
        ],
        [
            "The problem is to define is to learn.",
            "Compute the optimal policy or the optimal value function.",
            "So here is the.",
            "Recursive system of equations that are satisfied that is satisfied that.",
            "Whose solution is the optimal value function?",
            "So for all states S. The best you can do from states from state S?",
            "Is you consider all possible actions in that state?",
            "And for each action, you ask, what's the immediate reward I get for that action.",
            "Plus the discount.",
            "Again, the discount is missing here discounted expected.",
            "Best I could do for the next day.",
            "Right, the best I can do in this state is I consider every possible action in this state for each action I ask, what's the immediate reward I get, but what's the best I can do with expectation from the next states that I'll get to if I take that action, compute that number?",
            "If you like for each action, pick the best that's taking the best of this Max.",
            "That gives you the best you can do in this state.",
            "Again, the Markov decision process formulation prescribe the rewards and the transition probabilities.",
            "So now you have again any equations and unknowns.",
            "If N is a number of states, unknowns being the vystar optimal values.",
            "Again, this is only holds because the Markov assumption is being assumed, right?",
            "This is true under the Markov assumption.",
            "Same by the way, once you have the optimal value function, then you can derive the optimal policy optimal way of behaving from it as follows.",
            "The optimal action Pi star in state S is just the action.",
            "That maximizes the immediate reward plus the discount and expected best value from next day.",
            "So in other words, it's the action that achieves the Max on the right hand side of the Bellman optimality equation.",
            "That becomes the optimal action.",
            "So why, because?",
            "So we are interested in learning the optimal value function, because from it we can derive the optimal way of behaving.",
            "And here is the equivalent definition for state action values.",
            "So Q star avesco may again S, It simply means that you are committed to doing action in status and your uncommitted thereafter.",
            "So the op, the best you can do.",
            "The most value you can get if you do actually changed it S and you can do for free to do whatever you want to do afterwards is the immediate reward for that state action pair, because it committed for that one time step plus the discounted expected best you can do from the state you get to.",
            "Right again, this is a recursive system equations OK. Now, if you're given the Q star values.",
            "Then the optimal action in state S is simply the action that has the largest Q value associated with it.",
            "Because right because you're now you're having a learning of value for every action, so you simply pick the action that has the largest associated value and that's your.",
            "That's your optimal policy.",
            "Now and relationship between the state action values and state values is the best you can do in state S. Not surprisingly, is you consider every action.",
            "And look at the Q value for that action and the best the Max or that is the V star value.",
            "Yes, you're the fullest is deterministic.",
            "Where do you have probability of S prime?",
            "Given S?",
            "What does it mean?",
            "Ha.",
            "This is the probability distribution over next states given that you take action AIDS data.",
            "So this is a distribution over next states.",
            "We're not assuming that state transitions are deterministic policy.",
            "Being deterministic means that I have a fixed prescribed choice as to what action I take in every state.",
            "The state, the next state that happens is random.",
            "And so this is a distribution over next states, not a stochastic policy.",
            "By the way, you could write down these equations with stochastic policies as well, and should have an additional expectation over or actions."
        ],
        [
            "OK, I'm about to get to algorithms.",
            "Here's sort of a graphical view of a Markov decision process.",
            "Proceeding right, you're in some state S. You have a choice of actions.",
            "You pick one action because you can only do one action at a time.",
            "And that the distribution over next states.",
            "For that action, state Choice is a distribution over next states.",
            "In reality, one of these states will happen.",
            "And you repeat your pick.",
            "An action is a distribution over next states.",
            "You'll pick and action distribution next States and that's so this black line is that life trajectory.",
            "We just saw you have an observation would take an action, observation, action, observation.",
            "And you can imagine this another way.",
            "Let me remind you the problem is of course you may get rewarded, some very delayed point of time and the task is to figure out you know is to assign credit or blame to each of the actions you took along the way.",
            "And this is a temporary assignment problem.",
            "OK."
        ],
        [
            "Let's talk about the two key questions in reinforcement learning and then define the algorithms.",
            "So the two problems in that I'm going to talk about is the planning problem and the learning problem.",
            "What's the planning problem?",
            "The planning problem is."
        ],
        [
            "You're given a model of the Markov transition process.",
            "That is, you given the transition probabilities and you're given the reward function.",
            "Planning problem becomes that of then solving for the optimal values or the value function and the optimal policy.",
            "The learning problem, by contrast, will be when you're not given the reward function and you're not given the transition probabilities and you still facing the same problem.",
            "So that's the distinction when reinforcement learning.",
            "People say planning problem, they just mean you're giving him out of the world.",
            "When you, when you're learning, you're not given a problem.",
            "The model of the world.",
            "So let's consider how to solve the policy evaluation problem when you're in the planning setting.",
            "So panning setting means you're given the transition probabilities.",
            "And given the reward function.",
            "And in the policy evaluation setting you're given the policy and what you want to learn is the value function for that policy.",
            "So here is the first algorithm.",
            "This is an algorithm from dynamic programming.",
            "It's called value iteration, and it works like this.",
            "OK, so let's see.",
            "I guess this is the one for a stochastic policy, since I'm not talking about stochastic policies.",
            "Ignore this line.",
            "Let's look at this one.",
            "So here's the idea.",
            "I'm going to start with some arbitrary initial guess of the of the of the pie called V sub zero.",
            "So the subscripts here.",
            "Denote iteration.",
            "And at each iteration I'm going to update my guess.",
            "Of the of the pie as follows, and the care duration I'm going to say my next guess of the value of state, as in the policy Pi is just the immediate reward plus the discounted expected.",
            "Value of the next state under my current guess.",
            "So will update my guess.",
            "On the basis of my current guess.",
            "I'm just gonna iterate this.",
            "So what I've done?",
            "If you think back to the optimality Berman automatic equation is, I'm just recursing that equation.",
            "I'm feeding in my current guess on the right hand side and from the left hand side.",
            "I'm driving my next guess, feeding it back in and cycling that recursion.",
            "Just.",
            "Cracking that recursion.",
            "Starting with some arbitrary initial guess and one can easily show that this.",
            "This this algorithm converges to the value of that policy Pi and I'll do 1 proof in a few in a few minutes.",
            "Then I get through Q learning.",
            "Then I'll do a proof that illustrates the idea behind these convergence, which I won't really talk much about.",
            "And as you can imagine.",
            "Figure out when to stop and a good stopping criterion is when the change in value across two iterations is less than some threshold.",
            "In fact, you can prove that if this change in threshold is less than epsilon, then V sub K plus one is only so far from the pie, which is a target value that you don't know.",
            "OK, this algorithm clear I'm given the model.",
            "I'm giving a policy of mass to evaluate it I just cycle through this iteration an out in the end.",
            "When it finishes you have a.",
            "You have a good estimate of the pie.",
            "How good it is?",
            "How?",
            "How good is policy pie?",
            "So this is the policy evaluation evaluation algorithm.",
            "OK, now let's."
        ],
        [
            "Do.",
            "Oh, and you can do the same thing.",
            "You can do the same thing with the state action values one.",
            "I won't spend time on this.",
            "This is exactly the same iteration, but only done for the state action pair rather than the than the.",
            "State values, so once you say.",
            "Conversion value is unique or not?",
            "Yes.",
            "Why is it unique?",
            "Remind you the question the question is, is the pie unique?",
            "He said give me a pie.",
            "Is it unique and it is unique because you fix the policy.",
            "There's some reward distribution and went from first principles definition, right?",
            "The value of a status and the policy Pi is the expected discounted sum of rewards you would get and everything is well formulated so you will get a unique value function.",
            "So the pies are unique quality.",
            "What's not unique is the optimal policy, and I'll get when I get to that, then I'll I'll talk about that.",
            "OK."
        ],
        [
            "So now let's do optimal control.",
            "Optimal control is planning.",
            "An optimal control is.",
            "You're given a model that is, you're given the transition probabilities and you're given the reward function and your task is to find the optimal policy.",
            "How does that work exactly?",
            "The same idea I'm going to do exactly the same thing.",
            "It's called value iteration from dynamic programming.",
            "I'm just going to recurse the Bellman optimality equations.",
            "That is, I'm going to feed my initial guess on the right hand side on the left hand side, my next guess, another cycle that.",
            "So I'm sending my new guests to the Max or all actions of the immediate reward, but the discounted expected current value of the next day.",
            "And again I will talk about why this converges in a few when I talk about Q learnings, convergence.",
            "And the same thing here for a state action values, and in particular the optimal control question state action values will become very meaningful in a minute.",
            "So now you're updating every state action pair and the next guess is the immediate reward plus the discounted expected best value of the next state.",
            "Here occurs and you can you repeat this at every iteration and it converges to the Q star.",
            "It converges the limit to the optimal optimal value functions and what are the stopping criteria?",
            "Again, stopping criteria is same thing.",
            "The difference between two iterations, the Max norm, the maximum difference, absolute difference between.",
            "Any state action pairs values over 2 iterations is less than epsilon some bound.",
            "Right, so this is a solution to the optimal control problem in the planning case.",
            "And then I'll talk about convergence now."
        ],
        [
            "Anne.",
            "So let's see."
        ],
        [
            "Let me prove to you.",
            "And just because I want to one of these proofs because they are the foundation.",
            "If you like of the proofs, reinforcement learning has had has built over time.",
            "For reinforcement illustrate the idea that it's very simple proof of the proof of convergence for value iteration using the state action values as the as the instance of it.",
            "So what I'm going to prove to you now is that.",
            "Is what's called the contraction property of dynamic programming.",
            "So what is let me define what that means?",
            "So let Delta K. Be the error at the cat iteration.",
            "That is, it is the maximum error.",
            "Off the Q sub K from Q star.",
            "So I'm looking at Q star.",
            "I'm looking at which I don't know.",
            "Which I want to get to, but conceptually I'm saying let Delta X of K be the error at time step K defined in this fashion.",
            "That is I'm looking over Allstate action pairs and asking what's the absolute difference between cusip, Karen, Q, star or I'm going to prove to you is that Q sub K plus one is closer to Q stars and queues FK was by a factor of gamma by multiplicative factor of gamma.",
            "Let's do that proof.",
            "The cusip cables, one of us, a.",
            "Is the immediate reward plus the discounted expected best value of next state.",
            "So let's look at Q's of K. Let's look at the cat guess.",
            "I know the cat guess by this definition.",
            "Is.",
            "No more than Q star.",
            "The optimal value for that state action pair plus Delta K. Right, that's what this definition means.",
            "I've defined Delta K so that this inequality holds.",
            "So I'm replacing queues of K by something that I know is larger.",
            "Just Q star plus Delta K. And now the simple trick is the observation that Delta K is independent of a is independent of S. So I can just pull it out.",
            "When I put it out.",
            "I'm multiplied by gamma.",
            "So I get Gamma, Delta, K and I'm left with the thing in square brackets.",
            "And if you look at this.",
            "This is exactly by the mathematical equation.",
            "The optimal Q value of state, action pairs, thereby proving that.",
            "After the care iteration.",
            "The guest value of state action pairs come A is within Gamma Delta K. So whatever Delta K was.",
            "It has shrunk by multiplicative factor of gamma, so it's .9.",
            "If gamma is .9, then the error has shrunk by point .9, so let me illustrate that pictorially.",
            "Suppose Star is the unknown optimal value.",
            "Let's say my my initial guess is here.",
            "So and it's 2 dimensional state space.",
            "I'm doing it with Victoria Lee's imaginative dimensional state space, so there's a box that contains my initial guess.",
            "The smallest box that contains my initial guess.",
            "So what I've just shown to you is that after one iteration, the size of the box that contains my next guess will have shrunk by a multiplicative factor of gamma.",
            "So in every iteration.",
            "The box size the box shrinks.",
            "I don't know precisely where inside the box it is.",
            "So in particular, the error could grow from one iteration to the next, but I know that the bounding box shrinks at every iteration and this property of the shrinking error is called a contraction property and is the foundation if you like of the proof behind many of the reinforcement learning algorithms.",
            "And I illustrate this when I talk about the learning instance of optimal control Q learning algorithm.",
            "You should ask me questions if you have any of this is unclear because you'll get hopelessly unclear afterwards otherwise.",
            "So you know, feel free to ask me a.",
            "Maybe this is maybe this is very simple, maybe this is all very clear.",
            "We can just charge ahead.",
            "Any questions?",
            "No OK. Alright."
        ],
        [
            "Start with learning problem.",
            "So the learning problem is the same problems in terms of the goals, except we're not given a model right?",
            "So but I know this is tutorial materials.",
            "If you've taken a class in reinforcement learning, this will be fairly tutorial.",
            "I am going to get to more recent and advanced material eventually, but I don't know your background, so I'm assuming that this very tutorial material is indeed necessary.",
            "Maybe I'm maybe I'm wrong about it.",
            "OK, so.",
            "So learning MDP which is the problem that reinforcement learning is most interested in, and that's the setting where we don't have a model.",
            "So what do we have instead?",
            "Instead, we have the real system.",
            "That is a system in which we can take actions.",
            "Observe observations, get rewards and keep interacting with it in that fashion.",
            "Like that is will get to see the black line.",
            "We don't get to see the red things.",
            "The green things we get to see the black line.",
            "OK, so the most we can do is generate experience which looks like this.",
            "So the difference between the original trajectory you've seen in this trajectory is I'm assuming the Markov property, which means we're seeing the States.",
            "Rather than just some some observations.",
            "There are two classes of approaches to solving the reinforcement learning problem.",
            "The learning problem.",
            "One of them is called indirect methods and the other one is called direct methods, but we can learn how to make it interactive.",
            "Can anybody think of a way of learning?",
            "I told you a way of planning.",
            "Is it an obvious way of doing the learning, solving the learning problem and we think of obviously have been learning solving the learning problem?",
            "I've defined the learning problem right.",
            "We're making a Markov assumption.",
            "I've told you what do we have?",
            "We have the ability to take actions and observe States and subserve rewards.",
            "We notice all the planning problem, that is if you give me the transition probabilities of rewards, I know what to do.",
            "So how would you solve the learning problem?",
            "What's an obvious way of solving the learning problem?",
            "I'm trying to figure out the problem.",
            "Perfect, right?",
            "And that would be an instance of what's called the indirect method.",
            "So the indirect method for code enforcement learning problem is exactly what you said.",
            "You wonder about the world.",
            "You take action, then states you observe next states you observe rewards.",
            "Now you're faced with supervised learning problem, supervised learning problem of building a model.",
            "Building a model of the transition probabilities in the reward function.",
            "Once you've learned the transition property, the word function, then you can use planning algorithms with the learn model.",
            "And solve for the optimal policy.",
            "That's in a very nutshell, the indirect method for solving the learning problem.",
            "The direct methods.",
            "Attempt to do the same thing, except they never learn about.",
            "They never learn the transition probabilities and the level under award function, so let me make that make that distinction clear.",
            "OK, so let me give you the most."
        ],
        [
            "The most obvious way.",
            "The most basic simple way of solving the learning problem, which is exactly we just talked about.",
            "I use experience data estimate the model.",
            "How do I estimate the model?",
            "Let me give you a simple instance in the discrete setting, my estimated probability of transitioning to stage A1, taking action a state I is the number of times in that trajectory that I see a transition of data state jail actually divided by the number of times.",
            "This is wrong by the number of times I take action a state I.",
            "Go to whichever state.",
            "Like so did not make restaurant in the title.",
            "But what are you doing?",
            "Literally?",
            "Accounting right?",
            "This is the maximum likelihood estimate of the transition probabilities.",
            "I'm just counting how often I see my transition from state to state.",
            "Check an action, a divide that by the number of times I've taken action against data that will transition probability.",
            "Right, and it's too late to work function and then we compute the optimal policy with respect to the estimated model by this quantity is called the certainty equivalent policy.",
            "The one question you should be asking yourself at this point is.",
            "How do I choose actions?",
            "And this brings up what's called the Exploration Exploitation, dilemma, exploration, exploitation, policy enforcement, learning, and this is a key fundamental problem again.",
            "A problem that distinguishes reinforcement learning from other forms of machine learning.",
            "Other forms of machine learning don't face this question and we define you.",
            "Let me define the question for you.",
            "It's called the Expiration exploitation dilemma.",
            "Here's the problem.",
            "Suppose I'm facing the learning problem.",
            "So in agent in the world.",
            "And I choose my next action.",
            "I have a choice to make right.",
            "Either I can choose to do the action.",
            "That looks the best.",
            "According to the current model of world that is, I have so much experience.",
            "I have a certain amount of experience I can drive in order from it.",
            "I can look at the certainty equivalent policy.",
            "The policy that's greedy.",
            "With respect to my current model and you should take that action.",
            "That would be exploitation.",
            "While I can say yes.",
            "I stopped because Katie change."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I can.",
                    "label": 0
                },
                {
                    "sent": "I can buy the summer school in directions that I'm interested in, so judicially is always been a heavy kernel influence over the basic methods influence, so I'm happy to make it more reinforcement learning influence first with the tender this weekend and Michael.",
                    "label": 0
                },
                {
                    "sent": "Next week, but I'll let him do his.",
                    "label": 0
                },
                {
                    "sent": "I need direction that will probably thank you.",
                    "label": 0
                },
                {
                    "sent": "Before I start, let me just get a show of hands of how many of you are interested in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Think about reinforcement learning or just curious about reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So it keeps substantial fraction good.",
                    "label": 0
                },
                {
                    "sent": "OK, so as I said I'm sitting there saying I'm at the University of Michigan.",
                    "label": 1
                },
                {
                    "sent": "At this point, the material here, the perspectives here, more the perspectives and the material was a special thanks to many many people, but particularly rich Sutton who's been a great colleague and a mentor for me for many years.",
                    "label": 0
                },
                {
                    "sent": "And you'll see his hand if you like indirectly in much of what I'll talk about.",
                    "label": 0
                },
                {
                    "sent": "But of course I've had a lot of colleagues over the years.",
                    "label": 0
                },
                {
                    "sent": "Here are some of the ones that I've collaborated with or have been influenced by fairly closely.",
                    "label": 0
                },
                {
                    "sent": "I've been working in reinforcement learning for about 15 years now.",
                    "label": 0
                },
                {
                    "sent": "When I started my pH.",
                    "label": 0
                },
                {
                    "sent": "D. At the University of Massachusetts, Amherst with Andy Barter Combo Switch Sutton, and I did our PHD's with there about maybe five people in the world doing reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "When I started and I finished about 90.",
                    "label": 0
                },
                {
                    "sent": "Three.",
                    "label": 0
                },
                {
                    "sent": "Now of course there are thousands of people, I think.",
                    "label": 0
                },
                {
                    "sent": "All around the world.",
                    "label": 0
                },
                {
                    "sent": "In this area of research.",
                    "label": 0
                },
                {
                    "sent": "And so I started fairly early on in the reinforcement learning days in this area.",
                    "label": 0
                },
                {
                    "sent": "So you'll get a perspective that's somewhat different.",
                    "label": 0
                },
                {
                    "sent": "On reinforcement learning, then you will get from some of my more recent colleagues in this area, so that's a plus or minus, but just be aware that you're getting a particular perspective particular slice from someone who.",
                    "label": 0
                },
                {
                    "sent": "Has been in the field for for a fairly long time, so has a longer view of these things.",
                    "label": 0
                },
                {
                    "sent": "The specific slides I'm going to talk about today are on this website as well.",
                    "label": 0
                },
                {
                    "sent": "I know that you probably have them in your hand out.",
                    "label": 0
                },
                {
                    "sent": "There are slides that I'll use tomorrow.",
                    "label": 0
                },
                {
                    "sent": "Hang on, hang on.",
                    "label": 0
                },
                {
                    "sent": "OK, so those of you are trying to look for it will not find them, but these are available there on the slides are used.",
                    "label": 0
                },
                {
                    "sent": "Tomorrow will also be available on some website and.",
                    "label": 0
                },
                {
                    "sent": "Maybe even print it out.",
                    "label": 0
                },
                {
                    "sent": "Try it, I apologize for not being my putting back together and giving them to Doug in time, so any blame for this is on my shoulders and not on the organizers so that you know where to who complain about.",
                    "label": 0
                },
                {
                    "sent": "So here's an outline for the talk.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to begin by giving you a history and place a reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "Just sort of the content.",
                    "label": 0
                },
                {
                    "sent": "Place it in context of artificial intelligence and machine learning, but also other areas of research that have influenced reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to fairly quickly dive into the D dominant sort of framework.",
                    "label": 0
                },
                {
                    "sent": "The most productive framework, the reinforcement learning is used, which is that a Markov decision processes, and then once I've described that framework, I'm going to talk about planning an MVP's learning and EM DPS and function approximation reinforcement.",
                    "label": 0
                },
                {
                    "sent": "And and reinforcement learning, and I'll do all of that today.",
                    "label": 0
                },
                {
                    "sent": "I'm not gonna talk very much about partial Markov decision processes, because Michael Littman's going to talk about that when he comes next week, but I'll I'll touch on touch on this stuff and then tomorrow I'll go beyond MVP's in Palm DP's and Palm.",
                    "label": 1
                },
                {
                    "sent": "DP is being partially observable.",
                    "label": 0
                },
                {
                    "sent": "Markov decision processes and and perhaps today and some tomorrow.",
                    "label": 1
                },
                {
                    "sent": "Talk about some applications of reinforcement learning, so that's sort of the plan.",
                    "label": 0
                },
                {
                    "sent": "I should say that this is a small enough group that let's try to make this as interactive as possible, so feel free to ask me questions at anytime you want, right?",
                    "label": 0
                },
                {
                    "sent": "Just let's start.",
                    "label": 0
                },
                {
                    "sent": "Make it as as productive as possible.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Obligatory sort of very high level view of reinforcement learning, so this is a 30,000 feet view of the reinforcement learning problem.",
                    "label": 0
                },
                {
                    "sent": "What's the reinforcement learning problem?",
                    "label": 0
                },
                {
                    "sent": "You have an agent of some sort interacting with some environment.",
                    "label": 0
                },
                {
                    "sent": "The agent has sensors through it.",
                    "label": 0
                },
                {
                    "sent": "It perceives the environment.",
                    "label": 0
                },
                {
                    "sent": "And then it takes actions on the environment and has some sort of feedback or reward or reinforcement.",
                    "label": 0
                },
                {
                    "sent": "Or pay off all of these terms are synonymous.",
                    "label": 0
                },
                {
                    "sent": "And the agent's task is to act in the world so as to maximize some measure of reward it gets from the environment, right?",
                    "label": 0
                },
                {
                    "sent": "So that at a very high level view is the reinforcement learning problem.",
                    "label": 0
                },
                {
                    "sent": "And of course it's very abstract.",
                    "label": 0
                },
                {
                    "sent": "And at this level of abstraction, lots and lots of different problems can be formulated this way.",
                    "label": 0
                },
                {
                    "sent": "I'll show you examples of problems that can be formulated this way, but.",
                    "label": 0
                },
                {
                    "sent": "I tried to emphasize something and I'll repeatedly do this to distinguish it from supervised and unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "Is that in some ways reinforcement learning is sort of the complete AI problem, right?",
                    "label": 0
                },
                {
                    "sent": "What's the complete AI problem?",
                    "label": 0
                },
                {
                    "sent": "It's really the problem of building agents that can interact successfully in very rich environments, right?",
                    "label": 0
                },
                {
                    "sent": "So in that sense, of course this is a.",
                    "label": 0
                },
                {
                    "sent": "A large problem, a problem that that encompasses much of AI, encompasses supervised and unsupervised learning, for example.",
                    "label": 0
                },
                {
                    "sent": "So certain things that distinguish reinforcement learning, we have a complete agent.",
                    "label": 0
                },
                {
                    "sent": "It's temporally situated, that is, it's acting.",
                    "label": 0
                },
                {
                    "sent": "Overtime is continually learning and planning in its environment.",
                    "label": 1
                },
                {
                    "sent": "And its object is to affect the environment, right?",
                    "label": 1
                },
                {
                    "sent": "Subjective is to affect the environment.",
                    "label": 0
                },
                {
                    "sent": "And of course we live in a statistical sort of framework.",
                    "label": 0
                },
                {
                    "sent": "So the environment is stochastic and uncertain.",
                    "label": 1
                },
                {
                    "sent": "Will do things in a probabilistic and sort of stochastic sort of way, and no one can argue.",
                    "label": 0
                },
                {
                    "sent": "Of course about this, but in some ways, reinforcement learning is like life, right?",
                    "label": 0
                },
                {
                    "sent": "Because what we are?",
                    "label": 0
                },
                {
                    "sent": "Just acting or behaving in the world.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing, we're sensing, were acting occasionally.",
                    "label": 0
                },
                {
                    "sent": "We get some sort of intrinsic or extrinsic reward, and one can argue that.",
                    "label": 0
                },
                {
                    "sent": "That the agents task our task is to choose actions so as to optimize Mr Ward overtime.",
                    "label": 0
                },
                {
                    "sent": "So that's the reinforcement learning problem at a very high level.",
                    "label": 0
                },
                {
                    "sent": "Here's another slice of the reinforce.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning problem, you know, here's another way to view the reinforcement learning problem, which is that life for an agent is really a sequence of observations, actions and rewards.",
                    "label": 0
                },
                {
                    "sent": "So subscript here denotes time is action pose observation, ours reward.",
                    "label": 0
                },
                {
                    "sent": "So here the agent is born, the beginning of time gets an observation, takes an action, gets reward observation, action, reward and life sort of proceeds linearly.",
                    "label": 0
                },
                {
                    "sent": "If you like in a long trajectory like this.",
                    "label": 0
                },
                {
                    "sent": "And at any given point of time.",
                    "label": 0
                },
                {
                    "sent": "The agent has a history.",
                    "label": 0
                },
                {
                    "sent": "Of life and I should choose an action.",
                    "label": 0
                },
                {
                    "sent": "And his task is to choose actions so as to.",
                    "label": 1
                },
                {
                    "sent": "Maximize the amount of reward will get in the future.",
                    "label": 0
                },
                {
                    "sent": "Right, that's sort of the reinforcement learning problem.",
                    "label": 0
                },
                {
                    "sent": "Life.",
                    "label": 0
                },
                {
                    "sent": "Now an interesting thing that will come back over and over 2 is that you can think of a unit of experience.",
                    "label": 0
                },
                {
                    "sent": "The agent has a unit of experience in the world, which is it gets an observation, takes an action, gets a reward, and observes the next observation.",
                    "label": 0
                },
                {
                    "sent": "And that.",
                    "label": 0
                },
                {
                    "sent": "You can think of reinforcement learning as a problem of learning how to act from these units of experience, and this will become.",
                    "label": 0
                },
                {
                    "sent": "More meaningful as we go to the reinforcement learning problem, and again they didn't choose actions to maximize some expected cumulative reward over some time horizon and will talk about.",
                    "label": 1
                },
                {
                    "sent": "I'll make all this very precise as we.",
                    "label": 0
                },
                {
                    "sent": "Define the Markov decision process framework.",
                    "label": 0
                },
                {
                    "sent": "I'm just trying to make sure you understand the larger problem that reinforcement learning is bad.",
                    "label": 0
                },
                {
                    "sent": "Now observe, of course, that you know this looks very simple, but of course these observations can be vectors order structure, so it's arbitrary rich sensory observations.",
                    "label": 1
                },
                {
                    "sent": "The actions can be multidimensional, right?",
                    "label": 0
                },
                {
                    "sent": "Simultaneously moving my my my arms, my legs, my my vocal cord, and so on, right?",
                    "label": 0
                },
                {
                    "sent": "I'm doing all kinds of motor motor affecting all kinds of motor actuators.",
                    "label": 0
                },
                {
                    "sent": "And the one the one.",
                    "label": 0
                },
                {
                    "sent": "Aspect the reinforcement learning assumes that is less general.",
                    "label": 0
                },
                {
                    "sent": "If you like, then the extreme generality so far is that the we subscribe to what's called the reinforcement learning hypothesis.",
                    "label": 0
                },
                {
                    "sent": "What's the reinforcement learning hypothesis?",
                    "label": 0
                },
                {
                    "sent": "Is the hypothesis that an agent acts so as to maximize scalar measure of reward?",
                    "label": 1
                },
                {
                    "sent": "So we'll assume the reward is some scalar signal.",
                    "label": 1
                },
                {
                    "sent": "Whatever it is, and we'll see examples of where the reward might come from, but there is some scalar reward, and the agents task is to maximize some cumulative measure of this scalar reward.",
                    "label": 0
                },
                {
                    "sent": "And of course, the reward can be arbitrarily uninformative, and we'll see examples of that.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, this is very important because it may not be apparent from what I've been saying so far, is of course the agent has some partial knowledge about its environment.",
                    "label": 0
                },
                {
                    "sent": "The agent might have some ability to predict what will happen, as it takes actions in the world, so there is.",
                    "label": 0
                },
                {
                    "sent": "No commitment to the amount of knowledge the agent has about its world, so you'll see if you've heard reinforcement learning talks, you'll often hear talks in which no knowledge is assumed to the other extreme, where lots of information about the environment is already known to the agent, so we'll talk about this distinction as we go through as well.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a listing of some of the key ideas in reinforcement learning that I want to present today.",
                    "label": 0
                },
                {
                    "sent": "Except for the last one to do tomorrow.",
                    "label": 0
                },
                {
                    "sent": "So the first sort of key contribution off the area of modern reinforcement learning is what's called a temporal differencing or temporal differences idea.",
                    "label": 0
                },
                {
                    "sent": "And we'll talk in quite a bit of detail about that high level summary of what temporal differences is that it's really the idea of updating a guess on the basis of another guess, which makes it very different from supervised learning where you're updating a guess on the basis of teacher provided information.",
                    "label": 1
                },
                {
                    "sent": "So there's this sort of circularity in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Then I'll talk about.",
                    "label": 0
                },
                {
                    "sent": "That's the idea of temporal difference in another key idea of reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "That reinforcement learning has in it is the idea of eligibility traces.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about that.",
                    "label": 0
                },
                {
                    "sent": "The idea of policy learning.",
                    "label": 1
                },
                {
                    "sent": "So this is an outline function approximation for reinforcement learning, and then hierarchical reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Begin by giving you a quick demo, offer reinforcement learning problem.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's a reinforcement learning problem.",
                    "label": 0
                },
                {
                    "sent": "It's simulated robot soccer.",
                    "label": 0
                },
                {
                    "sent": "This is the work of Stone and Sutton.",
                    "label": 0
                },
                {
                    "sent": "This is a particular problem is called Keeper Way in which.",
                    "label": 0
                },
                {
                    "sent": "4 red players are trying to keep the ball away from the three blue players.",
                    "label": 0
                },
                {
                    "sent": "While staying within this square that surrounds them, the white square that surrounds them.",
                    "label": 0
                },
                {
                    "sent": "And these robots have sensors.",
                    "label": 0
                },
                {
                    "sent": "They can sense objects that are within a field of view.",
                    "label": 0
                },
                {
                    "sent": "The field of view is denoted by the white straight line.",
                    "label": 0
                },
                {
                    "sent": "On these on these circular robots and they have actions like kicking the ball and and dribbling the ball and so on.",
                    "label": 0
                },
                {
                    "sent": "And the task again.",
                    "label": 0
                },
                {
                    "sent": "As you can imagine, is to just keep the ball away from the blue players and what you're seeing is actually learned behavior.",
                    "label": 0
                },
                {
                    "sent": "I'll show you in a minute.",
                    "label": 0
                },
                {
                    "sent": "Random behavior, so here's the random B.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where this acting randomly, and hopefully you'll see a difference into as to how fast they lose the ball.",
                    "label": 0
                },
                {
                    "sent": "This is they lose the ball much faster here than they did in the learned behavior, yes.",
                    "label": 0
                },
                {
                    "sent": "The in this specific instance, the red team was learning and the blue team was not, so only the the defensive players.",
                    "label": 0
                },
                {
                    "sent": "If you like we're learning.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk a little bit more about is 1 interesting aspect of this, of course, is that this is a multi agent problem.",
                    "label": 0
                },
                {
                    "sent": "I think Michael Lipman's going to talk about multi agent systems.",
                    "label": 0
                },
                {
                    "sent": "I won't spend much time talking multi agent learning.",
                    "label": 0
                },
                {
                    "sent": "I'm really going to focus on a single agent learning, but just to give you an example of reinforcement learning problem here just to drive home the point, what might be good reward function.",
                    "label": 0
                },
                {
                    "sent": "The reward is basically going, imagine they get one unit of reward for every time step that they're able to keep the ball away and so maximizing reward would be maximizing the amount of time they can keep the ball to themselves.",
                    "label": 0
                },
                {
                    "sent": "And the sensors are again ability to detect objects within a certain field of view.",
                    "label": 0
                },
                {
                    "sent": "Those are the sensors, and the actuators are things like dribbling and kicking the ball.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So just to complete this little demo.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They showed that in this four versus 3 keep away task the learn players could keep the ball away for 10.2 seconds while the random player could keep the ball away from 6.3 seconds.",
                    "label": 0
                },
                {
                    "sent": "By the way, I'll cover the idea, the algorithms, and their function approximator today, so you'll see the algorithms in the architecture used today.",
                    "label": 0
                },
                {
                    "sent": "I just want to place making person anymore concrete by showing you a specific example and the random player.",
                    "label": 0
                },
                {
                    "sent": "By contrast, could keep the ball away for 6.3 seconds.",
                    "label": 0
                },
                {
                    "sent": "They also had five versus 4 keep away and they had this distinction between the learn and the random ones.",
                    "label": 0
                },
                {
                    "sent": "And here is 5 versus 4.",
                    "label": 0
                },
                {
                    "sent": "Keep away.",
                    "label": 0
                },
                {
                    "sent": "I won't really show it to you.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here's another demo of reinforcement learning problem.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mongolia against have you think about how this is a reinforcement learning problem?",
                    "label": 0
                },
                {
                    "sent": "And in other words, think about what the sensors are, what the actuators are, what the reward function is, because those are three basic elements of reinforcement learning problem, the sensors, actuators and reinforcement signal.",
                    "label": 0
                },
                {
                    "sent": "So here's Tetris.",
                    "label": 0
                },
                {
                    "sent": "We've all played this.",
                    "label": 0
                },
                {
                    "sent": "This is my work by Drew Bagnall and Jeff Schneider, and.",
                    "label": 0
                },
                {
                    "sent": "I guess you're not being able to see the block that's falling, but you're seeing the actions you're seeing the block, the where the block lands, and so the sensors in here are sensors that inform the agent about the shape and the height of the blocks around the floor and the shape of the object and the location of the object is falling and the actions are to move the object right or left objects falling or to push it all the way down.",
                    "label": 0
                },
                {
                    "sent": "The usual things that you would do when you play Tetris.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The reward function would be, for example, one unit of reward for every time step.",
                    "label": 0
                },
                {
                    "sent": "The game is not over.",
                    "label": 0
                },
                {
                    "sent": "So maximizing that would then lead to obvious sort of optimization.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to make those problems, make reinforcement learning concrete.",
                    "label": 0
                },
                {
                    "sent": "Now let me step back and talk a little bit more about the history in place of reinforcement learning, and I will then dive into the Markov decision process or framework.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In some ways, reinforcement learning has has.",
                    "label": 0
                },
                {
                    "sent": "Has had input from an.",
                    "label": 0
                },
                {
                    "sent": "Actually outputted ideas from multiple different disciplines.",
                    "label": 0
                },
                {
                    "sent": "Anylist some of these here because I'll essentially touch on some of these ideas that come from these different disciplines.",
                    "label": 0
                },
                {
                    "sent": "I won't really talk about neuroscience, so let me begin with that.",
                    "label": 0
                },
                {
                    "sent": "I won't talk much about that, so let me say a couple of sentences about it and then move on from there.",
                    "label": 0
                },
                {
                    "sent": "Particularly over the last five years or so, you know, great deal of excitement in the intersection of neuroscience and reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "In particular, there has been a.",
                    "label": 0
                },
                {
                    "sent": "Part of the brain.",
                    "label": 0
                },
                {
                    "sent": "The basal ganglia on the dopamine system in particular, that has been shown pretty convincingly now to actually implement an algorithm.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you about today.",
                    "label": 0
                },
                {
                    "sent": "The temporal differencing algorithm.",
                    "label": 0
                },
                {
                    "sent": "So there is a part of the brain that fairly convincingly now shown to be implementing the TD algorithm.",
                    "label": 0
                },
                {
                    "sent": "So as you can imagine, this is very exciting for reinforcement learning researchers, but also has been very exciting for neuroscience people because now they have a connection to all this wonderful, I think pieces of work and reinforcement learning that they can map their ideas.",
                    "label": 0
                },
                {
                    "sent": "My math and I just want to.",
                    "label": 0
                },
                {
                    "sent": "There's a whole bunch of other work in neuroscience that has, I guess, a reinforcement learning and it really won't talk about that at all.",
                    "label": 0
                },
                {
                    "sent": "If you're really interested in that, sort of that direction, I can mention a name from where to follow on the name is Peter Diane DAYN.",
                    "label": 0
                },
                {
                    "sent": "If you just do a Google search will find him an from his papers.",
                    "label": 0
                },
                {
                    "sent": "You can make lots and lots of connections in this space.",
                    "label": 0
                },
                {
                    "sent": "OK, a very strong intersection exists between operations research and reinforcement learning, and in fact the Markov decision process formulation.",
                    "label": 0
                },
                {
                    "sent": "The palm DP formulation come from operations research, so it's a very strong interplay between operations research and reinforcement learning, and some of the researchers have very good researchers and reinforcement learning come from this community, and there's been some contributions back to this community from reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "There's an obvious connection to artificial intelligence.",
                    "label": 1
                },
                {
                    "sent": "Much of the original reinforcement learning actually started with mathematical psychology.",
                    "label": 1
                },
                {
                    "sent": "Bush was tell her work and I'll show you a quote in a minute from mathematical psychology that has a nugget of the reinforcement learning idea in it and finally.",
                    "label": 0
                },
                {
                    "sent": "Control theory or optimal control intersects with operations research and reinforcement learning, because in some sense to reinforcement learning view of the AI problem is that AI problem is an optimal control problem, so there's a very strong and obvious sort of intersection.",
                    "label": 0
                },
                {
                    "sent": "Then you'll see me develop some of this.",
                    "label": 0
                },
                {
                    "sent": "So here's the quote that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I wanted to share with you.",
                    "label": 0
                },
                {
                    "sent": "This is from Thorndike's law of effect and really from way back in 1911 captures the essence of the idea of reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And I'll read it to you.",
                    "label": 0
                },
                {
                    "sent": "And of course we should follow along on the screen.",
                    "label": 0
                },
                {
                    "sent": "The basic idea of several responses made the same situation, those which are followed or accompanied closely in time.",
                    "label": 0
                },
                {
                    "sent": "My satisfaction to the animal will.",
                    "label": 0
                },
                {
                    "sent": "Other things being equal, be more firmly connected with that situation so that we're in the same situation.",
                    "label": 0
                },
                {
                    "sent": "Reoccurs, the reaction is more likely to reoccur and the opposite.",
                    "label": 0
                },
                {
                    "sent": "For the opposite case.",
                    "label": 0
                },
                {
                    "sent": "Right, so this idea of.",
                    "label": 0
                },
                {
                    "sent": "Temporal coincidence between a situation and effect followed by satisfaction leads to strengthening of that connection.",
                    "label": 0
                },
                {
                    "sent": "Has been around for a long time and essentially is the foundational principle of the of reinforcement learning, and I'll, you know, and I will develop this when I talk about temporal differences.",
                    "label": 0
                },
                {
                    "sent": "I won't really belabor much more about the history.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But there's a whole long history of.",
                    "label": 0
                },
                {
                    "sent": "People building reinforcement learning ideas in in the field of computer science.",
                    "label": 1
                },
                {
                    "sent": "An artificial intelligence starting from touring really well.",
                    "label": 0
                },
                {
                    "sent": "I do have programming computer learn by trial and error, Minsky and so on, and I won't spend more time on this, OK?",
                    "label": 1
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "I promise I'm going to get to some very formal work very soon.",
                    "label": 0
                },
                {
                    "sent": "I want to continue building the connection between reinforcement learning an other areas for a few more slides.",
                    "label": 0
                },
                {
                    "sent": "So here what I'm going to try and do is distinguished reinforcement learning from the other areas of machine learning that more people are familiar with, which is supervised and unsupervised learning so supervised learning, learning from examples, learning approaches to regression classification, unsupervised learning, are learning approach, dimensionality reduction, density estimation, and so on.",
                    "label": 1
                },
                {
                    "sent": "In this context, which reinforcement learning reinforcement learning is learning approaches to sequential decision making, learning from delayed reward?",
                    "label": 0
                },
                {
                    "sent": "So let me explain this distinction.",
                    "label": 0
                },
                {
                    "sent": "A key key question that reinforcement learning asks that is not asked by supervisor.",
                    "label": 0
                },
                {
                    "sent": "Unsupervised learning is the following.",
                    "label": 0
                },
                {
                    "sent": "Imagine you play a game of chess.",
                    "label": 0
                },
                {
                    "sent": "And you lose.",
                    "label": 0
                },
                {
                    "sent": "You made.",
                    "label": 0
                },
                {
                    "sent": "I don't know 50 moves and you've lost.",
                    "label": 0
                },
                {
                    "sent": "It's possible that exactly 1 move was not the right move, and everything else is perfect.",
                    "label": 0
                },
                {
                    "sent": "And by one move doomed you to lose.",
                    "label": 0
                },
                {
                    "sent": "How would you figure that out?",
                    "label": 0
                },
                {
                    "sent": "How would an agent figure out which one of its 50 moves, which one or more of his 50 moves was responsible for that failure?",
                    "label": 0
                },
                {
                    "sent": "This problem is called a temporal credit assignment problem.",
                    "label": 0
                },
                {
                    "sent": "And is the key problem that reinforcement learning solves and is not addressed at all by supervised and unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "It's this problem that distinguishes reinforcement learning from the other areas of machine learning.",
                    "label": 0
                },
                {
                    "sent": "This problem of temporary assignment.",
                    "label": 0
                },
                {
                    "sent": "How do you figure out what actions are responsible for good outcomes in the long run?",
                    "label": 0
                },
                {
                    "sent": "OK, here's the part.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "List of applications.",
                    "label": 0
                },
                {
                    "sent": "Um, as you can imagine.",
                    "label": 0
                },
                {
                    "sent": "Robotics and control theory has lots and lots of applications that can be formulated as reinforcement learning problems.",
                    "label": 1
                },
                {
                    "sent": "Lots of games, backgammon, Chester, Taylor, Tetris, have been.",
                    "label": 1
                },
                {
                    "sent": "Reinforcement learning is been applied to lots of fascinating work in control, including the very recent and I'll show you some videos often work by Andrew ING on Helicopter Control, which is really been quite dramatically successful at using reinforcement learning to address a problem that optimal control people have worked on for a very long time, and reinforcement came along and in fairly short order, admittedly at the hands of a very good person and ring produced a really.",
                    "label": 0
                },
                {
                    "sent": "Really quite successful.",
                    "label": 0
                },
                {
                    "sent": "Outstanding performance in a real helicopter.",
                    "label": 0
                },
                {
                    "sent": "Lots of operations research problems are formulated.",
                    "label": 1
                },
                {
                    "sent": "Reinforcement learning problems, other problems in HCI, which is one of my areas of interest and so on.",
                    "label": 0
                },
                {
                    "sent": "So I will see applications as we as we go along.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You need to see this.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm ready to now die.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Into the more meaty stuff.",
                    "label": 0
                },
                {
                    "sent": "So we're going to begin to think about how one formulates a reinforcement learning problem.",
                    "label": 0
                },
                {
                    "sent": "So you have an agent, environment agent, environment interaction.",
                    "label": 0
                },
                {
                    "sent": "We're going to make the following assumptions discrete time.",
                    "label": 0
                },
                {
                    "sent": "That is, the agent acts once every time interval.",
                    "label": 0
                },
                {
                    "sent": "Discrete observations, so the sensors give it one of N from a set of observations.",
                    "label": 0
                },
                {
                    "sent": "Discreet action so will stay.",
                    "label": 0
                },
                {
                    "sent": "In fact.",
                    "label": 0
                },
                {
                    "sent": "Alter this talk, except for tomorrow a little bit tomorrow in the discrete time discrete observations discrete actions framework.",
                    "label": 0
                },
                {
                    "sent": "If you like you can ask me questions I'll try to leave 510 minutes at the end for how deal with continuous actions and continuous observations.",
                    "label": 0
                },
                {
                    "sent": "I won't really talk about this.",
                    "label": 0
                },
                {
                    "sent": "With these assumptions, let's begin groups.",
                    "label": 0
                },
                {
                    "sent": "Let's begin to talk about how one defines a model of the reinforcement learning agent environment interaction.",
                    "label": 1
                },
                {
                    "sent": "So there are two pieces to the model.",
                    "label": 0
                },
                {
                    "sent": "What is what I call transition probabilities?",
                    "label": 0
                },
                {
                    "sent": "These define the probability of the observation at time T + 1 given the entire history of interaction.",
                    "label": 0
                },
                {
                    "sent": "So far.",
                    "label": 0
                },
                {
                    "sent": "So just say what might conditional distribution over the next observation would be given what I've seen so far.",
                    "label": 0
                },
                {
                    "sent": "And of course, the same thing for the reward.",
                    "label": 0
                },
                {
                    "sent": "So this is the probability density if you like over the next reward.",
                    "label": 0
                },
                {
                    "sent": "Given my history of interaction so far.",
                    "label": 0
                },
                {
                    "sent": "If I can define these two pieces, then in effect I have defined the agent environment agent environment interaction because agent will take a next action and then recursively we can define the next observation and the next reward and so on.",
                    "label": 0
                },
                {
                    "sent": "So these two pieces or what's needed to define the environment agent interaction?",
                    "label": 0
                },
                {
                    "sent": "Is that clear?",
                    "label": 0
                },
                {
                    "sent": "Everybody, yes, OK.",
                    "label": 0
                },
                {
                    "sent": "So now here is the MDP formula formalism.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the key key idea behind the formalism is that it makes a Markov assumption.",
                    "label": 0
                },
                {
                    "sent": "What's the Markov assumption?",
                    "label": 0
                },
                {
                    "sent": "The Markov assumption makes the following the observation at time T + 1.",
                    "label": 0
                },
                {
                    "sent": "The conditional distribution given the entire history is equal to the conditional distribution of the next observation given just the recent observation and the recent action.",
                    "label": 0
                },
                {
                    "sent": "The last observation in action.",
                    "label": 0
                },
                {
                    "sent": "This is the Markov assumption, same sort of absorbed assumption for the reward function.",
                    "label": 0
                },
                {
                    "sent": "This leads to defining a very simple compact model for.",
                    "label": 0
                },
                {
                    "sent": "Agent around interactions which defines these transition probabilities as follows.",
                    "label": 0
                },
                {
                    "sent": "Here's some notation.",
                    "label": 0
                },
                {
                    "sent": "This is the conditional probability the next state.",
                    "label": 0
                },
                {
                    "sent": "By the way, if you make the Markov assumption.",
                    "label": 0
                },
                {
                    "sent": "Then you essentially assuming that observations are state.",
                    "label": 0
                },
                {
                    "sent": "That is, the recent the latest observation is a sufficient statistic of the entire history of observations.",
                    "label": 0
                },
                {
                    "sent": "You say it differently then if you have the last observation, you don't need to keep the rest of the history.",
                    "label": 0
                },
                {
                    "sent": "Because the next observation is just determined by stochastically, determined by the last observation in the action.",
                    "label": 0
                },
                {
                    "sent": "So once you have the last observation, the entire rest of the history is irrelevant.",
                    "label": 0
                },
                {
                    "sent": "This Markov assumption is equivalent to saying that the last observation is just state, so we use a symbol S for state and you say PSS prime is the probability the next status prime given the current state of South and the current action is a similar award RASM.",
                    "label": 0
                },
                {
                    "sent": "Prime is, that is the expected immediate reward if the agent transitions from State S 2 S prime, an action A.",
                    "label": 0
                },
                {
                    "sent": "So this defines the Markov decision process model.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now I'm ready to define for you the reinforcement learning problem given the MDP formula.",
                    "label": 0
                },
                {
                    "sent": "OK so X is a finite state space is a finite action space.",
                    "label": 0
                },
                {
                    "sent": "The transition probabilities as just define R as a reward function.",
                    "label": 0
                },
                {
                    "sent": "Another term we need is the notion of a policy policy Pi.",
                    "label": 0
                },
                {
                    "sent": "If policy is a mapping from states to actions.",
                    "label": 0
                },
                {
                    "sent": "So policy prescribes the behavior of the agent.",
                    "label": 0
                },
                {
                    "sent": "It says what action the agent will do in every possible state of the world.",
                    "label": 0
                },
                {
                    "sent": "So policy is a mapping from states to actions.",
                    "label": 0
                },
                {
                    "sent": "In particular, will for MDP's will be talking about deterministic nonstationary policies.",
                    "label": 0
                },
                {
                    "sent": "So the non stationary policies, because they're not a function of time, the deterministic because they're not random, that is there not mapping some states to distributions over actions.",
                    "label": 0
                },
                {
                    "sent": "It turns out that we can consider deterministic non stationary policies only because.",
                    "label": 0
                },
                {
                    "sent": "In a Markov decision process, I won't prove this, but can be easily proven that the optimal policy belongs to this class.",
                    "label": 0
                },
                {
                    "sent": "That is, there always exists an optimal policy that is deterministic and non stationary.",
                    "label": 1
                },
                {
                    "sent": "I want really prove this, but I'm happy too.",
                    "label": 0
                },
                {
                    "sent": "If you go to any operations research book on dynamic Programming and MPs will see this proof.",
                    "label": 0
                },
                {
                    "sent": "OK, now the quantity V super pivi.",
                    "label": 0
                },
                {
                    "sent": "This is the return.",
                    "label": 0
                },
                {
                    "sent": "For policy Pi, when it started in state I.",
                    "label": 0
                },
                {
                    "sent": "So let's define the notion of return.",
                    "label": 0
                },
                {
                    "sent": "So V of Pi I is the expected long-term discounted sum of rewards.",
                    "label": 0
                },
                {
                    "sent": "If the agent starts in state eye and executes policy Pi, so it's a measure of how good policy Pi is.",
                    "label": 0
                },
                {
                    "sent": "If the agent starts in state I.",
                    "label": 1
                },
                {
                    "sent": "So we apply the value of state I under policy.",
                    "label": 0
                },
                {
                    "sent": "Pi is just how good it is to start in state I and execute policy \u03c0.",
                    "label": 0
                },
                {
                    "sent": "Where goodness is measured by the discounted sum of rewards.",
                    "label": 0
                },
                {
                    "sent": "So Gamma is a scalar between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Let's say it's .9.",
                    "label": 0
                },
                {
                    "sent": "And So what this discounted measure is saying is a dollar today.",
                    "label": 0
                },
                {
                    "sent": "Sorry, a dollar tomorrow is worth $0.90.",
                    "label": 0
                },
                {
                    "sent": "A dollar 2 days from now is worth $0.81.",
                    "label": 0
                },
                {
                    "sent": "Or three days from now is worth points, 729 cents, 72.9 cents, right?",
                    "label": 0
                },
                {
                    "sent": "So it's discounting future reward by this sort of geometric gamma.",
                    "label": 0
                },
                {
                    "sent": "Right, sort of.",
                    "label": 0
                },
                {
                    "sent": "The motivation for this is twofold.",
                    "label": 0
                },
                {
                    "sent": "The basic motivation for this comes from economics.",
                    "label": 0
                },
                {
                    "sent": "The idea is again, a dollar.",
                    "label": 0
                },
                {
                    "sent": "Today is worth more than a dollar tomorrow because it's actually put in the bank and an interest on it.",
                    "label": 0
                },
                {
                    "sent": "So this discount factor in effect captures that notion of interest.",
                    "label": 0
                },
                {
                    "sent": "The other motivations for this that yes.",
                    "label": 0
                },
                {
                    "sent": "This microphone.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "Coronation.",
                    "label": 0
                },
                {
                    "sent": "I see so the Markov assumption icing the question so one way to think about the Markov assumption might be that somehow the Markov assumption decouples the long run from the short run.",
                    "label": 0
                },
                {
                    "sent": "But that's not true.",
                    "label": 0
                },
                {
                    "sent": "Imagine the game of chess, the board, the Chess board is.",
                    "label": 0
                },
                {
                    "sent": "A state right in some sense, if I show you the chess board how I arrived, the chess board is irrelevant.",
                    "label": 0
                },
                {
                    "sent": "It captures all the information there is about the history of interaction.",
                    "label": 0
                },
                {
                    "sent": "Almost right, it doesn't model the agents opponents playing, but forget that for a minute.",
                    "label": 0
                },
                {
                    "sent": "Imagine you're playing is a fixed opponent, always plays the same way for just a minute.",
                    "label": 0
                },
                {
                    "sent": "So the board is then state.",
                    "label": 0
                },
                {
                    "sent": "But still there is.",
                    "label": 0
                },
                {
                    "sent": "Fundamental tradeoff that I should have observed before the temporal create assignment problem still remains this tradeoff between short term and long term gain still remains because it could be that I can take actions on the chess board to gain pieces in the short run.",
                    "label": 0
                },
                {
                    "sent": "But that could hurt me in the long run.",
                    "label": 0
                },
                {
                    "sent": "Similarly, the idea of trading off short term versus long term is still there in MDP in a Markov decision process framework, so it's not the case that the Markov assumption says I can choose actions greedily.",
                    "label": 0
                },
                {
                    "sent": "Then I don't have to worry about the impact of my actions on the long term, because when I take an action it affects the state I'll get to.",
                    "label": 0
                },
                {
                    "sent": "In chess, it affects the next board.",
                    "label": 0
                },
                {
                    "sent": "And the next board.",
                    "label": 0
                },
                {
                    "sent": "What action choices I have available will be different.",
                    "label": 0
                },
                {
                    "sent": "Will be affected by the board that I landed.",
                    "label": 0
                },
                {
                    "sent": "Right, so through this notion of state, I have long term consequences that I would worry about.",
                    "label": 0
                },
                {
                    "sent": "And to this value function that I care about him and I'll define the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "It is capturing the long term.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Another way of distinguishing reinforcement learning from machine from other machine learning methods is the fact that.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah, reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Is interested in the problem of the trading off between short term and long term consequences of actions, right?",
                    "label": 0
                },
                {
                    "sent": "So we can choose actions that give us high reward in the short run, but they may lead to states from which is not possible to obtain high reward in the long run.",
                    "label": 0
                },
                {
                    "sent": "Contrast that with the case where you might choose actions which give you low toward the longer in the short run, but lead you to states from which very high rewards obtainable in the long run, right?",
                    "label": 0
                },
                {
                    "sent": "This is a fundamental aspect of reinforcement learning, fundamental aspects of human life, animal life, and that's captured in this reinforcement learning Markov Decision process framework.",
                    "label": 0
                },
                {
                    "sent": "Now, by the way, again, to emphasize, this is called a discounted framework.",
                    "label": 0
                },
                {
                    "sent": "There is another framework and reinforcement that I won't talk much about is called the average reward framework and the average reward framework replaces this discounted sum of rewards by the average reward overtime.",
                    "label": 0
                },
                {
                    "sent": "And you can choose to optimize that quantity rather than a discounted framework.",
                    "label": 0
                },
                {
                    "sent": "OK, and in fact I say that here now is what framework will just take the expected average reward.",
                    "label": 0
                },
                {
                    "sent": "The limit of that overtime.",
                    "label": 0
                },
                {
                    "sent": "I should say the horizon here is Infinity, right?",
                    "label": 0
                },
                {
                    "sent": "We're going all the way to Infinity all the way to the future in this.",
                    "label": 0
                },
                {
                    "sent": "In this framework, OK?",
                    "label": 0
                },
                {
                    "sent": "Step.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Step of defining for you the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So we have a quantity called the optimal policy which we did not \u03c0 star.",
                    "label": 0
                },
                {
                    "sent": "The optimal policy is the policy that maximizes the value, that is, the policy that maximizes the value function.",
                    "label": 0
                },
                {
                    "sent": "So it's the policy so that you behave if you behave according to that.",
                    "label": 0
                },
                {
                    "sent": "There is no other policy that obtains higher reward.",
                    "label": 0
                },
                {
                    "sent": "The optimal value function we star is of course the optimal is the value associated with the optimal policy, or if in effect the optimal value of state.",
                    "label": 0
                },
                {
                    "sent": "I restarted.",
                    "label": 0
                },
                {
                    "sent": "Why is the most value you can obtain from state and any way of behaving?",
                    "label": 0
                },
                {
                    "sent": "Here's the interesting observation.",
                    "label": 0
                },
                {
                    "sent": "It turns out in MDP.",
                    "label": 0
                },
                {
                    "sent": "To always exist a optimal policy in the sense that a policy that optimizes the value of every state simultaneously.",
                    "label": 1
                },
                {
                    "sent": "Or in other words, it is not the case that depending on where you start, which state you start in, the policy changes.",
                    "label": 0
                },
                {
                    "sent": "It exists a policy that no matter where you start is the optimal policy.",
                    "label": 0
                },
                {
                    "sent": "So that's just a fundamental result from a from an MDP point of view, and that's something we'll exploit as we develop planning and learning algorithms in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Is the import of that tier right?",
                    "label": 0
                },
                {
                    "sent": "We're going to have optimal policy that assigns an action to every state and it just doesn't depend on which state you're starting on.",
                    "label": 0
                },
                {
                    "sent": "It's independent of that.",
                    "label": 0
                },
                {
                    "sent": "That property holds for mark operating processes, and we're going to exploit that as we go through.",
                    "label": 0
                },
                {
                    "sent": "Oh, let's see.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So just again to confirm just to drive home this point via Pi is a function that assigns or a vector, sometimes with the function, sometimes a vector that assigns a real number every state similarly star.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are two problems that one defines when bills reinforcement learning, one is called a policy evaluation problem or the prediction problem.",
                    "label": 0
                },
                {
                    "sent": "The policy evaluation problem is a problem given a policy.",
                    "label": 0
                },
                {
                    "sent": "Tell me how good that policy is so it's predicting how good is specific policy is.",
                    "label": 0
                },
                {
                    "sent": "As the policy evaluation problem, and I'll tell you how to solve it in a minute, and then I'll define what's called the optimal control problem, which is the problem of finding the optimal policy.",
                    "label": 0
                },
                {
                    "sent": "And the reason why we split things this way is because we can develop a lot of intuition and the algorithms from the policy evaluation problem with the prediction problem and then move on to the optimal control problem.",
                    "label": 0
                },
                {
                    "sent": "So the so the policy evaluation problem is to learn via Pi given a pie.",
                    "label": 0
                },
                {
                    "sent": "And the way we do it is, we define what's called a bellman optimality equations.",
                    "label": 1
                },
                {
                    "sent": "Or at least the policy evaluation version of these, which looks like this.",
                    "label": 0
                },
                {
                    "sent": "Basically you look at his first principles definition of the value for state I under policy \u03c0, and you can use the Markov assumption to derive this recursive system of equations, which says the value of state S under policy Pi is the immediate reward you get from that state if you behave according to policy \u03c0, plus the should be a discount.",
                    "label": 0
                },
                {
                    "sent": "There should be gamma here, it discounted expected value of the next state.",
                    "label": 0
                },
                {
                    "sent": "Policy pie.",
                    "label": 0
                },
                {
                    "sent": "So this is relating the value of the current state.",
                    "label": 0
                },
                {
                    "sent": "With the expected value of the states that are one time step away under policy pie.",
                    "label": 0
                },
                {
                    "sent": "So for all states S. Immediate value of status is immediate reward plus the discount.",
                    "label": 0
                },
                {
                    "sent": "The gamma is missing discount.",
                    "label": 0
                },
                {
                    "sent": "Expected value of the next date and the policy \u03c0.",
                    "label": 0
                },
                {
                    "sent": "So you set up this system of equations.",
                    "label": 0
                },
                {
                    "sent": "One for every state.",
                    "label": 0
                },
                {
                    "sent": "Whose solution?",
                    "label": 0
                },
                {
                    "sent": "Is the value for policy pie.",
                    "label": 0
                },
                {
                    "sent": "Right, this is a system of equations, one for every state, where the unknowns in the system of equations, the value function, what's known, what's defined by the Markov decision process as a reward functions and the transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "But either end states that any equations and then unknowns and you can solve this if you like for the value for the value functions of policy \u03c0.",
                    "label": 0
                },
                {
                    "sent": "It turns out it's very useful to also define a alternative definition for value function, which is called Q values or state action values, which defines a number not for states, as here, but for state action pair.",
                    "label": 0
                },
                {
                    "sent": "So let me define.",
                    "label": 0
                },
                {
                    "sent": "This will come in handy when we talk about sort of the most famous contribution of reinforcement learning, which is Q learning.",
                    "label": 0
                },
                {
                    "sent": "So here's the definition of the state action value for policy.",
                    "label": 0
                },
                {
                    "sent": "Pi is just the expected value of the discounted sum of rewards if you start in state S and the first action you take is A and thereafter you follow policy \u03c0.",
                    "label": 0
                },
                {
                    "sent": "So it delays policy Piper one step, and prescribes action A in the starting state and asks how good is that starting state action pair rather than the starting state, which is what we defined here.",
                    "label": 0
                },
                {
                    "sent": "And you can similarly define recursive system of equations.",
                    "label": 0
                },
                {
                    "sent": "The Q value for state action pair is the immediate reward you get if you take action A in state S now rather than the action pie of S. Plus the discounted gamma missing again expected value for the next state, but you have to be able to policy pie from the next day onwards.",
                    "label": 0
                },
                {
                    "sent": "Again, you have a system of equations now, whose solution is these?",
                    "label": 0
                },
                {
                    "sent": "Quantities of interest, which is the value of these state action pairs.",
                    "label": 0
                },
                {
                    "sent": "But if it isn't clear why these values are interesting, will become clear in a minute.",
                    "label": 0
                },
                {
                    "sent": "But let me anticipate that in just a second the value functions are interesting because they summarize.",
                    "label": 0
                },
                {
                    "sent": "The long term consequences of this policy right vypyr vest is a summary statistic, a number that says what's the long term utility of being in status.",
                    "label": 0
                },
                {
                    "sent": "If you're instead ask what's the long term benefit you will get if you behave according to \u03c0, so it captures a smooshed together.",
                    "label": 0
                },
                {
                    "sent": "Right or a future into this one number.",
                    "label": 0
                },
                {
                    "sent": "OK. Let me define a similar system equations for the optimal control problem, and then I think we already talk about algorithms and about to get to algorithms.",
                    "label": 0
                },
                {
                    "sent": "If you're feeling that I'm giving you too many definitions, let me just go through one more page definitions and will do algorithms OK.",
                    "label": 0
                },
                {
                    "sent": "So here's the optimal.",
                    "label": 0
                },
                {
                    "sent": "Here's the optimal control problems.",
                    "label": 1
                },
                {
                    "sent": "Automated equation so the applicant.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem is to define is to learn.",
                    "label": 0
                },
                {
                    "sent": "Compute the optimal policy or the optimal value function.",
                    "label": 0
                },
                {
                    "sent": "So here is the.",
                    "label": 0
                },
                {
                    "sent": "Recursive system of equations that are satisfied that is satisfied that.",
                    "label": 0
                },
                {
                    "sent": "Whose solution is the optimal value function?",
                    "label": 0
                },
                {
                    "sent": "So for all states S. The best you can do from states from state S?",
                    "label": 0
                },
                {
                    "sent": "Is you consider all possible actions in that state?",
                    "label": 0
                },
                {
                    "sent": "And for each action, you ask, what's the immediate reward I get for that action.",
                    "label": 0
                },
                {
                    "sent": "Plus the discount.",
                    "label": 0
                },
                {
                    "sent": "Again, the discount is missing here discounted expected.",
                    "label": 0
                },
                {
                    "sent": "Best I could do for the next day.",
                    "label": 0
                },
                {
                    "sent": "Right, the best I can do in this state is I consider every possible action in this state for each action I ask, what's the immediate reward I get, but what's the best I can do with expectation from the next states that I'll get to if I take that action, compute that number?",
                    "label": 0
                },
                {
                    "sent": "If you like for each action, pick the best that's taking the best of this Max.",
                    "label": 0
                },
                {
                    "sent": "That gives you the best you can do in this state.",
                    "label": 0
                },
                {
                    "sent": "Again, the Markov decision process formulation prescribe the rewards and the transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "So now you have again any equations and unknowns.",
                    "label": 0
                },
                {
                    "sent": "If N is a number of states, unknowns being the vystar optimal values.",
                    "label": 0
                },
                {
                    "sent": "Again, this is only holds because the Markov assumption is being assumed, right?",
                    "label": 0
                },
                {
                    "sent": "This is true under the Markov assumption.",
                    "label": 0
                },
                {
                    "sent": "Same by the way, once you have the optimal value function, then you can derive the optimal policy optimal way of behaving from it as follows.",
                    "label": 0
                },
                {
                    "sent": "The optimal action Pi star in state S is just the action.",
                    "label": 0
                },
                {
                    "sent": "That maximizes the immediate reward plus the discount and expected best value from next day.",
                    "label": 0
                },
                {
                    "sent": "So in other words, it's the action that achieves the Max on the right hand side of the Bellman optimality equation.",
                    "label": 1
                },
                {
                    "sent": "That becomes the optimal action.",
                    "label": 0
                },
                {
                    "sent": "So why, because?",
                    "label": 0
                },
                {
                    "sent": "So we are interested in learning the optimal value function, because from it we can derive the optimal way of behaving.",
                    "label": 0
                },
                {
                    "sent": "And here is the equivalent definition for state action values.",
                    "label": 0
                },
                {
                    "sent": "So Q star avesco may again S, It simply means that you are committed to doing action in status and your uncommitted thereafter.",
                    "label": 0
                },
                {
                    "sent": "So the op, the best you can do.",
                    "label": 0
                },
                {
                    "sent": "The most value you can get if you do actually changed it S and you can do for free to do whatever you want to do afterwards is the immediate reward for that state action pair, because it committed for that one time step plus the discounted expected best you can do from the state you get to.",
                    "label": 0
                },
                {
                    "sent": "Right again, this is a recursive system equations OK. Now, if you're given the Q star values.",
                    "label": 0
                },
                {
                    "sent": "Then the optimal action in state S is simply the action that has the largest Q value associated with it.",
                    "label": 0
                },
                {
                    "sent": "Because right because you're now you're having a learning of value for every action, so you simply pick the action that has the largest associated value and that's your.",
                    "label": 0
                },
                {
                    "sent": "That's your optimal policy.",
                    "label": 0
                },
                {
                    "sent": "Now and relationship between the state action values and state values is the best you can do in state S. Not surprisingly, is you consider every action.",
                    "label": 0
                },
                {
                    "sent": "And look at the Q value for that action and the best the Max or that is the V star value.",
                    "label": 0
                },
                {
                    "sent": "Yes, you're the fullest is deterministic.",
                    "label": 0
                },
                {
                    "sent": "Where do you have probability of S prime?",
                    "label": 0
                },
                {
                    "sent": "Given S?",
                    "label": 0
                },
                {
                    "sent": "What does it mean?",
                    "label": 0
                },
                {
                    "sent": "Ha.",
                    "label": 0
                },
                {
                    "sent": "This is the probability distribution over next states given that you take action AIDS data.",
                    "label": 0
                },
                {
                    "sent": "So this is a distribution over next states.",
                    "label": 0
                },
                {
                    "sent": "We're not assuming that state transitions are deterministic policy.",
                    "label": 0
                },
                {
                    "sent": "Being deterministic means that I have a fixed prescribed choice as to what action I take in every state.",
                    "label": 0
                },
                {
                    "sent": "The state, the next state that happens is random.",
                    "label": 0
                },
                {
                    "sent": "And so this is a distribution over next states, not a stochastic policy.",
                    "label": 0
                },
                {
                    "sent": "By the way, you could write down these equations with stochastic policies as well, and should have an additional expectation over or actions.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I'm about to get to algorithms.",
                    "label": 0
                },
                {
                    "sent": "Here's sort of a graphical view of a Markov decision process.",
                    "label": 1
                },
                {
                    "sent": "Proceeding right, you're in some state S. You have a choice of actions.",
                    "label": 0
                },
                {
                    "sent": "You pick one action because you can only do one action at a time.",
                    "label": 0
                },
                {
                    "sent": "And that the distribution over next states.",
                    "label": 0
                },
                {
                    "sent": "For that action, state Choice is a distribution over next states.",
                    "label": 0
                },
                {
                    "sent": "In reality, one of these states will happen.",
                    "label": 0
                },
                {
                    "sent": "And you repeat your pick.",
                    "label": 0
                },
                {
                    "sent": "An action is a distribution over next states.",
                    "label": 0
                },
                {
                    "sent": "You'll pick and action distribution next States and that's so this black line is that life trajectory.",
                    "label": 0
                },
                {
                    "sent": "We just saw you have an observation would take an action, observation, action, observation.",
                    "label": 0
                },
                {
                    "sent": "And you can imagine this another way.",
                    "label": 0
                },
                {
                    "sent": "Let me remind you the problem is of course you may get rewarded, some very delayed point of time and the task is to figure out you know is to assign credit or blame to each of the actions you took along the way.",
                    "label": 1
                },
                {
                    "sent": "And this is a temporary assignment problem.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's talk about the two key questions in reinforcement learning and then define the algorithms.",
                    "label": 0
                },
                {
                    "sent": "So the two problems in that I'm going to talk about is the planning problem and the learning problem.",
                    "label": 0
                },
                {
                    "sent": "What's the planning problem?",
                    "label": 0
                },
                {
                    "sent": "The planning problem is.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You're given a model of the Markov transition process.",
                    "label": 1
                },
                {
                    "sent": "That is, you given the transition probabilities and you're given the reward function.",
                    "label": 1
                },
                {
                    "sent": "Planning problem becomes that of then solving for the optimal values or the value function and the optimal policy.",
                    "label": 0
                },
                {
                    "sent": "The learning problem, by contrast, will be when you're not given the reward function and you're not given the transition probabilities and you still facing the same problem.",
                    "label": 0
                },
                {
                    "sent": "So that's the distinction when reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "People say planning problem, they just mean you're giving him out of the world.",
                    "label": 0
                },
                {
                    "sent": "When you, when you're learning, you're not given a problem.",
                    "label": 0
                },
                {
                    "sent": "The model of the world.",
                    "label": 0
                },
                {
                    "sent": "So let's consider how to solve the policy evaluation problem when you're in the planning setting.",
                    "label": 0
                },
                {
                    "sent": "So panning setting means you're given the transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "And given the reward function.",
                    "label": 0
                },
                {
                    "sent": "And in the policy evaluation setting you're given the policy and what you want to learn is the value function for that policy.",
                    "label": 0
                },
                {
                    "sent": "So here is the first algorithm.",
                    "label": 1
                },
                {
                    "sent": "This is an algorithm from dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "It's called value iteration, and it works like this.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see.",
                    "label": 0
                },
                {
                    "sent": "I guess this is the one for a stochastic policy, since I'm not talking about stochastic policies.",
                    "label": 0
                },
                {
                    "sent": "Ignore this line.",
                    "label": 0
                },
                {
                    "sent": "Let's look at this one.",
                    "label": 0
                },
                {
                    "sent": "So here's the idea.",
                    "label": 0
                },
                {
                    "sent": "I'm going to start with some arbitrary initial guess of the of the of the pie called V sub zero.",
                    "label": 0
                },
                {
                    "sent": "So the subscripts here.",
                    "label": 0
                },
                {
                    "sent": "Denote iteration.",
                    "label": 0
                },
                {
                    "sent": "And at each iteration I'm going to update my guess.",
                    "label": 0
                },
                {
                    "sent": "Of the of the pie as follows, and the care duration I'm going to say my next guess of the value of state, as in the policy Pi is just the immediate reward plus the discounted expected.",
                    "label": 0
                },
                {
                    "sent": "Value of the next state under my current guess.",
                    "label": 0
                },
                {
                    "sent": "So will update my guess.",
                    "label": 0
                },
                {
                    "sent": "On the basis of my current guess.",
                    "label": 0
                },
                {
                    "sent": "I'm just gonna iterate this.",
                    "label": 0
                },
                {
                    "sent": "So what I've done?",
                    "label": 0
                },
                {
                    "sent": "If you think back to the optimality Berman automatic equation is, I'm just recursing that equation.",
                    "label": 0
                },
                {
                    "sent": "I'm feeding in my current guess on the right hand side and from the left hand side.",
                    "label": 0
                },
                {
                    "sent": "I'm driving my next guess, feeding it back in and cycling that recursion.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "Cracking that recursion.",
                    "label": 0
                },
                {
                    "sent": "Starting with some arbitrary initial guess and one can easily show that this.",
                    "label": 0
                },
                {
                    "sent": "This this algorithm converges to the value of that policy Pi and I'll do 1 proof in a few in a few minutes.",
                    "label": 0
                },
                {
                    "sent": "Then I get through Q learning.",
                    "label": 0
                },
                {
                    "sent": "Then I'll do a proof that illustrates the idea behind these convergence, which I won't really talk much about.",
                    "label": 0
                },
                {
                    "sent": "And as you can imagine.",
                    "label": 0
                },
                {
                    "sent": "Figure out when to stop and a good stopping criterion is when the change in value across two iterations is less than some threshold.",
                    "label": 0
                },
                {
                    "sent": "In fact, you can prove that if this change in threshold is less than epsilon, then V sub K plus one is only so far from the pie, which is a target value that you don't know.",
                    "label": 0
                },
                {
                    "sent": "OK, this algorithm clear I'm given the model.",
                    "label": 0
                },
                {
                    "sent": "I'm giving a policy of mass to evaluate it I just cycle through this iteration an out in the end.",
                    "label": 0
                },
                {
                    "sent": "When it finishes you have a.",
                    "label": 0
                },
                {
                    "sent": "You have a good estimate of the pie.",
                    "label": 0
                },
                {
                    "sent": "How good it is?",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "How good is policy pie?",
                    "label": 0
                },
                {
                    "sent": "So this is the policy evaluation evaluation algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, now let's.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do.",
                    "label": 0
                },
                {
                    "sent": "Oh, and you can do the same thing.",
                    "label": 0
                },
                {
                    "sent": "You can do the same thing with the state action values one.",
                    "label": 0
                },
                {
                    "sent": "I won't spend time on this.",
                    "label": 0
                },
                {
                    "sent": "This is exactly the same iteration, but only done for the state action pair rather than the than the.",
                    "label": 0
                },
                {
                    "sent": "State values, so once you say.",
                    "label": 0
                },
                {
                    "sent": "Conversion value is unique or not?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Why is it unique?",
                    "label": 0
                },
                {
                    "sent": "Remind you the question the question is, is the pie unique?",
                    "label": 0
                },
                {
                    "sent": "He said give me a pie.",
                    "label": 0
                },
                {
                    "sent": "Is it unique and it is unique because you fix the policy.",
                    "label": 0
                },
                {
                    "sent": "There's some reward distribution and went from first principles definition, right?",
                    "label": 0
                },
                {
                    "sent": "The value of a status and the policy Pi is the expected discounted sum of rewards you would get and everything is well formulated so you will get a unique value function.",
                    "label": 0
                },
                {
                    "sent": "So the pies are unique quality.",
                    "label": 0
                },
                {
                    "sent": "What's not unique is the optimal policy, and I'll get when I get to that, then I'll I'll talk about that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's do optimal control.",
                    "label": 1
                },
                {
                    "sent": "Optimal control is planning.",
                    "label": 0
                },
                {
                    "sent": "An optimal control is.",
                    "label": 0
                },
                {
                    "sent": "You're given a model that is, you're given the transition probabilities and you're given the reward function and your task is to find the optimal policy.",
                    "label": 1
                },
                {
                    "sent": "How does that work exactly?",
                    "label": 1
                },
                {
                    "sent": "The same idea I'm going to do exactly the same thing.",
                    "label": 0
                },
                {
                    "sent": "It's called value iteration from dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to recurse the Bellman optimality equations.",
                    "label": 0
                },
                {
                    "sent": "That is, I'm going to feed my initial guess on the right hand side on the left hand side, my next guess, another cycle that.",
                    "label": 0
                },
                {
                    "sent": "So I'm sending my new guests to the Max or all actions of the immediate reward, but the discounted expected current value of the next day.",
                    "label": 0
                },
                {
                    "sent": "And again I will talk about why this converges in a few when I talk about Q learnings, convergence.",
                    "label": 0
                },
                {
                    "sent": "And the same thing here for a state action values, and in particular the optimal control question state action values will become very meaningful in a minute.",
                    "label": 0
                },
                {
                    "sent": "So now you're updating every state action pair and the next guess is the immediate reward plus the discounted expected best value of the next state.",
                    "label": 0
                },
                {
                    "sent": "Here occurs and you can you repeat this at every iteration and it converges to the Q star.",
                    "label": 0
                },
                {
                    "sent": "It converges the limit to the optimal optimal value functions and what are the stopping criteria?",
                    "label": 0
                },
                {
                    "sent": "Again, stopping criteria is same thing.",
                    "label": 0
                },
                {
                    "sent": "The difference between two iterations, the Max norm, the maximum difference, absolute difference between.",
                    "label": 0
                },
                {
                    "sent": "Any state action pairs values over 2 iterations is less than epsilon some bound.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is a solution to the optimal control problem in the planning case.",
                    "label": 0
                },
                {
                    "sent": "And then I'll talk about convergence now.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So let's see.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me prove to you.",
                    "label": 0
                },
                {
                    "sent": "And just because I want to one of these proofs because they are the foundation.",
                    "label": 0
                },
                {
                    "sent": "If you like of the proofs, reinforcement learning has had has built over time.",
                    "label": 0
                },
                {
                    "sent": "For reinforcement illustrate the idea that it's very simple proof of the proof of convergence for value iteration using the state action values as the as the instance of it.",
                    "label": 1
                },
                {
                    "sent": "So what I'm going to prove to you now is that.",
                    "label": 0
                },
                {
                    "sent": "Is what's called the contraction property of dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "So what is let me define what that means?",
                    "label": 0
                },
                {
                    "sent": "So let Delta K. Be the error at the cat iteration.",
                    "label": 0
                },
                {
                    "sent": "That is, it is the maximum error.",
                    "label": 0
                },
                {
                    "sent": "Off the Q sub K from Q star.",
                    "label": 0
                },
                {
                    "sent": "So I'm looking at Q star.",
                    "label": 0
                },
                {
                    "sent": "I'm looking at which I don't know.",
                    "label": 0
                },
                {
                    "sent": "Which I want to get to, but conceptually I'm saying let Delta X of K be the error at time step K defined in this fashion.",
                    "label": 0
                },
                {
                    "sent": "That is I'm looking over Allstate action pairs and asking what's the absolute difference between cusip, Karen, Q, star or I'm going to prove to you is that Q sub K plus one is closer to Q stars and queues FK was by a factor of gamma by multiplicative factor of gamma.",
                    "label": 0
                },
                {
                    "sent": "Let's do that proof.",
                    "label": 0
                },
                {
                    "sent": "The cusip cables, one of us, a.",
                    "label": 0
                },
                {
                    "sent": "Is the immediate reward plus the discounted expected best value of next state.",
                    "label": 0
                },
                {
                    "sent": "So let's look at Q's of K. Let's look at the cat guess.",
                    "label": 0
                },
                {
                    "sent": "I know the cat guess by this definition.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "No more than Q star.",
                    "label": 0
                },
                {
                    "sent": "The optimal value for that state action pair plus Delta K. Right, that's what this definition means.",
                    "label": 0
                },
                {
                    "sent": "I've defined Delta K so that this inequality holds.",
                    "label": 0
                },
                {
                    "sent": "So I'm replacing queues of K by something that I know is larger.",
                    "label": 0
                },
                {
                    "sent": "Just Q star plus Delta K. And now the simple trick is the observation that Delta K is independent of a is independent of S. So I can just pull it out.",
                    "label": 0
                },
                {
                    "sent": "When I put it out.",
                    "label": 0
                },
                {
                    "sent": "I'm multiplied by gamma.",
                    "label": 0
                },
                {
                    "sent": "So I get Gamma, Delta, K and I'm left with the thing in square brackets.",
                    "label": 0
                },
                {
                    "sent": "And if you look at this.",
                    "label": 0
                },
                {
                    "sent": "This is exactly by the mathematical equation.",
                    "label": 0
                },
                {
                    "sent": "The optimal Q value of state, action pairs, thereby proving that.",
                    "label": 0
                },
                {
                    "sent": "After the care iteration.",
                    "label": 0
                },
                {
                    "sent": "The guest value of state action pairs come A is within Gamma Delta K. So whatever Delta K was.",
                    "label": 0
                },
                {
                    "sent": "It has shrunk by multiplicative factor of gamma, so it's .9.",
                    "label": 0
                },
                {
                    "sent": "If gamma is .9, then the error has shrunk by point .9, so let me illustrate that pictorially.",
                    "label": 0
                },
                {
                    "sent": "Suppose Star is the unknown optimal value.",
                    "label": 0
                },
                {
                    "sent": "Let's say my my initial guess is here.",
                    "label": 0
                },
                {
                    "sent": "So and it's 2 dimensional state space.",
                    "label": 0
                },
                {
                    "sent": "I'm doing it with Victoria Lee's imaginative dimensional state space, so there's a box that contains my initial guess.",
                    "label": 0
                },
                {
                    "sent": "The smallest box that contains my initial guess.",
                    "label": 0
                },
                {
                    "sent": "So what I've just shown to you is that after one iteration, the size of the box that contains my next guess will have shrunk by a multiplicative factor of gamma.",
                    "label": 0
                },
                {
                    "sent": "So in every iteration.",
                    "label": 0
                },
                {
                    "sent": "The box size the box shrinks.",
                    "label": 0
                },
                {
                    "sent": "I don't know precisely where inside the box it is.",
                    "label": 0
                },
                {
                    "sent": "So in particular, the error could grow from one iteration to the next, but I know that the bounding box shrinks at every iteration and this property of the shrinking error is called a contraction property and is the foundation if you like of the proof behind many of the reinforcement learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "And I illustrate this when I talk about the learning instance of optimal control Q learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "You should ask me questions if you have any of this is unclear because you'll get hopelessly unclear afterwards otherwise.",
                    "label": 0
                },
                {
                    "sent": "So you know, feel free to ask me a.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is maybe this is very simple, maybe this is all very clear.",
                    "label": 0
                },
                {
                    "sent": "We can just charge ahead.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "No OK. Alright.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Start with learning problem.",
                    "label": 0
                },
                {
                    "sent": "So the learning problem is the same problems in terms of the goals, except we're not given a model right?",
                    "label": 0
                },
                {
                    "sent": "So but I know this is tutorial materials.",
                    "label": 1
                },
                {
                    "sent": "If you've taken a class in reinforcement learning, this will be fairly tutorial.",
                    "label": 0
                },
                {
                    "sent": "I am going to get to more recent and advanced material eventually, but I don't know your background, so I'm assuming that this very tutorial material is indeed necessary.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'm maybe I'm wrong about it.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So learning MDP which is the problem that reinforcement learning is most interested in, and that's the setting where we don't have a model.",
                    "label": 0
                },
                {
                    "sent": "So what do we have instead?",
                    "label": 0
                },
                {
                    "sent": "Instead, we have the real system.",
                    "label": 1
                },
                {
                    "sent": "That is a system in which we can take actions.",
                    "label": 0
                },
                {
                    "sent": "Observe observations, get rewards and keep interacting with it in that fashion.",
                    "label": 0
                },
                {
                    "sent": "Like that is will get to see the black line.",
                    "label": 0
                },
                {
                    "sent": "We don't get to see the red things.",
                    "label": 0
                },
                {
                    "sent": "The green things we get to see the black line.",
                    "label": 1
                },
                {
                    "sent": "OK, so the most we can do is generate experience which looks like this.",
                    "label": 0
                },
                {
                    "sent": "So the difference between the original trajectory you've seen in this trajectory is I'm assuming the Markov property, which means we're seeing the States.",
                    "label": 0
                },
                {
                    "sent": "Rather than just some some observations.",
                    "label": 0
                },
                {
                    "sent": "There are two classes of approaches to solving the reinforcement learning problem.",
                    "label": 1
                },
                {
                    "sent": "The learning problem.",
                    "label": 0
                },
                {
                    "sent": "One of them is called indirect methods and the other one is called direct methods, but we can learn how to make it interactive.",
                    "label": 0
                },
                {
                    "sent": "Can anybody think of a way of learning?",
                    "label": 0
                },
                {
                    "sent": "I told you a way of planning.",
                    "label": 0
                },
                {
                    "sent": "Is it an obvious way of doing the learning, solving the learning problem and we think of obviously have been learning solving the learning problem?",
                    "label": 0
                },
                {
                    "sent": "I've defined the learning problem right.",
                    "label": 0
                },
                {
                    "sent": "We're making a Markov assumption.",
                    "label": 0
                },
                {
                    "sent": "I've told you what do we have?",
                    "label": 0
                },
                {
                    "sent": "We have the ability to take actions and observe States and subserve rewards.",
                    "label": 0
                },
                {
                    "sent": "We notice all the planning problem, that is if you give me the transition probabilities of rewards, I know what to do.",
                    "label": 0
                },
                {
                    "sent": "So how would you solve the learning problem?",
                    "label": 0
                },
                {
                    "sent": "What's an obvious way of solving the learning problem?",
                    "label": 0
                },
                {
                    "sent": "I'm trying to figure out the problem.",
                    "label": 0
                },
                {
                    "sent": "Perfect, right?",
                    "label": 0
                },
                {
                    "sent": "And that would be an instance of what's called the indirect method.",
                    "label": 0
                },
                {
                    "sent": "So the indirect method for code enforcement learning problem is exactly what you said.",
                    "label": 0
                },
                {
                    "sent": "You wonder about the world.",
                    "label": 0
                },
                {
                    "sent": "You take action, then states you observe next states you observe rewards.",
                    "label": 0
                },
                {
                    "sent": "Now you're faced with supervised learning problem, supervised learning problem of building a model.",
                    "label": 0
                },
                {
                    "sent": "Building a model of the transition probabilities in the reward function.",
                    "label": 0
                },
                {
                    "sent": "Once you've learned the transition property, the word function, then you can use planning algorithms with the learn model.",
                    "label": 0
                },
                {
                    "sent": "And solve for the optimal policy.",
                    "label": 0
                },
                {
                    "sent": "That's in a very nutshell, the indirect method for solving the learning problem.",
                    "label": 0
                },
                {
                    "sent": "The direct methods.",
                    "label": 0
                },
                {
                    "sent": "Attempt to do the same thing, except they never learn about.",
                    "label": 0
                },
                {
                    "sent": "They never learn the transition probabilities and the level under award function, so let me make that make that distinction clear.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me give you the most.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The most obvious way.",
                    "label": 0
                },
                {
                    "sent": "The most basic simple way of solving the learning problem, which is exactly we just talked about.",
                    "label": 0
                },
                {
                    "sent": "I use experience data estimate the model.",
                    "label": 1
                },
                {
                    "sent": "How do I estimate the model?",
                    "label": 0
                },
                {
                    "sent": "Let me give you a simple instance in the discrete setting, my estimated probability of transitioning to stage A1, taking action a state I is the number of times in that trajectory that I see a transition of data state jail actually divided by the number of times.",
                    "label": 0
                },
                {
                    "sent": "This is wrong by the number of times I take action a state I.",
                    "label": 0
                },
                {
                    "sent": "Go to whichever state.",
                    "label": 0
                },
                {
                    "sent": "Like so did not make restaurant in the title.",
                    "label": 0
                },
                {
                    "sent": "But what are you doing?",
                    "label": 0
                },
                {
                    "sent": "Literally?",
                    "label": 0
                },
                {
                    "sent": "Accounting right?",
                    "label": 0
                },
                {
                    "sent": "This is the maximum likelihood estimate of the transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "I'm just counting how often I see my transition from state to state.",
                    "label": 0
                },
                {
                    "sent": "Check an action, a divide that by the number of times I've taken action against data that will transition probability.",
                    "label": 0
                },
                {
                    "sent": "Right, and it's too late to work function and then we compute the optimal policy with respect to the estimated model by this quantity is called the certainty equivalent policy.",
                    "label": 1
                },
                {
                    "sent": "The one question you should be asking yourself at this point is.",
                    "label": 0
                },
                {
                    "sent": "How do I choose actions?",
                    "label": 0
                },
                {
                    "sent": "And this brings up what's called the Exploration Exploitation, dilemma, exploration, exploitation, policy enforcement, learning, and this is a key fundamental problem again.",
                    "label": 0
                },
                {
                    "sent": "A problem that distinguishes reinforcement learning from other forms of machine learning.",
                    "label": 0
                },
                {
                    "sent": "Other forms of machine learning don't face this question and we define you.",
                    "label": 0
                },
                {
                    "sent": "Let me define the question for you.",
                    "label": 0
                },
                {
                    "sent": "It's called the Expiration exploitation dilemma.",
                    "label": 0
                },
                {
                    "sent": "Here's the problem.",
                    "label": 1
                },
                {
                    "sent": "Suppose I'm facing the learning problem.",
                    "label": 0
                },
                {
                    "sent": "So in agent in the world.",
                    "label": 0
                },
                {
                    "sent": "And I choose my next action.",
                    "label": 0
                },
                {
                    "sent": "I have a choice to make right.",
                    "label": 0
                },
                {
                    "sent": "Either I can choose to do the action.",
                    "label": 0
                },
                {
                    "sent": "That looks the best.",
                    "label": 1
                },
                {
                    "sent": "According to the current model of world that is, I have so much experience.",
                    "label": 0
                },
                {
                    "sent": "I have a certain amount of experience I can drive in order from it.",
                    "label": 0
                },
                {
                    "sent": "I can look at the certainty equivalent policy.",
                    "label": 0
                },
                {
                    "sent": "The policy that's greedy.",
                    "label": 0
                },
                {
                    "sent": "With respect to my current model and you should take that action.",
                    "label": 0
                },
                {
                    "sent": "That would be exploitation.",
                    "label": 0
                },
                {
                    "sent": "While I can say yes.",
                    "label": 0
                },
                {
                    "sent": "I stopped because Katie change.",
                    "label": 0
                }
            ]
        }
    }
}