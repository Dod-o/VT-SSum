{
    "id": "nvy4rslzasjcihet57yu2zkmbhh7fdwm",
    "title": "Semi-supervised Learning for Text Classification",
    "info": {
        "author": [
            "Anastasia Krithara, Xerox Research Centre Europe, Xerox"
        ],
        "published": "Nov. 9, 2007",
        "recorded": "August 2007",
        "category": [
            "Top->Computer Science->Text Mining",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/mlss07_krithara_ssl/",
    "segmentation": [
        [
            "You're going to tell you about Semi supervised learning for document classification.",
            "So go ahead.",
            "Hello, my name is Anastasia.",
            "You don't need that Xerox which has center Europe in Grenoble, France.",
            "Where I'm working under the supervision of zombie seller Ender.",
            "And collaborating with the University.",
            "Mercury in Paris, where I'm working with materials Armenian Patrick Glenorie.",
            "As the title implies, we're interesting in semi supervised, supervised learning and especially for the text classification task.",
            "Let's start by giving the."
        ],
        [
            "OK, doesn't.",
            "Let's start by giving the motivation of this work.",
            "In general, the classification algorithms the idea is to start with the training set where we have the documents or examples we use.",
            "It's not for document classification and their labels.",
            "We use them to classify to train our model and then estimate the decision function, which is our classifier.",
            "For example, in probability model would be a probability of upper level given the example.",
            "And there are very many very nice supervised methods in the literature that seems to work well.",
            "But one of the drawbacks of.",
            "Of supervised learning is the annotation cost.",
            "And in order to have our training set, we need time to annotate Daniel our results, our examples and when times is costly, because we need experts.",
            "Forgot my log file data or some.",
            "And This is why research have been done towards this direction to reduce the annotation course."
        ],
        [
            "And one of the nice ideas."
        ],
        [
            "Having pretending so much provides learning.",
            "Before I."
        ],
        [
            "Give the definition of supervised learning.",
            "That's an outline, so I'll give you the finishing of the semi supervised learning.",
            "Then I'll present to semi supervised variants of the PSA algorithm who have who have done and some some experiments and results in order to evaluate the."
        ],
        [
            "So we found some supervised learning.",
            "If we go back to the definition of supervised supervised learning, we said we have the training set and we use it to train."
        ],
        [
            "Model an in the semi supervised learning.",
            "We have the same goal for estimated decision function North classifier data.",
            "But in addition we have a set of unlabeled examples.",
            "Which in general there much more than the label one.",
            "Um?"
        ],
        [
            "And the idea is that the intuition behind behind some supervised learning is that they can give us an efficient about the distribution of the data.",
            "The probability of X.",
            "And this is what we're using.",
            "We're using some supervised learning an that algorithm will have.",
            "We have a.",
            "We have done a."
        ],
        [
            "Both based on supervised learning, but first we have to represent our data.",
            "We said we have a document collection.",
            "We use widely, used widely used representation, which is a term document matrix.",
            "So we take it document and we count the number of times it it's worth is there.",
            "I just said it too widely used, but one."
        ],
        [
            "The drawbacks that language is it's much more complicated, so many times.",
            "So we might have some problems because the language have many synonyms, so words that they have.",
            "Who won't maybe have the same meaning or where politics where one worker have different meaning.",
            "And this can lead to a disconnection between topics.",
            "For example, if the document has the world April inside, we don't know if it talks about fruits or Max.",
            "Anne and."
        ],
        [
            "The peerless algorithm, first introduced by Domus Hoffman 1999, tries to solve this problem.",
            "A fairly system for probabilistic London semantic analysis.",
            "And the flash to find something behind the words.",
            "Something about the topic.",
            "The documents stop."
        ],
        [
            "How does that it introduced actually Lantern alot of variable the Alpha?",
            "With which is the topic of the document.",
            "So also know what components.",
            "Anne."
        ],
        [
            "And the idea is to use.",
            "To model the data using a mixture model of carbon carbon assumption that the documents and water independent.",
            "And we use the probability of award of award given a topic which actually gives us the profile of the topic.",
            "The words that it stopping contained and also the probability of the components given the document, which is the topics that the document talks about.",
            "And I won't get more technical test because the time is limited.",
            "So we advise to use this mixture model and.",
            "Estimate the log likelihood, then maximize.",
            "Now, as we said we're interested in semi supervised learning, so we have very few training and label examples and the huge amount of Labor and the problems that can be caused from that.",
            "And where?"
        ],
        [
            "Here.",
            "And that is that we we might arrive in a moment with our topic.",
            "Our components have no label examples.",
            "They only got Dana label example.",
            "And there in that case.",
            "We will have some arbitrary probabilities that will be assigned in these topics.",
            "And our classification decision will be also arbitrary.",
            "So in the first sentence provides violent will propose is to introduce an additional label Peg label.",
            "An end.",
            "The idea is to keep all the label examples who give their own label, and they're very.",
            "The unlabeled examples will be assigned the new One South in order to make it more clear.",
            "If we see here the example, it's just for the binary case, but can be extended for multiclass.",
            "Here we have the label example that they keep their real label.",
            "If you have level one Level 2 and over level examples will be assigned a fake level.",
            "Here's just a graphical representation of PSA and also this service provides doesn't change.",
            "So you."
        ],
        [
            "And this fake label.",
            "We will introduce this fake label, the PSA model, so the para meters is this probability that's added to the parameters of the PSA, which the probability of their labels also included the fake labels.",
            "Given the component.",
            "And as you have London variables, we need to use EM algorithm introduced in previous lectures yesterday.",
            "But not as much matter look likelihood as I said, I won't give much details and technical.",
            "So what's the intuition of this model?",
            "Why we need a fake label and what how it can help us?",
            "The idea is that what we want to avoid is to have arbitrary probabilities to unlabeled components.",
            "That means not to give much confidence in this component.",
            "So using the pick label we're gonna do after we have our model, we will distribute the probability we have obtained in the in the label model in the fake labels to the true ones.",
            "And we used along the perimeter.",
            "Which will be really small in order to download the influence of the available components.",
            "So this is the idea of the first image provides variant."
        ],
        [
            "Is the second one because we use a different idea, which is a mislabeling error model.",
            "Here we introduce for its own label example to label.",
            "The one is the perfect label, which actually is the true one, the real one and the other is the VM perfect, which is the estimated one, the one that the algorithm give us.",
            "And using this to labels, we introduced the probability of an estimated label given the real one is actually give us a confidence in the decision of the algorithm.",
            "How confident this about the estimated label he gave?",
            "And our graphical representation changed a bit for the label examples.",
            "It stays the same, but for, in the unlabeled we introduce the estimated label, the new parameter."
        ],
        [
            "So.",
            "The parameters of our model in the parameters we have to add the better parameter we introduce.",
            "And now the log likelihood is the sum of the lucky likelihood for the label examples and for the unlabeled, because the estimate label only appears from the unlabeled examples.",
            "And again, we're using an expectation maximization algorithm in order to maximize our log likelihood and.",
            "And find our model.",
            "Let's see."
        ],
        [
            "Some some experiments we have done in order to."
        ],
        [
            "Evaluate this model so we just three one on data set.",
            "The first 2 weeks from the CMU or White and knowledge base and painting your scription wiped away.",
            "Attending user groups, it's about 10,000 documents, has plenty classes.",
            "And we have actually split over three in training set, which include few label examples and unlabeled examples, and the test set which we use only to estimate the accuracy.",
            "Hopefully actually get score because we use Discord to valuate.",
            "An the web, maybe it has around four 4000 documents for classes.",
            "Actually we use the four most popular classes.",
            "And so again, we split in training and test, and their authors for from their older data set.",
            "We selected the document that they have only one label and which is also the classes that they have more than 100 documents.",
            "So that results to have a around 4000, three 300 documents with seven classes.",
            "As I said, we used the score to valuate with the situation, give us the scores in the precision and the recall.",
            "So let's see some results."
        ],
        [
            "The first one for the pending issues.",
            "So in order to compare we have the two semi supervised blsa algorithms.",
            "The one with the fake level one and the mislabeling, and they fear will compare it with a with a base, but actually to be fair, we use the same supervised version of my base where we also use label label label example.",
            "So if you're going to have their score, and here we have the percentages of label of labeled data in the training set, so we try different different settings from 1% to 95% to see how.",
            "How they?",
            "How how well we can do.",
            "As you can see, the famous supervised LSA with mislabeling error outperformed the other two.",
            "Anne.",
            "I whatever the other one, it's not as good as the Navy, the Navy base.",
            "Here, with just the comparison using the supervised palisade to see how in general Semi supervised learning can help us and we can see a huge difference.",
            "For example if we use only 15% of the label examples of labeling samples and all the other ARM label we got.",
            "See difference of about.",
            "20%.",
            "Of course, as it goes along, the more we have, the less the differences.",
            "And the results are quite similar."
        ],
        [
            "Quite similar for the other two datasets we have the Web B and Reuters.",
            "Also, don't have put here but also their self with supervised LSA are.",
            "Are the same as before.",
            "That means system which provides work better.",
            "And something else to mention here.",
            "I don't know if you see this Alpha and PSA.",
            "What you have.",
            "Do you have the parameter of of?",
            "The Latin variable of components.",
            "The Alpha we said before and this has to be fixed before.",
            "So how much is actually in order to choose?",
            "We did the cross validation and actually the results are 10 fold cross validation results.",
            "In order to choose the number of components sofa Reuters will use components per class between the topics.",
            "For the worker made which is foreign pending his group, it's again too.",
            "And what we notice is that the the supervised learning.",
            "A mislabel mislabeling error.",
            "It's quite stable.",
            "Which way is stable even if we change the number of components, that means when we did the cross validation, even if we choose 234 or whatever, the results are didn't have much different.",
            "And on the other hand, this improvised PSA using Peg label was quite unstable, and when we're changing the number of components, the performance could take.",
            "Significantly."
        ],
        [
            "So let's go."
        ],
        [
            "So the conclusion just summary, so the motivation of this work is we try to reduce the cost index classification task.",
            "And that is to use the less less label examples if possible.",
            "We present it to some supervised variant of the probabilistic London semantic analysis algorithm.",
            "The one used available murder and the other one is mislabeling our model, and then of course we present some results of the this algorithms.",
            "And.",
            "Come down, thank you.",
            "Question.",
            "Yes.",
            "Sorry.",
            "On the evaluation of the performance of the algorithm, you've checked it against naive bias, have to try it another way, another benchmark.",
            "In the binary case they have the results, again presented with transductive SVM OK, and now I'm doing also the.",
            "I don't have them because it's an ongoing work for the multiclass, also pays better than his BMS, yeah?",
            "Not only with service providers.",
            "Thank you.",
            "I was thinking if you are thinking of any extensions on PSA in respect to supervised learning.",
            "I actually and ideas to try to combine.",
            "Also these two methods so.",
            "He eats event writes to try to solve a different aspects.",
            "That means the fact label tried to solve the problem of unlabeled components.",
            "Which label error model try to capture the mislabeling and to take advantage of the probability of the confidence, the confidence that the algorithm have.",
            "So an idea would be to combine these two, and the intention is that we will have a better performance.",
            "And then add something else.",
            "And also we don't have us this.",
            "It will shortly.",
            "What something else I'm working on is that we combine also with active learning.",
            "So we're doing the semi supervised field essay and then on the top of that we do.",
            "We have some active learning algorithms in order to tolerate more than other performance.",
            "OK, I'll just give you the mic.",
            "So just two questions.",
            "So one of 'em is what about graph based methods or semi supervised learning?",
            "So sorry I mean did I miss that?",
            "The other question is what about extensions to LDA Latent Dursley allocation in that it's a very small step from PPS eight.",
            "Yeah, it's possible to be done.",
            "Actually I'm going work.",
            "So then have in mind.",
            "And as you said.",
            "For the graphical, I think there is a work of zoo of harmonic functions that they're doing.",
            "Service provides learning and also the combined with active learning.",
            "So it would be a good idea to compare them off later.",
            "OK, let's think understand you again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You're going to tell you about Semi supervised learning for document classification.",
                    "label": 1
                },
                {
                    "sent": "So go ahead.",
                    "label": 0
                },
                {
                    "sent": "Hello, my name is Anastasia.",
                    "label": 0
                },
                {
                    "sent": "You don't need that Xerox which has center Europe in Grenoble, France.",
                    "label": 0
                },
                {
                    "sent": "Where I'm working under the supervision of zombie seller Ender.",
                    "label": 0
                },
                {
                    "sent": "And collaborating with the University.",
                    "label": 0
                },
                {
                    "sent": "Mercury in Paris, where I'm working with materials Armenian Patrick Glenorie.",
                    "label": 0
                },
                {
                    "sent": "As the title implies, we're interesting in semi supervised, supervised learning and especially for the text classification task.",
                    "label": 0
                },
                {
                    "sent": "Let's start by giving the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, doesn't.",
                    "label": 0
                },
                {
                    "sent": "Let's start by giving the motivation of this work.",
                    "label": 0
                },
                {
                    "sent": "In general, the classification algorithms the idea is to start with the training set where we have the documents or examples we use.",
                    "label": 0
                },
                {
                    "sent": "It's not for document classification and their labels.",
                    "label": 0
                },
                {
                    "sent": "We use them to classify to train our model and then estimate the decision function, which is our classifier.",
                    "label": 0
                },
                {
                    "sent": "For example, in probability model would be a probability of upper level given the example.",
                    "label": 0
                },
                {
                    "sent": "And there are very many very nice supervised methods in the literature that seems to work well.",
                    "label": 0
                },
                {
                    "sent": "But one of the drawbacks of.",
                    "label": 0
                },
                {
                    "sent": "Of supervised learning is the annotation cost.",
                    "label": 0
                },
                {
                    "sent": "And in order to have our training set, we need time to annotate Daniel our results, our examples and when times is costly, because we need experts.",
                    "label": 0
                },
                {
                    "sent": "Forgot my log file data or some.",
                    "label": 0
                },
                {
                    "sent": "And This is why research have been done towards this direction to reduce the annotation course.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And one of the nice ideas.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Having pretending so much provides learning.",
                    "label": 0
                },
                {
                    "sent": "Before I.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give the definition of supervised learning.",
                    "label": 0
                },
                {
                    "sent": "That's an outline, so I'll give you the finishing of the semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Then I'll present to semi supervised variants of the PSA algorithm who have who have done and some some experiments and results in order to evaluate the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we found some supervised learning.",
                    "label": 0
                },
                {
                    "sent": "If we go back to the definition of supervised supervised learning, we said we have the training set and we use it to train.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model an in the semi supervised learning.",
                    "label": 1
                },
                {
                    "sent": "We have the same goal for estimated decision function North classifier data.",
                    "label": 1
                },
                {
                    "sent": "But in addition we have a set of unlabeled examples.",
                    "label": 1
                },
                {
                    "sent": "Which in general there much more than the label one.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the idea is that the intuition behind behind some supervised learning is that they can give us an efficient about the distribution of the data.",
                    "label": 1
                },
                {
                    "sent": "The probability of X.",
                    "label": 0
                },
                {
                    "sent": "And this is what we're using.",
                    "label": 1
                },
                {
                    "sent": "We're using some supervised learning an that algorithm will have.",
                    "label": 0
                },
                {
                    "sent": "We have a.",
                    "label": 0
                },
                {
                    "sent": "We have done a.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Both based on supervised learning, but first we have to represent our data.",
                    "label": 1
                },
                {
                    "sent": "We said we have a document collection.",
                    "label": 1
                },
                {
                    "sent": "We use widely, used widely used representation, which is a term document matrix.",
                    "label": 0
                },
                {
                    "sent": "So we take it document and we count the number of times it it's worth is there.",
                    "label": 0
                },
                {
                    "sent": "I just said it too widely used, but one.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The drawbacks that language is it's much more complicated, so many times.",
                    "label": 0
                },
                {
                    "sent": "So we might have some problems because the language have many synonyms, so words that they have.",
                    "label": 0
                },
                {
                    "sent": "Who won't maybe have the same meaning or where politics where one worker have different meaning.",
                    "label": 1
                },
                {
                    "sent": "And this can lead to a disconnection between topics.",
                    "label": 0
                },
                {
                    "sent": "For example, if the document has the world April inside, we don't know if it talks about fruits or Max.",
                    "label": 0
                },
                {
                    "sent": "Anne and.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The peerless algorithm, first introduced by Domus Hoffman 1999, tries to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "A fairly system for probabilistic London semantic analysis.",
                    "label": 1
                },
                {
                    "sent": "And the flash to find something behind the words.",
                    "label": 1
                },
                {
                    "sent": "Something about the topic.",
                    "label": 0
                },
                {
                    "sent": "The documents stop.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How does that it introduced actually Lantern alot of variable the Alpha?",
                    "label": 0
                },
                {
                    "sent": "With which is the topic of the document.",
                    "label": 0
                },
                {
                    "sent": "So also know what components.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the idea is to use.",
                    "label": 0
                },
                {
                    "sent": "To model the data using a mixture model of carbon carbon assumption that the documents and water independent.",
                    "label": 1
                },
                {
                    "sent": "And we use the probability of award of award given a topic which actually gives us the profile of the topic.",
                    "label": 0
                },
                {
                    "sent": "The words that it stopping contained and also the probability of the components given the document, which is the topics that the document talks about.",
                    "label": 0
                },
                {
                    "sent": "And I won't get more technical test because the time is limited.",
                    "label": 0
                },
                {
                    "sent": "So we advise to use this mixture model and.",
                    "label": 0
                },
                {
                    "sent": "Estimate the log likelihood, then maximize.",
                    "label": 0
                },
                {
                    "sent": "Now, as we said we're interested in semi supervised learning, so we have very few training and label examples and the huge amount of Labor and the problems that can be caused from that.",
                    "label": 0
                },
                {
                    "sent": "And where?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "And that is that we we might arrive in a moment with our topic.",
                    "label": 0
                },
                {
                    "sent": "Our components have no label examples.",
                    "label": 0
                },
                {
                    "sent": "They only got Dana label example.",
                    "label": 0
                },
                {
                    "sent": "And there in that case.",
                    "label": 0
                },
                {
                    "sent": "We will have some arbitrary probabilities that will be assigned in these topics.",
                    "label": 1
                },
                {
                    "sent": "And our classification decision will be also arbitrary.",
                    "label": 0
                },
                {
                    "sent": "So in the first sentence provides violent will propose is to introduce an additional label Peg label.",
                    "label": 1
                },
                {
                    "sent": "An end.",
                    "label": 1
                },
                {
                    "sent": "The idea is to keep all the label examples who give their own label, and they're very.",
                    "label": 1
                },
                {
                    "sent": "The unlabeled examples will be assigned the new One South in order to make it more clear.",
                    "label": 1
                },
                {
                    "sent": "If we see here the example, it's just for the binary case, but can be extended for multiclass.",
                    "label": 0
                },
                {
                    "sent": "Here we have the label example that they keep their real label.",
                    "label": 0
                },
                {
                    "sent": "If you have level one Level 2 and over level examples will be assigned a fake level.",
                    "label": 0
                },
                {
                    "sent": "Here's just a graphical representation of PSA and also this service provides doesn't change.",
                    "label": 0
                },
                {
                    "sent": "So you.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this fake label.",
                    "label": 0
                },
                {
                    "sent": "We will introduce this fake label, the PSA model, so the para meters is this probability that's added to the parameters of the PSA, which the probability of their labels also included the fake labels.",
                    "label": 0
                },
                {
                    "sent": "Given the component.",
                    "label": 0
                },
                {
                    "sent": "And as you have London variables, we need to use EM algorithm introduced in previous lectures yesterday.",
                    "label": 0
                },
                {
                    "sent": "But not as much matter look likelihood as I said, I won't give much details and technical.",
                    "label": 0
                },
                {
                    "sent": "So what's the intuition of this model?",
                    "label": 0
                },
                {
                    "sent": "Why we need a fake label and what how it can help us?",
                    "label": 1
                },
                {
                    "sent": "The idea is that what we want to avoid is to have arbitrary probabilities to unlabeled components.",
                    "label": 0
                },
                {
                    "sent": "That means not to give much confidence in this component.",
                    "label": 0
                },
                {
                    "sent": "So using the pick label we're gonna do after we have our model, we will distribute the probability we have obtained in the in the label model in the fake labels to the true ones.",
                    "label": 1
                },
                {
                    "sent": "And we used along the perimeter.",
                    "label": 0
                },
                {
                    "sent": "Which will be really small in order to download the influence of the available components.",
                    "label": 0
                },
                {
                    "sent": "So this is the idea of the first image provides variant.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is the second one because we use a different idea, which is a mislabeling error model.",
                    "label": 1
                },
                {
                    "sent": "Here we introduce for its own label example to label.",
                    "label": 0
                },
                {
                    "sent": "The one is the perfect label, which actually is the true one, the real one and the other is the VM perfect, which is the estimated one, the one that the algorithm give us.",
                    "label": 1
                },
                {
                    "sent": "And using this to labels, we introduced the probability of an estimated label given the real one is actually give us a confidence in the decision of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "How confident this about the estimated label he gave?",
                    "label": 0
                },
                {
                    "sent": "And our graphical representation changed a bit for the label examples.",
                    "label": 0
                },
                {
                    "sent": "It stays the same, but for, in the unlabeled we introduce the estimated label, the new parameter.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The parameters of our model in the parameters we have to add the better parameter we introduce.",
                    "label": 0
                },
                {
                    "sent": "And now the log likelihood is the sum of the lucky likelihood for the label examples and for the unlabeled, because the estimate label only appears from the unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "And again, we're using an expectation maximization algorithm in order to maximize our log likelihood and.",
                    "label": 0
                },
                {
                    "sent": "And find our model.",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some some experiments we have done in order to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Evaluate this model so we just three one on data set.",
                    "label": 0
                },
                {
                    "sent": "The first 2 weeks from the CMU or White and knowledge base and painting your scription wiped away.",
                    "label": 0
                },
                {
                    "sent": "Attending user groups, it's about 10,000 documents, has plenty classes.",
                    "label": 0
                },
                {
                    "sent": "And we have actually split over three in training set, which include few label examples and unlabeled examples, and the test set which we use only to estimate the accuracy.",
                    "label": 0
                },
                {
                    "sent": "Hopefully actually get score because we use Discord to valuate.",
                    "label": 0
                },
                {
                    "sent": "An the web, maybe it has around four 4000 documents for classes.",
                    "label": 0
                },
                {
                    "sent": "Actually we use the four most popular classes.",
                    "label": 0
                },
                {
                    "sent": "And so again, we split in training and test, and their authors for from their older data set.",
                    "label": 0
                },
                {
                    "sent": "We selected the document that they have only one label and which is also the classes that they have more than 100 documents.",
                    "label": 0
                },
                {
                    "sent": "So that results to have a around 4000, three 300 documents with seven classes.",
                    "label": 0
                },
                {
                    "sent": "As I said, we used the score to valuate with the situation, give us the scores in the precision and the recall.",
                    "label": 0
                },
                {
                    "sent": "So let's see some results.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first one for the pending issues.",
                    "label": 0
                },
                {
                    "sent": "So in order to compare we have the two semi supervised blsa algorithms.",
                    "label": 0
                },
                {
                    "sent": "The one with the fake level one and the mislabeling, and they fear will compare it with a with a base, but actually to be fair, we use the same supervised version of my base where we also use label label label example.",
                    "label": 0
                },
                {
                    "sent": "So if you're going to have their score, and here we have the percentages of label of labeled data in the training set, so we try different different settings from 1% to 95% to see how.",
                    "label": 0
                },
                {
                    "sent": "How they?",
                    "label": 0
                },
                {
                    "sent": "How how well we can do.",
                    "label": 0
                },
                {
                    "sent": "As you can see, the famous supervised LSA with mislabeling error outperformed the other two.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I whatever the other one, it's not as good as the Navy, the Navy base.",
                    "label": 0
                },
                {
                    "sent": "Here, with just the comparison using the supervised palisade to see how in general Semi supervised learning can help us and we can see a huge difference.",
                    "label": 0
                },
                {
                    "sent": "For example if we use only 15% of the label examples of labeling samples and all the other ARM label we got.",
                    "label": 0
                },
                {
                    "sent": "See difference of about.",
                    "label": 0
                },
                {
                    "sent": "20%.",
                    "label": 0
                },
                {
                    "sent": "Of course, as it goes along, the more we have, the less the differences.",
                    "label": 0
                },
                {
                    "sent": "And the results are quite similar.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quite similar for the other two datasets we have the Web B and Reuters.",
                    "label": 0
                },
                {
                    "sent": "Also, don't have put here but also their self with supervised LSA are.",
                    "label": 0
                },
                {
                    "sent": "Are the same as before.",
                    "label": 0
                },
                {
                    "sent": "That means system which provides work better.",
                    "label": 0
                },
                {
                    "sent": "And something else to mention here.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you see this Alpha and PSA.",
                    "label": 0
                },
                {
                    "sent": "What you have.",
                    "label": 0
                },
                {
                    "sent": "Do you have the parameter of of?",
                    "label": 0
                },
                {
                    "sent": "The Latin variable of components.",
                    "label": 0
                },
                {
                    "sent": "The Alpha we said before and this has to be fixed before.",
                    "label": 0
                },
                {
                    "sent": "So how much is actually in order to choose?",
                    "label": 0
                },
                {
                    "sent": "We did the cross validation and actually the results are 10 fold cross validation results.",
                    "label": 0
                },
                {
                    "sent": "In order to choose the number of components sofa Reuters will use components per class between the topics.",
                    "label": 0
                },
                {
                    "sent": "For the worker made which is foreign pending his group, it's again too.",
                    "label": 0
                },
                {
                    "sent": "And what we notice is that the the supervised learning.",
                    "label": 0
                },
                {
                    "sent": "A mislabel mislabeling error.",
                    "label": 0
                },
                {
                    "sent": "It's quite stable.",
                    "label": 0
                },
                {
                    "sent": "Which way is stable even if we change the number of components, that means when we did the cross validation, even if we choose 234 or whatever, the results are didn't have much different.",
                    "label": 0
                },
                {
                    "sent": "And on the other hand, this improvised PSA using Peg label was quite unstable, and when we're changing the number of components, the performance could take.",
                    "label": 0
                },
                {
                    "sent": "Significantly.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's go.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the conclusion just summary, so the motivation of this work is we try to reduce the cost index classification task.",
                    "label": 0
                },
                {
                    "sent": "And that is to use the less less label examples if possible.",
                    "label": 0
                },
                {
                    "sent": "We present it to some supervised variant of the probabilistic London semantic analysis algorithm.",
                    "label": 0
                },
                {
                    "sent": "The one used available murder and the other one is mislabeling our model, and then of course we present some results of the this algorithms.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Come down, thank you.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "On the evaluation of the performance of the algorithm, you've checked it against naive bias, have to try it another way, another benchmark.",
                    "label": 1
                },
                {
                    "sent": "In the binary case they have the results, again presented with transductive SVM OK, and now I'm doing also the.",
                    "label": 0
                },
                {
                    "sent": "I don't have them because it's an ongoing work for the multiclass, also pays better than his BMS, yeah?",
                    "label": 0
                },
                {
                    "sent": "Not only with service providers.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "I was thinking if you are thinking of any extensions on PSA in respect to supervised learning.",
                    "label": 0
                },
                {
                    "sent": "I actually and ideas to try to combine.",
                    "label": 0
                },
                {
                    "sent": "Also these two methods so.",
                    "label": 0
                },
                {
                    "sent": "He eats event writes to try to solve a different aspects.",
                    "label": 0
                },
                {
                    "sent": "That means the fact label tried to solve the problem of unlabeled components.",
                    "label": 0
                },
                {
                    "sent": "Which label error model try to capture the mislabeling and to take advantage of the probability of the confidence, the confidence that the algorithm have.",
                    "label": 0
                },
                {
                    "sent": "So an idea would be to combine these two, and the intention is that we will have a better performance.",
                    "label": 0
                },
                {
                    "sent": "And then add something else.",
                    "label": 0
                },
                {
                    "sent": "And also we don't have us this.",
                    "label": 0
                },
                {
                    "sent": "It will shortly.",
                    "label": 0
                },
                {
                    "sent": "What something else I'm working on is that we combine also with active learning.",
                    "label": 0
                },
                {
                    "sent": "So we're doing the semi supervised field essay and then on the top of that we do.",
                    "label": 0
                },
                {
                    "sent": "We have some active learning algorithms in order to tolerate more than other performance.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll just give you the mic.",
                    "label": 0
                },
                {
                    "sent": "So just two questions.",
                    "label": 0
                },
                {
                    "sent": "So one of 'em is what about graph based methods or semi supervised learning?",
                    "label": 0
                },
                {
                    "sent": "So sorry I mean did I miss that?",
                    "label": 0
                },
                {
                    "sent": "The other question is what about extensions to LDA Latent Dursley allocation in that it's a very small step from PPS eight.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's possible to be done.",
                    "label": 0
                },
                {
                    "sent": "Actually I'm going work.",
                    "label": 0
                },
                {
                    "sent": "So then have in mind.",
                    "label": 0
                },
                {
                    "sent": "And as you said.",
                    "label": 1
                },
                {
                    "sent": "For the graphical, I think there is a work of zoo of harmonic functions that they're doing.",
                    "label": 0
                },
                {
                    "sent": "Service provides learning and also the combined with active learning.",
                    "label": 0
                },
                {
                    "sent": "So it would be a good idea to compare them off later.",
                    "label": 0
                },
                {
                    "sent": "OK, let's think understand you again.",
                    "label": 0
                }
            ]
        }
    }
}