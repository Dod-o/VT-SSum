{
    "id": "vscft6x3alj6sbuibgrvd56ec7gwsjqb",
    "title": "Interest Seam Image",
    "info": {
        "author": [
            "Gang Hua, Nokia Research Center"
        ],
        "published": "July 19, 2010",
        "recorded": "June 2010",
        "category": [
            "Top->Computer Science->Computer Vision->Image & Video Retrieval"
        ]
    },
    "url": "http://videolectures.net/cvpr2010_hua_isi/",
    "segmentation": [
        [
            "Good afternoon everyone.",
            "Today I will talk about the interest.",
            "Same image, efficient visual representation for larger scale video matching task.",
            "This is joint work with Shell John from Qinghai University and also my former colleague late John and Henry Schein from Microsoft.",
            "So Shell is my intent last summer who spends."
        ],
        [
            "Amounts of solid work on this research.",
            "If we take a close look of the massive number of videos on the Internet, it is not uncommon for us to find different versions of the same video.",
            "So our research goal here is to extract the video signature for each video to to some reservation content such that under certain similarity measures those which those signatures extracted from those earlier duplicate videos, we called a different version of videos from the same video source to be near duplicate images near duplicate videos.",
            "So our goal is to have this visual signature to be close to one another for those.",
            "Duplicate videos under certain similarity measures well for those along near deeply videos we want want their signature to be far away from."
        ],
        [
            "Each other.",
            "So.",
            "There are several technical goals we want to achieve.",
            "Their first, of course, their visual signature should be discriminated that that is, say, if two videos are different, we we want them to be differentiated and also the visual signature should be invariant.",
            "That means that it must be robust to certain video editing or video format transform forming.",
            "Sort of to be robust.",
            "Visual variations.",
            "Since our goal is to deal with the massive billions of online videos, scalability is something we have to take into consideration, so this, which we should signature, must be both."
        ],
        [
            "Computationally efficient and memory memory efficient.",
            "So to effectively approach to this course, there's two questions questions we need to answer here.",
            "So first, since video contains a lot of video frames, we just cannot use them more.",
            "So the first question we want to address is how to digest the important visual content across the video.",
            "The second question is then, how could we generate a compact visual signature from this digested with?"
        ],
        [
            "Content.",
            "To answer the first question, our proposal is basically to come out with the interest.",
            "Same image basically from this video.",
            "We first build build a spatial temporal energy map which where each energy pixel represents how important that specific video picks up pixel it is.",
            "Then from this spatial temporal energy map we identify SIM, which we call the which is connected parts of pixels.",
            "We're quite the interesting because it will encompass the most important video pixels there.",
            "From this we interest.",
            "Same way we will come up with sort of captured service.",
            "There we extract 1 theme of pixels from each video frame and put them column by column into a new image.",
            "And finally we come up with our image based.",
            "Basically summarize that which account of this video which we call interest the same image.",
            "I won't get into details of how we compute this special temporary energy map.",
            "You could refer to our paper for this."
        ],
        [
            "Yes, more formally, in case you are loaded.",
            "In case you're not familiar with the concept of same essentially as same as is defended defined as connected parts of pixels vertically across the image, and as you can see, each connected path will only intersect with one pixel rose by just one pixel there.",
            "As we can see the red curve here is loud as seen.",
            "So what is the interesting thing?",
            "If we define the energy of the same as the?",
            "Summation of order energy peaks energies.",
            "Each pixel heads along this same.",
            "Then we define the energy we define our interest same as the.",
            "As the same over all possible choices like because across the image you could have a lot of things that we define interest same as the same with the highest energy there.",
            "Of course following Avidan Shamir we can we can sort of solve this interest.",
            "Same using efficient dynamic programming.",
            "There I won't get into details to that cause it is really quite straightforward to to derive FC and you may refer to to our papers also for that."
        ],
        [
            "So let's just make a comparison of the interest.",
            "Same image of what people have used before for large scale video indexing tasks.",
            "So in in 2002 knock it out.",
            "They proposed representation called Slice image.",
            "Essentially, this summarizer video by cutting off a line of pixels in the center of this video frames and decoyed slice images.",
            "If you if you compare this.",
            "Two images are generated from the same video source intrinsically.",
            "If we compare this tool images here slice image and interest same image, you find that actually that this slice image are missing some important information, because as you can see in this interest the same image, you can still say the doctor.",
            "But this last image essentially lose those kind of information, maybe which may be essential to summarize the content of this video and also here you can see the.",
            "One thing here has three peaks, as in our interests image here.",
            "Although it is distorted a little bit, but you can still see three big sister vessels here in the slice image it is all gone.",
            "And another."
        ],
        [
            "Popular video summarization scheme is to to represent each video clip as the key frame.",
            "As we can see here we we extracted each row is is extracted from one image and those two from 2 videos from Tulare duplicate videos.",
            "As you can see the keyframes because because of there could be a lot of video editing there and which also makes the short boundary to be a little bit unstable there.",
            "As you can see that the keyframe based representation.",
            "Basically for this two year duplicated videos, they extract totally different K for himself versus if you look into the interest the same image representation, visually they are still quite similar to each other.",
            "That means when you're trying to match those two videos using this representation, it could be much easier because you you have overall summarization there.",
            "So once we have the."
        ],
        [
            "Interest, same image representation.",
            "How do we generate our video?",
            "See glacier.",
            "So just in summary we have.",
            "We have an interest.",
            "Same image here.",
            "Now what we perform these actually quite standard approach.",
            "There we first use.",
            "We detect the MSR region out of this interest.",
            "Same image.",
            "Then for each MCR region we extract further extract the SIFT descriptors out of each region.",
            "Then we end up with a bag of feature representation there.",
            "After running through our vocabulary tree to quantize each descriptors, we will come out of a bag of visual words representation.",
            "Then we run mean hush.",
            "Algorithm two to extract nearly like 60 min hash signature, so I won't.",
            "Again I won't get into details of all this.",
            "What I want to talk about is once we have this compact visual signature, how we could do the indexing more efficiently.",
            "Of course with, because essentially a set of."
        ],
        [
            "Cash value is just a set of integers for efficient video indexing.",
            "Usually large scale indexing.",
            "We could build that inverted file system, essentially each for each video we just foreach foreach words in the visual vocabulary will build a list to index, which we do actually contain this racial words in retrieval time.",
            "Of course you can do a forward look up that's and vote for each video, then the video with the highest vote will be your match.",
            "That essentially means if you have this database you want to.",
            "Find the YouTube, click it once on this query video here.",
            "But this is this process really efficient use of course.",
            "I mean is a common knowledge that if if this inverted list is sparse it you could, you could run, retrieve the relevant videos in a really efficient fashion, but based on the visual based on this kind of bag of visual words representation whatsoever, if you have enormous amount of videos, usually this inverted list is indeed dense, so that complexity is still.",
            "Om if on average each list contains like M videos there, so could we do better?",
            "The answer is yes with this."
        ],
        [
            "Does the same image representation, just that too too just a reminder this this interest same image.",
            "Actually the horizontal access still maintains the temporal information there.",
            "So suppose you have feature detected in this position.",
            "So if you measured the time length across the left boundary there, you could you could position it temporarily.",
            "So if we have the same like feature detected in two different locations in the.",
            "Second video, of course we would say that we would we would allow these two features to match versus the yellow and the red features will we?",
            "We cannot just cannot match them cause the temporal location is far away from each other.",
            "So with this we actually."
        ],
        [
            "We could redefine our similarity measure just just sort of to recall the similarity between 2:00 set of min hash values is just to see what's the percentage of mean hash values they overlap.",
            "Here, where we indeed could introduce that using the temporal context, essentially we count those count the number of overlaps of being hash values within a certain time period.",
            "If the those key those two kids are equal to each other, but they are out of the scope of time spend, we don't allow them to match with this.",
            "I have another one, this is."
        ],
        [
            "Should be the last slide.",
            "Sorry, so with this temporal context.",
            "Indeed, if we look into a specific inverted list there, we could use that temporal position indeed to sort all these videos.",
            "Then in retrieval time.",
            "We could if we want we firstly we will quickly identify time period there that largely reduced our search complexity because we can use something like binary search so that the complexity is essentially all log.",
            "So consider you have you perform this forward.",
            "Look at look up a lot of times in your retrieval process.",
            "This could save a significant amount of retrieval time that's showing in our experiments, which I shared talk.",
            "Very soon.",
            "So any?"
        ],
        [
            "Questions so far here.",
            "OK.",
            "So.",
            "Another important properties of this interest seems representation is.",
            "You can see they they they have very strong global characteristics.",
            "So another thing we we sort of proposes to extract the gist feature which is first proposed by only one to Robin in 2001.",
            "Essentially you could you could simplify as extracting safety feature for the whole image there.",
            "So essentially one larger 128 dimensional.",
            "Features that we in our CS, uh retrieval experiments.",
            "We use this gist features for four post verification there because because that simply is motivated, motivated by the strong global characteristics of this interests me."
        ],
        [
            "They just are.",
            "So.",
            "During post verification stage, we simply rank whatever we retrieved from the database by by sort of combined similarity of the minhash signatures as well as this global descriptor step.",
            "So I will talk about some results on that soon in the experiments."
        ],
        [
            "So.",
            "For evaluation we have connected very large.",
            "We do database essentially we crowd like 10,000 and 382 videos from the web they encompass.",
            "Quite wide variety of different types of videos, including TV shows, drama, MTV, movie trailers and etc.",
            "Out of this video datasets we build one set of unlabelled video clips because we will perform shot boundary detection to divide each videos each video into a set of video clips.",
            "Because it only makes sense to extract in the interest same image from one shot because across short boundary the meaning is not that clear to us.",
            "So out of this we build an unlabeled data set which contains like.",
            "Ha 246 more than 246 video clips that they are used as the noise database.",
            "Basically we use we mix our label data with it during retrieval tasks.",
            "So for experiment, quantitative experimental evaluation, we build two sets of labeled video clips.",
            "The first is actually quite cute is a controlled datasets.",
            "We have 150 videos each underground like a line, different controlled transformations including color lighting.",
            "SunTrust change and so on and so forth.",
            "The second data set is a real leader.",
            "Duplicate video datasets which contains nearly 800 videos.",
            "So for experiments.",
            "We take a live one hour straight."
        ],
        [
            "For each query video from either the QR or the cutie datasets, the rest of the QR QDR mixed with the unlabeled datasets.",
            "For then we perform retrieval tasks so.",
            "So for quantitative evaluation we use the mean average precision.",
            "So in case you're not familiar with average precision, essentially you take when you perform a retrieval you.",
            "Are you averaged the accuracy across all different kinds of record levels?",
            "So here give it give you example for query key.",
            "Suppose there are three relevant results and you rank them at at position 1, four and seven.",
            "Here is the equation how you sort of coming out of the average precision there.",
            "So for every for all the queries you will perform the you average.",
            "This average precision there we quite mean average precision, so I will skip the our experiments.",
            "Results on Q are essentially have.",
            "Uncut here because we have performed a lot of experiments, I want to be able to."
        ],
        [
            "Discuss all of them.",
            "So the first set of experimental results I want to mention here is is on the database.",
            "Q Are essentially we have compared with all the previous state of the art video retrieval system, including the.",
            "What is your reference charm it all in CCIVR 2007 and also this would Italians SM Multimedia 2009 so.",
            "So if we look into this table, the first row listed retrieval accuracy of our first full system which including using the interest same image representation plus being hash signature and plus seeing verification.",
            "As you can see across all the results, this full system achieves the highest accuracy there.",
            "Versus if we will also do something if we discard the scene verification scheme.",
            "Still it outperforms order previous results there.",
            "This shows the advantages of using the interest same image representation because some of the difference from the from the previous approaches that they use the keyframe based representation was slice image based representation.",
            "Here the last work they actually use the both local feature and global feature.",
            "As you can see.",
            "But about it, they don't use our interest.",
            "Same image representation as we can see.",
            "That's a direct comparison like using interest.",
            "Same images still improve the results quite a lot.",
            "There is from the experiments, so some other experiments were performed."
        ],
        [
            "Is to really measure the effects of the number of being harsh signatures you use.",
            "As you can see, in general if you increase the number of minhash functions you use, the retrieval accuracy will increase.",
            "Interesting observation we have is that if we have this same verification scheme posted at that.",
            "The highest accuracy we achieve is actually achieved at that point, which we actually reduce the memory usage.",
            "We believe that that is big, cause the.",
            "A global descriptor, the gist descriptor and the local local feature based representation.",
            "There are complementary to each other, so that essentially means when you're in the first step of retrieval, you're using local feature based representation.",
            "You can you can sort of focusing on just increasing the recall and then the global features will will help you to boost the Acura."
        ],
        [
            "Just just the final summary of the system performance.",
            "As you can see our full system for each video.",
            "We only use like 480 bytes versus if we use mean harsh plus this temporal context, it is absolutely better than the baseline, but it consumes much more memory and if you compare that retrieval time with this temporal context, because we can do this kind of binary search scenario, the retrieval time is significantly improved.",
            "Almost like a 20."
        ],
        [
            "Times speedup there, sure yeah.",
            "So just in conclusion, we have come out with this very efficient visual representation here and some future work.",
            "I just want to motivate the field in this.",
            "We have talked about we have a lot of work on interest point extraction and also interested region.",
            "Maybe the next step is focusing on lines and edges, thanks.",
            "I think this time for maybe 1 quick question and I'll request people to use the microphone.",
            "I have a question.",
            "What kind of invariances does your representation afford?",
            "Yes, we did.",
            "We did have a lot of performance, a lot of evaluation by under controlled datasets we have because we since synthetically transformed video like by adjusting the contrast, adjust lighting conditions and it seems like there's some results is presented in our paper.",
            "We found that in general our interest, same extraction, is quite robust too.",
            "Different kind of video editing and visual variations, but is part of our future work to do more sort of objective evaluation there.",
            "But thank you again, yeah, thanks."
        ],
        [
            "I see a lot of them."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good afternoon everyone.",
                    "label": 0
                },
                {
                    "sent": "Today I will talk about the interest.",
                    "label": 0
                },
                {
                    "sent": "Same image, efficient visual representation for larger scale video matching task.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Shell John from Qinghai University and also my former colleague late John and Henry Schein from Microsoft.",
                    "label": 0
                },
                {
                    "sent": "So Shell is my intent last summer who spends.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Amounts of solid work on this research.",
                    "label": 0
                },
                {
                    "sent": "If we take a close look of the massive number of videos on the Internet, it is not uncommon for us to find different versions of the same video.",
                    "label": 0
                },
                {
                    "sent": "So our research goal here is to extract the video signature for each video to to some reservation content such that under certain similarity measures those which those signatures extracted from those earlier duplicate videos, we called a different version of videos from the same video source to be near duplicate images near duplicate videos.",
                    "label": 0
                },
                {
                    "sent": "So our goal is to have this visual signature to be close to one another for those.",
                    "label": 0
                },
                {
                    "sent": "Duplicate videos under certain similarity measures well for those along near deeply videos we want want their signature to be far away from.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Each other.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There are several technical goals we want to achieve.",
                    "label": 0
                },
                {
                    "sent": "Their first, of course, their visual signature should be discriminated that that is, say, if two videos are different, we we want them to be differentiated and also the visual signature should be invariant.",
                    "label": 0
                },
                {
                    "sent": "That means that it must be robust to certain video editing or video format transform forming.",
                    "label": 0
                },
                {
                    "sent": "Sort of to be robust.",
                    "label": 0
                },
                {
                    "sent": "Visual variations.",
                    "label": 0
                },
                {
                    "sent": "Since our goal is to deal with the massive billions of online videos, scalability is something we have to take into consideration, so this, which we should signature, must be both.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Computationally efficient and memory memory efficient.",
                    "label": 0
                },
                {
                    "sent": "So to effectively approach to this course, there's two questions questions we need to answer here.",
                    "label": 0
                },
                {
                    "sent": "So first, since video contains a lot of video frames, we just cannot use them more.",
                    "label": 0
                },
                {
                    "sent": "So the first question we want to address is how to digest the important visual content across the video.",
                    "label": 1
                },
                {
                    "sent": "The second question is then, how could we generate a compact visual signature from this digested with?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Content.",
                    "label": 0
                },
                {
                    "sent": "To answer the first question, our proposal is basically to come out with the interest.",
                    "label": 0
                },
                {
                    "sent": "Same image basically from this video.",
                    "label": 0
                },
                {
                    "sent": "We first build build a spatial temporal energy map which where each energy pixel represents how important that specific video picks up pixel it is.",
                    "label": 0
                },
                {
                    "sent": "Then from this spatial temporal energy map we identify SIM, which we call the which is connected parts of pixels.",
                    "label": 0
                },
                {
                    "sent": "We're quite the interesting because it will encompass the most important video pixels there.",
                    "label": 0
                },
                {
                    "sent": "From this we interest.",
                    "label": 0
                },
                {
                    "sent": "Same way we will come up with sort of captured service.",
                    "label": 0
                },
                {
                    "sent": "There we extract 1 theme of pixels from each video frame and put them column by column into a new image.",
                    "label": 0
                },
                {
                    "sent": "And finally we come up with our image based.",
                    "label": 0
                },
                {
                    "sent": "Basically summarize that which account of this video which we call interest the same image.",
                    "label": 0
                },
                {
                    "sent": "I won't get into details of how we compute this special temporary energy map.",
                    "label": 0
                },
                {
                    "sent": "You could refer to our paper for this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, more formally, in case you are loaded.",
                    "label": 0
                },
                {
                    "sent": "In case you're not familiar with the concept of same essentially as same as is defended defined as connected parts of pixels vertically across the image, and as you can see, each connected path will only intersect with one pixel rose by just one pixel there.",
                    "label": 0
                },
                {
                    "sent": "As we can see the red curve here is loud as seen.",
                    "label": 0
                },
                {
                    "sent": "So what is the interesting thing?",
                    "label": 0
                },
                {
                    "sent": "If we define the energy of the same as the?",
                    "label": 0
                },
                {
                    "sent": "Summation of order energy peaks energies.",
                    "label": 0
                },
                {
                    "sent": "Each pixel heads along this same.",
                    "label": 0
                },
                {
                    "sent": "Then we define the energy we define our interest same as the.",
                    "label": 0
                },
                {
                    "sent": "As the same over all possible choices like because across the image you could have a lot of things that we define interest same as the same with the highest energy there.",
                    "label": 0
                },
                {
                    "sent": "Of course following Avidan Shamir we can we can sort of solve this interest.",
                    "label": 0
                },
                {
                    "sent": "Same using efficient dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "There I won't get into details to that cause it is really quite straightforward to to derive FC and you may refer to to our papers also for that.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's just make a comparison of the interest.",
                    "label": 0
                },
                {
                    "sent": "Same image of what people have used before for large scale video indexing tasks.",
                    "label": 0
                },
                {
                    "sent": "So in in 2002 knock it out.",
                    "label": 0
                },
                {
                    "sent": "They proposed representation called Slice image.",
                    "label": 0
                },
                {
                    "sent": "Essentially, this summarizer video by cutting off a line of pixels in the center of this video frames and decoyed slice images.",
                    "label": 0
                },
                {
                    "sent": "If you if you compare this.",
                    "label": 0
                },
                {
                    "sent": "Two images are generated from the same video source intrinsically.",
                    "label": 0
                },
                {
                    "sent": "If we compare this tool images here slice image and interest same image, you find that actually that this slice image are missing some important information, because as you can see in this interest the same image, you can still say the doctor.",
                    "label": 0
                },
                {
                    "sent": "But this last image essentially lose those kind of information, maybe which may be essential to summarize the content of this video and also here you can see the.",
                    "label": 0
                },
                {
                    "sent": "One thing here has three peaks, as in our interests image here.",
                    "label": 0
                },
                {
                    "sent": "Although it is distorted a little bit, but you can still see three big sister vessels here in the slice image it is all gone.",
                    "label": 0
                },
                {
                    "sent": "And another.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Popular video summarization scheme is to to represent each video clip as the key frame.",
                    "label": 0
                },
                {
                    "sent": "As we can see here we we extracted each row is is extracted from one image and those two from 2 videos from Tulare duplicate videos.",
                    "label": 0
                },
                {
                    "sent": "As you can see the keyframes because because of there could be a lot of video editing there and which also makes the short boundary to be a little bit unstable there.",
                    "label": 0
                },
                {
                    "sent": "As you can see that the keyframe based representation.",
                    "label": 0
                },
                {
                    "sent": "Basically for this two year duplicated videos, they extract totally different K for himself versus if you look into the interest the same image representation, visually they are still quite similar to each other.",
                    "label": 0
                },
                {
                    "sent": "That means when you're trying to match those two videos using this representation, it could be much easier because you you have overall summarization there.",
                    "label": 0
                },
                {
                    "sent": "So once we have the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interest, same image representation.",
                    "label": 0
                },
                {
                    "sent": "How do we generate our video?",
                    "label": 0
                },
                {
                    "sent": "See glacier.",
                    "label": 0
                },
                {
                    "sent": "So just in summary we have.",
                    "label": 0
                },
                {
                    "sent": "We have an interest.",
                    "label": 0
                },
                {
                    "sent": "Same image here.",
                    "label": 0
                },
                {
                    "sent": "Now what we perform these actually quite standard approach.",
                    "label": 0
                },
                {
                    "sent": "There we first use.",
                    "label": 0
                },
                {
                    "sent": "We detect the MSR region out of this interest.",
                    "label": 0
                },
                {
                    "sent": "Same image.",
                    "label": 0
                },
                {
                    "sent": "Then for each MCR region we extract further extract the SIFT descriptors out of each region.",
                    "label": 0
                },
                {
                    "sent": "Then we end up with a bag of feature representation there.",
                    "label": 0
                },
                {
                    "sent": "After running through our vocabulary tree to quantize each descriptors, we will come out of a bag of visual words representation.",
                    "label": 0
                },
                {
                    "sent": "Then we run mean hush.",
                    "label": 0
                },
                {
                    "sent": "Algorithm two to extract nearly like 60 min hash signature, so I won't.",
                    "label": 0
                },
                {
                    "sent": "Again I won't get into details of all this.",
                    "label": 0
                },
                {
                    "sent": "What I want to talk about is once we have this compact visual signature, how we could do the indexing more efficiently.",
                    "label": 0
                },
                {
                    "sent": "Of course with, because essentially a set of.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cash value is just a set of integers for efficient video indexing.",
                    "label": 0
                },
                {
                    "sent": "Usually large scale indexing.",
                    "label": 0
                },
                {
                    "sent": "We could build that inverted file system, essentially each for each video we just foreach foreach words in the visual vocabulary will build a list to index, which we do actually contain this racial words in retrieval time.",
                    "label": 0
                },
                {
                    "sent": "Of course you can do a forward look up that's and vote for each video, then the video with the highest vote will be your match.",
                    "label": 0
                },
                {
                    "sent": "That essentially means if you have this database you want to.",
                    "label": 0
                },
                {
                    "sent": "Find the YouTube, click it once on this query video here.",
                    "label": 0
                },
                {
                    "sent": "But this is this process really efficient use of course.",
                    "label": 0
                },
                {
                    "sent": "I mean is a common knowledge that if if this inverted list is sparse it you could, you could run, retrieve the relevant videos in a really efficient fashion, but based on the visual based on this kind of bag of visual words representation whatsoever, if you have enormous amount of videos, usually this inverted list is indeed dense, so that complexity is still.",
                    "label": 1
                },
                {
                    "sent": "Om if on average each list contains like M videos there, so could we do better?",
                    "label": 1
                },
                {
                    "sent": "The answer is yes with this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does the same image representation, just that too too just a reminder this this interest same image.",
                    "label": 0
                },
                {
                    "sent": "Actually the horizontal access still maintains the temporal information there.",
                    "label": 0
                },
                {
                    "sent": "So suppose you have feature detected in this position.",
                    "label": 0
                },
                {
                    "sent": "So if you measured the time length across the left boundary there, you could you could position it temporarily.",
                    "label": 0
                },
                {
                    "sent": "So if we have the same like feature detected in two different locations in the.",
                    "label": 0
                },
                {
                    "sent": "Second video, of course we would say that we would we would allow these two features to match versus the yellow and the red features will we?",
                    "label": 0
                },
                {
                    "sent": "We cannot just cannot match them cause the temporal location is far away from each other.",
                    "label": 0
                },
                {
                    "sent": "So with this we actually.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We could redefine our similarity measure just just sort of to recall the similarity between 2:00 set of min hash values is just to see what's the percentage of mean hash values they overlap.",
                    "label": 1
                },
                {
                    "sent": "Here, where we indeed could introduce that using the temporal context, essentially we count those count the number of overlaps of being hash values within a certain time period.",
                    "label": 0
                },
                {
                    "sent": "If the those key those two kids are equal to each other, but they are out of the scope of time spend, we don't allow them to match with this.",
                    "label": 0
                },
                {
                    "sent": "I have another one, this is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Should be the last slide.",
                    "label": 0
                },
                {
                    "sent": "Sorry, so with this temporal context.",
                    "label": 0
                },
                {
                    "sent": "Indeed, if we look into a specific inverted list there, we could use that temporal position indeed to sort all these videos.",
                    "label": 0
                },
                {
                    "sent": "Then in retrieval time.",
                    "label": 0
                },
                {
                    "sent": "We could if we want we firstly we will quickly identify time period there that largely reduced our search complexity because we can use something like binary search so that the complexity is essentially all log.",
                    "label": 0
                },
                {
                    "sent": "So consider you have you perform this forward.",
                    "label": 0
                },
                {
                    "sent": "Look at look up a lot of times in your retrieval process.",
                    "label": 0
                },
                {
                    "sent": "This could save a significant amount of retrieval time that's showing in our experiments, which I shared talk.",
                    "label": 0
                },
                {
                    "sent": "Very soon.",
                    "label": 0
                },
                {
                    "sent": "So any?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Questions so far here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Another important properties of this interest seems representation is.",
                    "label": 0
                },
                {
                    "sent": "You can see they they they have very strong global characteristics.",
                    "label": 1
                },
                {
                    "sent": "So another thing we we sort of proposes to extract the gist feature which is first proposed by only one to Robin in 2001.",
                    "label": 0
                },
                {
                    "sent": "Essentially you could you could simplify as extracting safety feature for the whole image there.",
                    "label": 0
                },
                {
                    "sent": "So essentially one larger 128 dimensional.",
                    "label": 0
                },
                {
                    "sent": "Features that we in our CS, uh retrieval experiments.",
                    "label": 0
                },
                {
                    "sent": "We use this gist features for four post verification there because because that simply is motivated, motivated by the strong global characteristics of this interests me.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They just are.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "During post verification stage, we simply rank whatever we retrieved from the database by by sort of combined similarity of the minhash signatures as well as this global descriptor step.",
                    "label": 0
                },
                {
                    "sent": "So I will talk about some results on that soon in the experiments.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For evaluation we have connected very large.",
                    "label": 0
                },
                {
                    "sent": "We do database essentially we crowd like 10,000 and 382 videos from the web they encompass.",
                    "label": 0
                },
                {
                    "sent": "Quite wide variety of different types of videos, including TV shows, drama, MTV, movie trailers and etc.",
                    "label": 1
                },
                {
                    "sent": "Out of this video datasets we build one set of unlabelled video clips because we will perform shot boundary detection to divide each videos each video into a set of video clips.",
                    "label": 0
                },
                {
                    "sent": "Because it only makes sense to extract in the interest same image from one shot because across short boundary the meaning is not that clear to us.",
                    "label": 0
                },
                {
                    "sent": "So out of this we build an unlabeled data set which contains like.",
                    "label": 0
                },
                {
                    "sent": "Ha 246 more than 246 video clips that they are used as the noise database.",
                    "label": 0
                },
                {
                    "sent": "Basically we use we mix our label data with it during retrieval tasks.",
                    "label": 0
                },
                {
                    "sent": "So for experiment, quantitative experimental evaluation, we build two sets of labeled video clips.",
                    "label": 1
                },
                {
                    "sent": "The first is actually quite cute is a controlled datasets.",
                    "label": 1
                },
                {
                    "sent": "We have 150 videos each underground like a line, different controlled transformations including color lighting.",
                    "label": 0
                },
                {
                    "sent": "SunTrust change and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "The second data set is a real leader.",
                    "label": 0
                },
                {
                    "sent": "Duplicate video datasets which contains nearly 800 videos.",
                    "label": 0
                },
                {
                    "sent": "So for experiments.",
                    "label": 0
                },
                {
                    "sent": "We take a live one hour straight.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For each query video from either the QR or the cutie datasets, the rest of the QR QDR mixed with the unlabeled datasets.",
                    "label": 1
                },
                {
                    "sent": "For then we perform retrieval tasks so.",
                    "label": 0
                },
                {
                    "sent": "So for quantitative evaluation we use the mean average precision.",
                    "label": 0
                },
                {
                    "sent": "So in case you're not familiar with average precision, essentially you take when you perform a retrieval you.",
                    "label": 0
                },
                {
                    "sent": "Are you averaged the accuracy across all different kinds of record levels?",
                    "label": 0
                },
                {
                    "sent": "So here give it give you example for query key.",
                    "label": 0
                },
                {
                    "sent": "Suppose there are three relevant results and you rank them at at position 1, four and seven.",
                    "label": 0
                },
                {
                    "sent": "Here is the equation how you sort of coming out of the average precision there.",
                    "label": 0
                },
                {
                    "sent": "So for every for all the queries you will perform the you average.",
                    "label": 0
                },
                {
                    "sent": "This average precision there we quite mean average precision, so I will skip the our experiments.",
                    "label": 0
                },
                {
                    "sent": "Results on Q are essentially have.",
                    "label": 0
                },
                {
                    "sent": "Uncut here because we have performed a lot of experiments, I want to be able to.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Discuss all of them.",
                    "label": 0
                },
                {
                    "sent": "So the first set of experimental results I want to mention here is is on the database.",
                    "label": 0
                },
                {
                    "sent": "Q Are essentially we have compared with all the previous state of the art video retrieval system, including the.",
                    "label": 1
                },
                {
                    "sent": "What is your reference charm it all in CCIVR 2007 and also this would Italians SM Multimedia 2009 so.",
                    "label": 1
                },
                {
                    "sent": "So if we look into this table, the first row listed retrieval accuracy of our first full system which including using the interest same image representation plus being hash signature and plus seeing verification.",
                    "label": 0
                },
                {
                    "sent": "As you can see across all the results, this full system achieves the highest accuracy there.",
                    "label": 0
                },
                {
                    "sent": "Versus if we will also do something if we discard the scene verification scheme.",
                    "label": 1
                },
                {
                    "sent": "Still it outperforms order previous results there.",
                    "label": 0
                },
                {
                    "sent": "This shows the advantages of using the interest same image representation because some of the difference from the from the previous approaches that they use the keyframe based representation was slice image based representation.",
                    "label": 0
                },
                {
                    "sent": "Here the last work they actually use the both local feature and global feature.",
                    "label": 1
                },
                {
                    "sent": "As you can see.",
                    "label": 0
                },
                {
                    "sent": "But about it, they don't use our interest.",
                    "label": 0
                },
                {
                    "sent": "Same image representation as we can see.",
                    "label": 0
                },
                {
                    "sent": "That's a direct comparison like using interest.",
                    "label": 0
                },
                {
                    "sent": "Same images still improve the results quite a lot.",
                    "label": 0
                },
                {
                    "sent": "There is from the experiments, so some other experiments were performed.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is to really measure the effects of the number of being harsh signatures you use.",
                    "label": 0
                },
                {
                    "sent": "As you can see, in general if you increase the number of minhash functions you use, the retrieval accuracy will increase.",
                    "label": 0
                },
                {
                    "sent": "Interesting observation we have is that if we have this same verification scheme posted at that.",
                    "label": 0
                },
                {
                    "sent": "The highest accuracy we achieve is actually achieved at that point, which we actually reduce the memory usage.",
                    "label": 0
                },
                {
                    "sent": "We believe that that is big, cause the.",
                    "label": 0
                },
                {
                    "sent": "A global descriptor, the gist descriptor and the local local feature based representation.",
                    "label": 0
                },
                {
                    "sent": "There are complementary to each other, so that essentially means when you're in the first step of retrieval, you're using local feature based representation.",
                    "label": 0
                },
                {
                    "sent": "You can you can sort of focusing on just increasing the recall and then the global features will will help you to boost the Acura.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just just the final summary of the system performance.",
                    "label": 0
                },
                {
                    "sent": "As you can see our full system for each video.",
                    "label": 0
                },
                {
                    "sent": "We only use like 480 bytes versus if we use mean harsh plus this temporal context, it is absolutely better than the baseline, but it consumes much more memory and if you compare that retrieval time with this temporal context, because we can do this kind of binary search scenario, the retrieval time is significantly improved.",
                    "label": 1
                },
                {
                    "sent": "Almost like a 20.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Times speedup there, sure yeah.",
                    "label": 0
                },
                {
                    "sent": "So just in conclusion, we have come out with this very efficient visual representation here and some future work.",
                    "label": 1
                },
                {
                    "sent": "I just want to motivate the field in this.",
                    "label": 0
                },
                {
                    "sent": "We have talked about we have a lot of work on interest point extraction and also interested region.",
                    "label": 1
                },
                {
                    "sent": "Maybe the next step is focusing on lines and edges, thanks.",
                    "label": 0
                },
                {
                    "sent": "I think this time for maybe 1 quick question and I'll request people to use the microphone.",
                    "label": 0
                },
                {
                    "sent": "I have a question.",
                    "label": 0
                },
                {
                    "sent": "What kind of invariances does your representation afford?",
                    "label": 0
                },
                {
                    "sent": "Yes, we did.",
                    "label": 0
                },
                {
                    "sent": "We did have a lot of performance, a lot of evaluation by under controlled datasets we have because we since synthetically transformed video like by adjusting the contrast, adjust lighting conditions and it seems like there's some results is presented in our paper.",
                    "label": 0
                },
                {
                    "sent": "We found that in general our interest, same extraction, is quite robust too.",
                    "label": 0
                },
                {
                    "sent": "Different kind of video editing and visual variations, but is part of our future work to do more sort of objective evaluation there.",
                    "label": 0
                },
                {
                    "sent": "But thank you again, yeah, thanks.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I see a lot of them.",
                    "label": 0
                }
            ]
        }
    }
}