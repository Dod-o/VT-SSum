{
    "id": "rgtxzhp2pmsig32v4e4t5n4wqxc6okt6",
    "title": "Large-Scale Sparse Logistic Regression",
    "info": {
        "author": [
            "Jieping Ye, Department of Electrical Engineering and Computer Science, University of Michigan"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/kdd09_liu_lsslr/",
    "segmentation": [
        [
            "OK, so the last talk.",
            "Discuss regularization and L1 regularization so this makes my talk much easy, so let's talk about 3 components.",
            "First is logistic regression, so it's for classification.",
            "Let's parse means that we will embed feature selection inside the classifier, like a one organization and a key part of this talk is how to make this large scale so this is joint work with my postdoctoral and my student generator."
        ],
        [
            "Offline state.",
            "So let's say you go to a hospital.",
            "You want to check certain type of disease and you may take some measurement using your image or microarray or other technology.",
            "So you want to know whether you have this disease or not.",
            "So it's abandoning prediction.",
            "And they also want to know confidence of prediction, but there's a huge difference of.",
            "Let's say you have 10%.",
            "You have this disease or zero 1% disease, so essentially won't give probability.",
            "And finally I want to make a prediction.",
            "Typically will look, certainly Jehovah images or certain features you want to identify least informative features, so it needs work.",
            "We are interesting classifier which will achieve all of these three tasks.",
            "We want to do a prediction with confidence with probability and which we also do select features.",
            "That's what we focus on sparse load retrogression."
        ],
        [
            "So maybe the first question why?",
            "Why would we focus on logic question?",
            "So it's actually so it's very well known technique.",
            "It has been applied in many applications and they're being extensive study comparing logic, crashing another classifier such as SVM, and they are in many cases are compatible in terms of performance.",
            "But there's a big difference in terms of efficiency or how to solve the problem.",
            "So for example SVN, the loss function is not smooth, so that's why in the last talk they use the subgradient.",
            "But because of the objective is not Morse and at least makes the problem much harder to solve or much slower or the loss function in allergic reaction, it's smooth.",
            "So long you have plastic rain dissent or more advanced technology to make this faster and it has been applied in many applications.",
            "Regularization are commonly used to reduce overfitting and make more robust.",
            "So we can use it both L1 and L2.",
            "As discussed in the last talk."
        ],
        [
            "And you need to walk away.",
            "Focus on L1 regularization cause it leads to a sparse solution.",
            "So why do we care about sparse if we get sparse model and essentially we will do simultaneous feature selection and classification and the model is more interpretable and typically the performance will be improved.",
            "It has been applied in many applications, typically biological applications, to find informative feeds."
        ],
        [
            "Thanks so how the many applications that data is huge number of dimensions.",
            "For example release neuroimaging we use in this study.",
            "It contains more than 1,000,000 features.",
            "We've actually even more 7 million features depending on how you do a standardization.",
            "So lucky questions.",
            "Here is how we can scale sparse logistic regression.",
            "Two large size problems, huge sense problems, large dimensions.",
            "So this is the main motivation behind this work."
        ],
        [
            "So OK, so the key contribution of this work we propose.",
            "Lost plop.",
            "It's called large scale sparse logistic regression for large scale problems an it's a first order method cause if you apply 2nd order method you need to compute second all information caching at least can lease will be fully expensive for large scale problems, so typically for large scale problems we are limited to 1st order methods.",
            "So this means each iteration you only evaluate function value and gradient and we can show each iteration of this algorithm only involve matrix vector multiplication.",
            "So it's very efficient scale to large size problems.",
            "And since we're doing matrix vector multiplication, so if the data is sparse, learning this will be really efficient.",
            "We can use the sparsity of data, but more importantly the algorithm will achieve the optimal convergence rate among all first order methods.",
            "OK, so the convergence rate is one of those cases where people have left me a lot of studies on sparse on large scale 1st order method.",
            "For smooth problems we have shown that one of his greatest optimal we can achieve and our proposed algorithm achieve the optimal convergence rate.",
            "So this shows in theory you cannot do better, so all the squares optimal lonely.",
            "So you can try better is trying to reduce the constant."
        ],
        [
            "So here's the outline of the rest of the talk.",
            "First, I will briefly introduce Iowa, and then we'll discuss sparse error based on one organization, and then we'll discuss the proposed average."
        ],
        [
            "And always son results.",
            "Kate Smith so in a logic question with this model we have, we compute the probability of a given sample for particular class.",
            "Label will be so here a example is your data.",
            "Image Dimension is the dimension collage like 1 million dimension and B is a class label.",
            "Here we focus on two class either plus or minus one disease or not.",
            "So in awhile we compute this conditional probability give example it's a class label given by a signal function.",
            "An act on linear function over the samples.",
            "OK, so he can buy the signal function.",
            "We have S kind of shape list map any real value to 01 because here it's a.",
            "It's a probability, so we want to make sure that the value is between zero and one."
        ],
        [
            "And in the training staff will give a collection of sample data points.",
            "A1A 2AN, and we have a corresponding labels B1B2B N. So for each sample we can compute its probability based on at least WCR parameter.",
            "We try to estimate.",
            "It is a weighted vector list is the offset.",
            "So for each sample training sample we can estimate the probability.",
            "Then we can compute the joint probability of all the samples computer this product.",
            "And so the way we do estimation is we want.",
            "Maximize these are joint probability to estimate both W&C.",
            "And this is equivalent.",
            "To maximize this, one is equal to minimize log of this joint probability.",
            "So this is a logistic regression.",
            "We try to minimize this average logistical loss to compute W&C.",
            "So these are two unknowns WC."
        ],
        [
            "Covid list simple minimisation list will actually to overfitting, especially if your sample size is smaller and your dimension is large and typically way to regularization.",
            "You can do L2, norm L1 or so in this case where we mainly interesting organization, but for in this work we use both.",
            "OK, so this is the logistic loss.",
            "At least this part is 2 known regularization.",
            "We put in objective.",
            "At least one is 1 organization, so you can actually move this one on.",
            "Tool.",
            "Objective is equivalent, but it's not quality.",
            "And if law is zero, let's say Louisiana Lenny Century at least standard sparse allergic reaction.",
            "So more general low can be any value larger equal to 0.",
            "So because we were at least one penalty to these little radius.",
            "So this defines the objective defines L1 ball radius easy.",
            "So this is a little sparse solution.",
            "So question is here how we can solve this L1 book constraint sparse logistic regression?"
        ],
        [
            "Efficiently.",
            "So thankful objective is smooth.",
            "So the first method we count is gradient descent.",
            "So let's consider more general minimisation problem.",
            "We try to minimize object G, so in this case GG is least part.",
            "Summation of F and to know OK, OK, after a smooth function and we have a particular constraint X belong to this set G and in our case just say one ball.",
            "So let's say we are currently at XSK.",
            "At case iteration learn for gradient descent, we compute its gradient, geez gradient, and now we move along anti gradient direction.",
            "We certain step size 1 / L K. So that's the first step.",
            "We move along NT gradient direction.",
            "Since we have this constraint, X belong to D way, we need to project back to our constraint.",
            "So this is L1 ball constraint.",
            "So after SK we do projections.",
            "So that will be our next point, then will repeat until convergence.",
            "So there are two issues.",
            "Here is how to.",
            "Compute this a step size to make sure it converge.",
            "The second one is how to do projection."
        ],
        [
            "So in our case, the set GL1 ball, so these are 2D case list has been shown in the last talk.",
            "OK, so let's look at the simple example.",
            "Let's say you are here.",
            "These two DK simple case, right?",
            "So when you do a projection means that you want to find the nearest point, find the nearest point within this region to this V1 so you can track if the ones here learns nearest point is this corner point Z.",
            "So if Lenny's point is called a point Z learner X axis zero, that's why get sparsity.",
            "OK, so this is YL, one over little sparse version.",
            "If your if your point of view is here, lines along this line, so there's no sparsity.",
            "This relies least Counterpoint Line also leads to sparse solutions, so that's why if you apply one, or that's why you get sparse solutions.",
            "And for two decades you get a close one solution, hauling 1 dimensions large like 10,001 million dimension list is not straightforward, but there are some algorithm which can compute projection in linear time, can be very efficient.",
            "Even 4 million dimensions."
        ],
        [
            "So that's quite a decent now already have some algorithm.",
            "You do projection, you get gradient and you do projection.",
            "OK so this will lead to algorithm converge 1 / K This already very fast compared to subgradient.",
            "However this is not optimal because we know if the object is smooth and optimal convergence rate is lower case squared.",
            "OK, and then suddenly is 1 algorithm called nest of algorithm which actually achieved optimal convergence rate.",
            "So one of the square.",
            "Kate soul.",
            "Is one of the key case.",
            "The number of iterations, so over K means after let's say K iteration the error term is 1 K smaller better right?",
            "So after castration is one of the key error small but this even smaller one of the script.",
            "So if case is 1000 list is 0.01 this is much smaller."
        ],
        [
            "So let's make huge difference.",
            "Let's look at one example on the difference.",
            "If we want to achieve accuracy of listento apart minus 8.",
            "So all error is we want to achieve error turned to a -- 8 then we want to check how many iterations we need to run OK for both algorithms.",
            "If you run green dissent is turned to 8.",
            "Iteration is huge number.",
            "If you want a nest of messages, turn through Port 4 so there's huge difference in terms of convergence rate."
        ],
        [
            "OK, so that's why you need since I work away.",
            "Appliance top method will call is lost its first order black black box method.",
            "So at each iteration we only evaluate function value and gradient and then we'll plan a stop method.",
            "Follow estimation of data points.",
            "And as we mentioned in the grade center, two key issues wine.",
            "So how to how to do projection onto a one ball?",
            "And this can be done anytime the.",
            "Otherwise how to choose step size?",
            "It turns out this will make a huge difference in terms of in terms of the speed.",
            "So although you can plan the standard technique on this approach right, but the step size typically extremely small after a lot of let's say 10,000 iterations, so this will still converge slowly in practice.",
            "Also, theoretically it's very far right.",
            "It's one of the case group, but practically it will still be fairly slow.",
            "And the line search make huge difference.",
            "OK so we need to walk away, apply the key contribution, we apply adaptive lines such scheme to make at least one efficient."
        ],
        [
            "So here we show difference between Grande sent an install method just one slide, but for more detail in converting paper in the gradient descent URSKK then you compute this gradient you along with the anti gradient as we discussed with certain step size one over Lambda K and we show this will achieve if you choose LK incorrect way you achieve a conversion rate of 1 / K. So what's the difference between this and next old method?",
            "Let's say you are SK.",
            "OK so we know SK minus one last point S K -- 1.",
            "So you do, you compute the linear combination of S, K -- 1 SK to get SK SK is just on the same line.",
            "Then you apply NT gradient direction at SK to get XC password.",
            "So let's just minor difference.",
            "So essentially in.",
            "Pretty decent way to not use the history of list procedure.",
            "While you need to approach, we use two data points, not just wanted to point, it.",
            "Turns out this minor difference make huge difference in terms of the convergence rate.",
            "So there are two separate issues here.",
            "Why is how to choose better K?",
            "The second one?",
            "How to choose this LK step size?",
            "There are two well known approach for estimating the stepsize OK.",
            "The first one is just a constant scheme.",
            "Just say it's a constant.",
            "And then run and guarantee convergence.",
            "The second one is lesser one buyer proposed by Noam Chomsky.",
            "We discussing next slide."
        ],
        [
            "So in the first game we set LK to be a constant 1 / K, and it turns out that we have to set our larger than or less certain constant we call Lipschitz continuous gradient of function if we choose Elks Lodge and L then you are OK. You get 1 / K ^2 convergence rate.",
            "So the problem is here this L can be huge, can be large.",
            "And then your step size is 1 / L. So if L is large, this means your step size is very small.",
            "So each time you make very small progress, that's why they may converge slowly.",
            "So let's not skin proposed by Noam Chomsky.",
            "Is LK can be much smaller L?",
            "OK, that's good.",
            "Now you make big progress.",
            "However, the LK at iteration has to improve.",
            "Then this means L 1000 must be largely oh 999 LK value.",
            "This value must increase cannot increase, so this is necessary to guarantee it converge with the optimal convergence rate.",
            "So that's a limitation, OK?"
        ],
        [
            "So in our proposed scheme, we allow LK to be tuned adaptively based on our data.",
            "So can increase can decrease it turns out.",
            "If you allow OK to decrease or increase, learn how to make sure your algorithm still converge as one of his square is not trivial, but I will not.",
            "I will not discuss detail here.",
            "You can find in the paper.",
            "It's essentially we show even with this choice.",
            "OK, OK, can be increased decreasing.",
            "We can still converge.",
            "We can still show like algorithm converge in one of the square."
        ],
        [
            "So there's been a lot of recent work on Ernesto method.",
            "This method has been actually known for some time, but it's only recently it become quite popular.",
            "So especially this year 2009 there are many, many papers.",
            "The first part, blue, is only use of this method for sparse learning.",
            "For Lasso Las Vegas, how to make this really fast?",
            "Waller, last two paper is used.",
            "This method for attrition organization.",
            "For example, you do multi task learning matrix completion.",
            "OK, so it's kind of interesting.",
            "This method has not been on for awhile, but it's only recently.",
            "Next last 1 two years this become very popular and people show this really converge really fast."
        ],
        [
            "So for evaluation we use six datasets here sample size.",
            "Flown several to 7000 and dimension is from 2002 / 1,000,000 dimensions.",
            "Here is non zeros."
        ],
        [
            "So first, the way we compare the proposed lines, such scheme ways low on proposed by Imotski.",
            "And this graph shows LK value, so we know 1 / L K is the step size.",
            "So OK, if OK, smaller learn a step size larger we make larger progress, let them converge faster.",
            "You can see this red lines, the adept schemes much smaller learn this blue line OK?",
            "That's why that's why the shows least converge faster.",
            "This objective and redline laposky conversion much fast compared to learning more scheme the same thing."
        ],
        [
            "The next two datasets I will skip.",
            "Pics so when you do sparse largest great you have a constant Z Caesar radius of 1 bar and then the performance depends on the.",
            "If this if the value is zero learn.",
            "The solution is very sparse.",
            "If these large is not sparse.",
            "But in practice we don't know which is good.",
            "So typically we do cross validation.",
            "We choose a set of Z values like 7.1 one 100 and then you choose the best one.",
            "So essentially we have to run this algorithm.",
            "If we have N candidate follows the value.",
            "So good part of these algorithms we can we can.",
            "We can use one start.",
            "OK So what we do is we run the sparse logistic regression one Z1Z small.",
            "So this bus runs very fast.",
            "Then we run sparse logistic rating on the next value we use the result from the last Z value as the initial point.",
            "Now we continue.",
            "It.",
            "Turns out if you do this one start, it's much faster.",
            "Let's see this green line century.",
            "Say we don't.",
            "Ron wants that every time we've long scratch.",
            "So Lisa curb the axis is running time while recognise issue each time to computer.",
            "This solution for a particular value we use the result from the smaller the value as the starting point.",
            "Now we do incremental updating and you can see this is much more efficient.",
            "So this shows this shows list algorithm is efficient when computing ipassword solution you want to compute solution for a collection of Z values."
        ],
        [
            "So we compare with the one we called projection L1 and turns out this is more efficient.",
            "I will skip."
        ],
        [
            "The same thing."
        ],
        [
            "Here.",
            "OK, so one of the state of art for sparse logistic regression is no one proposed by core and boy Stephen Boyd on standard 2007 and we compare or adaptive with algorithm and for all the data set with different choice of these parameters values and we can we can see they are compatible or more efficient.",
            "So I will not go into detail here."
        ],
        [
            "So finally I will show one example.",
            "So why this past is actually useful sometimes?",
            "So we apply sparse lodge aggression on biological images here.",
            "Each each is embryo image.",
            "At different stages we normally would develop from early stage to later stage left Stage 5 two 12.",
            "So the biology divide actually the stage into 17 stage ranges, 17 stage so you can see the morphological patterns for different stages are quite different because they're different activities at different times of the embryo development.",
            "So we want to see how we can actually identify features specific for."
        ],
        [
            "Each stage.",
            "Followed biological domain knowledge way.",
            "No at all.",
            "This stage range we know important activity at each of these stages.",
            "These are all activities, so we want to show if we apply sparse logistic regression on this data.",
            "Can we identify something which is similar to the biological knowledge we have, so case result?",
            "So here here all the features identified by sparse logistic regression.",
            "So I will not go into detail, but let's look at some examples.",
            "So let's see stage 6K.",
            "Here we have some pattern here and 4627 and see this list.",
            "This band I should move to left side is here but no to 7 is moved to the left side, 8 even left nine and 10 is less pattern move to a left side.",
            "And you can see we look at features 6 like here.",
            "Suddenly something here outside eight in the middle is actually moved to left side and the 9th clear 10 similar.",
            "So you can see there's a pattern an after 1011 list will actually check back.",
            "This band actually will check back from least reverse direction reverse directions Ann.",
            "If you look at the image here from 10:50, it's actually track back.",
            "12 is now disappears.",
            "So this shows the promise of.",
            "Sparse logistic regression for finding is informative."
        ],
        [
            "Truss that's the end of my talk.",
            "Yeah, thank you."
        ],
        [
            "Quick questions.",
            "So bellessa code if you want to run algorithm.",
            "Lily's link is in also in the paper.",
            "If you want to run this algorithm.",
            "So you're based on the number of method which uses the previous XK and the next day minus one before that, so that makes it similar quasi Newton method that's using the second derivative.",
            "So and there are limited memory quasi Newton methods that don't be prohibitive storage, so would it be possible to buy something like limited memory FGS?",
            "OK, so yeah, I think that's a good question, so if you apply that one essentially do 2nd order method right?",
            "But you can try to make this.",
            "Efficient, so I guess there are two directions is here.",
            "We do first order method.",
            "We don't need hashing, adjust function value and opportunity gradient or Newton method, 2nd order method.",
            "And then there's also direction.",
            "Try to make the 2nd order most efficient.",
            "Memory efficient, right?",
            "So it will be interesting to compare, but I think the first automatically less scalability is better for larger sense problems.",
            "We only use the 1st order information.",
            "By using two previous point, your invincibly using some curvature information.",
            "So it's somewhere in between the 1st order and 2nd order method.",
            "Yeah, OK, so let's definitely try to mimic so in the first automatic, if you do first element is one OK, but we try to mimic what we did in 2nd order method.",
            "So the whole approach is still fast automatic but converge like second order method is 1 / K ^2.",
            "Anyone else?",
            "Also by personal biological applications.",
            "So do you think that your sparsity inherently deal with this kind of looking for solution, so we may not even image is incomplete.",
            "All you have to do some additional work so Evel Knievel image if the data is incomplete so far will not deal with this issue yet.",
            "So here we are.",
            "Going over features are complete evil feature actually missing.",
            "We can potentially do missing data estimation first.",
            "Learn apply this approach.",
            "Structured solution you can.",
            "Just.",
            "Yeah, maybe, but it's only after we know it's possible before.",
            "I don't know which which region is passed, which is not right.",
            "OK, yeah, thank you.",
            "OK, that's clear again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the last talk.",
                    "label": 0
                },
                {
                    "sent": "Discuss regularization and L1 regularization so this makes my talk much easy, so let's talk about 3 components.",
                    "label": 0
                },
                {
                    "sent": "First is logistic regression, so it's for classification.",
                    "label": 0
                },
                {
                    "sent": "Let's parse means that we will embed feature selection inside the classifier, like a one organization and a key part of this talk is how to make this large scale so this is joint work with my postdoctoral and my student generator.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Offline state.",
                    "label": 0
                },
                {
                    "sent": "So let's say you go to a hospital.",
                    "label": 0
                },
                {
                    "sent": "You want to check certain type of disease and you may take some measurement using your image or microarray or other technology.",
                    "label": 0
                },
                {
                    "sent": "So you want to know whether you have this disease or not.",
                    "label": 0
                },
                {
                    "sent": "So it's abandoning prediction.",
                    "label": 0
                },
                {
                    "sent": "And they also want to know confidence of prediction, but there's a huge difference of.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have 10%.",
                    "label": 0
                },
                {
                    "sent": "You have this disease or zero 1% disease, so essentially won't give probability.",
                    "label": 0
                },
                {
                    "sent": "And finally I want to make a prediction.",
                    "label": 0
                },
                {
                    "sent": "Typically will look, certainly Jehovah images or certain features you want to identify least informative features, so it needs work.",
                    "label": 0
                },
                {
                    "sent": "We are interesting classifier which will achieve all of these three tasks.",
                    "label": 0
                },
                {
                    "sent": "We want to do a prediction with confidence with probability and which we also do select features.",
                    "label": 0
                },
                {
                    "sent": "That's what we focus on sparse load retrogression.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So maybe the first question why?",
                    "label": 0
                },
                {
                    "sent": "Why would we focus on logic question?",
                    "label": 0
                },
                {
                    "sent": "So it's actually so it's very well known technique.",
                    "label": 0
                },
                {
                    "sent": "It has been applied in many applications and they're being extensive study comparing logic, crashing another classifier such as SVM, and they are in many cases are compatible in terms of performance.",
                    "label": 0
                },
                {
                    "sent": "But there's a big difference in terms of efficiency or how to solve the problem.",
                    "label": 0
                },
                {
                    "sent": "So for example SVN, the loss function is not smooth, so that's why in the last talk they use the subgradient.",
                    "label": 0
                },
                {
                    "sent": "But because of the objective is not Morse and at least makes the problem much harder to solve or much slower or the loss function in allergic reaction, it's smooth.",
                    "label": 0
                },
                {
                    "sent": "So long you have plastic rain dissent or more advanced technology to make this faster and it has been applied in many applications.",
                    "label": 1
                },
                {
                    "sent": "Regularization are commonly used to reduce overfitting and make more robust.",
                    "label": 1
                },
                {
                    "sent": "So we can use it both L1 and L2.",
                    "label": 0
                },
                {
                    "sent": "As discussed in the last talk.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you need to walk away.",
                    "label": 0
                },
                {
                    "sent": "Focus on L1 regularization cause it leads to a sparse solution.",
                    "label": 1
                },
                {
                    "sent": "So why do we care about sparse if we get sparse model and essentially we will do simultaneous feature selection and classification and the model is more interpretable and typically the performance will be improved.",
                    "label": 1
                },
                {
                    "sent": "It has been applied in many applications, typically biological applications, to find informative feeds.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks so how the many applications that data is huge number of dimensions.",
                    "label": 1
                },
                {
                    "sent": "For example release neuroimaging we use in this study.",
                    "label": 0
                },
                {
                    "sent": "It contains more than 1,000,000 features.",
                    "label": 1
                },
                {
                    "sent": "We've actually even more 7 million features depending on how you do a standardization.",
                    "label": 0
                },
                {
                    "sent": "So lucky questions.",
                    "label": 0
                },
                {
                    "sent": "Here is how we can scale sparse logistic regression.",
                    "label": 1
                },
                {
                    "sent": "Two large size problems, huge sense problems, large dimensions.",
                    "label": 0
                },
                {
                    "sent": "So this is the main motivation behind this work.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK, so the key contribution of this work we propose.",
                    "label": 0
                },
                {
                    "sent": "Lost plop.",
                    "label": 0
                },
                {
                    "sent": "It's called large scale sparse logistic regression for large scale problems an it's a first order method cause if you apply 2nd order method you need to compute second all information caching at least can lease will be fully expensive for large scale problems, so typically for large scale problems we are limited to 1st order methods.",
                    "label": 0
                },
                {
                    "sent": "So this means each iteration you only evaluate function value and gradient and we can show each iteration of this algorithm only involve matrix vector multiplication.",
                    "label": 0
                },
                {
                    "sent": "So it's very efficient scale to large size problems.",
                    "label": 1
                },
                {
                    "sent": "And since we're doing matrix vector multiplication, so if the data is sparse, learning this will be really efficient.",
                    "label": 0
                },
                {
                    "sent": "We can use the sparsity of data, but more importantly the algorithm will achieve the optimal convergence rate among all first order methods.",
                    "label": 1
                },
                {
                    "sent": "OK, so the convergence rate is one of those cases where people have left me a lot of studies on sparse on large scale 1st order method.",
                    "label": 0
                },
                {
                    "sent": "For smooth problems we have shown that one of his greatest optimal we can achieve and our proposed algorithm achieve the optimal convergence rate.",
                    "label": 0
                },
                {
                    "sent": "So this shows in theory you cannot do better, so all the squares optimal lonely.",
                    "label": 0
                },
                {
                    "sent": "So you can try better is trying to reduce the constant.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the outline of the rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "First, I will briefly introduce Iowa, and then we'll discuss sparse error based on one organization, and then we'll discuss the proposed average.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And always son results.",
                    "label": 0
                },
                {
                    "sent": "Kate Smith so in a logic question with this model we have, we compute the probability of a given sample for particular class.",
                    "label": 0
                },
                {
                    "sent": "Label will be so here a example is your data.",
                    "label": 0
                },
                {
                    "sent": "Image Dimension is the dimension collage like 1 million dimension and B is a class label.",
                    "label": 1
                },
                {
                    "sent": "Here we focus on two class either plus or minus one disease or not.",
                    "label": 0
                },
                {
                    "sent": "So in awhile we compute this conditional probability give example it's a class label given by a signal function.",
                    "label": 0
                },
                {
                    "sent": "An act on linear function over the samples.",
                    "label": 0
                },
                {
                    "sent": "OK, so he can buy the signal function.",
                    "label": 0
                },
                {
                    "sent": "We have S kind of shape list map any real value to 01 because here it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a probability, so we want to make sure that the value is between zero and one.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in the training staff will give a collection of sample data points.",
                    "label": 0
                },
                {
                    "sent": "A1A 2AN, and we have a corresponding labels B1B2B N. So for each sample we can compute its probability based on at least WCR parameter.",
                    "label": 0
                },
                {
                    "sent": "We try to estimate.",
                    "label": 0
                },
                {
                    "sent": "It is a weighted vector list is the offset.",
                    "label": 0
                },
                {
                    "sent": "So for each sample training sample we can estimate the probability.",
                    "label": 0
                },
                {
                    "sent": "Then we can compute the joint probability of all the samples computer this product.",
                    "label": 1
                },
                {
                    "sent": "And so the way we do estimation is we want.",
                    "label": 0
                },
                {
                    "sent": "Maximize these are joint probability to estimate both W&C.",
                    "label": 0
                },
                {
                    "sent": "And this is equivalent.",
                    "label": 0
                },
                {
                    "sent": "To maximize this, one is equal to minimize log of this joint probability.",
                    "label": 1
                },
                {
                    "sent": "So this is a logistic regression.",
                    "label": 0
                },
                {
                    "sent": "We try to minimize this average logistical loss to compute W&C.",
                    "label": 0
                },
                {
                    "sent": "So these are two unknowns WC.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Covid list simple minimisation list will actually to overfitting, especially if your sample size is smaller and your dimension is large and typically way to regularization.",
                    "label": 0
                },
                {
                    "sent": "You can do L2, norm L1 or so in this case where we mainly interesting organization, but for in this work we use both.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the logistic loss.",
                    "label": 0
                },
                {
                    "sent": "At least this part is 2 known regularization.",
                    "label": 0
                },
                {
                    "sent": "We put in objective.",
                    "label": 0
                },
                {
                    "sent": "At least one is 1 organization, so you can actually move this one on.",
                    "label": 0
                },
                {
                    "sent": "Tool.",
                    "label": 0
                },
                {
                    "sent": "Objective is equivalent, but it's not quality.",
                    "label": 0
                },
                {
                    "sent": "And if law is zero, let's say Louisiana Lenny Century at least standard sparse allergic reaction.",
                    "label": 0
                },
                {
                    "sent": "So more general low can be any value larger equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So because we were at least one penalty to these little radius.",
                    "label": 0
                },
                {
                    "sent": "So this defines the objective defines L1 ball radius easy.",
                    "label": 0
                },
                {
                    "sent": "So this is a little sparse solution.",
                    "label": 1
                },
                {
                    "sent": "So question is here how we can solve this L1 book constraint sparse logistic regression?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Efficiently.",
                    "label": 0
                },
                {
                    "sent": "So thankful objective is smooth.",
                    "label": 0
                },
                {
                    "sent": "So the first method we count is gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So let's consider more general minimisation problem.",
                    "label": 0
                },
                {
                    "sent": "We try to minimize object G, so in this case GG is least part.",
                    "label": 0
                },
                {
                    "sent": "Summation of F and to know OK, OK, after a smooth function and we have a particular constraint X belong to this set G and in our case just say one ball.",
                    "label": 0
                },
                {
                    "sent": "So let's say we are currently at XSK.",
                    "label": 0
                },
                {
                    "sent": "At case iteration learn for gradient descent, we compute its gradient, geez gradient, and now we move along anti gradient direction.",
                    "label": 0
                },
                {
                    "sent": "We certain step size 1 / L K. So that's the first step.",
                    "label": 0
                },
                {
                    "sent": "We move along NT gradient direction.",
                    "label": 0
                },
                {
                    "sent": "Since we have this constraint, X belong to D way, we need to project back to our constraint.",
                    "label": 0
                },
                {
                    "sent": "So this is L1 ball constraint.",
                    "label": 0
                },
                {
                    "sent": "So after SK we do projections.",
                    "label": 0
                },
                {
                    "sent": "So that will be our next point, then will repeat until convergence.",
                    "label": 0
                },
                {
                    "sent": "So there are two issues.",
                    "label": 0
                },
                {
                    "sent": "Here is how to.",
                    "label": 0
                },
                {
                    "sent": "Compute this a step size to make sure it converge.",
                    "label": 0
                },
                {
                    "sent": "The second one is how to do projection.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in our case, the set GL1 ball, so these are 2D case list has been shown in the last talk.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's look at the simple example.",
                    "label": 0
                },
                {
                    "sent": "Let's say you are here.",
                    "label": 0
                },
                {
                    "sent": "These two DK simple case, right?",
                    "label": 0
                },
                {
                    "sent": "So when you do a projection means that you want to find the nearest point, find the nearest point within this region to this V1 so you can track if the ones here learns nearest point is this corner point Z.",
                    "label": 0
                },
                {
                    "sent": "So if Lenny's point is called a point Z learner X axis zero, that's why get sparsity.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is YL, one over little sparse version.",
                    "label": 0
                },
                {
                    "sent": "If your if your point of view is here, lines along this line, so there's no sparsity.",
                    "label": 0
                },
                {
                    "sent": "This relies least Counterpoint Line also leads to sparse solutions, so that's why if you apply one, or that's why you get sparse solutions.",
                    "label": 0
                },
                {
                    "sent": "And for two decades you get a close one solution, hauling 1 dimensions large like 10,001 million dimension list is not straightforward, but there are some algorithm which can compute projection in linear time, can be very efficient.",
                    "label": 1
                },
                {
                    "sent": "Even 4 million dimensions.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's quite a decent now already have some algorithm.",
                    "label": 0
                },
                {
                    "sent": "You do projection, you get gradient and you do projection.",
                    "label": 0
                },
                {
                    "sent": "OK so this will lead to algorithm converge 1 / K This already very fast compared to subgradient.",
                    "label": 0
                },
                {
                    "sent": "However this is not optimal because we know if the object is smooth and optimal convergence rate is lower case squared.",
                    "label": 0
                },
                {
                    "sent": "OK, and then suddenly is 1 algorithm called nest of algorithm which actually achieved optimal convergence rate.",
                    "label": 0
                },
                {
                    "sent": "So one of the square.",
                    "label": 0
                },
                {
                    "sent": "Kate soul.",
                    "label": 0
                },
                {
                    "sent": "Is one of the key case.",
                    "label": 0
                },
                {
                    "sent": "The number of iterations, so over K means after let's say K iteration the error term is 1 K smaller better right?",
                    "label": 0
                },
                {
                    "sent": "So after castration is one of the key error small but this even smaller one of the script.",
                    "label": 0
                },
                {
                    "sent": "So if case is 1000 list is 0.01 this is much smaller.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's make huge difference.",
                    "label": 0
                },
                {
                    "sent": "Let's look at one example on the difference.",
                    "label": 0
                },
                {
                    "sent": "If we want to achieve accuracy of listento apart minus 8.",
                    "label": 0
                },
                {
                    "sent": "So all error is we want to achieve error turned to a -- 8 then we want to check how many iterations we need to run OK for both algorithms.",
                    "label": 0
                },
                {
                    "sent": "If you run green dissent is turned to 8.",
                    "label": 0
                },
                {
                    "sent": "Iteration is huge number.",
                    "label": 0
                },
                {
                    "sent": "If you want a nest of messages, turn through Port 4 so there's huge difference in terms of convergence rate.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's why you need since I work away.",
                    "label": 0
                },
                {
                    "sent": "Appliance top method will call is lost its first order black black box method.",
                    "label": 0
                },
                {
                    "sent": "So at each iteration we only evaluate function value and gradient and then we'll plan a stop method.",
                    "label": 1
                },
                {
                    "sent": "Follow estimation of data points.",
                    "label": 0
                },
                {
                    "sent": "And as we mentioned in the grade center, two key issues wine.",
                    "label": 0
                },
                {
                    "sent": "So how to how to do projection onto a one ball?",
                    "label": 0
                },
                {
                    "sent": "And this can be done anytime the.",
                    "label": 1
                },
                {
                    "sent": "Otherwise how to choose step size?",
                    "label": 0
                },
                {
                    "sent": "It turns out this will make a huge difference in terms of in terms of the speed.",
                    "label": 0
                },
                {
                    "sent": "So although you can plan the standard technique on this approach right, but the step size typically extremely small after a lot of let's say 10,000 iterations, so this will still converge slowly in practice.",
                    "label": 0
                },
                {
                    "sent": "Also, theoretically it's very far right.",
                    "label": 1
                },
                {
                    "sent": "It's one of the case group, but practically it will still be fairly slow.",
                    "label": 0
                },
                {
                    "sent": "And the line search make huge difference.",
                    "label": 0
                },
                {
                    "sent": "OK so we need to walk away, apply the key contribution, we apply adaptive lines such scheme to make at least one efficient.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we show difference between Grande sent an install method just one slide, but for more detail in converting paper in the gradient descent URSKK then you compute this gradient you along with the anti gradient as we discussed with certain step size one over Lambda K and we show this will achieve if you choose LK incorrect way you achieve a conversion rate of 1 / K. So what's the difference between this and next old method?",
                    "label": 0
                },
                {
                    "sent": "Let's say you are SK.",
                    "label": 0
                },
                {
                    "sent": "OK so we know SK minus one last point S K -- 1.",
                    "label": 0
                },
                {
                    "sent": "So you do, you compute the linear combination of S, K -- 1 SK to get SK SK is just on the same line.",
                    "label": 0
                },
                {
                    "sent": "Then you apply NT gradient direction at SK to get XC password.",
                    "label": 0
                },
                {
                    "sent": "So let's just minor difference.",
                    "label": 0
                },
                {
                    "sent": "So essentially in.",
                    "label": 0
                },
                {
                    "sent": "Pretty decent way to not use the history of list procedure.",
                    "label": 0
                },
                {
                    "sent": "While you need to approach, we use two data points, not just wanted to point, it.",
                    "label": 0
                },
                {
                    "sent": "Turns out this minor difference make huge difference in terms of the convergence rate.",
                    "label": 0
                },
                {
                    "sent": "So there are two separate issues here.",
                    "label": 0
                },
                {
                    "sent": "Why is how to choose better K?",
                    "label": 0
                },
                {
                    "sent": "The second one?",
                    "label": 0
                },
                {
                    "sent": "How to choose this LK step size?",
                    "label": 0
                },
                {
                    "sent": "There are two well known approach for estimating the stepsize OK.",
                    "label": 0
                },
                {
                    "sent": "The first one is just a constant scheme.",
                    "label": 0
                },
                {
                    "sent": "Just say it's a constant.",
                    "label": 0
                },
                {
                    "sent": "And then run and guarantee convergence.",
                    "label": 0
                },
                {
                    "sent": "The second one is lesser one buyer proposed by Noam Chomsky.",
                    "label": 0
                },
                {
                    "sent": "We discussing next slide.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the first game we set LK to be a constant 1 / K, and it turns out that we have to set our larger than or less certain constant we call Lipschitz continuous gradient of function if we choose Elks Lodge and L then you are OK. You get 1 / K ^2 convergence rate.",
                    "label": 1
                },
                {
                    "sent": "So the problem is here this L can be huge, can be large.",
                    "label": 0
                },
                {
                    "sent": "And then your step size is 1 / L. So if L is large, this means your step size is very small.",
                    "label": 0
                },
                {
                    "sent": "So each time you make very small progress, that's why they may converge slowly.",
                    "label": 0
                },
                {
                    "sent": "So let's not skin proposed by Noam Chomsky.",
                    "label": 0
                },
                {
                    "sent": "Is LK can be much smaller L?",
                    "label": 0
                },
                {
                    "sent": "OK, that's good.",
                    "label": 0
                },
                {
                    "sent": "Now you make big progress.",
                    "label": 0
                },
                {
                    "sent": "However, the LK at iteration has to improve.",
                    "label": 0
                },
                {
                    "sent": "Then this means L 1000 must be largely oh 999 LK value.",
                    "label": 0
                },
                {
                    "sent": "This value must increase cannot increase, so this is necessary to guarantee it converge with the optimal convergence rate.",
                    "label": 0
                },
                {
                    "sent": "So that's a limitation, OK?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in our proposed scheme, we allow LK to be tuned adaptively based on our data.",
                    "label": 0
                },
                {
                    "sent": "So can increase can decrease it turns out.",
                    "label": 0
                },
                {
                    "sent": "If you allow OK to decrease or increase, learn how to make sure your algorithm still converge as one of his square is not trivial, but I will not.",
                    "label": 0
                },
                {
                    "sent": "I will not discuss detail here.",
                    "label": 0
                },
                {
                    "sent": "You can find in the paper.",
                    "label": 0
                },
                {
                    "sent": "It's essentially we show even with this choice.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, can be increased decreasing.",
                    "label": 0
                },
                {
                    "sent": "We can still converge.",
                    "label": 0
                },
                {
                    "sent": "We can still show like algorithm converge in one of the square.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's been a lot of recent work on Ernesto method.",
                    "label": 0
                },
                {
                    "sent": "This method has been actually known for some time, but it's only recently it become quite popular.",
                    "label": 0
                },
                {
                    "sent": "So especially this year 2009 there are many, many papers.",
                    "label": 0
                },
                {
                    "sent": "The first part, blue, is only use of this method for sparse learning.",
                    "label": 0
                },
                {
                    "sent": "For Lasso Las Vegas, how to make this really fast?",
                    "label": 0
                },
                {
                    "sent": "Waller, last two paper is used.",
                    "label": 0
                },
                {
                    "sent": "This method for attrition organization.",
                    "label": 0
                },
                {
                    "sent": "For example, you do multi task learning matrix completion.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "This method has not been on for awhile, but it's only recently.",
                    "label": 0
                },
                {
                    "sent": "Next last 1 two years this become very popular and people show this really converge really fast.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for evaluation we use six datasets here sample size.",
                    "label": 0
                },
                {
                    "sent": "Flown several to 7000 and dimension is from 2002 / 1,000,000 dimensions.",
                    "label": 0
                },
                {
                    "sent": "Here is non zeros.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first, the way we compare the proposed lines, such scheme ways low on proposed by Imotski.",
                    "label": 1
                },
                {
                    "sent": "And this graph shows LK value, so we know 1 / L K is the step size.",
                    "label": 0
                },
                {
                    "sent": "So OK, if OK, smaller learn a step size larger we make larger progress, let them converge faster.",
                    "label": 0
                },
                {
                    "sent": "You can see this red lines, the adept schemes much smaller learn this blue line OK?",
                    "label": 0
                },
                {
                    "sent": "That's why that's why the shows least converge faster.",
                    "label": 0
                },
                {
                    "sent": "This objective and redline laposky conversion much fast compared to learning more scheme the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next two datasets I will skip.",
                    "label": 0
                },
                {
                    "sent": "Pics so when you do sparse largest great you have a constant Z Caesar radius of 1 bar and then the performance depends on the.",
                    "label": 0
                },
                {
                    "sent": "If this if the value is zero learn.",
                    "label": 0
                },
                {
                    "sent": "The solution is very sparse.",
                    "label": 0
                },
                {
                    "sent": "If these large is not sparse.",
                    "label": 0
                },
                {
                    "sent": "But in practice we don't know which is good.",
                    "label": 0
                },
                {
                    "sent": "So typically we do cross validation.",
                    "label": 0
                },
                {
                    "sent": "We choose a set of Z values like 7.1 one 100 and then you choose the best one.",
                    "label": 0
                },
                {
                    "sent": "So essentially we have to run this algorithm.",
                    "label": 0
                },
                {
                    "sent": "If we have N candidate follows the value.",
                    "label": 0
                },
                {
                    "sent": "So good part of these algorithms we can we can.",
                    "label": 0
                },
                {
                    "sent": "We can use one start.",
                    "label": 0
                },
                {
                    "sent": "OK So what we do is we run the sparse logistic regression one Z1Z small.",
                    "label": 0
                },
                {
                    "sent": "So this bus runs very fast.",
                    "label": 0
                },
                {
                    "sent": "Then we run sparse logistic rating on the next value we use the result from the last Z value as the initial point.",
                    "label": 0
                },
                {
                    "sent": "Now we continue.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "Turns out if you do this one start, it's much faster.",
                    "label": 0
                },
                {
                    "sent": "Let's see this green line century.",
                    "label": 0
                },
                {
                    "sent": "Say we don't.",
                    "label": 0
                },
                {
                    "sent": "Ron wants that every time we've long scratch.",
                    "label": 0
                },
                {
                    "sent": "So Lisa curb the axis is running time while recognise issue each time to computer.",
                    "label": 0
                },
                {
                    "sent": "This solution for a particular value we use the result from the smaller the value as the starting point.",
                    "label": 0
                },
                {
                    "sent": "Now we do incremental updating and you can see this is much more efficient.",
                    "label": 0
                },
                {
                    "sent": "So this shows this shows list algorithm is efficient when computing ipassword solution you want to compute solution for a collection of Z values.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we compare with the one we called projection L1 and turns out this is more efficient.",
                    "label": 0
                },
                {
                    "sent": "I will skip.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same thing.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "OK, so one of the state of art for sparse logistic regression is no one proposed by core and boy Stephen Boyd on standard 2007 and we compare or adaptive with algorithm and for all the data set with different choice of these parameters values and we can we can see they are compatible or more efficient.",
                    "label": 0
                },
                {
                    "sent": "So I will not go into detail here.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally I will show one example.",
                    "label": 0
                },
                {
                    "sent": "So why this past is actually useful sometimes?",
                    "label": 0
                },
                {
                    "sent": "So we apply sparse lodge aggression on biological images here.",
                    "label": 0
                },
                {
                    "sent": "Each each is embryo image.",
                    "label": 0
                },
                {
                    "sent": "At different stages we normally would develop from early stage to later stage left Stage 5 two 12.",
                    "label": 0
                },
                {
                    "sent": "So the biology divide actually the stage into 17 stage ranges, 17 stage so you can see the morphological patterns for different stages are quite different because they're different activities at different times of the embryo development.",
                    "label": 0
                },
                {
                    "sent": "So we want to see how we can actually identify features specific for.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Each stage.",
                    "label": 0
                },
                {
                    "sent": "Followed biological domain knowledge way.",
                    "label": 0
                },
                {
                    "sent": "No at all.",
                    "label": 0
                },
                {
                    "sent": "This stage range we know important activity at each of these stages.",
                    "label": 0
                },
                {
                    "sent": "These are all activities, so we want to show if we apply sparse logistic regression on this data.",
                    "label": 0
                },
                {
                    "sent": "Can we identify something which is similar to the biological knowledge we have, so case result?",
                    "label": 0
                },
                {
                    "sent": "So here here all the features identified by sparse logistic regression.",
                    "label": 0
                },
                {
                    "sent": "So I will not go into detail, but let's look at some examples.",
                    "label": 0
                },
                {
                    "sent": "So let's see stage 6K.",
                    "label": 0
                },
                {
                    "sent": "Here we have some pattern here and 4627 and see this list.",
                    "label": 0
                },
                {
                    "sent": "This band I should move to left side is here but no to 7 is moved to the left side, 8 even left nine and 10 is less pattern move to a left side.",
                    "label": 0
                },
                {
                    "sent": "And you can see we look at features 6 like here.",
                    "label": 0
                },
                {
                    "sent": "Suddenly something here outside eight in the middle is actually moved to left side and the 9th clear 10 similar.",
                    "label": 0
                },
                {
                    "sent": "So you can see there's a pattern an after 1011 list will actually check back.",
                    "label": 0
                },
                {
                    "sent": "This band actually will check back from least reverse direction reverse directions Ann.",
                    "label": 0
                },
                {
                    "sent": "If you look at the image here from 10:50, it's actually track back.",
                    "label": 0
                },
                {
                    "sent": "12 is now disappears.",
                    "label": 0
                },
                {
                    "sent": "So this shows the promise of.",
                    "label": 0
                },
                {
                    "sent": "Sparse logistic regression for finding is informative.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Truss that's the end of my talk.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quick questions.",
                    "label": 0
                },
                {
                    "sent": "So bellessa code if you want to run algorithm.",
                    "label": 0
                },
                {
                    "sent": "Lily's link is in also in the paper.",
                    "label": 0
                },
                {
                    "sent": "If you want to run this algorithm.",
                    "label": 0
                },
                {
                    "sent": "So you're based on the number of method which uses the previous XK and the next day minus one before that, so that makes it similar quasi Newton method that's using the second derivative.",
                    "label": 0
                },
                {
                    "sent": "So and there are limited memory quasi Newton methods that don't be prohibitive storage, so would it be possible to buy something like limited memory FGS?",
                    "label": 0
                },
                {
                    "sent": "OK, so yeah, I think that's a good question, so if you apply that one essentially do 2nd order method right?",
                    "label": 0
                },
                {
                    "sent": "But you can try to make this.",
                    "label": 0
                },
                {
                    "sent": "Efficient, so I guess there are two directions is here.",
                    "label": 0
                },
                {
                    "sent": "We do first order method.",
                    "label": 0
                },
                {
                    "sent": "We don't need hashing, adjust function value and opportunity gradient or Newton method, 2nd order method.",
                    "label": 0
                },
                {
                    "sent": "And then there's also direction.",
                    "label": 0
                },
                {
                    "sent": "Try to make the 2nd order most efficient.",
                    "label": 0
                },
                {
                    "sent": "Memory efficient, right?",
                    "label": 0
                },
                {
                    "sent": "So it will be interesting to compare, but I think the first automatically less scalability is better for larger sense problems.",
                    "label": 0
                },
                {
                    "sent": "We only use the 1st order information.",
                    "label": 0
                },
                {
                    "sent": "By using two previous point, your invincibly using some curvature information.",
                    "label": 0
                },
                {
                    "sent": "So it's somewhere in between the 1st order and 2nd order method.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so let's definitely try to mimic so in the first automatic, if you do first element is one OK, but we try to mimic what we did in 2nd order method.",
                    "label": 0
                },
                {
                    "sent": "So the whole approach is still fast automatic but converge like second order method is 1 / K ^2.",
                    "label": 0
                },
                {
                    "sent": "Anyone else?",
                    "label": 0
                },
                {
                    "sent": "Also by personal biological applications.",
                    "label": 0
                },
                {
                    "sent": "So do you think that your sparsity inherently deal with this kind of looking for solution, so we may not even image is incomplete.",
                    "label": 0
                },
                {
                    "sent": "All you have to do some additional work so Evel Knievel image if the data is incomplete so far will not deal with this issue yet.",
                    "label": 0
                },
                {
                    "sent": "So here we are.",
                    "label": 0
                },
                {
                    "sent": "Going over features are complete evil feature actually missing.",
                    "label": 0
                },
                {
                    "sent": "We can potentially do missing data estimation first.",
                    "label": 0
                },
                {
                    "sent": "Learn apply this approach.",
                    "label": 0
                },
                {
                    "sent": "Structured solution you can.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe, but it's only after we know it's possible before.",
                    "label": 0
                },
                {
                    "sent": "I don't know which which region is passed, which is not right.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, that's clear again.",
                    "label": 0
                }
            ]
        }
    }
}