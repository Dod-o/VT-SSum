{
    "id": "wwxw6fr5qghlsz6febaqw2xdi3kxwvcb",
    "title": "Learning Languages and Rational Kernels",
    "info": {
        "author": [
            "Mehryar Mohri, Courant Institute of Mathematical Sciences, Google Research"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Programming Languages"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_mohri_llrk/",
    "segmentation": [
        [
            "Yes, so I'm with the Courant Institute at New York University and also with Google Research and this is joint work with Carina Quartus and Ari or Leo Qatar which who is at the Wiseman Institute.",
            "Let me start by saying that I've actually just finished preparing these slides just a few minutes ago, so have some mercy have given other type this morning that for which I also had to prefer to slice it last minute.",
            "So, but this is a workshop.",
            "I guess that's fine.",
            "The main purpose for me would be here to sort of invite you to look at certain ways of learning languages, which perhaps is different from what people have tried.",
            "In the past, and since it's again a workshop to invite you to discuss this more and use this as an inspiration."
        ],
        [
            "So let me start with a very general paradigm that people have used in the past for learning.",
            "Well, perhaps not general languages, but learning finite automata.",
            "One can think of them as being the sort of state merging or state splitting paradigm, and that is common to a number of research papers that you probably have all seen in the past.",
            "The work of Dana Angluin on CNET, Al, Run at other.",
            "The list is very long.",
            "And essentially this consists of doing the following.",
            "You start with an atomic time or a tree accepting all the examples which would then correspond to that, and then with the finest partition.",
            "Because each state would then represent a block of your equivalence class, and then iteratively you would start merging these states.",
            "If this is the merging one technique, and so you would, and these states would then correspond to partition blocks while preserving some congruence.",
            "OK, that you have to come with.",
            "And then you keep doing this.",
            "And at the end, once you cannot merge states anymore while preserving that congruence while you said this is the automaton that I have learned OK, and in some sense the entire algorithm is directly defined by the choice of the Congress that you come with, essentially a whole family of algorithms could be defined this way.",
            "Come with your congruence done.",
            "That's the algorithm.",
            "I mean something similar can be done about the reverse, where you start with a one state automaton and you start splitting it according to again a congruence.",
            "OK, so this is the standard state merging or splitting power."
        ],
        [
            "I'm the talk, I'm and here's an example of that.",
            "This is, for example, for zero reversible languages and the work of Dana Angluin Zero reversible languages are those languages that can be accepted by an automaton that is both deterministic as you read it left to right, but also right to left OK. And here, for example you start.",
            "With a tree representing the set of strings that you have seen.",
            "And you keep merging these States and this is what you get at the end.",
            "Here I've been lazy with a double arrow represent arrows that are going both directions and essentially what you get here is a set of strings with an even number of Asian bees.",
            "So this is what you have learned starting from the finite set that you had at the beginning, OK?"
        ],
        [
            "Now the work that I'm going to present is really departing from that standard paradigm in the following sense that it is now seeking to map strings to a high dimensional feature space.",
            "And then look, you know, find a separating hyperplane in that space that would define your strings OK.",
            "So basically, strings that will be on one side of that hyperplane, or of this nonlinear mapping if you're going.",
            "If I is a nonlinear mapping to some feature space would define the strings of your language.",
            "OK, so in some sense this is not surprising to many of you here who are familiar with kernel methods, and you would say that in some sense this is just almost like using kernel methods to learn languages.",
            "And then of course, the first question that comes up is what kernels should I choose?",
            "How do I define the similarity between these sequences?"
        ],
        [
            "And if you think about it, there are of course many different ways of defining sequences similarity between sequences.",
            "One can come up with actually very complicated ways of doing that.",
            "The the language or the strength of the machines that you're using to define those similarities could be context free, context sensitive, but in some sense if you go beyond a certain point, the whole idea of kernels is lost, right?",
            "Because kernels are supposed to be efficient to compute.",
            "So once you go to already, the level of context free grammars, you've already lost quite a lot of that because not only it's not even context free parsing that you need to do is really something that corresponds to combining together to two algebraic series.",
            "This is quite costly, OK?",
            "Alright, so then, on the other hand, if you look at what people have done in the past, not to learn languages, but really to do classification or regression related to strings or sequences, here is what they've been looking at.",
            "There are.",
            "They've been using ngram kernels, kernels that are based on counting N grams in a sequence, gappy engrams tree kernels for parsing.",
            "Moment kernels, which are kernels that we have been looking at, which sort of take into account, not just accounts, but also the moments, other moments of the counts.",
            "Locality improved kernels used by ZN.",
            "It Alan that it Al includes a bunch of names including Bernard, chopped off and any others that I cannot remember.",
            "Eyes a mismatch kernels that have been used by defined by Christina Leslie for also in the context of computational biology.",
            "So it's a bunch of kernels that people have used and that one way or another to them has represented what they think they knew about that task.",
            "But one thing that is common to all these kernels is that.",
            "They are all.",
            "Rational kernels.",
            "OK, there are special instances of rational kernels.",
            "In fact, in general I have not still found an example of a kernel sequence kernel used in practice weather in natural language processing, computational biology, or other context that is not a rational kernel.",
            "OK, now what are rational kernels?",
            "You could say?",
            "And I'm going to you're about to ask this.",
            "Thank you.",
            "Well, I'm going to just define them.",
            "I'm going to precede."
        ],
        [
            "We define this and this is what this talk.",
            "How did stock is going to be structured so?",
            "I'm going to talk about rational kernels and define what they correspond to.",
            "But in order for me to introduce rational kernels, I first need to talk about weighted transducers for two reasons.",
            "One sort of because the definition of rational kernels is based on weighted transducers, but also because if you want to talk a little bit about proofs and about so that you get a little bit more of a familiarity with what these correspond to, we need to talk about them.",
            "So after having introduced rational kernels, I will then ask a number of questions.",
            "About learning with the learning languages with rational kernels, and this brings you to study the linear separability.",
            "Of sets of sequences or languages using rational kernels and now then give a number of results related to that.",
            "So let me start with weighted transducers, perhaps for some of you this would be elementary."
        ],
        [
            "But I think it's good for us to all have the same terminology if nothing else.",
            "So here is an example of a weighted transducer.",
            "It is a directed graph.",
            "I'm going to denote by Bolt Circle the initial States and by a double circle the final states.",
            "It is a directed graph where each edge or called transition is labeled with both an input label.",
            "You're A and output label.",
            "Here B and some weight that is indicated here after the slash symbol, which here is 0.1 OK. And the final state also is assigned some weight final states.",
            "There might be several of them.",
            "Some weight here is 0.1 indicated after after the state #3, which is the weight that you take as you leave the machine.",
            "OK, how do you use a weighted transducer?",
            "What is used in the following way?",
            "The weight associated by the transducer T to the pair of strings X&Y?",
            "Is this some of the weights of all accepted path with input X and output Y?",
            "So the the the the input label of a path is the string that you obtained by concatenating input labels.",
            "So here ABB for example OK and similarly with the output labels.",
            "So.",
            "There are two operations.",
            "One is the sum to sum up the weights of all the path with input X and output Y and one is product.",
            "Because we the weight of a path is obtained by multiplying the weights along that path.",
            "OK, so in particular here for example, the transmit transducer associated stupid to repair ABB BAA, the following weight.",
            "Because there are two paths labeled with the pair ABBA one is ABB Cups.",
            "And on the output you can see that it's BA, so that's the top path, another one is.",
            "Anne.",
            "ABB, right on the input and similarly with the output cable.",
            "So there are two paths and you can see that the the weight is obtained.",
            "The weight of each path is obtained by multiplying the weight of that path, including the .1 that you obtain for the final state, and then the two weights are summed up OK. Now this specific case of weighted transducers is known as weighted automata.",
            "This is the particular case where input and output label of every.",
            "Transition is the same.",
            "OK, so in other words it will be BB or AA or BB on every transition.",
            "The input and output label would be the same.",
            "And when that that is the case, then to be concise.",
            "There's no reason to keep around the two.",
            "It suffices to keep just one label.",
            "And it's also not necessary anymore to talk about the value of the transducer for repair, because really, when the pairs that are for which there is a non 0 value are those that have the Form XX.",
            "So this is also the notation is then.",
            "Reduced to being just T of X instead of being T of expire OK."
        ],
        [
            "Now there are certain number of operations with weighted transducers that I'm going to quickly tell you about one.",
            "Is this some so it turns out that if you have two weighted transducers T1 and T2, there's some of these transducers which is defined as such as the transducer that associated to X&Y is some of the values associated by T1 and T-22 XY.",
            "Is also a weighted transducer.",
            "In fact, you can obtain it very simply.",
            "Here are two weighted transducers, right?",
            "The one that corresponds to the sum.",
            "You can also think of it as a union.",
            "It suffices to create a new state zero and asociates and create Epson Epson transitions epsilon.",
            "Here for me is the empty word OK or the null work so?",
            "Create 2 transitions to what used to be the initial state of these machines.",
            "And the second you have done that, now you can read everything that you could read with the first machine and everything that you could read with the second machine and then therefore any XY that could be labeled that would correspond to a path here is also.",
            "You could also have its weight here, so by definition you have the sum.",
            "It's very simple."
        ],
        [
            "The product of two transducers product and perhaps I could also say concatenation is defined as such.",
            "T 1 * T two of XY.",
            "Is this some over all possible ways of decomposing the string X into a prefix X1 Anna suffix X2?",
            "And string wire into prefix why one suffix Y 2?",
            "The sum over these of T1 of X1Y1T2 of X2Y2.",
            "Now actually, the in this case the machine is even easier to understand that maybe the formula.",
            "Because all that you do 2 from 2 transducers to compute the product or the concatenation is simply to connect the final states of the first machine to the initial states of the second machine.",
            "With absolute Epson transitions, but since you have a final wait here, that final weight is going to become the weight of these SNS on transitions.",
            "OK, this extremely simple and that's why the product of two transducers corresponds to.",
            "Now when the transducer T2 coincides with T1.",
            "This is also abbreviated into T1 squared, and similarly if you multiply a T1 by itself many times, it becomes T 1 to the power of N."
        ],
        [
            "The fundamental operation.",
            "So another operation would be the closure or also known as the Kleene closure, right?",
            "And similarly here you can compute.",
            "From I think I don't, it's not from these two.",
            "It's from another way transducer I've been too lazy here with this slides.",
            "Just copying what I had in the previous one.",
            "You can compute this one from the first one, essentially by doing the same as what you would do in the case of a single unweighted automaton.",
            "But the most important operation here is known as composition."
        ],
        [
            "And consists of taking two transducers T1 and T2.",
            "And composing them together in the following way.",
            "The weight associated to explain now is this.",
            "Some of T1 of XG times T2 of ZY over all possible strings Z.",
            "The way to understand this is that the transducer T1 is a functional view that is going to help you understand the transducer T one is going to map the string X to a bunch of string Z, many of them right?",
            "The transducer T2 is going to take those strings E and map them to some other 2 two Y. OK so in how many different ways you can have this matching of outputs of T14X that map the input of T2 OK and that's what the sum is over all the all these matching these.",
            "OK, there's another view of this.",
            "And it is the one that corresponds to the case where the number of Z's, here in the middle, would be finite.",
            "If you think about this, in that case, this operation is simply.",
            "Can someone say here?",
            "What it corresponds to.",
            "It simply metrics multiplication, right?",
            "If the number of.",
            "Hi, in that case if the number of XYZ czar is finite, this composition is actually just precisely matrix multiplication.",
            "Now this is of course, in the finite case, we don't have the finite case here.",
            "The number of these could be actually infinite.",
            "And, uh.",
            "In addition to this, the infinite sets of these are rational sets, regular sets, so there is structure here.",
            "Now let me quickly give you an idea of how to compose two transducers T1 and T2 to obtain another one.",
            "If you have two transducers, T1 and T2 represent as such this states of the composition can be identified with pairs.",
            "Where 0110121 where the first element of the pair is the state number in the first machine and the second one is state number in the second machine.",
            "And what you do is that you start from the initial states.",
            "Here 00 the pair 00 and then you ask.",
            "Is there a transition leaving this state zero whose output label matches the input label of a transition matching leaving 0 here?",
            "When you find them, you construct a new transducer with input a, an output B here, and whose weight is multiply.",
            "Multiply multiplication of the two ways .1 times .1 here .01.",
            "And which state do you reach?",
            "You state that the pair that correspond to the state you would have reached using these matching transitions and you go on and this is the way the algorithm works.",
            "It's quite simple as you can see in the very very worst case where the all the transitions of the first machine match all the transition of the second machine you are actually going to create something that is quadratic.",
            "In fact, you're going to obtain all the possible states as well, and so in the very worst case this would be quadratic in the size of the first machine times aside.",
            "Size of the second machine.",
            "In practice, for most reasonable cases this is far from being the case."
        ],
        [
            "Alright, so let me also introduce something else about weighted.",
            "Transducers are not simply the sum of the weights of all the path of a way to transducer.",
            "OK.",
            "I am going to denote by capital SFT the some of the weights.",
            "In other words, the sum of the accepted path, the weights of the accepted path where value of Pi pies await Python path that is accepted times the weight of the final state that it's reaching an of Pi is the end of the path and role of that state is the final weight of that state?",
            "OK, so this is just the sum of the weights of all the path.",
            "And actually, it can be computed in a very efficient way using a generalization of standard shortest distance algorithms to from the tropical semiring.",
            "The mean plus it hearing to the plus times.",
            "Another name for it that probably is more familiar to you is simply the forward backward algorithm.",
            "When the graph is acyclic, you can just use the forward backward algorithm, but it's not.",
            "There are more complications.",
            "OK, so it's very simple.",
            "You can easily compute the sum of the ways of all the path.",
            "I had some properties the some of the weights of T 1 + T two is just the sum of the weights of T1 plus some of T2.",
            "If you multiply a transducer T by some Lambda, that means like multiplying its final, the weight of its final weights by Lambda.",
            "That's the same as doing Lambda times the sum of the weights trivial.",
            "And then.",
            "This one is just as simple, but it's important this some of the weights of Lambda T1 composed with T2.",
            "Is also simply Lambda SFT, one composed with teacher and this is very clear from the definition as well.",
            "I gave in to.",
            "Huh?",
            "Can you pull out Lambda as well?",
            "Can you put can I put it on T2?",
            "You mean yes I can do that as well.",
            "Yes, I'm just getting lazy.",
            "You could also put it on this one, right?",
            "So yes, it's you know you have all these linearity."
        ],
        [
            "OK, so this was all beautiful.",
            "Let me now give you a first intuition of why weighted transducers are good for you.",
            "They're good for you because in a variety of cases where you wish to define a similarity between two sequences.",
            "These similarities are based on some sort of statistics on some counts.",
            "I'm count some some, some substrings, right?",
            "It turns out weighted transducers can count for you.",
            "OK, here is how they count.",
            "Suppose that you have a string capital X, which here could be just for example AB or more generally, let X be a an automaton, an arbitrarily finite automaton representing some regular expression that you're interested in.",
            "Then this transducer, the one that is reduced to one state with a zombies map to the empty string with weight 1.",
            "All the weights are one here in this transducer.",
            "Followed by that automaton or string mapped to itself.",
            "Followed by again a self loops that are basically erasing self loops.",
            "They everything they read their race.",
            "OK well this weighted automata Norway transducer rather is taking some input.",
            "So say that your input string is E. It is going to count the number of times that X appears in Z.",
            "And the number of times that X appears in Z is going to be exactly.",
            "The sum of the weights of the path of Z composed with this transducer.",
            "That's very nice, right?",
            "You?",
            "You have strings E but you some text.",
            "And you compose it with this transducer.",
            "It has just two states basically.",
            "I mean if you forget about the states of X.",
            "And then you take the sum of the weights of all the path that does it for you.",
            "How let me explain on an example how it works.",
            "It would not be more difficult on a in the general case to explain it.",
            "Suppose that your input string is this one.",
            "And suppose that the string that you want to count in this text is a D. What this transducer does is essentially.",
            "Here, taking care of some prefix instead 0 instead one taking care of themselves some suffix.",
            "Well, how does it work?",
            "It reads some prefix and erases it Maps it to epsilon, then it reads AB here or the string that you want.",
            "Then it reads some suffix that is just erasing.",
            "But how many different ways can you go from that state?",
            "0 to the state one?",
            "Every time that you wish to read some prefix and then eventually go to state one, you have to have seen the string AB.",
            "And that's the only way that you can go to the state one to a final state.",
            "So the only the number of path that successful path that you're going to have is going to be exactly the number of times that the string AB occurs in the input string Z.",
            "And in this case you see this.",
            "It reads the transducer reads from state one some prefix an Maps it to epsilon, then with a beta goes to state one, then Maps everything to epsilon.",
            "Similarly, here it reads some other prefix here, then a B, then epsilon OK.",
            "So it happens two path.",
            "Essentially the weight of these two path is just one because all the weights are one, so adding them together because that's that's what this S does.",
            "This is some of the weights of the path that just gives you exactly the weight.",
            "The count of this string AB in the input Z, and things can be completely generalized using the same transducer, in other words.",
            "Once again, if instead of the string AB, Capital X was an arbitrary automaton representing an arbitrary regular expression, it would be counting all the strings accepted by that automaton.",
            "And even more, it could be generalized if Z was not a string, was what was it, but in fact was awaited automaton.",
            "It would just be then computing the expected counts of these strings in that weighted automata.",
            "It's very, very general."
        ],
        [
            "Right, but that's the generality that I need because to now give you just already an example of what you can do with this.",
            "The same this transducer can be used to, for example count bigrams.",
            "OK, so you remember a few minutes ago I gave an example of an engram kernel people have been using in practice for biology or natural language processing.",
            "One of the simplest one of them is an ngram model, so a background model is one that so this transducer, what it does is that it's mapping every input string.",
            "To the set of bigrams that that string contains.",
            "That is a a ABB ABB.",
            "In this case.",
            "If the if I reduce the alphabet to something manageable then I can write on the slide OK. And more Interestingly, then it is going to count the number of occurrences of every one of these bigrams in the input string Z.",
            "You don't have to explicitly go and write it because when you compose these two transverses it's giving you an implicit compact representation of the counts.",
            "But if you want to explicitly counts AB for example, again composi with T with a B computer, some of the weight of the path and you have that count OK. Alright, so."
        ],
        [
            "Now that we have done this introduction to Wakefield transducers, let me now talk about what rational kernels are and why they become relevant then."
        ],
        [
            "Irrational kernel key which we can view as a similarity measure between two strings X&Y.",
            "I would say it Colonel Kelly sweat K is rational when actually it can be written as you have XY when you is a weighted transducer.",
            "Very simple definition, right?",
            "A kernel is rational whenever it corresponds basically to a weighted transducer.",
            "OK. Now it's very nice because as we said before, using composition and shortest distance, you can actually compute care of XY because they have expire is the short is S of.",
            "I am going to denote by Autofix as as the automaton linear automaton representing X. Compose with you compose without of Y and then summing the weights of all the path.",
            "And that again because we said previously that composition is a quadratic algorithm.",
            "The complexity of this is just the length of X times the length of Y.",
            "You here is a constant right?",
            "It's fixed on once and for all.",
            "And you can actually even do things in a faster, more efficient way in specific cases, if you know something specific about you, OK?",
            "It's quite simple."
        ],
        [
            "So.",
            "Let me now say another result that is interesting about rational kernels, and that's the fact that if you denote by key minus one, the weighted transducer you obtain from T by swapping the input and output label of every transition.",
            "OK.",
            "It turns out that the transducer you defined by T composed with T -- 1.",
            "Defines a positive definite symmetric kernel.",
            "Always let me give you another thing.",
            "Another fact previously I said that every one of the string kernels I have seen in practice and competition by your natural language processing the correspond to rational kernels.",
            "I'm going to be even more specific there.",
            "All rational kernels of this kind.",
            "OK, they all can be written as teaching minus one.",
            "Now one can show that any as I said, anything like this is positive and symmetric.",
            "But just to give you an intuition of what this corresponds to.",
            "Here is why.",
            "The transducer T you can think of it as a mapping.",
            "It is taking the input string and is mapping it to a set of strings and regular language OK with some weights associated to each one of those strings.",
            "Similarly, so that's what T20 is applied to X.",
            "Similarly, when you apply T2Y.",
            "It is mapped to a whole set of strings in that some regular language, and there are some weights associated to those.",
            "If you take the dot product of those vectors with which have which are infinite dimensional vectors, each one of them having the weights, the transducer associated to those strings.",
            "That is exactly what corresponds to doing the composition that I just described.",
            "That is exactly what corresponds to teaching minus one.",
            "OK, another way of saying it is that if you're familiar with kernels and you if you whenever you get a kernel Ku thinking about the some feature mapping file that corresponds to this, you can almost think of T as being defy.",
            "The only difference is that Phi does not explicitly write a feature vector.",
            "Instead it is giving you a compact representation of a high dimensional vector OK?",
            "So hopefully I've given a little bit of the intuition behind this."
        ],
        [
            "Now previously I said that you can define a bigram kernel with just three states.",
            "Well, people have used for text categorization.",
            "What they have called a gappy bigram successfully, and without more states, we can extend that transducer to not count bigrams, because gappy bigrams because we can just add a self loop here, which penalizes by some penalty factor Lambda the gap between the first symbol of a diagram and the second symbol.",
            "And since by the observation that I made earlier, any one of these can count what's in the middle, this is the middle.",
            "This is the capital X that I had in the middle.",
            "You can count also gapi backgrounds using exactly the same things in exactly the same operation."
        ],
        [
            "Another example of what you would see in practice is the mismatch kernel that I refer to earlier on.",
            "And here is the example for not going to go into the details of this obviously, but here are the parameters that are defining this particular mismatch.",
            "Kernel 3 and 2 two is the levels of three is how many the length of the window.",
            "And again, that has been successfully used for computational biology, and this is actually what it corresponds to.",
            "If you write it with weighted transducers, it is a particular case of teaching minus one rational kernel.",
            "This is the team that corresponds to it.",
            "OK, in some sense you see a little bit better what's behind it.",
            "I would say OK."
        ],
        [
            "Here is yet another version of this where I have for reasons of space I put teaching minus one directly, and that corresponds to a locality.",
            "Improved kernel used by a bunch of people, again for the recognition of translation initial initiation sites in biology.",
            "Again, that is a specific case of rational kernels.",
            "I could the list is long, right?",
            "I could I could give you every one of the kernels that you have seen in practice and show you the transducer that corresponds to everyone of the sequence kernels, including parsing kernels, tree kernels, OK."
        ],
        [
            "Alright, so but then the question that arises here naturally is the following.",
            "What is a set of languages that are linearly separable using rational kernels?",
            "And now if I fix the particular rational kernel, what is a set of languages that I can use with that particular fixed rational kernel?",
            "And when is it that linear separation with rational kernels actually guarantees a positive margin?",
            "So margin theory tells us that if we can.",
            "Separate if you have a separation with a non zero margin, that's good generalization.",
            "But just because you have linear separation.",
            "It doesn't mean that you have a margin.",
            "This might be counter intuitive, but just because you have linear separation, the margin is not zero.",
            "It is not necessarily non 0.",
            "And then the other.",
            "The last question that comes up is how do we define transducers?",
            "That sort of guarantee a positive margin?",
            "OK, so all of these questions come into matter in some sense.",
            "From this.",
            "From a second you have these tools.",
            "This questions are the basic questions that you would like to answer right?",
            "We want to understand what is it that first of all people are trying to.",
            "What languages are they trying to learn in practice?",
            "That's on one view of it.",
            "Another view is from another point of view in this workshop.",
            "Probably what languages can I learn if I want to do linear separation?",
            "So I'm going to try to give some answers to these questions, but I'm happy to leave many questions unresolved for you guys to investigate later."
        ],
        [
            "Because I think it's a.",
            "There's a lot of interesting questions here to look at, so this new new part of this talk is the linear separability.",
            "Questions with rational kernels.",
            "So to."
        ],
        [
            "Talk about this I first I'm first going to talk about some basic notions of related to automata, and that goes back to 1963 or 1971, with Rabin's definition of probabilistic automata.",
            "Um?",
            "And Wichita Tomaten is set to be probabilistic if it has no negative wait.",
            "OK, and if the weight of the outgoing transitions sums to one at every state, now be careful because as much as this is a classical definition and you know I'm sure all of us have seen this in undergrad classes, this is different from another definition of probabilistic automaton automata, which is probably one that is more widely used.",
            "The most widely used definition of public companies, the one that says that the sum of the weights of all the strings.",
            "Is 1.",
            "But some of the ways of all the path of that, probably sick, There's one.",
            "That's the way the one that is used in statistical language modeling.",
            "For example, this is different.",
            "This is saying that the sum of the weights of outgoing transitions is 1, so it defines in fact the conditional probability.",
            "It says that basically from any state there is some given that you're in a state.",
            "There is some probability of going to some other state following a bunch of transitions.",
            "Just to be clear about this, 'cause that's important.",
            "And then.",
            "And now this definition is not anymore that of Michael Rabin.",
            "I think although it's very related to the kind of things that have been discussing, a language is set to be stochastic or even are stochastic because one could define other fields for that if and only if there existed probabilistic automaton, sort of this kind, such that this set of strings of this language, or those that are of the form that verify a of X more than Lambda.",
            "OK, instead of all strings whose weights it's at least Lambda, you could almost.",
            "If you want to make the connection between these two worlds, think of Lambda as being some sort of a margin here, right?"
        ],
        [
            "Now there is a theorem in formal language theory, which is that of turakina and not very well known, and I could prove it without too much pain.",
            "But I think you know it's nice if we just put it here.",
            "It says the following if you have awaited automaton arbitrary way to determine S, there exists some see some constancy that could be quite large such that the SFX the value associated by S to the string X is C to the power of X.",
            "Done.",
            "Some some parenthesis which contains the bees are probabilistic automaton.",
            "The value of public on minus 1 / N + 3 and is a constant.",
            "It corresponds to the number of states of the automaton.",
            "So basically what this theorem does is that it is relating in arbitrary weighted automaton.",
            "S. To a probabilistic automaton.",
            "It is saying now that if you have this set of strings, a language that is defined by F of X positive right but instead of strings whose values by this way to determine is positive.",
            "Then S is stochastic.",
            "OK, because that means that this quantity here is positive.",
            "It's quite nice because it's a nice observation, right?",
            "You could suddenly we see that hey, people have done some work in a formal language theory related to these things and something we can use them for the purpose of the definitions that we had.",
            "OK, so I'm going to use that."
        ],
        [
            "I'm going to use that to actually give a characterization of this set of languages that can be linearly separated by rational kernels.",
            "In fact, the theorem says that it is that language is linearly separable by rational kernel if and only if that at T -- 1 if and only if it is stochastic.",
            "It's a strong result because, you know, as I said, people in the practice are doing all kinds of things, trying to learn languages in biology, all kinds of other places.",
            "Or we might wish to just use this.",
            "Paradigm of mapping strings to high dimensional space and trying to do linear separation.",
            "Well, this is what you can learn.",
            "OK. And the proof is not very difficult actually.",
            "Now that we've done this extra work before, using to recognize algorithm theorem.",
            "So suppose that L is this look.",
            "I'm just going to this proof and nothing else, so don't be worried too much.",
            "Suppose that the language L is stochastic.",
            "We can assume that there is a weighted sum.",
            "It S such that I can write L as the sum of X such that F of X is positive.",
            "So.",
            "Now suppose that X zero is some element of L. Since it's an element of L, SX0 is positive, too, right?",
            "And now I'm going to define a very simple transducer, the one that takes the automaton S and Maps it to epsilon.",
            "Every string is mapped to epsilon.",
            "Kind of simple than that.",
            "Then in that case, and I'm going to find in the kernel K that is defined by T -- 1.",
            "Then if you look at TT minus one of XX0.",
            "Since there is only one way to match the output of T with the input of T -- 1, that's with epsilons.",
            "This is exactly equal to T of X epsilon times T -- 1 of epsilon X0.",
            "And so, by definition it is equal to CFX, CFX 0.",
            "Right now, since S of X0 is positive.",
            "The set of string such that F of X is positive is the same as the set of string such that T -- 1 of XX0 is positive.",
            "Right, but teaching minus one of XX0 is this is K of X60, so I've actually shown.",
            "Linear separation OK with just one string.",
            "Actually I just need one string at 0, so the set of string is just a set of kicks X zero OK being positive.",
            "The convo"
        ],
        [
            "The converse for the IAA.",
            "OK.",
            "So let me see if this is going to work, because sometimes I have this problem with this new keynote and it goes away without too much pain and.",
            "Otherwise, from now, and I could sort of had I not prepared slides and I said I did it at the last minute, I could have claimed here.",
            "Look my I don't have anymore slides, it doesn't work.",
            "But but it turns out I did finish present preparing them, right?",
            "You laugh, it sounds like it's something you've done in the past.",
            "So so so it did work.",
            "So here is let's let's do the converse.",
            "Now suppose that L is a linearly separable language right?",
            "In other words, it means that there are some A1A M. And so such that sum of Alpha IKEA vex I X + B positive characterizes the language right?",
            "So let's see what some of our key of XX is equal to.",
            "Well, some of my character XI can rewrite by definition and some of our fi S of remember auto excise.",
            "The automaton defined by the string XI composed with decomposed material composed with RBC, right?",
            "And now I'm going to use the linearity of S that we put it out before so I can take all of these sums inside so it becomes the sum the S of the some of these composed with decomposed material composed with this.",
            "Let's call this one an automaton a right away to terminate.",
            "In fact, this weighted automata is acyclic, by the way.",
            "OK, so now it becomes a survey composed with capacity management composed with other X, and now let me just project this weighted transducer on the output.",
            "It is not going to change at all.",
            "The weight of all the path.",
            "I just want to find a weighted automaton projected.",
            "I'm gonna call it, call it R. Now it's really it's a var composed with out of X and that is exactly our of X is the value that are associated to the string X.",
            "And now they find another way to determine U which is R + V. That's straightforward.",
            "Then the language is precisely the set of X such that FX is strictly positive, which bite or Canis theorem is.",
            "Stochastic.",
            "This is interesting.",
            "Is this theorem is actually constructive?",
            "OK. Alright, so.",
            "So that brings you to to considering stochastic languages, yes.",
            "Beasley, Constance is offsets that you need in linear separation, right?",
            "No, it's not.",
            "The margin is the offset.",
            "The offset from zero.",
            "How much of that there's a hyperplane the the the beat that corresponds to that?",
            "Alright, so."
        ],
        [
            "So here are some examples.",
            "That I thought you might like because first of all, let me bring you to thinking that stochastic languages are important languages here.",
            "For linear separation, first of all.",
            "They are crucial for linear suppression with kernels, but what are they?",
            "What do they cover?",
            "So actually they are not following Trump Chomsky's hierarchy.",
            "But then again, who cares about jumps his car key, right?",
            "I mean for years people have suffered from the errors of Chomsky, why should one care but also Chomsky's hierarchy, right?",
            "So so maybe one should care about stochastic languages much more because maybe they matter a lot more.",
            "OK, I'm going to give you some examples.",
            "This language, the first one here is stochastic.",
            "Is obviously not a regular language, it is, I think.",
            "Can someone say from the top of his head?",
            "I think it's a context sensitive language, OK?",
            "This one is a little more complex.",
            "A to the M beta, the beta the MN is not stochastic.",
            "I'm not particularly better.",
            "We're interested in doing sort of the formal language theory exercises for you.",
            "Here we can go and try to study many of those by hand and look at these languages and find out.",
            "It's just to give you some examples right?",
            "I don't think it's necessarily relevant to whether natural language processing or computational biology to look at these.",
            "These are languages, it's just a good homework.",
            "But one that is interesting, I think, is that the language of palindromes that won't be known as a context free language.",
            "Well, this one is stochastic.",
            "It's quite interesting actually to see how you can show this.",
            "A few years ago we define this simple weighted automata operated transducer.",
            "That is that I think you should know about.",
            "Regardless, it's a nice one.",
            "It's the one that takes a sequence of Asian bees.",
            "Well, you can interpret the Asus being zero and the bees as one, and now it's a binary sequence.",
            "It takes that sequence and it gives you the integer value of that sequence.",
            "It's kind of nice, right?",
            "It takes a sequence of binary sequence.",
            "It gives you the integer value.",
            "What does it mean?",
            "It gives you the integer value.",
            "It means that if I compose that sequence with this way to the top 10 and some of the weights of the other path, that is the value of that binary sequence in base two, you can do it in other bases in the same way.",
            "Ah, but now if you have a string that is a palindrome, that means that its weight, its integer value.",
            "When I read it, left to right, should be equal to its integer value left right to left.",
            "Reading right to left is not an issue for a weighted automaton, you can just reverse this weighted automaton and it would just do that for you.",
            "No need to reverse the string.",
            "OK, now it tells you why you can learn use define palindromes this way because the if you apply this way to minus its reverse, that is also weighted automaton.",
            "It actually gives a value of 0 to everyone of the palindromes.",
            "There's a little bit more of work to do here, because we didn't define stochastic languages as those languages such that there is a way to socialize with zero to all of its strings.",
            "But now I'm going to use the fact that these are all integers.",
            "By using a Lambda that is not an integer, you can show that that is also a stochastic language OK.",
            "There are many other interesting languages that are stochastic languages.",
            "In fact, if they were not interesting, why would people try to learn with kernels, right?"
        ],
        [
            "Now this was all for the general family of kernels, right?",
            "General family of rational kernels.",
            "What it tells you is if I give you this family of kernels, what what kind of languages you can learn.",
            "But now what if, what about a particular can I give you a fixed rational kernel?",
            "And I want to know what I can learn with it.",
            "OK. That's even more if I could characterize this all beautiful.",
            "But here I need your help because I've done.",
            "Maybe you know the first step there are.",
            "There's a.",
            "There's a marathon to do that, so I'm going to say the rest is yours.",
            "But here's what you can see already.",
            "Suppose first of all that you're in a simple case where the rational kernel has finite range.",
            "What I mean by this is that T. Of XY.",
            "Text T takes values in a finite set if you like.",
            "So in other words, OK for a particular.",
            "So the set of deity of XY is finite.",
            "The set of checks checks were OK.",
            "If that is the case.",
            "If you have a rational kernel, that is that has finite range.",
            "Then you consider the language is linearly separable by this rational kernel if and only if it is a finite Boolean combination.",
            "The finite Boolean combination for those of you here are formal language theory training people.",
            "That seems a very natural thing, right?",
            "It's a finite Boolean combination of these pre image sets.",
            "This pre image is what is a set of strings X such that Y of exploit is equal to V. OK, so this is a very so, so this seems to be very abstract.",
            "Right now I'm just going to give you a couple of examples to illustrate the theorem."
        ],
        [
            "For example, suppose that you are dealing with subsequence kernels.",
            "These are kernels that are mapping the strings to the set of its sub sequences.",
            "Then then the preimage language in that case is what's also known as the shuffle ideal of a string.",
            "Is this kind of sets Sigma star?",
            "Why one Sigma star, etc.",
            "Yny one INR the symbols defining Y?",
            "So then that just tells you that the set of languages learned by a subsequence kernel is actually the set of Boolean combinations, final Boolean combinations of shuffle ideals.",
            "But that actually is a theorem that we have proven separately elsewhere.",
            "We did not prove it this way, but now everything matches.",
            "This actually corresponds to piecewise testable languages, OK?",
            "Another set of another example to illustrate this.",
            "If you look at factor kernels or mismatched kernels, some people have also called them in biology.",
            "Well there are the pre image sets are just a set of Sigma star.",
            "Why Sigma start a set of strings that contain a particular substring Y OK and finite Boolean combinations of those would correspond to this kind of kernels, so we have.",
            "As I said though, this is really a first step.",
            "There is a lot more to say to characterize to understand what languages are.",
            "Or learn with this specific kernel, but this is a first step."
        ],
        [
            "Now let me go to some sort of more the learning theory aspect of that.",
            "So.",
            "Let me re emphasize the first bullet item.",
            "Linear separation does not guarantee non zero margin.",
            "This might seem counter intuitive, but you have to think infinite dimensional space.",
            "In infinite dimension, if you just have a hyperplane, if you have vector W separating all the points, it doesn't mean that it doesn't guarantee a non zero margin because points could be arbitrarily close to that hyperplane.",
            "OK even if you have a good separation.",
            "However, there is a theorem that says that if the kernel key has finite range, then actually necessarily you have positive margin.",
            "So I should how much more do I have?",
            "I should stop now, so I'm just going to stop almost here.",
            "Then I would have a lot more to say.",
            "But so basically.",
            "So let me just almost wrap it up in a couple of minutes.",
            "If you give me that first, I want to point out this fact that if you have a transducer with finite range then it guarantees a kernel with finite rigid guarantees.",
            "You have positive margin.",
            "It's important because then there is a standard margin property that tells you that."
        ],
        [
            "You know?",
            "With high probability, the generalization error of your hypothesis would vary as big of 1 / M * R squared over Rose Rose rose squared, where rule is the best margin you would have for that particular concept.",
            "OK, so the margin is not going to be 0 if you have a. I've already talked about piecewise testable languages.",
            "I'm just going to finish with a couple of nodes.",
            "One is that."
        ],
        [
            "It turns out that these transducers, this kernels with finite range or crucial because not only they help you learn languages, but on top of that they guarantee positive margin.",
            "The question that arises immediately is can I build mappings that have this nice property?",
            "How do I come up with kernels that guarantee positive margin?",
            "In some cases we have done this.",
            "This is for example on the left hand side transducer that takes it, takes any input and Maps it to the set of its sub sequences.",
            "From that we have constructed another transducer and it's not trivial.",
            "It is not trivial to do this.",
            "We have constructed another transducer that Maps.",
            "This same string to the set of its subsequences, but only one of them.",
            "Every subsequences of appearing only once, so that guarantees that the wait is going to be at most one, which means that it's a finite range transducer.",
            "Here is another example."
        ],
        [
            "As we have done for unigram transducers, there's a lot more to say about this."
        ],
        [
            "I'm going to conclude by saying that.",
            "Is a characterization of linear linearly separable languages as?",
            "Stochastic languages this should really be an invitation to going and studying these languages much more carefully and understanding their connection with what people have actually tried to learn in practice.",
            "Rational kernels with finite range seems to seem to have remarkable properties.",
            "The positive margin aspect that I mentioned already.",
            "The fact that they lead to languages that are finite Boolean combinations of preimages, and perhaps there is the beginning of a technique to take a transducer that is not finite range and turn it into something that has finite range, which could correspond to coming with the transducer that you think is relevant to your particular task, and we would turn into something that would guarantee you positive margin.",
            "Stopping here, the rest is for you to investigate.",
            "Yes.",
            "Tips for getting good generalization properties?",
            "Obviously having positive margins are important, but if the margin is is positive but in some sense exponentially small like you might still have of course.",
            "The margin.",
            "And you get an idea of what the world circumstances.",
            "The margin is like, not just positive, but OK.",
            "So let's let's go back to just some fundamental things here in from the learning POV, right?",
            "If you are dealing with a concept classic class of languages that has infinite VC dimension.",
            "Pack learning is not possible.",
            "So we should not be able to expect something like this.",
            "No need to even just you know, resort to strong theorems to saying this kind of this obvious infinite VC dimension.",
            "No PAC learning but.",
            "The fact that there is no PAC learning does not say that learning is not possible.",
            "What it says is that in fact you might have a learning might be possible with a with a constant if you like.",
            "This depends on your distribution or your concept, your particular concept you're learning.",
            "This is what this is guaranteeing.",
            "It's telling you look if you are learning a concept that has a good margin.",
            "I'm gonna give you actually a good guarantee for generalization, but I cannot guarantee you uniformly over all possible concepts.",
            "A good margin, because if I were doing this, I would guarantee you packpack learning.",
            "That's impossible.",
            "I know it's impossible, at least for non trivial set of languages, so this is basically what it says.",
            "Let me point out, by the way that.",
            "Are interesting things to say, but Infinite VC dimension in general, even when it comes to structural risk minimization, right?",
            "'cause even in such cases you could think that the family of hypothesis that you consider in structural risk mutation has overall infinite VC dimension, But every one of those that you pick has finite VC dimension.",
            "The learning guarantees that you get in such cases for structural risk.",
            "Minimization is a guarantee that is going to depend on the particular distribution, so it's the same context.",
            "Same thing, I actually want to think of it as we could even do here things that are related to directly special risk minimization actually.",
            "So I think there is a very deep connection between these two here.",
            "This is the best we can do I think.",
            "If.",
            "If you don't have, but if you don't have any constraint on how the margins scales of time, you say you have some family representation classes.",
            "Well, it doesn't matter with most end states.",
            "Oh then then then everything is going to be very simple.",
            "Then we can give actually just found out that I have with a bounded number of states is not going to have infinite VC dimension.",
            "Everything is going to be very straightforward there.",
            "Then we can give very strong guarantees their true true in those cases.",
            "Definitely yes.",
            "Do you get these very small margins anyway?",
            "Rather long strings so that is kind of so you would want to think that you could get these small margins for difficult concept class concepts with many States and many complicated things, but I'm not going to take any risk here and I'm going to leave it to you to investigate this, but I believe that it is something like this should be true.",
            "That's true.",
            "That's also what I think.",
            "Right?",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, so I'm with the Courant Institute at New York University and also with Google Research and this is joint work with Carina Quartus and Ari or Leo Qatar which who is at the Wiseman Institute.",
                    "label": 1
                },
                {
                    "sent": "Let me start by saying that I've actually just finished preparing these slides just a few minutes ago, so have some mercy have given other type this morning that for which I also had to prefer to slice it last minute.",
                    "label": 0
                },
                {
                    "sent": "So, but this is a workshop.",
                    "label": 0
                },
                {
                    "sent": "I guess that's fine.",
                    "label": 0
                },
                {
                    "sent": "The main purpose for me would be here to sort of invite you to look at certain ways of learning languages, which perhaps is different from what people have tried.",
                    "label": 0
                },
                {
                    "sent": "In the past, and since it's again a workshop to invite you to discuss this more and use this as an inspiration.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me start with a very general paradigm that people have used in the past for learning.",
                    "label": 0
                },
                {
                    "sent": "Well, perhaps not general languages, but learning finite automata.",
                    "label": 0
                },
                {
                    "sent": "One can think of them as being the sort of state merging or state splitting paradigm, and that is common to a number of research papers that you probably have all seen in the past.",
                    "label": 0
                },
                {
                    "sent": "The work of Dana Angluin on CNET, Al, Run at other.",
                    "label": 0
                },
                {
                    "sent": "The list is very long.",
                    "label": 0
                },
                {
                    "sent": "And essentially this consists of doing the following.",
                    "label": 0
                },
                {
                    "sent": "You start with an atomic time or a tree accepting all the examples which would then correspond to that, and then with the finest partition.",
                    "label": 1
                },
                {
                    "sent": "Because each state would then represent a block of your equivalence class, and then iteratively you would start merging these states.",
                    "label": 0
                },
                {
                    "sent": "If this is the merging one technique, and so you would, and these states would then correspond to partition blocks while preserving some congruence.",
                    "label": 1
                },
                {
                    "sent": "OK, that you have to come with.",
                    "label": 0
                },
                {
                    "sent": "And then you keep doing this.",
                    "label": 0
                },
                {
                    "sent": "And at the end, once you cannot merge states anymore while preserving that congruence while you said this is the automaton that I have learned OK, and in some sense the entire algorithm is directly defined by the choice of the Congress that you come with, essentially a whole family of algorithms could be defined this way.",
                    "label": 0
                },
                {
                    "sent": "Come with your congruence done.",
                    "label": 0
                },
                {
                    "sent": "That's the algorithm.",
                    "label": 0
                },
                {
                    "sent": "I mean something similar can be done about the reverse, where you start with a one state automaton and you start splitting it according to again a congruence.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the standard state merging or splitting power.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm the talk, I'm and here's an example of that.",
                    "label": 0
                },
                {
                    "sent": "This is, for example, for zero reversible languages and the work of Dana Angluin Zero reversible languages are those languages that can be accepted by an automaton that is both deterministic as you read it left to right, but also right to left OK. And here, for example you start.",
                    "label": 0
                },
                {
                    "sent": "With a tree representing the set of strings that you have seen.",
                    "label": 0
                },
                {
                    "sent": "And you keep merging these States and this is what you get at the end.",
                    "label": 0
                },
                {
                    "sent": "Here I've been lazy with a double arrow represent arrows that are going both directions and essentially what you get here is a set of strings with an even number of Asian bees.",
                    "label": 0
                },
                {
                    "sent": "So this is what you have learned starting from the finite set that you had at the beginning, OK?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the work that I'm going to present is really departing from that standard paradigm in the following sense that it is now seeking to map strings to a high dimensional feature space.",
                    "label": 1
                },
                {
                    "sent": "And then look, you know, find a separating hyperplane in that space that would define your strings OK.",
                    "label": 1
                },
                {
                    "sent": "So basically, strings that will be on one side of that hyperplane, or of this nonlinear mapping if you're going.",
                    "label": 0
                },
                {
                    "sent": "If I is a nonlinear mapping to some feature space would define the strings of your language.",
                    "label": 0
                },
                {
                    "sent": "OK, so in some sense this is not surprising to many of you here who are familiar with kernel methods, and you would say that in some sense this is just almost like using kernel methods to learn languages.",
                    "label": 0
                },
                {
                    "sent": "And then of course, the first question that comes up is what kernels should I choose?",
                    "label": 0
                },
                {
                    "sent": "How do I define the similarity between these sequences?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if you think about it, there are of course many different ways of defining sequences similarity between sequences.",
                    "label": 0
                },
                {
                    "sent": "One can come up with actually very complicated ways of doing that.",
                    "label": 0
                },
                {
                    "sent": "The the language or the strength of the machines that you're using to define those similarities could be context free, context sensitive, but in some sense if you go beyond a certain point, the whole idea of kernels is lost, right?",
                    "label": 0
                },
                {
                    "sent": "Because kernels are supposed to be efficient to compute.",
                    "label": 0
                },
                {
                    "sent": "So once you go to already, the level of context free grammars, you've already lost quite a lot of that because not only it's not even context free parsing that you need to do is really something that corresponds to combining together to two algebraic series.",
                    "label": 0
                },
                {
                    "sent": "This is quite costly, OK?",
                    "label": 0
                },
                {
                    "sent": "Alright, so then, on the other hand, if you look at what people have done in the past, not to learn languages, but really to do classification or regression related to strings or sequences, here is what they've been looking at.",
                    "label": 0
                },
                {
                    "sent": "There are.",
                    "label": 0
                },
                {
                    "sent": "They've been using ngram kernels, kernels that are based on counting N grams in a sequence, gappy engrams tree kernels for parsing.",
                    "label": 1
                },
                {
                    "sent": "Moment kernels, which are kernels that we have been looking at, which sort of take into account, not just accounts, but also the moments, other moments of the counts.",
                    "label": 0
                },
                {
                    "sent": "Locality improved kernels used by ZN.",
                    "label": 1
                },
                {
                    "sent": "It Alan that it Al includes a bunch of names including Bernard, chopped off and any others that I cannot remember.",
                    "label": 0
                },
                {
                    "sent": "Eyes a mismatch kernels that have been used by defined by Christina Leslie for also in the context of computational biology.",
                    "label": 0
                },
                {
                    "sent": "So it's a bunch of kernels that people have used and that one way or another to them has represented what they think they knew about that task.",
                    "label": 0
                },
                {
                    "sent": "But one thing that is common to all these kernels is that.",
                    "label": 0
                },
                {
                    "sent": "They are all.",
                    "label": 0
                },
                {
                    "sent": "Rational kernels.",
                    "label": 0
                },
                {
                    "sent": "OK, there are special instances of rational kernels.",
                    "label": 1
                },
                {
                    "sent": "In fact, in general I have not still found an example of a kernel sequence kernel used in practice weather in natural language processing, computational biology, or other context that is not a rational kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, now what are rational kernels?",
                    "label": 0
                },
                {
                    "sent": "You could say?",
                    "label": 0
                },
                {
                    "sent": "And I'm going to you're about to ask this.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Well, I'm going to just define them.",
                    "label": 0
                },
                {
                    "sent": "I'm going to precede.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We define this and this is what this talk.",
                    "label": 1
                },
                {
                    "sent": "How did stock is going to be structured so?",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about rational kernels and define what they correspond to.",
                    "label": 1
                },
                {
                    "sent": "But in order for me to introduce rational kernels, I first need to talk about weighted transducers for two reasons.",
                    "label": 0
                },
                {
                    "sent": "One sort of because the definition of rational kernels is based on weighted transducers, but also because if you want to talk a little bit about proofs and about so that you get a little bit more of a familiarity with what these correspond to, we need to talk about them.",
                    "label": 0
                },
                {
                    "sent": "So after having introduced rational kernels, I will then ask a number of questions.",
                    "label": 0
                },
                {
                    "sent": "About learning with the learning languages with rational kernels, and this brings you to study the linear separability.",
                    "label": 1
                },
                {
                    "sent": "Of sets of sequences or languages using rational kernels and now then give a number of results related to that.",
                    "label": 0
                },
                {
                    "sent": "So let me start with weighted transducers, perhaps for some of you this would be elementary.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But I think it's good for us to all have the same terminology if nothing else.",
                    "label": 0
                },
                {
                    "sent": "So here is an example of a weighted transducer.",
                    "label": 0
                },
                {
                    "sent": "It is a directed graph.",
                    "label": 0
                },
                {
                    "sent": "I'm going to denote by Bolt Circle the initial States and by a double circle the final states.",
                    "label": 0
                },
                {
                    "sent": "It is a directed graph where each edge or called transition is labeled with both an input label.",
                    "label": 0
                },
                {
                    "sent": "You're A and output label.",
                    "label": 0
                },
                {
                    "sent": "Here B and some weight that is indicated here after the slash symbol, which here is 0.1 OK. And the final state also is assigned some weight final states.",
                    "label": 0
                },
                {
                    "sent": "There might be several of them.",
                    "label": 0
                },
                {
                    "sent": "Some weight here is 0.1 indicated after after the state #3, which is the weight that you take as you leave the machine.",
                    "label": 0
                },
                {
                    "sent": "OK, how do you use a weighted transducer?",
                    "label": 0
                },
                {
                    "sent": "What is used in the following way?",
                    "label": 0
                },
                {
                    "sent": "The weight associated by the transducer T to the pair of strings X&Y?",
                    "label": 0
                },
                {
                    "sent": "Is this some of the weights of all accepted path with input X and output Y?",
                    "label": 1
                },
                {
                    "sent": "So the the the the input label of a path is the string that you obtained by concatenating input labels.",
                    "label": 0
                },
                {
                    "sent": "So here ABB for example OK and similarly with the output labels.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There are two operations.",
                    "label": 0
                },
                {
                    "sent": "One is the sum to sum up the weights of all the path with input X and output Y and one is product.",
                    "label": 0
                },
                {
                    "sent": "Because we the weight of a path is obtained by multiplying the weights along that path.",
                    "label": 0
                },
                {
                    "sent": "OK, so in particular here for example, the transmit transducer associated stupid to repair ABB BAA, the following weight.",
                    "label": 0
                },
                {
                    "sent": "Because there are two paths labeled with the pair ABBA one is ABB Cups.",
                    "label": 0
                },
                {
                    "sent": "And on the output you can see that it's BA, so that's the top path, another one is.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "ABB, right on the input and similarly with the output cable.",
                    "label": 0
                },
                {
                    "sent": "So there are two paths and you can see that the the weight is obtained.",
                    "label": 0
                },
                {
                    "sent": "The weight of each path is obtained by multiplying the weight of that path, including the .1 that you obtain for the final state, and then the two weights are summed up OK. Now this specific case of weighted transducers is known as weighted automata.",
                    "label": 0
                },
                {
                    "sent": "This is the particular case where input and output label of every.",
                    "label": 0
                },
                {
                    "sent": "Transition is the same.",
                    "label": 0
                },
                {
                    "sent": "OK, so in other words it will be BB or AA or BB on every transition.",
                    "label": 0
                },
                {
                    "sent": "The input and output label would be the same.",
                    "label": 0
                },
                {
                    "sent": "And when that that is the case, then to be concise.",
                    "label": 0
                },
                {
                    "sent": "There's no reason to keep around the two.",
                    "label": 0
                },
                {
                    "sent": "It suffices to keep just one label.",
                    "label": 0
                },
                {
                    "sent": "And it's also not necessary anymore to talk about the value of the transducer for repair, because really, when the pairs that are for which there is a non 0 value are those that have the Form XX.",
                    "label": 0
                },
                {
                    "sent": "So this is also the notation is then.",
                    "label": 0
                },
                {
                    "sent": "Reduced to being just T of X instead of being T of expire OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now there are certain number of operations with weighted transducers that I'm going to quickly tell you about one.",
                    "label": 0
                },
                {
                    "sent": "Is this some so it turns out that if you have two weighted transducers T1 and T2, there's some of these transducers which is defined as such as the transducer that associated to X&Y is some of the values associated by T1 and T-22 XY.",
                    "label": 0
                },
                {
                    "sent": "Is also a weighted transducer.",
                    "label": 0
                },
                {
                    "sent": "In fact, you can obtain it very simply.",
                    "label": 0
                },
                {
                    "sent": "Here are two weighted transducers, right?",
                    "label": 0
                },
                {
                    "sent": "The one that corresponds to the sum.",
                    "label": 0
                },
                {
                    "sent": "You can also think of it as a union.",
                    "label": 0
                },
                {
                    "sent": "It suffices to create a new state zero and asociates and create Epson Epson transitions epsilon.",
                    "label": 0
                },
                {
                    "sent": "Here for me is the empty word OK or the null work so?",
                    "label": 0
                },
                {
                    "sent": "Create 2 transitions to what used to be the initial state of these machines.",
                    "label": 0
                },
                {
                    "sent": "And the second you have done that, now you can read everything that you could read with the first machine and everything that you could read with the second machine and then therefore any XY that could be labeled that would correspond to a path here is also.",
                    "label": 0
                },
                {
                    "sent": "You could also have its weight here, so by definition you have the sum.",
                    "label": 0
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The product of two transducers product and perhaps I could also say concatenation is defined as such.",
                    "label": 0
                },
                {
                    "sent": "T 1 * T two of XY.",
                    "label": 0
                },
                {
                    "sent": "Is this some over all possible ways of decomposing the string X into a prefix X1 Anna suffix X2?",
                    "label": 0
                },
                {
                    "sent": "And string wire into prefix why one suffix Y 2?",
                    "label": 0
                },
                {
                    "sent": "The sum over these of T1 of X1Y1T2 of X2Y2.",
                    "label": 0
                },
                {
                    "sent": "Now actually, the in this case the machine is even easier to understand that maybe the formula.",
                    "label": 0
                },
                {
                    "sent": "Because all that you do 2 from 2 transducers to compute the product or the concatenation is simply to connect the final states of the first machine to the initial states of the second machine.",
                    "label": 0
                },
                {
                    "sent": "With absolute Epson transitions, but since you have a final wait here, that final weight is going to become the weight of these SNS on transitions.",
                    "label": 0
                },
                {
                    "sent": "OK, this extremely simple and that's why the product of two transducers corresponds to.",
                    "label": 0
                },
                {
                    "sent": "Now when the transducer T2 coincides with T1.",
                    "label": 0
                },
                {
                    "sent": "This is also abbreviated into T1 squared, and similarly if you multiply a T1 by itself many times, it becomes T 1 to the power of N.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The fundamental operation.",
                    "label": 0
                },
                {
                    "sent": "So another operation would be the closure or also known as the Kleene closure, right?",
                    "label": 0
                },
                {
                    "sent": "And similarly here you can compute.",
                    "label": 0
                },
                {
                    "sent": "From I think I don't, it's not from these two.",
                    "label": 0
                },
                {
                    "sent": "It's from another way transducer I've been too lazy here with this slides.",
                    "label": 0
                },
                {
                    "sent": "Just copying what I had in the previous one.",
                    "label": 0
                },
                {
                    "sent": "You can compute this one from the first one, essentially by doing the same as what you would do in the case of a single unweighted automaton.",
                    "label": 0
                },
                {
                    "sent": "But the most important operation here is known as composition.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And consists of taking two transducers T1 and T2.",
                    "label": 0
                },
                {
                    "sent": "And composing them together in the following way.",
                    "label": 0
                },
                {
                    "sent": "The weight associated to explain now is this.",
                    "label": 0
                },
                {
                    "sent": "Some of T1 of XG times T2 of ZY over all possible strings Z.",
                    "label": 0
                },
                {
                    "sent": "The way to understand this is that the transducer T1 is a functional view that is going to help you understand the transducer T one is going to map the string X to a bunch of string Z, many of them right?",
                    "label": 0
                },
                {
                    "sent": "The transducer T2 is going to take those strings E and map them to some other 2 two Y. OK so in how many different ways you can have this matching of outputs of T14X that map the input of T2 OK and that's what the sum is over all the all these matching these.",
                    "label": 0
                },
                {
                    "sent": "OK, there's another view of this.",
                    "label": 0
                },
                {
                    "sent": "And it is the one that corresponds to the case where the number of Z's, here in the middle, would be finite.",
                    "label": 0
                },
                {
                    "sent": "If you think about this, in that case, this operation is simply.",
                    "label": 0
                },
                {
                    "sent": "Can someone say here?",
                    "label": 0
                },
                {
                    "sent": "What it corresponds to.",
                    "label": 0
                },
                {
                    "sent": "It simply metrics multiplication, right?",
                    "label": 0
                },
                {
                    "sent": "If the number of.",
                    "label": 0
                },
                {
                    "sent": "Hi, in that case if the number of XYZ czar is finite, this composition is actually just precisely matrix multiplication.",
                    "label": 0
                },
                {
                    "sent": "Now this is of course, in the finite case, we don't have the finite case here.",
                    "label": 0
                },
                {
                    "sent": "The number of these could be actually infinite.",
                    "label": 0
                },
                {
                    "sent": "And, uh.",
                    "label": 0
                },
                {
                    "sent": "In addition to this, the infinite sets of these are rational sets, regular sets, so there is structure here.",
                    "label": 0
                },
                {
                    "sent": "Now let me quickly give you an idea of how to compose two transducers T1 and T2 to obtain another one.",
                    "label": 0
                },
                {
                    "sent": "If you have two transducers, T1 and T2 represent as such this states of the composition can be identified with pairs.",
                    "label": 0
                },
                {
                    "sent": "Where 0110121 where the first element of the pair is the state number in the first machine and the second one is state number in the second machine.",
                    "label": 0
                },
                {
                    "sent": "And what you do is that you start from the initial states.",
                    "label": 0
                },
                {
                    "sent": "Here 00 the pair 00 and then you ask.",
                    "label": 0
                },
                {
                    "sent": "Is there a transition leaving this state zero whose output label matches the input label of a transition matching leaving 0 here?",
                    "label": 0
                },
                {
                    "sent": "When you find them, you construct a new transducer with input a, an output B here, and whose weight is multiply.",
                    "label": 0
                },
                {
                    "sent": "Multiply multiplication of the two ways .1 times .1 here .01.",
                    "label": 0
                },
                {
                    "sent": "And which state do you reach?",
                    "label": 0
                },
                {
                    "sent": "You state that the pair that correspond to the state you would have reached using these matching transitions and you go on and this is the way the algorithm works.",
                    "label": 0
                },
                {
                    "sent": "It's quite simple as you can see in the very very worst case where the all the transitions of the first machine match all the transition of the second machine you are actually going to create something that is quadratic.",
                    "label": 0
                },
                {
                    "sent": "In fact, you're going to obtain all the possible states as well, and so in the very worst case this would be quadratic in the size of the first machine times aside.",
                    "label": 0
                },
                {
                    "sent": "Size of the second machine.",
                    "label": 0
                },
                {
                    "sent": "In practice, for most reasonable cases this is far from being the case.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so let me also introduce something else about weighted.",
                    "label": 0
                },
                {
                    "sent": "Transducers are not simply the sum of the weights of all the path of a way to transducer.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I am going to denote by capital SFT the some of the weights.",
                    "label": 0
                },
                {
                    "sent": "In other words, the sum of the accepted path, the weights of the accepted path where value of Pi pies await Python path that is accepted times the weight of the final state that it's reaching an of Pi is the end of the path and role of that state is the final weight of that state?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just the sum of the weights of all the path.",
                    "label": 0
                },
                {
                    "sent": "And actually, it can be computed in a very efficient way using a generalization of standard shortest distance algorithms to from the tropical semiring.",
                    "label": 0
                },
                {
                    "sent": "The mean plus it hearing to the plus times.",
                    "label": 0
                },
                {
                    "sent": "Another name for it that probably is more familiar to you is simply the forward backward algorithm.",
                    "label": 0
                },
                {
                    "sent": "When the graph is acyclic, you can just use the forward backward algorithm, but it's not.",
                    "label": 0
                },
                {
                    "sent": "There are more complications.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's very simple.",
                    "label": 0
                },
                {
                    "sent": "You can easily compute the sum of the ways of all the path.",
                    "label": 0
                },
                {
                    "sent": "I had some properties the some of the weights of T 1 + T two is just the sum of the weights of T1 plus some of T2.",
                    "label": 0
                },
                {
                    "sent": "If you multiply a transducer T by some Lambda, that means like multiplying its final, the weight of its final weights by Lambda.",
                    "label": 0
                },
                {
                    "sent": "That's the same as doing Lambda times the sum of the weights trivial.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "This one is just as simple, but it's important this some of the weights of Lambda T1 composed with T2.",
                    "label": 0
                },
                {
                    "sent": "Is also simply Lambda SFT, one composed with teacher and this is very clear from the definition as well.",
                    "label": 0
                },
                {
                    "sent": "I gave in to.",
                    "label": 0
                },
                {
                    "sent": "Huh?",
                    "label": 0
                },
                {
                    "sent": "Can you pull out Lambda as well?",
                    "label": 0
                },
                {
                    "sent": "Can you put can I put it on T2?",
                    "label": 0
                },
                {
                    "sent": "You mean yes I can do that as well.",
                    "label": 0
                },
                {
                    "sent": "Yes, I'm just getting lazy.",
                    "label": 0
                },
                {
                    "sent": "You could also put it on this one, right?",
                    "label": 0
                },
                {
                    "sent": "So yes, it's you know you have all these linearity.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this was all beautiful.",
                    "label": 0
                },
                {
                    "sent": "Let me now give you a first intuition of why weighted transducers are good for you.",
                    "label": 0
                },
                {
                    "sent": "They're good for you because in a variety of cases where you wish to define a similarity between two sequences.",
                    "label": 0
                },
                {
                    "sent": "These similarities are based on some sort of statistics on some counts.",
                    "label": 0
                },
                {
                    "sent": "I'm count some some, some substrings, right?",
                    "label": 0
                },
                {
                    "sent": "It turns out weighted transducers can count for you.",
                    "label": 0
                },
                {
                    "sent": "OK, here is how they count.",
                    "label": 0
                },
                {
                    "sent": "Suppose that you have a string capital X, which here could be just for example AB or more generally, let X be a an automaton, an arbitrarily finite automaton representing some regular expression that you're interested in.",
                    "label": 1
                },
                {
                    "sent": "Then this transducer, the one that is reduced to one state with a zombies map to the empty string with weight 1.",
                    "label": 0
                },
                {
                    "sent": "All the weights are one here in this transducer.",
                    "label": 0
                },
                {
                    "sent": "Followed by that automaton or string mapped to itself.",
                    "label": 0
                },
                {
                    "sent": "Followed by again a self loops that are basically erasing self loops.",
                    "label": 0
                },
                {
                    "sent": "They everything they read their race.",
                    "label": 0
                },
                {
                    "sent": "OK well this weighted automata Norway transducer rather is taking some input.",
                    "label": 0
                },
                {
                    "sent": "So say that your input string is E. It is going to count the number of times that X appears in Z.",
                    "label": 0
                },
                {
                    "sent": "And the number of times that X appears in Z is going to be exactly.",
                    "label": 0
                },
                {
                    "sent": "The sum of the weights of the path of Z composed with this transducer.",
                    "label": 0
                },
                {
                    "sent": "That's very nice, right?",
                    "label": 0
                },
                {
                    "sent": "You?",
                    "label": 0
                },
                {
                    "sent": "You have strings E but you some text.",
                    "label": 0
                },
                {
                    "sent": "And you compose it with this transducer.",
                    "label": 0
                },
                {
                    "sent": "It has just two states basically.",
                    "label": 0
                },
                {
                    "sent": "I mean if you forget about the states of X.",
                    "label": 0
                },
                {
                    "sent": "And then you take the sum of the weights of all the path that does it for you.",
                    "label": 0
                },
                {
                    "sent": "How let me explain on an example how it works.",
                    "label": 0
                },
                {
                    "sent": "It would not be more difficult on a in the general case to explain it.",
                    "label": 0
                },
                {
                    "sent": "Suppose that your input string is this one.",
                    "label": 0
                },
                {
                    "sent": "And suppose that the string that you want to count in this text is a D. What this transducer does is essentially.",
                    "label": 0
                },
                {
                    "sent": "Here, taking care of some prefix instead 0 instead one taking care of themselves some suffix.",
                    "label": 0
                },
                {
                    "sent": "Well, how does it work?",
                    "label": 0
                },
                {
                    "sent": "It reads some prefix and erases it Maps it to epsilon, then it reads AB here or the string that you want.",
                    "label": 0
                },
                {
                    "sent": "Then it reads some suffix that is just erasing.",
                    "label": 0
                },
                {
                    "sent": "But how many different ways can you go from that state?",
                    "label": 0
                },
                {
                    "sent": "0 to the state one?",
                    "label": 0
                },
                {
                    "sent": "Every time that you wish to read some prefix and then eventually go to state one, you have to have seen the string AB.",
                    "label": 0
                },
                {
                    "sent": "And that's the only way that you can go to the state one to a final state.",
                    "label": 0
                },
                {
                    "sent": "So the only the number of path that successful path that you're going to have is going to be exactly the number of times that the string AB occurs in the input string Z.",
                    "label": 0
                },
                {
                    "sent": "And in this case you see this.",
                    "label": 0
                },
                {
                    "sent": "It reads the transducer reads from state one some prefix an Maps it to epsilon, then with a beta goes to state one, then Maps everything to epsilon.",
                    "label": 0
                },
                {
                    "sent": "Similarly, here it reads some other prefix here, then a B, then epsilon OK.",
                    "label": 0
                },
                {
                    "sent": "So it happens two path.",
                    "label": 0
                },
                {
                    "sent": "Essentially the weight of these two path is just one because all the weights are one, so adding them together because that's that's what this S does.",
                    "label": 0
                },
                {
                    "sent": "This is some of the weights of the path that just gives you exactly the weight.",
                    "label": 0
                },
                {
                    "sent": "The count of this string AB in the input Z, and things can be completely generalized using the same transducer, in other words.",
                    "label": 0
                },
                {
                    "sent": "Once again, if instead of the string AB, Capital X was an arbitrary automaton representing an arbitrary regular expression, it would be counting all the strings accepted by that automaton.",
                    "label": 0
                },
                {
                    "sent": "And even more, it could be generalized if Z was not a string, was what was it, but in fact was awaited automaton.",
                    "label": 0
                },
                {
                    "sent": "It would just be then computing the expected counts of these strings in that weighted automata.",
                    "label": 0
                },
                {
                    "sent": "It's very, very general.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, but that's the generality that I need because to now give you just already an example of what you can do with this.",
                    "label": 0
                },
                {
                    "sent": "The same this transducer can be used to, for example count bigrams.",
                    "label": 0
                },
                {
                    "sent": "OK, so you remember a few minutes ago I gave an example of an engram kernel people have been using in practice for biology or natural language processing.",
                    "label": 0
                },
                {
                    "sent": "One of the simplest one of them is an ngram model, so a background model is one that so this transducer, what it does is that it's mapping every input string.",
                    "label": 0
                },
                {
                    "sent": "To the set of bigrams that that string contains.",
                    "label": 0
                },
                {
                    "sent": "That is a a ABB ABB.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "If the if I reduce the alphabet to something manageable then I can write on the slide OK. And more Interestingly, then it is going to count the number of occurrences of every one of these bigrams in the input string Z.",
                    "label": 0
                },
                {
                    "sent": "You don't have to explicitly go and write it because when you compose these two transverses it's giving you an implicit compact representation of the counts.",
                    "label": 0
                },
                {
                    "sent": "But if you want to explicitly counts AB for example, again composi with T with a B computer, some of the weight of the path and you have that count OK. Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now that we have done this introduction to Wakefield transducers, let me now talk about what rational kernels are and why they become relevant then.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Irrational kernel key which we can view as a similarity measure between two strings X&Y.",
                    "label": 0
                },
                {
                    "sent": "I would say it Colonel Kelly sweat K is rational when actually it can be written as you have XY when you is a weighted transducer.",
                    "label": 0
                },
                {
                    "sent": "Very simple definition, right?",
                    "label": 0
                },
                {
                    "sent": "A kernel is rational whenever it corresponds basically to a weighted transducer.",
                    "label": 1
                },
                {
                    "sent": "OK. Now it's very nice because as we said before, using composition and shortest distance, you can actually compute care of XY because they have expire is the short is S of.",
                    "label": 0
                },
                {
                    "sent": "I am going to denote by Autofix as as the automaton linear automaton representing X. Compose with you compose without of Y and then summing the weights of all the path.",
                    "label": 0
                },
                {
                    "sent": "And that again because we said previously that composition is a quadratic algorithm.",
                    "label": 0
                },
                {
                    "sent": "The complexity of this is just the length of X times the length of Y.",
                    "label": 0
                },
                {
                    "sent": "You here is a constant right?",
                    "label": 1
                },
                {
                    "sent": "It's fixed on once and for all.",
                    "label": 1
                },
                {
                    "sent": "And you can actually even do things in a faster, more efficient way in specific cases, if you know something specific about you, OK?",
                    "label": 0
                },
                {
                    "sent": "It's quite simple.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let me now say another result that is interesting about rational kernels, and that's the fact that if you denote by key minus one, the weighted transducer you obtain from T by swapping the input and output label of every transition.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 1
                },
                {
                    "sent": "It turns out that the transducer you defined by T composed with T -- 1.",
                    "label": 0
                },
                {
                    "sent": "Defines a positive definite symmetric kernel.",
                    "label": 1
                },
                {
                    "sent": "Always let me give you another thing.",
                    "label": 0
                },
                {
                    "sent": "Another fact previously I said that every one of the string kernels I have seen in practice and competition by your natural language processing the correspond to rational kernels.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be even more specific there.",
                    "label": 0
                },
                {
                    "sent": "All rational kernels of this kind.",
                    "label": 0
                },
                {
                    "sent": "OK, they all can be written as teaching minus one.",
                    "label": 0
                },
                {
                    "sent": "Now one can show that any as I said, anything like this is positive and symmetric.",
                    "label": 0
                },
                {
                    "sent": "But just to give you an intuition of what this corresponds to.",
                    "label": 0
                },
                {
                    "sent": "Here is why.",
                    "label": 0
                },
                {
                    "sent": "The transducer T you can think of it as a mapping.",
                    "label": 0
                },
                {
                    "sent": "It is taking the input string and is mapping it to a set of strings and regular language OK with some weights associated to each one of those strings.",
                    "label": 0
                },
                {
                    "sent": "Similarly, so that's what T20 is applied to X.",
                    "label": 0
                },
                {
                    "sent": "Similarly, when you apply T2Y.",
                    "label": 0
                },
                {
                    "sent": "It is mapped to a whole set of strings in that some regular language, and there are some weights associated to those.",
                    "label": 0
                },
                {
                    "sent": "If you take the dot product of those vectors with which have which are infinite dimensional vectors, each one of them having the weights, the transducer associated to those strings.",
                    "label": 0
                },
                {
                    "sent": "That is exactly what corresponds to doing the composition that I just described.",
                    "label": 0
                },
                {
                    "sent": "That is exactly what corresponds to teaching minus one.",
                    "label": 0
                },
                {
                    "sent": "OK, another way of saying it is that if you're familiar with kernels and you if you whenever you get a kernel Ku thinking about the some feature mapping file that corresponds to this, you can almost think of T as being defy.",
                    "label": 0
                },
                {
                    "sent": "The only difference is that Phi does not explicitly write a feature vector.",
                    "label": 0
                },
                {
                    "sent": "Instead it is giving you a compact representation of a high dimensional vector OK?",
                    "label": 0
                },
                {
                    "sent": "So hopefully I've given a little bit of the intuition behind this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now previously I said that you can define a bigram kernel with just three states.",
                    "label": 1
                },
                {
                    "sent": "Well, people have used for text categorization.",
                    "label": 0
                },
                {
                    "sent": "What they have called a gappy bigram successfully, and without more states, we can extend that transducer to not count bigrams, because gappy bigrams because we can just add a self loop here, which penalizes by some penalty factor Lambda the gap between the first symbol of a diagram and the second symbol.",
                    "label": 1
                },
                {
                    "sent": "And since by the observation that I made earlier, any one of these can count what's in the middle, this is the middle.",
                    "label": 0
                },
                {
                    "sent": "This is the capital X that I had in the middle.",
                    "label": 0
                },
                {
                    "sent": "You can count also gapi backgrounds using exactly the same things in exactly the same operation.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another example of what you would see in practice is the mismatch kernel that I refer to earlier on.",
                    "label": 0
                },
                {
                    "sent": "And here is the example for not going to go into the details of this obviously, but here are the parameters that are defining this particular mismatch.",
                    "label": 0
                },
                {
                    "sent": "Kernel 3 and 2 two is the levels of three is how many the length of the window.",
                    "label": 0
                },
                {
                    "sent": "And again, that has been successfully used for computational biology, and this is actually what it corresponds to.",
                    "label": 0
                },
                {
                    "sent": "If you write it with weighted transducers, it is a particular case of teaching minus one rational kernel.",
                    "label": 0
                },
                {
                    "sent": "This is the team that corresponds to it.",
                    "label": 0
                },
                {
                    "sent": "OK, in some sense you see a little bit better what's behind it.",
                    "label": 0
                },
                {
                    "sent": "I would say OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is yet another version of this where I have for reasons of space I put teaching minus one directly, and that corresponds to a locality.",
                    "label": 0
                },
                {
                    "sent": "Improved kernel used by a bunch of people, again for the recognition of translation initial initiation sites in biology.",
                    "label": 1
                },
                {
                    "sent": "Again, that is a specific case of rational kernels.",
                    "label": 0
                },
                {
                    "sent": "I could the list is long, right?",
                    "label": 0
                },
                {
                    "sent": "I could I could give you every one of the kernels that you have seen in practice and show you the transducer that corresponds to everyone of the sequence kernels, including parsing kernels, tree kernels, OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so but then the question that arises here naturally is the following.",
                    "label": 0
                },
                {
                    "sent": "What is a set of languages that are linearly separable using rational kernels?",
                    "label": 1
                },
                {
                    "sent": "And now if I fix the particular rational kernel, what is a set of languages that I can use with that particular fixed rational kernel?",
                    "label": 1
                },
                {
                    "sent": "And when is it that linear separation with rational kernels actually guarantees a positive margin?",
                    "label": 0
                },
                {
                    "sent": "So margin theory tells us that if we can.",
                    "label": 0
                },
                {
                    "sent": "Separate if you have a separation with a non zero margin, that's good generalization.",
                    "label": 0
                },
                {
                    "sent": "But just because you have linear separation.",
                    "label": 0
                },
                {
                    "sent": "It doesn't mean that you have a margin.",
                    "label": 0
                },
                {
                    "sent": "This might be counter intuitive, but just because you have linear separation, the margin is not zero.",
                    "label": 0
                },
                {
                    "sent": "It is not necessarily non 0.",
                    "label": 0
                },
                {
                    "sent": "And then the other.",
                    "label": 0
                },
                {
                    "sent": "The last question that comes up is how do we define transducers?",
                    "label": 0
                },
                {
                    "sent": "That sort of guarantee a positive margin?",
                    "label": 0
                },
                {
                    "sent": "OK, so all of these questions come into matter in some sense.",
                    "label": 0
                },
                {
                    "sent": "From this.",
                    "label": 0
                },
                {
                    "sent": "From a second you have these tools.",
                    "label": 0
                },
                {
                    "sent": "This questions are the basic questions that you would like to answer right?",
                    "label": 1
                },
                {
                    "sent": "We want to understand what is it that first of all people are trying to.",
                    "label": 0
                },
                {
                    "sent": "What languages are they trying to learn in practice?",
                    "label": 0
                },
                {
                    "sent": "That's on one view of it.",
                    "label": 0
                },
                {
                    "sent": "Another view is from another point of view in this workshop.",
                    "label": 0
                },
                {
                    "sent": "Probably what languages can I learn if I want to do linear separation?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to try to give some answers to these questions, but I'm happy to leave many questions unresolved for you guys to investigate later.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because I think it's a.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of interesting questions here to look at, so this new new part of this talk is the linear separability.",
                    "label": 1
                },
                {
                    "sent": "Questions with rational kernels.",
                    "label": 0
                },
                {
                    "sent": "So to.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk about this I first I'm first going to talk about some basic notions of related to automata, and that goes back to 1963 or 1971, with Rabin's definition of probabilistic automata.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And Wichita Tomaten is set to be probabilistic if it has no negative wait.",
                    "label": 1
                },
                {
                    "sent": "OK, and if the weight of the outgoing transitions sums to one at every state, now be careful because as much as this is a classical definition and you know I'm sure all of us have seen this in undergrad classes, this is different from another definition of probabilistic automaton automata, which is probably one that is more widely used.",
                    "label": 0
                },
                {
                    "sent": "The most widely used definition of public companies, the one that says that the sum of the weights of all the strings.",
                    "label": 0
                },
                {
                    "sent": "Is 1.",
                    "label": 0
                },
                {
                    "sent": "But some of the ways of all the path of that, probably sick, There's one.",
                    "label": 0
                },
                {
                    "sent": "That's the way the one that is used in statistical language modeling.",
                    "label": 0
                },
                {
                    "sent": "For example, this is different.",
                    "label": 1
                },
                {
                    "sent": "This is saying that the sum of the weights of outgoing transitions is 1, so it defines in fact the conditional probability.",
                    "label": 0
                },
                {
                    "sent": "It says that basically from any state there is some given that you're in a state.",
                    "label": 0
                },
                {
                    "sent": "There is some probability of going to some other state following a bunch of transitions.",
                    "label": 0
                },
                {
                    "sent": "Just to be clear about this, 'cause that's important.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "And now this definition is not anymore that of Michael Rabin.",
                    "label": 0
                },
                {
                    "sent": "I think although it's very related to the kind of things that have been discussing, a language is set to be stochastic or even are stochastic because one could define other fields for that if and only if there existed probabilistic automaton, sort of this kind, such that this set of strings of this language, or those that are of the form that verify a of X more than Lambda.",
                    "label": 0
                },
                {
                    "sent": "OK, instead of all strings whose weights it's at least Lambda, you could almost.",
                    "label": 0
                },
                {
                    "sent": "If you want to make the connection between these two worlds, think of Lambda as being some sort of a margin here, right?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now there is a theorem in formal language theory, which is that of turakina and not very well known, and I could prove it without too much pain.",
                    "label": 0
                },
                {
                    "sent": "But I think you know it's nice if we just put it here.",
                    "label": 0
                },
                {
                    "sent": "It says the following if you have awaited automaton arbitrary way to determine S, there exists some see some constancy that could be quite large such that the SFX the value associated by S to the string X is C to the power of X.",
                    "label": 0
                },
                {
                    "sent": "Done.",
                    "label": 0
                },
                {
                    "sent": "Some some parenthesis which contains the bees are probabilistic automaton.",
                    "label": 0
                },
                {
                    "sent": "The value of public on minus 1 / N + 3 and is a constant.",
                    "label": 1
                },
                {
                    "sent": "It corresponds to the number of states of the automaton.",
                    "label": 0
                },
                {
                    "sent": "So basically what this theorem does is that it is relating in arbitrary weighted automaton.",
                    "label": 0
                },
                {
                    "sent": "S. To a probabilistic automaton.",
                    "label": 0
                },
                {
                    "sent": "It is saying now that if you have this set of strings, a language that is defined by F of X positive right but instead of strings whose values by this way to determine is positive.",
                    "label": 0
                },
                {
                    "sent": "Then S is stochastic.",
                    "label": 0
                },
                {
                    "sent": "OK, because that means that this quantity here is positive.",
                    "label": 0
                },
                {
                    "sent": "It's quite nice because it's a nice observation, right?",
                    "label": 0
                },
                {
                    "sent": "You could suddenly we see that hey, people have done some work in a formal language theory related to these things and something we can use them for the purpose of the definitions that we had.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to use that.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to use that to actually give a characterization of this set of languages that can be linearly separated by rational kernels.",
                    "label": 0
                },
                {
                    "sent": "In fact, the theorem says that it is that language is linearly separable by rational kernel if and only if that at T -- 1 if and only if it is stochastic.",
                    "label": 1
                },
                {
                    "sent": "It's a strong result because, you know, as I said, people in the practice are doing all kinds of things, trying to learn languages in biology, all kinds of other places.",
                    "label": 0
                },
                {
                    "sent": "Or we might wish to just use this.",
                    "label": 0
                },
                {
                    "sent": "Paradigm of mapping strings to high dimensional space and trying to do linear separation.",
                    "label": 0
                },
                {
                    "sent": "Well, this is what you can learn.",
                    "label": 0
                },
                {
                    "sent": "OK. And the proof is not very difficult actually.",
                    "label": 0
                },
                {
                    "sent": "Now that we've done this extra work before, using to recognize algorithm theorem.",
                    "label": 0
                },
                {
                    "sent": "So suppose that L is this look.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to this proof and nothing else, so don't be worried too much.",
                    "label": 0
                },
                {
                    "sent": "Suppose that the language L is stochastic.",
                    "label": 1
                },
                {
                    "sent": "We can assume that there is a weighted sum.",
                    "label": 0
                },
                {
                    "sent": "It S such that I can write L as the sum of X such that F of X is positive.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now suppose that X zero is some element of L. Since it's an element of L, SX0 is positive, too, right?",
                    "label": 0
                },
                {
                    "sent": "And now I'm going to define a very simple transducer, the one that takes the automaton S and Maps it to epsilon.",
                    "label": 0
                },
                {
                    "sent": "Every string is mapped to epsilon.",
                    "label": 0
                },
                {
                    "sent": "Kind of simple than that.",
                    "label": 0
                },
                {
                    "sent": "Then in that case, and I'm going to find in the kernel K that is defined by T -- 1.",
                    "label": 0
                },
                {
                    "sent": "Then if you look at TT minus one of XX0.",
                    "label": 0
                },
                {
                    "sent": "Since there is only one way to match the output of T with the input of T -- 1, that's with epsilons.",
                    "label": 0
                },
                {
                    "sent": "This is exactly equal to T of X epsilon times T -- 1 of epsilon X0.",
                    "label": 0
                },
                {
                    "sent": "And so, by definition it is equal to CFX, CFX 0.",
                    "label": 0
                },
                {
                    "sent": "Right now, since S of X0 is positive.",
                    "label": 0
                },
                {
                    "sent": "The set of string such that F of X is positive is the same as the set of string such that T -- 1 of XX0 is positive.",
                    "label": 0
                },
                {
                    "sent": "Right, but teaching minus one of XX0 is this is K of X60, so I've actually shown.",
                    "label": 0
                },
                {
                    "sent": "Linear separation OK with just one string.",
                    "label": 0
                },
                {
                    "sent": "Actually I just need one string at 0, so the set of string is just a set of kicks X zero OK being positive.",
                    "label": 0
                },
                {
                    "sent": "The convo",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The converse for the IAA.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let me see if this is going to work, because sometimes I have this problem with this new keynote and it goes away without too much pain and.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, from now, and I could sort of had I not prepared slides and I said I did it at the last minute, I could have claimed here.",
                    "label": 0
                },
                {
                    "sent": "Look my I don't have anymore slides, it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "But but it turns out I did finish present preparing them, right?",
                    "label": 0
                },
                {
                    "sent": "You laugh, it sounds like it's something you've done in the past.",
                    "label": 0
                },
                {
                    "sent": "So so so it did work.",
                    "label": 0
                },
                {
                    "sent": "So here is let's let's do the converse.",
                    "label": 0
                },
                {
                    "sent": "Now suppose that L is a linearly separable language right?",
                    "label": 1
                },
                {
                    "sent": "In other words, it means that there are some A1A M. And so such that sum of Alpha IKEA vex I X + B positive characterizes the language right?",
                    "label": 0
                },
                {
                    "sent": "So let's see what some of our key of XX is equal to.",
                    "label": 0
                },
                {
                    "sent": "Well, some of my character XI can rewrite by definition and some of our fi S of remember auto excise.",
                    "label": 0
                },
                {
                    "sent": "The automaton defined by the string XI composed with decomposed material composed with RBC, right?",
                    "label": 0
                },
                {
                    "sent": "And now I'm going to use the linearity of S that we put it out before so I can take all of these sums inside so it becomes the sum the S of the some of these composed with decomposed material composed with this.",
                    "label": 0
                },
                {
                    "sent": "Let's call this one an automaton a right away to terminate.",
                    "label": 0
                },
                {
                    "sent": "In fact, this weighted automata is acyclic, by the way.",
                    "label": 0
                },
                {
                    "sent": "OK, so now it becomes a survey composed with capacity management composed with other X, and now let me just project this weighted transducer on the output.",
                    "label": 0
                },
                {
                    "sent": "It is not going to change at all.",
                    "label": 0
                },
                {
                    "sent": "The weight of all the path.",
                    "label": 0
                },
                {
                    "sent": "I just want to find a weighted automaton projected.",
                    "label": 1
                },
                {
                    "sent": "I'm gonna call it, call it R. Now it's really it's a var composed with out of X and that is exactly our of X is the value that are associated to the string X.",
                    "label": 0
                },
                {
                    "sent": "And now they find another way to determine U which is R + V. That's straightforward.",
                    "label": 0
                },
                {
                    "sent": "Then the language is precisely the set of X such that FX is strictly positive, which bite or Canis theorem is.",
                    "label": 0
                },
                {
                    "sent": "Stochastic.",
                    "label": 0
                },
                {
                    "sent": "This is interesting.",
                    "label": 0
                },
                {
                    "sent": "Is this theorem is actually constructive?",
                    "label": 1
                },
                {
                    "sent": "OK. Alright, so.",
                    "label": 0
                },
                {
                    "sent": "So that brings you to to considering stochastic languages, yes.",
                    "label": 0
                },
                {
                    "sent": "Beasley, Constance is offsets that you need in linear separation, right?",
                    "label": 0
                },
                {
                    "sent": "No, it's not.",
                    "label": 0
                },
                {
                    "sent": "The margin is the offset.",
                    "label": 0
                },
                {
                    "sent": "The offset from zero.",
                    "label": 0
                },
                {
                    "sent": "How much of that there's a hyperplane the the the beat that corresponds to that?",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are some examples.",
                    "label": 0
                },
                {
                    "sent": "That I thought you might like because first of all, let me bring you to thinking that stochastic languages are important languages here.",
                    "label": 0
                },
                {
                    "sent": "For linear separation, first of all.",
                    "label": 0
                },
                {
                    "sent": "They are crucial for linear suppression with kernels, but what are they?",
                    "label": 0
                },
                {
                    "sent": "What do they cover?",
                    "label": 0
                },
                {
                    "sent": "So actually they are not following Trump Chomsky's hierarchy.",
                    "label": 0
                },
                {
                    "sent": "But then again, who cares about jumps his car key, right?",
                    "label": 0
                },
                {
                    "sent": "I mean for years people have suffered from the errors of Chomsky, why should one care but also Chomsky's hierarchy, right?",
                    "label": 0
                },
                {
                    "sent": "So so maybe one should care about stochastic languages much more because maybe they matter a lot more.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to give you some examples.",
                    "label": 0
                },
                {
                    "sent": "This language, the first one here is stochastic.",
                    "label": 1
                },
                {
                    "sent": "Is obviously not a regular language, it is, I think.",
                    "label": 0
                },
                {
                    "sent": "Can someone say from the top of his head?",
                    "label": 0
                },
                {
                    "sent": "I think it's a context sensitive language, OK?",
                    "label": 0
                },
                {
                    "sent": "This one is a little more complex.",
                    "label": 0
                },
                {
                    "sent": "A to the M beta, the beta the MN is not stochastic.",
                    "label": 1
                },
                {
                    "sent": "I'm not particularly better.",
                    "label": 0
                },
                {
                    "sent": "We're interested in doing sort of the formal language theory exercises for you.",
                    "label": 0
                },
                {
                    "sent": "Here we can go and try to study many of those by hand and look at these languages and find out.",
                    "label": 0
                },
                {
                    "sent": "It's just to give you some examples right?",
                    "label": 0
                },
                {
                    "sent": "I don't think it's necessarily relevant to whether natural language processing or computational biology to look at these.",
                    "label": 0
                },
                {
                    "sent": "These are languages, it's just a good homework.",
                    "label": 1
                },
                {
                    "sent": "But one that is interesting, I think, is that the language of palindromes that won't be known as a context free language.",
                    "label": 1
                },
                {
                    "sent": "Well, this one is stochastic.",
                    "label": 0
                },
                {
                    "sent": "It's quite interesting actually to see how you can show this.",
                    "label": 0
                },
                {
                    "sent": "A few years ago we define this simple weighted automata operated transducer.",
                    "label": 0
                },
                {
                    "sent": "That is that I think you should know about.",
                    "label": 0
                },
                {
                    "sent": "Regardless, it's a nice one.",
                    "label": 0
                },
                {
                    "sent": "It's the one that takes a sequence of Asian bees.",
                    "label": 1
                },
                {
                    "sent": "Well, you can interpret the Asus being zero and the bees as one, and now it's a binary sequence.",
                    "label": 0
                },
                {
                    "sent": "It takes that sequence and it gives you the integer value of that sequence.",
                    "label": 1
                },
                {
                    "sent": "It's kind of nice, right?",
                    "label": 0
                },
                {
                    "sent": "It takes a sequence of binary sequence.",
                    "label": 0
                },
                {
                    "sent": "It gives you the integer value.",
                    "label": 0
                },
                {
                    "sent": "What does it mean?",
                    "label": 0
                },
                {
                    "sent": "It gives you the integer value.",
                    "label": 0
                },
                {
                    "sent": "It means that if I compose that sequence with this way to the top 10 and some of the weights of the other path, that is the value of that binary sequence in base two, you can do it in other bases in the same way.",
                    "label": 0
                },
                {
                    "sent": "Ah, but now if you have a string that is a palindrome, that means that its weight, its integer value.",
                    "label": 0
                },
                {
                    "sent": "When I read it, left to right, should be equal to its integer value left right to left.",
                    "label": 0
                },
                {
                    "sent": "Reading right to left is not an issue for a weighted automaton, you can just reverse this weighted automaton and it would just do that for you.",
                    "label": 0
                },
                {
                    "sent": "No need to reverse the string.",
                    "label": 0
                },
                {
                    "sent": "OK, now it tells you why you can learn use define palindromes this way because the if you apply this way to minus its reverse, that is also weighted automaton.",
                    "label": 0
                },
                {
                    "sent": "It actually gives a value of 0 to everyone of the palindromes.",
                    "label": 0
                },
                {
                    "sent": "There's a little bit more of work to do here, because we didn't define stochastic languages as those languages such that there is a way to socialize with zero to all of its strings.",
                    "label": 0
                },
                {
                    "sent": "But now I'm going to use the fact that these are all integers.",
                    "label": 0
                },
                {
                    "sent": "By using a Lambda that is not an integer, you can show that that is also a stochastic language OK.",
                    "label": 0
                },
                {
                    "sent": "There are many other interesting languages that are stochastic languages.",
                    "label": 0
                },
                {
                    "sent": "In fact, if they were not interesting, why would people try to learn with kernels, right?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now this was all for the general family of kernels, right?",
                    "label": 0
                },
                {
                    "sent": "General family of rational kernels.",
                    "label": 0
                },
                {
                    "sent": "What it tells you is if I give you this family of kernels, what what kind of languages you can learn.",
                    "label": 0
                },
                {
                    "sent": "But now what if, what about a particular can I give you a fixed rational kernel?",
                    "label": 0
                },
                {
                    "sent": "And I want to know what I can learn with it.",
                    "label": 0
                },
                {
                    "sent": "OK. That's even more if I could characterize this all beautiful.",
                    "label": 0
                },
                {
                    "sent": "But here I need your help because I've done.",
                    "label": 0
                },
                {
                    "sent": "Maybe you know the first step there are.",
                    "label": 0
                },
                {
                    "sent": "There's a.",
                    "label": 0
                },
                {
                    "sent": "There's a marathon to do that, so I'm going to say the rest is yours.",
                    "label": 0
                },
                {
                    "sent": "But here's what you can see already.",
                    "label": 0
                },
                {
                    "sent": "Suppose first of all that you're in a simple case where the rational kernel has finite range.",
                    "label": 0
                },
                {
                    "sent": "What I mean by this is that T. Of XY.",
                    "label": 0
                },
                {
                    "sent": "Text T takes values in a finite set if you like.",
                    "label": 0
                },
                {
                    "sent": "So in other words, OK for a particular.",
                    "label": 0
                },
                {
                    "sent": "So the set of deity of XY is finite.",
                    "label": 0
                },
                {
                    "sent": "The set of checks checks were OK.",
                    "label": 0
                },
                {
                    "sent": "If that is the case.",
                    "label": 0
                },
                {
                    "sent": "If you have a rational kernel, that is that has finite range.",
                    "label": 0
                },
                {
                    "sent": "Then you consider the language is linearly separable by this rational kernel if and only if it is a finite Boolean combination.",
                    "label": 1
                },
                {
                    "sent": "The finite Boolean combination for those of you here are formal language theory training people.",
                    "label": 0
                },
                {
                    "sent": "That seems a very natural thing, right?",
                    "label": 1
                },
                {
                    "sent": "It's a finite Boolean combination of these pre image sets.",
                    "label": 0
                },
                {
                    "sent": "This pre image is what is a set of strings X such that Y of exploit is equal to V. OK, so this is a very so, so this seems to be very abstract.",
                    "label": 0
                },
                {
                    "sent": "Right now I'm just going to give you a couple of examples to illustrate the theorem.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example, suppose that you are dealing with subsequence kernels.",
                    "label": 1
                },
                {
                    "sent": "These are kernels that are mapping the strings to the set of its sub sequences.",
                    "label": 0
                },
                {
                    "sent": "Then then the preimage language in that case is what's also known as the shuffle ideal of a string.",
                    "label": 1
                },
                {
                    "sent": "Is this kind of sets Sigma star?",
                    "label": 0
                },
                {
                    "sent": "Why one Sigma star, etc.",
                    "label": 0
                },
                {
                    "sent": "Yny one INR the symbols defining Y?",
                    "label": 0
                },
                {
                    "sent": "So then that just tells you that the set of languages learned by a subsequence kernel is actually the set of Boolean combinations, final Boolean combinations of shuffle ideals.",
                    "label": 0
                },
                {
                    "sent": "But that actually is a theorem that we have proven separately elsewhere.",
                    "label": 0
                },
                {
                    "sent": "We did not prove it this way, but now everything matches.",
                    "label": 0
                },
                {
                    "sent": "This actually corresponds to piecewise testable languages, OK?",
                    "label": 1
                },
                {
                    "sent": "Another set of another example to illustrate this.",
                    "label": 0
                },
                {
                    "sent": "If you look at factor kernels or mismatched kernels, some people have also called them in biology.",
                    "label": 1
                },
                {
                    "sent": "Well there are the pre image sets are just a set of Sigma star.",
                    "label": 0
                },
                {
                    "sent": "Why Sigma start a set of strings that contain a particular substring Y OK and finite Boolean combinations of those would correspond to this kind of kernels, so we have.",
                    "label": 0
                },
                {
                    "sent": "As I said though, this is really a first step.",
                    "label": 0
                },
                {
                    "sent": "There is a lot more to say to characterize to understand what languages are.",
                    "label": 0
                },
                {
                    "sent": "Or learn with this specific kernel, but this is a first step.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let me go to some sort of more the learning theory aspect of that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let me re emphasize the first bullet item.",
                    "label": 0
                },
                {
                    "sent": "Linear separation does not guarantee non zero margin.",
                    "label": 1
                },
                {
                    "sent": "This might seem counter intuitive, but you have to think infinite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "In infinite dimension, if you just have a hyperplane, if you have vector W separating all the points, it doesn't mean that it doesn't guarantee a non zero margin because points could be arbitrarily close to that hyperplane.",
                    "label": 0
                },
                {
                    "sent": "OK even if you have a good separation.",
                    "label": 1
                },
                {
                    "sent": "However, there is a theorem that says that if the kernel key has finite range, then actually necessarily you have positive margin.",
                    "label": 0
                },
                {
                    "sent": "So I should how much more do I have?",
                    "label": 0
                },
                {
                    "sent": "I should stop now, so I'm just going to stop almost here.",
                    "label": 0
                },
                {
                    "sent": "Then I would have a lot more to say.",
                    "label": 0
                },
                {
                    "sent": "But so basically.",
                    "label": 0
                },
                {
                    "sent": "So let me just almost wrap it up in a couple of minutes.",
                    "label": 0
                },
                {
                    "sent": "If you give me that first, I want to point out this fact that if you have a transducer with finite range then it guarantees a kernel with finite rigid guarantees.",
                    "label": 1
                },
                {
                    "sent": "You have positive margin.",
                    "label": 0
                },
                {
                    "sent": "It's important because then there is a standard margin property that tells you that.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "With high probability, the generalization error of your hypothesis would vary as big of 1 / M * R squared over Rose Rose rose squared, where rule is the best margin you would have for that particular concept.",
                    "label": 0
                },
                {
                    "sent": "OK, so the margin is not going to be 0 if you have a. I've already talked about piecewise testable languages.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to finish with a couple of nodes.",
                    "label": 0
                },
                {
                    "sent": "One is that.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It turns out that these transducers, this kernels with finite range or crucial because not only they help you learn languages, but on top of that they guarantee positive margin.",
                    "label": 0
                },
                {
                    "sent": "The question that arises immediately is can I build mappings that have this nice property?",
                    "label": 0
                },
                {
                    "sent": "How do I come up with kernels that guarantee positive margin?",
                    "label": 0
                },
                {
                    "sent": "In some cases we have done this.",
                    "label": 0
                },
                {
                    "sent": "This is for example on the left hand side transducer that takes it, takes any input and Maps it to the set of its sub sequences.",
                    "label": 0
                },
                {
                    "sent": "From that we have constructed another transducer and it's not trivial.",
                    "label": 0
                },
                {
                    "sent": "It is not trivial to do this.",
                    "label": 0
                },
                {
                    "sent": "We have constructed another transducer that Maps.",
                    "label": 0
                },
                {
                    "sent": "This same string to the set of its subsequences, but only one of them.",
                    "label": 0
                },
                {
                    "sent": "Every subsequences of appearing only once, so that guarantees that the wait is going to be at most one, which means that it's a finite range transducer.",
                    "label": 0
                },
                {
                    "sent": "Here is another example.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we have done for unigram transducers, there's a lot more to say about this.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to conclude by saying that.",
                    "label": 0
                },
                {
                    "sent": "Is a characterization of linear linearly separable languages as?",
                    "label": 1
                },
                {
                    "sent": "Stochastic languages this should really be an invitation to going and studying these languages much more carefully and understanding their connection with what people have actually tried to learn in practice.",
                    "label": 1
                },
                {
                    "sent": "Rational kernels with finite range seems to seem to have remarkable properties.",
                    "label": 1
                },
                {
                    "sent": "The positive margin aspect that I mentioned already.",
                    "label": 0
                },
                {
                    "sent": "The fact that they lead to languages that are finite Boolean combinations of preimages, and perhaps there is the beginning of a technique to take a transducer that is not finite range and turn it into something that has finite range, which could correspond to coming with the transducer that you think is relevant to your particular task, and we would turn into something that would guarantee you positive margin.",
                    "label": 0
                },
                {
                    "sent": "Stopping here, the rest is for you to investigate.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Tips for getting good generalization properties?",
                    "label": 0
                },
                {
                    "sent": "Obviously having positive margins are important, but if the margin is is positive but in some sense exponentially small like you might still have of course.",
                    "label": 0
                },
                {
                    "sent": "The margin.",
                    "label": 0
                },
                {
                    "sent": "And you get an idea of what the world circumstances.",
                    "label": 0
                },
                {
                    "sent": "The margin is like, not just positive, but OK.",
                    "label": 0
                },
                {
                    "sent": "So let's let's go back to just some fundamental things here in from the learning POV, right?",
                    "label": 0
                },
                {
                    "sent": "If you are dealing with a concept classic class of languages that has infinite VC dimension.",
                    "label": 0
                },
                {
                    "sent": "Pack learning is not possible.",
                    "label": 0
                },
                {
                    "sent": "So we should not be able to expect something like this.",
                    "label": 0
                },
                {
                    "sent": "No need to even just you know, resort to strong theorems to saying this kind of this obvious infinite VC dimension.",
                    "label": 0
                },
                {
                    "sent": "No PAC learning but.",
                    "label": 0
                },
                {
                    "sent": "The fact that there is no PAC learning does not say that learning is not possible.",
                    "label": 0
                },
                {
                    "sent": "What it says is that in fact you might have a learning might be possible with a with a constant if you like.",
                    "label": 0
                },
                {
                    "sent": "This depends on your distribution or your concept, your particular concept you're learning.",
                    "label": 0
                },
                {
                    "sent": "This is what this is guaranteeing.",
                    "label": 0
                },
                {
                    "sent": "It's telling you look if you are learning a concept that has a good margin.",
                    "label": 0
                },
                {
                    "sent": "I'm gonna give you actually a good guarantee for generalization, but I cannot guarantee you uniformly over all possible concepts.",
                    "label": 0
                },
                {
                    "sent": "A good margin, because if I were doing this, I would guarantee you packpack learning.",
                    "label": 0
                },
                {
                    "sent": "That's impossible.",
                    "label": 0
                },
                {
                    "sent": "I know it's impossible, at least for non trivial set of languages, so this is basically what it says.",
                    "label": 0
                },
                {
                    "sent": "Let me point out, by the way that.",
                    "label": 0
                },
                {
                    "sent": "Are interesting things to say, but Infinite VC dimension in general, even when it comes to structural risk minimization, right?",
                    "label": 0
                },
                {
                    "sent": "'cause even in such cases you could think that the family of hypothesis that you consider in structural risk mutation has overall infinite VC dimension, But every one of those that you pick has finite VC dimension.",
                    "label": 0
                },
                {
                    "sent": "The learning guarantees that you get in such cases for structural risk.",
                    "label": 0
                },
                {
                    "sent": "Minimization is a guarantee that is going to depend on the particular distribution, so it's the same context.",
                    "label": 0
                },
                {
                    "sent": "Same thing, I actually want to think of it as we could even do here things that are related to directly special risk minimization actually.",
                    "label": 0
                },
                {
                    "sent": "So I think there is a very deep connection between these two here.",
                    "label": 0
                },
                {
                    "sent": "This is the best we can do I think.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "If you don't have, but if you don't have any constraint on how the margins scales of time, you say you have some family representation classes.",
                    "label": 0
                },
                {
                    "sent": "Well, it doesn't matter with most end states.",
                    "label": 0
                },
                {
                    "sent": "Oh then then then everything is going to be very simple.",
                    "label": 0
                },
                {
                    "sent": "Then we can give actually just found out that I have with a bounded number of states is not going to have infinite VC dimension.",
                    "label": 0
                },
                {
                    "sent": "Everything is going to be very straightforward there.",
                    "label": 0
                },
                {
                    "sent": "Then we can give very strong guarantees their true true in those cases.",
                    "label": 0
                },
                {
                    "sent": "Definitely yes.",
                    "label": 0
                },
                {
                    "sent": "Do you get these very small margins anyway?",
                    "label": 0
                },
                {
                    "sent": "Rather long strings so that is kind of so you would want to think that you could get these small margins for difficult concept class concepts with many States and many complicated things, but I'm not going to take any risk here and I'm going to leave it to you to investigate this, but I believe that it is something like this should be true.",
                    "label": 0
                },
                {
                    "sent": "That's true.",
                    "label": 0
                },
                {
                    "sent": "That's also what I think.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}