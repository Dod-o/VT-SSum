{
    "id": "jenc74omkk62sbdpfrt5xtv6zbit52ed",
    "title": "Divide and Conquer Kernel Ridge Regression",
    "info": {
        "author": [
            "Yuchen Zhang, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "Sept. 2, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_zhang_regression/",
    "segmentation": [
        [
            "I am you chain and this is a joint work with strong Duchenne Martin, right from UC Berkeley.",
            "Our start our work is a study of the conversation based approach to Kernel Ridge regression.",
            "An, as the title suggests, the main idea is to widen, conquer and we're able to get two aspects of result.",
            "First is that by using divide and conquer you can reduce the time and space complexity of performing kernel Ridge regression by substantial level and 2nd we can prove that the optimal statistical accuracy of kernel Ridge regression is preserved in this process.",
            "So for this talk I will first go through the background and some intuition quickly, and then I will present the main result."
        ],
        [
            "For the kernel regression, the problem is that you are given the samples of X&Y, and the goal is to estimate the function of F of X to be a estimator of Y, and we obtain this estimator by minimizing the population risk function, which is also the mean square error between FX&Y and.",
            "We also require that F belongs to some function space which is reproducing kernel Hilbert space and it is defined based on the kernel function K Anne.",
            "Any function belongs to this function space if and only if it can be represented by a linear combination of kernel.",
            "So by using a different choices of kernels you can get different structures of the function space.",
            "To do this optimization."
        ],
        [
            "Here's a quick review of the motivation and kernel regression, so the most the main limitation of linear regression is that only fits linear functions.",
            "So if you want to feed more complicated nonlinear function, a way of doing this is to map the input vector X into some higher dimensional space using a mapping function 5, and then you can learn a linear model with respect to 5X.",
            "So if Phi itself is nonlinear mapping, then the resulting function affect is nonlinear.",
            "That's the two figures suggest here and nonlinear surface in the original space can become a linear hyperplane after mapping into a higher dimensional space."
        ],
        [
            "Usually the mapping function 5 itself is hard to compute 'cause it is high dimensional or even infinite dimensional, but the kernel tricks that you don't have to compute exactly 5.",
            "You have to.",
            "You only compute the inner product of this mappings so it find this function KSK of X&X prime as the inner product of 5X and 5X prime an it is called the kernel function an defining a kernel implicitly defined a magnet function and memory function.",
            "Defines the function space from which you pick a function to fit your data.",
            "Of course there are many kernels that people are using practice and giving 3 examples here very typical.",
            "The first one is the polynomial kernel.",
            "By using it you can fit all the polynomial functions and 2nd is the Gaussian kernel.",
            "By using the Gaussian kernel you can fit all functions with infinite number of infinite differentiability.",
            "The infinite smoothness and the last one is the symbol of kernel and by using the server kernel you can solve kernel of order one.",
            "You can fit all functions that Scripture.",
            "Continue writing."
        ],
        [
            "So back to the setting of kernel regression becausw.",
            "We are given a finite number of data.",
            "We want to minimize empirical risk function instead of the population risk function and we also use the additive regularization term coefficient by Lambda to avoid overfitting.",
            "So this minimization problem is well known to have a close form solution.",
            "The estimator app hat is equal to a linear combination of kernels and the key step here is to compute the coefficient of linear combination which is Alpha and Alpha.",
            "In turn have a closed form solution.",
            "It is obtained by first converting the regularised kernel matrix and then multiply it by the response vector Y.",
            "So the kernel matrix is follows the standard definition.",
            "Is IDs entry is equal to evaluation of the kernel matrix using sample XI and XJ?"
        ],
        [
            "So big cause the computation of the coefficient involves inverting a matrix.",
            "So in the naive implementation it has the computation complexity of N square and cube and the space complexity of N ^2 N is the number of samples in your data set.",
            "So this could be prohibitively expensive if you're have a very large end, especially when it's greater than hundreds of thousands.",
            "So it makes the kernel Ridge regression not very easy to be scalable to to very large data.",
            "So people have developed fast ways to approach.",
            "Approach is to compute Kernel Ridge regression.",
            "The most popular way of doing this is based on the low rank matrix approximation.",
            "The basic intuition is that you approximate the kernel matrix by the product of 2 lower rank smaller matrices.",
            "The concrete examples of this approach includes.",
            "Kernel PCA, an incomplete roller skater compensation and Nystrom sampling."
        ],
        [
            "Another line of research is the iterative algorithm to optimize Alpha directly, an this includes gradient descent kind of method and the contractor gradient descent, and this method use early stopping rules instead of instead of regularization to avoid overfitting.",
            "But anyway, although this kind of approximation based method provides some efficiency to the computation of courage aggression, but the accuracy is still an issue cause it is not, it is not clear how the performance of this algorithm approximation based algorithm is compared with the exact algorithm of kernel Ridge regression.",
            "So in some cases it is possible that the user has to make some tradeoff between accuracy and efficiency.",
            "So our contribution in this work is that we show an algorithm.",
            "That preserve accuracy and efficiency at the same time, it means that our approach runs as fast as this approximation based approaches and at the same time it doesn't sacrifice any accuracy."
        ],
        [
            "So our main idea is a little different from low rank matrix matrix approximation, but it is very simple so big cause the most expensive step in kernel regression is to inverse the kernel matrix.",
            "We want to modify it to simplify it to make it easier to inverse.",
            "So our modification has two steps.",
            "The first one is to reshuffle the rows and columns of the kernel matrix based on some random permutation of the sample and the second step is to partition the.",
            "That the kernel matrix into small some smaller sub matrices an only keep the diagonal sub matrices and remove all the other interests and replace it by zero.",
            "So if we replace the original kernel matrix by this modified kernel matrix and we do the inversion on this modified matrix, then the inversion is reduced.",
            "The problem of reversing this smaller sub matrices which is substantially easier and we can show that as long as the size of the sub matrices are not too small.",
            "Then immersing this modified matrix still gives good results in terms of statistical accuracy, so although we have actually discarded most of the entries in the original kernel matrix, it turns out that essential information to do this kernel regression estimation is not lost in this process."
        ],
        [
            "So formally, we have this divide and conquer procedure.",
            "A three step.",
            "The first step is that we divide the data set into some smaller divided partitioning uniformly at random into an subsets, and the second step is that we take each of these subsets and we do.",
            "We compute the current Ridge regression estimator based on solely based on this subset.",
            "So it is the local estimation procedure.",
            "But one thing worth noting is that we use a special regularizer for this local problems.",
            "Use the regularization factor for this local problem, which is equal to the regularization we use for the global problem becausw.",
            "Usually the regularization coefficient goes smaller as your sample size grows larger and we use the global regularization for local problems.",
            "So it means that the local problem is under regularised, if it uses AV correct weather.",
            "So this regularization and regularization has two effects.",
            "First one is that it introduces a smaller bias because the bias is introduced by regularization and we use a weaker regularization and 2nd effect is that it causes larger variance be cause if you are using a vehicle regularization, you have more overfitting.",
            "So the variance is larger.",
            "But in the last step of the algorithm, the average or the local estimate to form a global estimator and cause this local estimators independent.",
            "It reduces the variance to low levels.",
            "Eventually you have a small bias and small variance, and their combination matches the optimal convergence rate.",
            "If you look at the complexity, most of the computation is in the second step, so it is very easy to verify that both the time and space complexity is reduced by a factor of N square.",
            "So if we can know the maximum value of M, which is the partition number, then we can plug it this number.",
            "This value of M into the formula of the complexities and we can get concrete complexity for this algorithm.",
            "So the next slide I will answer the question of.",
            "Which is the maximum M we can choose for this algorithm to preserve optimal accuracy?"
        ],
        [
            "So there's a general theorem that works for order Colonel classes, the upper bound, the estimation error by three terms, the first 2 terms that bias invariances that I mentioned it last slice.",
            "So here's a critical quantity, which is gamma Lambda.",
            "Lambda is the regularization coefficient and it is referred as the effective dimensionality in the previous literature, and it is depending on both the regularization factor and also depending on the decreasing rate of the eigenvalues and the kernel matrix.",
            "And so the key observation here is that in both 2 terms it is.",
            "They're not, they're not, they're independent to the partition number M. So all you have to do is to choose the regularization coefficient that make the tradeoff between bias and variance and the classical literature says that, well, this matches the optimal convergence rate.",
            "The only thing bothers is the third term that we see that it is exponentially decreasing term.",
            "So as long as you choose M that is not larger than.",
            "The number of samples divided by the effective dimensionality square.",
            "You can get this term if knowledgeably small comparing to the first 2 terms, so it doesn't hurt accuracy, so it gives us a path to determine the number M. First you choose a regularization coefficient, Lambda, and second, you compute the effective dimensionality based on Lambda and based on the problem and serve you choose your M. According to this former, it shouldn't be greater than.",
            "It should be smaller than the number of samples divided by the effective effective dimensionality squared.",
            "And finally, plugging this M into the complexity expressions and get the complexity for the algorithm."
        ],
        [
            "The next two slides shows some consequence to the general theorem.",
            "We apply it to three specific kernels and follow this path to see what is.",
            "How is the complexity is reduced.",
            "So the first example is the polynomial kernels for this kernel.",
            "It turns out that you can choose the number of partitions to be approximately equal to the number of samples divided by the log of number of samples, and then plugging this M you see that the computation complexity.",
            "Is reduced to something that is almost linear to end, which is nearly optimal and the space complexity is only having a logarithm dependence on the number of samples.",
            "And the same phenomenon is observed for the golf in kernels and still using a correct choice of up M and Lambda you can reduce the time complexity to almost linear to the number of samples and also the space complexity is logarithm Lee.",
            "Depending on the number of samples.",
            "Distribution you design points with these results.",
            "We do have some assumptions, but they are not very strong.",
            "We have some moments assumptions on our base functions of the kernel, like we assume that the case moment or the infinite or responded by the bounded is too strong assumption.",
            "So you can assume some moment assumption.",
            "Yeah, basically we need at least four moments on the basis functions of the kernel.",
            "This is not a sample."
        ],
        [
            "So for the situation is a little more complicated for the soul of kernels of smoothness, mubi 'cause there's extra parameter of mu.",
            "So recall that you're using a sub of kernel smoothness you you can feed all the functions that has new differentiability.",
            "So if using this kernel class then it turns out that most of time and space complexity is reduced to some polynomial of the sample size and power of the polynomial is determined by mu.",
            "And in the extreme case, if you assume that means very large means that you're feeding a very smooth function.",
            "Then again you get the time complexity is almost linear to end, and space complexity is depending on the logarithm of N."
        ],
        [
            "So at last I want to show a simulation study that gives more intuitive illustration of how our algorithm works.",
            "So we generate the data set by continuous but non smooth function.",
            "It is the minimum of X and Y -- X and we add a Gaussian noise to probate the data."
        ],
        [
            "Across the true function is Lipschitz continuous, so we can use the soul of kernel of smoothness, smoothness one to feed the data and we plot mean square error curve by increasing the sample size.",
            "An from this plot we see that.",
            "The number of partitions is smaller than 16.",
            "Then the performance of divide and Conquer's very close and specifically when M = 1, It means that there is no divide and conquer.",
            "So it is exactly the standard kernel regression and it is the optimal baseline.",
            "Actually, even if the number of even if the number of samples is equal to 64.",
            "But as the number of even if the partition number is 64 as the sample size increases, you can see that the performance get very close to the optimal baseline."
        ],
        [
            "And it is also meaningful to study the the maximum number of partitions we can use given a fixed sample size.",
            "So in this experiment we fixed the sample size and we draw the square error curve by varying the partition number.",
            "And it is observed that there is a stress hold in the plot below this threshold of partition number.",
            "The mean square error doesn't change and after that it started to increase.",
            "So by examine this probably see that this transport is a polynomial function of the sample size and more precisely it is N to the point at 45.",
            "So it confirms the theory that you can choose the in the kernel of smoothness.",
            "One you can choose a partition number.",
            "That is a polynomial function of the sample size."
        ],
        [
            "So let me quickly summarize the results.",
            "First we develop divide and conquer approach to encourage regression.",
            "It is very simple, but it reduces the complexity very substantially.",
            "And Secondly, we prove that it preserved by choosing the partition number correctly.",
            "It preserved the optimal statistical accuracy and we also apply the general theory to specific kernels will show that force of kernels.",
            "You can get acceleration rate.",
            "That is polynomial to the number of samples an for the finite rank kernels and Gaussian kernels you can get a time complexity that is almost linear to the number of samples."
        ],
        [
            "There are several open problems around to raise.",
            "The first one is how to get a partition numbering practice optimal partition number, cause in order theory, the computation of the partition number depends on the effective dimensionality of the problem, but in generally in general problem this computation could be expensive and 2nd.",
            "So remember that we actually sampled the original kernel matrix, we only keep the diagonal blocks.",
            "So natural question is what is the minimum number of entries you have to?",
            "Have to preserve in original kernel matrix to guarantee the optimal convergence rate, so this is lower bound and the last question is whether the divide and conquer approach works for more general kernel based methods like kernel SVM.",
            "Still this is not questioned the Council, so that's it.",
            "Thanks for attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I am you chain and this is a joint work with strong Duchenne Martin, right from UC Berkeley.",
                    "label": 0
                },
                {
                    "sent": "Our start our work is a study of the conversation based approach to Kernel Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "An, as the title suggests, the main idea is to widen, conquer and we're able to get two aspects of result.",
                    "label": 0
                },
                {
                    "sent": "First is that by using divide and conquer you can reduce the time and space complexity of performing kernel Ridge regression by substantial level and 2nd we can prove that the optimal statistical accuracy of kernel Ridge regression is preserved in this process.",
                    "label": 1
                },
                {
                    "sent": "So for this talk I will first go through the background and some intuition quickly, and then I will present the main result.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the kernel regression, the problem is that you are given the samples of X&Y, and the goal is to estimate the function of F of X to be a estimator of Y, and we obtain this estimator by minimizing the population risk function, which is also the mean square error between FX&Y and.",
                    "label": 0
                },
                {
                    "sent": "We also require that F belongs to some function space which is reproducing kernel Hilbert space and it is defined based on the kernel function K Anne.",
                    "label": 1
                },
                {
                    "sent": "Any function belongs to this function space if and only if it can be represented by a linear combination of kernel.",
                    "label": 0
                },
                {
                    "sent": "So by using a different choices of kernels you can get different structures of the function space.",
                    "label": 0
                },
                {
                    "sent": "To do this optimization.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's a quick review of the motivation and kernel regression, so the most the main limitation of linear regression is that only fits linear functions.",
                    "label": 1
                },
                {
                    "sent": "So if you want to feed more complicated nonlinear function, a way of doing this is to map the input vector X into some higher dimensional space using a mapping function 5, and then you can learn a linear model with respect to 5X.",
                    "label": 1
                },
                {
                    "sent": "So if Phi itself is nonlinear mapping, then the resulting function affect is nonlinear.",
                    "label": 0
                },
                {
                    "sent": "That's the two figures suggest here and nonlinear surface in the original space can become a linear hyperplane after mapping into a higher dimensional space.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Usually the mapping function 5 itself is hard to compute 'cause it is high dimensional or even infinite dimensional, but the kernel tricks that you don't have to compute exactly 5.",
                    "label": 1
                },
                {
                    "sent": "You have to.",
                    "label": 0
                },
                {
                    "sent": "You only compute the inner product of this mappings so it find this function KSK of X&X prime as the inner product of 5X and 5X prime an it is called the kernel function an defining a kernel implicitly defined a magnet function and memory function.",
                    "label": 1
                },
                {
                    "sent": "Defines the function space from which you pick a function to fit your data.",
                    "label": 1
                },
                {
                    "sent": "Of course there are many kernels that people are using practice and giving 3 examples here very typical.",
                    "label": 1
                },
                {
                    "sent": "The first one is the polynomial kernel.",
                    "label": 0
                },
                {
                    "sent": "By using it you can fit all the polynomial functions and 2nd is the Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "By using the Gaussian kernel you can fit all functions with infinite number of infinite differentiability.",
                    "label": 0
                },
                {
                    "sent": "The infinite smoothness and the last one is the symbol of kernel and by using the server kernel you can solve kernel of order one.",
                    "label": 0
                },
                {
                    "sent": "You can fit all functions that Scripture.",
                    "label": 0
                },
                {
                    "sent": "Continue writing.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So back to the setting of kernel regression becausw.",
                    "label": 0
                },
                {
                    "sent": "We are given a finite number of data.",
                    "label": 0
                },
                {
                    "sent": "We want to minimize empirical risk function instead of the population risk function and we also use the additive regularization term coefficient by Lambda to avoid overfitting.",
                    "label": 1
                },
                {
                    "sent": "So this minimization problem is well known to have a close form solution.",
                    "label": 1
                },
                {
                    "sent": "The estimator app hat is equal to a linear combination of kernels and the key step here is to compute the coefficient of linear combination which is Alpha and Alpha.",
                    "label": 0
                },
                {
                    "sent": "In turn have a closed form solution.",
                    "label": 0
                },
                {
                    "sent": "It is obtained by first converting the regularised kernel matrix and then multiply it by the response vector Y.",
                    "label": 1
                },
                {
                    "sent": "So the kernel matrix is follows the standard definition.",
                    "label": 0
                },
                {
                    "sent": "Is IDs entry is equal to evaluation of the kernel matrix using sample XI and XJ?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So big cause the computation of the coefficient involves inverting a matrix.",
                    "label": 0
                },
                {
                    "sent": "So in the naive implementation it has the computation complexity of N square and cube and the space complexity of N ^2 N is the number of samples in your data set.",
                    "label": 0
                },
                {
                    "sent": "So this could be prohibitively expensive if you're have a very large end, especially when it's greater than hundreds of thousands.",
                    "label": 0
                },
                {
                    "sent": "So it makes the kernel Ridge regression not very easy to be scalable to to very large data.",
                    "label": 0
                },
                {
                    "sent": "So people have developed fast ways to approach.",
                    "label": 0
                },
                {
                    "sent": "Approach is to compute Kernel Ridge regression.",
                    "label": 1
                },
                {
                    "sent": "The most popular way of doing this is based on the low rank matrix approximation.",
                    "label": 0
                },
                {
                    "sent": "The basic intuition is that you approximate the kernel matrix by the product of 2 lower rank smaller matrices.",
                    "label": 0
                },
                {
                    "sent": "The concrete examples of this approach includes.",
                    "label": 0
                },
                {
                    "sent": "Kernel PCA, an incomplete roller skater compensation and Nystrom sampling.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another line of research is the iterative algorithm to optimize Alpha directly, an this includes gradient descent kind of method and the contractor gradient descent, and this method use early stopping rules instead of instead of regularization to avoid overfitting.",
                    "label": 0
                },
                {
                    "sent": "But anyway, although this kind of approximation based method provides some efficiency to the computation of courage aggression, but the accuracy is still an issue cause it is not, it is not clear how the performance of this algorithm approximation based algorithm is compared with the exact algorithm of kernel Ridge regression.",
                    "label": 1
                },
                {
                    "sent": "So in some cases it is possible that the user has to make some tradeoff between accuracy and efficiency.",
                    "label": 0
                },
                {
                    "sent": "So our contribution in this work is that we show an algorithm.",
                    "label": 0
                },
                {
                    "sent": "That preserve accuracy and efficiency at the same time, it means that our approach runs as fast as this approximation based approaches and at the same time it doesn't sacrifice any accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our main idea is a little different from low rank matrix matrix approximation, but it is very simple so big cause the most expensive step in kernel regression is to inverse the kernel matrix.",
                    "label": 1
                },
                {
                    "sent": "We want to modify it to simplify it to make it easier to inverse.",
                    "label": 0
                },
                {
                    "sent": "So our modification has two steps.",
                    "label": 0
                },
                {
                    "sent": "The first one is to reshuffle the rows and columns of the kernel matrix based on some random permutation of the sample and the second step is to partition the.",
                    "label": 1
                },
                {
                    "sent": "That the kernel matrix into small some smaller sub matrices an only keep the diagonal sub matrices and remove all the other interests and replace it by zero.",
                    "label": 1
                },
                {
                    "sent": "So if we replace the original kernel matrix by this modified kernel matrix and we do the inversion on this modified matrix, then the inversion is reduced.",
                    "label": 0
                },
                {
                    "sent": "The problem of reversing this smaller sub matrices which is substantially easier and we can show that as long as the size of the sub matrices are not too small.",
                    "label": 0
                },
                {
                    "sent": "Then immersing this modified matrix still gives good results in terms of statistical accuracy, so although we have actually discarded most of the entries in the original kernel matrix, it turns out that essential information to do this kernel regression estimation is not lost in this process.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So formally, we have this divide and conquer procedure.",
                    "label": 1
                },
                {
                    "sent": "A three step.",
                    "label": 0
                },
                {
                    "sent": "The first step is that we divide the data set into some smaller divided partitioning uniformly at random into an subsets, and the second step is that we take each of these subsets and we do.",
                    "label": 1
                },
                {
                    "sent": "We compute the current Ridge regression estimator based on solely based on this subset.",
                    "label": 0
                },
                {
                    "sent": "So it is the local estimation procedure.",
                    "label": 0
                },
                {
                    "sent": "But one thing worth noting is that we use a special regularizer for this local problems.",
                    "label": 0
                },
                {
                    "sent": "Use the regularization factor for this local problem, which is equal to the regularization we use for the global problem becausw.",
                    "label": 0
                },
                {
                    "sent": "Usually the regularization coefficient goes smaller as your sample size grows larger and we use the global regularization for local problems.",
                    "label": 0
                },
                {
                    "sent": "So it means that the local problem is under regularised, if it uses AV correct weather.",
                    "label": 0
                },
                {
                    "sent": "So this regularization and regularization has two effects.",
                    "label": 0
                },
                {
                    "sent": "First one is that it introduces a smaller bias because the bias is introduced by regularization and we use a weaker regularization and 2nd effect is that it causes larger variance be cause if you are using a vehicle regularization, you have more overfitting.",
                    "label": 0
                },
                {
                    "sent": "So the variance is larger.",
                    "label": 0
                },
                {
                    "sent": "But in the last step of the algorithm, the average or the local estimate to form a global estimator and cause this local estimators independent.",
                    "label": 0
                },
                {
                    "sent": "It reduces the variance to low levels.",
                    "label": 0
                },
                {
                    "sent": "Eventually you have a small bias and small variance, and their combination matches the optimal convergence rate.",
                    "label": 0
                },
                {
                    "sent": "If you look at the complexity, most of the computation is in the second step, so it is very easy to verify that both the time and space complexity is reduced by a factor of N square.",
                    "label": 0
                },
                {
                    "sent": "So if we can know the maximum value of M, which is the partition number, then we can plug it this number.",
                    "label": 0
                },
                {
                    "sent": "This value of M into the formula of the complexities and we can get concrete complexity for this algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the next slide I will answer the question of.",
                    "label": 0
                },
                {
                    "sent": "Which is the maximum M we can choose for this algorithm to preserve optimal accuracy?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's a general theorem that works for order Colonel classes, the upper bound, the estimation error by three terms, the first 2 terms that bias invariances that I mentioned it last slice.",
                    "label": 0
                },
                {
                    "sent": "So here's a critical quantity, which is gamma Lambda.",
                    "label": 0
                },
                {
                    "sent": "Lambda is the regularization coefficient and it is referred as the effective dimensionality in the previous literature, and it is depending on both the regularization factor and also depending on the decreasing rate of the eigenvalues and the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "And so the key observation here is that in both 2 terms it is.",
                    "label": 0
                },
                {
                    "sent": "They're not, they're not, they're independent to the partition number M. So all you have to do is to choose the regularization coefficient that make the tradeoff between bias and variance and the classical literature says that, well, this matches the optimal convergence rate.",
                    "label": 0
                },
                {
                    "sent": "The only thing bothers is the third term that we see that it is exponentially decreasing term.",
                    "label": 0
                },
                {
                    "sent": "So as long as you choose M that is not larger than.",
                    "label": 0
                },
                {
                    "sent": "The number of samples divided by the effective dimensionality square.",
                    "label": 0
                },
                {
                    "sent": "You can get this term if knowledgeably small comparing to the first 2 terms, so it doesn't hurt accuracy, so it gives us a path to determine the number M. First you choose a regularization coefficient, Lambda, and second, you compute the effective dimensionality based on Lambda and based on the problem and serve you choose your M. According to this former, it shouldn't be greater than.",
                    "label": 0
                },
                {
                    "sent": "It should be smaller than the number of samples divided by the effective effective dimensionality squared.",
                    "label": 0
                },
                {
                    "sent": "And finally, plugging this M into the complexity expressions and get the complexity for the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next two slides shows some consequence to the general theorem.",
                    "label": 0
                },
                {
                    "sent": "We apply it to three specific kernels and follow this path to see what is.",
                    "label": 0
                },
                {
                    "sent": "How is the complexity is reduced.",
                    "label": 0
                },
                {
                    "sent": "So the first example is the polynomial kernels for this kernel.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you can choose the number of partitions to be approximately equal to the number of samples divided by the log of number of samples, and then plugging this M you see that the computation complexity.",
                    "label": 0
                },
                {
                    "sent": "Is reduced to something that is almost linear to end, which is nearly optimal and the space complexity is only having a logarithm dependence on the number of samples.",
                    "label": 0
                },
                {
                    "sent": "And the same phenomenon is observed for the golf in kernels and still using a correct choice of up M and Lambda you can reduce the time complexity to almost linear to the number of samples and also the space complexity is logarithm Lee.",
                    "label": 0
                },
                {
                    "sent": "Depending on the number of samples.",
                    "label": 0
                },
                {
                    "sent": "Distribution you design points with these results.",
                    "label": 0
                },
                {
                    "sent": "We do have some assumptions, but they are not very strong.",
                    "label": 0
                },
                {
                    "sent": "We have some moments assumptions on our base functions of the kernel, like we assume that the case moment or the infinite or responded by the bounded is too strong assumption.",
                    "label": 0
                },
                {
                    "sent": "So you can assume some moment assumption.",
                    "label": 0
                },
                {
                    "sent": "Yeah, basically we need at least four moments on the basis functions of the kernel.",
                    "label": 0
                },
                {
                    "sent": "This is not a sample.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the situation is a little more complicated for the soul of kernels of smoothness, mubi 'cause there's extra parameter of mu.",
                    "label": 0
                },
                {
                    "sent": "So recall that you're using a sub of kernel smoothness you you can feed all the functions that has new differentiability.",
                    "label": 0
                },
                {
                    "sent": "So if using this kernel class then it turns out that most of time and space complexity is reduced to some polynomial of the sample size and power of the polynomial is determined by mu.",
                    "label": 0
                },
                {
                    "sent": "And in the extreme case, if you assume that means very large means that you're feeding a very smooth function.",
                    "label": 0
                },
                {
                    "sent": "Then again you get the time complexity is almost linear to end, and space complexity is depending on the logarithm of N.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at last I want to show a simulation study that gives more intuitive illustration of how our algorithm works.",
                    "label": 0
                },
                {
                    "sent": "So we generate the data set by continuous but non smooth function.",
                    "label": 0
                },
                {
                    "sent": "It is the minimum of X and Y -- X and we add a Gaussian noise to probate the data.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Across the true function is Lipschitz continuous, so we can use the soul of kernel of smoothness, smoothness one to feed the data and we plot mean square error curve by increasing the sample size.",
                    "label": 1
                },
                {
                    "sent": "An from this plot we see that.",
                    "label": 0
                },
                {
                    "sent": "The number of partitions is smaller than 16.",
                    "label": 1
                },
                {
                    "sent": "Then the performance of divide and Conquer's very close and specifically when M = 1, It means that there is no divide and conquer.",
                    "label": 1
                },
                {
                    "sent": "So it is exactly the standard kernel regression and it is the optimal baseline.",
                    "label": 0
                },
                {
                    "sent": "Actually, even if the number of even if the number of samples is equal to 64.",
                    "label": 0
                },
                {
                    "sent": "But as the number of even if the partition number is 64 as the sample size increases, you can see that the performance get very close to the optimal baseline.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it is also meaningful to study the the maximum number of partitions we can use given a fixed sample size.",
                    "label": 0
                },
                {
                    "sent": "So in this experiment we fixed the sample size and we draw the square error curve by varying the partition number.",
                    "label": 1
                },
                {
                    "sent": "And it is observed that there is a stress hold in the plot below this threshold of partition number.",
                    "label": 0
                },
                {
                    "sent": "The mean square error doesn't change and after that it started to increase.",
                    "label": 1
                },
                {
                    "sent": "So by examine this probably see that this transport is a polynomial function of the sample size and more precisely it is N to the point at 45.",
                    "label": 0
                },
                {
                    "sent": "So it confirms the theory that you can choose the in the kernel of smoothness.",
                    "label": 0
                },
                {
                    "sent": "One you can choose a partition number.",
                    "label": 0
                },
                {
                    "sent": "That is a polynomial function of the sample size.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me quickly summarize the results.",
                    "label": 0
                },
                {
                    "sent": "First we develop divide and conquer approach to encourage regression.",
                    "label": 0
                },
                {
                    "sent": "It is very simple, but it reduces the complexity very substantially.",
                    "label": 0
                },
                {
                    "sent": "And Secondly, we prove that it preserved by choosing the partition number correctly.",
                    "label": 0
                },
                {
                    "sent": "It preserved the optimal statistical accuracy and we also apply the general theory to specific kernels will show that force of kernels.",
                    "label": 0
                },
                {
                    "sent": "You can get acceleration rate.",
                    "label": 0
                },
                {
                    "sent": "That is polynomial to the number of samples an for the finite rank kernels and Gaussian kernels you can get a time complexity that is almost linear to the number of samples.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are several open problems around to raise.",
                    "label": 1
                },
                {
                    "sent": "The first one is how to get a partition numbering practice optimal partition number, cause in order theory, the computation of the partition number depends on the effective dimensionality of the problem, but in generally in general problem this computation could be expensive and 2nd.",
                    "label": 1
                },
                {
                    "sent": "So remember that we actually sampled the original kernel matrix, we only keep the diagonal blocks.",
                    "label": 0
                },
                {
                    "sent": "So natural question is what is the minimum number of entries you have to?",
                    "label": 0
                },
                {
                    "sent": "Have to preserve in original kernel matrix to guarantee the optimal convergence rate, so this is lower bound and the last question is whether the divide and conquer approach works for more general kernel based methods like kernel SVM.",
                    "label": 1
                },
                {
                    "sent": "Still this is not questioned the Council, so that's it.",
                    "label": 0
                },
                {
                    "sent": "Thanks for attention.",
                    "label": 0
                }
            ]
        }
    }
}