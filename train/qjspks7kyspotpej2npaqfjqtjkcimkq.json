{
    "id": "qjspks7kyspotpej2npaqfjqtjkcimkq",
    "title": "The Feature Importance Ranking Measure",
    "info": {
        "author": [
            "Nicole Kr\u00e4mer, Machine Learning and Intelligent Data Analysis Group, TU Berlin"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Feature Selection"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_kramer_firm/",
    "segmentation": [
        [
            "So before I start, I would like to mention that this is joint work with my colleagues, Alex Teen Zones on work and Rich who are right now on Netflix, Mitchell Laboratory in Tubing."
        ],
        [
            "So what I want to talk about, so we introduce a measure that which we called firms of feature importance ranking measure and there's four nice features so to speak.",
            "So first of all, it assesses the importance of features a posteriori.",
            "So after learning any machine learning algorithm.",
            "So this is in contrast to what you would normally do that you say that you assess a feature by restricting to some linear models or some sparse models.",
            "And as I will show later, generalizes some existing variable importance measures and generalized.",
            "For example, in the sense that it can also compute the importance of features that are not directly observed given that you have some background information of all your features are generated and generalize them in the sense that it also takes the dependence of the input features into account.",
            "OK, So what do we want to do?",
            "So we have some."
        ],
        [
            "Output of some machine learning algorithm, maybe a regression function or a support vector machine.",
            "And then we want to have an interpretation what this actually does, and in particular we want to assess the importance of some feature vector.",
            "So it might be an input variable, but it might be also some some function of input variables or something that is not observed but closely related to your input features.",
            "And what people normally do and we already saw that.",
            "For example, if people restrict themselves to learning machines that are linear that are sparse, and then they hope that they have an interpretation for these weight vectors.",
            "So regression coefficients."
        ],
        [
            "So the optical obstacles.",
            "That's also what we saw in overstock that these linear methods are these sparse methods are often not more accurate than the full of the nonlinear."
        ],
        [
            "Methods in the sentence that that I've heard a lot in talks or even in papers is that they say, OK, we sacrificed some predictive power in order to gain interpretability, and that's I want to make a point that we should do this.",
            "And also that maybe this interpretability is also not given for these weight vectors.",
            "And so maybe we would rather use some nonlinear methods like kernel machines.",
            "But the problem is that the independency between input and output is.",
            "Really gave him.",
            "Of course we have formula but there are no weight vectors to for interpretation.",
            "And also the the input features are typically highly correlated, so the interpretation of these sparse linear algorithms misleading.",
            "So everybody knows when features are correlated, it does not make endless Sue select one of the variables does not make sense to say this is important because the highly correlated, But then on the other end, people typically do this."
        ],
        [
            "So we want to come up with something with an alternative.",
            "So we have this speech importance ranking measure.",
            "So as I said we it's plugged in after after the learning.",
            "The function S so it's a posteriori and we hope that we can have best of both worlds, so we can have a learner that has good performance and we can also have some interpretation of it.",
            "And.",
            "It's general in the sense that it also includes these non observed features and you can plug in your knowledge about the dependence of the features."
        ],
        [
            "OK, so and we proceed in two steps.",
            "So the first step is you can ask yourself.",
            "So how what comes out of this training learning machine S If we know that some feature attains a certain value.",
            "So if we know that the features fixed a certain value doesn't affect this learning machine."
        ],
        [
            "OK, so this is what we call conditional expected score.",
            "So we ask yourself, what's the expected value of this Q?",
            "Sorry of this learner, S if we know that the feature value is fixed to T, so it's a conditional expectation here."
        ],
        [
            "So and so.",
            "The idea behind this is the following.",
            "So so if this condition expected score varies a lot and T so that means that this feature is supposed to have a very high importance for the output because the output varies a lot in.",
            "Inti.",
            "OK, now."
        ],
        [
            "The the other way around, if it does not guarantee then we say that this feature doesn't have any importance for the output."
        ],
        [
            "OK, so.",
            "And this was already introduced by Fondell on in 2006, after some normalization, and he calls this the marginal variable importance."
        ],
        [
            "OK, so and of course because we compute expectations that depends on the unknown distribution of X.",
            "But as I will show in the rest of."
        ],
        [
            "Talk, we can compute approximate them if we impose some additional assumptions on the data.",
            "OK, so as I said the this we look at this score and look if it varies a lot.",
            "Then we say that the feature is important so we define."
        ],
        [
            "Feature importance ranking measure.",
            "Just as the variance of this conditional expected score and then we just.",
            "Use the square root for normalization."
        ],
        [
            "OK, as I said before, it generalizes existing variable importance measures, so this conditional expected score is the one I just talked about by Fenella, and think that I will talk about in a minute.",
            "Is that it also generalizes?",
            "Variable importance measures defined by Friedman in 2001 S, and it generalizes in the sense that it takes the correlation structure into account.",
            "And it's nice because it's a very general measure, so you can.",
            "It's defined for any learning machine, and for any feature.",
            "Independent of the data.",
            "So it's so general that of course."
        ],
        [
            "The bad news is you cannot compute it like if it's as generous as this.",
            "You don't have any chance because you need the disk."
        ],
        [
            "And of the input data.",
            "OK, but the good news is that we can compute the approximated under additional assumptions on the data.",
            "And in the paper notes on the poster, I have some more examples on binary data and sequence data and."
        ],
        [
            "But for the talk I am.",
            "Focus on approximation.",
            "If we four if we assume a normally distributed data.",
            "So.",
            "If the input features are normal distributed, we can actually compute this conditional expected score because.",
            "As you probably know, if the input data is normally distributed, then this conditional variable random varies also normally distributed, and you can compute the expectation based.",
            "On the value that it attains and also on the covariant structure.",
            "OK. OK, that was step one that was the conditional expected score.",
            "Now we need to compute the variance of.",
            "As of this guy."
        ],
        [
            "So this is the conditional expected score and we have to compute the variance of this one here."
        ],
        [
            "So of course in general is some function, but we cannot compute the variance, But if it's differentiable then we can just use first order Taylor approximation to and then we also have that this is also approximately normally distributed.",
            "Just computing the first derivative and then if we plug this into the formula of a firm for measure."
        ],
        [
            "Then we get something like this.",
            "So what is it?",
            "So we have this first derivative of the learned machine learning algorithm and has nice interpretation that looks at the sensitivity of the learner with respect to your variable.",
            "But then we also.",
            "Scale this with this Jason Column of the covariance matrix that that is.",
            "It also takes the correlation structure into account, so it also doesn't only look at the one very, but also at the variables that are correlated."
        ],
        [
            "OK, so that's what I just said, so it's nice because it also takes this correlation structure into account.",
            "And."
        ],
        [
            "SS linear learner, then the approximations, of course.",
            "Exact because.",
            "The 1st order Taylor approximation, except so an interesting thing is what?"
        ],
        [
            "We then later found out that it generalizes this variable importance measure by Friedman, so it looks very much the same.",
            "We also have the first derivative.",
            "We have this normalization constant.",
            "And they're both the same, if the.",
            "The input features are correlated.",
            "So this basic doesn't take the correlation structure into account, but the other one does so.",
            "Uh, they're both saying the same spooked."
        ],
        [
            "OK, and once we have this approximation we can.",
            "Under this normal assumption, we can for example.",
            "Compute this firm for Gaussian kernel machines because.",
            "In Goshen learning the Learn Function S are some basis expansion and we can just compute the first derivatives of this.",
            "So we have a formula for that.",
            "And also what I just said if."
        ],
        [
            "The function is linear then.",
            "We also have a solution and I think I think this formula is pretty obvious, so we have this.",
            "We learn the linear function, But then we have to multiply it with the correlation matrix because we also have to be aware that all the features are correlated."
        ],
        [
            "OK, so and then the remaining of my talk I want to illustrate this in two examples.",
            "So in the first example is binary data, so the task is to learn this Boolean formula.",
            "And we train a support vector machine on this with the polynomial kernel and we don't only use X1 and X2, but also completely irrelevant variable extreme and we train the support vector machine on all different assignments of this.",
            "And in this case, in contrast to the second example, all the features are uncorrelated, so we have correlated data.",
            "And what we do, we compute the support vector machine weights for all these X One X3 and then all of the three products and then we after that we compute firm or based on this train support vector machine weights and just to check we also compute firm based on the true widths."
        ],
        [
            "OK, so here are the results.",
            "What other results so so the three columns are the three methods to support vector machine or measure.",
            "And once on the computed on the W and then on the truth.",
            "So what does the first column show?",
            "So in the first row we have the weights that the support vector machine assigns to the individual variables.",
            "So X1 gets highway, takes 2, gets very high negative weight and X3 gets way close to 0.",
            "That's what we would expect.",
            "And also in the 2nd row.",
            "Heat Maps show the weights that are.",
            "Are given to the product of these features.",
            "So while the support vector machines does a good job in finding these weights, what firm is also able to do it?",
            "You do this conditional expected score.",
            "It can also say give attaching importance to different Boolean.",
            "Two to the individual values that are given by this Boolean function.",
            "For example, you can see.",
            "So this is the product of X1 and X2 and in addition to just having one particular weight, you can also assign an importance to the what happens if it's one or zero X20 and this is also the importance what happens if it's?",
            "Wanna zero antics two is 1 so we see.",
            "So we have a more refined interpretation of this Boolean representation of this function.",
            "OK, in the second example."
        ],
        [
            "Little bit of a different flavor, so this is an example on sequence classification and it's taking from the paper of my colleagues.",
            "And in contrast to what I just showed you before, here, the features that we have highly correlated features because we're looking at substrings and the features are substrings and they all overlap quite heavily, so we have very highly correlated features.",
            "And in the example setting which I will explain in a second.",
            "We have features that are very important, but they are.",
            "We also mutate these features that are important these strings, so we cannot really directly observe them.",
            "We only observe some some mutation of them.",
            "And the third feature of this example is that we have some.",
            "Additional note about the distribution of the input data.",
            "Because they are strings and we can, we can assume a Markov distribution.",
            "So for this case we really have some idea how the features."
        ],
        [
            "Are distributed.",
            "OK, so the."
        ],
        [
            "Treatment was set up as follows, so we randomly generated DNA sequences.",
            "And then we."
        ],
        [
            "Edit Two class classification problem out of it and for the positive class we planted a subsequence called Gattaca.",
            "At a random position somewhere around 35.",
            "So.",
            "Some random normal distribution and put some standard deviation of seven.",
            "So somewhere between 30 and 40 we placed this subsequence.",
            "And the task is then to to to learn to learn this decision rule whether this sequence subsequence is in the string or not.",
            "And to make the."
        ],
        [
            "Problem with more complicated.",
            "Every time we plant this subsequence gattiker.",
            "We also permute a single position.",
            "Um?",
            "OK, so yeah, we choose one of the position and then one of the letters are randomly permuted.",
            "Sorry, randomly replaced.",
            "OK."
        ],
        [
            "And what we then do we use the support vector machine with a weighted degree kernel.",
            "That's a kernel.",
            "In particular, these sequence data, we Trenton among lots of data, 2500 positive points and as many negative points.",
            "So and what I would want to show you now is that of course, because this is basically a linear support vector machine, we have some weights for the support vector machine which we could use for interpretation if we feel like and the point that I want to make is that our measure gives much more sensible interpretation."
        ],
        [
            "OK, So what is this?",
            "So for both, so the left figure is.",
            "Support vector machine weights and this is how feature importance measure.",
            "So on the X axis we have the sequence position.",
            "So these define the features.",
            "Of a problem and the Y axis are other weights.",
            "OK, let's first look at this at this red line so that we have.",
            "That's the problem.",
            "We don't even mutate.",
            "So the sequence Gattaca, so the task is to find to detect the strings that have the skeptic inside, and we already see if there are no mutations.",
            "The support vector machine.",
            "Results is hardly interpretable, so we would expect to have some important features here because it's the place where we planted the substring.",
            "But because these data is so highly correlated because we just use look at all these substrings, the support vector machine really gets into trouble.",
            "In contrast, for the for the firm importance measure, the red one also indicates the importance that we touched it, and this does pretty good job because as I told you in part, the important part should be here.",
            "And it also shows that these features are important because it does not look at one particular.",
            "Wait, but it also looks at those that are close to them.",
            "And of course, the.",
            "The results get worse when we mutate the data, so this is for one mutation and gene mutations and this is 4 seven.",
            "So this will all the means and we have.",
            "We also plotted the standard deviation because the results can be unstable but still.",
            "Firm is always much better than the support vector machine."
        ],
        [
            "OK so I come to my conclusion so I talked about firm or feature importance ranking measure and my point of view.",
            "Does four nights features so we we in principle we can use any any machine learning algorithm that we like and then plug it in as another posteriori analysis so we don't have to stick to linear past methods, we can just use what what perform performs best and then assess the importance.",
            "It's a generalization of existing variable importance measures so that.",
            "Fastest.",
            "With some credibility and just set in this example of this substring analysis, we can also compute the importance of features that are not directly observed.",
            "For example, for this sequence data we because we have the complete distribution of the input features, we could also assess the importance of the feature Gattaca which we didn't see in the in the training data, but because we know the distribution.",
            "We can also do this or more generally speaking, if we have a feature that was not present in the training data but we.",
            "What we know about how it relates to the other input features, we can also assess this.",
            "And the fourth point, which I also think is very important, that is that it takes the dependence of the input features into account.",
            "OK, thank you."
        ],
        [
            "Well first thank you with this we need first.",
            "So now if I understand correctly, this is basically your last point.",
            "You stress on modeling independence.",
            "But then you middle data covariance matrix, right?",
            "We work with Michael reading that we had 2000 features.",
            "So do we have or more?",
            "We had problems to estimate those cooperates matrix reliably in order to be able to remember or ways around.",
            "Yeah, OK, so.",
            "OK, that may be there two things.",
            "If you have like 50 or 100,000 variables, you don't want to compute the covariance matrix.",
            "But if you don't have too much examples, of course you can use that X.",
            "Your data matrix is rectangular.",
            "That saves you sometime.",
            "But yeah, OK, but it's yeah.",
            "So this is one point, but from the so I think.",
            "So from of course, if you have very few data, your covariance matrix is maybe not the best choice, but I think the problem, which is that you have with this covariance matrix normally only turns up when when you invert it like to have to answer so that the first thing is, I think the covariance matrix is not doesn't have so many problems, as long as you don't invert it.",
            "And second, there's a paper by chef French Trimmer, Unregularized, estimation of covariance matrix that's based on papers by lead Warren Wolf, where they can regularize it, and they can automatically compute the regularization from the data.",
            "So as I said, I don't think that it's it's a very hard problem that affects it too much, and if it affects it then I would suggest something like this.",
            "More questions.",
            "Not next speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before I start, I would like to mention that this is joint work with my colleagues, Alex Teen Zones on work and Rich who are right now on Netflix, Mitchell Laboratory in Tubing.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I want to talk about, so we introduce a measure that which we called firms of feature importance ranking measure and there's four nice features so to speak.",
                    "label": 0
                },
                {
                    "sent": "So first of all, it assesses the importance of features a posteriori.",
                    "label": 1
                },
                {
                    "sent": "So after learning any machine learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is in contrast to what you would normally do that you say that you assess a feature by restricting to some linear models or some sparse models.",
                    "label": 1
                },
                {
                    "sent": "And as I will show later, generalizes some existing variable importance measures and generalized.",
                    "label": 0
                },
                {
                    "sent": "For example, in the sense that it can also compute the importance of features that are not directly observed given that you have some background information of all your features are generated and generalize them in the sense that it also takes the dependence of the input features into account.",
                    "label": 1
                },
                {
                    "sent": "OK, So what do we want to do?",
                    "label": 0
                },
                {
                    "sent": "So we have some.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Output of some machine learning algorithm, maybe a regression function or a support vector machine.",
                    "label": 1
                },
                {
                    "sent": "And then we want to have an interpretation what this actually does, and in particular we want to assess the importance of some feature vector.",
                    "label": 0
                },
                {
                    "sent": "So it might be an input variable, but it might be also some some function of input variables or something that is not observed but closely related to your input features.",
                    "label": 0
                },
                {
                    "sent": "And what people normally do and we already saw that.",
                    "label": 0
                },
                {
                    "sent": "For example, if people restrict themselves to learning machines that are linear that are sparse, and then they hope that they have an interpretation for these weight vectors.",
                    "label": 0
                },
                {
                    "sent": "So regression coefficients.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the optical obstacles.",
                    "label": 0
                },
                {
                    "sent": "That's also what we saw in overstock that these linear methods are these sparse methods are often not more accurate than the full of the nonlinear.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Methods in the sentence that that I've heard a lot in talks or even in papers is that they say, OK, we sacrificed some predictive power in order to gain interpretability, and that's I want to make a point that we should do this.",
                    "label": 1
                },
                {
                    "sent": "And also that maybe this interpretability is also not given for these weight vectors.",
                    "label": 0
                },
                {
                    "sent": "And so maybe we would rather use some nonlinear methods like kernel machines.",
                    "label": 1
                },
                {
                    "sent": "But the problem is that the independency between input and output is.",
                    "label": 0
                },
                {
                    "sent": "Really gave him.",
                    "label": 0
                },
                {
                    "sent": "Of course we have formula but there are no weight vectors to for interpretation.",
                    "label": 0
                },
                {
                    "sent": "And also the the input features are typically highly correlated, so the interpretation of these sparse linear algorithms misleading.",
                    "label": 1
                },
                {
                    "sent": "So everybody knows when features are correlated, it does not make endless Sue select one of the variables does not make sense to say this is important because the highly correlated, But then on the other end, people typically do this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we want to come up with something with an alternative.",
                    "label": 0
                },
                {
                    "sent": "So we have this speech importance ranking measure.",
                    "label": 1
                },
                {
                    "sent": "So as I said we it's plugged in after after the learning.",
                    "label": 0
                },
                {
                    "sent": "The function S so it's a posteriori and we hope that we can have best of both worlds, so we can have a learner that has good performance and we can also have some interpretation of it.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It's general in the sense that it also includes these non observed features and you can plug in your knowledge about the dependence of the features.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so and we proceed in two steps.",
                    "label": 0
                },
                {
                    "sent": "So the first step is you can ask yourself.",
                    "label": 0
                },
                {
                    "sent": "So how what comes out of this training learning machine S If we know that some feature attains a certain value.",
                    "label": 1
                },
                {
                    "sent": "So if we know that the features fixed a certain value doesn't affect this learning machine.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is what we call conditional expected score.",
                    "label": 1
                },
                {
                    "sent": "So we ask yourself, what's the expected value of this Q?",
                    "label": 0
                },
                {
                    "sent": "Sorry of this learner, S if we know that the feature value is fixed to T, so it's a conditional expectation here.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So and so.",
                    "label": 0
                },
                {
                    "sent": "The idea behind this is the following.",
                    "label": 0
                },
                {
                    "sent": "So so if this condition expected score varies a lot and T so that means that this feature is supposed to have a very high importance for the output because the output varies a lot in.",
                    "label": 1
                },
                {
                    "sent": "Inti.",
                    "label": 0
                },
                {
                    "sent": "OK, now.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The the other way around, if it does not guarantee then we say that this feature doesn't have any importance for the output.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "And this was already introduced by Fondell on in 2006, after some normalization, and he calls this the marginal variable importance.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so and of course because we compute expectations that depends on the unknown distribution of X.",
                    "label": 0
                },
                {
                    "sent": "But as I will show in the rest of.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk, we can compute approximate them if we impose some additional assumptions on the data.",
                    "label": 1
                },
                {
                    "sent": "OK, so as I said the this we look at this score and look if it varies a lot.",
                    "label": 0
                },
                {
                    "sent": "Then we say that the feature is important so we define.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Feature importance ranking measure.",
                    "label": 0
                },
                {
                    "sent": "Just as the variance of this conditional expected score and then we just.",
                    "label": 1
                },
                {
                    "sent": "Use the square root for normalization.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, as I said before, it generalizes existing variable importance measures, so this conditional expected score is the one I just talked about by Fenella, and think that I will talk about in a minute.",
                    "label": 1
                },
                {
                    "sent": "Is that it also generalizes?",
                    "label": 0
                },
                {
                    "sent": "Variable importance measures defined by Friedman in 2001 S, and it generalizes in the sense that it takes the correlation structure into account.",
                    "label": 1
                },
                {
                    "sent": "And it's nice because it's a very general measure, so you can.",
                    "label": 0
                },
                {
                    "sent": "It's defined for any learning machine, and for any feature.",
                    "label": 0
                },
                {
                    "sent": "Independent of the data.",
                    "label": 0
                },
                {
                    "sent": "So it's so general that of course.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The bad news is you cannot compute it like if it's as generous as this.",
                    "label": 0
                },
                {
                    "sent": "You don't have any chance because you need the disk.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of the input data.",
                    "label": 1
                },
                {
                    "sent": "OK, but the good news is that we can compute the approximated under additional assumptions on the data.",
                    "label": 1
                },
                {
                    "sent": "And in the paper notes on the poster, I have some more examples on binary data and sequence data and.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But for the talk I am.",
                    "label": 0
                },
                {
                    "sent": "Focus on approximation.",
                    "label": 0
                },
                {
                    "sent": "If we four if we assume a normally distributed data.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If the input features are normal distributed, we can actually compute this conditional expected score because.",
                    "label": 0
                },
                {
                    "sent": "As you probably know, if the input data is normally distributed, then this conditional variable random varies also normally distributed, and you can compute the expectation based.",
                    "label": 1
                },
                {
                    "sent": "On the value that it attains and also on the covariant structure.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, that was step one that was the conditional expected score.",
                    "label": 0
                },
                {
                    "sent": "Now we need to compute the variance of.",
                    "label": 0
                },
                {
                    "sent": "As of this guy.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the conditional expected score and we have to compute the variance of this one here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So of course in general is some function, but we cannot compute the variance, But if it's differentiable then we can just use first order Taylor approximation to and then we also have that this is also approximately normally distributed.",
                    "label": 0
                },
                {
                    "sent": "Just computing the first derivative and then if we plug this into the formula of a firm for measure.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we get something like this.",
                    "label": 0
                },
                {
                    "sent": "So what is it?",
                    "label": 0
                },
                {
                    "sent": "So we have this first derivative of the learned machine learning algorithm and has nice interpretation that looks at the sensitivity of the learner with respect to your variable.",
                    "label": 0
                },
                {
                    "sent": "But then we also.",
                    "label": 0
                },
                {
                    "sent": "Scale this with this Jason Column of the covariance matrix that that is.",
                    "label": 0
                },
                {
                    "sent": "It also takes the correlation structure into account, so it also doesn't only look at the one very, but also at the variables that are correlated.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's what I just said, so it's nice because it also takes this correlation structure into account.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "SS linear learner, then the approximations, of course.",
                    "label": 0
                },
                {
                    "sent": "Exact because.",
                    "label": 0
                },
                {
                    "sent": "The 1st order Taylor approximation, except so an interesting thing is what?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We then later found out that it generalizes this variable importance measure by Friedman, so it looks very much the same.",
                    "label": 1
                },
                {
                    "sent": "We also have the first derivative.",
                    "label": 0
                },
                {
                    "sent": "We have this normalization constant.",
                    "label": 0
                },
                {
                    "sent": "And they're both the same, if the.",
                    "label": 1
                },
                {
                    "sent": "The input features are correlated.",
                    "label": 1
                },
                {
                    "sent": "So this basic doesn't take the correlation structure into account, but the other one does so.",
                    "label": 0
                },
                {
                    "sent": "Uh, they're both saying the same spooked.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and once we have this approximation we can.",
                    "label": 0
                },
                {
                    "sent": "Under this normal assumption, we can for example.",
                    "label": 0
                },
                {
                    "sent": "Compute this firm for Gaussian kernel machines because.",
                    "label": 1
                },
                {
                    "sent": "In Goshen learning the Learn Function S are some basis expansion and we can just compute the first derivatives of this.",
                    "label": 0
                },
                {
                    "sent": "So we have a formula for that.",
                    "label": 0
                },
                {
                    "sent": "And also what I just said if.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The function is linear then.",
                    "label": 0
                },
                {
                    "sent": "We also have a solution and I think I think this formula is pretty obvious, so we have this.",
                    "label": 0
                },
                {
                    "sent": "We learn the linear function, But then we have to multiply it with the correlation matrix because we also have to be aware that all the features are correlated.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so and then the remaining of my talk I want to illustrate this in two examples.",
                    "label": 0
                },
                {
                    "sent": "So in the first example is binary data, so the task is to learn this Boolean formula.",
                    "label": 1
                },
                {
                    "sent": "And we train a support vector machine on this with the polynomial kernel and we don't only use X1 and X2, but also completely irrelevant variable extreme and we train the support vector machine on all different assignments of this.",
                    "label": 1
                },
                {
                    "sent": "And in this case, in contrast to the second example, all the features are uncorrelated, so we have correlated data.",
                    "label": 0
                },
                {
                    "sent": "And what we do, we compute the support vector machine weights for all these X One X3 and then all of the three products and then we after that we compute firm or based on this train support vector machine weights and just to check we also compute firm based on the true widths.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here are the results.",
                    "label": 0
                },
                {
                    "sent": "What other results so so the three columns are the three methods to support vector machine or measure.",
                    "label": 0
                },
                {
                    "sent": "And once on the computed on the W and then on the truth.",
                    "label": 0
                },
                {
                    "sent": "So what does the first column show?",
                    "label": 0
                },
                {
                    "sent": "So in the first row we have the weights that the support vector machine assigns to the individual variables.",
                    "label": 0
                },
                {
                    "sent": "So X1 gets highway, takes 2, gets very high negative weight and X3 gets way close to 0.",
                    "label": 0
                },
                {
                    "sent": "That's what we would expect.",
                    "label": 0
                },
                {
                    "sent": "And also in the 2nd row.",
                    "label": 0
                },
                {
                    "sent": "Heat Maps show the weights that are.",
                    "label": 0
                },
                {
                    "sent": "Are given to the product of these features.",
                    "label": 0
                },
                {
                    "sent": "So while the support vector machines does a good job in finding these weights, what firm is also able to do it?",
                    "label": 0
                },
                {
                    "sent": "You do this conditional expected score.",
                    "label": 0
                },
                {
                    "sent": "It can also say give attaching importance to different Boolean.",
                    "label": 0
                },
                {
                    "sent": "Two to the individual values that are given by this Boolean function.",
                    "label": 0
                },
                {
                    "sent": "For example, you can see.",
                    "label": 0
                },
                {
                    "sent": "So this is the product of X1 and X2 and in addition to just having one particular weight, you can also assign an importance to the what happens if it's one or zero X20 and this is also the importance what happens if it's?",
                    "label": 0
                },
                {
                    "sent": "Wanna zero antics two is 1 so we see.",
                    "label": 0
                },
                {
                    "sent": "So we have a more refined interpretation of this Boolean representation of this function.",
                    "label": 0
                },
                {
                    "sent": "OK, in the second example.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Little bit of a different flavor, so this is an example on sequence classification and it's taking from the paper of my colleagues.",
                    "label": 1
                },
                {
                    "sent": "And in contrast to what I just showed you before, here, the features that we have highly correlated features because we're looking at substrings and the features are substrings and they all overlap quite heavily, so we have very highly correlated features.",
                    "label": 0
                },
                {
                    "sent": "And in the example setting which I will explain in a second.",
                    "label": 0
                },
                {
                    "sent": "We have features that are very important, but they are.",
                    "label": 0
                },
                {
                    "sent": "We also mutate these features that are important these strings, so we cannot really directly observe them.",
                    "label": 0
                },
                {
                    "sent": "We only observe some some mutation of them.",
                    "label": 0
                },
                {
                    "sent": "And the third feature of this example is that we have some.",
                    "label": 1
                },
                {
                    "sent": "Additional note about the distribution of the input data.",
                    "label": 0
                },
                {
                    "sent": "Because they are strings and we can, we can assume a Markov distribution.",
                    "label": 0
                },
                {
                    "sent": "So for this case we really have some idea how the features.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are distributed.",
                    "label": 0
                },
                {
                    "sent": "OK, so the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Treatment was set up as follows, so we randomly generated DNA sequences.",
                    "label": 0
                },
                {
                    "sent": "And then we.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Edit Two class classification problem out of it and for the positive class we planted a subsequence called Gattaca.",
                    "label": 1
                },
                {
                    "sent": "At a random position somewhere around 35.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Some random normal distribution and put some standard deviation of seven.",
                    "label": 0
                },
                {
                    "sent": "So somewhere between 30 and 40 we placed this subsequence.",
                    "label": 0
                },
                {
                    "sent": "And the task is then to to to learn to learn this decision rule whether this sequence subsequence is in the string or not.",
                    "label": 0
                },
                {
                    "sent": "And to make the.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem with more complicated.",
                    "label": 0
                },
                {
                    "sent": "Every time we plant this subsequence gattiker.",
                    "label": 0
                },
                {
                    "sent": "We also permute a single position.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so yeah, we choose one of the position and then one of the letters are randomly permuted.",
                    "label": 0
                },
                {
                    "sent": "Sorry, randomly replaced.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we then do we use the support vector machine with a weighted degree kernel.",
                    "label": 1
                },
                {
                    "sent": "That's a kernel.",
                    "label": 0
                },
                {
                    "sent": "In particular, these sequence data, we Trenton among lots of data, 2500 positive points and as many negative points.",
                    "label": 1
                },
                {
                    "sent": "So and what I would want to show you now is that of course, because this is basically a linear support vector machine, we have some weights for the support vector machine which we could use for interpretation if we feel like and the point that I want to make is that our measure gives much more sensible interpretation.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what is this?",
                    "label": 0
                },
                {
                    "sent": "So for both, so the left figure is.",
                    "label": 0
                },
                {
                    "sent": "Support vector machine weights and this is how feature importance measure.",
                    "label": 0
                },
                {
                    "sent": "So on the X axis we have the sequence position.",
                    "label": 0
                },
                {
                    "sent": "So these define the features.",
                    "label": 0
                },
                {
                    "sent": "Of a problem and the Y axis are other weights.",
                    "label": 0
                },
                {
                    "sent": "OK, let's first look at this at this red line so that we have.",
                    "label": 0
                },
                {
                    "sent": "That's the problem.",
                    "label": 0
                },
                {
                    "sent": "We don't even mutate.",
                    "label": 0
                },
                {
                    "sent": "So the sequence Gattaca, so the task is to find to detect the strings that have the skeptic inside, and we already see if there are no mutations.",
                    "label": 0
                },
                {
                    "sent": "The support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Results is hardly interpretable, so we would expect to have some important features here because it's the place where we planted the substring.",
                    "label": 0
                },
                {
                    "sent": "But because these data is so highly correlated because we just use look at all these substrings, the support vector machine really gets into trouble.",
                    "label": 0
                },
                {
                    "sent": "In contrast, for the for the firm importance measure, the red one also indicates the importance that we touched it, and this does pretty good job because as I told you in part, the important part should be here.",
                    "label": 0
                },
                {
                    "sent": "And it also shows that these features are important because it does not look at one particular.",
                    "label": 0
                },
                {
                    "sent": "Wait, but it also looks at those that are close to them.",
                    "label": 0
                },
                {
                    "sent": "And of course, the.",
                    "label": 0
                },
                {
                    "sent": "The results get worse when we mutate the data, so this is for one mutation and gene mutations and this is 4 seven.",
                    "label": 0
                },
                {
                    "sent": "So this will all the means and we have.",
                    "label": 0
                },
                {
                    "sent": "We also plotted the standard deviation because the results can be unstable but still.",
                    "label": 0
                },
                {
                    "sent": "Firm is always much better than the support vector machine.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I come to my conclusion so I talked about firm or feature importance ranking measure and my point of view.",
                    "label": 1
                },
                {
                    "sent": "Does four nights features so we we in principle we can use any any machine learning algorithm that we like and then plug it in as another posteriori analysis so we don't have to stick to linear past methods, we can just use what what perform performs best and then assess the importance.",
                    "label": 0
                },
                {
                    "sent": "It's a generalization of existing variable importance measures so that.",
                    "label": 1
                },
                {
                    "sent": "Fastest.",
                    "label": 0
                },
                {
                    "sent": "With some credibility and just set in this example of this substring analysis, we can also compute the importance of features that are not directly observed.",
                    "label": 1
                },
                {
                    "sent": "For example, for this sequence data we because we have the complete distribution of the input features, we could also assess the importance of the feature Gattaca which we didn't see in the in the training data, but because we know the distribution.",
                    "label": 0
                },
                {
                    "sent": "We can also do this or more generally speaking, if we have a feature that was not present in the training data but we.",
                    "label": 0
                },
                {
                    "sent": "What we know about how it relates to the other input features, we can also assess this.",
                    "label": 1
                },
                {
                    "sent": "And the fourth point, which I also think is very important, that is that it takes the dependence of the input features into account.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well first thank you with this we need first.",
                    "label": 0
                },
                {
                    "sent": "So now if I understand correctly, this is basically your last point.",
                    "label": 0
                },
                {
                    "sent": "You stress on modeling independence.",
                    "label": 0
                },
                {
                    "sent": "But then you middle data covariance matrix, right?",
                    "label": 0
                },
                {
                    "sent": "We work with Michael reading that we had 2000 features.",
                    "label": 0
                },
                {
                    "sent": "So do we have or more?",
                    "label": 0
                },
                {
                    "sent": "We had problems to estimate those cooperates matrix reliably in order to be able to remember or ways around.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so.",
                    "label": 0
                },
                {
                    "sent": "OK, that may be there two things.",
                    "label": 0
                },
                {
                    "sent": "If you have like 50 or 100,000 variables, you don't want to compute the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "But if you don't have too much examples, of course you can use that X.",
                    "label": 0
                },
                {
                    "sent": "Your data matrix is rectangular.",
                    "label": 0
                },
                {
                    "sent": "That saves you sometime.",
                    "label": 0
                },
                {
                    "sent": "But yeah, OK, but it's yeah.",
                    "label": 0
                },
                {
                    "sent": "So this is one point, but from the so I think.",
                    "label": 0
                },
                {
                    "sent": "So from of course, if you have very few data, your covariance matrix is maybe not the best choice, but I think the problem, which is that you have with this covariance matrix normally only turns up when when you invert it like to have to answer so that the first thing is, I think the covariance matrix is not doesn't have so many problems, as long as you don't invert it.",
                    "label": 0
                },
                {
                    "sent": "And second, there's a paper by chef French Trimmer, Unregularized, estimation of covariance matrix that's based on papers by lead Warren Wolf, where they can regularize it, and they can automatically compute the regularization from the data.",
                    "label": 0
                },
                {
                    "sent": "So as I said, I don't think that it's it's a very hard problem that affects it too much, and if it affects it then I would suggest something like this.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "Not next speaker.",
                    "label": 0
                }
            ]
        }
    }
}