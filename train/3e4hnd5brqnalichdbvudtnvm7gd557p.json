{
    "id": "3e4hnd5brqnalichdbvudtnvm7gd557p",
    "title": "Learning to Communicate with Deep Multi\u00ad-Agent Reinforcement Learning",
    "info": {
        "author": [
            "Jakob Foerster, Department of Computer Science, University of Oxford"
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_foerster_learning_communicate/",
    "segmentation": [
        [
            "My name is Jacob Forster.",
            "I work with Shimon Whiteson and Defreitas in Oxford.",
            "And first of all, before we get started is Nana's in Oxford.",
            "Yes, he is an Oxford.",
            "Do I do?",
            "I seem every week, no, but whenever I see ICM, I'm very happy to see him.",
            "So this work is really an emergent communication.",
            "It tries to address the question how can a team of agents learn to communicate in order to solve a task when no prior protocol has been provided, it's not an LP, it's not dialogue modeling.",
            "Yet it's really about.",
            "The learnability and the emergence of language amongst AI agents, and we're obviously not the first people to be fascinated with the role of communication and to wonder what does communication mean for human level intelligence, but also for the core human existence."
        ],
        [
            "In the beginning was the word, and the word was with God.",
            "Citing John ET al.",
            "from the Bible, it was also realized very early on 2600 years ago that communication and language is key in allowing teams of agents to collaborate and to coordinate as one people speaking the same language.",
            "They have begun to do this than nothing.",
            "They plan to do will be impossible.",
            "This topic of communication was discussed so fiercely in 1866 that the peace loving French issued a ban on the topic of the origin of human language.",
            "Now when we want to address."
        ],
        [
            "This topic of communication we have to study this in domains that take a few boxes.",
            "First of all, I'm not interested in schizophrenic people talking to themselves.",
            "We require multiple agents here you can see all the different minions lost and we also require an incentive to communicate.",
            "That means everybody here probably gets on.",
            "If we hated each other, we wouldn't talk to each other.",
            "She had rewards.",
            "Cooperative settings is the answer.",
            "We also have partials ability which means.",
            "That well, this is really loud.",
            "OK, we also possibility, which means the two Markov state that you've heard about today is not observable.",
            "Instead, agents have to memorize and have to communicate what's happening.",
            "Obviously communication requires a channel in these tasks.",
            "We're looking at settings that have a discrete communication channel between the agents.",
            "There's also differentiation between the centralized learning phase and the decentralized execution phase.",
            "During the centralized learning, agents are allowed to exchange extra information whether figuring out the policy.",
            "However, at the execution time only the information, the only information exchange is in the channel provided by the task.",
            "And I reiterate, there's no protocol provided.",
            "The agents have to learn they have to come up with their own language amongst themselves, and that's the challenge in this work.",
            "One of these is powered by the strong guy in the back DQ and now I hope everyone remembers DQ&Q learning.",
            "We're using deep Q networks to approximate value function Q functions."
        ],
        [
            "However, to use these networks to model communication, we have a number of options, and we're exploring two different methods here.",
            "One of them, the bad guys called rile, reinforce intelligent learning.",
            "The second one is called I'll differentiable intelligent learning and just focus on the R&D reinforced and differentiable.",
            "What this means in the reinforced interactive learning, we're treating the act to communicate like any other action.",
            "It gets selected based on the Q value that is produced for the action in a state.",
            "In the differential case, we're exchanging gradients across agents in order to decide what to communicate and how to communicate it."
        ],
        [
            "Now if that was confusing, this is an attempt at Victoria explaining what's happening in violent.",
            "I'll respectively what you can see here.",
            "Each agent is a Q network, and then roll it across two time steps, agent one and agent two.",
            "In the reinforced case, the Q network produces Q values both for the action and for the message, and then during training time when we update the gradients, they come from the environment based on the Q and error as they do in any decrease in case and get passed back the action selected.",
            "In network no gradients are exchanged across the different agents.",
            "In the differential case.",
            "We produce both iMessage activation and a Q value for the actions, but the message is now treated as differentiable entity that can be exchanged across agents, and what this means is, if I send a message to you guys I get back some feedback that says hey, you just confused us why you didn't confuse us and I can update my message with respect to the DQN error of the receiving agent.",
            "And that's illustrated by the Red Tower here, which is the gradient that's being passed back from the recipient agent to the sending agent.",
            "So effectively updating my weights as a sender to reduce the DQ error of the downstream agents that have received my message.",
            "Now, if this is still confusing to you, you can think of it as assembling one big network on the fly, where the message with the channel represents a bottleneck between those different agents.",
            "And if you got it now, then you should be confused, because how can I differentiate with respect to discrete message?",
            "Well, obviously that's challenging.",
            "Using standard back prop.",
            "So what we do is that during training we actually model the message as a continuous variable because we are in a centralized learning phase, I'm allowed to exchange extra information and in this case the extra information is the gradient.",
            "Of the message of the dictionary with respect to the message.",
            "Obviously, at execution time we need to discretize this discontinuous variable into something that fits the channel and the channel is a single bit that can be exchanged between the agents.",
            "And to be able to do this.",
            "We came up with a little trick which is quite similar tricks being used in the past, and that is to add noise to the training to the message in the channel.",
            "Because EU, the discretize regularize unit."
        ],
        [
            "And what we're showing here is basically for some given input message M. We send the signal through sigmoid, but also add Gaussian noise and the Gaussian noise.",
            "Disturbs values in the middle of the range more than at details, so it encourages the agents to learn during training of protocol that can be discretized execution time and then obviously at execution time we have to discretize into binary messages.",
            "Can you guys not if you're bored?",
            "OK."
        ],
        [
            "Is going to get more confusing?",
            "OK, so obviously before these possibilities, so we need memory.",
            "We're doing this by using GI use.",
            "There's two layers used in the middle.",
            "This is showing the Gru for one agent unfolded overtime, so really, for multiple agents, each agent is one of these networks.",
            "You have to draw a 3D diagram which gets really busy, so inputs are processed by the GI use, producing the outputs.",
            "That's the Q values and these message values that get passed through the Du.",
            "The task we're looking at."
        ],
        [
            "Is a relatively well known Riddle.",
            "It's formulated as follows.",
            "100 prisoners are in individual cells.",
            "Each day one of these prisoners is picked to go to interrogation room.",
            "In this room is a light bulb.",
            "The task of the prisoners is to figure out if every single one of them has been to this room.",
            "If that's the case, they can choose the action tell.",
            "If it's right, everyone is spin, they survive.",
            "If it's false, they will die.",
            "The challenge is the only way to communicate who has been to this room, because every day somebody random goes with replacement is by using the light bulb in the room as a binary channel.",
            "This would have been formulated for humans, but we can obviously reformulated as a deep multi agent RL problem by replacing every single one of these hundred prisoners with the DQ and network.",
            "And then you get the other formulation as shown below.",
            "What's the solution?",
            "You can never spoiler Riddle, but I'll give you.",
            "I'll give you answers for the case of a small number of agents, not 400, and then you can think about the 100 agents case later, But basically we can reformat this Riddle if you haven't heard about it before, it's pretty cool.",
            "Think about it and we can now represent this as a multi agent enforcement problem with communication where this one bit that is the light bulb is represented as a channel that we saw before in this wild diagrams with the red hours."
        ],
        [
            "And this sounds nice and cute, but it's actually really, really hard because it's a.",
            "It's a.",
            "It's a non mark of the Markov states entire operation history.",
            "Therefore the policy space grows double exponential time horizon, so even for small problem like 4 prisoners there's 4 to 350,000 different policies in the policy space model policy space.",
            "On the."
        ],
        [
            "Results the good news is it works.",
            "The other good news is good guy beats bad guy.",
            "Remember dial and rile on these on these slides we see the results for three and four prisoners and really using gradients using differentiation across different agents in order to learn what to communicate.",
            "The green line beats by far the blue line which is right and the 2nd result is that parameter sharing across these networks really helps to speed up learning.",
            "So I didn't mention before we experiment with and without parameter sharing parameter sharing the continuous lines beat the dashed lines.",
            "The Orange line is non communication baseline.",
            "So reinforce running works even without communication the communication task.",
            "But obviously you can't get for rewards for business.",
            "The results are similar.",
            "Again good guy bad guy, but what's quite impressive is that without parameter sharing, the reinforcement based learning doesn't exceed the non communication baseline.",
            "Now, Joshua's earlier, what's the strategy?"
        ],
        [
            "Well, you can think about the 100 agent case afterwards, but for three prisoners there is a wonderful encoding of mapping the true mark of state who has been to the switch position because only one or two prisoners could have been.",
            "If three prisoners have been, I wouldn't be in the room because the game would be over, so the prisoners learned in this.",
            "In this mildly general setting, to encode the underlying state in the communication channel.",
            "Such that if I go in and I see that the light is actually on, then I can terminate the game because two others have been assuming I haven't been in the room before.",
            "That's pretty quick.",
            "This actually learned they figured this out before me were just kind of embarrassing superhuman AI.",
            "Maybe already there assuming.",
            "That the research is dumb enough.",
            "OK, the next class."
        ],
        [
            "The problem is."
        ],
        [
            "About using visual input and figuring seeing if agents can learn to map from a very high dimensional visual input into a low dimensional channel and to communicate in that way and what we're seeing here is each agent receives a handwritten digit.",
            "MNIST 'cause everyone loves them list and they can across five time steps send a single bit of information at every time step at the end.",
            "At step five they each have to guess the digit of the other agent.",
            "Now obviously computer science wise this is easy.",
            "We have 4 bits, ten options easy, but imagine they don't know what they're looking at and they have to learn to recognize this as well as to communicate what's happening."
        ],
        [
            "On the results side.",
            "But you can see here in this case differential interaction learning again, works fantastically.",
            "Reinforce learning doesn't exceed the baseline, and we can again abstract out a policy.",
            "What have these agents actually learned, and it shouldn't be surprising that they achieve perfect performance.",
            "There needs to be a perfect code and they really learn a binary code.",
            "They learn to map for 20 or 28 bits into these single bit messages across four time steps, and that's what you can see here at every time step.",
            "Depending on the two digit, the agents looking at, they send a unique code and the other agent learns decode this information, then act appropriately maximizing reward."
        ],
        [
            "What is also interesting I mentioned before that noise is required in order to learn binarize aghbal activations, and that's illustrated in this slide, so without noise.",
            "On the left, yes.",
            "On the left you can see that the agent learns single activation.",
            "That's in the middle of the around 0.",
            "What this means is that if you discretize it into zero and one will make you just causation error.",
            "However, adding noise signal of two during learning the distribution clearly splits into two separate peaks.",
            "And that's really what we want.",
            "We want to be at the point where we can discretize the learn protocol that was done with continuous variables using back prop without introducing discretization error."
        ],
        [
            "There's a lot of fantastic future work ahead.",
            "We want to scale to more ambitious games, something like Counter Strike, where my players can chat to each other on a chat tool first.",
            "Want to look at my serial settings where there's no share reward, potential, strategic communication?",
            "Maybe we do have to talk to people that we don't necessarily like we obviously there's also another area of interesting work which is around teaching and learning as a form of communication."
        ],
        [
            "Last but not least, conclusions.",
            "So what we've seen so far is the initial work, showing that AI agents really can discover languages in order to solve tasks.",
            "There was nothing here provided.",
            "There was nothing that said, this is how you encode or decode information.",
            "It's an emergent communication as part of solving the task.",
            "This shouldn't be a surprise for anyone here.",
            "Deep learning is based on gradients, but gradients do really help in that process of learning to communicate.",
            "Parameter sharing can also accelerate the training because it allows agents to aggregate the experience across all the different episodes.",
            "The noise was essential in allowing the distribution to be binarized.",
            "But for me, one of the more exciting parts of this research was to actually look at the protocols and to learn something about the problem that I hadn't solved myself and to say, hey, these agents have figured out a way of doing this.",
            "What are they actually doing?"
        ],
        [
            "With that I would like to thank you for your attention and."
        ],
        [
            "Articulate."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is Jacob Forster.",
                    "label": 0
                },
                {
                    "sent": "I work with Shimon Whiteson and Defreitas in Oxford.",
                    "label": 1
                },
                {
                    "sent": "And first of all, before we get started is Nana's in Oxford.",
                    "label": 0
                },
                {
                    "sent": "Yes, he is an Oxford.",
                    "label": 0
                },
                {
                    "sent": "Do I do?",
                    "label": 0
                },
                {
                    "sent": "I seem every week, no, but whenever I see ICM, I'm very happy to see him.",
                    "label": 0
                },
                {
                    "sent": "So this work is really an emergent communication.",
                    "label": 0
                },
                {
                    "sent": "It tries to address the question how can a team of agents learn to communicate in order to solve a task when no prior protocol has been provided, it's not an LP, it's not dialogue modeling.",
                    "label": 0
                },
                {
                    "sent": "Yet it's really about.",
                    "label": 0
                },
                {
                    "sent": "The learnability and the emergence of language amongst AI agents, and we're obviously not the first people to be fascinated with the role of communication and to wonder what does communication mean for human level intelligence, but also for the core human existence.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the beginning was the word, and the word was with God.",
                    "label": 1
                },
                {
                    "sent": "Citing John ET al.",
                    "label": 1
                },
                {
                    "sent": "from the Bible, it was also realized very early on 2600 years ago that communication and language is key in allowing teams of agents to collaborate and to coordinate as one people speaking the same language.",
                    "label": 1
                },
                {
                    "sent": "They have begun to do this than nothing.",
                    "label": 1
                },
                {
                    "sent": "They plan to do will be impossible.",
                    "label": 0
                },
                {
                    "sent": "This topic of communication was discussed so fiercely in 1866 that the peace loving French issued a ban on the topic of the origin of human language.",
                    "label": 0
                },
                {
                    "sent": "Now when we want to address.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This topic of communication we have to study this in domains that take a few boxes.",
                    "label": 0
                },
                {
                    "sent": "First of all, I'm not interested in schizophrenic people talking to themselves.",
                    "label": 0
                },
                {
                    "sent": "We require multiple agents here you can see all the different minions lost and we also require an incentive to communicate.",
                    "label": 0
                },
                {
                    "sent": "That means everybody here probably gets on.",
                    "label": 0
                },
                {
                    "sent": "If we hated each other, we wouldn't talk to each other.",
                    "label": 0
                },
                {
                    "sent": "She had rewards.",
                    "label": 0
                },
                {
                    "sent": "Cooperative settings is the answer.",
                    "label": 0
                },
                {
                    "sent": "We also have partials ability which means.",
                    "label": 0
                },
                {
                    "sent": "That well, this is really loud.",
                    "label": 0
                },
                {
                    "sent": "OK, we also possibility, which means the two Markov state that you've heard about today is not observable.",
                    "label": 0
                },
                {
                    "sent": "Instead, agents have to memorize and have to communicate what's happening.",
                    "label": 0
                },
                {
                    "sent": "Obviously communication requires a channel in these tasks.",
                    "label": 0
                },
                {
                    "sent": "We're looking at settings that have a discrete communication channel between the agents.",
                    "label": 1
                },
                {
                    "sent": "There's also differentiation between the centralized learning phase and the decentralized execution phase.",
                    "label": 0
                },
                {
                    "sent": "During the centralized learning, agents are allowed to exchange extra information whether figuring out the policy.",
                    "label": 0
                },
                {
                    "sent": "However, at the execution time only the information, the only information exchange is in the channel provided by the task.",
                    "label": 0
                },
                {
                    "sent": "And I reiterate, there's no protocol provided.",
                    "label": 1
                },
                {
                    "sent": "The agents have to learn they have to come up with their own language amongst themselves, and that's the challenge in this work.",
                    "label": 0
                },
                {
                    "sent": "One of these is powered by the strong guy in the back DQ and now I hope everyone remembers DQ&Q learning.",
                    "label": 0
                },
                {
                    "sent": "We're using deep Q networks to approximate value function Q functions.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, to use these networks to model communication, we have a number of options, and we're exploring two different methods here.",
                    "label": 0
                },
                {
                    "sent": "One of them, the bad guys called rile, reinforce intelligent learning.",
                    "label": 0
                },
                {
                    "sent": "The second one is called I'll differentiable intelligent learning and just focus on the R&D reinforced and differentiable.",
                    "label": 0
                },
                {
                    "sent": "What this means in the reinforced interactive learning, we're treating the act to communicate like any other action.",
                    "label": 0
                },
                {
                    "sent": "It gets selected based on the Q value that is produced for the action in a state.",
                    "label": 0
                },
                {
                    "sent": "In the differential case, we're exchanging gradients across agents in order to decide what to communicate and how to communicate it.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now if that was confusing, this is an attempt at Victoria explaining what's happening in violent.",
                    "label": 0
                },
                {
                    "sent": "I'll respectively what you can see here.",
                    "label": 0
                },
                {
                    "sent": "Each agent is a Q network, and then roll it across two time steps, agent one and agent two.",
                    "label": 0
                },
                {
                    "sent": "In the reinforced case, the Q network produces Q values both for the action and for the message, and then during training time when we update the gradients, they come from the environment based on the Q and error as they do in any decrease in case and get passed back the action selected.",
                    "label": 0
                },
                {
                    "sent": "In network no gradients are exchanged across the different agents.",
                    "label": 0
                },
                {
                    "sent": "In the differential case.",
                    "label": 0
                },
                {
                    "sent": "We produce both iMessage activation and a Q value for the actions, but the message is now treated as differentiable entity that can be exchanged across agents, and what this means is, if I send a message to you guys I get back some feedback that says hey, you just confused us why you didn't confuse us and I can update my message with respect to the DQN error of the receiving agent.",
                    "label": 0
                },
                {
                    "sent": "And that's illustrated by the Red Tower here, which is the gradient that's being passed back from the recipient agent to the sending agent.",
                    "label": 0
                },
                {
                    "sent": "So effectively updating my weights as a sender to reduce the DQ error of the downstream agents that have received my message.",
                    "label": 0
                },
                {
                    "sent": "Now, if this is still confusing to you, you can think of it as assembling one big network on the fly, where the message with the channel represents a bottleneck between those different agents.",
                    "label": 0
                },
                {
                    "sent": "And if you got it now, then you should be confused, because how can I differentiate with respect to discrete message?",
                    "label": 0
                },
                {
                    "sent": "Well, obviously that's challenging.",
                    "label": 0
                },
                {
                    "sent": "Using standard back prop.",
                    "label": 0
                },
                {
                    "sent": "So what we do is that during training we actually model the message as a continuous variable because we are in a centralized learning phase, I'm allowed to exchange extra information and in this case the extra information is the gradient.",
                    "label": 0
                },
                {
                    "sent": "Of the message of the dictionary with respect to the message.",
                    "label": 0
                },
                {
                    "sent": "Obviously, at execution time we need to discretize this discontinuous variable into something that fits the channel and the channel is a single bit that can be exchanged between the agents.",
                    "label": 0
                },
                {
                    "sent": "And to be able to do this.",
                    "label": 0
                },
                {
                    "sent": "We came up with a little trick which is quite similar tricks being used in the past, and that is to add noise to the training to the message in the channel.",
                    "label": 0
                },
                {
                    "sent": "Because EU, the discretize regularize unit.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we're showing here is basically for some given input message M. We send the signal through sigmoid, but also add Gaussian noise and the Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "Disturbs values in the middle of the range more than at details, so it encourages the agents to learn during training of protocol that can be discretized execution time and then obviously at execution time we have to discretize into binary messages.",
                    "label": 0
                },
                {
                    "sent": "Can you guys not if you're bored?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is going to get more confusing?",
                    "label": 0
                },
                {
                    "sent": "OK, so obviously before these possibilities, so we need memory.",
                    "label": 0
                },
                {
                    "sent": "We're doing this by using GI use.",
                    "label": 0
                },
                {
                    "sent": "There's two layers used in the middle.",
                    "label": 0
                },
                {
                    "sent": "This is showing the Gru for one agent unfolded overtime, so really, for multiple agents, each agent is one of these networks.",
                    "label": 0
                },
                {
                    "sent": "You have to draw a 3D diagram which gets really busy, so inputs are processed by the GI use, producing the outputs.",
                    "label": 0
                },
                {
                    "sent": "That's the Q values and these message values that get passed through the Du.",
                    "label": 0
                },
                {
                    "sent": "The task we're looking at.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a relatively well known Riddle.",
                    "label": 0
                },
                {
                    "sent": "It's formulated as follows.",
                    "label": 0
                },
                {
                    "sent": "100 prisoners are in individual cells.",
                    "label": 0
                },
                {
                    "sent": "Each day one of these prisoners is picked to go to interrogation room.",
                    "label": 0
                },
                {
                    "sent": "In this room is a light bulb.",
                    "label": 0
                },
                {
                    "sent": "The task of the prisoners is to figure out if every single one of them has been to this room.",
                    "label": 0
                },
                {
                    "sent": "If that's the case, they can choose the action tell.",
                    "label": 0
                },
                {
                    "sent": "If it's right, everyone is spin, they survive.",
                    "label": 0
                },
                {
                    "sent": "If it's false, they will die.",
                    "label": 0
                },
                {
                    "sent": "The challenge is the only way to communicate who has been to this room, because every day somebody random goes with replacement is by using the light bulb in the room as a binary channel.",
                    "label": 0
                },
                {
                    "sent": "This would have been formulated for humans, but we can obviously reformulated as a deep multi agent RL problem by replacing every single one of these hundred prisoners with the DQ and network.",
                    "label": 0
                },
                {
                    "sent": "And then you get the other formulation as shown below.",
                    "label": 0
                },
                {
                    "sent": "What's the solution?",
                    "label": 0
                },
                {
                    "sent": "You can never spoiler Riddle, but I'll give you.",
                    "label": 0
                },
                {
                    "sent": "I'll give you answers for the case of a small number of agents, not 400, and then you can think about the 100 agents case later, But basically we can reformat this Riddle if you haven't heard about it before, it's pretty cool.",
                    "label": 0
                },
                {
                    "sent": "Think about it and we can now represent this as a multi agent enforcement problem with communication where this one bit that is the light bulb is represented as a channel that we saw before in this wild diagrams with the red hours.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this sounds nice and cute, but it's actually really, really hard because it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "It's a non mark of the Markov states entire operation history.",
                    "label": 0
                },
                {
                    "sent": "Therefore the policy space grows double exponential time horizon, so even for small problem like 4 prisoners there's 4 to 350,000 different policies in the policy space model policy space.",
                    "label": 0
                },
                {
                    "sent": "On the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results the good news is it works.",
                    "label": 0
                },
                {
                    "sent": "The other good news is good guy beats bad guy.",
                    "label": 0
                },
                {
                    "sent": "Remember dial and rile on these on these slides we see the results for three and four prisoners and really using gradients using differentiation across different agents in order to learn what to communicate.",
                    "label": 0
                },
                {
                    "sent": "The green line beats by far the blue line which is right and the 2nd result is that parameter sharing across these networks really helps to speed up learning.",
                    "label": 0
                },
                {
                    "sent": "So I didn't mention before we experiment with and without parameter sharing parameter sharing the continuous lines beat the dashed lines.",
                    "label": 0
                },
                {
                    "sent": "The Orange line is non communication baseline.",
                    "label": 0
                },
                {
                    "sent": "So reinforce running works even without communication the communication task.",
                    "label": 0
                },
                {
                    "sent": "But obviously you can't get for rewards for business.",
                    "label": 0
                },
                {
                    "sent": "The results are similar.",
                    "label": 0
                },
                {
                    "sent": "Again good guy bad guy, but what's quite impressive is that without parameter sharing, the reinforcement based learning doesn't exceed the non communication baseline.",
                    "label": 0
                },
                {
                    "sent": "Now, Joshua's earlier, what's the strategy?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, you can think about the 100 agent case afterwards, but for three prisoners there is a wonderful encoding of mapping the true mark of state who has been to the switch position because only one or two prisoners could have been.",
                    "label": 0
                },
                {
                    "sent": "If three prisoners have been, I wouldn't be in the room because the game would be over, so the prisoners learned in this.",
                    "label": 0
                },
                {
                    "sent": "In this mildly general setting, to encode the underlying state in the communication channel.",
                    "label": 0
                },
                {
                    "sent": "Such that if I go in and I see that the light is actually on, then I can terminate the game because two others have been assuming I haven't been in the room before.",
                    "label": 0
                },
                {
                    "sent": "That's pretty quick.",
                    "label": 0
                },
                {
                    "sent": "This actually learned they figured this out before me were just kind of embarrassing superhuman AI.",
                    "label": 0
                },
                {
                    "sent": "Maybe already there assuming.",
                    "label": 0
                },
                {
                    "sent": "That the research is dumb enough.",
                    "label": 0
                },
                {
                    "sent": "OK, the next class.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About using visual input and figuring seeing if agents can learn to map from a very high dimensional visual input into a low dimensional channel and to communicate in that way and what we're seeing here is each agent receives a handwritten digit.",
                    "label": 0
                },
                {
                    "sent": "MNIST 'cause everyone loves them list and they can across five time steps send a single bit of information at every time step at the end.",
                    "label": 0
                },
                {
                    "sent": "At step five they each have to guess the digit of the other agent.",
                    "label": 0
                },
                {
                    "sent": "Now obviously computer science wise this is easy.",
                    "label": 0
                },
                {
                    "sent": "We have 4 bits, ten options easy, but imagine they don't know what they're looking at and they have to learn to recognize this as well as to communicate what's happening.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the results side.",
                    "label": 0
                },
                {
                    "sent": "But you can see here in this case differential interaction learning again, works fantastically.",
                    "label": 0
                },
                {
                    "sent": "Reinforce learning doesn't exceed the baseline, and we can again abstract out a policy.",
                    "label": 0
                },
                {
                    "sent": "What have these agents actually learned, and it shouldn't be surprising that they achieve perfect performance.",
                    "label": 0
                },
                {
                    "sent": "There needs to be a perfect code and they really learn a binary code.",
                    "label": 0
                },
                {
                    "sent": "They learn to map for 20 or 28 bits into these single bit messages across four time steps, and that's what you can see here at every time step.",
                    "label": 0
                },
                {
                    "sent": "Depending on the two digit, the agents looking at, they send a unique code and the other agent learns decode this information, then act appropriately maximizing reward.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is also interesting I mentioned before that noise is required in order to learn binarize aghbal activations, and that's illustrated in this slide, so without noise.",
                    "label": 0
                },
                {
                    "sent": "On the left, yes.",
                    "label": 0
                },
                {
                    "sent": "On the left you can see that the agent learns single activation.",
                    "label": 0
                },
                {
                    "sent": "That's in the middle of the around 0.",
                    "label": 0
                },
                {
                    "sent": "What this means is that if you discretize it into zero and one will make you just causation error.",
                    "label": 0
                },
                {
                    "sent": "However, adding noise signal of two during learning the distribution clearly splits into two separate peaks.",
                    "label": 0
                },
                {
                    "sent": "And that's really what we want.",
                    "label": 0
                },
                {
                    "sent": "We want to be at the point where we can discretize the learn protocol that was done with continuous variables using back prop without introducing discretization error.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a lot of fantastic future work ahead.",
                    "label": 0
                },
                {
                    "sent": "We want to scale to more ambitious games, something like Counter Strike, where my players can chat to each other on a chat tool first.",
                    "label": 0
                },
                {
                    "sent": "Want to look at my serial settings where there's no share reward, potential, strategic communication?",
                    "label": 0
                },
                {
                    "sent": "Maybe we do have to talk to people that we don't necessarily like we obviously there's also another area of interesting work which is around teaching and learning as a form of communication.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Last but not least, conclusions.",
                    "label": 0
                },
                {
                    "sent": "So what we've seen so far is the initial work, showing that AI agents really can discover languages in order to solve tasks.",
                    "label": 0
                },
                {
                    "sent": "There was nothing here provided.",
                    "label": 0
                },
                {
                    "sent": "There was nothing that said, this is how you encode or decode information.",
                    "label": 0
                },
                {
                    "sent": "It's an emergent communication as part of solving the task.",
                    "label": 0
                },
                {
                    "sent": "This shouldn't be a surprise for anyone here.",
                    "label": 0
                },
                {
                    "sent": "Deep learning is based on gradients, but gradients do really help in that process of learning to communicate.",
                    "label": 0
                },
                {
                    "sent": "Parameter sharing can also accelerate the training because it allows agents to aggregate the experience across all the different episodes.",
                    "label": 0
                },
                {
                    "sent": "The noise was essential in allowing the distribution to be binarized.",
                    "label": 0
                },
                {
                    "sent": "But for me, one of the more exciting parts of this research was to actually look at the protocols and to learn something about the problem that I hadn't solved myself and to say, hey, these agents have figured out a way of doing this.",
                    "label": 0
                },
                {
                    "sent": "What are they actually doing?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With that I would like to thank you for your attention and.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Articulate.",
                    "label": 0
                }
            ]
        }
    }
}