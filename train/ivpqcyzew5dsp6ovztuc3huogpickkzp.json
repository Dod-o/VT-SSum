{
    "id": "ivpqcyzew5dsp6ovztuc3huogpickkzp",
    "title": "A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization",
    "info": {
        "author": [
            "Lu Wang, Department of Computer Science, Cornell University"
        ],
        "published": "Oct. 2, 2013",
        "recorded": "August 2013",
        "category": [
            "Top->Computer Science->Computational Linguistics"
        ]
    },
    "url": "http://videolectures.net/acl2013_wang_sentence/",
    "segmentation": [
        [
            "Hello everybody, I'm going to present this joint work with IBM Research."
        ],
        [
            "So the query focused multi document summarization is defined in a way that given a complex query in a set of relevant documents, we would like to generate a fluent and concept summary that can answer that query."
        ],
        [
            "Document Understanding Conference has fostered this task since 2004.",
            "However, up to now the most top performing systems are still extractive or bunch of machine learning based approaches has been proposed to generate extractive summaries."
        ],
        [
            "Unfortunately, extractive summaries are not compact enough.",
            "For example, in the news where data, the sentences can be as long as 60 or 70 words, and those lenses and entities that are partially relevant to the query can be other excluded from the summary or prevent the selection of other important sentences.",
            "So, in addition, if you look at a human construct, abstracts people tend to abstract the content and seldom used.",
            "Entire sentence from the original documents and you can observe paraphrase, obstruction, reordering and sentence compression in human rating.",
            "Summaries.",
            "In this work I will focus on the sentence compression."
        ],
        [
            "Here's an example.",
            "Given the Cori check, the thread of the West Nile virus through the United States and efforts taken to control it, we retrieve one relevant sentence from the documents.",
            "And if you look at this sentence, we can definitely safely remove the last part of it and return the remaining information as part of the answer."
        ],
        [
            "Here's another example.",
            "Given the query, in what ways have stolen artworks being recovered?",
            "How often are suspects arrested for all prosecuted for the steps from one human written abstract will find this person use a sentence from the original document, but remove those unnecessary details from it.",
            "And this sentence compression operation has been observed a lot in those human written abstracts."
        ],
        [
            "So in general, to summarize the contributions we have in this work, we first present this learning based sentence compression framework for Corey focused multi document summarization.",
            "Then we implement a beam search decoder to efficiently find highly probable compressions.",
            "We will show how to increase integrate various indicative metrics such as linguistic motivation and core relevance into this compression process by evaluation on standard corpora.",
            "We show that our approach can provide significantly improvement over pure extraction based methods in both automatic and human evaluation."
        ],
        [
            "Our work is related to sentence compression.",
            "For example, Netan marquee is noisy channel model to generate compressions and then Gary and Michael and extend it by using this synchronous context.",
            "Free grammars also discriminative learning has also been investigated for deciding if a term should be removed based on syntax information.",
            "Clark and Lopata further integrate discourse structure to guide this compression process."
        ],
        [
            "In addition, our work is also inspired by those sentence compression to multi document summarization work.",
            "Previously people have been using heuristic rules to generate multiple alternative compressions and rank them along with the original sentences only.",
            "Currently Martins and Smith present the learning model based on the dependence Tree an which can determine if a node should be removed.",
            "Based on this, Berg Kirkpatrick at all use a discriminative learning.",
            "To drawing model, distraction and the compression at the same time."
        ],
        [
            "Offering work consists of mainly three steps.",
            "Firstly we use sentence ranking to determine the importance of each sentence given the query.",
            "Then we use the sentence compression model to internally generates most likely compression and then we applies coreference resolution Anna sentence ordering to produce summaries of better quality."
        ],
        [
            "Now I'm going to briefly discuss sentence ranking model."
        ],
        [
            "We experiment with support vector regression and Lambda Mart love.",
            "The Mart is the state of the Art for information retrieval and by the hasn't been used in summarization.",
            "We trained on the 2005 corpus along with their manually constructed abstracts.",
            "We use root to score as the objective."
        ],
        [
            "Here's some sample features.",
            "We use a query relevant features such as unigram or background TF added personality.",
            "We also consider query independent features such as the relative position or length of the sentence."
        ],
        [
            "After distances ranking we got a bunch of important sentences.",
            "Then we will apply the sentence compression tech."
        ],
        [
            "So for a long time, people haven't successfully applied sentence compression technique for summarization.",
            "Becausw without a carefully design technique is very easy to generate.",
            "Angry medical summary."
        ],
        [
            "So.",
            "We will propose a parse tree based compression technique, where each sentence is represented by its parse tree.",
            "And this work is in line.",
            "This approach is in language.",
            "The syntax driven approaches where we do not want to produce a new parse tree, but instead we will identify a set of nodes to remove to be removed."
        ],
        [
            "Here is an example.",
            "We have sentence.",
            "Malaria causes millions of deaths according to WHL."
        ],
        [
            "We would like to remove the last part because it might not be relevant to the query."
        ],
        [
            "We first represented us this parse tree.",
            "And we."
        ],
        [
            "Identify the set of nodes will be removed in the process.",
            "If a node has been removed in the process, all those words subsumed by this node will be dropped from the sentence."
        ],
        [
            "Formally, our algorithm takes us input or parse tree T. And the T is represented as list of ordered consonants according to a given tree traversal algorithm.",
            "For example, like post order coversyl.",
            "And our system will output a set of labels corresponding to each node in T. And the label is one of return.",
            "Remove partial remove."
        ],
        [
            "When is the return?",
            "It means the current node and all of these children will be preserved in the."
        ],
        [
            "For example, like the NP node in this example.",
            "Similarly for."
        ],
        [
            "So the current node and all of his shooters will be dropped during the compression process.",
            "All.",
            "Like the grayed out PP node."
        ],
        [
            "For partial."
        ],
        [
            "We move on.",
            "It means some of the children has been removed and the other children have been preserved."
        ],
        [
            "In this example, like the VP node in our parts tree."
        ],
        [
            "RBM."
        ],
        [
            "So now I'm going to present our beam search decoder to find the highly probable compressions for the given sentence.",
            "Our beam search decoder will consider all of the ordered con students in sequence.",
            "In each iteration, all of the sentence compression hypothesis will be expanded by a single new node.",
            "Before that, we need to score to rank all those themes to rank all those hypothesis in our bin."
        ],
        [
            "So the first scorer we are proposing here is called as BASIC, which takes a subsequence of the order nodes, nodes, label and output some log probability of the label given that node."
        ],
        [
            "Here's an example, assuming we're at the red node prepositional phrase PD.",
            "We're using a post order traversal.",
            "All it is also shown here is the top three scored beam.",
            "The next level with it is is parent."
        ],
        [
            "And depending on the label of this current node, we will update the score for each beam there.",
            "Similarly, we always did with Peanut."
        ],
        [
            "Anyway, again, updated score for each team.",
            "At the end of the process, we will use a language model, train a gig world data to rerank all those beams, and output the first one as the final will be included in the final summary."
        ],
        [
            "And for learning the probability of the label, we use a maximum entropy classifier."
        ],
        [
            "Anitta trained on the data from cloud and laptop to laptop 2008.",
            "And it contains 82 news where article with one manually produced compression along with each other in a sentence.",
            "Simple features include context dependence, relation, Anna hand node information."
        ],
        [
            "In addition to this statistical model, we also can see their linguistically motivated compression rules, because Turner and China have shown that applying those rules can improve both the content and language quality."
        ],
        [
            "Let's look at this example again.",
            "This human written this human constructed abstract sentence in this human construct sentence.",
            "Our human annotator removes unnecessary details such as the modifier or complaints or time information.",
            "So."
        ],
        [
            "For phrases like relative date.",
            "In"
        ],
        [
            "Central Attribution, we can just safely remove it."
        ],
        [
            "We synthesize a list of rules from existing work an encode them as the features for each node.",
            "Whenever the feature function is fired, we will for for that node we will set the probability of the label, remove to one."
        ],
        [
            "Our motto is flexible in the sense that we can change the tree traversal algorithm or we can tailor the scoring function to our summarization task."
        ],
        [
            "I have talked about a post order travel so already."
        ],
        [
            "Now I'm going to introduce to other search algorithms.",
            "One is called content aware search.",
            "This is based on the intuition that the prediction on the context node can help with the presentation on the current node.",
            "So where we use the context labels as the new features for the prediction on the current node?",
            "Another"
        ],
        [
            "Search algorithm is called have driven search.",
            "This is we will with it handled first, then other nodes later.",
            "This is intuitive because if a head node has been removed, we don't need to preserve its modifier anymore."
        ],
        [
            "The other flexibility is are lying in the scorer we have up to now we only have a scorer to measure the compression probability of each node."
        ],
        [
            "But we can extend it by adding like Corey relevance score.",
            "We also measure the importance of the current hypothesis.",
            "It takes the form of the sum of some basic score.",
            "This some basis core is adopted from the work from Twitter Nova in 2007.",
            "Again, we use this train language model to compute the probability of the current hypothesis.",
            "We also consider crowds sentence redundancy.",
            "Now we have a modest core, takes the form of the linear interpolation of different indicative metrics here, and those parameters will be tuned on the test set on the development set."
        ],
        [
            "At the end we simply replace each pronoun with is a reference an.",
            "We ordered a sentence based on the time step and then the position in a source document."
        ],
        [
            "We valued our approach on 2 standard corpora.",
            "One is stuck 2006 and the other ones doctors on 7.",
            "We for automatic evaluation we use route which is standard metric for summarization, evaluation for human evaluation we first use pyramid, which measures the content coverage or which is beyond lesco similarity, and for linguistic quality we also ask people to measure the fluency or grammaticality of our generated summary."
        ],
        [
            "Firstly, we compare with the best duck system in this to Corporal.",
            "And then we compare that with at all or which work reportes the best results up to now on this to corporate and they use a latent semantic analysis based approach.",
            "An IT is extractive or based method.",
            "And then we also show to the results from our tour anchors here.",
            "Before testing our compression based tree based compression based system, we want to just apply those heroes rules and see how does it work.",
            "And from the results we can see that the RT score already got improved.",
            "And then we test our post order based compressor along with the basic scorer and we can see that we got a better compression rate.",
            "Andrew Jazz you Foursquare also got like improved.",
            "Again, we apply our context aware search with the basic score and also have driven search with the basic score an for both of the experiment.",
            "We observed that they can remove further, remove the redundancy in the summary and improve the rule as you first score.",
            "So to combine all those like factors into one system, the final system used to have driven search with the multiple scorer Anet.",
            "Get best performance on both Route 2 score and Root as a silver score, and on both Scarborough.",
            "It's on the last line here."
        ],
        [
            "For human evaluation we first here for different human judges, to evaluate our different linguistic quality metrics, like whether it is grammatical or horrid, and it is, and we compare our system with two systems.",
            "One is the best system with the highest root score, the other one is the best system with the highest overall linguist quality.",
            "From these five different metrics, we can see that our system can produce comparable performance with those type of performing system in the dark competition."
        ],
        [
            "After dis linguistic quality evaluation will also ask people to evaluate by using the pyramid score, and we can see that our system produced statistically significant better result than the best stock system with high salute score."
        ],
        [
            "In addition, we also want evaluated sentence compression component in our system, independent of the summarization.",
            "So we use the data set from Clark Analoga 2008 and compared with three systems here, one is called the hedge.",
            "Trimmer is a rule based compression system.",
            "The second one is McDonald's 2006 is based on a discriminative learning with software syntactic evidence.",
            "The third one is the Martins and Smith.",
            "They implement our dependency tree based compressor.",
            "We use the unigram precision, recall and F matter according to Martins and Smith work.",
            "And we also measured it bends relations overlap between the original sentence and compression."
        ],
        [
            "So here's the results for the three compared systems.",
            "Again, we first just apply the rules on it and see how does it work.",
            "From the result we can see on the compression rate is a little bit higher than the compare systems compared systems, which means it is not that effective to remove those ordinances.",
            "And then we test it on our tree based compression system.",
            "And the results show that on the last line we can see our head driven based compressor can produce significantly better unigram precision and the relation effort matter when it.",
            "Has a lower complication rate, which means we remove more redundancy."
        ],
        [
            "Here's a simple system output.",
            "Given the query, however, the bombings of the US embassies in Kenya and Tanzania conducted what terrorism terrorist groups and individuals were responsible.",
            "How and where would attacks planned?",
            "Our system output?",
            "A summary which is drawn here.",
            "We can see that our system can successfully remove some unnecessary details and also produce this fluent summary."
        ],
        [
            "To conclude.",
            "We have presented a framework for query focused multi document, multi document summarization based on the sentence compression technique and we also show that our system produce significantly better results than the pure extraction based methods and the state of the art in both automatic and human evaluation.",
            "Thanks."
        ],
        [
            "So in your current system, you're doing the first ranking.",
            "Then you are doing the compression.",
            "Then you are reordering right?",
            "So any idea of how these three tasks can be done jointly at the same time?",
            "So when you extract you do the compression.",
            "You do the importance measure and you do the end of the ordering.",
            "We have been thinking about that, but.",
            "So, so this work.",
            "Actually we focus more on the compression part and see how to produce the grammatical compressions.",
            "And for existing work there is a more, but they're doing the extraction and summarization compression at the same time, but they never report how much information has been removed and how does the result look like.",
            "So that could be a future work I think.",
            "Yeah, thanks.",
            "Can you please say how the quality of the parses influences the compression so I know that it might happen that some phrases are miss attached and then probably some actually necessary parts of the sentence are removed during the compression.",
            "Does it happen?",
            "It's a very good question, actually.",
            "We do observe that if the parser produces like wrong person tree, our system could be infected.",
            "That's why we also proposed the multi scorer where we can take like other metrics into consideration.",
            "We won't be affected directly by the wrong parsing tree.",
            "We can consider other like factors into consideration like the query relevance.",
            "So it's more like robust by using more metrics.",
            "OK, thank you.",
            "Very nice talk.",
            "Hi, thanks for the talk.",
            "As far as I understood the main port in the main part of your PowerPoint that your query information actually affects the result is the first part, while in the compression you might lose some information that are correlated.",
            "And your compression method might not get them as relevant and remove them.",
            "Do you have any idea of hardegree them really works in order to avoid those mistakes by relevance.",
            "What do you mean by relevant?",
            "You have a query and then your summary should be related to your query, right?",
            "So if you in your query you ask when this happened and then in your ants in your compression, which is the second part of your pipeline Tuesday.",
            "Is removed, which is very likely.",
            "Do you have anything to avoid those kind of mistakes?",
            "Actually, it could be fixed by using the query relevance metric in our model score this quarter, right?",
            "Yeah, exactly for example, if we can definitely use like a more sophisticated score at that part.",
            "For example, you asked and what is the time of it?",
            "And if there is like Thursday in it, even though it may not be the correct answer for that, it could get some credit.",
            "So we can definitely like change this quarter and make it.",
            "Better, but that's at the same point.",
            "Now you're only affecting the query at the first stage, right?",
            "No II, also score.",
            "Yeah, that's why we proposed a modest car.",
            "We take a lot of factors into consideration.",
            "Yeah, thank you.",
            "Mark"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everybody, I'm going to present this joint work with IBM Research.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the query focused multi document summarization is defined in a way that given a complex query in a set of relevant documents, we would like to generate a fluent and concept summary that can answer that query.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Document Understanding Conference has fostered this task since 2004.",
                    "label": 0
                },
                {
                    "sent": "However, up to now the most top performing systems are still extractive or bunch of machine learning based approaches has been proposed to generate extractive summaries.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Unfortunately, extractive summaries are not compact enough.",
                    "label": 1
                },
                {
                    "sent": "For example, in the news where data, the sentences can be as long as 60 or 70 words, and those lenses and entities that are partially relevant to the query can be other excluded from the summary or prevent the selection of other important sentences.",
                    "label": 0
                },
                {
                    "sent": "So, in addition, if you look at a human construct, abstracts people tend to abstract the content and seldom used.",
                    "label": 0
                },
                {
                    "sent": "Entire sentence from the original documents and you can observe paraphrase, obstruction, reordering and sentence compression in human rating.",
                    "label": 0
                },
                {
                    "sent": "Summaries.",
                    "label": 0
                },
                {
                    "sent": "In this work I will focus on the sentence compression.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's an example.",
                    "label": 0
                },
                {
                    "sent": "Given the Cori check, the thread of the West Nile virus through the United States and efforts taken to control it, we retrieve one relevant sentence from the documents.",
                    "label": 1
                },
                {
                    "sent": "And if you look at this sentence, we can definitely safely remove the last part of it and return the remaining information as part of the answer.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's another example.",
                    "label": 0
                },
                {
                    "sent": "Given the query, in what ways have stolen artworks being recovered?",
                    "label": 1
                },
                {
                    "sent": "How often are suspects arrested for all prosecuted for the steps from one human written abstract will find this person use a sentence from the original document, but remove those unnecessary details from it.",
                    "label": 0
                },
                {
                    "sent": "And this sentence compression operation has been observed a lot in those human written abstracts.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in general, to summarize the contributions we have in this work, we first present this learning based sentence compression framework for Corey focused multi document summarization.",
                    "label": 0
                },
                {
                    "sent": "Then we implement a beam search decoder to efficiently find highly probable compressions.",
                    "label": 1
                },
                {
                    "sent": "We will show how to increase integrate various indicative metrics such as linguistic motivation and core relevance into this compression process by evaluation on standard corpora.",
                    "label": 1
                },
                {
                    "sent": "We show that our approach can provide significantly improvement over pure extraction based methods in both automatic and human evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our work is related to sentence compression.",
                    "label": 0
                },
                {
                    "sent": "For example, Netan marquee is noisy channel model to generate compressions and then Gary and Michael and extend it by using this synchronous context.",
                    "label": 1
                },
                {
                    "sent": "Free grammars also discriminative learning has also been investigated for deciding if a term should be removed based on syntax information.",
                    "label": 1
                },
                {
                    "sent": "Clark and Lopata further integrate discourse structure to guide this compression process.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In addition, our work is also inspired by those sentence compression to multi document summarization work.",
                    "label": 0
                },
                {
                    "sent": "Previously people have been using heuristic rules to generate multiple alternative compressions and rank them along with the original sentences only.",
                    "label": 1
                },
                {
                    "sent": "Currently Martins and Smith present the learning model based on the dependence Tree an which can determine if a node should be removed.",
                    "label": 1
                },
                {
                    "sent": "Based on this, Berg Kirkpatrick at all use a discriminative learning.",
                    "label": 0
                },
                {
                    "sent": "To drawing model, distraction and the compression at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Offering work consists of mainly three steps.",
                    "label": 0
                },
                {
                    "sent": "Firstly we use sentence ranking to determine the importance of each sentence given the query.",
                    "label": 1
                },
                {
                    "sent": "Then we use the sentence compression model to internally generates most likely compression and then we applies coreference resolution Anna sentence ordering to produce summaries of better quality.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'm going to briefly discuss sentence ranking model.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We experiment with support vector regression and Lambda Mart love.",
                    "label": 1
                },
                {
                    "sent": "The Mart is the state of the Art for information retrieval and by the hasn't been used in summarization.",
                    "label": 0
                },
                {
                    "sent": "We trained on the 2005 corpus along with their manually constructed abstracts.",
                    "label": 1
                },
                {
                    "sent": "We use root to score as the objective.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's some sample features.",
                    "label": 0
                },
                {
                    "sent": "We use a query relevant features such as unigram or background TF added personality.",
                    "label": 0
                },
                {
                    "sent": "We also consider query independent features such as the relative position or length of the sentence.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After distances ranking we got a bunch of important sentences.",
                    "label": 0
                },
                {
                    "sent": "Then we will apply the sentence compression tech.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for a long time, people haven't successfully applied sentence compression technique for summarization.",
                    "label": 0
                },
                {
                    "sent": "Becausw without a carefully design technique is very easy to generate.",
                    "label": 0
                },
                {
                    "sent": "Angry medical summary.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We will propose a parse tree based compression technique, where each sentence is represented by its parse tree.",
                    "label": 1
                },
                {
                    "sent": "And this work is in line.",
                    "label": 0
                },
                {
                    "sent": "This approach is in language.",
                    "label": 0
                },
                {
                    "sent": "The syntax driven approaches where we do not want to produce a new parse tree, but instead we will identify a set of nodes to remove to be removed.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is an example.",
                    "label": 0
                },
                {
                    "sent": "We have sentence.",
                    "label": 0
                },
                {
                    "sent": "Malaria causes millions of deaths according to WHL.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We would like to remove the last part because it might not be relevant to the query.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We first represented us this parse tree.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Identify the set of nodes will be removed in the process.",
                    "label": 0
                },
                {
                    "sent": "If a node has been removed in the process, all those words subsumed by this node will be dropped from the sentence.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Formally, our algorithm takes us input or parse tree T. And the T is represented as list of ordered consonants according to a given tree traversal algorithm.",
                    "label": 1
                },
                {
                    "sent": "For example, like post order coversyl.",
                    "label": 0
                },
                {
                    "sent": "And our system will output a set of labels corresponding to each node in T. And the label is one of return.",
                    "label": 0
                },
                {
                    "sent": "Remove partial remove.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When is the return?",
                    "label": 0
                },
                {
                    "sent": "It means the current node and all of these children will be preserved in the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, like the NP node in this example.",
                    "label": 0
                },
                {
                    "sent": "Similarly for.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the current node and all of his shooters will be dropped during the compression process.",
                    "label": 0
                },
                {
                    "sent": "All.",
                    "label": 0
                },
                {
                    "sent": "Like the grayed out PP node.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For partial.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We move on.",
                    "label": 0
                },
                {
                    "sent": "It means some of the children has been removed and the other children have been preserved.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this example, like the VP node in our parts tree.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "RBM.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I'm going to present our beam search decoder to find the highly probable compressions for the given sentence.",
                    "label": 0
                },
                {
                    "sent": "Our beam search decoder will consider all of the ordered con students in sequence.",
                    "label": 1
                },
                {
                    "sent": "In each iteration, all of the sentence compression hypothesis will be expanded by a single new node.",
                    "label": 0
                },
                {
                    "sent": "Before that, we need to score to rank all those themes to rank all those hypothesis in our bin.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first scorer we are proposing here is called as BASIC, which takes a subsequence of the order nodes, nodes, label and output some log probability of the label given that node.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example, assuming we're at the red node prepositional phrase PD.",
                    "label": 0
                },
                {
                    "sent": "We're using a post order traversal.",
                    "label": 0
                },
                {
                    "sent": "All it is also shown here is the top three scored beam.",
                    "label": 0
                },
                {
                    "sent": "The next level with it is is parent.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And depending on the label of this current node, we will update the score for each beam there.",
                    "label": 0
                },
                {
                    "sent": "Similarly, we always did with Peanut.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyway, again, updated score for each team.",
                    "label": 0
                },
                {
                    "sent": "At the end of the process, we will use a language model, train a gig world data to rerank all those beams, and output the first one as the final will be included in the final summary.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for learning the probability of the label, we use a maximum entropy classifier.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anitta trained on the data from cloud and laptop to laptop 2008.",
                    "label": 0
                },
                {
                    "sent": "And it contains 82 news where article with one manually produced compression along with each other in a sentence.",
                    "label": 1
                },
                {
                    "sent": "Simple features include context dependence, relation, Anna hand node information.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In addition to this statistical model, we also can see their linguistically motivated compression rules, because Turner and China have shown that applying those rules can improve both the content and language quality.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's look at this example again.",
                    "label": 0
                },
                {
                    "sent": "This human written this human constructed abstract sentence in this human construct sentence.",
                    "label": 0
                },
                {
                    "sent": "Our human annotator removes unnecessary details such as the modifier or complaints or time information.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For phrases like relative date.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Central Attribution, we can just safely remove it.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We synthesize a list of rules from existing work an encode them as the features for each node.",
                    "label": 0
                },
                {
                    "sent": "Whenever the feature function is fired, we will for for that node we will set the probability of the label, remove to one.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our motto is flexible in the sense that we can change the tree traversal algorithm or we can tailor the scoring function to our summarization task.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have talked about a post order travel so already.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'm going to introduce to other search algorithms.",
                    "label": 0
                },
                {
                    "sent": "One is called content aware search.",
                    "label": 0
                },
                {
                    "sent": "This is based on the intuition that the prediction on the context node can help with the presentation on the current node.",
                    "label": 0
                },
                {
                    "sent": "So where we use the context labels as the new features for the prediction on the current node?",
                    "label": 0
                },
                {
                    "sent": "Another",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Search algorithm is called have driven search.",
                    "label": 0
                },
                {
                    "sent": "This is we will with it handled first, then other nodes later.",
                    "label": 0
                },
                {
                    "sent": "This is intuitive because if a head node has been removed, we don't need to preserve its modifier anymore.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other flexibility is are lying in the scorer we have up to now we only have a scorer to measure the compression probability of each node.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we can extend it by adding like Corey relevance score.",
                    "label": 0
                },
                {
                    "sent": "We also measure the importance of the current hypothesis.",
                    "label": 0
                },
                {
                    "sent": "It takes the form of the sum of some basic score.",
                    "label": 0
                },
                {
                    "sent": "This some basis core is adopted from the work from Twitter Nova in 2007.",
                    "label": 1
                },
                {
                    "sent": "Again, we use this train language model to compute the probability of the current hypothesis.",
                    "label": 1
                },
                {
                    "sent": "We also consider crowds sentence redundancy.",
                    "label": 0
                },
                {
                    "sent": "Now we have a modest core, takes the form of the linear interpolation of different indicative metrics here, and those parameters will be tuned on the test set on the development set.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the end we simply replace each pronoun with is a reference an.",
                    "label": 0
                },
                {
                    "sent": "We ordered a sentence based on the time step and then the position in a source document.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We valued our approach on 2 standard corpora.",
                    "label": 0
                },
                {
                    "sent": "One is stuck 2006 and the other ones doctors on 7.",
                    "label": 1
                },
                {
                    "sent": "We for automatic evaluation we use route which is standard metric for summarization, evaluation for human evaluation we first use pyramid, which measures the content coverage or which is beyond lesco similarity, and for linguistic quality we also ask people to measure the fluency or grammaticality of our generated summary.",
                    "label": 1
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Firstly, we compare with the best duck system in this to Corporal.",
                    "label": 0
                },
                {
                    "sent": "And then we compare that with at all or which work reportes the best results up to now on this to corporate and they use a latent semantic analysis based approach.",
                    "label": 0
                },
                {
                    "sent": "An IT is extractive or based method.",
                    "label": 0
                },
                {
                    "sent": "And then we also show to the results from our tour anchors here.",
                    "label": 0
                },
                {
                    "sent": "Before testing our compression based tree based compression based system, we want to just apply those heroes rules and see how does it work.",
                    "label": 0
                },
                {
                    "sent": "And from the results we can see that the RT score already got improved.",
                    "label": 0
                },
                {
                    "sent": "And then we test our post order based compressor along with the basic scorer and we can see that we got a better compression rate.",
                    "label": 0
                },
                {
                    "sent": "Andrew Jazz you Foursquare also got like improved.",
                    "label": 0
                },
                {
                    "sent": "Again, we apply our context aware search with the basic score and also have driven search with the basic score an for both of the experiment.",
                    "label": 0
                },
                {
                    "sent": "We observed that they can remove further, remove the redundancy in the summary and improve the rule as you first score.",
                    "label": 0
                },
                {
                    "sent": "So to combine all those like factors into one system, the final system used to have driven search with the multiple scorer Anet.",
                    "label": 0
                },
                {
                    "sent": "Get best performance on both Route 2 score and Root as a silver score, and on both Scarborough.",
                    "label": 0
                },
                {
                    "sent": "It's on the last line here.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For human evaluation we first here for different human judges, to evaluate our different linguistic quality metrics, like whether it is grammatical or horrid, and it is, and we compare our system with two systems.",
                    "label": 1
                },
                {
                    "sent": "One is the best system with the highest root score, the other one is the best system with the highest overall linguist quality.",
                    "label": 0
                },
                {
                    "sent": "From these five different metrics, we can see that our system can produce comparable performance with those type of performing system in the dark competition.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After dis linguistic quality evaluation will also ask people to evaluate by using the pyramid score, and we can see that our system produced statistically significant better result than the best stock system with high salute score.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In addition, we also want evaluated sentence compression component in our system, independent of the summarization.",
                    "label": 0
                },
                {
                    "sent": "So we use the data set from Clark Analoga 2008 and compared with three systems here, one is called the hedge.",
                    "label": 0
                },
                {
                    "sent": "Trimmer is a rule based compression system.",
                    "label": 0
                },
                {
                    "sent": "The second one is McDonald's 2006 is based on a discriminative learning with software syntactic evidence.",
                    "label": 1
                },
                {
                    "sent": "The third one is the Martins and Smith.",
                    "label": 0
                },
                {
                    "sent": "They implement our dependency tree based compressor.",
                    "label": 0
                },
                {
                    "sent": "We use the unigram precision, recall and F matter according to Martins and Smith work.",
                    "label": 0
                },
                {
                    "sent": "And we also measured it bends relations overlap between the original sentence and compression.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the results for the three compared systems.",
                    "label": 0
                },
                {
                    "sent": "Again, we first just apply the rules on it and see how does it work.",
                    "label": 0
                },
                {
                    "sent": "From the result we can see on the compression rate is a little bit higher than the compare systems compared systems, which means it is not that effective to remove those ordinances.",
                    "label": 0
                },
                {
                    "sent": "And then we test it on our tree based compression system.",
                    "label": 0
                },
                {
                    "sent": "And the results show that on the last line we can see our head driven based compressor can produce significantly better unigram precision and the relation effort matter when it.",
                    "label": 0
                },
                {
                    "sent": "Has a lower complication rate, which means we remove more redundancy.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's a simple system output.",
                    "label": 0
                },
                {
                    "sent": "Given the query, however, the bombings of the US embassies in Kenya and Tanzania conducted what terrorism terrorist groups and individuals were responsible.",
                    "label": 1
                },
                {
                    "sent": "How and where would attacks planned?",
                    "label": 0
                },
                {
                    "sent": "Our system output?",
                    "label": 0
                },
                {
                    "sent": "A summary which is drawn here.",
                    "label": 0
                },
                {
                    "sent": "We can see that our system can successfully remove some unnecessary details and also produce this fluent summary.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To conclude.",
                    "label": 0
                },
                {
                    "sent": "We have presented a framework for query focused multi document, multi document summarization based on the sentence compression technique and we also show that our system produce significantly better results than the pure extraction based methods and the state of the art in both automatic and human evaluation.",
                    "label": 1
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in your current system, you're doing the first ranking.",
                    "label": 0
                },
                {
                    "sent": "Then you are doing the compression.",
                    "label": 0
                },
                {
                    "sent": "Then you are reordering right?",
                    "label": 0
                },
                {
                    "sent": "So any idea of how these three tasks can be done jointly at the same time?",
                    "label": 0
                },
                {
                    "sent": "So when you extract you do the compression.",
                    "label": 0
                },
                {
                    "sent": "You do the importance measure and you do the end of the ordering.",
                    "label": 0
                },
                {
                    "sent": "We have been thinking about that, but.",
                    "label": 0
                },
                {
                    "sent": "So, so this work.",
                    "label": 0
                },
                {
                    "sent": "Actually we focus more on the compression part and see how to produce the grammatical compressions.",
                    "label": 0
                },
                {
                    "sent": "And for existing work there is a more, but they're doing the extraction and summarization compression at the same time, but they never report how much information has been removed and how does the result look like.",
                    "label": 0
                },
                {
                    "sent": "So that could be a future work I think.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thanks.",
                    "label": 0
                },
                {
                    "sent": "Can you please say how the quality of the parses influences the compression so I know that it might happen that some phrases are miss attached and then probably some actually necessary parts of the sentence are removed during the compression.",
                    "label": 0
                },
                {
                    "sent": "Does it happen?",
                    "label": 0
                },
                {
                    "sent": "It's a very good question, actually.",
                    "label": 0
                },
                {
                    "sent": "We do observe that if the parser produces like wrong person tree, our system could be infected.",
                    "label": 0
                },
                {
                    "sent": "That's why we also proposed the multi scorer where we can take like other metrics into consideration.",
                    "label": 0
                },
                {
                    "sent": "We won't be affected directly by the wrong parsing tree.",
                    "label": 0
                },
                {
                    "sent": "We can consider other like factors into consideration like the query relevance.",
                    "label": 0
                },
                {
                    "sent": "So it's more like robust by using more metrics.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Very nice talk.",
                    "label": 0
                },
                {
                    "sent": "Hi, thanks for the talk.",
                    "label": 0
                },
                {
                    "sent": "As far as I understood the main port in the main part of your PowerPoint that your query information actually affects the result is the first part, while in the compression you might lose some information that are correlated.",
                    "label": 0
                },
                {
                    "sent": "And your compression method might not get them as relevant and remove them.",
                    "label": 0
                },
                {
                    "sent": "Do you have any idea of hardegree them really works in order to avoid those mistakes by relevance.",
                    "label": 0
                },
                {
                    "sent": "What do you mean by relevant?",
                    "label": 0
                },
                {
                    "sent": "You have a query and then your summary should be related to your query, right?",
                    "label": 0
                },
                {
                    "sent": "So if you in your query you ask when this happened and then in your ants in your compression, which is the second part of your pipeline Tuesday.",
                    "label": 0
                },
                {
                    "sent": "Is removed, which is very likely.",
                    "label": 0
                },
                {
                    "sent": "Do you have anything to avoid those kind of mistakes?",
                    "label": 0
                },
                {
                    "sent": "Actually, it could be fixed by using the query relevance metric in our model score this quarter, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, exactly for example, if we can definitely use like a more sophisticated score at that part.",
                    "label": 0
                },
                {
                    "sent": "For example, you asked and what is the time of it?",
                    "label": 0
                },
                {
                    "sent": "And if there is like Thursday in it, even though it may not be the correct answer for that, it could get some credit.",
                    "label": 0
                },
                {
                    "sent": "So we can definitely like change this quarter and make it.",
                    "label": 0
                },
                {
                    "sent": "Better, but that's at the same point.",
                    "label": 0
                },
                {
                    "sent": "Now you're only affecting the query at the first stage, right?",
                    "label": 0
                },
                {
                    "sent": "No II, also score.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's why we proposed a modest car.",
                    "label": 0
                },
                {
                    "sent": "We take a lot of factors into consideration.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thank you.",
                    "label": 0
                },
                {
                    "sent": "Mark",
                    "label": 0
                }
            ]
        }
    }
}