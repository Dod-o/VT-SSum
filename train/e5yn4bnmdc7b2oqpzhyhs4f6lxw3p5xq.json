{
    "id": "e5yn4bnmdc7b2oqpzhyhs4f6lxw3p5xq",
    "title": "Learning Nonparametric Priors from Multiple Tasks",
    "info": {
        "author": [
            "Shipeng Yu, LinkedIn Corporation"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/oh06_yu_lnpmt/",
    "segmentation": [
        [
            "First speaker is shipping tool with replacing photographs.",
            "These are not the employees.",
            "Thanks, good morning everyone.",
            "In shipping and this is joint work with Kaiyuan Focal test.",
            "Unfortunately, focus is very busy with some projects in Siemens, so I come here to present this talk for him.",
            "Title is learning nonparametric priors for from multiple tasks.",
            "Anne, OK.",
            "So multitask learning.",
            "I think we have a set of."
        ],
        [
            "Related tasks to learn and the basic assumption here is that we have some dependencies between these tasks, and in this talk we assume that all the tasks share the same input space.",
            "So basically they're working on the same set of objects.",
            "So the idea is to learn them jointly instead of independently.",
            "For some examples here, we consider multi label text categorization, in which each document is belongs to several different categories.",
            "So here each task is classification problem.",
            "In the second example we have multi task ranking problem.",
            "So each task here is the ranking.",
            "Problem so so here we have some connections to the collaborative filtering, so let's talk about more focusing on the second example here.",
            "To give a scenario of what we're working on, so we have set of American movies and each movie has some features."
        ],
        [
            "Like the rest of the movie The directors, actors and some descriptions of the movie.",
            "So we we presented system to 1 user.",
            "And we ask this guy to rate for several of these movies from the very dislike to very like so we have maybe 5 levels of different preference labels.",
            "So this guy makes some ranking here.",
            "So the task here is to predict.",
            "For the rest of the movie, What's the most likely label this this user might might take?",
            "So here is basically a drinking problem and."
        ],
        [
            "People call this is ordinal regression problem because this is like a regression problem, but.",
            "The regression has labels.",
            "Is discrete labels here?",
            "It's not like continuous real numbers.",
            "So this is just the ranking problem.",
            "But in reality we have lots of different users.",
            "We have things that movies and issues are many rank.",
            "Part of this movies and the rankings are different.",
            "So here the task is to how to predict.",
            "Observe the labels for all these users.",
            "So this is the multi task ranking problem and we call in this talk.",
            "Is this collaborative ordinal regression?",
            "Because we have the multiple users?",
            "This is more general than the collaborative filtering because we have some features here.",
            "I think in the pure collaborative filtering framework we just use this part of the.",
            "Data to learn.",
            "But here we have a bunch of features.",
            "So how to incorporate features into this framework?",
            "This is the problem.",
            "So here this."
        ],
        [
            "Assumptions is again that we assume that all the ranking problems are correlated.",
            "They are not independent to each other.",
            "The challenge here is how to model the dependencies between the users and how to make use of the features of each item elegantly.",
            "And we could have some missing ratings in this data and we want to generalization to new items as well as new ranking functions.",
            "So in this sense this is more general than the pure collaborative filtering cause in the pure Collaboration framework we don't have features and we cannot generalize to new items.",
            "OK, we want this generalization to new items and new features, new functions.",
            "OK."
        ],
        [
            "So this is outline.",
            "Rest of my talk.",
            "I will first start with some ranking problem.",
            "So how to formally define a ranking problem?",
            "And I represent appears in framework for ordinal regression.",
            "This is special ranking problem.",
            "Now will say how to learn multiple ranking in the same way so it's a collaborative on regression.",
            "Now we present some experiments results in some conclusion extensions.",
            "So in ranking problem we would like."
        ],
        [
            "To assign ranks to objects.",
            "Assume we have N objects X one to XN in the D dimensional space.",
            "So for ranking we have two different ways of doing this.",
            "So first way we call ordinal regression.",
            "So basically we assume there's R labels.",
            "We assume we assign each label to one.",
            "These labels to each of these data points.",
            "So we assume one is the lowest rank and R is the highest rank.",
            "So it looks like a regression problem.",
            "So basically you can assign that OK for each of item we assign one number here to each items based on this speech matrix.",
            "So each item can only take one of these values.",
            "So we call this is ordinal regression.",
            "This is about ranking.",
            "So how to rank items the 2nd way of doing ranking is to directly output our preferences.",
            "At least so we say extend is preferred than X1X1 Professor X2 and so on we call this is preference learning.",
            "So this problem is different from the.",
            "The traditional classification regression problem cause in binary classification we have only two labels.",
            "So you can say this special case of the ranking problems and for the multiclass classification we don't have any ordering between all the labels.",
            "And in the regression case we have only real number outputs.",
            "So here is the discrete output.",
            "In this talk I will mostly focusing on the 1st type of ranking problems, ordinary regression.",
            "So."
        ],
        [
            "So in order regression we would like to assign ordered labels for objects, so the most.",
            "Popular way of doing ordinary regression is that we assume there is a latent function F which Maps all the points into a real line.",
            "So every point after mapping value here.",
            "And we we can assume there is a boundary here.",
            "So basically the boundary defines what's the most likely ranking label should be.",
            "So we have R + 1 ranking boundaries.",
            "Then we can say OK if the points lie in this area, we can assign the value to one to this point and so on.",
            "So basically we have our regions for ranking.",
            "So based on this boundary we can assign outputs to objects.",
            "So this is sort of the popular way of doing ordinary regression, and we have lots of applications here, like how to predict the user preferences and how to rank web pages for search engines.",
            "If we have more than."
        ],
        [
            "One task.",
            "With a set of functions.",
            "So basically the assumptions similar, so we assume each ranking problem has a little function F and we in the same time we map all the points into all the functions here.",
            "So here the assumption is these functions are correlated and each function may only rank the part of the data.",
            "So we have a lot of missing data here.",
            "So this is a multi task setting and this is very common in the real world problems.",
            "Things like copy of filtering.",
            "So we want to predict the preferences.",
            "And also for Rep ranking.",
            "In fact pranking you can think each each of these tasks is a query and this is a document set web pages.",
            "So we want to rank for each query.",
            "What's likely ranking list of all documents so you can see it's a very natural multi task learning problem and you can say all the queries have some relation because the words are sort of have some common meaning.",
            "So this tasks are sort of related.",
            "OK, the question is how to learn this related ranking tasks jointly."
        ],
        [
            "OK, we first present appearance in framework for this kind of problem.",
            "Then we can generalize this to multitask learning.",
            "Um?",
            "For if there's only one task, we have a base and treatment of this ranking problems, so we have.",
            "This is a conditional model on ranking labels.",
            "And we."
        ],
        [
            "1st.",
            "Define a ranking likelihood so based on latent function F, input data X and some parameter Theta, the output of the all the label, all the.",
            "Items why this vector basically is just this likelihood and we can say we're running this concrete form.",
            "So here is a vector of the latent values of all items to some parameters.",
            "So this is basically the ranking likelihood we will come to this later.",
            "And given this likelihood, we can define prior in terms of PSN POV.",
            "So for this latent function F we assign a nonparametric priors costing process.",
            "Prior to this function.",
            "So here.",
            "So basically this means on any finite set of items.",
            "This finite vector is a cost and distributed with mean function H and kernel covariance function K. So here H is the main function.",
            "Basically measures what's likely most likely mean of all this function values and K is the covariance matrix.",
            "Matrix measures similarity between every two items in terms of the function space.",
            "Then, given this prior and likelihood we can marginalise the latent function F in this way.",
            "So this is much less ranking likelihood integrate out.",
            "So this is a general base and treatment of this problem and for the ordinal regression likelihood, we assume that given the function value F, they are independent with each other, so it's factorized.",
            "Every items.",
            "So if you look at the graphical model."
        ],
        [
            "This is a graphical model of the problem.",
            "We have items.",
            "And so this is operation, why I gave an effect size and function F. We don't directly give a parametric form for F, but we have signed a nonparametric priors.",
            "So H is the main function case kernel matrix.",
            "So this defines a nonparametric prior for the little function and we have the observations.",
            "So here.",
            "From the Gaussian process prior to foreface nonparametric priors and from F. To the observation, this is basically ordinary regression likelihood.",
            "And.",
            "So basically we're trying to model the 1st and 2nd statistics in this nonparametric prior.",
            "And as you can see, if you assign different likelihood here, you can also model normal regression problem or classification problem and so on.",
            "So it's very general framework for any kind of.",
            "Supervised learning tasks.",
            "So here we."
        ],
        [
            "We will give two example ranking likelihood for ordinary regression here.",
            "The first one is basically just a regression, so we assume that the Y is directly is sort of a real number, but we ignore.",
            "This is discrete.",
            "Somehow.",
            "We assume that the likelihood that why I take some value is basically the.",
            "The output of this Gaussian likelihood, specially if you have a mean FX I have, say, pass it.",
            "You have the variance Sigma square and you observe this is likely function.",
            "You just read the value of each.",
            "Of this five rank labels.",
            "We're just a regression on the output of the ranking.",
            "Very easy one.",
            "The second one is a little bit difficult complex, so it's proposed."
        ],
        [
            "Yeah, true anchor money last year they call this Gaussian process ordinal regression.",
            "They assign this kind of likelihood to the ranking labels.",
            "So basically we assume we have a bunch of ranking labels boundaries is P12B four they define five regions and basically have a mean FX I have for variance Sigma so you have likely function here.",
            "Then you simply calculate the area under this curve.",
            "We've seen this boundaries, so basically for the number 4 you can calculate the area under this curve in these two boundaries here.",
            "So this is this is the likelihood that while I takes #4.",
            "At least it's automatically sum over sums to one and this super nice ranking likelihood, but you have more parameters like the Sigma and some boundary parameters be here.",
            "This is specific for ranking problem.",
            "OK, OK so that's just for one task."
        ],
        [
            "Drinking problem.",
            "So how do we do multitask learning with ranking?",
            "So if you stick to this framework."
        ],
        [
            "First approach you can think about is to learn a Gaussian process model for each task.",
            "Then you will lose sort of connection between all the tasks.",
            "So there's no share of information between tasks, so this is not good.",
            "The second approach, maybe you can feed only one parametric kernel, because normally people do look awesome process learning people assume the kernel matrix is parametric kernel function and you just learn try to learn this kernel parameters from this kernel function.",
            "So in the second approach you can try to feed one parametric kernel functions for all these tasks.",
            "But then the problem is the parametric kernel maybe very restrictive, so you cannot show the freedom is.",
            "Constraint to this specific domain, so no matter which kind of comment you choose, you cannot really get the.",
            "Who knows?",
            "Yep.",
            "I mean.",
            "I think in the Spring 1 task learning problem people normally assume."
        ],
        [
            "Assume this kernel K comes from the covariance function.",
            "You can think about RBF kernel function, so there's a parameter.",
            "Here is basically the width of the kernel.",
            "Then you try to adapt this concept from the data.",
            "This is the normal way of doing Gaussian process learning, so it's not directly learn a kernel matrix.",
            "But learn some parameters here.",
            "Yeah, so nonparametrics to learn everything.",
            "Learn both the main function and kernel will show that.",
            "OK, so."
        ],
        [
            "So here the key points of multitask learning for ranking is to model the collaborative effects.",
            "So here we can say in terms of the ranking.",
            "Say in some recommendation system for moving and rulers, we have two kind of collaborative effects.",
            "The first line is common preferences.",
            "So basically functions with share similar regression values in terms of the collaborative filtering, you can say all the users tend to share the same ranking labels.",
            "For some items.",
            "Some movies are very good, so everybody likes it.",
            "Dislike it.",
            "The second one is to model a similar similar variability, so this is basically a second order statistics.",
            "So basically, if two items are very similar, then functions tend to have similar predictions for these two items, so this is basically a second order statistics, so we will try to model these two collaborative effects in this model."
        ],
        [
            "So.",
            "So in this collaborative ordinal regression model, is a hierarchical Gaussian process model for my task ranking learning.",
            "So here we assume for the same set of items, excellent X we have functions while wanted line.",
            "So this is observed labels.",
            "So how do?",
            "How do you learn?",
            "Is this multi task learning problem?",
            "So we assume that the same for each of these tasks.",
            "We have a little function which generates this ranking likelihood.",
            "So this is basically the ranking order regression likelihood.",
            "And we assign this.",
            "We assume there's a same Gaussian process prior for all these functions.",
            "So here this is nonparametric prior and the other thing for all the tasks sort of standard hierarchical Bayesian learning.",
            "So basically the main function models the common preferences and covariance kernel, recurrence matrix, model similarity, similar variabilities.",
            "Points is that we can learn both the mean function and the covariance matrix directly from the data.",
            "So this covers metrics, not stationary.",
            "It can be nice stationary.",
            "And I think things you have more than one tasks to learn here.",
            "So it is possible to learn both malfunction and covariance matrix.",
            "Here we will show it later."
        ],
        [
            "OK, so here's the model.",
            "This is a hierarchical based model for ranking functions.",
            "So basically assume all the latent functions are samples from the same causing process prior.",
            "Then given this functions we assign we allow different parametric setting for different tasks.",
            "So basically here each task given this prior tasks are independent of each other and each should be integrated out over the latent function value.",
            "So we also assign some hyper prior for this patient came here and we observe only part of the rank labels.",
            "So far."
        ],
        [
            "Graphical model here.",
            "So this is again the one task ranking problem and this is a hierarchical Gaussian process model.",
            "So basically we assume we have functions to learn, so each one is L and given this function value we have we have an L may be different for each one for all the functions items.",
            "So we observe part of the data here.",
            "So we assign the same Gaussian process prior for all these functions cause an functions and too constrained.",
            "The prior we assign hyper prior try to sort of constrain the freedom of this hyper prior.",
            "So we call this graphical models hierarchical Gaussian process model.",
            "And here the the key points is the."
        ],
        [
            "In process prior connects all this ranking tasks.",
            "So we try to model the 1st and 2nd order statistics.",
            "And the low level features are sort of incorporate naturally 'cause this is a.",
            "It's a conditional model on the input, so the load features can be very easily incorporated.",
            "So in this sense is more general than the pure collaborative filtering model.",
            "So instead of fixing a parametric form for the kernel matrix, we basically assign a conjugate prior for both of the main function Ancona matrix.",
            "So this is the normal inverse Wishart.",
            "Prior we have some hyperparameters, H0, Pi and Powell and K0.",
            "So for this case 0 can use any base kernel because any sort of parametric kernels to internalize this one.",
            "But this is just for prior and you can see if you allow Pi and Tau going to Infinity, then it's basically means.",
            "Prior all the tasks are totally independent.",
            "So in this sense you mean you can try to assign different value for \u03c0 and Tau, and you kind of sign.",
            "You can obtain different.",
            "Collaborative effect.",
            "So here we can make predictions for both the new input data and also new tasks.",
            "In this framework I will show one point data for prediction for new tasks."
        ],
        [
            "So this is a toy problem.",
            "Well, we assume we have 315 items here.",
            "And this is prediction labels, so this thick black sick line is mean function.",
            "And.",
            "The black dashed line is like a staircase lines.",
            "Basically the mean ranking labels.",
            "So basically assign the labels to be the most nearby integer here.",
            "So this is basically the model.",
            "And we sample files functions from the Gaussian process based on this mean function and this kernel matrix.",
            "So this is the true kernel matrix for this 315 points.",
            "So in this way we basically want to show that the econometrics very so the function should be honest, moves in the beginning phase and ending phase.",
            "But but this, but they are very very very large variability in this space.",
            "So basically here is value is very very shallow.",
            "So we want to.",
            "Reflect the truth that in a pure true system of reference system for very good movies and very bad movies, all the functions are sort of agree, so there's no very large variance for very good one, very bad one.",
            "And for the for the movies in the middle.",
            "And people maybe have very large variance for different users, so we assume that so for this movies people have large variance here.",
            "So just just want to mimic the reality.",
            "So here if you.",
            "Is there a new function so you have observed only four points here?",
            "This is 4 sort of outputs observations for a new function.",
            "If you don't know anything about the kernel, if you assign RBF kernel here, then you do prediction is fine, but you say this variance are the same for all the points, so you cannot incorporate prior knowledge that the kernel is.",
            "This shape is not RBF kernel, so even if you learn best idea, Colonel is still get.",
            "This prediction is not very good, but if you try to learn the kernel and if you make the prediction here.",
            "I got better prediction function here and you know.",
            "In the in for this point, look at very large variance for this point.",
            "In, the variance is very low because of the kernel tells you this.",
            "So in this sense, look at you, try to make a prediction for a new function.",
            "If you can learn the nonstationary econometrics, you get better prediction for the new functions.",
            "Just try to give you a sense of generation 2 new tasks.",
            "OK."
        ],
        [
            "Tell me, let's talk about the learning.",
            "Can we use the style of learning in this framework so we have a variational lower bound this basic assigned by applying the distance quality and this is some over all the tasks?",
            "And do you have a Q function?",
            "Here is basically you can take any form for Q here.",
            "This is lower bound style.",
            "We first we try to estimate the Q function.",
            "Here in M step we try to learn the parameter.",
            "But here the problem is.",
            "The likelihood is not Gaussian, maybe not causing so.",
            "Here we use EP expectation propagation to learn a posterior Gaussian, so we're still assume Q keeps the Gaussian function.",
            "When we try to estimate this FNC head using, the EP2 approximation is framework and after you learn the Q here, you fix Q and you maximize respect to the parameter for each task and the joint Gaussian process prior.",
            "So basically we make custom app estimates for discussion process prior.",
            "So here this is the true posterior.",
            "Offer of 1 function.",
            "Basically this is the prior authorization process prior for this function.",
            "This is electrical term.",
            "This factorized for all the terms.",
            "So here in general the electrical term for each of these item is not Gaussian, especially in the regression case there should be some more complicated form.",
            "So in this framework we use TP switches proposed by mean car.",
            "So try to approximate."
        ],
        [
            "Each of these terms to be a Gaussian sequentially.",
            "So what we did is following three steps.",
            "So we first initialize all this term.",
            "Lies all approximation from TK for all the terms.",
            "So just random initialize.",
            "Then we try to repeat these three steps until convergence.",
            "The first one we delete, we delete one of these factors from this approximate Gaussian.",
            "The second one is we try to match the moments of adding the true likelihood term inside and we try to match the moments between the true posterior and the approximate posterior and in the third step we try to update the factor TK.",
            "So basically we do this sort of sequentially, approximating each item each electrical term to be a Gaussian, and afterwards we can.",
            "We can get to the joint Gaussian.",
            "So that was the question, I think, although it's not very important for this AP proximation.",
            "Or what is it?",
            "Here is fixes parameter.",
            "But before signing in on stuff, we optimize this data in the East that we fixed the setup.",
            "So so basically sitar.",
            "Sitar is the same as a Theta is the same as a Gaussian process priors.",
            "So these are all parameters of the model.",
            "So this is where we optimize this in one step."
        ],
        [
            "Let me know what kind of phone do you have?",
            "Yeah, I mean this is about general listening in the sort of the example model we showed in the beginning for the one task order regression, we have some parameters.",
            "Car show you."
        ],
        [
            "So in this case basically just the variance.",
            "Mr Darcy and Paul sometimes variance in the second model City is more complicated.",
            "So here."
        ],
        [
            "Put all the variance term Sigma and also the boundary of this four of this.",
            "For boundary parameters.",
            "So in this model of data is sort of five.",
            "Menus, so I mean you can define another kind of likelihood here, but maybe in general you have some parameters set up here.",
            "We assume that sort of task specific is not.",
            "Depend on other tasks.",
            "Estimating those moments hard.",
            "Maybe I mean stuff you need to do some computation.",
            "Yeah, I think.",
            "I think just yeah, just this kind of likelihood make you make the step very difficult.",
            "Yeah, yeah, so you have to do some approximation, so this is not Gaussian so we cannot just do normal.",
            "Please stop OK."
        ],
        [
            "So Fortunately we can do this approximation approximation analytically.",
            "For our example models.",
            "So for this kind of profit ranking likelihood you can do analytically the update for this decay.",
            "About but yeah, in general, if you if you choose any arbitrary form for this, then you should do some integration.",
            "11 dimensional integration here.",
            "So after we do this EP, we basically obtain a Gaussian approximation QFJ for each of these functions.",
            "OK, so this is for Easter."
        ],
        [
            "And for M step we try to fix the queue and we try to update all the model parameters.",
            "So we first update the nonparametric costing process prior.",
            "So this is the equation.",
            "This is update for H is update for K. As you can see this is basically a weighted average over all the tasks.",
            "The main function of all the tasks plus the prior term.",
            "And for the kernel is also.",
            "A similar is.",
            "Basically this is a prior term.",
            "This is the average over all the tasks, so that means we can learn directly your nonparametric kernel matrix K instead of learning some parametric parameter here.",
            "And this update does not depend on any form of ranking likelihood.",
            "So general and the contract prior just sort of the smallest term in this update.",
            "And we also update likelihood primacy to J for each of the tasks.",
            "And we can do this separately for each task.",
            "I think this sort of computational cost is the same as if you just do one single task learning, so we don't really have more computational cost in this model.",
            "I mean for for two and carmony's likelihood here, this step is very costly because you have to do concrete gradient update lots of parameters to learn.",
            "OK, this is."
        ],
        [
            "Learning and how about inference?",
            "OK, so if we want to.",
            "For Unk ranking label for new test data.",
            "So here given the training data.",
            "Given the learned parameters set to Jake HNC for new item X star, we want to infer the ranking label for the JS function.",
            "This is why J.",
            "So we do this also in the basin way, so we integrate out the latent function latent value FJ star.",
            "So this basically the JS function on this star value.",
            "So in the integration in the first term is very easy, just the likelihood ranking likelihood second term is.",
            "It's not very easy to learn.",
            "We have we show here.",
            "So after you learn everything you just, particularly by assigning the maximal value of YJ two to this value.",
            "So basically this is sort of how we do inference in this model.",
            "But if the star is known in training, so it's the transductive setting, so you know the test data before you.",
            "You make me very trained model, then the posterior of listening function value.",
            "So basically this posterior is obtained from the E step when you do the EP approximation directly.",
            "So then you can do this very elegantly.",
            "But if the test data is not known in the training phase so basically that means the exercise new item.",
            "You don't know this Atom before you train the model.",
            "OK, so then there are some problems, because the nonstationary kernel cause in the learning you want to learn.",
            "You want to learn econometrics ahead.",
            "This is basically done by N and this number of training points.",
            "You know nothing about the test points here.",
            "So this is the problem.",
            "So if you want to do inductive inference then use an S stationary kernel on the test data is not known.",
            "So when you calculate this term you get some problem.",
            "So how how do we do inductive inference?"
        ],
        [
            "We can do a mapping here so we can walk into our space instead of in a functional space, so this is what we do in the in the in the beginning.",
            "So this is the posterior for each function.",
            "This is we do in the EP step in step.",
            "Then we can.",
            "We can go from the functional space to the weight vector space.",
            "So basically assume that FJ is this form.",
            "So we can.",
            "We can turn the problem of learning FJ to learning Alpha J.",
            "So if after is this form after is access.",
            "So Africa is also a Gaussian distributed.",
            "So if we know the FJ here, so in the EP we can learn aperture instead of FJ, then in the test data we can use the base kernel so you know the base kernel of the test points user base kernel to make predictions for the little function.",
            "So this way you can do inductive inference here.",
            "So basically we want to do a mapping from the functional space F space to the Alpha space.",
            "But here we have some.",
            "You have some price to pay, so basically the predicted means are the same.",
            "Maybe in the function of space and enough space, but the predicted variance are the different.",
            "So you have you lost something?",
            "Because this is only so you can do in for inductive inference.",
            "In the Gaussian process model and we have theoretical results here so."
        ],
        [
            "OK, so given the given the constant process prior HNK sample from the hyper prior which is normal inverse patient prior.",
            "And then there exists unique UMR fancy arfe.",
            "So basically this models.",
            "Cost in prior for our phone here so H&K can be written in this form and in any finite vector of the F function there exists are here, so there's sort of 1 to one equivalence here.",
            "And then you often see also follow normal inverse Wishart distribution.",
            "So that means we can equivalently work in that well space of Arfa instead of function space F. So this way you can do inductive inference.",
            "But you will pay something because the prediction variance are lossed.",
            "OK, so this is basically the model."
        ],
        [
            "Let me do some experiments."
        ],
        [
            "For ranking.",
            "We want to predict the user ratings for some movie data and we use two different data for first line movie lines we have.",
            "With 500 movies and more than 100 users and we use some features for this task and for simplicity we just use the rank part.",
            "So basically what the type of the movie it is and we have 19 features for movie lens and for each movie we do something more.",
            "We try to go to the website of each movie and we try to get some text from the online database.",
            "So and we do Jeff Idea features so it's more than.",
            "20,000 features for each movie data, basically just the director, actor, and some description with movies.",
            "Experiment setting, we pick up the binder users with the most ratings to be tasks.",
            "So we fix the tasks and randomly choose some ratings for each of the users for training and we try to predict the rest of the ratings.",
            "And for best kernel we use normally used cosine similarity kernel for.",
            "For both of these tasks.",
            "And we have."
        ],
        [
            "So several comparison metrics, first part of the compared metrics is specially ordinal regression evaluation.",
            "This is mostly used for voting regression.",
            "People first lines mean absolute error basically count so so.",
            "So this is this is a particular label.",
            "This is the true label.",
            "Try to count the how large difference of these two is on average this is absolute error.",
            "I mean 01 error is very easy.",
            "You just.",
            "If the labels are different, you have an error one.",
            "If they are the same iris 0.",
            "And because this is a much task learning process, we have macro and micro average for over the tasks and we also evaluated ranking directly ranking evaluation.",
            "This is used by some ranking people is called normalized discounted cumulative gain.",
            "So basically, given one ranking our head, we try to calculate 1 score and this call is basically you.",
            "You thought you thought your rated items.",
            "Based on the partition label and try to go from the top to the end.",
            "So here the assumption is the top ranked items are most important for evaluation.",
            "You don't really care about the last ranked items, so this is very useful for search engine evaluation stuff and we have discounted value of the of the position in the rank list.",
            "We have some gain of.",
            "Off getting some correct prediction value and you attend.",
            "Basically, we don't.",
            "We just kind of top 10 ranked items so we ignore the rest of the items for evaluation."
        ],
        [
            "This is the result.",
            "So here then is number of training items which user we 1020 and 50 and we compare 5 models.",
            "The first model is causing progression.",
            "Basically just do regression on the ranking labels.",
            "Second wise collaborative version of this.",
            "So you have much task setting and the third one is Gaussian process.",
            "Ordinary regression using true Anchorman is likelihood and 3rd 1/4 one is a collaborative version of this last one is we select one very recent work for.",
            "Covering filtering model maximum maximum margin metrics.",
            "Factorization is last year in XML.",
            "We try to compare this of this ordinary regression model with this collaborative filtering model and we have six meaningful error, macro, macro and mean 0101.",
            "Monkey mattress factorization also allow the features no no.",
            "Unfortunately, because they cannot use features from this framework.",
            "Yeah, so yeah.",
            "So in this sense.",
            "Regression model have some benefits over the pure CF.",
            "You should do better than that, right?",
            "'cause you're using features.",
            "Yeah, we're doing features.",
            "Not using pizza.",
            "It's possible it's possible to sort info ordinary regression model if you just assign the best country identity matrix.",
            "So I mean you ignore the features, there's no features.",
            "Then it's possible to do this.",
            "We haven't happened to empirical study on this on this idea.",
            "Because we argue this is sort of the benefit of the model, I think this is very interesting to do, as in Korean, could you just treat the features as another users with these other techniques cheating, but just treat them the feature value as the rank of the user you use.",
            "Part of the users of the features for each feature you just create as a user.",
            "In some sense you know OK. You mean using the message to incorporate features features?",
            "Yeah, I think that's that's interesting to compare, I think, But yeah, But in this talk we didn't use them.",
            "I think we can.",
            "We should compare this I think.",
            "Yeah, I think full results.",
            "I think the first observation is a collaborative version, which is more always better than the individual version.",
            "I think it should be very small and here.",
            "For the first of all measures, the smaller the better.",
            "For the last two is, the higher the better.",
            "So for the first 2 manifold that mean absolute error.",
            "The collaborative version of Gaussian process regression is the best.",
            "And for the bin 01 error, I think the collaborative version of the regression likelihood is the best.",
            "And for last measure, I think for this task MF is the best because it directly maximize DCG like cost.",
            "So so in this MF is very good for ranking related evaluation.",
            "But for the second one."
        ],
        [
            "For each movie that I think even for the new situation score, I think the collaborative version of regression is the best and for the for the other scores thing is the same.",
            "So it's interesting because I think for for the regression likelihood they use a more specific likelihood for ranking, but only on this score it get better results.",
            "I think one assumption, one sort of explanation is has a lot of parameters, so every task I think in our model we have five different labels, so we have 4 boundary values and we have variance.",
            "So for this model we have 5 parameters to learn and for this one just one particular, so basically maybe because of the overfitting of the tasks for each of the tasks.",
            "And yeah, but this very simple regression model got very good results for for some measure.",
            "And we."
        ],
        [
            "So compare we also tried to make predictions for new functions.",
            "So have I used the movie lines as testbed?",
            "And here we so from the beginning from top to end we have.",
            "So first of all, it's based kernel.",
            "You space kernel to make predictions for test user.",
            "So we use just causing kernel for test user and then we use different number of training users to learn the kernel and user kernel to predict full test users.",
            "So as you can see if you use more so it's 5200 two 100.",
            "So if you use more users for training kernel.",
            "You got a better partition for the rest of users, so this is for actual error.",
            "This was 01 arrow is 2 digit score.",
            "So you mean you can see when you use a lot of users to learn the kernel, you get better prediction for all these measures.",
            "So that means you can learn a better kernel if you use more tasks.",
            "So this is sort of the conclusion we use more users we can learn better kernel.",
            "For ranking.",
            "So for the motivations, we observe that a collaborative version."
        ],
        [
            "Is always better than the individual version.",
            "We can learn very good nonstationary come from users and from our experience these two models are very fast training and our robust in testing.",
            "So there's no sort of approximation everything is causing.",
            "The second model is very slow in any steps and sometimes over feet.",
            "And we can also use some other ranking likelihood, but in this talk we just use the two example likelihood.",
            "But it is possible to use other likelihood.",
            "Or maybe you need to do some integration in each step."
        ],
        [
            "And we also did some experiments with multi label text categorization.",
            "We compare the much task costing process model with Ridge regression SVM.",
            "We use RC1 testbed for task categorization, and we training.",
            "We fixed the 50 categories and we have 10 repeats and choose 1000 documents randomly labeled.",
            "Some of them and protester.",
            "We use 10,000 documents, so we compare the UC score at macro, not micro and our method get better results here.",
            "And if you have a partially labeled items is also the same trend.",
            "And if you if you try to make predictions for the other 30 one categories and if you can."
        ],
        [
            "Pears are.",
            "Micro Micro and F measure you get better performance for frozen process.",
            "How can process model this model is better than the regression?",
            "Also better than the spam classifier?"
        ],
        [
            "OK, for some conclusion we"
        ],
        [
            "We present the base into framework for much task rec learning and this is efficient GMP learning algorithm.",
            "So we are able to learn the informative nonparametric prior.",
            "In a couple of version is always better than all individual version and expenditures currently results."
        ],
        [
            "So for some extensions we can also apply this model for model purposes learning.",
            "So in this talk we mainly focus on the original regression likelihood because it's very easy to extend this framework to model preference learning.",
            "So each of this task is the preferences pairs of items.",
            "So there's one work on single task, Gaussian process model for preference learning last year and it's very easy to extend this to elaborate version so we can also I think.",
            "Purchase the presenter rank net for ranking of documents and we can also propose a public version of this based on this sort of likelihood definition.",
            "So it's also possible to extend the clarity version.",
            "Much task learning for preference.",
            "Learning.",
            "The second one is to consider the Gaussian process mixture model for multi task learning.",
            "'cause if you think about users case an maybe there's some user groups of all the tasks so it's something like a task.",
            "Grouping so we could assign across the mixture model instead of 1 Gaussian for the hyperparameter.",
            "So the prediction then is the linear combination of of learn kernels.",
            "And if you allow the mixture number to be Infinity then you have some connection to additional process.",
            "Model is even higher level of nonparametric modeling.",
            "I think."
        ],
        [
            "Have a little some little work, but maybe I don't have time to go over it.",
            "And some little work on ranking."
        ],
        [
            "Learning ranking, so something like a margin based method and paragraphs translation and basic approach.",
            "Yeah, I think this is basically the talk.",
            "Thank you very much."
        ],
        [
            "This is.",
            "Numbers I don't quite understand your method explain to me what is work OK. You mean the results or the?",
            "PR work is CDPR.",
            "OK, I think I didn't mention this.",
            "Because we have two example models in for one task ranking right.",
            "So basically GPS is Johnson process regression this is.",
            "Yeah, we call this GPR so if you just directly just do regression on the labels, this is called the regression model.",
            "And if you use this new likelihood function, we call this GPRS ordinary regression model.",
            "And in the table we used C. So if you.",
            "So we use CPR to represent the collaborative version of Multitask learning.",
            "So so here's the CPI.",
            "Basically the cabbage.",
            "So much task learning with scrap defect.",
            "Do you learn independent tasks independently of all the tasks and this collaborative version of gotten personal regression model?",
            "You sort of.",
            "You have the collaborative effect in the modeling for this one.",
            "You don't really consider the collaborative effects.",
            "I don't consider the collaborative effect, so you learn every task, each task independently.",
            "Ignore the dependence.",
            "So therefore they all use some more features than the.",
            "That's right, that's right.",
            "Yeah, so somehow it's not fair for anonymous, but the longer it is.",
            "This is sort of the benefit of regression model so, so that's what I mean.",
            "The collaborative ordinal regression is more general than collaborative filtering because it can use more information very elegantly.",
            "Is the conditional model on features.",
            "OK, so I don't know your prior to time your do you have any low graphics structure on your panel?",
            "No, I think this work.",
            "We learn the kernel matrix directly so we don't assume any low rank."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First speaker is shipping tool with replacing photographs.",
                    "label": 0
                },
                {
                    "sent": "These are not the employees.",
                    "label": 0
                },
                {
                    "sent": "Thanks, good morning everyone.",
                    "label": 0
                },
                {
                    "sent": "In shipping and this is joint work with Kaiyuan Focal test.",
                    "label": 1
                },
                {
                    "sent": "Unfortunately, focus is very busy with some projects in Siemens, so I come here to present this talk for him.",
                    "label": 0
                },
                {
                    "sent": "Title is learning nonparametric priors for from multiple tasks.",
                    "label": 1
                },
                {
                    "sent": "Anne, OK.",
                    "label": 0
                },
                {
                    "sent": "So multitask learning.",
                    "label": 0
                },
                {
                    "sent": "I think we have a set of.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Related tasks to learn and the basic assumption here is that we have some dependencies between these tasks, and in this talk we assume that all the tasks share the same input space.",
                    "label": 1
                },
                {
                    "sent": "So basically they're working on the same set of objects.",
                    "label": 1
                },
                {
                    "sent": "So the idea is to learn them jointly instead of independently.",
                    "label": 1
                },
                {
                    "sent": "For some examples here, we consider multi label text categorization, in which each document is belongs to several different categories.",
                    "label": 0
                },
                {
                    "sent": "So here each task is classification problem.",
                    "label": 0
                },
                {
                    "sent": "In the second example we have multi task ranking problem.",
                    "label": 0
                },
                {
                    "sent": "So each task here is the ranking.",
                    "label": 0
                },
                {
                    "sent": "Problem so so here we have some connections to the collaborative filtering, so let's talk about more focusing on the second example here.",
                    "label": 0
                },
                {
                    "sent": "To give a scenario of what we're working on, so we have set of American movies and each movie has some features.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like the rest of the movie The directors, actors and some descriptions of the movie.",
                    "label": 1
                },
                {
                    "sent": "So we we presented system to 1 user.",
                    "label": 0
                },
                {
                    "sent": "And we ask this guy to rate for several of these movies from the very dislike to very like so we have maybe 5 levels of different preference labels.",
                    "label": 1
                },
                {
                    "sent": "So this guy makes some ranking here.",
                    "label": 0
                },
                {
                    "sent": "So the task here is to predict.",
                    "label": 0
                },
                {
                    "sent": "For the rest of the movie, What's the most likely label this this user might might take?",
                    "label": 0
                },
                {
                    "sent": "So here is basically a drinking problem and.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "People call this is ordinal regression problem because this is like a regression problem, but.",
                    "label": 0
                },
                {
                    "sent": "The regression has labels.",
                    "label": 0
                },
                {
                    "sent": "Is discrete labels here?",
                    "label": 0
                },
                {
                    "sent": "It's not like continuous real numbers.",
                    "label": 0
                },
                {
                    "sent": "So this is just the ranking problem.",
                    "label": 1
                },
                {
                    "sent": "But in reality we have lots of different users.",
                    "label": 0
                },
                {
                    "sent": "We have things that movies and issues are many rank.",
                    "label": 0
                },
                {
                    "sent": "Part of this movies and the rankings are different.",
                    "label": 0
                },
                {
                    "sent": "So here the task is to how to predict.",
                    "label": 0
                },
                {
                    "sent": "Observe the labels for all these users.",
                    "label": 0
                },
                {
                    "sent": "So this is the multi task ranking problem and we call in this talk.",
                    "label": 0
                },
                {
                    "sent": "Is this collaborative ordinal regression?",
                    "label": 1
                },
                {
                    "sent": "Because we have the multiple users?",
                    "label": 0
                },
                {
                    "sent": "This is more general than the collaborative filtering because we have some features here.",
                    "label": 0
                },
                {
                    "sent": "I think in the pure collaborative filtering framework we just use this part of the.",
                    "label": 0
                },
                {
                    "sent": "Data to learn.",
                    "label": 0
                },
                {
                    "sent": "But here we have a bunch of features.",
                    "label": 0
                },
                {
                    "sent": "So how to incorporate features into this framework?",
                    "label": 0
                },
                {
                    "sent": "This is the problem.",
                    "label": 0
                },
                {
                    "sent": "So here this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Assumptions is again that we assume that all the ranking problems are correlated.",
                    "label": 1
                },
                {
                    "sent": "They are not independent to each other.",
                    "label": 1
                },
                {
                    "sent": "The challenge here is how to model the dependencies between the users and how to make use of the features of each item elegantly.",
                    "label": 1
                },
                {
                    "sent": "And we could have some missing ratings in this data and we want to generalization to new items as well as new ranking functions.",
                    "label": 0
                },
                {
                    "sent": "So in this sense this is more general than the pure collaborative filtering cause in the pure Collaboration framework we don't have features and we cannot generalize to new items.",
                    "label": 0
                },
                {
                    "sent": "OK, we want this generalization to new items and new features, new functions.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is outline.",
                    "label": 0
                },
                {
                    "sent": "Rest of my talk.",
                    "label": 0
                },
                {
                    "sent": "I will first start with some ranking problem.",
                    "label": 0
                },
                {
                    "sent": "So how to formally define a ranking problem?",
                    "label": 0
                },
                {
                    "sent": "And I represent appears in framework for ordinal regression.",
                    "label": 1
                },
                {
                    "sent": "This is special ranking problem.",
                    "label": 0
                },
                {
                    "sent": "Now will say how to learn multiple ranking in the same way so it's a collaborative on regression.",
                    "label": 0
                },
                {
                    "sent": "Now we present some experiments results in some conclusion extensions.",
                    "label": 0
                },
                {
                    "sent": "So in ranking problem we would like.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To assign ranks to objects.",
                    "label": 1
                },
                {
                    "sent": "Assume we have N objects X one to XN in the D dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So for ranking we have two different ways of doing this.",
                    "label": 1
                },
                {
                    "sent": "So first way we call ordinal regression.",
                    "label": 0
                },
                {
                    "sent": "So basically we assume there's R labels.",
                    "label": 0
                },
                {
                    "sent": "We assume we assign each label to one.",
                    "label": 0
                },
                {
                    "sent": "These labels to each of these data points.",
                    "label": 1
                },
                {
                    "sent": "So we assume one is the lowest rank and R is the highest rank.",
                    "label": 0
                },
                {
                    "sent": "So it looks like a regression problem.",
                    "label": 0
                },
                {
                    "sent": "So basically you can assign that OK for each of item we assign one number here to each items based on this speech matrix.",
                    "label": 0
                },
                {
                    "sent": "So each item can only take one of these values.",
                    "label": 0
                },
                {
                    "sent": "So we call this is ordinal regression.",
                    "label": 0
                },
                {
                    "sent": "This is about ranking.",
                    "label": 1
                },
                {
                    "sent": "So how to rank items the 2nd way of doing ranking is to directly output our preferences.",
                    "label": 0
                },
                {
                    "sent": "At least so we say extend is preferred than X1X1 Professor X2 and so on we call this is preference learning.",
                    "label": 0
                },
                {
                    "sent": "So this problem is different from the.",
                    "label": 0
                },
                {
                    "sent": "The traditional classification regression problem cause in binary classification we have only two labels.",
                    "label": 0
                },
                {
                    "sent": "So you can say this special case of the ranking problems and for the multiclass classification we don't have any ordering between all the labels.",
                    "label": 0
                },
                {
                    "sent": "And in the regression case we have only real number outputs.",
                    "label": 0
                },
                {
                    "sent": "So here is the discrete output.",
                    "label": 0
                },
                {
                    "sent": "In this talk I will mostly focusing on the 1st type of ranking problems, ordinary regression.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in order regression we would like to assign ordered labels for objects, so the most.",
                    "label": 1
                },
                {
                    "sent": "Popular way of doing ordinary regression is that we assume there is a latent function F which Maps all the points into a real line.",
                    "label": 0
                },
                {
                    "sent": "So every point after mapping value here.",
                    "label": 0
                },
                {
                    "sent": "And we we can assume there is a boundary here.",
                    "label": 0
                },
                {
                    "sent": "So basically the boundary defines what's the most likely ranking label should be.",
                    "label": 0
                },
                {
                    "sent": "So we have R + 1 ranking boundaries.",
                    "label": 0
                },
                {
                    "sent": "Then we can say OK if the points lie in this area, we can assign the value to one to this point and so on.",
                    "label": 0
                },
                {
                    "sent": "So basically we have our regions for ranking.",
                    "label": 1
                },
                {
                    "sent": "So based on this boundary we can assign outputs to objects.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of the popular way of doing ordinary regression, and we have lots of applications here, like how to predict the user preferences and how to rank web pages for search engines.",
                    "label": 0
                },
                {
                    "sent": "If we have more than.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One task.",
                    "label": 0
                },
                {
                    "sent": "With a set of functions.",
                    "label": 0
                },
                {
                    "sent": "So basically the assumptions similar, so we assume each ranking problem has a little function F and we in the same time we map all the points into all the functions here.",
                    "label": 0
                },
                {
                    "sent": "So here the assumption is these functions are correlated and each function may only rank the part of the data.",
                    "label": 1
                },
                {
                    "sent": "So we have a lot of missing data here.",
                    "label": 1
                },
                {
                    "sent": "So this is a multi task setting and this is very common in the real world problems.",
                    "label": 0
                },
                {
                    "sent": "Things like copy of filtering.",
                    "label": 0
                },
                {
                    "sent": "So we want to predict the preferences.",
                    "label": 0
                },
                {
                    "sent": "And also for Rep ranking.",
                    "label": 0
                },
                {
                    "sent": "In fact pranking you can think each each of these tasks is a query and this is a document set web pages.",
                    "label": 0
                },
                {
                    "sent": "So we want to rank for each query.",
                    "label": 0
                },
                {
                    "sent": "What's likely ranking list of all documents so you can see it's a very natural multi task learning problem and you can say all the queries have some relation because the words are sort of have some common meaning.",
                    "label": 0
                },
                {
                    "sent": "So this tasks are sort of related.",
                    "label": 0
                },
                {
                    "sent": "OK, the question is how to learn this related ranking tasks jointly.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we first present appearance in framework for this kind of problem.",
                    "label": 1
                },
                {
                    "sent": "Then we can generalize this to multitask learning.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "For if there's only one task, we have a base and treatment of this ranking problems, so we have.",
                    "label": 0
                },
                {
                    "sent": "This is a conditional model on ranking labels.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "1st.",
                    "label": 0
                },
                {
                    "sent": "Define a ranking likelihood so based on latent function F, input data X and some parameter Theta, the output of the all the label, all the.",
                    "label": 1
                },
                {
                    "sent": "Items why this vector basically is just this likelihood and we can say we're running this concrete form.",
                    "label": 0
                },
                {
                    "sent": "So here is a vector of the latent values of all items to some parameters.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the ranking likelihood we will come to this later.",
                    "label": 0
                },
                {
                    "sent": "And given this likelihood, we can define prior in terms of PSN POV.",
                    "label": 1
                },
                {
                    "sent": "So for this latent function F we assign a nonparametric priors costing process.",
                    "label": 0
                },
                {
                    "sent": "Prior to this function.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "So basically this means on any finite set of items.",
                    "label": 0
                },
                {
                    "sent": "This finite vector is a cost and distributed with mean function H and kernel covariance function K. So here H is the main function.",
                    "label": 0
                },
                {
                    "sent": "Basically measures what's likely most likely mean of all this function values and K is the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Matrix measures similarity between every two items in terms of the function space.",
                    "label": 0
                },
                {
                    "sent": "Then, given this prior and likelihood we can marginalise the latent function F in this way.",
                    "label": 1
                },
                {
                    "sent": "So this is much less ranking likelihood integrate out.",
                    "label": 1
                },
                {
                    "sent": "So this is a general base and treatment of this problem and for the ordinal regression likelihood, we assume that given the function value F, they are independent with each other, so it's factorized.",
                    "label": 0
                },
                {
                    "sent": "Every items.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the graphical model.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a graphical model of the problem.",
                    "label": 1
                },
                {
                    "sent": "We have items.",
                    "label": 0
                },
                {
                    "sent": "And so this is operation, why I gave an effect size and function F. We don't directly give a parametric form for F, but we have signed a nonparametric priors.",
                    "label": 0
                },
                {
                    "sent": "So H is the main function case kernel matrix.",
                    "label": 1
                },
                {
                    "sent": "So this defines a nonparametric prior for the little function and we have the observations.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "From the Gaussian process prior to foreface nonparametric priors and from F. To the observation, this is basically ordinary regression likelihood.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So basically we're trying to model the 1st and 2nd statistics in this nonparametric prior.",
                    "label": 1
                },
                {
                    "sent": "And as you can see, if you assign different likelihood here, you can also model normal regression problem or classification problem and so on.",
                    "label": 0
                },
                {
                    "sent": "So it's very general framework for any kind of.",
                    "label": 0
                },
                {
                    "sent": "Supervised learning tasks.",
                    "label": 0
                },
                {
                    "sent": "So here we.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We will give two example ranking likelihood for ordinary regression here.",
                    "label": 1
                },
                {
                    "sent": "The first one is basically just a regression, so we assume that the Y is directly is sort of a real number, but we ignore.",
                    "label": 0
                },
                {
                    "sent": "This is discrete.",
                    "label": 0
                },
                {
                    "sent": "Somehow.",
                    "label": 0
                },
                {
                    "sent": "We assume that the likelihood that why I take some value is basically the.",
                    "label": 0
                },
                {
                    "sent": "The output of this Gaussian likelihood, specially if you have a mean FX I have, say, pass it.",
                    "label": 0
                },
                {
                    "sent": "You have the variance Sigma square and you observe this is likely function.",
                    "label": 0
                },
                {
                    "sent": "You just read the value of each.",
                    "label": 0
                },
                {
                    "sent": "Of this five rank labels.",
                    "label": 0
                },
                {
                    "sent": "We're just a regression on the output of the ranking.",
                    "label": 1
                },
                {
                    "sent": "Very easy one.",
                    "label": 0
                },
                {
                    "sent": "The second one is a little bit difficult complex, so it's proposed.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, true anchor money last year they call this Gaussian process ordinal regression.",
                    "label": 1
                },
                {
                    "sent": "They assign this kind of likelihood to the ranking labels.",
                    "label": 0
                },
                {
                    "sent": "So basically we assume we have a bunch of ranking labels boundaries is P12B four they define five regions and basically have a mean FX I have for variance Sigma so you have likely function here.",
                    "label": 0
                },
                {
                    "sent": "Then you simply calculate the area under this curve.",
                    "label": 1
                },
                {
                    "sent": "We've seen this boundaries, so basically for the number 4 you can calculate the area under this curve in these two boundaries here.",
                    "label": 0
                },
                {
                    "sent": "So this is this is the likelihood that while I takes #4.",
                    "label": 0
                },
                {
                    "sent": "At least it's automatically sum over sums to one and this super nice ranking likelihood, but you have more parameters like the Sigma and some boundary parameters be here.",
                    "label": 0
                },
                {
                    "sent": "This is specific for ranking problem.",
                    "label": 0
                },
                {
                    "sent": "OK, OK so that's just for one task.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Drinking problem.",
                    "label": 0
                },
                {
                    "sent": "So how do we do multitask learning with ranking?",
                    "label": 0
                },
                {
                    "sent": "So if you stick to this framework.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First approach you can think about is to learn a Gaussian process model for each task.",
                    "label": 1
                },
                {
                    "sent": "Then you will lose sort of connection between all the tasks.",
                    "label": 0
                },
                {
                    "sent": "So there's no share of information between tasks, so this is not good.",
                    "label": 1
                },
                {
                    "sent": "The second approach, maybe you can feed only one parametric kernel, because normally people do look awesome process learning people assume the kernel matrix is parametric kernel function and you just learn try to learn this kernel parameters from this kernel function.",
                    "label": 0
                },
                {
                    "sent": "So in the second approach you can try to feed one parametric kernel functions for all these tasks.",
                    "label": 0
                },
                {
                    "sent": "But then the problem is the parametric kernel maybe very restrictive, so you cannot show the freedom is.",
                    "label": 0
                },
                {
                    "sent": "Constraint to this specific domain, so no matter which kind of comment you choose, you cannot really get the.",
                    "label": 0
                },
                {
                    "sent": "Who knows?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "I think in the Spring 1 task learning problem people normally assume.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assume this kernel K comes from the covariance function.",
                    "label": 0
                },
                {
                    "sent": "You can think about RBF kernel function, so there's a parameter.",
                    "label": 0
                },
                {
                    "sent": "Here is basically the width of the kernel.",
                    "label": 0
                },
                {
                    "sent": "Then you try to adapt this concept from the data.",
                    "label": 0
                },
                {
                    "sent": "This is the normal way of doing Gaussian process learning, so it's not directly learn a kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "But learn some parameters here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so nonparametrics to learn everything.",
                    "label": 0
                },
                {
                    "sent": "Learn both the main function and kernel will show that.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here the key points of multitask learning for ranking is to model the collaborative effects.",
                    "label": 0
                },
                {
                    "sent": "So here we can say in terms of the ranking.",
                    "label": 0
                },
                {
                    "sent": "Say in some recommendation system for moving and rulers, we have two kind of collaborative effects.",
                    "label": 0
                },
                {
                    "sent": "The first line is common preferences.",
                    "label": 1
                },
                {
                    "sent": "So basically functions with share similar regression values in terms of the collaborative filtering, you can say all the users tend to share the same ranking labels.",
                    "label": 1
                },
                {
                    "sent": "For some items.",
                    "label": 0
                },
                {
                    "sent": "Some movies are very good, so everybody likes it.",
                    "label": 0
                },
                {
                    "sent": "Dislike it.",
                    "label": 0
                },
                {
                    "sent": "The second one is to model a similar similar variability, so this is basically a second order statistics.",
                    "label": 0
                },
                {
                    "sent": "So basically, if two items are very similar, then functions tend to have similar predictions for these two items, so this is basically a second order statistics, so we will try to model these two collaborative effects in this model.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So in this collaborative ordinal regression model, is a hierarchical Gaussian process model for my task ranking learning.",
                    "label": 1
                },
                {
                    "sent": "So here we assume for the same set of items, excellent X we have functions while wanted line.",
                    "label": 0
                },
                {
                    "sent": "So this is observed labels.",
                    "label": 0
                },
                {
                    "sent": "So how do?",
                    "label": 0
                },
                {
                    "sent": "How do you learn?",
                    "label": 0
                },
                {
                    "sent": "Is this multi task learning problem?",
                    "label": 0
                },
                {
                    "sent": "So we assume that the same for each of these tasks.",
                    "label": 0
                },
                {
                    "sent": "We have a little function which generates this ranking likelihood.",
                    "label": 1
                },
                {
                    "sent": "So this is basically the ranking order regression likelihood.",
                    "label": 0
                },
                {
                    "sent": "And we assign this.",
                    "label": 0
                },
                {
                    "sent": "We assume there's a same Gaussian process prior for all these functions.",
                    "label": 0
                },
                {
                    "sent": "So here this is nonparametric prior and the other thing for all the tasks sort of standard hierarchical Bayesian learning.",
                    "label": 1
                },
                {
                    "sent": "So basically the main function models the common preferences and covariance kernel, recurrence matrix, model similarity, similar variabilities.",
                    "label": 0
                },
                {
                    "sent": "Points is that we can learn both the mean function and the covariance matrix directly from the data.",
                    "label": 1
                },
                {
                    "sent": "So this covers metrics, not stationary.",
                    "label": 0
                },
                {
                    "sent": "It can be nice stationary.",
                    "label": 0
                },
                {
                    "sent": "And I think things you have more than one tasks to learn here.",
                    "label": 0
                },
                {
                    "sent": "So it is possible to learn both malfunction and covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Here we will show it later.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's the model.",
                    "label": 1
                },
                {
                    "sent": "This is a hierarchical based model for ranking functions.",
                    "label": 0
                },
                {
                    "sent": "So basically assume all the latent functions are samples from the same causing process prior.",
                    "label": 1
                },
                {
                    "sent": "Then given this functions we assign we allow different parametric setting for different tasks.",
                    "label": 0
                },
                {
                    "sent": "So basically here each task given this prior tasks are independent of each other and each should be integrated out over the latent function value.",
                    "label": 0
                },
                {
                    "sent": "So we also assign some hyper prior for this patient came here and we observe only part of the rank labels.",
                    "label": 0
                },
                {
                    "sent": "So far.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graphical model here.",
                    "label": 0
                },
                {
                    "sent": "So this is again the one task ranking problem and this is a hierarchical Gaussian process model.",
                    "label": 0
                },
                {
                    "sent": "So basically we assume we have functions to learn, so each one is L and given this function value we have we have an L may be different for each one for all the functions items.",
                    "label": 0
                },
                {
                    "sent": "So we observe part of the data here.",
                    "label": 0
                },
                {
                    "sent": "So we assign the same Gaussian process prior for all these functions cause an functions and too constrained.",
                    "label": 0
                },
                {
                    "sent": "The prior we assign hyper prior try to sort of constrain the freedom of this hyper prior.",
                    "label": 0
                },
                {
                    "sent": "So we call this graphical models hierarchical Gaussian process model.",
                    "label": 0
                },
                {
                    "sent": "And here the the key points is the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In process prior connects all this ranking tasks.",
                    "label": 1
                },
                {
                    "sent": "So we try to model the 1st and 2nd order statistics.",
                    "label": 0
                },
                {
                    "sent": "And the low level features are sort of incorporate naturally 'cause this is a.",
                    "label": 0
                },
                {
                    "sent": "It's a conditional model on the input, so the load features can be very easily incorporated.",
                    "label": 0
                },
                {
                    "sent": "So in this sense is more general than the pure collaborative filtering model.",
                    "label": 1
                },
                {
                    "sent": "So instead of fixing a parametric form for the kernel matrix, we basically assign a conjugate prior for both of the main function Ancona matrix.",
                    "label": 1
                },
                {
                    "sent": "So this is the normal inverse Wishart.",
                    "label": 0
                },
                {
                    "sent": "Prior we have some hyperparameters, H0, Pi and Powell and K0.",
                    "label": 0
                },
                {
                    "sent": "So for this case 0 can use any base kernel because any sort of parametric kernels to internalize this one.",
                    "label": 0
                },
                {
                    "sent": "But this is just for prior and you can see if you allow Pi and Tau going to Infinity, then it's basically means.",
                    "label": 0
                },
                {
                    "sent": "Prior all the tasks are totally independent.",
                    "label": 0
                },
                {
                    "sent": "So in this sense you mean you can try to assign different value for \u03c0 and Tau, and you kind of sign.",
                    "label": 0
                },
                {
                    "sent": "You can obtain different.",
                    "label": 0
                },
                {
                    "sent": "Collaborative effect.",
                    "label": 0
                },
                {
                    "sent": "So here we can make predictions for both the new input data and also new tasks.",
                    "label": 1
                },
                {
                    "sent": "In this framework I will show one point data for prediction for new tasks.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a toy problem.",
                    "label": 1
                },
                {
                    "sent": "Well, we assume we have 315 items here.",
                    "label": 1
                },
                {
                    "sent": "And this is prediction labels, so this thick black sick line is mean function.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The black dashed line is like a staircase lines.",
                    "label": 0
                },
                {
                    "sent": "Basically the mean ranking labels.",
                    "label": 0
                },
                {
                    "sent": "So basically assign the labels to be the most nearby integer here.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the model.",
                    "label": 0
                },
                {
                    "sent": "And we sample files functions from the Gaussian process based on this mean function and this kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "So this is the true kernel matrix for this 315 points.",
                    "label": 0
                },
                {
                    "sent": "So in this way we basically want to show that the econometrics very so the function should be honest, moves in the beginning phase and ending phase.",
                    "label": 0
                },
                {
                    "sent": "But but this, but they are very very very large variability in this space.",
                    "label": 0
                },
                {
                    "sent": "So basically here is value is very very shallow.",
                    "label": 0
                },
                {
                    "sent": "So we want to.",
                    "label": 0
                },
                {
                    "sent": "Reflect the truth that in a pure true system of reference system for very good movies and very bad movies, all the functions are sort of agree, so there's no very large variance for very good one, very bad one.",
                    "label": 0
                },
                {
                    "sent": "And for the for the movies in the middle.",
                    "label": 0
                },
                {
                    "sent": "And people maybe have very large variance for different users, so we assume that so for this movies people have large variance here.",
                    "label": 0
                },
                {
                    "sent": "So just just want to mimic the reality.",
                    "label": 0
                },
                {
                    "sent": "So here if you.",
                    "label": 0
                },
                {
                    "sent": "Is there a new function so you have observed only four points here?",
                    "label": 0
                },
                {
                    "sent": "This is 4 sort of outputs observations for a new function.",
                    "label": 0
                },
                {
                    "sent": "If you don't know anything about the kernel, if you assign RBF kernel here, then you do prediction is fine, but you say this variance are the same for all the points, so you cannot incorporate prior knowledge that the kernel is.",
                    "label": 0
                },
                {
                    "sent": "This shape is not RBF kernel, so even if you learn best idea, Colonel is still get.",
                    "label": 0
                },
                {
                    "sent": "This prediction is not very good, but if you try to learn the kernel and if you make the prediction here.",
                    "label": 0
                },
                {
                    "sent": "I got better prediction function here and you know.",
                    "label": 0
                },
                {
                    "sent": "In the in for this point, look at very large variance for this point.",
                    "label": 0
                },
                {
                    "sent": "In, the variance is very low because of the kernel tells you this.",
                    "label": 0
                },
                {
                    "sent": "So in this sense, look at you, try to make a prediction for a new function.",
                    "label": 0
                },
                {
                    "sent": "If you can learn the nonstationary econometrics, you get better prediction for the new functions.",
                    "label": 0
                },
                {
                    "sent": "Just try to give you a sense of generation 2 new tasks.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tell me, let's talk about the learning.",
                    "label": 0
                },
                {
                    "sent": "Can we use the style of learning in this framework so we have a variational lower bound this basic assigned by applying the distance quality and this is some over all the tasks?",
                    "label": 1
                },
                {
                    "sent": "And do you have a Q function?",
                    "label": 0
                },
                {
                    "sent": "Here is basically you can take any form for Q here.",
                    "label": 0
                },
                {
                    "sent": "This is lower bound style.",
                    "label": 1
                },
                {
                    "sent": "We first we try to estimate the Q function.",
                    "label": 0
                },
                {
                    "sent": "Here in M step we try to learn the parameter.",
                    "label": 0
                },
                {
                    "sent": "But here the problem is.",
                    "label": 0
                },
                {
                    "sent": "The likelihood is not Gaussian, maybe not causing so.",
                    "label": 1
                },
                {
                    "sent": "Here we use EP expectation propagation to learn a posterior Gaussian, so we're still assume Q keeps the Gaussian function.",
                    "label": 0
                },
                {
                    "sent": "When we try to estimate this FNC head using, the EP2 approximation is framework and after you learn the Q here, you fix Q and you maximize respect to the parameter for each task and the joint Gaussian process prior.",
                    "label": 0
                },
                {
                    "sent": "So basically we make custom app estimates for discussion process prior.",
                    "label": 0
                },
                {
                    "sent": "So here this is the true posterior.",
                    "label": 0
                },
                {
                    "sent": "Offer of 1 function.",
                    "label": 0
                },
                {
                    "sent": "Basically this is the prior authorization process prior for this function.",
                    "label": 0
                },
                {
                    "sent": "This is electrical term.",
                    "label": 0
                },
                {
                    "sent": "This factorized for all the terms.",
                    "label": 0
                },
                {
                    "sent": "So here in general the electrical term for each of these item is not Gaussian, especially in the regression case there should be some more complicated form.",
                    "label": 0
                },
                {
                    "sent": "So in this framework we use TP switches proposed by mean car.",
                    "label": 0
                },
                {
                    "sent": "So try to approximate.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Each of these terms to be a Gaussian sequentially.",
                    "label": 1
                },
                {
                    "sent": "So what we did is following three steps.",
                    "label": 0
                },
                {
                    "sent": "So we first initialize all this term.",
                    "label": 0
                },
                {
                    "sent": "Lies all approximation from TK for all the terms.",
                    "label": 0
                },
                {
                    "sent": "So just random initialize.",
                    "label": 0
                },
                {
                    "sent": "Then we try to repeat these three steps until convergence.",
                    "label": 0
                },
                {
                    "sent": "The first one we delete, we delete one of these factors from this approximate Gaussian.",
                    "label": 0
                },
                {
                    "sent": "The second one is we try to match the moments of adding the true likelihood term inside and we try to match the moments between the true posterior and the approximate posterior and in the third step we try to update the factor TK.",
                    "label": 1
                },
                {
                    "sent": "So basically we do this sort of sequentially, approximating each item each electrical term to be a Gaussian, and afterwards we can.",
                    "label": 0
                },
                {
                    "sent": "We can get to the joint Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So that was the question, I think, although it's not very important for this AP proximation.",
                    "label": 0
                },
                {
                    "sent": "Or what is it?",
                    "label": 0
                },
                {
                    "sent": "Here is fixes parameter.",
                    "label": 0
                },
                {
                    "sent": "But before signing in on stuff, we optimize this data in the East that we fixed the setup.",
                    "label": 0
                },
                {
                    "sent": "So so basically sitar.",
                    "label": 0
                },
                {
                    "sent": "Sitar is the same as a Theta is the same as a Gaussian process priors.",
                    "label": 0
                },
                {
                    "sent": "So these are all parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "So this is where we optimize this in one step.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me know what kind of phone do you have?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean this is about general listening in the sort of the example model we showed in the beginning for the one task order regression, we have some parameters.",
                    "label": 0
                },
                {
                    "sent": "Car show you.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case basically just the variance.",
                    "label": 0
                },
                {
                    "sent": "Mr Darcy and Paul sometimes variance in the second model City is more complicated.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Put all the variance term Sigma and also the boundary of this four of this.",
                    "label": 0
                },
                {
                    "sent": "For boundary parameters.",
                    "label": 0
                },
                {
                    "sent": "So in this model of data is sort of five.",
                    "label": 0
                },
                {
                    "sent": "Menus, so I mean you can define another kind of likelihood here, but maybe in general you have some parameters set up here.",
                    "label": 0
                },
                {
                    "sent": "We assume that sort of task specific is not.",
                    "label": 0
                },
                {
                    "sent": "Depend on other tasks.",
                    "label": 0
                },
                {
                    "sent": "Estimating those moments hard.",
                    "label": 0
                },
                {
                    "sent": "Maybe I mean stuff you need to do some computation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think.",
                    "label": 0
                },
                {
                    "sent": "I think just yeah, just this kind of likelihood make you make the step very difficult.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, so you have to do some approximation, so this is not Gaussian so we cannot just do normal.",
                    "label": 0
                },
                {
                    "sent": "Please stop OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Fortunately we can do this approximation approximation analytically.",
                    "label": 0
                },
                {
                    "sent": "For our example models.",
                    "label": 0
                },
                {
                    "sent": "So for this kind of profit ranking likelihood you can do analytically the update for this decay.",
                    "label": 0
                },
                {
                    "sent": "About but yeah, in general, if you if you choose any arbitrary form for this, then you should do some integration.",
                    "label": 0
                },
                {
                    "sent": "11 dimensional integration here.",
                    "label": 0
                },
                {
                    "sent": "So after we do this EP, we basically obtain a Gaussian approximation QFJ for each of these functions.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is for Easter.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for M step we try to fix the queue and we try to update all the model parameters.",
                    "label": 0
                },
                {
                    "sent": "So we first update the nonparametric costing process prior.",
                    "label": 0
                },
                {
                    "sent": "So this is the equation.",
                    "label": 0
                },
                {
                    "sent": "This is update for H is update for K. As you can see this is basically a weighted average over all the tasks.",
                    "label": 0
                },
                {
                    "sent": "The main function of all the tasks plus the prior term.",
                    "label": 0
                },
                {
                    "sent": "And for the kernel is also.",
                    "label": 0
                },
                {
                    "sent": "A similar is.",
                    "label": 0
                },
                {
                    "sent": "Basically this is a prior term.",
                    "label": 0
                },
                {
                    "sent": "This is the average over all the tasks, so that means we can learn directly your nonparametric kernel matrix K instead of learning some parametric parameter here.",
                    "label": 0
                },
                {
                    "sent": "And this update does not depend on any form of ranking likelihood.",
                    "label": 1
                },
                {
                    "sent": "So general and the contract prior just sort of the smallest term in this update.",
                    "label": 0
                },
                {
                    "sent": "And we also update likelihood primacy to J for each of the tasks.",
                    "label": 1
                },
                {
                    "sent": "And we can do this separately for each task.",
                    "label": 0
                },
                {
                    "sent": "I think this sort of computational cost is the same as if you just do one single task learning, so we don't really have more computational cost in this model.",
                    "label": 0
                },
                {
                    "sent": "I mean for for two and carmony's likelihood here, this step is very costly because you have to do concrete gradient update lots of parameters to learn.",
                    "label": 0
                },
                {
                    "sent": "OK, this is.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning and how about inference?",
                    "label": 0
                },
                {
                    "sent": "OK, so if we want to.",
                    "label": 0
                },
                {
                    "sent": "For Unk ranking label for new test data.",
                    "label": 1
                },
                {
                    "sent": "So here given the training data.",
                    "label": 0
                },
                {
                    "sent": "Given the learned parameters set to Jake HNC for new item X star, we want to infer the ranking label for the JS function.",
                    "label": 0
                },
                {
                    "sent": "This is why J.",
                    "label": 0
                },
                {
                    "sent": "So we do this also in the basin way, so we integrate out the latent function latent value FJ star.",
                    "label": 0
                },
                {
                    "sent": "So this basically the JS function on this star value.",
                    "label": 0
                },
                {
                    "sent": "So in the integration in the first term is very easy, just the likelihood ranking likelihood second term is.",
                    "label": 0
                },
                {
                    "sent": "It's not very easy to learn.",
                    "label": 0
                },
                {
                    "sent": "We have we show here.",
                    "label": 0
                },
                {
                    "sent": "So after you learn everything you just, particularly by assigning the maximal value of YJ two to this value.",
                    "label": 0
                },
                {
                    "sent": "So basically this is sort of how we do inference in this model.",
                    "label": 0
                },
                {
                    "sent": "But if the star is known in training, so it's the transductive setting, so you know the test data before you.",
                    "label": 1
                },
                {
                    "sent": "You make me very trained model, then the posterior of listening function value.",
                    "label": 0
                },
                {
                    "sent": "So basically this posterior is obtained from the E step when you do the EP approximation directly.",
                    "label": 0
                },
                {
                    "sent": "So then you can do this very elegantly.",
                    "label": 0
                },
                {
                    "sent": "But if the test data is not known in the training phase so basically that means the exercise new item.",
                    "label": 0
                },
                {
                    "sent": "You don't know this Atom before you train the model.",
                    "label": 0
                },
                {
                    "sent": "OK, so then there are some problems, because the nonstationary kernel cause in the learning you want to learn.",
                    "label": 0
                },
                {
                    "sent": "You want to learn econometrics ahead.",
                    "label": 0
                },
                {
                    "sent": "This is basically done by N and this number of training points.",
                    "label": 0
                },
                {
                    "sent": "You know nothing about the test points here.",
                    "label": 0
                },
                {
                    "sent": "So this is the problem.",
                    "label": 1
                },
                {
                    "sent": "So if you want to do inductive inference then use an S stationary kernel on the test data is not known.",
                    "label": 0
                },
                {
                    "sent": "So when you calculate this term you get some problem.",
                    "label": 0
                },
                {
                    "sent": "So how how do we do inductive inference?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can do a mapping here so we can walk into our space instead of in a functional space, so this is what we do in the in the in the beginning.",
                    "label": 0
                },
                {
                    "sent": "So this is the posterior for each function.",
                    "label": 1
                },
                {
                    "sent": "This is we do in the EP step in step.",
                    "label": 0
                },
                {
                    "sent": "Then we can.",
                    "label": 1
                },
                {
                    "sent": "We can go from the functional space to the weight vector space.",
                    "label": 0
                },
                {
                    "sent": "So basically assume that FJ is this form.",
                    "label": 0
                },
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "We can turn the problem of learning FJ to learning Alpha J.",
                    "label": 0
                },
                {
                    "sent": "So if after is this form after is access.",
                    "label": 0
                },
                {
                    "sent": "So Africa is also a Gaussian distributed.",
                    "label": 1
                },
                {
                    "sent": "So if we know the FJ here, so in the EP we can learn aperture instead of FJ, then in the test data we can use the base kernel so you know the base kernel of the test points user base kernel to make predictions for the little function.",
                    "label": 1
                },
                {
                    "sent": "So this way you can do inductive inference here.",
                    "label": 0
                },
                {
                    "sent": "So basically we want to do a mapping from the functional space F space to the Alpha space.",
                    "label": 1
                },
                {
                    "sent": "But here we have some.",
                    "label": 0
                },
                {
                    "sent": "You have some price to pay, so basically the predicted means are the same.",
                    "label": 1
                },
                {
                    "sent": "Maybe in the function of space and enough space, but the predicted variance are the different.",
                    "label": 0
                },
                {
                    "sent": "So you have you lost something?",
                    "label": 0
                },
                {
                    "sent": "Because this is only so you can do in for inductive inference.",
                    "label": 0
                },
                {
                    "sent": "In the Gaussian process model and we have theoretical results here so.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so given the given the constant process prior HNK sample from the hyper prior which is normal inverse patient prior.",
                    "label": 1
                },
                {
                    "sent": "And then there exists unique UMR fancy arfe.",
                    "label": 0
                },
                {
                    "sent": "So basically this models.",
                    "label": 0
                },
                {
                    "sent": "Cost in prior for our phone here so H&K can be written in this form and in any finite vector of the F function there exists are here, so there's sort of 1 to one equivalence here.",
                    "label": 0
                },
                {
                    "sent": "And then you often see also follow normal inverse Wishart distribution.",
                    "label": 0
                },
                {
                    "sent": "So that means we can equivalently work in that well space of Arfa instead of function space F. So this way you can do inductive inference.",
                    "label": 1
                },
                {
                    "sent": "But you will pay something because the prediction variance are lossed.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is basically the model.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me do some experiments.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For ranking.",
                    "label": 0
                },
                {
                    "sent": "We want to predict the user ratings for some movie data and we use two different data for first line movie lines we have.",
                    "label": 0
                },
                {
                    "sent": "With 500 movies and more than 100 users and we use some features for this task and for simplicity we just use the rank part.",
                    "label": 0
                },
                {
                    "sent": "So basically what the type of the movie it is and we have 19 features for movie lens and for each movie we do something more.",
                    "label": 0
                },
                {
                    "sent": "We try to go to the website of each movie and we try to get some text from the online database.",
                    "label": 1
                },
                {
                    "sent": "So and we do Jeff Idea features so it's more than.",
                    "label": 0
                },
                {
                    "sent": "20,000 features for each movie data, basically just the director, actor, and some description with movies.",
                    "label": 1
                },
                {
                    "sent": "Experiment setting, we pick up the binder users with the most ratings to be tasks.",
                    "label": 1
                },
                {
                    "sent": "So we fix the tasks and randomly choose some ratings for each of the users for training and we try to predict the rest of the ratings.",
                    "label": 0
                },
                {
                    "sent": "And for best kernel we use normally used cosine similarity kernel for.",
                    "label": 0
                },
                {
                    "sent": "For both of these tasks.",
                    "label": 0
                },
                {
                    "sent": "And we have.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So several comparison metrics, first part of the compared metrics is specially ordinal regression evaluation.",
                    "label": 1
                },
                {
                    "sent": "This is mostly used for voting regression.",
                    "label": 1
                },
                {
                    "sent": "People first lines mean absolute error basically count so so.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a particular label.",
                    "label": 0
                },
                {
                    "sent": "This is the true label.",
                    "label": 0
                },
                {
                    "sent": "Try to count the how large difference of these two is on average this is absolute error.",
                    "label": 1
                },
                {
                    "sent": "I mean 01 error is very easy.",
                    "label": 0
                },
                {
                    "sent": "You just.",
                    "label": 0
                },
                {
                    "sent": "If the labels are different, you have an error one.",
                    "label": 0
                },
                {
                    "sent": "If they are the same iris 0.",
                    "label": 0
                },
                {
                    "sent": "And because this is a much task learning process, we have macro and micro average for over the tasks and we also evaluated ranking directly ranking evaluation.",
                    "label": 0
                },
                {
                    "sent": "This is used by some ranking people is called normalized discounted cumulative gain.",
                    "label": 1
                },
                {
                    "sent": "So basically, given one ranking our head, we try to calculate 1 score and this call is basically you.",
                    "label": 0
                },
                {
                    "sent": "You thought you thought your rated items.",
                    "label": 0
                },
                {
                    "sent": "Based on the partition label and try to go from the top to the end.",
                    "label": 0
                },
                {
                    "sent": "So here the assumption is the top ranked items are most important for evaluation.",
                    "label": 0
                },
                {
                    "sent": "You don't really care about the last ranked items, so this is very useful for search engine evaluation stuff and we have discounted value of the of the position in the rank list.",
                    "label": 0
                },
                {
                    "sent": "We have some gain of.",
                    "label": 0
                },
                {
                    "sent": "Off getting some correct prediction value and you attend.",
                    "label": 1
                },
                {
                    "sent": "Basically, we don't.",
                    "label": 0
                },
                {
                    "sent": "We just kind of top 10 ranked items so we ignore the rest of the items for evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the result.",
                    "label": 0
                },
                {
                    "sent": "So here then is number of training items which user we 1020 and 50 and we compare 5 models.",
                    "label": 1
                },
                {
                    "sent": "The first model is causing progression.",
                    "label": 0
                },
                {
                    "sent": "Basically just do regression on the ranking labels.",
                    "label": 0
                },
                {
                    "sent": "Second wise collaborative version of this.",
                    "label": 0
                },
                {
                    "sent": "So you have much task setting and the third one is Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Ordinary regression using true Anchorman is likelihood and 3rd 1/4 one is a collaborative version of this last one is we select one very recent work for.",
                    "label": 1
                },
                {
                    "sent": "Covering filtering model maximum maximum margin metrics.",
                    "label": 0
                },
                {
                    "sent": "Factorization is last year in XML.",
                    "label": 0
                },
                {
                    "sent": "We try to compare this of this ordinary regression model with this collaborative filtering model and we have six meaningful error, macro, macro and mean 0101.",
                    "label": 0
                },
                {
                    "sent": "Monkey mattress factorization also allow the features no no.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, because they cannot use features from this framework.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so yeah.",
                    "label": 0
                },
                {
                    "sent": "So in this sense.",
                    "label": 0
                },
                {
                    "sent": "Regression model have some benefits over the pure CF.",
                    "label": 0
                },
                {
                    "sent": "You should do better than that, right?",
                    "label": 0
                },
                {
                    "sent": "'cause you're using features.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we're doing features.",
                    "label": 0
                },
                {
                    "sent": "Not using pizza.",
                    "label": 0
                },
                {
                    "sent": "It's possible it's possible to sort info ordinary regression model if you just assign the best country identity matrix.",
                    "label": 0
                },
                {
                    "sent": "So I mean you ignore the features, there's no features.",
                    "label": 0
                },
                {
                    "sent": "Then it's possible to do this.",
                    "label": 0
                },
                {
                    "sent": "We haven't happened to empirical study on this on this idea.",
                    "label": 0
                },
                {
                    "sent": "Because we argue this is sort of the benefit of the model, I think this is very interesting to do, as in Korean, could you just treat the features as another users with these other techniques cheating, but just treat them the feature value as the rank of the user you use.",
                    "label": 0
                },
                {
                    "sent": "Part of the users of the features for each feature you just create as a user.",
                    "label": 0
                },
                {
                    "sent": "In some sense you know OK. You mean using the message to incorporate features features?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think that's that's interesting to compare, I think, But yeah, But in this talk we didn't use them.",
                    "label": 0
                },
                {
                    "sent": "I think we can.",
                    "label": 0
                },
                {
                    "sent": "We should compare this I think.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think full results.",
                    "label": 0
                },
                {
                    "sent": "I think the first observation is a collaborative version, which is more always better than the individual version.",
                    "label": 0
                },
                {
                    "sent": "I think it should be very small and here.",
                    "label": 0
                },
                {
                    "sent": "For the first of all measures, the smaller the better.",
                    "label": 0
                },
                {
                    "sent": "For the last two is, the higher the better.",
                    "label": 0
                },
                {
                    "sent": "So for the first 2 manifold that mean absolute error.",
                    "label": 0
                },
                {
                    "sent": "The collaborative version of Gaussian process regression is the best.",
                    "label": 0
                },
                {
                    "sent": "And for the bin 01 error, I think the collaborative version of the regression likelihood is the best.",
                    "label": 0
                },
                {
                    "sent": "And for last measure, I think for this task MF is the best because it directly maximize DCG like cost.",
                    "label": 0
                },
                {
                    "sent": "So so in this MF is very good for ranking related evaluation.",
                    "label": 0
                },
                {
                    "sent": "But for the second one.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For each movie that I think even for the new situation score, I think the collaborative version of regression is the best and for the for the other scores thing is the same.",
                    "label": 0
                },
                {
                    "sent": "So it's interesting because I think for for the regression likelihood they use a more specific likelihood for ranking, but only on this score it get better results.",
                    "label": 0
                },
                {
                    "sent": "I think one assumption, one sort of explanation is has a lot of parameters, so every task I think in our model we have five different labels, so we have 4 boundary values and we have variance.",
                    "label": 0
                },
                {
                    "sent": "So for this model we have 5 parameters to learn and for this one just one particular, so basically maybe because of the overfitting of the tasks for each of the tasks.",
                    "label": 0
                },
                {
                    "sent": "And yeah, but this very simple regression model got very good results for for some measure.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So compare we also tried to make predictions for new functions.",
                    "label": 0
                },
                {
                    "sent": "So have I used the movie lines as testbed?",
                    "label": 0
                },
                {
                    "sent": "And here we so from the beginning from top to end we have.",
                    "label": 0
                },
                {
                    "sent": "So first of all, it's based kernel.",
                    "label": 0
                },
                {
                    "sent": "You space kernel to make predictions for test user.",
                    "label": 0
                },
                {
                    "sent": "So we use just causing kernel for test user and then we use different number of training users to learn the kernel and user kernel to predict full test users.",
                    "label": 0
                },
                {
                    "sent": "So as you can see if you use more so it's 5200 two 100.",
                    "label": 0
                },
                {
                    "sent": "So if you use more users for training kernel.",
                    "label": 1
                },
                {
                    "sent": "You got a better partition for the rest of users, so this is for actual error.",
                    "label": 0
                },
                {
                    "sent": "This was 01 arrow is 2 digit score.",
                    "label": 0
                },
                {
                    "sent": "So you mean you can see when you use a lot of users to learn the kernel, you get better prediction for all these measures.",
                    "label": 0
                },
                {
                    "sent": "So that means you can learn a better kernel if you use more tasks.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of the conclusion we use more users we can learn better kernel.",
                    "label": 1
                },
                {
                    "sent": "For ranking.",
                    "label": 0
                },
                {
                    "sent": "So for the motivations, we observe that a collaborative version.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is always better than the individual version.",
                    "label": 1
                },
                {
                    "sent": "We can learn very good nonstationary come from users and from our experience these two models are very fast training and our robust in testing.",
                    "label": 1
                },
                {
                    "sent": "So there's no sort of approximation everything is causing.",
                    "label": 1
                },
                {
                    "sent": "The second model is very slow in any steps and sometimes over feet.",
                    "label": 0
                },
                {
                    "sent": "And we can also use some other ranking likelihood, but in this talk we just use the two example likelihood.",
                    "label": 0
                },
                {
                    "sent": "But it is possible to use other likelihood.",
                    "label": 1
                },
                {
                    "sent": "Or maybe you need to do some integration in each step.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we also did some experiments with multi label text categorization.",
                    "label": 1
                },
                {
                    "sent": "We compare the much task costing process model with Ridge regression SVM.",
                    "label": 0
                },
                {
                    "sent": "We use RC1 testbed for task categorization, and we training.",
                    "label": 0
                },
                {
                    "sent": "We fixed the 50 categories and we have 10 repeats and choose 1000 documents randomly labeled.",
                    "label": 0
                },
                {
                    "sent": "Some of them and protester.",
                    "label": 0
                },
                {
                    "sent": "We use 10,000 documents, so we compare the UC score at macro, not micro and our method get better results here.",
                    "label": 0
                },
                {
                    "sent": "And if you have a partially labeled items is also the same trend.",
                    "label": 0
                },
                {
                    "sent": "And if you if you try to make predictions for the other 30 one categories and if you can.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pears are.",
                    "label": 0
                },
                {
                    "sent": "Micro Micro and F measure you get better performance for frozen process.",
                    "label": 0
                },
                {
                    "sent": "How can process model this model is better than the regression?",
                    "label": 0
                },
                {
                    "sent": "Also better than the spam classifier?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, for some conclusion we",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We present the base into framework for much task rec learning and this is efficient GMP learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "So we are able to learn the informative nonparametric prior.",
                    "label": 1
                },
                {
                    "sent": "In a couple of version is always better than all individual version and expenditures currently results.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for some extensions we can also apply this model for model purposes learning.",
                    "label": 0
                },
                {
                    "sent": "So in this talk we mainly focus on the original regression likelihood because it's very easy to extend this framework to model preference learning.",
                    "label": 0
                },
                {
                    "sent": "So each of this task is the preferences pairs of items.",
                    "label": 0
                },
                {
                    "sent": "So there's one work on single task, Gaussian process model for preference learning last year and it's very easy to extend this to elaborate version so we can also I think.",
                    "label": 0
                },
                {
                    "sent": "Purchase the presenter rank net for ranking of documents and we can also propose a public version of this based on this sort of likelihood definition.",
                    "label": 0
                },
                {
                    "sent": "So it's also possible to extend the clarity version.",
                    "label": 0
                },
                {
                    "sent": "Much task learning for preference.",
                    "label": 0
                },
                {
                    "sent": "Learning.",
                    "label": 0
                },
                {
                    "sent": "The second one is to consider the Gaussian process mixture model for multi task learning.",
                    "label": 1
                },
                {
                    "sent": "'cause if you think about users case an maybe there's some user groups of all the tasks so it's something like a task.",
                    "label": 1
                },
                {
                    "sent": "Grouping so we could assign across the mixture model instead of 1 Gaussian for the hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "So the prediction then is the linear combination of of learn kernels.",
                    "label": 1
                },
                {
                    "sent": "And if you allow the mixture number to be Infinity then you have some connection to additional process.",
                    "label": 0
                },
                {
                    "sent": "Model is even higher level of nonparametric modeling.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have a little some little work, but maybe I don't have time to go over it.",
                    "label": 0
                },
                {
                    "sent": "And some little work on ranking.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning ranking, so something like a margin based method and paragraphs translation and basic approach.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think this is basically the talk.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Numbers I don't quite understand your method explain to me what is work OK. You mean the results or the?",
                    "label": 0
                },
                {
                    "sent": "PR work is CDPR.",
                    "label": 0
                },
                {
                    "sent": "OK, I think I didn't mention this.",
                    "label": 0
                },
                {
                    "sent": "Because we have two example models in for one task ranking right.",
                    "label": 0
                },
                {
                    "sent": "So basically GPS is Johnson process regression this is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we call this GPR so if you just directly just do regression on the labels, this is called the regression model.",
                    "label": 0
                },
                {
                    "sent": "And if you use this new likelihood function, we call this GPRS ordinary regression model.",
                    "label": 0
                },
                {
                    "sent": "And in the table we used C. So if you.",
                    "label": 0
                },
                {
                    "sent": "So we use CPR to represent the collaborative version of Multitask learning.",
                    "label": 0
                },
                {
                    "sent": "So so here's the CPI.",
                    "label": 0
                },
                {
                    "sent": "Basically the cabbage.",
                    "label": 0
                },
                {
                    "sent": "So much task learning with scrap defect.",
                    "label": 0
                },
                {
                    "sent": "Do you learn independent tasks independently of all the tasks and this collaborative version of gotten personal regression model?",
                    "label": 0
                },
                {
                    "sent": "You sort of.",
                    "label": 0
                },
                {
                    "sent": "You have the collaborative effect in the modeling for this one.",
                    "label": 0
                },
                {
                    "sent": "You don't really consider the collaborative effects.",
                    "label": 0
                },
                {
                    "sent": "I don't consider the collaborative effect, so you learn every task, each task independently.",
                    "label": 0
                },
                {
                    "sent": "Ignore the dependence.",
                    "label": 0
                },
                {
                    "sent": "So therefore they all use some more features than the.",
                    "label": 0
                },
                {
                    "sent": "That's right, that's right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so somehow it's not fair for anonymous, but the longer it is.",
                    "label": 0
                },
                {
                    "sent": "This is sort of the benefit of regression model so, so that's what I mean.",
                    "label": 0
                },
                {
                    "sent": "The collaborative ordinal regression is more general than collaborative filtering because it can use more information very elegantly.",
                    "label": 0
                },
                {
                    "sent": "Is the conditional model on features.",
                    "label": 0
                },
                {
                    "sent": "OK, so I don't know your prior to time your do you have any low graphics structure on your panel?",
                    "label": 0
                },
                {
                    "sent": "No, I think this work.",
                    "label": 0
                },
                {
                    "sent": "We learn the kernel matrix directly so we don't assume any low rank.",
                    "label": 0
                }
            ]
        }
    }
}