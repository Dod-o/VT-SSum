{
    "id": "e5btutch6qls5zkih2ufq5tcy6n4yw77",
    "title": "Message-passing for Graph-structured Linear Programs",
    "info": {
        "author": [
            "Alekh Agarwal, Microsoft Research"
        ],
        "published": "Aug. 1, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_agarwal_mpg/",
    "segmentation": [
        [
            "Morning Sir, my little girl and today I'll be talking about Proxamol projections and running schemes for message passing and graph structured linear programs, and this is joint work with Pradeep Kumar and Martin Wainwright at UC Berkeley."
        ],
        [
            "So Markov random field is a collection of random variables and we assume we're dealing with discrete random variables.",
            "Here's that.",
            "Take values in, let's say one through M an.",
            "We assume that the probability distribution characterizing these random variables is given by a set of para meters.",
            "I know which are the node potentials that specify the local properties of each random variable and a set of edge potentials that specify the nature of interaction between pairs of random variables and it is well known that any probability distribution factorizing according to the clicks of a graph can be represented in this form.",
            "So the map labeling for the vertices of this graph is the highest probability labeling according to this probability distribution an from the form of the distribution.",
            "Easily seem to be the optimizer over all possible discrete labelings of the near objective function inside the exponential."
        ],
        [
            "So.",
            "A lot of previous work has been done on this problem, starting from Max product, which is a dynamic programming algorithm that is known to be exact on trees.",
            "People have looked in vision community extensively at graph cuts which are for instance known to be exact on certain binary problems and bike off at all provide a nice summary of work in this area.",
            "Related Max product is one of the popular algorithms for this problem that was proposed by Wainwright Allen further studied by Kolmogorov ET Al Ain.",
            "Person and a Cola have recently proposed globally convex version of this algorithm using the idea of oriented trees convex free energy approximations have been looked at by Airways and several convex relaxations to linear programs.",
            "Quadratic programs, 2nd order kodenkan programs and SDP's, etc.",
            "Having looked into quite a bit of detail as well.",
            "Some more recent work includes Lagrangian relaxation and simulated annealing by Johnson ET al, and building composition and subgradient methods by, DocuSign, coworkers."
        ],
        [
            "We are in particular concern with the 1st order relaxation for this problem in this top, which is obtained by noticing that we are dealing with a linear optimization problem here.",
            "So instead of just optimizing over discrete configurations, we can optimize over the convex Hull of all the labelings and not change the value of the optimum.",
            "So when we do this we end up with a set of optimization variables that are the node marginal probabilities of a given node taking a particular label and edge marginal probabilities of.",
            "Node pair taking on a particular label pair and of course, these probability of a certain constraints."
        ],
        [
            "However, the number of these constraints is exponentially larger.",
            "The problem size in general, so people use various subsets of constraints to perform this optimization efficiently.",
            "One particularly well known approximation is to use the obvious nonnegativity constraint along with the constraint that the node marginal overall node labels sum up to one for every node and at the edge.",
            "Marginals are consistent with node marginal, which is the marginalization constraint.",
            "And this constraint set is referred to as a local polytope because of the local nature of constraints."
        ],
        [
            "So at this point we have a linear program and we can invoke any of the shelf LP solving procedure like interior point methods, subgradient methods etc.",
            "An we can solve the problem in principle.",
            "However these methods often turn out to be too expensive or too slow for this problem and the reason is that the number of optimization variables here scales as number of edges times number of labels squared.",
            "So the complexity of these methods is often prohibitive for typical problem sizes and the reason is that these methods are not.",
            "Specifically designed for this problem and hence they fail to exploit the particular problem structure here.",
            "Also, these methods are often hard to implement in a paralyzed distributed fashion, which is very important concern at these problem sizes prevented Max product does address some of these concerns and it solves this particular dual of this linear program.",
            "However, the optimization method is particularly tailored to the structure of the local polytope and hence it's not clear that if one wants to incorporate new constraints.",
            "Then how to do this in a seamless fashion so this?"
        ],
        [
            "We're clearly looking for a map estimation algorithm that can provide us an exact solution to the LP relaxation and in a time complexity that's competitive with algorithms like Max product entry, weighted Max product.",
            "We want to be able to come up with distributable and parallelizable updates.",
            "So essentially we are looking at some kind of iterative methods that have look easy to obtain local computations an we want to be able to incorporate new constraints.",
            "Into our algorithm without much work.",
            "So to do this we need to invoke some relevant tools from optimized."
        ],
        [
            "In theory, and the one that comes in particularly handy here is the principle of proximal minimization, where if you want to optimize a particular convex function F that is not very nice to optimize over directly, then you add a second to your objective, which is some kind of a generalized distance function.",
            "In our case, we have a linear objective function, and we augment it with the Bregman divergences so.",
            "As you can.",
            "We set up a series of subproblems where at every iteration we we minimize the linear objective plus the Bregman divergences, the previous hydration, and we repeat this process until we converge, so one immediate consequence of adding this Bregman divergences that the objective is strictly convex Now, so you can go and solve this problem and the dual if you wish to.",
            "The idea of adding a second function to make the objective well behaved is sort of close to any link an indeed can be.",
            "This method can be equivalent to any link for certain choices of the Bregman divergences.",
            "However, there is a critical difference in general, which is that when this method converges because of the fact that the second term is a Bregman divergences, it goes to 0 by itself.",
            "So even if your multiplier WN.",
            "Is not going to Infinity as is needed for annealing.",
            "These methods will still converge to the only possible fixed point, that is the that is the optimizer of your original objective function, which makes this method somewhat different from annealing.",
            "So since we have Bregman divergences involved here, the 2nd."
        ],
        [
            "Tool that can come in handy is that of Bregman projections and the idea here is that given any reference point we can define its projection onto a convex set as the as the point in the set that minimizes Bregman divergences.",
            "To this reference point and.",
            "This is uniquely defined because of the strict convexity of Bregman divergences in its first argument.",
            "Now the interesting aspect of these projections is that if we have a complicated convex set which can be described as the intersection of simpler convex sets, then suppose we start with a point P that we want to project onto the intersection of these two lines.",
            "Then we first project P onto the first line, take the projection projected onto the second line, and we repeat this process iteratively take.",
            "Cycling over all the convex sets that feature in this intersection and it has been shown that this method is always guaranteed to converge and it converges to the projection of our reference point onto the intersection of these sets.",
            "So if."
        ],
        [
            "We can set up our proximal minimization problem as as the minimization of a Bregman divergences.",
            "Then it's conceivable that we might be able to leverage this very attractive theory in performing our optimization sufficiently so to put things into perspective.",
            "We start off with a linear program that we wish to minimize."
        ],
        [
            "We augmented with the proximal function."
        ],
        [
            "Now we do a slight rewriting at this point to absorb the linear term into the Bregman divergences, and."
        ],
        [
            "This is done by updating the reference reference perimetre mean to mute~ N and the way you go from you to Matilda to preserve the sequence of course depends on the particular choice of Bregman divergences, so in particular, if you use an L2 distance as your Bregman divergences, then the update is very simple and just consists of adding Theta vector to your current titrate.",
            "For the case of KL divergences, it is obtained as Newman Times X plus data, where product and exponentiation are.",
            "Defined element wise for vectors and now we know that we are performing this optimization over a polytope, which is just an intersection of a bunch of linear constraints.",
            "So we have obtained a Bregman projection problem that we want to perform over intersection of linear constraints, and if we can figure out how to project onto linear constraints easily, then we just need to invoke cyclic projections to complete the optimization problem at hand."
        ],
        [
            "This gives us the basic outline of our algorithm where we start from an initial point, perform the initialization step that we just described in the previous slide using psychic predictions."
        ],
        [
            "We project neutral one onto the polytope."
        ],
        [
            "And we repeat this process an until we converge and we are guaranteed to converge to the optimum of the linear program by approximation theory."
        ],
        [
            "So just to give you a flavor of what these what this algorithm looks like, they initialize.",
            "We give a version of the algorithm using KL divergences as the Bregman function, and so the initialization step has already been mentioned in the previous slide.",
            "For this particular choice projection onto normalization constraint is really simple, it just amounts to dividing every node marginal to the by the sum of node marginal overall.",
            "Labels for that node for the edge marginalization constraint.",
            "The update has been shown here and if you are working in the message passing then you will see the immediate resemblance of this to the belief propagation updates, and this is encouraging because this means that our method will indeed inherit all the good properties of belief propagation like it's local nature, which makes it easy to implement in a very distributed fashion and the iterative nature which allows.",
            "Parallelized implementations, and so on and similar derivations are possible with a bunch of other divergences, and we have worked out a couple of examples in the paper."
        ],
        [
            "So to go back to our set of desired properties, it's obvious at this point that we have an algorithm that is exactly solving the LP relaxation.",
            "By approximation theory we we have updates that are easy to implement in a parallelized and distributed fashion because of the similarity to belief propagation, and it's easy to incorporate new constraints because you just need to compute the projection onto individual constraints, which is usually not too hard.",
            "The only point that's not clear immediately is what is the time complexity of these schemes?",
            "An if if the linear programming relaxation is inexact, that is, the LP has a non integral solution.",
            "Then indeed these methods can take awhile to converge.",
            "However, the interesting cases when the LP is indeed integral.",
            "In this case we can use very effective rounding schemes to obtain a solution in time complexity competitive with Max product and related Max product so."
        ],
        [
            "So how do rounding schemes come in?",
            "Well, we need to recall that we are interested in solving an integer linear program here and find the Max maximum probability labeling.",
            "So although we are solving the relaxation if at any point we take our set of marginals at that point and we can round them to produce labeling for the nodes, that is guaranteed to be optimal, then we're done.",
            "The problem is solved, so the rounding schemes aimed to do exactly this an once one such.",
            "The instance of a rounding scheme is to just find the local optimal label for every node from your current estimate of node marginals, but such a naive scheme is bound to fail.",
            "An literature is bound with examples where this scheme will not work.",
            "So we moved to slightly more higher order."
        ],
        [
            "Rounding, which involves edges essentially so.",
            "The idea is that you associate a local quantity mu Baron with every edge that that involves.",
            "That is a function of the node and edge marginals at that point.",
            "So Mu Barron is defined as muse to the one over degree of is times muti to the one over degree of tee times the edge marginal, and this is a local quantity that involves an edge and both the vertices.",
            "Now you find the label pair that is optimal for this local quantity and.",
            "If if these these label pairs are found to be consistent across all edges, So what does that mean?",
            "Suppose you look, you find the optimizing label pairs for two edges, and you look at the value they produce on a node that is shared between them.",
            "Then this label should be consistent.",
            "So if this consistency holds across all edges in the graph, then you declare that you have found a map labeling.",
            "You have found the solution to the integer linear program.",
            "And you stop."
        ],
        [
            "Well, you can fail to find such a consistency, and in that case you just say that I haven't found the solution at yet and you continue your iterations.",
            "From areas we can also move to."
        ],
        [
            "Other higher order neighborhoods like Stars, which is a node and all its neighbors, we can again do the same scheme that we find the local optimal configuration for this star Ann.",
            "If these configurations are consistent across All Stars, then we declare optimality and terminate."
        ],
        [
            "We can move on to trees, repeat the same argument."
        ],
        [
            "So wild and I rounding scheme does not enjoy any guarantees in general, the."
        ],
        [
            "The theorem that we can show is that at any iteration, if any of the edge star or tree rounding schemes converge, an find a consistent labeling across all neighborhoods of a particular kind, then that configuration is the map configuration indeed, so so they have this strong.",
            "These rounding schemes have this strong guarantee backing them an this showing.",
            "These essentially involves some invariants that are maintained by our algorithm.",
            "At all iterations."
        ],
        [
            "And Moreover, once we you start using these rounding schemes, it usually takes us 8 to 10 rounds worth of privated Max product time to converge by rounding for an integral LP.",
            "So you have to incur some some additional computational cost for solving the LP exactly, but I think it's not, it's not.",
            "It's still in the same bowl."
        ],
        [
            "Park.",
            "So to go to rates of convergence, it has been shown for proximal minimization methods that they have a super linear rate of convergence for a wide class of Bregman divergences, assuming that the proximal subproblem at every step is solved exactly.",
            "Now of course this is not possible because at every step you terminate with some finite tolerances.",
            "So the inner problem is only being solved in exactly what is nice.",
            "However, that is that guarantees exist that say that if there is a procedure for solving this inner loop in exactly that has a linear rate of convergence, then the overall proximal method will still be consistent.",
            "If the errors don't accumulate in a bad way, Ann will have an overall linear rate of convergence.",
            "An cyclic Bregman projections do meet this constraint they have they have a linear rate of convergence, which means that even in the exact set of our method does have an overall linear rate of convergence.",
            "At least an we will again like to emphasize that for the case of Integral LP's we have a much faster rate of convergence using the rounding schemes, and this is not just an empirical statement, but you can actually show that in theory that this the convergence happens really fast using rounding.",
            "By analyzing certain sufficient conditions for the rounding schemes to work.",
            "So this is not currently included in the paper, but is being written up at the moment."
        ],
        [
            "So to verify our experimental performance of our algorithms, we looked at grid graphs between 100 to 900 nodes and with five node labels.",
            "We experimented with the parts potential with varying SNR, which is the ratio of the strength of node potential to that of edge potential.",
            "So the low SNR is a harder regime than a higher SNR.",
            "In this problem, we wanted to empirically verify are super linear rates of convergence and the effectiveness of our rounding schemes."
        ],
        [
            "So further rates of convergence, we plotted the log of.",
            "L2 distance of our marginal vector at every iteration to the optimal.",
            "Optimal solution as a function of the number of iterations, and we did this for three different problem sizes and here the plot is shown for a representative value of SNR and it's easily seen that the rate of convergence is super linear.",
            "If it were just linear, then we would have obtained straight lines.",
            "In the log plot."
        ],
        [
            "For testing the effectiveness of rounding schemes, we plotted the number of the fraction of edges that violate the edge consistency constraint in the edge rounding scheme.",
            "Again, as a function of number of iterations, and again for variety of problem sizes, and you can see that this comes down really fast for the case of Integral LP's and what's even more impressive is that it does not scale too badly with the.",
            "With the increase in problem size.",
            "So this means that our methods really have a very good scalability property."
        ],
        [
            "We also wanted to look at the energy of the rounded solution cause energy in some sense is the the.",
            "Value of the objective function in this case, which you are.",
            "Which is the main quantity of interest anyways.",
            "So we plotted the ratio of the energy of rounded solution to the energy of the true solution, and again it can be seen that this ratio approaches one really fast and it does so without and this does not change with an increase in problem size in a significant way."
        ],
        [
            "So to sum up, we have proposed a new class of map estimation algorithms that use the approximal minimization framework, an using graph structured Bregman driver divergences.",
            "We can exploit the problem structure very effectively, and the cyclic Bregman projections allow us to exploit the simple structure of the local local constraint pollito pan solve this problem in a very efficient manner.",
            "We derive simple message passing updates that have a guarantee of convergence to the LP Optimum.",
            "And we would like to emphasize this is different.",
            "This is different from some of the other message passing based methods which were able to provide some partial guarantees when the LP is integral, but we're completely in dark when the LP relaxation is inexact, so we have a guarantee of convergence to LP optimum even in that case an we show rounding schemes that allow us to come up with extremely fast solution for the case of Integral LP's.",
            "Thanks a lot for your attention.",
            "So how does this compare?",
            "So as we said, for the case of Integral LP's, it usually takes us like so each iteration of the each solving each proxamol problem is almost as costly as doing a tree weighted minimization, and we take a 210 iterations empirically, so that's a good estimate of the cost of this method.",
            "Yeah.",
            "So we we the experiments that are shown here are with KL divergences.",
            "We also experimented with L2 distance and some more.",
            "L2 distance was not that effective.",
            "Kill was empirically much better than L2 an there are there are more interesting divergences that can be used but are hard to describe in this talk.",
            "When you do, the projection gets positive.",
            "Don't get around.",
            "You're likely to meet their version control.",
            "L2 actually does not hit the vertices of the polytope.",
            "The projections are the projections.",
            "So first of all, cyclic projections takes a long time to converge when you are using L2, so each each subproblem solution becomes very inefficient using L2.",
            "Cal Divergance converges really fast in that sense.",
            "For the inner loop.",
            "We just use like numerical tolerance.",
            "Once all the constraints are so the thing with Bregman projections is that the first time you hit us feasible point it will be the correct feasible point.",
            "It will be the projection.",
            "So we just check constraint violation and once it is below us numerical tolerance we.",
            "For 2300, it actually varies, so at the at the start it often takes more passes an as your proxamol problem is nearing convergence.",
            "Even the inner loop speed up.",
            "No, no we we have.",
            "In fact we have experimented even with overcomplete potentials mixed mixed over complete.",
            "But we just included parts because more more papers have.",
            "Yeah, no, no parts again mix, so yeah.",
            "753 yes.",
            "I mean, we haven't.",
            "We haven't empirically compared our method with that, but.",
            "It is possible that in certain cases at least, the two might be exactly equivalent, and but in general this could look pretty different, I think.",
            "Becausw because this is, I mean, because this is a Bregman divergences and not just the entropy of us of your current solution, so this.",
            "This makes the algorithm look a bit different.",
            "Yes, yes, so cyclic projections as they see sequential procedure.",
            "So how do you parallelize it?",
            "So once."
        ],
        [
            "Once you have these.",
            "These updates you can.",
            "In parallelize these across edges like what one edge is doing, does not have to coordinate with what another edge is doing, so you can use that to run.",
            "No, that's the whole.",
            "That's the whole thing, right?",
            "The local polytope has just constraints involving nodes and edges.",
            "If you use, you use more global constraints, then you will get more global updates and then parallelization would.",
            "As you as you make your constraints more and more global, you have to look at higher order subgraphs of some kind essentially.",
            "But here we are dealing with the local polytope, so."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Morning Sir, my little girl and today I'll be talking about Proxamol projections and running schemes for message passing and graph structured linear programs, and this is joint work with Pradeep Kumar and Martin Wainwright at UC Berkeley.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Markov random field is a collection of random variables and we assume we're dealing with discrete random variables.",
                    "label": 1
                },
                {
                    "sent": "Here's that.",
                    "label": 0
                },
                {
                    "sent": "Take values in, let's say one through M an.",
                    "label": 0
                },
                {
                    "sent": "We assume that the probability distribution characterizing these random variables is given by a set of para meters.",
                    "label": 0
                },
                {
                    "sent": "I know which are the node potentials that specify the local properties of each random variable and a set of edge potentials that specify the nature of interaction between pairs of random variables and it is well known that any probability distribution factorizing according to the clicks of a graph can be represented in this form.",
                    "label": 0
                },
                {
                    "sent": "So the map labeling for the vertices of this graph is the highest probability labeling according to this probability distribution an from the form of the distribution.",
                    "label": 0
                },
                {
                    "sent": "Easily seem to be the optimizer over all possible discrete labelings of the near objective function inside the exponential.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "A lot of previous work has been done on this problem, starting from Max product, which is a dynamic programming algorithm that is known to be exact on trees.",
                    "label": 0
                },
                {
                    "sent": "People have looked in vision community extensively at graph cuts which are for instance known to be exact on certain binary problems and bike off at all provide a nice summary of work in this area.",
                    "label": 0
                },
                {
                    "sent": "Related Max product is one of the popular algorithms for this problem that was proposed by Wainwright Allen further studied by Kolmogorov ET Al Ain.",
                    "label": 0
                },
                {
                    "sent": "Person and a Cola have recently proposed globally convex version of this algorithm using the idea of oriented trees convex free energy approximations have been looked at by Airways and several convex relaxations to linear programs.",
                    "label": 0
                },
                {
                    "sent": "Quadratic programs, 2nd order kodenkan programs and SDP's, etc.",
                    "label": 0
                },
                {
                    "sent": "Having looked into quite a bit of detail as well.",
                    "label": 0
                },
                {
                    "sent": "Some more recent work includes Lagrangian relaxation and simulated annealing by Johnson ET al, and building composition and subgradient methods by, DocuSign, coworkers.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are in particular concern with the 1st order relaxation for this problem in this top, which is obtained by noticing that we are dealing with a linear optimization problem here.",
                    "label": 0
                },
                {
                    "sent": "So instead of just optimizing over discrete configurations, we can optimize over the convex Hull of all the labelings and not change the value of the optimum.",
                    "label": 0
                },
                {
                    "sent": "So when we do this we end up with a set of optimization variables that are the node marginal probabilities of a given node taking a particular label and edge marginal probabilities of.",
                    "label": 0
                },
                {
                    "sent": "Node pair taking on a particular label pair and of course, these probability of a certain constraints.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, the number of these constraints is exponentially larger.",
                    "label": 0
                },
                {
                    "sent": "The problem size in general, so people use various subsets of constraints to perform this optimization efficiently.",
                    "label": 0
                },
                {
                    "sent": "One particularly well known approximation is to use the obvious nonnegativity constraint along with the constraint that the node marginal overall node labels sum up to one for every node and at the edge.",
                    "label": 0
                },
                {
                    "sent": "Marginals are consistent with node marginal, which is the marginalization constraint.",
                    "label": 0
                },
                {
                    "sent": "And this constraint set is referred to as a local polytope because of the local nature of constraints.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So at this point we have a linear program and we can invoke any of the shelf LP solving procedure like interior point methods, subgradient methods etc.",
                    "label": 1
                },
                {
                    "sent": "An we can solve the problem in principle.",
                    "label": 0
                },
                {
                    "sent": "However these methods often turn out to be too expensive or too slow for this problem and the reason is that the number of optimization variables here scales as number of edges times number of labels squared.",
                    "label": 1
                },
                {
                    "sent": "So the complexity of these methods is often prohibitive for typical problem sizes and the reason is that these methods are not.",
                    "label": 0
                },
                {
                    "sent": "Specifically designed for this problem and hence they fail to exploit the particular problem structure here.",
                    "label": 0
                },
                {
                    "sent": "Also, these methods are often hard to implement in a paralyzed distributed fashion, which is very important concern at these problem sizes prevented Max product does address some of these concerns and it solves this particular dual of this linear program.",
                    "label": 0
                },
                {
                    "sent": "However, the optimization method is particularly tailored to the structure of the local polytope and hence it's not clear that if one wants to incorporate new constraints.",
                    "label": 0
                },
                {
                    "sent": "Then how to do this in a seamless fashion so this?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're clearly looking for a map estimation algorithm that can provide us an exact solution to the LP relaxation and in a time complexity that's competitive with algorithms like Max product entry, weighted Max product.",
                    "label": 1
                },
                {
                    "sent": "We want to be able to come up with distributable and parallelizable updates.",
                    "label": 1
                },
                {
                    "sent": "So essentially we are looking at some kind of iterative methods that have look easy to obtain local computations an we want to be able to incorporate new constraints.",
                    "label": 0
                },
                {
                    "sent": "Into our algorithm without much work.",
                    "label": 0
                },
                {
                    "sent": "So to do this we need to invoke some relevant tools from optimized.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In theory, and the one that comes in particularly handy here is the principle of proximal minimization, where if you want to optimize a particular convex function F that is not very nice to optimize over directly, then you add a second to your objective, which is some kind of a generalized distance function.",
                    "label": 0
                },
                {
                    "sent": "In our case, we have a linear objective function, and we augment it with the Bregman divergences so.",
                    "label": 0
                },
                {
                    "sent": "As you can.",
                    "label": 0
                },
                {
                    "sent": "We set up a series of subproblems where at every iteration we we minimize the linear objective plus the Bregman divergences, the previous hydration, and we repeat this process until we converge, so one immediate consequence of adding this Bregman divergences that the objective is strictly convex Now, so you can go and solve this problem and the dual if you wish to.",
                    "label": 0
                },
                {
                    "sent": "The idea of adding a second function to make the objective well behaved is sort of close to any link an indeed can be.",
                    "label": 0
                },
                {
                    "sent": "This method can be equivalent to any link for certain choices of the Bregman divergences.",
                    "label": 1
                },
                {
                    "sent": "However, there is a critical difference in general, which is that when this method converges because of the fact that the second term is a Bregman divergences, it goes to 0 by itself.",
                    "label": 0
                },
                {
                    "sent": "So even if your multiplier WN.",
                    "label": 0
                },
                {
                    "sent": "Is not going to Infinity as is needed for annealing.",
                    "label": 0
                },
                {
                    "sent": "These methods will still converge to the only possible fixed point, that is the that is the optimizer of your original objective function, which makes this method somewhat different from annealing.",
                    "label": 0
                },
                {
                    "sent": "So since we have Bregman divergences involved here, the 2nd.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tool that can come in handy is that of Bregman projections and the idea here is that given any reference point we can define its projection onto a convex set as the as the point in the set that minimizes Bregman divergences.",
                    "label": 0
                },
                {
                    "sent": "To this reference point and.",
                    "label": 0
                },
                {
                    "sent": "This is uniquely defined because of the strict convexity of Bregman divergences in its first argument.",
                    "label": 0
                },
                {
                    "sent": "Now the interesting aspect of these projections is that if we have a complicated convex set which can be described as the intersection of simpler convex sets, then suppose we start with a point P that we want to project onto the intersection of these two lines.",
                    "label": 0
                },
                {
                    "sent": "Then we first project P onto the first line, take the projection projected onto the second line, and we repeat this process iteratively take.",
                    "label": 0
                },
                {
                    "sent": "Cycling over all the convex sets that feature in this intersection and it has been shown that this method is always guaranteed to converge and it converges to the projection of our reference point onto the intersection of these sets.",
                    "label": 0
                },
                {
                    "sent": "So if.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can set up our proximal minimization problem as as the minimization of a Bregman divergences.",
                    "label": 0
                },
                {
                    "sent": "Then it's conceivable that we might be able to leverage this very attractive theory in performing our optimization sufficiently so to put things into perspective.",
                    "label": 0
                },
                {
                    "sent": "We start off with a linear program that we wish to minimize.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We augmented with the proximal function.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we do a slight rewriting at this point to absorb the linear term into the Bregman divergences, and.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is done by updating the reference reference perimetre mean to mute~ N and the way you go from you to Matilda to preserve the sequence of course depends on the particular choice of Bregman divergences, so in particular, if you use an L2 distance as your Bregman divergences, then the update is very simple and just consists of adding Theta vector to your current titrate.",
                    "label": 0
                },
                {
                    "sent": "For the case of KL divergences, it is obtained as Newman Times X plus data, where product and exponentiation are.",
                    "label": 0
                },
                {
                    "sent": "Defined element wise for vectors and now we know that we are performing this optimization over a polytope, which is just an intersection of a bunch of linear constraints.",
                    "label": 0
                },
                {
                    "sent": "So we have obtained a Bregman projection problem that we want to perform over intersection of linear constraints, and if we can figure out how to project onto linear constraints easily, then we just need to invoke cyclic projections to complete the optimization problem at hand.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This gives us the basic outline of our algorithm where we start from an initial point, perform the initialization step that we just described in the previous slide using psychic predictions.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We project neutral one onto the polytope.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we repeat this process an until we converge and we are guaranteed to converge to the optimum of the linear program by approximation theory.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to give you a flavor of what these what this algorithm looks like, they initialize.",
                    "label": 0
                },
                {
                    "sent": "We give a version of the algorithm using KL divergences as the Bregman function, and so the initialization step has already been mentioned in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "For this particular choice projection onto normalization constraint is really simple, it just amounts to dividing every node marginal to the by the sum of node marginal overall.",
                    "label": 0
                },
                {
                    "sent": "Labels for that node for the edge marginalization constraint.",
                    "label": 0
                },
                {
                    "sent": "The update has been shown here and if you are working in the message passing then you will see the immediate resemblance of this to the belief propagation updates, and this is encouraging because this means that our method will indeed inherit all the good properties of belief propagation like it's local nature, which makes it easy to implement in a very distributed fashion and the iterative nature which allows.",
                    "label": 0
                },
                {
                    "sent": "Parallelized implementations, and so on and similar derivations are possible with a bunch of other divergences, and we have worked out a couple of examples in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to go back to our set of desired properties, it's obvious at this point that we have an algorithm that is exactly solving the LP relaxation.",
                    "label": 0
                },
                {
                    "sent": "By approximation theory we we have updates that are easy to implement in a parallelized and distributed fashion because of the similarity to belief propagation, and it's easy to incorporate new constraints because you just need to compute the projection onto individual constraints, which is usually not too hard.",
                    "label": 1
                },
                {
                    "sent": "The only point that's not clear immediately is what is the time complexity of these schemes?",
                    "label": 0
                },
                {
                    "sent": "An if if the linear programming relaxation is inexact, that is, the LP has a non integral solution.",
                    "label": 0
                },
                {
                    "sent": "Then indeed these methods can take awhile to converge.",
                    "label": 0
                },
                {
                    "sent": "However, the interesting cases when the LP is indeed integral.",
                    "label": 1
                },
                {
                    "sent": "In this case we can use very effective rounding schemes to obtain a solution in time complexity competitive with Max product and related Max product so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do rounding schemes come in?",
                    "label": 0
                },
                {
                    "sent": "Well, we need to recall that we are interested in solving an integer linear program here and find the Max maximum probability labeling.",
                    "label": 0
                },
                {
                    "sent": "So although we are solving the relaxation if at any point we take our set of marginals at that point and we can round them to produce labeling for the nodes, that is guaranteed to be optimal, then we're done.",
                    "label": 0
                },
                {
                    "sent": "The problem is solved, so the rounding schemes aimed to do exactly this an once one such.",
                    "label": 0
                },
                {
                    "sent": "The instance of a rounding scheme is to just find the local optimal label for every node from your current estimate of node marginals, but such a naive scheme is bound to fail.",
                    "label": 0
                },
                {
                    "sent": "An literature is bound with examples where this scheme will not work.",
                    "label": 0
                },
                {
                    "sent": "So we moved to slightly more higher order.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rounding, which involves edges essentially so.",
                    "label": 0
                },
                {
                    "sent": "The idea is that you associate a local quantity mu Baron with every edge that that involves.",
                    "label": 0
                },
                {
                    "sent": "That is a function of the node and edge marginals at that point.",
                    "label": 0
                },
                {
                    "sent": "So Mu Barron is defined as muse to the one over degree of is times muti to the one over degree of tee times the edge marginal, and this is a local quantity that involves an edge and both the vertices.",
                    "label": 0
                },
                {
                    "sent": "Now you find the label pair that is optimal for this local quantity and.",
                    "label": 0
                },
                {
                    "sent": "If if these these label pairs are found to be consistent across all edges, So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "Suppose you look, you find the optimizing label pairs for two edges, and you look at the value they produce on a node that is shared between them.",
                    "label": 0
                },
                {
                    "sent": "Then this label should be consistent.",
                    "label": 0
                },
                {
                    "sent": "So if this consistency holds across all edges in the graph, then you declare that you have found a map labeling.",
                    "label": 0
                },
                {
                    "sent": "You have found the solution to the integer linear program.",
                    "label": 0
                },
                {
                    "sent": "And you stop.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, you can fail to find such a consistency, and in that case you just say that I haven't found the solution at yet and you continue your iterations.",
                    "label": 0
                },
                {
                    "sent": "From areas we can also move to.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other higher order neighborhoods like Stars, which is a node and all its neighbors, we can again do the same scheme that we find the local optimal configuration for this star Ann.",
                    "label": 0
                },
                {
                    "sent": "If these configurations are consistent across All Stars, then we declare optimality and terminate.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can move on to trees, repeat the same argument.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So wild and I rounding scheme does not enjoy any guarantees in general, the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The theorem that we can show is that at any iteration, if any of the edge star or tree rounding schemes converge, an find a consistent labeling across all neighborhoods of a particular kind, then that configuration is the map configuration indeed, so so they have this strong.",
                    "label": 0
                },
                {
                    "sent": "These rounding schemes have this strong guarantee backing them an this showing.",
                    "label": 0
                },
                {
                    "sent": "These essentially involves some invariants that are maintained by our algorithm.",
                    "label": 0
                },
                {
                    "sent": "At all iterations.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Moreover, once we you start using these rounding schemes, it usually takes us 8 to 10 rounds worth of privated Max product time to converge by rounding for an integral LP.",
                    "label": 0
                },
                {
                    "sent": "So you have to incur some some additional computational cost for solving the LP exactly, but I think it's not, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's still in the same bowl.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Park.",
                    "label": 0
                },
                {
                    "sent": "So to go to rates of convergence, it has been shown for proximal minimization methods that they have a super linear rate of convergence for a wide class of Bregman divergences, assuming that the proximal subproblem at every step is solved exactly.",
                    "label": 0
                },
                {
                    "sent": "Now of course this is not possible because at every step you terminate with some finite tolerances.",
                    "label": 0
                },
                {
                    "sent": "So the inner problem is only being solved in exactly what is nice.",
                    "label": 0
                },
                {
                    "sent": "However, that is that guarantees exist that say that if there is a procedure for solving this inner loop in exactly that has a linear rate of convergence, then the overall proximal method will still be consistent.",
                    "label": 0
                },
                {
                    "sent": "If the errors don't accumulate in a bad way, Ann will have an overall linear rate of convergence.",
                    "label": 0
                },
                {
                    "sent": "An cyclic Bregman projections do meet this constraint they have they have a linear rate of convergence, which means that even in the exact set of our method does have an overall linear rate of convergence.",
                    "label": 0
                },
                {
                    "sent": "At least an we will again like to emphasize that for the case of Integral LP's we have a much faster rate of convergence using the rounding schemes, and this is not just an empirical statement, but you can actually show that in theory that this the convergence happens really fast using rounding.",
                    "label": 0
                },
                {
                    "sent": "By analyzing certain sufficient conditions for the rounding schemes to work.",
                    "label": 0
                },
                {
                    "sent": "So this is not currently included in the paper, but is being written up at the moment.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to verify our experimental performance of our algorithms, we looked at grid graphs between 100 to 900 nodes and with five node labels.",
                    "label": 0
                },
                {
                    "sent": "We experimented with the parts potential with varying SNR, which is the ratio of the strength of node potential to that of edge potential.",
                    "label": 0
                },
                {
                    "sent": "So the low SNR is a harder regime than a higher SNR.",
                    "label": 0
                },
                {
                    "sent": "In this problem, we wanted to empirically verify are super linear rates of convergence and the effectiveness of our rounding schemes.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So further rates of convergence, we plotted the log of.",
                    "label": 0
                },
                {
                    "sent": "L2 distance of our marginal vector at every iteration to the optimal.",
                    "label": 0
                },
                {
                    "sent": "Optimal solution as a function of the number of iterations, and we did this for three different problem sizes and here the plot is shown for a representative value of SNR and it's easily seen that the rate of convergence is super linear.",
                    "label": 0
                },
                {
                    "sent": "If it were just linear, then we would have obtained straight lines.",
                    "label": 0
                },
                {
                    "sent": "In the log plot.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For testing the effectiveness of rounding schemes, we plotted the number of the fraction of edges that violate the edge consistency constraint in the edge rounding scheme.",
                    "label": 0
                },
                {
                    "sent": "Again, as a function of number of iterations, and again for variety of problem sizes, and you can see that this comes down really fast for the case of Integral LP's and what's even more impressive is that it does not scale too badly with the.",
                    "label": 0
                },
                {
                    "sent": "With the increase in problem size.",
                    "label": 0
                },
                {
                    "sent": "So this means that our methods really have a very good scalability property.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also wanted to look at the energy of the rounded solution cause energy in some sense is the the.",
                    "label": 1
                },
                {
                    "sent": "Value of the objective function in this case, which you are.",
                    "label": 0
                },
                {
                    "sent": "Which is the main quantity of interest anyways.",
                    "label": 0
                },
                {
                    "sent": "So we plotted the ratio of the energy of rounded solution to the energy of the true solution, and again it can be seen that this ratio approaches one really fast and it does so without and this does not change with an increase in problem size in a significant way.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to sum up, we have proposed a new class of map estimation algorithms that use the approximal minimization framework, an using graph structured Bregman driver divergences.",
                    "label": 0
                },
                {
                    "sent": "We can exploit the problem structure very effectively, and the cyclic Bregman projections allow us to exploit the simple structure of the local local constraint pollito pan solve this problem in a very efficient manner.",
                    "label": 0
                },
                {
                    "sent": "We derive simple message passing updates that have a guarantee of convergence to the LP Optimum.",
                    "label": 1
                },
                {
                    "sent": "And we would like to emphasize this is different.",
                    "label": 0
                },
                {
                    "sent": "This is different from some of the other message passing based methods which were able to provide some partial guarantees when the LP is integral, but we're completely in dark when the LP relaxation is inexact, so we have a guarantee of convergence to LP optimum even in that case an we show rounding schemes that allow us to come up with extremely fast solution for the case of Integral LP's.",
                    "label": 0
                },
                {
                    "sent": "Thanks a lot for your attention.",
                    "label": 0
                },
                {
                    "sent": "So how does this compare?",
                    "label": 0
                },
                {
                    "sent": "So as we said, for the case of Integral LP's, it usually takes us like so each iteration of the each solving each proxamol problem is almost as costly as doing a tree weighted minimization, and we take a 210 iterations empirically, so that's a good estimate of the cost of this method.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So we we the experiments that are shown here are with KL divergences.",
                    "label": 0
                },
                {
                    "sent": "We also experimented with L2 distance and some more.",
                    "label": 0
                },
                {
                    "sent": "L2 distance was not that effective.",
                    "label": 0
                },
                {
                    "sent": "Kill was empirically much better than L2 an there are there are more interesting divergences that can be used but are hard to describe in this talk.",
                    "label": 0
                },
                {
                    "sent": "When you do, the projection gets positive.",
                    "label": 0
                },
                {
                    "sent": "Don't get around.",
                    "label": 0
                },
                {
                    "sent": "You're likely to meet their version control.",
                    "label": 0
                },
                {
                    "sent": "L2 actually does not hit the vertices of the polytope.",
                    "label": 1
                },
                {
                    "sent": "The projections are the projections.",
                    "label": 0
                },
                {
                    "sent": "So first of all, cyclic projections takes a long time to converge when you are using L2, so each each subproblem solution becomes very inefficient using L2.",
                    "label": 0
                },
                {
                    "sent": "Cal Divergance converges really fast in that sense.",
                    "label": 0
                },
                {
                    "sent": "For the inner loop.",
                    "label": 0
                },
                {
                    "sent": "We just use like numerical tolerance.",
                    "label": 0
                },
                {
                    "sent": "Once all the constraints are so the thing with Bregman projections is that the first time you hit us feasible point it will be the correct feasible point.",
                    "label": 0
                },
                {
                    "sent": "It will be the projection.",
                    "label": 0
                },
                {
                    "sent": "So we just check constraint violation and once it is below us numerical tolerance we.",
                    "label": 0
                },
                {
                    "sent": "For 2300, it actually varies, so at the at the start it often takes more passes an as your proxamol problem is nearing convergence.",
                    "label": 0
                },
                {
                    "sent": "Even the inner loop speed up.",
                    "label": 0
                },
                {
                    "sent": "No, no we we have.",
                    "label": 0
                },
                {
                    "sent": "In fact we have experimented even with overcomplete potentials mixed mixed over complete.",
                    "label": 0
                },
                {
                    "sent": "But we just included parts because more more papers have.",
                    "label": 0
                },
                {
                    "sent": "Yeah, no, no parts again mix, so yeah.",
                    "label": 0
                },
                {
                    "sent": "753 yes.",
                    "label": 0
                },
                {
                    "sent": "I mean, we haven't.",
                    "label": 0
                },
                {
                    "sent": "We haven't empirically compared our method with that, but.",
                    "label": 0
                },
                {
                    "sent": "It is possible that in certain cases at least, the two might be exactly equivalent, and but in general this could look pretty different, I think.",
                    "label": 0
                },
                {
                    "sent": "Becausw because this is, I mean, because this is a Bregman divergences and not just the entropy of us of your current solution, so this.",
                    "label": 0
                },
                {
                    "sent": "This makes the algorithm look a bit different.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, so cyclic projections as they see sequential procedure.",
                    "label": 0
                },
                {
                    "sent": "So how do you parallelize it?",
                    "label": 0
                },
                {
                    "sent": "So once.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once you have these.",
                    "label": 0
                },
                {
                    "sent": "These updates you can.",
                    "label": 0
                },
                {
                    "sent": "In parallelize these across edges like what one edge is doing, does not have to coordinate with what another edge is doing, so you can use that to run.",
                    "label": 0
                },
                {
                    "sent": "No, that's the whole.",
                    "label": 0
                },
                {
                    "sent": "That's the whole thing, right?",
                    "label": 0
                },
                {
                    "sent": "The local polytope has just constraints involving nodes and edges.",
                    "label": 0
                },
                {
                    "sent": "If you use, you use more global constraints, then you will get more global updates and then parallelization would.",
                    "label": 0
                },
                {
                    "sent": "As you as you make your constraints more and more global, you have to look at higher order subgraphs of some kind essentially.",
                    "label": 0
                },
                {
                    "sent": "But here we are dealing with the local polytope, so.",
                    "label": 0
                }
            ]
        }
    }
}