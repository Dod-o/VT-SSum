{
    "id": "mzkuhj6w7o5fmhskdquhfvfs4spebgcy",
    "title": "Learnable Representations for Natural Language",
    "info": {
        "author": [
            "Alexander Clark, Royal Holloway, University of London"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_clark_lrnl/",
    "segmentation": [
        [
            "OK, so first of all, this is obviously nobody works on their own and this is joint work with a number of people, especially a Frank Taylor, Vimeo and Ameri Abra."
        ],
        [
            "So what I'm going to talk about here, I'm going to present some algorithms for unsupervised learning of representations for natural languages, especially for natural language syntax.",
            "And these arguments are efficient in the sense that they're sort of formal sense that they use polynomial amounts of computation.",
            "And they're also efficient in the sense that you know you can.",
            "Write the code and run it and it it runs very rapidly.",
            "Um?",
            "And we get these results essentially by using new representations that are designed to be learnable.",
            "The base, broadly speaking, on distributional learning, so ideas of distribution analysis from structural linguistics.",
            "People like Zellig, Harris, and.",
            "Eastern European researchers that followed his work, and they empiricist in a sense that the representation is basically determined by the data.",
            "And we get representations for richly structured context sensitive languages and the class of languages we get from this seems to be quite a good match to the class of natural languages.",
            "And the more advanced representations are based on lattice theory, particularly resituated lattices.",
            "And one slightly shocking thing for a NIPS audiences that I'm using kind of a symbolic learning paradigm.",
            "So rather assuming that we have a. Heretical, so rather than using a distribution over strings, I'm assuming they were given.",
            "Rather, using a distribution over strings, I'm assuming that we have some sort of formal language collection of grammatical strings, and I'm going to be using slightly implausible learning models that use membership oracles and so on.",
            "This is not because I hate statistical modeling, I think, obviously statistical modeling is is a much more realistic paradigm, but you can, I think aren't out some of the wrinkles you can get some of the conceptual problems out of the way by focusing in this sort of rather simplified model.",
            "I'm.",
            "So."
        ],
        [
            "What am I going to talk about?",
            "Well I'ma start off talking about methodology.",
            "If you kind of accept the methodology then the rest of it follows quite naturally.",
            "But methodology may be a sticking point for some people.",
            "I will look at regular languages Becausw.",
            "The illustrated, quite the other straighten methodology quite well and we all understand regulations.",
            "We have very good intuitions about regular languages, and it's also a case where the representation class we get through this method corresponds to a natural representation class, the Chomsky hierarchy.",
            "But I'm going to look at learning some context free languages, and then we're going to look at two more novel representations.",
            "Contextual binary feature grammars and lattice representations, which are more context sensitive formalism.",
            "And then I'll conclude.",
            "So methodology."
        ],
        [
            "So we're here.",
            "I guess we're all in this room because we're interested in representations for natural language.",
            "And there are, I guess, two motivations.",
            "One is a scientific goal with sort of cognitive science goal of trying to understand how language is processed in humans.",
            "And the second is an engineering goal.",
            "We want to build language processing systems that actually work, and the current approach is broadly based on supervised learning from annotated data.",
            "And it's not really an adequate solution to the problem.",
            "It's not a very good solution to engineering goal because we only have limited amounts of this data for some languages, and it's certainly not a solution to the scientific goal.",
            "Becausw children aren't provided with parse trees."
        ],
        [
            "So obviously, like a lot of people have been interested in looking unsupervised learning.",
            "And how should we approach this?",
            "The sort of study of learnability?",
            "Well, one approach, the normal approach.",
            "I guess it takes an existing class of representations like context free grammars, tree adjoining grammars or something more fancy.",
            "And try to design learning algorithms for that class.",
            "And this is a I spent a few years in the trenches doing this myself.",
            "I think it's ultimately I think it's it's misconceived.",
            "Having you know, having spent several years doing this myself, I feel I can say that."
        ],
        [
            "And I think there are two problems.",
            "The first problem is that we don't really know what the representations are.",
            "So linguists often talk as though we know what these tree structures are, but in fact they can't agree about anything.",
            "And the reason they can't agree about anything is as Jim Blevins took it.",
            "Put it in a recent article in general linguistics at the most fundamental level did not click.",
            "There is any meaningful empirical motivation for the representation assumptions of any current form model syntax, which is pretty harsh, but I think it's true.",
            "I think that if you look at the justifications for the empirical representation, there very, very weak and the second problem is that learning these standard representations is computationally extremely hard.",
            "Even when you assume a quite benign learning model, like in this Anglin and version of paper learning, even things like nondeterministic finite state automotor, which is roughly equivalent to regular grammars, the bottom level of the Chomsky hierarchy is really hard.",
            "Computationally, so how to move?"
        ],
        [
            "Forward.",
            "Well, yeah.",
            "In first language acquisition, we don't know what the representations are, but we do know that they are learnable.",
            "OK, so it seems like this approach of picking some representations and then trying to learn them is completely the wrong way around.",
            "What we should do and putting as a slogan is to put learnability first.",
            "We should be prepared to look for new representations.",
            "We should insist that they are learnable.",
            "OK, we should learn ability is kind of unusual property.",
            "You're not going to get it by accident.",
            "You're only going to get it if you design it in from the beginning."
        ],
        [
            "And so, how do we put this into practice?",
            "Well, normally what we do is we have a function from the representation to the language.",
            "So given some context free grammar G, we define their derivation process and so on, and we will define a language L. So this is a function from the representation to the language given a non terminal we can define the set of strings derived from that non terminal.",
            "We need to flip this around and go in."
        ],
        [
            "The other direction we need to focus on looking to function from the language given the language, how do we construct a representation from that language?",
            "So given a language L, we want to have a function which gives us the representation for that language from a set of strings to some representational primitive of the formulas, and whether it's a state or nonterminal symbol or something like that.",
            "And again, putting this again as a slogan, I'm going to move on from slogans to technical content shortly, so it's not entirely.",
            "But the structural representation should be based on the structure of language and not something arbitrarily imposing it from outside.",
            "OK, so if you're going to have a learnable representation, you should first identify some structure in the language.",
            "Show how that structure can be observed, and then we construct a representation based on that structure.",
            "And if you get a richer structure, then you're going to get a richer and more powerful expressive representation.",
            "OK, so how do we actually do this?"
        ],
        [
            "Well, I'll illustrate this with example from regular languages.",
            "So regular language is kind of one of the few success stories of grammatical inference of learning.",
            "We define our regular language by a deterministic finite automation which has a finite set of States and some transition function.",
            "And for each state here we define the language generator from that state just to be in in the standard way.",
            "And the language defined by the automation is a language that you derive from the initial state.",
            "Now I'm.",
            "An elementary but very important result.",
            "Is it my whole narrow term where if we define a right congruence here, where we say that two strings U&V are congruent if and only if they have exactly the same set of suffixes?",
            "I mean sort of residual language is equal.",
            "And we can write them.",
            "You are here for the congruence class, in particular, string.",
            "Now my hero theorem says the number of states in the minimal DFA that generates language is exactly equal to the number of equivalence classes under this relation.",
            "So we have some structure in the language here, which is this this congruence structure here.",
            "This set of Congress classes.",
            "And we can base gives of this, we can tell whether two strings are congruent just by taking a large sample and comparing the suffixes of you with the suffixes RV in oversea depends on your language model.",
            "And there might not always always be.",
            "Be possible to do this with great accuracy, but in principle this is an observable property of the language.",
            "Hum.",
            "So."
        ],
        [
            "So let's say you have just some trivial, trivial regular language like a beast our Lambda.",
            "Here's the empty string and we put some prefixes.",
            "Here we have a set of suffixes here, and we can see that these strings here fall into 3 Congress courses.",
            "Which.",
            "Three consequences here.",
            "Here we have a class of congruence of strings that don't occur as prefixes or language.",
            "So we get these three.",
            "We can observe these three.",
            "These three Congress courses.",
            "And now."
        ],
        [
            "We can do is we can just define the states of our automation to be these congruence classes.",
            "And the key point.",
            "It's kind of an elementary point, but it's quite important is that if we have.",
            "If you have the same set of suffixes V, then UA.",
            "Has the same social services via.",
            "That's because it's a right congruence.",
            "And what this means is we can define a transition from the state U to the state UA.",
            "OK, so this is kind of this seems entirely entirely vacuous.",
            "But let's say we take some element of XAA.",
            "We append B, we get a B which is equal to the Chronicles of Lambda.",
            "So this gives us transition from the state A to this other state there.",
            "OK, So what we've done here is we've defined a.",
            "Hey, some structure in that we recognize identified some structure in language and we've based representation."
        ],
        [
            "Entirely on that, there's a correspondence between.",
            "The representational primitives of our model.",
            "Which of these states?",
            "And some property of the language.",
            "These right congruence classes OK, and we can use this to define a Canonical deterministic finite automation."
        ],
        [
            "And this gives us some standard results for learning deterministic finite automatron probabilistic deterministic finite automata, which may be the most important for anguineus paper.",
            "From 1982, Jose on scene as paper from 92 and Donna Ron's paper on PAC Learning Acyclic PDF's.",
            "So we now have a.",
            "This is a success story of matter.",
            "We now have good effective algorithms for learning.",
            "Regular automater I mean obviously it doesn't work for everything.",
            "There are some limitations.",
            "It relies on frequent suffixes, Co occurring with frequent prefixes, and so this won't work if you have some sort of uniform distribution over an bitstrings or something like that.",
            "And you can construct some.",
            "There's some hardness results that rule out learning in those circumstances.",
            "OK, so we have this success for regular languages.",
            "How do we move?"
        ],
        [
            "From these regular languages onto context, free languages and beyond.",
            "Um?"
        ],
        [
            "So we need to do is rather than moving rather than looking at the relationship between prefixes and suffixes, we're going to look at the relationship between contexts and substrings, so a context in this case is just going to be a pair of strings.",
            "LR and we combine a context with the substring just by wrapping it, wrapping it together.",
            "And so the distribution of a string here.",
            "And this is distribution in the linguistic sense.",
            "Rather distribution.",
            "The statistical sense is just the set of all contexts LR that can occur with a particular particular string, and this gives us a syntactic congruence, which is a fairly standard bit of language theory, which we say that two strings U&V are congruent for each other if and only if they have the same distribution and we're going to write the square brackets you for the congruence class.",
            "Of you.",
            "So distributional learning, which is used quite extensively in natural language processing, essentially models these.",
            "The distributions of strings an if you use us to do.",
            "Unsupervised learning of parts of speech classes and things like this.",
            "It works extremely well, and in fact the Brown Brown at our results that we had in the previous talk are based precisely on doing some clustering of these distributions.",
            "OK."
        ],
        [
            "So again, this is an observable structure, right?",
            "We can more or less tell whether you is congruent to V. We just take a whole bunch of data, look at the observed context of you, and compare those observed contexts of V. Now the key point here is that if you is congruent to U prime and V is congruent to V primed, then UV is congruent to U prime V prime.",
            "Again, it's very trivial point of this is that if we identify our congruence copses with non terminals, if we create a context free grammar and we say that the non terminals are context free grammar are going to be exactly the congruence classes of the language.",
            "Then it turns out that we can write down rules of this form here, where UV goes to UV.",
            "OK, So what this means at this point means is we pick any element of this congruence class and combine it with any element of this congruence class.",
            "We're going to get an element of this congruence class.",
            "OK, so the context free production here exactly matches the algebraic structure of the language.",
            "In particular this.",
            "What is called the syntactic more like, which is kind of for those that are algebraically inclined.",
            "It's the you know the quotient of the Mona.",
            "I'd buy this equivalence relation."
        ],
        [
            "So let's take a trivial example here.",
            "You're kind of very standard example of context free language, then beat.",
            "Then we have a finite set of congruence classes.",
            "We select a finite set of comments classes.",
            "It's worth pointing out the number of congruence classes is going to be finite if and only if the language is regular.",
            "So in this case they're going to be infinite number of Congress classes.",
            "We take some finite set of them that look like this, and then we can just write down."
        ],
        [
            "Rules.",
            "We gotta have some trivial rules like this.",
            "Saying the conference call so they can be rewritten as a an we have some.",
            "Rules like this AB goes to ABABB goes to ABB.",
            "These are all kind of look, sort of vacuous.",
            "Then we had some ones that look less vacuous, since AB is exactly the same as a ABB.",
            "Since AP is congruent to ABB.",
            "If we taken a this conference calls with a in the current class ABB, and we combine them, we will get a congruence class of a B. OK, so in this case if we just take this finite set of these small set of these Congress classes.",
            "Would you define distributionally?",
            "We can simply write down an appropriate context free grammar by using the this structure.",
            "Yes, just information.",
            "So did you say that even if the language is context free, there's an infinite number of congruence classes, so the.",
            "The number of congruence classes of a language is finite if and only if the language is regular.",
            "So a language like this which is not regular will have an infinite number of Congress passes, and you can see why that is, because you're going to have Congress classes like AAAAAA to the K is going to have different congruence class from 8K plus one.",
            "Number of context free rules, but you get you.",
            "You only need to have a finite number of a finite subset of them for a certain class of context free languages.",
            "So not all context free languages can be represented through a finite set of congruence classes.",
            "OK, is there any way you could tell us how to find out?",
            "Well, it's I I'm I'm moving on from this week and there is an interesting language class which is defined by those ones that are finding it slightly too small.",
            "So I'll talk about.",
            "Delayed."
        ],
        [
            "OK, so.",
            "So this gives you some basic results for learning context free grammar, so the most trivial result, which is kind of the first one, is to call the substitutable languages, which is if you assume that if the distribution whenever you have two strings, you envy which have a nonempty intersection of the distribution.",
            "Or you can find one context that lies that accepts both of these substrings, then there syntactically congruent OK.",
            "This is clearly a completely false for natural languages.",
            "That enables you to get some kind of elementary results for these algorithms.",
            "And what's kind of surprising about?",
            "I mean this paper, what's the the really interesting thing about this paper, which I'm a Co author of, is the is the year right?",
            "And this paper didn't appear until well, another version of here couple years earlier, but this paper really should be written about 1985.",
            "OK, and this is the kind of embarrassing thing about this.",
            "This whole field.",
            "And then following from this we have some results for PAC Learning on some other variants of it, OK?"
        ],
        [
            "The real problem though is that this idea of having one symbol per congruence class just isn't going to work for natural languages.",
            "It'll work fine for these sort of simple, trivial examples like ATM BTN, but it's just not going to work for natural languages, and that's the cause.",
            "The number of congruence that you have a very many, and they're often very close to each other.",
            "The learning model here by treating each congruence class as a separate atomic symbol.",
            "Assumes that either these Congress costs are identical.",
            "They're completely unrelated.",
            "We clearly need to have a much more powerful representation that can represent, in some sense of structure.",
            "These Congress passes, and Furthermore, languages are context free.",
            "So just give a kind of."
        ],
        [
            "Pretty simple example.",
            "In a context free grammar for natural language, you might have something like.",
            "A noun phrase consists of a determiner, followed by now.",
            "Now if you put some determiners here and you put some nouns here, you find it's not true that you can take any determine and combine it with any noun, and you'll get a grammatical noun phrase, because there is.",
            "There are singular and plural nouns.",
            "There're count noun mass noun distinctions, the alternation of the definite indefinite article depends on the phone.",
            "A logical properties of the start of the noun.",
            "And in fact, if you look at this, you see that."
        ],
        [
            "The determiner.",
            "An every noun is in a different congruence class here, so you need to have a different rule for each combination.",
            "And so clearly that's impractical.",
            "So we need to come up with some slightly richer structure that will enable us to to learn this sort of stuff.",
            "So how are we going to do that?"
        ],
        [
            "So the so the key point is really that to recognize that the distribution of a string you is a set of contexts.",
            "And since it's a set, the appropriate algebraic structure to represent it where it is as a lattice, and the first person to.",
            "To discover this was a Frenchman called Sestiere in 1960.",
            "As far as I can tell, he only wrote one paper, but it's quite a good paper, so it's a it's a it's a success rate.",
            "We should all we should all aspire to.",
            "The basic idea here is that linguistically, if you just take a simple, simple linguistic example here, so you take a personal pronoun her, well, that could be.",
            "I gave her a cake or I stole her camera and so if you look at the distribution of of her, it's basically going to be the Union of the distribution of his and the distribution of him.",
            "OK, so in order to start to get a more linguistically nuanced model, we need to have operations that correspond to these sort of unions and intersections.",
            "And that means we need to start using a lattice."
        ],
        [
            "And we can do this really by looking at contextual features.",
            "So what we do is we pick some finite set of of K context.",
            "So again, context is just a pair of strings, so we have some set finite set of these F and we're going to represent the distribution of a string.",
            "Just buy a subset of elements of F so we can 'cause there is a subset or just a binary vector of length K. And this gives us two Takei possible congruence classes.",
            "Yeah, go back.",
            "Yeah, so so represented by the way, by what remains up you take that or whether or not that intersection is empty or not empty by the set of elements here by this set here.",
            "So this is a subset of F that consists of the set of features contextual features that can occur with a particular substring, OK?",
            "So."
        ],
        [
            "We had this lattice structure going to talk in a bit more detail about the lattice structure later on, but the key point here is that if we have any strings you you prime V prime.",
            "If the distribution of you contains the distribution of you prime and the distribution of the contains the distribution of the prime, then the distribution of UV contains the distribution of your prime view prompt.",
            "OK, so again, this gives us a way where we can compute.",
            "An estimate of some the distribution of a string based on the distribution of its parts.",
            "We can recursively compute an estimate of the distribution based on the distributions of its substrings."
        ],
        [
            "So if we just rewrite this in a much less penetrable form, we get this sort of thing.",
            "We can say that if we have some string W and we're interested in finding out what its context is, we know it's going to contain.",
            "If we split the W into any different combination of U&V such that you envy concatenated W and we have some finite set K of strings.",
            "If we take the union of all of these, you primes which are contained in U&V, and we take the Union of the context of you prime V prime, and we know that this is going to be less so.",
            "This gives us.",
            "Essentially a.",
            "A lower bound.",
            "On the distribution of these things, which we can recursively compute.",
            "So this is the basis of our representation.",
            "We just turn this directly into a representation kayworth signal start, then that would be a quality yes."
        ],
        [
            "So how does this work?",
            "So here we'll just do some some lattice diagrams here.",
            "Again, I'm slightly vague way at the moment.",
            "So if we have some unv and we know that you is below you primed and veers below V primed, then we know that UV is going to be below you from the prompt, but that's kind of trivial, will slightly."
        ],
        [
            "Esther is that we can split it in more than one different way.",
            "So if we have some string here, UVW we can either split into UV&W and we know the UV is below X&W is blue?",
            "Why so UVW has to be below XY or we can split it the other way?",
            "In which case we know that you VW is gonna Bill Opq.",
            "OK, so this.",
            "Representation is fundamentally non context free.",
            "OK Becausw we can combine the results from different different derivations together.",
            "Sophie."
        ],
        [
            "Normally we take a.",
            "We'll call this contextual binary feature grammar.",
            "So we have F is our set of features.",
            "We're going to have one special element of of F, which is just this sort of empty feature, where Lambda is the empty string.",
            "So clearly a string will have this feature if and only if it's in the language and we have some productions we have.",
            "So productions that introduced the set of features for one of our atomic symbols and we also have productions that combine.",
            "Our features together so XY zed here are subsets of F, so informing this production like this says that if we have you has features Y&V has features Ed then when we stick together we know that you and view the is going to have a set of features X.",
            "And we can.",
            "Features here contexts.",
            "OK, so there are obviously more sophisticated ways of defining these features, but we're going to define would take the most basic one where one context equals one feature.",
            "One might want to have cases where features correspond to sets of contexts, so we define some."
        ],
        [
            "Recursive feature map here.",
            "Where we for if if we have a an atomic symbol string of length one, we just look it up in our lexicon, effectively an.",
            "Otherwise we just recursively compute it using this this.",
            "Just applying the rules."
        ],
        [
            "And this gives you is very similar to a CKY parsing algorithm, which we can do in in cubic time.",
            "OK."
        ],
        [
            "So, um.",
            "So the.",
            "I'll skip this like misleading Hank."
        ],
        [
            "So, um.",
            "So let's suppose we have some finite set of strings K in a finite set of features F, and we have very helpfully membership Oracle which you can use to query or some very very large data source.",
            "So if we have UV and UV Erin K, then we can know that CNU and CV combined to form C of UV.",
            "So we just add a production.",
            "Um?",
            "Which is based precisely on these this observed combination.",
            "So we observe two strings U&V.",
            "You combine in a certain way so we put together rules saying that whenever we have a string which has the set of features that you have, and a string that has a set of features that we have, we can combine those to produce a string which will have the set of features that UV has.",
            "And the lexer productions.",
            "We just say, well, A has a set of features that A has.",
            "Um?",
            "So given this, given the survey, we can just essentially write down.",
            "There's no computation involved.",
            "If we're estimating it statistically, then there is some nontrivial computation, but in this symbolic learning paradigm, there's no computation at all.",
            "We just write it down."
        ],
        [
            "So given this article and a choice McCain F, we can write down a grammar KLF The question is, is it easy to find the right values of K&F or is it a hard such problem?",
            "We just substituted one hard problem for another one, which it turns out to be very easy to find suitable K&F."
        ],
        [
            "So first of all, as K increases, as we increase the set of the set of strings K, the language increases.",
            "OK, so if we have.",
            "K is a set of strings.",
            "We increase it slightly to have K plus.",
            "Then the language defined by KLF is a subset of the language defined by K plus LF.",
            "OK, that's kind of obvious.",
            "As as we increase the number of examples, the set of rules that we get increases."
        ],
        [
            "Well, slightly less obvious is that.",
            "The is the monotonicity of F as we increase Fr, set of features are set of contexts, the language actually decreases monotonically.",
            "And this is because really the features define the conditions under which we predict a feature.",
            "So if you has features, Y&V has features Ed, then we predict the UV has some features X.",
            "If we increase the set of features that are going to be increasing Y&Z, and so we're going to be decreasing the number of times that this rule applies.",
            "OK so as we increase the set of context, the language monotonically decreases.",
            "Um?"
        ],
        [
            "So here's a simple example here.",
            "This is the.",
            "We have a just a standard trivial example we have.",
            "Here we increase our set of K just to be the most frequently occurring substrings.",
            "Here we have the most frequently training contexts we see here.",
            "The this is the over generalization where white means it doesn't overgeneralize.",
            "So here as we increase the set of features, I as we go vertically up here, we see the over generalization decreases monotonically.",
            "And converse."
        ],
        [
            "If we look at the under generalization as we increase the set of strings that we use, the under generation decreases monotonically.",
            "Yeah, so you get out books.",
            "You get a diagonal wedge I'll.",
            "I'll give a example it."
        ],
        [
            "So algorithm.",
            "Not, I'm not interested in trying to prove really practical algorithms, I'm just trying to get a simple algorithms that are going to be probably correct.",
            "So basically, if your language under generates you add some strings to your your K, your set of kernels, which means you go to the right if the language over generates you add some contexts and you go up so."
        ],
        [
            "Here's a diagram here.",
            "Use or if you're if you know if you're if you're not generating something, you move over to the right.",
            "If you're generating things you shouldn't, you move up and eventually you're going to get into this correct area.",
            "Um?"
        ],
        [
            "So what are the classes of languages which this works so it doesn't work for all context free languages?",
            "OK, so one key point here is the the finite context property.",
            "So if you have a finite set of context, you such that for any string V, if F of you is a subset of the distribution of Y, then see if you as a subset of COV.",
            "OK, So what you're saying really is string has the language if you can.",
            "If you can say that every string that contains this finite set of context will contain all the other contexts, and it's a kind of quite a natural."
        ],
        [
            "It's quite a natural."
        ],
        [
            "Hypothesis it's quite a natural language theoretic property, and so here's some some sort of technical detail.",
            "What we basically we define this notion of fiducials, where we have a status of features, is sufficiently strong sufficiently big to represent a particular set of of strings K. And then what we can prove?",
            "Basically the key lemma is that if this set is large enough.",
            "Then this the binary feature game is going only going to predict correct features.",
            "OK, so that means that the set of features that we predict is going to be a subset of the true features, OK?",
            "So in the infinite limit when F, if we allow ourselves to have infinite sets of features, then this is always going to be correct."
        ],
        [
            "So all regular languages have this property.",
            "There are some context free language that don't have the finance context property.",
            "This is quite an important example here, so this is a simple example.",
            "I could come up with of context free grammar, context free language that doesn't have this property and you just don't get this sort of phenomenon in natural languages.",
            "So I think it's quite a plausible hypothesis that natural languages have the finite context property.",
            "Most programming languages can find context operated well.",
            "Yes, thank they all do I mean, insofar as their context, yeah.",
            "I mean there's some debate about exactly how you define the set of well formed strings of a programming language.",
            "But yeah, they normally do.",
            "And let me just.",
            "I'll skip on."
        ],
        [
            "Two OK, so the result.",
            "We have a this Arg.",
            "We can prove that it is a pool name.",
            "Will update time that identifies the limit a large class of context free languages which includes all regular languages.",
            "And most of the standard examples that you had come up with.",
            "So let me."
        ],
        [
            "Move on, just I'll skip through.",
            "So what we have there is we're using a context sensitive representation and we can learn efficiently.",
            "Quite a large class of context free languages were going to move now is to slightly more radical representation that allows us to also learn context sensitive languages and I'll just."
        ],
        [
            "Talk about that briefly so.",
            "I don't have people are familiar with concept lattices, so they've been fairly widely studied in data mining under various different names, so the maximum bipartite cliques of A of a bipartite graph, or the.",
            "Frequent, I've got the dating money, time, frequent complete datasets, and we broadly speaking, if you have a relationship between a set of properties instead of objects, we can consider this sort of maximum rectangles of the grid."
        ],
        [
            "I'll define it formally here.",
            "So we have the syntactic concept lattice.",
            "So what we're going to do here is take a representation.",
            "We're going to base it directly in this algebraic structure that we have for a for language.",
            "So given a set of, if we have a set of strings S, then we can consider the set of context that occur with every element of that set, and Conversely, given a set of context, we can consider the set of strings that occur with every element of those things OK. And a syntactic concept is an ordered pair of strings.",
            "An concepts such that C prime does.",
            "S&S Prime does see OK.",
            "So."
        ],
        [
            "They are if we have again take out trivial regular example here a beast are we take a set A we can say as primes a set of all context will occur with a which concludes context like Lambda B, Lambda, PAB and so on.",
            "As double primed is a set of all strings that can occur that occur with all of these Contacts, which consists of a ABA, ABA, ABA, an as triple primed is equal to S primed OK, so it's a closure operator.",
            "And."
        ],
        [
            "These concepts these syntactic concepts form a complete lattice.",
            "OK, so this is the lattice for this simple language here, so we have.",
            "Each element here consists of a set of strings and a set of contexts at the top.",
            "Here we have the set of all strings and the set of concepts.",
            "The set of contexts that are shared by that, which is the empty set at the bottom we have the set of.",
            "Nothing and all Contacts and in between here we have this the structure.",
            "So for every language we have this syntactic concept lattice OK. We"
        ],
        [
            "Can also define a concatenation operation on this lattice by Justin the simple way, saying if we have SX concatenated with us, why we just take the concept formed by the concatenation of SX&SY.",
            "So this concatenation operation on this lattice."
        ],
        [
            "And this gives us what's called algebraic resituated latter, so it's a lattice.",
            "But we also have this concatenation operation for linguists in the audience.",
            "We also have some residual atian operations that are kind of similar to the situation operations you use in categorial grammar, but there they mean something slightly different."
        ],
        [
            "OK, so given this."
        ],
        [
            "Yeah.",
            "And this lattice again we can compute it from raw data.",
            "Basically, if we take a set of strings, we take a set of contexts we fill in the data in some way, and we can just compute this lattice from that."
        ],
        [
            "So the question is how?",
            "How do we use this to define a representation of this lattice structure?"
        ],
        [
            "Well, basically the lattice is a representation already.",
            "OK, we don't actually need to do anything else to it.",
            "We know what the the concept for each individual string is, and we can just recursively compute the the representation of our strings by using the concatenation operation here.",
            "OK, and then if the concept of a particular string contains Lambda Lambda, then it is in the language.",
            "So this lattice is a representation itself.",
            "Have the only problem is this latches are going to be infinite for non regular languages."
        ],
        [
            "Is the concatenation operation associative?",
            "Yes, yes.",
            "So you can compute that for the tostring just by exactly anyway you want, OK?",
            "So as in, so should have anything that you think is the same answer.",
            "OK, so this is fine if the lattice is finite.",
            "If we have regular languages.",
            "If the latter is finite, then we just.",
            "I.",
            "It's very straightforward, but we already know how to do regular languages.",
            "The question is, how do we do non regular languages?",
            "Tell me."
        ],
        [
            "If we have non regular languages then this lapse is going to be infinite.",
            "OK so here's a simple example of eight Ambien.",
            "Again, you have an infinite set of these things.",
            "However, it's important to recognize that those infinite the structure of these sort of central bits is quite simple.",
            "So we can, if we can infer the structure of some this part here, then we can probably use this chunk of the lattice as the basis for our representation.",
            "So."
        ],
        [
            "How do you do this?",
            "Well?",
            "We fix a finite set of contexts F and we define a complete letter lattice just restricted in this way.",
            "Concatenations to find us before, but it's no longer a resituated latters because it stops being associative.",
            "OK, once we have a fine."
        ],
        [
            "Lattice, let's take this example.",
            "Here we have 8, then beat then.",
            "We take this set of 6 features here.",
            "This gives us this finite lattice.",
            "Which again, we can infer efficiently."
        ],
        [
            "But what we find is that it's no longer associative, right?",
            "If we could concatenation of the concepts of unv isn't may not no longer necessary, equal this.",
            "So if we take 800 and we concatenate with beta 100, this is just top connecting with the top which is taught, which is not the same as the concept of the 101 hundred, which is a nice concept on the thing.",
            "So we lose associativity.",
            "But we can still."
        ],
        [
            "We can still use this as an upper bound there.",
            "Can't we recursively compute our concept?",
            "We know that.",
            "AB is going to be less than equal to the concatenation of this.",
            "This we can prove so.",
            "AB is going to be less than equal to the meat of both of these.",
            "So generally if we have.",
            "We know the concept W here is going to be less than or equal to this computation here.",
            "We take all the splits of W. We compute the lower bound from each of these things and we meet them together.",
            "So we just have.",
            "This is a recursive computation.",
            "From this lattice."
        ],
        [
            "OK, so now we have a lattice representation.",
            "We take this lattice here and we have this recursive computation where we can again do this using standard dynamic programming techniques in cubic time.",
            "And we define the language defined by this representation to be just the set of all strings where we predict that it has this sentence feature.",
            "OK.",
            "So."
        ],
        [
            "So the power of the representation.",
            "So let's just say let's the upper bound is really the set of all languages L such there's a finite set of context such that L is equal to the language defined by the lattice defined by the language, and it includes all regular languages.",
            "Some, but not all, context free languages and some non context free languages, including crucially, some of the classic examples of non context freeness that we see in natural language."
        ],
        [
            "So monotonicity as we increase the set of features F. The language increases, so this now we're doing a slight different thing because we are generalizing much more radically as we increase the set of features, the language increases in this case, so any large enough set of features is OK."
        ],
        [
            "But obviously.",
            "Where we have really is a set of.",
            "We need to consider the case where we have some finite set of features.",
            "We have some fun instead of features F we have a finite set of strings K we're going to write down.",
            "We're going to find our lattice be of KLF here as before.",
            "This is getting a complete lattice.",
            "There are a few technical wrinkles that we may not be able to define concatenation.",
            "Perfectly."
        ],
        [
            "But if concatenation is is defined, we say it's closed under concatenation.",
            "That means that we have essentially a complete model for our fragment."
        ],
        [
            "So I'm I'm aware of the time limits.",
            "I'm I'm accelerating a little bit.",
            "And again we have a second monotonicity limit here, which basically says that as as we increase the set of strings, the language here decreases.",
            "For any sufficiently large K, though, we can recover exactly the is going to be isomorphic to the correct lattice.",
            "OK, So what we have with this lattice then is we have.",
            "We have the true infinite lattice for our language.",
            "We define a set of features that will carve up a relevant chunk of that lattice.",
            "We take a finite set of strings and we can easily compute this lattice here from that, given that lattice, this gives us immediately a representation for for the language we're interested."
        ],
        [
            "So that the last words of the result, yeah?",
            "Kate gets larger.",
            "The second concept gets smaller.",
            "What happens as K as K gets larger?",
            "Yeah, but so basic as K increases the number.",
            "If you have a particular set, the number of context that is shared by all the elements of K can only decrease right?",
            "So as you increase the set of examples, you're decreasing the set of features that they all have.",
            "So that means your predictions then will move.",
            "We will move down in in the representation, so if we have some language.",
            "Yeah, it's a little difficult to spell if, as the.",
            "If you have some language like this, for example here.",
            "Hey there be enterin we we we we remove a single element from it 81 hundred 100 if you only see short strings of this, you're going to assume it's this language.",
            "When you then see a large enough string that you notice this is missing, then your language is going to contract.",
            "OK, so as you as the set of strings, K increases the language you defined."
        ],
        [
            "Faces so again and the search problem is trivial, getting these two monotonicity lemmas.",
            "It's quite straightforward to find a search to find a suitable suitable set of K&F.",
            "OK, so let me."
        ],
        [
            "Wrap it up.",
            "So really two points here.",
            "First of all, we're moving away from the Chomsky hierarchy, but that's not necessarily a problem.",
            "There's an interesting quote from Chomsky which where he says the concept of phrase structure grammar, and he's talking about his his paper 1953 that we sort of kicked off formal language theory was explicitly designed to express the richest system that could reasonably be expected to result from the application of Paris.",
            "Correct procedures to corpus.",
            "So his goal in designing context free grammars also come up with a learnable representation.",
            "That was what he was explicitly trying to do, and not just learnable learnable through distributional learning.",
            "So if distributional learning doesn't work on CFG's, you know, then this isn't a problem with distribution learning.",
            "It's a problem with the context free grammars.",
            "And.",
            "We abandoned some of the representational assumptions of context free grammars and particularly the structural descriptions we get are no longer going to be necessary trees.",
            "They are going to in general be Dags or some slightly richer structure directed acyclic graphs."
        ],
        [
            "OK so um.",
            "That I have to say something about statistical modeling.",
            "Causal will Lynch me.",
            "So we have these efficient algorithms for constructing representations and I've kind of completely ignored all the problems of sparsity and noise and so much of the the very serious problems to learning natural language.",
            "What are we talking realistically?",
            "What we're going to have is some large, very large, very noisy corpus.",
            "We need to replace the assumption of these entire context by partial context, typically adjacent words or word classes.",
            "You can use some sort of.",
            "Statistical clustering to fill in the blocks in your in your matrix and the model parameters.",
            "Can I think one of the exciting things from a statistical modeling point of view, is the model params can represent either very specific or very abstract combinatorial properties.",
            "You have various positions in the lattice where you can attach your parameters."
        ],
        [
            "OK, so the final slide.",
            "So linguistic one Ray Jackendoff wrote a paper called Alternative Minimalist Conceptions of Language, and he identified three constraints that we really want from a theory of language, the descriptors, constraint.",
            "The classical languages must be sufficiently rich to represent natural language is a learnability constraint, so we have to have some plausible story about how these representations could be constructed.",
            "An an evolutionary constraint.",
            "You mustn't posit some evolutionary implausible universal grammar.",
            "And I think potentially here we have maybe not these specific models that I'm talking about, but I think we have a research direction here which could potentially satisfy all three of these criteria.",
            "OK, thanks for listening.",
            "So what languages have you tried your algorithm?",
            "I'm.",
            "So we've done some experiments where we generate random context free grammars.",
            "Generate a sample of strings from the grammar and then.",
            "Test based on the the set of observed substring instead of reserve context constructor grammar from that and then test it and that works pretty well, but it's very difficult to tell to test over generalization.",
            "I think we haven't quite determined whether it over generalizes slightly 'cause it's quite difficult to generate plausible synthetic examples.",
            "Mention that this doesn't is not people running all comics, yeah?",
            "Finding Contacts, yeah.",
            "Do you apply this to say he's an important minutes, right?",
            "So one of the problems in designing domain specific languages that the user has to define grammars, context free grammars, and learning how this whole thing works is Big Barrier.",
            "Yeah, so we've been looking for tools that can allow people to spread sample programs and their target language and then infer the property.",
            "So you mentioned earlier that most programming languages would be finding context.",
            "Yeah, so have you tried this on anyway?",
            "I haven't tried any programming just now.",
            "I mean my my own research interests abroad linguistic, but I I do accept that there are other people who are interested in other things.",
            "I mean, The thing is that programming languages designed in some way to resemble natural languages, so they're easy for humans to read, and so I think they will inherit some of the properties of natural languages have.",
            "We need stuff so.",
            "You know the the very terminology that you used leeches.",
            "Yeah, suggest that maybe in fact actually tracking the full context actually necessary.",
            "So I can imagine some constraints that if you did decide to abstract away from the context, features that you might need in general, do you think that's actually possible and.",
            "How much does the learner have to remember about the left and the right context?",
            "OK, so that's a very good question.",
            "I so I think I think one of the things we can take from like Klein and Manning's work, is that also from the Brown work is that in certain English, just the local context.",
            "Just one word on the left and one more on the right gives you a very strong clear.",
            "I think it may not be true for all languages, so I think particularly free word order languages like maybe a new Polish.",
            "I have a feeling that sort of stuff might not work so well.",
            "'cause you might need to look at.",
            "A longer context, but I certainly think for English not very narrow, context would probably be sufficient one or possibly 2 words on either side if you could even try and expect characteristics of exactly.",
            "Exactly, I mean you can use any any any statistic you want to extract from them to compare the similarity of things.",
            "So if you try to learn a language from really.",
            "There's Anna construction.",
            "Different Curry could occur for syntactically.",
            "So how do you change this?",
            "Yeah, that's that's a that's a subtle question, um?",
            "So what I've been so the full model here is, I assume we are given some you know L which is a subset of Sigma star, right?",
            "And I'm being kind of I don't really think of that.",
            "Necessary is meaning grammatical, just grammatical sentences.",
            "It's got to be much more about acceptability and you gotta let some in practice there's going to be some semantic constraints in the.",
            "So I did do some experiments, but some sort of data exploration exercises on the Google corpus trying to just seeing if you throw huge amounts of data at it, whether there is sufficient.",
            "Rich and what that did bring home to me is that the distribution of the data is is in a way very, very heavily influenced by semantic factors rather than just the syntactic factors.",
            "So there are two ways to two strategies to deal with that.",
            "One is that you do some preliminary kind of syntactic clustering first, so you replace words by some syntactically different, like Brown style clusters and try and work on that, or Alternatively.",
            "I forgot the second strategy is, so the first strategy is probably the best one then too.",
            "That may be very nice, so there is clearly a job.",
            "Security is the structure of the sentence itself, yeah?",
            "This together yeah.",
            "However, if you have past three, you can do confirmations.",
            "If you have a transformation Grandma, you can.",
            "So which one is easier to know?",
            "Like transformation grammars are very very hard to learn I think.",
            "So I mean if you look at I'm not really interested in the transformations between sentences, but I think the structural representations between sentences are best represented through similarities between the structural things rather than representing explicitly.",
            "But insofar as there is formal work on the learning of transformational grammars, it's.",
            "That they're pretty hard to learn, I think.",
            "Yes, possibly, but I'm I'm not, I'm I don't think it's necessary to learn transformations between sentences.",
            "I think you need to learn the structure of individual sentences and you need to learn some way of doing semantic interpretation with those structures.",
            "And I don't.",
            "I've never accepted the arguments that Harrison Chomsky put forward for kind of sentence transformations.",
            "So maybe I'm not answering the right question.",
            "I understand your position.",
            "I would like to know why.",
            "What's your intuition to take it that way?",
            "Well, my intuition is that it would be very difficult to do that, and that it's not necessary, so that's that's why I have a feeling I'm not quite understanding your question there.",
            "Maybe you could talk about afterwards.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so first of all, this is obviously nobody works on their own and this is joint work with a number of people, especially a Frank Taylor, Vimeo and Ameri Abra.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I'm going to talk about here, I'm going to present some algorithms for unsupervised learning of representations for natural languages, especially for natural language syntax.",
                    "label": 1
                },
                {
                    "sent": "And these arguments are efficient in the sense that they're sort of formal sense that they use polynomial amounts of computation.",
                    "label": 0
                },
                {
                    "sent": "And they're also efficient in the sense that you know you can.",
                    "label": 0
                },
                {
                    "sent": "Write the code and run it and it it runs very rapidly.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "And we get these results essentially by using new representations that are designed to be learnable.",
                    "label": 1
                },
                {
                    "sent": "The base, broadly speaking, on distributional learning, so ideas of distribution analysis from structural linguistics.",
                    "label": 1
                },
                {
                    "sent": "People like Zellig, Harris, and.",
                    "label": 0
                },
                {
                    "sent": "Eastern European researchers that followed his work, and they empiricist in a sense that the representation is basically determined by the data.",
                    "label": 0
                },
                {
                    "sent": "And we get representations for richly structured context sensitive languages and the class of languages we get from this seems to be quite a good match to the class of natural languages.",
                    "label": 1
                },
                {
                    "sent": "And the more advanced representations are based on lattice theory, particularly resituated lattices.",
                    "label": 0
                },
                {
                    "sent": "And one slightly shocking thing for a NIPS audiences that I'm using kind of a symbolic learning paradigm.",
                    "label": 0
                },
                {
                    "sent": "So rather assuming that we have a. Heretical, so rather than using a distribution over strings, I'm assuming they were given.",
                    "label": 0
                },
                {
                    "sent": "Rather, using a distribution over strings, I'm assuming that we have some sort of formal language collection of grammatical strings, and I'm going to be using slightly implausible learning models that use membership oracles and so on.",
                    "label": 0
                },
                {
                    "sent": "This is not because I hate statistical modeling, I think, obviously statistical modeling is is a much more realistic paradigm, but you can, I think aren't out some of the wrinkles you can get some of the conceptual problems out of the way by focusing in this sort of rather simplified model.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What am I going to talk about?",
                    "label": 0
                },
                {
                    "sent": "Well I'ma start off talking about methodology.",
                    "label": 0
                },
                {
                    "sent": "If you kind of accept the methodology then the rest of it follows quite naturally.",
                    "label": 0
                },
                {
                    "sent": "But methodology may be a sticking point for some people.",
                    "label": 0
                },
                {
                    "sent": "I will look at regular languages Becausw.",
                    "label": 1
                },
                {
                    "sent": "The illustrated, quite the other straighten methodology quite well and we all understand regulations.",
                    "label": 0
                },
                {
                    "sent": "We have very good intuitions about regular languages, and it's also a case where the representation class we get through this method corresponds to a natural representation class, the Chomsky hierarchy.",
                    "label": 1
                },
                {
                    "sent": "But I'm going to look at learning some context free languages, and then we're going to look at two more novel representations.",
                    "label": 1
                },
                {
                    "sent": "Contextual binary feature grammars and lattice representations, which are more context sensitive formalism.",
                    "label": 0
                },
                {
                    "sent": "And then I'll conclude.",
                    "label": 0
                },
                {
                    "sent": "So methodology.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're here.",
                    "label": 0
                },
                {
                    "sent": "I guess we're all in this room because we're interested in representations for natural language.",
                    "label": 1
                },
                {
                    "sent": "And there are, I guess, two motivations.",
                    "label": 1
                },
                {
                    "sent": "One is a scientific goal with sort of cognitive science goal of trying to understand how language is processed in humans.",
                    "label": 0
                },
                {
                    "sent": "And the second is an engineering goal.",
                    "label": 0
                },
                {
                    "sent": "We want to build language processing systems that actually work, and the current approach is broadly based on supervised learning from annotated data.",
                    "label": 1
                },
                {
                    "sent": "And it's not really an adequate solution to the problem.",
                    "label": 0
                },
                {
                    "sent": "It's not a very good solution to engineering goal because we only have limited amounts of this data for some languages, and it's certainly not a solution to the scientific goal.",
                    "label": 0
                },
                {
                    "sent": "Becausw children aren't provided with parse trees.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So obviously, like a lot of people have been interested in looking unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "And how should we approach this?",
                    "label": 0
                },
                {
                    "sent": "The sort of study of learnability?",
                    "label": 0
                },
                {
                    "sent": "Well, one approach, the normal approach.",
                    "label": 1
                },
                {
                    "sent": "I guess it takes an existing class of representations like context free grammars, tree adjoining grammars or something more fancy.",
                    "label": 1
                },
                {
                    "sent": "And try to design learning algorithms for that class.",
                    "label": 1
                },
                {
                    "sent": "And this is a I spent a few years in the trenches doing this myself.",
                    "label": 0
                },
                {
                    "sent": "I think it's ultimately I think it's it's misconceived.",
                    "label": 0
                },
                {
                    "sent": "Having you know, having spent several years doing this myself, I feel I can say that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I think there are two problems.",
                    "label": 1
                },
                {
                    "sent": "The first problem is that we don't really know what the representations are.",
                    "label": 0
                },
                {
                    "sent": "So linguists often talk as though we know what these tree structures are, but in fact they can't agree about anything.",
                    "label": 0
                },
                {
                    "sent": "And the reason they can't agree about anything is as Jim Blevins took it.",
                    "label": 0
                },
                {
                    "sent": "Put it in a recent article in general linguistics at the most fundamental level did not click.",
                    "label": 1
                },
                {
                    "sent": "There is any meaningful empirical motivation for the representation assumptions of any current form model syntax, which is pretty harsh, but I think it's true.",
                    "label": 1
                },
                {
                    "sent": "I think that if you look at the justifications for the empirical representation, there very, very weak and the second problem is that learning these standard representations is computationally extremely hard.",
                    "label": 1
                },
                {
                    "sent": "Even when you assume a quite benign learning model, like in this Anglin and version of paper learning, even things like nondeterministic finite state automotor, which is roughly equivalent to regular grammars, the bottom level of the Chomsky hierarchy is really hard.",
                    "label": 0
                },
                {
                    "sent": "Computationally, so how to move?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Forward.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah.",
                    "label": 0
                },
                {
                    "sent": "In first language acquisition, we don't know what the representations are, but we do know that they are learnable.",
                    "label": 1
                },
                {
                    "sent": "OK, so it seems like this approach of picking some representations and then trying to learn them is completely the wrong way around.",
                    "label": 0
                },
                {
                    "sent": "What we should do and putting as a slogan is to put learnability first.",
                    "label": 1
                },
                {
                    "sent": "We should be prepared to look for new representations.",
                    "label": 0
                },
                {
                    "sent": "We should insist that they are learnable.",
                    "label": 0
                },
                {
                    "sent": "OK, we should learn ability is kind of unusual property.",
                    "label": 0
                },
                {
                    "sent": "You're not going to get it by accident.",
                    "label": 0
                },
                {
                    "sent": "You're only going to get it if you design it in from the beginning.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so, how do we put this into practice?",
                    "label": 0
                },
                {
                    "sent": "Well, normally what we do is we have a function from the representation to the language.",
                    "label": 0
                },
                {
                    "sent": "So given some context free grammar G, we define their derivation process and so on, and we will define a language L. So this is a function from the representation to the language given a non terminal we can define the set of strings derived from that non terminal.",
                    "label": 1
                },
                {
                    "sent": "We need to flip this around and go in.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other direction we need to focus on looking to function from the language given the language, how do we construct a representation from that language?",
                    "label": 1
                },
                {
                    "sent": "So given a language L, we want to have a function which gives us the representation for that language from a set of strings to some representational primitive of the formulas, and whether it's a state or nonterminal symbol or something like that.",
                    "label": 1
                },
                {
                    "sent": "And again, putting this again as a slogan, I'm going to move on from slogans to technical content shortly, so it's not entirely.",
                    "label": 0
                },
                {
                    "sent": "But the structural representation should be based on the structure of language and not something arbitrarily imposing it from outside.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you're going to have a learnable representation, you should first identify some structure in the language.",
                    "label": 0
                },
                {
                    "sent": "Show how that structure can be observed, and then we construct a representation based on that structure.",
                    "label": 0
                },
                {
                    "sent": "And if you get a richer structure, then you're going to get a richer and more powerful expressive representation.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we actually do this?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, I'll illustrate this with example from regular languages.",
                    "label": 1
                },
                {
                    "sent": "So regular language is kind of one of the few success stories of grammatical inference of learning.",
                    "label": 0
                },
                {
                    "sent": "We define our regular language by a deterministic finite automation which has a finite set of States and some transition function.",
                    "label": 0
                },
                {
                    "sent": "And for each state here we define the language generator from that state just to be in in the standard way.",
                    "label": 0
                },
                {
                    "sent": "And the language defined by the automation is a language that you derive from the initial state.",
                    "label": 0
                },
                {
                    "sent": "Now I'm.",
                    "label": 0
                },
                {
                    "sent": "An elementary but very important result.",
                    "label": 0
                },
                {
                    "sent": "Is it my whole narrow term where if we define a right congruence here, where we say that two strings U&V are congruent if and only if they have exactly the same set of suffixes?",
                    "label": 0
                },
                {
                    "sent": "I mean sort of residual language is equal.",
                    "label": 0
                },
                {
                    "sent": "And we can write them.",
                    "label": 0
                },
                {
                    "sent": "You are here for the congruence class, in particular, string.",
                    "label": 0
                },
                {
                    "sent": "Now my hero theorem says the number of states in the minimal DFA that generates language is exactly equal to the number of equivalence classes under this relation.",
                    "label": 0
                },
                {
                    "sent": "So we have some structure in the language here, which is this this congruence structure here.",
                    "label": 1
                },
                {
                    "sent": "This set of Congress classes.",
                    "label": 0
                },
                {
                    "sent": "And we can base gives of this, we can tell whether two strings are congruent just by taking a large sample and comparing the suffixes of you with the suffixes RV in oversea depends on your language model.",
                    "label": 0
                },
                {
                    "sent": "And there might not always always be.",
                    "label": 1
                },
                {
                    "sent": "Be possible to do this with great accuracy, but in principle this is an observable property of the language.",
                    "label": 0
                },
                {
                    "sent": "Hum.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's say you have just some trivial, trivial regular language like a beast our Lambda.",
                    "label": 0
                },
                {
                    "sent": "Here's the empty string and we put some prefixes.",
                    "label": 0
                },
                {
                    "sent": "Here we have a set of suffixes here, and we can see that these strings here fall into 3 Congress courses.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Three consequences here.",
                    "label": 0
                },
                {
                    "sent": "Here we have a class of congruence of strings that don't occur as prefixes or language.",
                    "label": 0
                },
                {
                    "sent": "So we get these three.",
                    "label": 0
                },
                {
                    "sent": "We can observe these three.",
                    "label": 0
                },
                {
                    "sent": "These three Congress courses.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can do is we can just define the states of our automation to be these congruence classes.",
                    "label": 0
                },
                {
                    "sent": "And the key point.",
                    "label": 0
                },
                {
                    "sent": "It's kind of an elementary point, but it's quite important is that if we have.",
                    "label": 0
                },
                {
                    "sent": "If you have the same set of suffixes V, then UA.",
                    "label": 0
                },
                {
                    "sent": "Has the same social services via.",
                    "label": 0
                },
                {
                    "sent": "That's because it's a right congruence.",
                    "label": 0
                },
                {
                    "sent": "And what this means is we can define a transition from the state U to the state UA.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is kind of this seems entirely entirely vacuous.",
                    "label": 0
                },
                {
                    "sent": "But let's say we take some element of XAA.",
                    "label": 0
                },
                {
                    "sent": "We append B, we get a B which is equal to the Chronicles of Lambda.",
                    "label": 1
                },
                {
                    "sent": "So this gives us transition from the state A to this other state there.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we've done here is we've defined a.",
                    "label": 0
                },
                {
                    "sent": "Hey, some structure in that we recognize identified some structure in language and we've based representation.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Entirely on that, there's a correspondence between.",
                    "label": 1
                },
                {
                    "sent": "The representational primitives of our model.",
                    "label": 1
                },
                {
                    "sent": "Which of these states?",
                    "label": 0
                },
                {
                    "sent": "And some property of the language.",
                    "label": 0
                },
                {
                    "sent": "These right congruence classes OK, and we can use this to define a Canonical deterministic finite automation.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this gives us some standard results for learning deterministic finite automatron probabilistic deterministic finite automata, which may be the most important for anguineus paper.",
                    "label": 0
                },
                {
                    "sent": "From 1982, Jose on scene as paper from 92 and Donna Ron's paper on PAC Learning Acyclic PDF's.",
                    "label": 0
                },
                {
                    "sent": "So we now have a.",
                    "label": 0
                },
                {
                    "sent": "This is a success story of matter.",
                    "label": 0
                },
                {
                    "sent": "We now have good effective algorithms for learning.",
                    "label": 0
                },
                {
                    "sent": "Regular automater I mean obviously it doesn't work for everything.",
                    "label": 0
                },
                {
                    "sent": "There are some limitations.",
                    "label": 0
                },
                {
                    "sent": "It relies on frequent suffixes, Co occurring with frequent prefixes, and so this won't work if you have some sort of uniform distribution over an bitstrings or something like that.",
                    "label": 1
                },
                {
                    "sent": "And you can construct some.",
                    "label": 0
                },
                {
                    "sent": "There's some hardness results that rule out learning in those circumstances.",
                    "label": 1
                },
                {
                    "sent": "OK, so we have this success for regular languages.",
                    "label": 0
                },
                {
                    "sent": "How do we move?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From these regular languages onto context, free languages and beyond.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we need to do is rather than moving rather than looking at the relationship between prefixes and suffixes, we're going to look at the relationship between contexts and substrings, so a context in this case is just going to be a pair of strings.",
                    "label": 1
                },
                {
                    "sent": "LR and we combine a context with the substring just by wrapping it, wrapping it together.",
                    "label": 0
                },
                {
                    "sent": "And so the distribution of a string here.",
                    "label": 1
                },
                {
                    "sent": "And this is distribution in the linguistic sense.",
                    "label": 0
                },
                {
                    "sent": "Rather distribution.",
                    "label": 0
                },
                {
                    "sent": "The statistical sense is just the set of all contexts LR that can occur with a particular particular string, and this gives us a syntactic congruence, which is a fairly standard bit of language theory, which we say that two strings U&V are congruent for each other if and only if they have the same distribution and we're going to write the square brackets you for the congruence class.",
                    "label": 0
                },
                {
                    "sent": "Of you.",
                    "label": 1
                },
                {
                    "sent": "So distributional learning, which is used quite extensively in natural language processing, essentially models these.",
                    "label": 0
                },
                {
                    "sent": "The distributions of strings an if you use us to do.",
                    "label": 0
                },
                {
                    "sent": "Unsupervised learning of parts of speech classes and things like this.",
                    "label": 0
                },
                {
                    "sent": "It works extremely well, and in fact the Brown Brown at our results that we had in the previous talk are based precisely on doing some clustering of these distributions.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, this is an observable structure, right?",
                    "label": 0
                },
                {
                    "sent": "We can more or less tell whether you is congruent to V. We just take a whole bunch of data, look at the observed context of you, and compare those observed contexts of V. Now the key point here is that if you is congruent to U prime and V is congruent to V primed, then UV is congruent to U prime V prime.",
                    "label": 1
                },
                {
                    "sent": "Again, it's very trivial point of this is that if we identify our congruence copses with non terminals, if we create a context free grammar and we say that the non terminals are context free grammar are going to be exactly the congruence classes of the language.",
                    "label": 0
                },
                {
                    "sent": "Then it turns out that we can write down rules of this form here, where UV goes to UV.",
                    "label": 0
                },
                {
                    "sent": "OK, So what this means at this point means is we pick any element of this congruence class and combine it with any element of this congruence class.",
                    "label": 0
                },
                {
                    "sent": "We're going to get an element of this congruence class.",
                    "label": 0
                },
                {
                    "sent": "OK, so the context free production here exactly matches the algebraic structure of the language.",
                    "label": 0
                },
                {
                    "sent": "In particular this.",
                    "label": 0
                },
                {
                    "sent": "What is called the syntactic more like, which is kind of for those that are algebraically inclined.",
                    "label": 0
                },
                {
                    "sent": "It's the you know the quotient of the Mona.",
                    "label": 0
                },
                {
                    "sent": "I'd buy this equivalence relation.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's take a trivial example here.",
                    "label": 0
                },
                {
                    "sent": "You're kind of very standard example of context free language, then beat.",
                    "label": 1
                },
                {
                    "sent": "Then we have a finite set of congruence classes.",
                    "label": 1
                },
                {
                    "sent": "We select a finite set of comments classes.",
                    "label": 0
                },
                {
                    "sent": "It's worth pointing out the number of congruence classes is going to be finite if and only if the language is regular.",
                    "label": 0
                },
                {
                    "sent": "So in this case they're going to be infinite number of Congress classes.",
                    "label": 0
                },
                {
                    "sent": "We take some finite set of them that look like this, and then we can just write down.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rules.",
                    "label": 0
                },
                {
                    "sent": "We gotta have some trivial rules like this.",
                    "label": 0
                },
                {
                    "sent": "Saying the conference call so they can be rewritten as a an we have some.",
                    "label": 0
                },
                {
                    "sent": "Rules like this AB goes to ABABB goes to ABB.",
                    "label": 0
                },
                {
                    "sent": "These are all kind of look, sort of vacuous.",
                    "label": 0
                },
                {
                    "sent": "Then we had some ones that look less vacuous, since AB is exactly the same as a ABB.",
                    "label": 0
                },
                {
                    "sent": "Since AP is congruent to ABB.",
                    "label": 0
                },
                {
                    "sent": "If we taken a this conference calls with a in the current class ABB, and we combine them, we will get a congruence class of a B. OK, so in this case if we just take this finite set of these small set of these Congress classes.",
                    "label": 0
                },
                {
                    "sent": "Would you define distributionally?",
                    "label": 0
                },
                {
                    "sent": "We can simply write down an appropriate context free grammar by using the this structure.",
                    "label": 0
                },
                {
                    "sent": "Yes, just information.",
                    "label": 0
                },
                {
                    "sent": "So did you say that even if the language is context free, there's an infinite number of congruence classes, so the.",
                    "label": 0
                },
                {
                    "sent": "The number of congruence classes of a language is finite if and only if the language is regular.",
                    "label": 0
                },
                {
                    "sent": "So a language like this which is not regular will have an infinite number of Congress passes, and you can see why that is, because you're going to have Congress classes like AAAAAA to the K is going to have different congruence class from 8K plus one.",
                    "label": 0
                },
                {
                    "sent": "Number of context free rules, but you get you.",
                    "label": 0
                },
                {
                    "sent": "You only need to have a finite number of a finite subset of them for a certain class of context free languages.",
                    "label": 0
                },
                {
                    "sent": "So not all context free languages can be represented through a finite set of congruence classes.",
                    "label": 1
                },
                {
                    "sent": "OK, is there any way you could tell us how to find out?",
                    "label": 0
                },
                {
                    "sent": "Well, it's I I'm I'm moving on from this week and there is an interesting language class which is defined by those ones that are finding it slightly too small.",
                    "label": 0
                },
                {
                    "sent": "So I'll talk about.",
                    "label": 0
                },
                {
                    "sent": "Delayed.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So this gives you some basic results for learning context free grammar, so the most trivial result, which is kind of the first one, is to call the substitutable languages, which is if you assume that if the distribution whenever you have two strings, you envy which have a nonempty intersection of the distribution.",
                    "label": 1
                },
                {
                    "sent": "Or you can find one context that lies that accepts both of these substrings, then there syntactically congruent OK.",
                    "label": 0
                },
                {
                    "sent": "This is clearly a completely false for natural languages.",
                    "label": 1
                },
                {
                    "sent": "That enables you to get some kind of elementary results for these algorithms.",
                    "label": 0
                },
                {
                    "sent": "And what's kind of surprising about?",
                    "label": 0
                },
                {
                    "sent": "I mean this paper, what's the the really interesting thing about this paper, which I'm a Co author of, is the is the year right?",
                    "label": 0
                },
                {
                    "sent": "And this paper didn't appear until well, another version of here couple years earlier, but this paper really should be written about 1985.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is the kind of embarrassing thing about this.",
                    "label": 1
                },
                {
                    "sent": "This whole field.",
                    "label": 0
                },
                {
                    "sent": "And then following from this we have some results for PAC Learning on some other variants of it, OK?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The real problem though is that this idea of having one symbol per congruence class just isn't going to work for natural languages.",
                    "label": 1
                },
                {
                    "sent": "It'll work fine for these sort of simple, trivial examples like ATM BTN, but it's just not going to work for natural languages, and that's the cause.",
                    "label": 0
                },
                {
                    "sent": "The number of congruence that you have a very many, and they're often very close to each other.",
                    "label": 0
                },
                {
                    "sent": "The learning model here by treating each congruence class as a separate atomic symbol.",
                    "label": 1
                },
                {
                    "sent": "Assumes that either these Congress costs are identical.",
                    "label": 0
                },
                {
                    "sent": "They're completely unrelated.",
                    "label": 1
                },
                {
                    "sent": "We clearly need to have a much more powerful representation that can represent, in some sense of structure.",
                    "label": 0
                },
                {
                    "sent": "These Congress passes, and Furthermore, languages are context free.",
                    "label": 0
                },
                {
                    "sent": "So just give a kind of.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pretty simple example.",
                    "label": 0
                },
                {
                    "sent": "In a context free grammar for natural language, you might have something like.",
                    "label": 1
                },
                {
                    "sent": "A noun phrase consists of a determiner, followed by now.",
                    "label": 0
                },
                {
                    "sent": "Now if you put some determiners here and you put some nouns here, you find it's not true that you can take any determine and combine it with any noun, and you'll get a grammatical noun phrase, because there is.",
                    "label": 0
                },
                {
                    "sent": "There are singular and plural nouns.",
                    "label": 0
                },
                {
                    "sent": "There're count noun mass noun distinctions, the alternation of the definite indefinite article depends on the phone.",
                    "label": 0
                },
                {
                    "sent": "A logical properties of the start of the noun.",
                    "label": 0
                },
                {
                    "sent": "And in fact, if you look at this, you see that.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The determiner.",
                    "label": 0
                },
                {
                    "sent": "An every noun is in a different congruence class here, so you need to have a different rule for each combination.",
                    "label": 1
                },
                {
                    "sent": "And so clearly that's impractical.",
                    "label": 0
                },
                {
                    "sent": "So we need to come up with some slightly richer structure that will enable us to to learn this sort of stuff.",
                    "label": 0
                },
                {
                    "sent": "So how are we going to do that?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the so the key point is really that to recognize that the distribution of a string you is a set of contexts.",
                    "label": 1
                },
                {
                    "sent": "And since it's a set, the appropriate algebraic structure to represent it where it is as a lattice, and the first person to.",
                    "label": 0
                },
                {
                    "sent": "To discover this was a Frenchman called Sestiere in 1960.",
                    "label": 1
                },
                {
                    "sent": "As far as I can tell, he only wrote one paper, but it's quite a good paper, so it's a it's a it's a success rate.",
                    "label": 0
                },
                {
                    "sent": "We should all we should all aspire to.",
                    "label": 0
                },
                {
                    "sent": "The basic idea here is that linguistically, if you just take a simple, simple linguistic example here, so you take a personal pronoun her, well, that could be.",
                    "label": 0
                },
                {
                    "sent": "I gave her a cake or I stole her camera and so if you look at the distribution of of her, it's basically going to be the Union of the distribution of his and the distribution of him.",
                    "label": 1
                },
                {
                    "sent": "OK, so in order to start to get a more linguistically nuanced model, we need to have operations that correspond to these sort of unions and intersections.",
                    "label": 0
                },
                {
                    "sent": "And that means we need to start using a lattice.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can do this really by looking at contextual features.",
                    "label": 1
                },
                {
                    "sent": "So what we do is we pick some finite set of of K context.",
                    "label": 1
                },
                {
                    "sent": "So again, context is just a pair of strings, so we have some set finite set of these F and we're going to represent the distribution of a string.",
                    "label": 1
                },
                {
                    "sent": "Just buy a subset of elements of F so we can 'cause there is a subset or just a binary vector of length K. And this gives us two Takei possible congruence classes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, go back.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so represented by the way, by what remains up you take that or whether or not that intersection is empty or not empty by the set of elements here by this set here.",
                    "label": 0
                },
                {
                    "sent": "So this is a subset of F that consists of the set of features contextual features that can occur with a particular substring, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We had this lattice structure going to talk in a bit more detail about the lattice structure later on, but the key point here is that if we have any strings you you prime V prime.",
                    "label": 1
                },
                {
                    "sent": "If the distribution of you contains the distribution of you prime and the distribution of the contains the distribution of the prime, then the distribution of UV contains the distribution of your prime view prompt.",
                    "label": 0
                },
                {
                    "sent": "OK, so again, this gives us a way where we can compute.",
                    "label": 0
                },
                {
                    "sent": "An estimate of some the distribution of a string based on the distribution of its parts.",
                    "label": 0
                },
                {
                    "sent": "We can recursively compute an estimate of the distribution based on the distributions of its substrings.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we just rewrite this in a much less penetrable form, we get this sort of thing.",
                    "label": 0
                },
                {
                    "sent": "We can say that if we have some string W and we're interested in finding out what its context is, we know it's going to contain.",
                    "label": 0
                },
                {
                    "sent": "If we split the W into any different combination of U&V such that you envy concatenated W and we have some finite set K of strings.",
                    "label": 0
                },
                {
                    "sent": "If we take the union of all of these, you primes which are contained in U&V, and we take the Union of the context of you prime V prime, and we know that this is going to be less so.",
                    "label": 0
                },
                {
                    "sent": "This gives us.",
                    "label": 0
                },
                {
                    "sent": "Essentially a.",
                    "label": 0
                },
                {
                    "sent": "A lower bound.",
                    "label": 0
                },
                {
                    "sent": "On the distribution of these things, which we can recursively compute.",
                    "label": 0
                },
                {
                    "sent": "So this is the basis of our representation.",
                    "label": 0
                },
                {
                    "sent": "We just turn this directly into a representation kayworth signal start, then that would be a quality yes.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how does this work?",
                    "label": 0
                },
                {
                    "sent": "So here we'll just do some some lattice diagrams here.",
                    "label": 0
                },
                {
                    "sent": "Again, I'm slightly vague way at the moment.",
                    "label": 0
                },
                {
                    "sent": "So if we have some unv and we know that you is below you primed and veers below V primed, then we know that UV is going to be below you from the prompt, but that's kind of trivial, will slightly.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Esther is that we can split it in more than one different way.",
                    "label": 0
                },
                {
                    "sent": "So if we have some string here, UVW we can either split into UV&W and we know the UV is below X&W is blue?",
                    "label": 0
                },
                {
                    "sent": "Why so UVW has to be below XY or we can split it the other way?",
                    "label": 0
                },
                {
                    "sent": "In which case we know that you VW is gonna Bill Opq.",
                    "label": 0
                },
                {
                    "sent": "OK, so this.",
                    "label": 0
                },
                {
                    "sent": "Representation is fundamentally non context free.",
                    "label": 1
                },
                {
                    "sent": "OK Becausw we can combine the results from different different derivations together.",
                    "label": 0
                },
                {
                    "sent": "Sophie.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Normally we take a.",
                    "label": 0
                },
                {
                    "sent": "We'll call this contextual binary feature grammar.",
                    "label": 1
                },
                {
                    "sent": "So we have F is our set of features.",
                    "label": 0
                },
                {
                    "sent": "We're going to have one special element of of F, which is just this sort of empty feature, where Lambda is the empty string.",
                    "label": 0
                },
                {
                    "sent": "So clearly a string will have this feature if and only if it's in the language and we have some productions we have.",
                    "label": 0
                },
                {
                    "sent": "So productions that introduced the set of features for one of our atomic symbols and we also have productions that combine.",
                    "label": 0
                },
                {
                    "sent": "Our features together so XY zed here are subsets of F, so informing this production like this says that if we have you has features Y&V has features Ed then when we stick together we know that you and view the is going to have a set of features X.",
                    "label": 0
                },
                {
                    "sent": "And we can.",
                    "label": 0
                },
                {
                    "sent": "Features here contexts.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are obviously more sophisticated ways of defining these features, but we're going to define would take the most basic one where one context equals one feature.",
                    "label": 0
                },
                {
                    "sent": "One might want to have cases where features correspond to sets of contexts, so we define some.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recursive feature map here.",
                    "label": 0
                },
                {
                    "sent": "Where we for if if we have a an atomic symbol string of length one, we just look it up in our lexicon, effectively an.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we just recursively compute it using this this.",
                    "label": 0
                },
                {
                    "sent": "Just applying the rules.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this gives you is very similar to a CKY parsing algorithm, which we can do in in cubic time.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "I'll skip this like misleading Hank.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose we have some finite set of strings K in a finite set of features F, and we have very helpfully membership Oracle which you can use to query or some very very large data source.",
                    "label": 1
                },
                {
                    "sent": "So if we have UV and UV Erin K, then we can know that CNU and CV combined to form C of UV.",
                    "label": 0
                },
                {
                    "sent": "So we just add a production.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Which is based precisely on these this observed combination.",
                    "label": 0
                },
                {
                    "sent": "So we observe two strings U&V.",
                    "label": 0
                },
                {
                    "sent": "You combine in a certain way so we put together rules saying that whenever we have a string which has the set of features that you have, and a string that has a set of features that we have, we can combine those to produce a string which will have the set of features that UV has.",
                    "label": 0
                },
                {
                    "sent": "And the lexer productions.",
                    "label": 0
                },
                {
                    "sent": "We just say, well, A has a set of features that A has.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So given this, given the survey, we can just essentially write down.",
                    "label": 0
                },
                {
                    "sent": "There's no computation involved.",
                    "label": 0
                },
                {
                    "sent": "If we're estimating it statistically, then there is some nontrivial computation, but in this symbolic learning paradigm, there's no computation at all.",
                    "label": 0
                },
                {
                    "sent": "We just write it down.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So given this article and a choice McCain F, we can write down a grammar KLF The question is, is it easy to find the right values of K&F or is it a hard such problem?",
                    "label": 0
                },
                {
                    "sent": "We just substituted one hard problem for another one, which it turns out to be very easy to find suitable K&F.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, as K increases, as we increase the set of the set of strings K, the language increases.",
                    "label": 1
                },
                {
                    "sent": "OK, so if we have.",
                    "label": 0
                },
                {
                    "sent": "K is a set of strings.",
                    "label": 0
                },
                {
                    "sent": "We increase it slightly to have K plus.",
                    "label": 0
                },
                {
                    "sent": "Then the language defined by KLF is a subset of the language defined by K plus LF.",
                    "label": 0
                },
                {
                    "sent": "OK, that's kind of obvious.",
                    "label": 0
                },
                {
                    "sent": "As as we increase the number of examples, the set of rules that we get increases.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, slightly less obvious is that.",
                    "label": 0
                },
                {
                    "sent": "The is the monotonicity of F as we increase Fr, set of features are set of contexts, the language actually decreases monotonically.",
                    "label": 0
                },
                {
                    "sent": "And this is because really the features define the conditions under which we predict a feature.",
                    "label": 1
                },
                {
                    "sent": "So if you has features, Y&V has features Ed, then we predict the UV has some features X.",
                    "label": 0
                },
                {
                    "sent": "If we increase the set of features that are going to be increasing Y&Z, and so we're going to be decreasing the number of times that this rule applies.",
                    "label": 0
                },
                {
                    "sent": "OK so as we increase the set of context, the language monotonically decreases.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a simple example here.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "We have a just a standard trivial example we have.",
                    "label": 0
                },
                {
                    "sent": "Here we increase our set of K just to be the most frequently occurring substrings.",
                    "label": 0
                },
                {
                    "sent": "Here we have the most frequently training contexts we see here.",
                    "label": 0
                },
                {
                    "sent": "The this is the over generalization where white means it doesn't overgeneralize.",
                    "label": 0
                },
                {
                    "sent": "So here as we increase the set of features, I as we go vertically up here, we see the over generalization decreases monotonically.",
                    "label": 0
                },
                {
                    "sent": "And converse.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look at the under generalization as we increase the set of strings that we use, the under generation decreases monotonically.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you get out books.",
                    "label": 0
                },
                {
                    "sent": "You get a diagonal wedge I'll.",
                    "label": 0
                },
                {
                    "sent": "I'll give a example it.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So algorithm.",
                    "label": 0
                },
                {
                    "sent": "Not, I'm not interested in trying to prove really practical algorithms, I'm just trying to get a simple algorithms that are going to be probably correct.",
                    "label": 0
                },
                {
                    "sent": "So basically, if your language under generates you add some strings to your your K, your set of kernels, which means you go to the right if the language over generates you add some contexts and you go up so.",
                    "label": 1
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's a diagram here.",
                    "label": 0
                },
                {
                    "sent": "Use or if you're if you know if you're if you're not generating something, you move over to the right.",
                    "label": 0
                },
                {
                    "sent": "If you're generating things you shouldn't, you move up and eventually you're going to get into this correct area.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are the classes of languages which this works so it doesn't work for all context free languages?",
                    "label": 1
                },
                {
                    "sent": "OK, so one key point here is the the finite context property.",
                    "label": 1
                },
                {
                    "sent": "So if you have a finite set of context, you such that for any string V, if F of you is a subset of the distribution of Y, then see if you as a subset of COV.",
                    "label": 1
                },
                {
                    "sent": "OK, So what you're saying really is string has the language if you can.",
                    "label": 0
                },
                {
                    "sent": "If you can say that every string that contains this finite set of context will contain all the other contexts, and it's a kind of quite a natural.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's quite a natural.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hypothesis it's quite a natural language theoretic property, and so here's some some sort of technical detail.",
                    "label": 0
                },
                {
                    "sent": "What we basically we define this notion of fiducials, where we have a status of features, is sufficiently strong sufficiently big to represent a particular set of of strings K. And then what we can prove?",
                    "label": 1
                },
                {
                    "sent": "Basically the key lemma is that if this set is large enough.",
                    "label": 0
                },
                {
                    "sent": "Then this the binary feature game is going only going to predict correct features.",
                    "label": 0
                },
                {
                    "sent": "OK, so that means that the set of features that we predict is going to be a subset of the true features, OK?",
                    "label": 0
                },
                {
                    "sent": "So in the infinite limit when F, if we allow ourselves to have infinite sets of features, then this is always going to be correct.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So all regular languages have this property.",
                    "label": 1
                },
                {
                    "sent": "There are some context free language that don't have the finance context property.",
                    "label": 0
                },
                {
                    "sent": "This is quite an important example here, so this is a simple example.",
                    "label": 1
                },
                {
                    "sent": "I could come up with of context free grammar, context free language that doesn't have this property and you just don't get this sort of phenomenon in natural languages.",
                    "label": 0
                },
                {
                    "sent": "So I think it's quite a plausible hypothesis that natural languages have the finite context property.",
                    "label": 0
                },
                {
                    "sent": "Most programming languages can find context operated well.",
                    "label": 0
                },
                {
                    "sent": "Yes, thank they all do I mean, insofar as their context, yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean there's some debate about exactly how you define the set of well formed strings of a programming language.",
                    "label": 0
                },
                {
                    "sent": "But yeah, they normally do.",
                    "label": 0
                },
                {
                    "sent": "And let me just.",
                    "label": 0
                },
                {
                    "sent": "I'll skip on.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two OK, so the result.",
                    "label": 0
                },
                {
                    "sent": "We have a this Arg.",
                    "label": 0
                },
                {
                    "sent": "We can prove that it is a pool name.",
                    "label": 0
                },
                {
                    "sent": "Will update time that identifies the limit a large class of context free languages which includes all regular languages.",
                    "label": 1
                },
                {
                    "sent": "And most of the standard examples that you had come up with.",
                    "label": 0
                },
                {
                    "sent": "So let me.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Move on, just I'll skip through.",
                    "label": 0
                },
                {
                    "sent": "So what we have there is we're using a context sensitive representation and we can learn efficiently.",
                    "label": 0
                },
                {
                    "sent": "Quite a large class of context free languages were going to move now is to slightly more radical representation that allows us to also learn context sensitive languages and I'll just.",
                    "label": 1
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk about that briefly so.",
                    "label": 0
                },
                {
                    "sent": "I don't have people are familiar with concept lattices, so they've been fairly widely studied in data mining under various different names, so the maximum bipartite cliques of A of a bipartite graph, or the.",
                    "label": 1
                },
                {
                    "sent": "Frequent, I've got the dating money, time, frequent complete datasets, and we broadly speaking, if you have a relationship between a set of properties instead of objects, we can consider this sort of maximum rectangles of the grid.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll define it formally here.",
                    "label": 0
                },
                {
                    "sent": "So we have the syntactic concept lattice.",
                    "label": 1
                },
                {
                    "sent": "So what we're going to do here is take a representation.",
                    "label": 0
                },
                {
                    "sent": "We're going to base it directly in this algebraic structure that we have for a for language.",
                    "label": 0
                },
                {
                    "sent": "So given a set of, if we have a set of strings S, then we can consider the set of context that occur with every element of that set, and Conversely, given a set of context, we can consider the set of strings that occur with every element of those things OK. And a syntactic concept is an ordered pair of strings.",
                    "label": 1
                },
                {
                    "sent": "An concepts such that C prime does.",
                    "label": 0
                },
                {
                    "sent": "S&S Prime does see OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They are if we have again take out trivial regular example here a beast are we take a set A we can say as primes a set of all context will occur with a which concludes context like Lambda B, Lambda, PAB and so on.",
                    "label": 0
                },
                {
                    "sent": "As double primed is a set of all strings that can occur that occur with all of these Contacts, which consists of a ABA, ABA, ABA, an as triple primed is equal to S primed OK, so it's a closure operator.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These concepts these syntactic concepts form a complete lattice.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the lattice for this simple language here, so we have.",
                    "label": 0
                },
                {
                    "sent": "Each element here consists of a set of strings and a set of contexts at the top.",
                    "label": 0
                },
                {
                    "sent": "Here we have the set of all strings and the set of concepts.",
                    "label": 0
                },
                {
                    "sent": "The set of contexts that are shared by that, which is the empty set at the bottom we have the set of.",
                    "label": 0
                },
                {
                    "sent": "Nothing and all Contacts and in between here we have this the structure.",
                    "label": 0
                },
                {
                    "sent": "So for every language we have this syntactic concept lattice OK. We",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can also define a concatenation operation on this lattice by Justin the simple way, saying if we have SX concatenated with us, why we just take the concept formed by the concatenation of SX&SY.",
                    "label": 0
                },
                {
                    "sent": "So this concatenation operation on this lattice.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this gives us what's called algebraic resituated latter, so it's a lattice.",
                    "label": 0
                },
                {
                    "sent": "But we also have this concatenation operation for linguists in the audience.",
                    "label": 0
                },
                {
                    "sent": "We also have some residual atian operations that are kind of similar to the situation operations you use in categorial grammar, but there they mean something slightly different.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so given this.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And this lattice again we can compute it from raw data.",
                    "label": 1
                },
                {
                    "sent": "Basically, if we take a set of strings, we take a set of contexts we fill in the data in some way, and we can just compute this lattice from that.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the question is how?",
                    "label": 0
                },
                {
                    "sent": "How do we use this to define a representation of this lattice structure?",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, basically the lattice is a representation already.",
                    "label": 1
                },
                {
                    "sent": "OK, we don't actually need to do anything else to it.",
                    "label": 0
                },
                {
                    "sent": "We know what the the concept for each individual string is, and we can just recursively compute the the representation of our strings by using the concatenation operation here.",
                    "label": 0
                },
                {
                    "sent": "OK, and then if the concept of a particular string contains Lambda Lambda, then it is in the language.",
                    "label": 1
                },
                {
                    "sent": "So this lattice is a representation itself.",
                    "label": 0
                },
                {
                    "sent": "Have the only problem is this latches are going to be infinite for non regular languages.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is the concatenation operation associative?",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "So you can compute that for the tostring just by exactly anyway you want, OK?",
                    "label": 0
                },
                {
                    "sent": "So as in, so should have anything that you think is the same answer.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is fine if the lattice is finite.",
                    "label": 1
                },
                {
                    "sent": "If we have regular languages.",
                    "label": 0
                },
                {
                    "sent": "If the latter is finite, then we just.",
                    "label": 1
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "It's very straightforward, but we already know how to do regular languages.",
                    "label": 0
                },
                {
                    "sent": "The question is, how do we do non regular languages?",
                    "label": 0
                },
                {
                    "sent": "Tell me.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we have non regular languages then this lapse is going to be infinite.",
                    "label": 0
                },
                {
                    "sent": "OK so here's a simple example of eight Ambien.",
                    "label": 0
                },
                {
                    "sent": "Again, you have an infinite set of these things.",
                    "label": 0
                },
                {
                    "sent": "However, it's important to recognize that those infinite the structure of these sort of central bits is quite simple.",
                    "label": 0
                },
                {
                    "sent": "So we can, if we can infer the structure of some this part here, then we can probably use this chunk of the lattice as the basis for our representation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How do you do this?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "We fix a finite set of contexts F and we define a complete letter lattice just restricted in this way.",
                    "label": 1
                },
                {
                    "sent": "Concatenations to find us before, but it's no longer a resituated latters because it stops being associative.",
                    "label": 0
                },
                {
                    "sent": "OK, once we have a fine.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lattice, let's take this example.",
                    "label": 0
                },
                {
                    "sent": "Here we have 8, then beat then.",
                    "label": 0
                },
                {
                    "sent": "We take this set of 6 features here.",
                    "label": 0
                },
                {
                    "sent": "This gives us this finite lattice.",
                    "label": 0
                },
                {
                    "sent": "Which again, we can infer efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But what we find is that it's no longer associative, right?",
                    "label": 0
                },
                {
                    "sent": "If we could concatenation of the concepts of unv isn't may not no longer necessary, equal this.",
                    "label": 0
                },
                {
                    "sent": "So if we take 800 and we concatenate with beta 100, this is just top connecting with the top which is taught, which is not the same as the concept of the 101 hundred, which is a nice concept on the thing.",
                    "label": 0
                },
                {
                    "sent": "So we lose associativity.",
                    "label": 0
                },
                {
                    "sent": "But we can still.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can still use this as an upper bound there.",
                    "label": 1
                },
                {
                    "sent": "Can't we recursively compute our concept?",
                    "label": 0
                },
                {
                    "sent": "We know that.",
                    "label": 0
                },
                {
                    "sent": "AB is going to be less than equal to the concatenation of this.",
                    "label": 0
                },
                {
                    "sent": "This we can prove so.",
                    "label": 0
                },
                {
                    "sent": "AB is going to be less than equal to the meat of both of these.",
                    "label": 0
                },
                {
                    "sent": "So generally if we have.",
                    "label": 1
                },
                {
                    "sent": "We know the concept W here is going to be less than or equal to this computation here.",
                    "label": 0
                },
                {
                    "sent": "We take all the splits of W. We compute the lower bound from each of these things and we meet them together.",
                    "label": 0
                },
                {
                    "sent": "So we just have.",
                    "label": 0
                },
                {
                    "sent": "This is a recursive computation.",
                    "label": 0
                },
                {
                    "sent": "From this lattice.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we have a lattice representation.",
                    "label": 0
                },
                {
                    "sent": "We take this lattice here and we have this recursive computation where we can again do this using standard dynamic programming techniques in cubic time.",
                    "label": 0
                },
                {
                    "sent": "And we define the language defined by this representation to be just the set of all strings where we predict that it has this sentence feature.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the power of the representation.",
                    "label": 1
                },
                {
                    "sent": "So let's just say let's the upper bound is really the set of all languages L such there's a finite set of context such that L is equal to the language defined by the lattice defined by the language, and it includes all regular languages.",
                    "label": 1
                },
                {
                    "sent": "Some, but not all, context free languages and some non context free languages, including crucially, some of the classic examples of non context freeness that we see in natural language.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So monotonicity as we increase the set of features F. The language increases, so this now we're doing a slight different thing because we are generalizing much more radically as we increase the set of features, the language increases in this case, so any large enough set of features is OK.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But obviously.",
                    "label": 0
                },
                {
                    "sent": "Where we have really is a set of.",
                    "label": 0
                },
                {
                    "sent": "We need to consider the case where we have some finite set of features.",
                    "label": 0
                },
                {
                    "sent": "We have some fun instead of features F we have a finite set of strings K we're going to write down.",
                    "label": 1
                },
                {
                    "sent": "We're going to find our lattice be of KLF here as before.",
                    "label": 0
                },
                {
                    "sent": "This is getting a complete lattice.",
                    "label": 0
                },
                {
                    "sent": "There are a few technical wrinkles that we may not be able to define concatenation.",
                    "label": 0
                },
                {
                    "sent": "Perfectly.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if concatenation is is defined, we say it's closed under concatenation.",
                    "label": 0
                },
                {
                    "sent": "That means that we have essentially a complete model for our fragment.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm I'm aware of the time limits.",
                    "label": 0
                },
                {
                    "sent": "I'm I'm accelerating a little bit.",
                    "label": 0
                },
                {
                    "sent": "And again we have a second monotonicity limit here, which basically says that as as we increase the set of strings, the language here decreases.",
                    "label": 0
                },
                {
                    "sent": "For any sufficiently large K, though, we can recover exactly the is going to be isomorphic to the correct lattice.",
                    "label": 1
                },
                {
                    "sent": "OK, So what we have with this lattice then is we have.",
                    "label": 0
                },
                {
                    "sent": "We have the true infinite lattice for our language.",
                    "label": 0
                },
                {
                    "sent": "We define a set of features that will carve up a relevant chunk of that lattice.",
                    "label": 0
                },
                {
                    "sent": "We take a finite set of strings and we can easily compute this lattice here from that, given that lattice, this gives us immediately a representation for for the language we're interested.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that the last words of the result, yeah?",
                    "label": 0
                },
                {
                    "sent": "Kate gets larger.",
                    "label": 0
                },
                {
                    "sent": "The second concept gets smaller.",
                    "label": 0
                },
                {
                    "sent": "What happens as K as K gets larger?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but so basic as K increases the number.",
                    "label": 0
                },
                {
                    "sent": "If you have a particular set, the number of context that is shared by all the elements of K can only decrease right?",
                    "label": 0
                },
                {
                    "sent": "So as you increase the set of examples, you're decreasing the set of features that they all have.",
                    "label": 0
                },
                {
                    "sent": "So that means your predictions then will move.",
                    "label": 0
                },
                {
                    "sent": "We will move down in in the representation, so if we have some language.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a little difficult to spell if, as the.",
                    "label": 0
                },
                {
                    "sent": "If you have some language like this, for example here.",
                    "label": 0
                },
                {
                    "sent": "Hey there be enterin we we we we remove a single element from it 81 hundred 100 if you only see short strings of this, you're going to assume it's this language.",
                    "label": 0
                },
                {
                    "sent": "When you then see a large enough string that you notice this is missing, then your language is going to contract.",
                    "label": 0
                },
                {
                    "sent": "OK, so as you as the set of strings, K increases the language you defined.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Faces so again and the search problem is trivial, getting these two monotonicity lemmas.",
                    "label": 1
                },
                {
                    "sent": "It's quite straightforward to find a search to find a suitable suitable set of K&F.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wrap it up.",
                    "label": 0
                },
                {
                    "sent": "So really two points here.",
                    "label": 0
                },
                {
                    "sent": "First of all, we're moving away from the Chomsky hierarchy, but that's not necessarily a problem.",
                    "label": 1
                },
                {
                    "sent": "There's an interesting quote from Chomsky which where he says the concept of phrase structure grammar, and he's talking about his his paper 1953 that we sort of kicked off formal language theory was explicitly designed to express the richest system that could reasonably be expected to result from the application of Paris.",
                    "label": 1
                },
                {
                    "sent": "Correct procedures to corpus.",
                    "label": 0
                },
                {
                    "sent": "So his goal in designing context free grammars also come up with a learnable representation.",
                    "label": 0
                },
                {
                    "sent": "That was what he was explicitly trying to do, and not just learnable learnable through distributional learning.",
                    "label": 1
                },
                {
                    "sent": "So if distributional learning doesn't work on CFG's, you know, then this isn't a problem with distribution learning.",
                    "label": 0
                },
                {
                    "sent": "It's a problem with the context free grammars.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We abandoned some of the representational assumptions of context free grammars and particularly the structural descriptions we get are no longer going to be necessary trees.",
                    "label": 0
                },
                {
                    "sent": "They are going to in general be Dags or some slightly richer structure directed acyclic graphs.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so um.",
                    "label": 0
                },
                {
                    "sent": "That I have to say something about statistical modeling.",
                    "label": 0
                },
                {
                    "sent": "Causal will Lynch me.",
                    "label": 0
                },
                {
                    "sent": "So we have these efficient algorithms for constructing representations and I've kind of completely ignored all the problems of sparsity and noise and so much of the the very serious problems to learning natural language.",
                    "label": 1
                },
                {
                    "sent": "What are we talking realistically?",
                    "label": 0
                },
                {
                    "sent": "What we're going to have is some large, very large, very noisy corpus.",
                    "label": 1
                },
                {
                    "sent": "We need to replace the assumption of these entire context by partial context, typically adjacent words or word classes.",
                    "label": 1
                },
                {
                    "sent": "You can use some sort of.",
                    "label": 0
                },
                {
                    "sent": "Statistical clustering to fill in the blocks in your in your matrix and the model parameters.",
                    "label": 0
                },
                {
                    "sent": "Can I think one of the exciting things from a statistical modeling point of view, is the model params can represent either very specific or very abstract combinatorial properties.",
                    "label": 0
                },
                {
                    "sent": "You have various positions in the lattice where you can attach your parameters.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the final slide.",
                    "label": 0
                },
                {
                    "sent": "So linguistic one Ray Jackendoff wrote a paper called Alternative Minimalist Conceptions of Language, and he identified three constraints that we really want from a theory of language, the descriptors, constraint.",
                    "label": 0
                },
                {
                    "sent": "The classical languages must be sufficiently rich to represent natural language is a learnability constraint, so we have to have some plausible story about how these representations could be constructed.",
                    "label": 1
                },
                {
                    "sent": "An an evolutionary constraint.",
                    "label": 0
                },
                {
                    "sent": "You mustn't posit some evolutionary implausible universal grammar.",
                    "label": 0
                },
                {
                    "sent": "And I think potentially here we have maybe not these specific models that I'm talking about, but I think we have a research direction here which could potentially satisfy all three of these criteria.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks for listening.",
                    "label": 0
                },
                {
                    "sent": "So what languages have you tried your algorithm?",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So we've done some experiments where we generate random context free grammars.",
                    "label": 1
                },
                {
                    "sent": "Generate a sample of strings from the grammar and then.",
                    "label": 0
                },
                {
                    "sent": "Test based on the the set of observed substring instead of reserve context constructor grammar from that and then test it and that works pretty well, but it's very difficult to tell to test over generalization.",
                    "label": 0
                },
                {
                    "sent": "I think we haven't quite determined whether it over generalizes slightly 'cause it's quite difficult to generate plausible synthetic examples.",
                    "label": 0
                },
                {
                    "sent": "Mention that this doesn't is not people running all comics, yeah?",
                    "label": 0
                },
                {
                    "sent": "Finding Contacts, yeah.",
                    "label": 0
                },
                {
                    "sent": "Do you apply this to say he's an important minutes, right?",
                    "label": 0
                },
                {
                    "sent": "So one of the problems in designing domain specific languages that the user has to define grammars, context free grammars, and learning how this whole thing works is Big Barrier.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we've been looking for tools that can allow people to spread sample programs and their target language and then infer the property.",
                    "label": 0
                },
                {
                    "sent": "So you mentioned earlier that most programming languages would be finding context.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so have you tried this on anyway?",
                    "label": 0
                },
                {
                    "sent": "I haven't tried any programming just now.",
                    "label": 0
                },
                {
                    "sent": "I mean my my own research interests abroad linguistic, but I I do accept that there are other people who are interested in other things.",
                    "label": 0
                },
                {
                    "sent": "I mean, The thing is that programming languages designed in some way to resemble natural languages, so they're easy for humans to read, and so I think they will inherit some of the properties of natural languages have.",
                    "label": 0
                },
                {
                    "sent": "We need stuff so.",
                    "label": 0
                },
                {
                    "sent": "You know the the very terminology that you used leeches.",
                    "label": 0
                },
                {
                    "sent": "Yeah, suggest that maybe in fact actually tracking the full context actually necessary.",
                    "label": 0
                },
                {
                    "sent": "So I can imagine some constraints that if you did decide to abstract away from the context, features that you might need in general, do you think that's actually possible and.",
                    "label": 0
                },
                {
                    "sent": "How much does the learner have to remember about the left and the right context?",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a very good question.",
                    "label": 0
                },
                {
                    "sent": "I so I think I think one of the things we can take from like Klein and Manning's work, is that also from the Brown work is that in certain English, just the local context.",
                    "label": 0
                },
                {
                    "sent": "Just one word on the left and one more on the right gives you a very strong clear.",
                    "label": 0
                },
                {
                    "sent": "I think it may not be true for all languages, so I think particularly free word order languages like maybe a new Polish.",
                    "label": 0
                },
                {
                    "sent": "I have a feeling that sort of stuff might not work so well.",
                    "label": 0
                },
                {
                    "sent": "'cause you might need to look at.",
                    "label": 0
                },
                {
                    "sent": "A longer context, but I certainly think for English not very narrow, context would probably be sufficient one or possibly 2 words on either side if you could even try and expect characteristics of exactly.",
                    "label": 0
                },
                {
                    "sent": "Exactly, I mean you can use any any any statistic you want to extract from them to compare the similarity of things.",
                    "label": 0
                },
                {
                    "sent": "So if you try to learn a language from really.",
                    "label": 0
                },
                {
                    "sent": "There's Anna construction.",
                    "label": 0
                },
                {
                    "sent": "Different Curry could occur for syntactically.",
                    "label": 0
                },
                {
                    "sent": "So how do you change this?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's a that's a subtle question, um?",
                    "label": 0
                },
                {
                    "sent": "So what I've been so the full model here is, I assume we are given some you know L which is a subset of Sigma star, right?",
                    "label": 0
                },
                {
                    "sent": "And I'm being kind of I don't really think of that.",
                    "label": 0
                },
                {
                    "sent": "Necessary is meaning grammatical, just grammatical sentences.",
                    "label": 0
                },
                {
                    "sent": "It's got to be much more about acceptability and you gotta let some in practice there's going to be some semantic constraints in the.",
                    "label": 0
                },
                {
                    "sent": "So I did do some experiments, but some sort of data exploration exercises on the Google corpus trying to just seeing if you throw huge amounts of data at it, whether there is sufficient.",
                    "label": 0
                },
                {
                    "sent": "Rich and what that did bring home to me is that the distribution of the data is is in a way very, very heavily influenced by semantic factors rather than just the syntactic factors.",
                    "label": 0
                },
                {
                    "sent": "So there are two ways to two strategies to deal with that.",
                    "label": 0
                },
                {
                    "sent": "One is that you do some preliminary kind of syntactic clustering first, so you replace words by some syntactically different, like Brown style clusters and try and work on that, or Alternatively.",
                    "label": 0
                },
                {
                    "sent": "I forgot the second strategy is, so the first strategy is probably the best one then too.",
                    "label": 0
                },
                {
                    "sent": "That may be very nice, so there is clearly a job.",
                    "label": 0
                },
                {
                    "sent": "Security is the structure of the sentence itself, yeah?",
                    "label": 0
                },
                {
                    "sent": "This together yeah.",
                    "label": 0
                },
                {
                    "sent": "However, if you have past three, you can do confirmations.",
                    "label": 0
                },
                {
                    "sent": "If you have a transformation Grandma, you can.",
                    "label": 0
                },
                {
                    "sent": "So which one is easier to know?",
                    "label": 0
                },
                {
                    "sent": "Like transformation grammars are very very hard to learn I think.",
                    "label": 0
                },
                {
                    "sent": "So I mean if you look at I'm not really interested in the transformations between sentences, but I think the structural representations between sentences are best represented through similarities between the structural things rather than representing explicitly.",
                    "label": 0
                },
                {
                    "sent": "But insofar as there is formal work on the learning of transformational grammars, it's.",
                    "label": 1
                },
                {
                    "sent": "That they're pretty hard to learn, I think.",
                    "label": 0
                },
                {
                    "sent": "Yes, possibly, but I'm I'm not, I'm I don't think it's necessary to learn transformations between sentences.",
                    "label": 0
                },
                {
                    "sent": "I think you need to learn the structure of individual sentences and you need to learn some way of doing semantic interpretation with those structures.",
                    "label": 0
                },
                {
                    "sent": "And I don't.",
                    "label": 0
                },
                {
                    "sent": "I've never accepted the arguments that Harrison Chomsky put forward for kind of sentence transformations.",
                    "label": 0
                },
                {
                    "sent": "So maybe I'm not answering the right question.",
                    "label": 0
                },
                {
                    "sent": "I understand your position.",
                    "label": 0
                },
                {
                    "sent": "I would like to know why.",
                    "label": 0
                },
                {
                    "sent": "What's your intuition to take it that way?",
                    "label": 0
                },
                {
                    "sent": "Well, my intuition is that it would be very difficult to do that, and that it's not necessary, so that's that's why I have a feeling I'm not quite understanding your question there.",
                    "label": 0
                },
                {
                    "sent": "Maybe you could talk about afterwards.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}