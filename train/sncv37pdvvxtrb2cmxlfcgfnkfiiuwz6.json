{
    "id": "sncv37pdvvxtrb2cmxlfcgfnkfiiuwz6",
    "title": "Improved Testing of Low Rank Matrices",
    "info": {
        "author": [
            "Yi Li, Max Planck Institute for Informatics, Max Planck Institute"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_li_low_rank_matrices/",
    "segmentation": [
        [
            "Thank you and this is a joint work with showing you one now at Harvard and they be David Woodruff and IBM."
        ],
        [
            "So what is property testing?",
            "Here we can see the property P on matrices.",
            "This property may depend on the parameter D, so the parameter D is independent of and it is very much smaller than N and wish to make queries on the entry of the input matrix in order to distinguish the following two cases.",
            "The first case is that the input matrix satisfies this property P. The other case is that input matrix is absolute far from satisfying.",
            "Property P, that is, we need to make we need to change in the epsilon fraction of entries in the matrix so that it can satisfy the property P and the number of queries we make is called query complexity, which we hope is independent of North.",
            "But it can depend on D and epsilon and all those testing algorithms are randomized, But the randomness is in the algorithm itself, so we do not make any assumption on the input.",
            "That is, the algorithm should succeed with good probability even on the worst case input."
        ],
        [
            "So we consider to kind of properties.",
            "One is ranked, the other is stable rank.",
            "So for the rank property the property simply says that the rank of the input matrix is at most some parameter D. This problem was first proposed by crowds gameron session in 2003, and here we consider the entry access model.",
            "That is, every query is to read exactly 1 entry of the matrix and there are two kinds of queries, one adaptive, the other non adaptive.",
            "For nonadaptive queries, we fixed the query positions at the beginning before we see the matrix and once we read once, we see the matrix.",
            "We read the corresponding entries for the adaptive case.",
            "The next query position could depend on the outcome of all previous query positions.",
            "So if the query complexity are the same, then nonadaptive queries are much stronger algorithm.",
            "This has potentially a application related.",
            "To robust PCA in robots PCA, we write an input matrix A as a sum of low rank matrix and the sparse matrix, and if we consider this ranks property testing.",
            "Problem we can determine whether a is close to a low rank matrix before running robust PCA.",
            "Remember that the query complexity only depends on D, but not to end well.",
            "PCA running times that depends on dimension of the matrix.",
            "So if we can decide this, we may save some time not running robots, PCA and in the same people of crowds gamer in session.",
            "In 2003 the show an algorithm that.",
            "Takes Nonadaptive queries of.",
            "These squared over epsilon squared entries of the matrix."
        ],
        [
            "And another.",
            "Property we consider is stable rank, which is defined as squared for Brittany's norm of a divided by squared operator norm of A and the property is simply to test whether this rank the stable rank of the matrix is the most parameter D. But here we consider the problem in real access model.",
            "That means each query reads an entire row of the matrix.",
            "But here we need to impose we need to be careful.",
            "We need to impose some additional constraint.",
            "For instance, we need to assume that each row has two normal most one cause one can change a single entry to an arbitrarily large value so that the stable rank is arbitrarily close to 1.",
            "So we have to put upper bounds on rownames.",
            "And another thing is what the epsilon phonics mean.",
            "Here the natural idea is that.",
            "One needs to change at least epsilon fraction of rose to make it to make the stable rank.",
            "Mostly.",
            "However, we observe that the change 1 / D fraction of rose to the same unit vector always reduces the stable rank to almost D. So the correct which makes more sense.",
            "The correct definition of Epsilon fan is here is that one needs to change at least epsilon over D fraction of rose to make the matrix.",
            "Having to make the matrix have stable rank mostly instead of.",
            "Epsilon fraction of rose."
        ],
        [
            "Here are our results.",
            "So for the rank problem with entry access model and we show an adaptive query tester that reads the squared over epsilon entries, which is in one over epsilon factor less than nonadaptive ones by crowd gamer than Sass.",
            "And Meanwhile we also prove the lower bound of one over epsilon entries.",
            "This means that our upper bound is optimal up to constant when D is a constant and we also tried to improve nonadaptive.",
            "Queries improve the lower bound of log one over epsilon over epsilon.",
            "Meanwhile we have an upper bound of log squared, one over epsilon overlap zone, but it's only for D = 1.",
            "But this is it's only suboptimal by a log one over epsilon factor.",
            "So for stable rank we consider the problem in real access model and its non adaptive queries we have an upper bound of D over epsilon squared times polylog N rows and the lower bound is the overlaps and rose so.",
            "An upper bound is just sub optimal by log factors."
        ],
        [
            "So now let's see how to actually test the rank.",
            "So the credits gamer and Sessions algorithm.",
            "It's simple, they simply choose random D over epsilon times the over epsilon submatrix of the submatrix of the input matrix.",
            "So if this matrix has rank listy, it reports that it's at least epsilon far from having ranked most D. If the sub matrix has rank amongst the, then it says it has rank most D. Adaptive case we don't choose this submatrix at one time is that we grow this submatrix in these steps, so we start from a submatrix one by one, somebody shakes and each step we grow the dimension by one.",
            "So here, as illustrated, we suppose we have 80 by teacher matrix and then we choose a pivot entry in remaining positions randomly.",
            "Ann, and add the corresponding row and column to this TI by teaser matrix.",
            "So we obtain a T + 1 by T + 1 submatrix.",
            "Then we see if this new sub matrix has a rank that is 1 larger than the previous one.",
            "If it is, we keep it, otherwise we choose another pivot entry and we can show that we can try only one over epsilon people.",
            "The entries in each step to find a pivot entry that really increases the low rank.",
            "So so then, so at each step we read T over epsilon entries and summing from summing T from one 2D we get TD squared over epsilon entries.",
            "That's a number of entries we read in total."
        ],
        [
            "How about so?",
            "We also try to improve on nonadaptive queries.",
            "We try to reduce the number of queries D squared over epsilon squared down to D squared over epsilon.",
            "Here here is to choose different shapes of rectangles instead of a square rectangle.",
            "So we start with a rectangle with a submatrix of the overeater times D and each time we shrink the number of rows by half and we double the number of columns, so up to until we reach D times the overeater.",
            "So there are log one overeater different shapes here, and if we choose eaters such that it'll times log one of equals epsilon and we read a total number of log one over Eater.",
            "Divided by Eater entries, but however we only proved this algorithm works for D = 1 Despite the fact we think it should work for the greater than one."
        ],
        [
            "So the next thing is how to test the stable rank.",
            "Here we first choose Q rows of a uniformly forming a smaller matrix, a~ where Q is about the correct quantity, it's D over epsilon squared times polylog.",
            "Now we see that this normalized for business Norm is a good estimate of the original message of the fabulous norm of the original matrix, and if there's if they estimate it's small, then we directly claim that a hashtable rank at most D. Otherwise we do the standard thing.",
            "We simply compare an estimate of the operator norm with.",
            "Frobenius norm squared over D. Here we need to know how good the estimate is.",
            "They estimate it simply normalized operator norm of a tilter.",
            "To show that it is a good estimate of the operator norm of the original matrix, we need results from McDonald's Mail which says that when Q is the least the stable rank of a over epsilon squared times some login factors, there is normalized operator norm is fairly a good.",
            "Ultimate is a Jolly good estimate of the operator norm of a.",
            "So this solved the case when the stable stable rank of a it's small, but when the stable rank of A is large, we invoke.",
            "Result by Roman machining that upper bounds the operator norm of this submatrix a tilter.",
            "Because in this case, what we want to see is that the operator norm squared is at most X / D, so an upper bound.",
            "Operator norm of a~ suffices."
        ],
        [
            "And.",
            "And these are theoretical bounds, but Erica bounds are often pessimistic in practice.",
            "For instance, if we choke, if we choose epsilon to be 0.01, then all the theoretical bounds prescribe these word over epsilon and even epsilon squared, that is 2 too many entries.",
            "It could be even be more than the total number of entries in the Matrix.",
            "So here we choose epsilon.",
            "In our experiments, we choose epsilon to be 0.01 and success probability.",
            "Showed to be 0.9 is we want the algorithm to succeed with probability at least 0.9.",
            "For the stable rank problem, we used the University of Florida sparse matrix collection and we run the algorithm on all the matrices of dimension between 100 and thousand with the equals 2, three and five.",
            "There are about 200 matrices in each case.",
            "And our results shows that we need only to sample about 10% to 50% of the rows.",
            "And this is significantly smaller than the bound.",
            "Then what the theoretical bounds prescribes?",
            "For the rank problem we test we use synthetic data and shows that the number of adaptive queries is only 10% of the number of non adaptive queries when D is some small constant 125."
        ],
        [
            "So last I like to mention two major open problems.",
            "So as I mentioned before, we have a new approach to nonadaptive queries, but we only prove the correctness for the equals one, but we think it should.",
            "Be true for D greater than one.",
            "And any improvement for on the greater than one should probably based on testing different shapes, as as we thought and the other thing is to close the gap between the upper and lower bound as before, the lower bound does not depend on D. While we hope that we can prove a lower bound with dependence on the IT should be quadratic dependence.",
            "But even a linear dependence is a good process.",
            "Is a good progress OK?",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you and this is a joint work with showing you one now at Harvard and they be David Woodruff and IBM.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is property testing?",
                    "label": 1
                },
                {
                    "sent": "Here we can see the property P on matrices.",
                    "label": 0
                },
                {
                    "sent": "This property may depend on the parameter D, so the parameter D is independent of and it is very much smaller than N and wish to make queries on the entry of the input matrix in order to distinguish the following two cases.",
                    "label": 0
                },
                {
                    "sent": "The first case is that the input matrix satisfies this property P. The other case is that input matrix is absolute far from satisfying.",
                    "label": 1
                },
                {
                    "sent": "Property P, that is, we need to make we need to change in the epsilon fraction of entries in the matrix so that it can satisfy the property P and the number of queries we make is called query complexity, which we hope is independent of North.",
                    "label": 0
                },
                {
                    "sent": "But it can depend on D and epsilon and all those testing algorithms are randomized, But the randomness is in the algorithm itself, so we do not make any assumption on the input.",
                    "label": 0
                },
                {
                    "sent": "That is, the algorithm should succeed with good probability even on the worst case input.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we consider to kind of properties.",
                    "label": 0
                },
                {
                    "sent": "One is ranked, the other is stable rank.",
                    "label": 0
                },
                {
                    "sent": "So for the rank property the property simply says that the rank of the input matrix is at most some parameter D. This problem was first proposed by crowds gameron session in 2003, and here we consider the entry access model.",
                    "label": 0
                },
                {
                    "sent": "That is, every query is to read exactly 1 entry of the matrix and there are two kinds of queries, one adaptive, the other non adaptive.",
                    "label": 0
                },
                {
                    "sent": "For nonadaptive queries, we fixed the query positions at the beginning before we see the matrix and once we read once, we see the matrix.",
                    "label": 0
                },
                {
                    "sent": "We read the corresponding entries for the adaptive case.",
                    "label": 0
                },
                {
                    "sent": "The next query position could depend on the outcome of all previous query positions.",
                    "label": 0
                },
                {
                    "sent": "So if the query complexity are the same, then nonadaptive queries are much stronger algorithm.",
                    "label": 0
                },
                {
                    "sent": "This has potentially a application related.",
                    "label": 0
                },
                {
                    "sent": "To robust PCA in robots PCA, we write an input matrix A as a sum of low rank matrix and the sparse matrix, and if we consider this ranks property testing.",
                    "label": 0
                },
                {
                    "sent": "Problem we can determine whether a is close to a low rank matrix before running robust PCA.",
                    "label": 1
                },
                {
                    "sent": "Remember that the query complexity only depends on D, but not to end well.",
                    "label": 0
                },
                {
                    "sent": "PCA running times that depends on dimension of the matrix.",
                    "label": 0
                },
                {
                    "sent": "So if we can decide this, we may save some time not running robots, PCA and in the same people of crowds gamer in session.",
                    "label": 0
                },
                {
                    "sent": "In 2003 the show an algorithm that.",
                    "label": 0
                },
                {
                    "sent": "Takes Nonadaptive queries of.",
                    "label": 0
                },
                {
                    "sent": "These squared over epsilon squared entries of the matrix.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And another.",
                    "label": 0
                },
                {
                    "sent": "Property we consider is stable rank, which is defined as squared for Brittany's norm of a divided by squared operator norm of A and the property is simply to test whether this rank the stable rank of the matrix is the most parameter D. But here we consider the problem in real access model.",
                    "label": 0
                },
                {
                    "sent": "That means each query reads an entire row of the matrix.",
                    "label": 0
                },
                {
                    "sent": "But here we need to impose we need to be careful.",
                    "label": 0
                },
                {
                    "sent": "We need to impose some additional constraint.",
                    "label": 0
                },
                {
                    "sent": "For instance, we need to assume that each row has two normal most one cause one can change a single entry to an arbitrarily large value so that the stable rank is arbitrarily close to 1.",
                    "label": 1
                },
                {
                    "sent": "So we have to put upper bounds on rownames.",
                    "label": 0
                },
                {
                    "sent": "And another thing is what the epsilon phonics mean.",
                    "label": 0
                },
                {
                    "sent": "Here the natural idea is that.",
                    "label": 0
                },
                {
                    "sent": "One needs to change at least epsilon fraction of rose to make it to make the stable rank.",
                    "label": 0
                },
                {
                    "sent": "Mostly.",
                    "label": 1
                },
                {
                    "sent": "However, we observe that the change 1 / D fraction of rose to the same unit vector always reduces the stable rank to almost D. So the correct which makes more sense.",
                    "label": 1
                },
                {
                    "sent": "The correct definition of Epsilon fan is here is that one needs to change at least epsilon over D fraction of rose to make the matrix.",
                    "label": 0
                },
                {
                    "sent": "Having to make the matrix have stable rank mostly instead of.",
                    "label": 0
                },
                {
                    "sent": "Epsilon fraction of rose.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are our results.",
                    "label": 0
                },
                {
                    "sent": "So for the rank problem with entry access model and we show an adaptive query tester that reads the squared over epsilon entries, which is in one over epsilon factor less than nonadaptive ones by crowd gamer than Sass.",
                    "label": 0
                },
                {
                    "sent": "And Meanwhile we also prove the lower bound of one over epsilon entries.",
                    "label": 1
                },
                {
                    "sent": "This means that our upper bound is optimal up to constant when D is a constant and we also tried to improve nonadaptive.",
                    "label": 0
                },
                {
                    "sent": "Queries improve the lower bound of log one over epsilon over epsilon.",
                    "label": 1
                },
                {
                    "sent": "Meanwhile we have an upper bound of log squared, one over epsilon overlap zone, but it's only for D = 1.",
                    "label": 0
                },
                {
                    "sent": "But this is it's only suboptimal by a log one over epsilon factor.",
                    "label": 0
                },
                {
                    "sent": "So for stable rank we consider the problem in real access model and its non adaptive queries we have an upper bound of D over epsilon squared times polylog N rows and the lower bound is the overlaps and rose so.",
                    "label": 1
                },
                {
                    "sent": "An upper bound is just sub optimal by log factors.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's see how to actually test the rank.",
                    "label": 0
                },
                {
                    "sent": "So the credits gamer and Sessions algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's simple, they simply choose random D over epsilon times the over epsilon submatrix of the submatrix of the input matrix.",
                    "label": 0
                },
                {
                    "sent": "So if this matrix has rank listy, it reports that it's at least epsilon far from having ranked most D. If the sub matrix has rank amongst the, then it says it has rank most D. Adaptive case we don't choose this submatrix at one time is that we grow this submatrix in these steps, so we start from a submatrix one by one, somebody shakes and each step we grow the dimension by one.",
                    "label": 0
                },
                {
                    "sent": "So here, as illustrated, we suppose we have 80 by teacher matrix and then we choose a pivot entry in remaining positions randomly.",
                    "label": 0
                },
                {
                    "sent": "Ann, and add the corresponding row and column to this TI by teaser matrix.",
                    "label": 0
                },
                {
                    "sent": "So we obtain a T + 1 by T + 1 submatrix.",
                    "label": 1
                },
                {
                    "sent": "Then we see if this new sub matrix has a rank that is 1 larger than the previous one.",
                    "label": 0
                },
                {
                    "sent": "If it is, we keep it, otherwise we choose another pivot entry and we can show that we can try only one over epsilon people.",
                    "label": 0
                },
                {
                    "sent": "The entries in each step to find a pivot entry that really increases the low rank.",
                    "label": 1
                },
                {
                    "sent": "So so then, so at each step we read T over epsilon entries and summing from summing T from one 2D we get TD squared over epsilon entries.",
                    "label": 0
                },
                {
                    "sent": "That's a number of entries we read in total.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How about so?",
                    "label": 0
                },
                {
                    "sent": "We also try to improve on nonadaptive queries.",
                    "label": 1
                },
                {
                    "sent": "We try to reduce the number of queries D squared over epsilon squared down to D squared over epsilon.",
                    "label": 0
                },
                {
                    "sent": "Here here is to choose different shapes of rectangles instead of a square rectangle.",
                    "label": 1
                },
                {
                    "sent": "So we start with a rectangle with a submatrix of the overeater times D and each time we shrink the number of rows by half and we double the number of columns, so up to until we reach D times the overeater.",
                    "label": 0
                },
                {
                    "sent": "So there are log one overeater different shapes here, and if we choose eaters such that it'll times log one of equals epsilon and we read a total number of log one over Eater.",
                    "label": 0
                },
                {
                    "sent": "Divided by Eater entries, but however we only proved this algorithm works for D = 1 Despite the fact we think it should work for the greater than one.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next thing is how to test the stable rank.",
                    "label": 0
                },
                {
                    "sent": "Here we first choose Q rows of a uniformly forming a smaller matrix, a~ where Q is about the correct quantity, it's D over epsilon squared times polylog.",
                    "label": 0
                },
                {
                    "sent": "Now we see that this normalized for business Norm is a good estimate of the original message of the fabulous norm of the original matrix, and if there's if they estimate it's small, then we directly claim that a hashtable rank at most D. Otherwise we do the standard thing.",
                    "label": 1
                },
                {
                    "sent": "We simply compare an estimate of the operator norm with.",
                    "label": 0
                },
                {
                    "sent": "Frobenius norm squared over D. Here we need to know how good the estimate is.",
                    "label": 0
                },
                {
                    "sent": "They estimate it simply normalized operator norm of a tilter.",
                    "label": 0
                },
                {
                    "sent": "To show that it is a good estimate of the operator norm of the original matrix, we need results from McDonald's Mail which says that when Q is the least the stable rank of a over epsilon squared times some login factors, there is normalized operator norm is fairly a good.",
                    "label": 1
                },
                {
                    "sent": "Ultimate is a Jolly good estimate of the operator norm of a.",
                    "label": 0
                },
                {
                    "sent": "So this solved the case when the stable stable rank of a it's small, but when the stable rank of A is large, we invoke.",
                    "label": 0
                },
                {
                    "sent": "Result by Roman machining that upper bounds the operator norm of this submatrix a tilter.",
                    "label": 0
                },
                {
                    "sent": "Because in this case, what we want to see is that the operator norm squared is at most X / D, so an upper bound.",
                    "label": 0
                },
                {
                    "sent": "Operator norm of a~ suffices.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And these are theoretical bounds, but Erica bounds are often pessimistic in practice.",
                    "label": 0
                },
                {
                    "sent": "For instance, if we choke, if we choose epsilon to be 0.01, then all the theoretical bounds prescribe these word over epsilon and even epsilon squared, that is 2 too many entries.",
                    "label": 0
                },
                {
                    "sent": "It could be even be more than the total number of entries in the Matrix.",
                    "label": 0
                },
                {
                    "sent": "So here we choose epsilon.",
                    "label": 0
                },
                {
                    "sent": "In our experiments, we choose epsilon to be 0.01 and success probability.",
                    "label": 0
                },
                {
                    "sent": "Showed to be 0.9 is we want the algorithm to succeed with probability at least 0.9.",
                    "label": 0
                },
                {
                    "sent": "For the stable rank problem, we used the University of Florida sparse matrix collection and we run the algorithm on all the matrices of dimension between 100 and thousand with the equals 2, three and five.",
                    "label": 1
                },
                {
                    "sent": "There are about 200 matrices in each case.",
                    "label": 1
                },
                {
                    "sent": "And our results shows that we need only to sample about 10% to 50% of the rows.",
                    "label": 1
                },
                {
                    "sent": "And this is significantly smaller than the bound.",
                    "label": 0
                },
                {
                    "sent": "Then what the theoretical bounds prescribes?",
                    "label": 0
                },
                {
                    "sent": "For the rank problem we test we use synthetic data and shows that the number of adaptive queries is only 10% of the number of non adaptive queries when D is some small constant 125.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So last I like to mention two major open problems.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned before, we have a new approach to nonadaptive queries, but we only prove the correctness for the equals one, but we think it should.",
                    "label": 0
                },
                {
                    "sent": "Be true for D greater than one.",
                    "label": 0
                },
                {
                    "sent": "And any improvement for on the greater than one should probably based on testing different shapes, as as we thought and the other thing is to close the gap between the upper and lower bound as before, the lower bound does not depend on D. While we hope that we can prove a lower bound with dependence on the IT should be quadratic dependence.",
                    "label": 1
                },
                {
                    "sent": "But even a linear dependence is a good process.",
                    "label": 0
                },
                {
                    "sent": "Is a good progress OK?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}