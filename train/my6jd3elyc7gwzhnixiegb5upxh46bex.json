{
    "id": "my6jd3elyc7gwzhnixiegb5upxh46bex",
    "title": "Joint Max Margin and Max Entropy Learning of Graphical Models",
    "info": {
        "author": [
            "Eric P. Xing, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_xing_jmmelgm/",
    "segmentation": [
        [
            "Thank you very much for the invitation that very nice introduction.",
            "So I'm going to share with you some of our recent work.",
            "On a slightly fancier learning paradigm on a generic class of graphical models, mainly generalized linear models.",
            "So the motivating prob."
        ],
        [
            "Is again this structured input output problem, which I'm going to introduce briefly, so in the general predicted problem which are unstructured, we basically call it a classification problem, and it is a very simple setting where you have a set of input features and then you're going to make a."
        ],
        [
            "Prediction on that the structured input output prediction problem is the innocence similar, except that the output becomes a sequence or a matrix or predictors which are Inter correlated and we can find various reasons about why this is a good thing to do, because the labels such as labeling image parts or part of speech is should be dependent on each other.",
            "If you want to make sense, they global sense out of the prediction.",
            "So how people approach this problem again?",
            "There are some classical approaches for unstructured prediction problem which is built on some particular function.",
            "You're maximize that pretty function over your predictor and you get your prediction and you can have a very flexible design of this particular function.",
            "In cases age linear model and you have different ways of training that and here I highlight two ways of training which has been frequently used in our domain.",
            "One is to train a likelihood based loss function such as a conditional likelihood for the label given the input in something.",
            "For example like a logistic regression setting.",
            "Or you can train a base classifier.",
            "And here is the training procedure and the other very favorite parity, maybe more favored in certain areas.",
            "Training a discrete model based on a different loss function, known as the maximum margin, and by working through the algebra one can lend it himself at this loss function, which is the L2 norm of the weight vector of the decision boundary plus some slack variable.",
            "And then you optimize this problem in the space of a margin space constraint where the product margin must be bigger than some lower bounds."
        ],
        [
            "So there are pros and cons of these two approaches, and it is an extremely flexible design.",
            "And that leads us a direct way of promoting these models to structure input problem.",
            "So here again give you 2 very.",
            "Popular models in the structure input upper learning domain and the first model is called a conditional random field where you build a random field model, often a chain Markov model over the predicted labels, and then you still train A.",
            "A conditional probability solution of the output given the input.",
            "And here you can also train the whole thing through a maximum margin procedure, and if you stare at all this equation you will see no difference, at least symbolically of this loss functions to the unstructured version, except that now your feature design become different you can design fencing features such as features defined on pairwise labels or potentials that reflects a Markov dependencies between nodes.",
            "OK, you can even actually get some other.",
            "Our dependencies between features, so that's why this model is more flexible in a sense."
        ],
        [
            "Favored by many people, there are some debates in our community about which procedure is better.",
            "Some people felt that if you know how to deploy the conditional model, you can get amazing results, for example by introducing various regularization terms to increase sparsity or promote some bias.",
            "But some people believe that this so called maximum margin Markov network, which is structured version of SVM, has a very nice property 'cause they can be projected into a dual space where you.",
            "Worker use the kernel tracks.",
            "You can achieve something called the dual sparsity where your distributor is defined by a few support vectors.",
            "So it is really debatable you know about which one is better, and that's not my interest here.",
            "What I'm trying to promote is an approach which allow you to hopefully harvest the benefit of both approaches so that we can learn a model which is simultaneously pushing a version of a maximum margin type of loss function, but also doing this optimization in the space defined by margins so that.",
            "You know you hopefully can get benefit from both approaches, and again, I don't think I need to really motivate this problem.",
            "There are tons of problems of this nature, such as the people in computer vision doing image segmentation, or in my favorite area of population genetics where."
        ],
        [
            "You know you have this very high dimensional input of the human genome and you have this also very high dimensional output space, which is the disease phenotypes.",
            "Say you are sick, you have high blood pressure.",
            "You have all sorts of weird symptoms and the goal is to make these connections.",
            "Maybe also predict how good you are in the next few days.",
            "You know based on your genetic traits, again the key element here is that the prediction has to be structured because all these output vectors output variables are Inter dependent on each other.",
            "So as I said, we."
        ],
        [
            "I've always nice foundation for training the model and but there are some challenges.",
            "For example, one of the challenges that if you really want to go to high dimensional type of structure, input of model input output model, you have to worry about the sparsity because your data is never as many as I mentioned.",
            "You have to deal with.",
            "Therefore you need to regularize the model to achieve not only sparsity but also even model correctness and Secondly maybe in many domains you have prior information you know certain.",
            "Features are more important over others.",
            "How can you incorporate such features into the model?",
            "I guess people would go for the likelihood based approach because you have a natural way to write down the posterior, but I will tell today that you can do the same for the margin based approach.",
            "And so I can go on this list to list all these challenges.",
            "I'm going to hopefully touch a few of this point in my following implementation."
        ],
        [
            "So here is the outline of my presentation.",
            "I'm going to show you a general theory of the so called maximal entropy disclination Markov network and you will see why I give this weird very long term because it is really doing every element and listing here and I'm going to show you some special cases.",
            "One of the special cases of this general model is to introduce a particular type of regularization, so that's the model reduces to a very simple model known as the maximizing Markov network weapon Tasker.",
            "It's a very, very nice model.",
            "And we show that we can reduce our model to that model.",
            "Therefore using all the inference backwards over there.",
            "But if you don't want to do a trivial regulation or do some fancy regulation, you actually achieve some.",
            "You know maybe sparse version of the MCM cube network or some other desirable effects."
        ],
        [
            "You can also use this model to introduce hidden random variables.",
            "People talking about training partially absorbed as structured SVM.",
            "We can do it here as well, and by using a pretty flexible and generic algorithm.",
            "And finally I'm going to show you a few examples of how to apply.",
            "This approach is to tray really fancy models that will have to be Markov network.",
            "It could be a directed graph model such as a topic model.",
            "You can still trade topic modeling.",
            "Distributed sense.",
            "Alright, so let's see, as I mentioned that the maximum likelihood estimator and the maximizing.",
            "Estimator had this competing advantages and here I just list a few and want to point out the main reason people prefer one way or the other.",
            "Or perhaps what I highlighted in red.",
            "For example, in the likelihood based training you can do Bayesian inference, and that's a good thing because you know this Bayesian inference allow you to introduce these other priors.",
            "Unfined and Secondly because it is a probability model.",
            "It is very natural for you to introduce all the hidden random variables.",
            "We can always write down a marginal likelihood over the over the observations and they're using the same type of approach to fill in the hidden random variables.",
            "That's a very natural algorithm.",
            "But it's not so easy to do the same thing in a maximizing Markov network type of framework, but here you have some other advantages.",
            "In particular, you can do this kernel trick, cause the operation problem can be solved in the dual space where you see the product of decision boundary vectors of decision boundaries, and then you can replace this dot product with a kernel function.",
            "You're suddenly in the area of nonlinear space.",
            "I had mentioned reproducing kernel space.",
            "Then you can do all these things, but you know doing all this kind of stuff, it's becoming more difficult.",
            "So I notice that a couple of years ago there has been a very exciting line of work initiated by Tommy Accolade and his colleagues at MIT, which somehow get unnoticed in the following couple of years.",
            "And this framework is called the maximum entropy discrimination model.",
            "And here is the idea.",
            "Basically, you can imagine that your model is all about learning the parameters of a model and therefore it is a vector of numbers, right?",
            "And for this and that usually get a point estimation of all these numbers.",
            "So that you can get the boundary or the conditional distribution in their framework instead, they want to go somewhat beige, and although it is not the true beige and work in the sense that they want to find a distribution of these parameters.",
            "And their decision boundary will be made based on this distribution using a so-called model averaging framework.",
            "Basically, do a weighted combination of all these predictors, maybe possibly infinite predictors.",
            "And then you're going to get this single Arg Max type of prediction.",
            "How to learn this?",
            "What the learning criteria is kind of strange.",
            "It is a learning criterion Bridge which is promoting a loss function defined on the KL divergences between the posterior distribution of the W over some.",
            "Predefined prior distribution over double and you can imagine that as a minimum additional information you want to inject over the prior.",
            "Something like that, but such optimization is not done in open space.",
            "It is done in this space which is defined directly over this expectation division rule, which is called the expected magic OK and this framework has been used in training support vector machine and they show some very nice property.",
            "So just to give you."
        ],
        [
            "The full picture of my plan in the rest of the talk here is the learning parity that I'm going to pursue and which is the landmark that the landscape of this whole learning paradigm in SVM and make some margin.",
            "So we start off with the SVM and we know that by introducing fancy features, structural features, you turn it into a cube network and you can train the whole thing as if you are training SVM using virtually the same criteria.",
            "You go to the other direction and will now pursue a distribution.",
            "W then you'll get the maximum entropy discrimination where you know you have a expected decision boundary for model averaging and you have this very strange loss function.",
            "So once you see these two things in the map, it is very natural for you to consider maybe a combination of these two.",
            "Check just to find a maximum entropy."
        ],
        [
            "Version of the structure prediction model.",
            "So it's a very very simple logic.",
            "That's exactly what we pursued in this talk, and surprisingly it is not that difficult, but it gave us tremendous advantage in many of the learning problems.",
            "OK, just to recap, in the structured SVM or some people prefer to create N cube network, it is often good to develop its good insight about the dual and primal form of a problem because you can see nice properties in either form, especially in this particular dual form of the of the.",
            "M Cube network you start to see some nice insight of this model.",
            "First of all, the decision boundary is now explicitly expressed as a linear combination of support vectors defined over the prediction rule and the coefficients of the support vectors are solved by a quadratic programming noticed complex operation problem and do this Katie condition is optimality condition.",
            "You can show that many of the Alpha will be 0."
        ],
        [
            "That's why people call such model a dual sparse model because you will be using only a few support vectors, closer division boundary.",
            "And obviously you can see other advantages because you can see this decision boundary appeared in the division Rule and in the loss function as a dot product.",
            "Therefore you can replace it with kernels.",
            "So now I'm going to slightly generalize this idea according to the road map I just outlined to make them a average model and using the maximum entropy idea.",
            "So here is how I do that.",
            "I'm going to now again pursue a distribution of the coefficient of this decision boundary, and you need to know what the form of that.",
            "Let's imagine it's a symbolic abstract symbolic for maybe a log linear model, and then I'm going to use them to define a average division rule.",
            "How do we achieve that role?",
            "Or as usual, you minimize the KL divergences between the posterior of the W over a prior distribution of the W and here actually.",
            "Explicit about your how they look like, because in fact you have the freedom of choice in various forms.",
            "If you don't care about computational cost.",
            "And of course, when important issues that you have to solve this P in the space of P which is defined by the margin constraint, so that if you are using this poster discussion to make your dish and they have the satisfied margin constraints, that doesn't have to be a hard constraint.",
            "If your label are dirty, you can use a select function to allow mislabeled David data, so this is very much similar to the SVM setting, except that everything turned from a point estimator to a distribution of property.",
            "So hopefully this idea is pretty simple and here is a. Geometric picture of how do you look at that?",
            "You are going to have a reference point of P0.",
            "You're going to search something which is closest to the zero in the space defined by this logic.",
            "OK. And the.",
            "How to solve that?",
            "Well, people are familiar ways for solving maximum entropy models subject to constraints, and so, not surprisingly, is going to lead you a expansion family model distribution and lots."
        ],
        [
            "A generic symbolic form of the solution, it looks like simple, but I'm going to tell you it's not my simple to obtain, at least in publicly you can prove very easily OK using a very calculus of variations that post distribution of W will be a log linear model, which is in the form of a product of your prior over a normalized form of this weighted combination of support vectors.",
            "So if you look at this part, this is very very similar to the dual form of the support vector machine.",
            "The MQ network we just saw before and, but this is not the end of the story because they don't know how to solve the alphabet.",
            "In fact, solving the value of the Alpha is actually the core piece of the difficulty of this problem, 'cause they have to be obtained by solving a dual optimization problem, which can be very nontrivial."
        ],
        [
            "We are how complex your partition function is.",
            "So that's where we have rooms to play and trade off complexity and performance and all sorts of days.",
            "Features, for example, may I suggest that I use a prior distribution defined as a trivial standard Gaussian like 0 mean and one variant dosing, and you plug in this P into this form."
        ],
        [
            "OK, and you plug the normalizer normalizer of the distribution into all this and that.",
            "You can show to yourself that in the end."
        ],
        [
            "At the end of the day, you get a posterior distribution of PW, which is also a normal distribution with a shifted mean and a unit covariance is shifted.",
            "Mean has a very interesting representation, which is actually this, which is the weighted sum of support vectors.",
            "So if you remember our under support vectors, by the way, is supported is solved by solving this operation problem.",
            "So if you look at this form closely you will notice that it is exactly the same thing.",
            "You used to solve the Ncube network.",
            "Therefore, I claim that you know if you do a Gaussian maximum entropy from network, you're going to reduce yourself to M cubed network cause your division rule is going to be using directly the meaning of this.",
            "Solution to make division by over inputs.",
            "So that's a good thing.",
            "What's the input?",
            "What's the model here?",
            "The model here is that since they did the same thing, you can solve this equation, for example, using whatever tool people developed for them cube network.",
            "Therefore we were free of additional trouble in finding new inference algorithms for this.",
            "Yeah.",
            "The covariance to be an identity.",
            "OK, I believe you will get some complication, but it should be still a reasonably similar form of this one.",
            "I didn't try here, I just want to show you a simplest reduction.",
            "You will see a fancy reduction works which is.",
            "Which is the?",
            "How about we use the Laplace priors people expected, right so?",
            "As I said, you can have this very simple, nice reduction, but of course that's not what I want.",
            "We didn't get anything over what is existing, so if you are willing to.",
            "Introduce more tricks and flexibility.",
            "You actually found that this maximum entropy network will have at least three advantages.",
            "First of all."
        ],
        [
            "Is a generic advantage.",
            "I wouldn't want to make a big deal of that.",
            "It is average model and naturally can prove APAC based bound to bound the convergence rate of the correct prediction.",
            "And there is a lengthy proof of that which I'm not really quite accelerated standard proof.",
            "The second one is a very useful one 'cause you need to supply a prior distribution of the coefficients and people call this technique essay Entropic regularization and not surprisingly by choosing some special priors such as those priors which might be introducing.",
            "A concentration of shrinkage effect you may expect.",
            "Expect to see some nice feature selection or sparsifying effect as I will show you in a second and Thirdly as not as you can expect it.",
            "This model didn't say anything about how we need to come up with the predicted model.",
            "In fact, pretty model can be a feature based model or can be a very generic descriptive alternative model.",
            "All you do is to plug in all these models and use this.",
            "Combination of training criteriums and you can get some very nice effects such as if the model is having hidden random variables.",
            "You can handle that in in a principled way, so I've been very fast, but you will see some details for that.",
            "I would like to."
        ],
        [
            "Skip this generalized guarantee because that's the kind of the least pragmatically interesting thing, but still it is comforting because they can show that the convergence rate of making of the division is going to be as grow as expected in the rate of logging over."
        ],
        [
            "And the size of the of the party space.",
            "The second part is what I want to play up, so we know that we have this intuition that by introducing a Laplace prior you will be hoping for some more shrinkage effect and this is true under this.",
            "Under this particular model 'cause you can prove a Connery and show that if you use this Laplace prior you can derive a posterior mean of the coefficient to be of this form where each other is actually.",
            "The linear combination of support vectors, so this form is very interesting because if you remember in the Gaussian empty net worth, the maximum entry network, our position is exactly this.",
            "OK in here it is like this and this writer has a shrinkage effect.",
            "If you have somewhat small gamma, this one will push the hose into smaller OK.",
            "If you have a bigger then it's not getting the similar amount of shrinkage.",
            "Therefore it is penalizing small which is almost like what you.",
            "We expect to see in the L1 regularization to achieve a."
        ],
        [
            "Denoising or maybe a sparsification effect and I will say a few more words about that in a second.",
            "Actually, you can see the intuition from here.",
            "So how to see them more closely?",
            "A common example people used to motivate the sparsification of L1 regularization is to view them as a constraint operation problem, where you optimize a loss function and their inside inside a division know a feasibility space.",
            "For example, people know that we under the L2 regularizer you'll have this circular feasible space and under the L1 regularization you have this diamond feasible space.",
            "And if your loss function needs to touch this.",
            "Feasibility space then if you have this out like it's going to achieve the nice sparsity effect because it can touch only at corner most likely.",
            "So under our model you can show that you know one of the transformation.",
            "This is equivalent to solving these constraints operation problem.",
            "You have the usual margin constraint, but your loss function is summation of two terms.",
            "The first term is a horribly complicated thing which I would call a KL norm.",
            "Basically it is defining a feasible space of.",
            "This type of shape is kind of a softer version of the diamond.",
            "Depending on the value of the Lambda.",
            "If you change the value of Lambda here, like from, I couldn't see the number here from maybe 10 to 100 you will see that you have a way to push this whole thing.",
            "In the entire range from like something like a circle to something like a diamond.",
            "Therefore you have these tweaking parameter to encourage a regularizer that is exactly the same as a L2 or L1 regularizer that would be within a second and then here is the usual slacking variable.",
            "So in the end it is.",
            "You can view them as a approximation to AL, one regularised support vector machine structure.",
            "Oh, it's up.",
            "There is no there is no reason for that.",
            "It's just for convenience and he's actually not a norm.",
            "Not not.",
            "Not the technically and Norma just want to avoid using all these terms repeatedly.",
            "Alright, so that's the kind of intuition, and if you compare again with this one, put out one, you can see that it is almost like a. IOS VM or L1 structure SVM, except that now you have some flexibility in controlling the shrinkage effect.",
            "So."
        ],
        [
            "Solving this kind of problem is actually not easy as you could expect.",
            "So here I basically gave you both the primal and dual form of a Laplace maximum entropy screen network and you can see in either the prior and the dual you see something much more complicated than the Gaussian version.",
            "Here you see this horrible KL norm and here you see this nonlinear effect on the support vectors.",
            "So we actually get stuck with the available approach.",
            "Find some trick.",
            "So what are the trick?",
            "What the trick is that people no one can write down a. Laplace distribution.",
            "Using a you know a composite form, which is that you can view this prostitution as a product of a Gaussian distribution over the parameters and conditioning on a parameter which is which is the variance of Gaussian.",
            "Another itself is following another explosion.",
            "If you marginalized out this call, you will recover the Laplace distribution.",
            "OK, this is a tricky way of writing down the plus prior, but it turns out that once you use this representation you can introduce.",
            "Extra auxiliary random variables, which makes the inference easy.",
            "The trick is that once.",
            "So every iteration you hold constant.",
            "Visa.",
            "People and cars are the ones you hold it constant.",
            "You turn the whole thing into a Gaussian distribution.",
            "OK, so here is the trick.",
            "No, I'm going to minimize this scale, divergent as usual.",
            "But what I do is to use a type of iterative operation.",
            "Proper procedure in the first space I'm going to.",
            "Ignore this stochastic city I'm holding to be constant and then as you can see, this way is a Gaussian distribution.",
            "Therefore, I'm not back into a Gaussian maximum entropy Markov network and you can solve this problem trivially using the standard algorithm.",
            "And Secondly, once you have this posterior PW to be to be solved in the previous iteration, you keep it constant and then try to solve appear off target Lambda and turns out that this is has a nice form you can come up with a code solution to that date to.",
            "That computer is at all estimation, so we do this iteratively and use nothing but the algorithm used in the network and in the end you hopefully convergence.",
            "Since this has a spirit of EM algorithm, it is a local operation algorithm, but as usual we can do the usual check of random restarts in order to achieve it.",
            "Convergence speed is no slower than the stand."
        ],
        [
            "Yeah, like when you would expect to see.",
            "Alright, so now let's see some performance.",
            "I think I'm verifying to the algorithm.",
            "I hope to not boring you with more equations, and here are some results.",
            "So I begin with this very standard.",
            "Datasets that then used in his very groundbreaking Ncube network and let's see how at least our model compared to that one.",
            "So we did a pretty exhaustive."
        ],
        [
            "Harrison, on all possible variations you can imagine based on the current models.",
            "So here we code up.",
            "Of course, a traditional CRF.",
            "We code up various ways of regularising the CRF, such as L1 norm and L2 norm, and we called up.",
            "Of course, the M3 network and Q Network and also.",
            "People may ask, why don't you just use a direct implementation of the L1 regularised?",
            "No MQ network.",
            "It turns out such sympathy exists in the literature, so we got a new paper just to develop that.",
            "But in the end we actually made it happen and we also have this law plus maximum margin Markov network.",
            "So here's the comparison and the various.",
            "The training size.",
            "You can see that you know this one seems to be constantly outperforming all the other alternatives over different conditions, which is quite comforting effect from the training under standard test data set we went."
        ],
        [
            "OK, this I don't want to show this is kind of the features that actually affect which is not showing significant differences between different algorithms.",
            "Here is maybe something that is."
        ],
        [
            "So why we prefer one thing over the other?",
            "For example, what about implement the L1M cube network?",
            "Well, it costs us a new paper.",
            "That means it's not trivial, but you can still do it.",
            "The problem is that it turns out that all this appointed as matter of models are kind of sensitive to the magnitude of the regularization coefficients.",
            "You have to be tricky in tuning that number, and for unknown reason we found that our model seems to be.",
            "Less sensitive when you are training a training, different magnitude of the coefficients, we didn't get a good insight about that, but I believe maybe it has something to do with this model averaging.",
            "In fact it is more tolerant to errors, but again, this is not something I'm sure about.",
            "Affirmative the your students for their fire that's easier.",
            "Yeah, I mean that that that that extra parameter is equivalent to the Lambda in your L1 regularization in other models, right?",
            "Just basing formal expressing that.",
            "No, no, I don't.",
            "I don't have those.",
            "And then when I use the prior I don't like you no longer have this Lambda to write because it's a product effect in the end.",
            "All the excess is just a symbolic dimension where you change the value of the regulation constant.",
            "OK, I'm here is in fact all these are not even comparable because you cannot compare different models on the same scale of regularization.",
            "It's just the way that we choose the 10 different values of regulation and we show you a range of the variance of their performance.",
            "No, they're not, because the loss function is different.",
            "Their income problem.",
            "We basically fixed arrange a reasonable range that we expect to see the good numbers, and then we're going to change that and see how sensitive there.",
            "So it's not.",
            "It's not a good graph, not good with autograph.",
            "So in the paper we actually proud to five different graph.",
            "So real range of the numbers here.",
            "I just want to see this effect of how sensitive this whole thing is to changing values of the regularization."
        ],
        [
            "OK, so so sensitivity to regular constant and then I want to went on.",
            "I want to move on to show you some advantages of this model because this lasting arguably is OK Advantage.",
            "But you can still achieve the same thing using a.",
            "You know other approach but there are some other advantages which is not so easily achievable using other approaches such as a model with theoretical or hidden structures and this is often a good model to make complex predictions which contain internal.",
            "You know a semantic structure such as you do Web web page segmentation.",
            "If you want to just flexor plantation in here you know you lose the insight that the web is composed in.",
            "A semantically informed way and also syntactically interesting and coherent way.",
            "And one can easily come up with this labeling Iraqis of the web content.",
            "The problem, that the reason people don't do that in learning is because getting training information for this internal layers are very, very tedious and you would want to do it.",
            "Or it's very costly.",
            "Great, so in our case we're going to treat all this as hidden random variables and we're going to build our structure prediction as usual by assuming that maybe they are no different from the observed features.",
            "There we often define features based on this graphical model.",
            "So what do you get in the?"
        ],
        [
            "And when you have hidden hidden random variables in credit is something very simple becausw in your in the 1st place.",
            "When you solve this.",
            "The maximum entropy from network you are already approaching a distribution, which is the distribution of the parameters.",
            "Now we have more hidden random variables, it's just about.",
            "Enriching the distribution to be continued, both the parameters and the C&C and your decision boundary has to be made by integrate out now, not only the parameters of the of the model, but also that hidden variables you don't care about.",
            "OK, so we did nothing in changing the general framework, which is this maximum entropy switching call loss function plus some expected margin constraint on the distributions, except that now we are having one more loop of integration over a.",
            "Extra subset of random variables.",
            "So you do this and you imagine the predictive rule will be of course slightly more complicated.",
            "Now you are going to make integration over the W, and also imagine the city to be discreet when you sum over all the possible these.",
            "So nothing is significantly different than the training is again very very."
        ],
        [
            "Expect it is going to be quite straightforward because all you did is that you need to introduce some prior now over 2 subsets of hidden random variables, say the WNZ, and you want to be simple by introducing a fully factorizable prior freedom, you can go fancier, but this fully feathered prior gives you the computational simplicity and then again you can imagine a type of EM approach where you keep hold one thing constant and optimize the other.",
            "Therefore you are resolving a sub problem that we see before in the baseline.",
            "Maximum entropy screen my whole network, so do this and and in every of this subroutine sub step you can basically using you use the existing standard algorithm of your favor to achieve the solution and you do it a couple of times.",
            "So in the sense it is more expensive than any of this building block algorithm, But the additional price is roughly linear in a few more steps, more complex because your iteration will converge in a couple steps.",
            "OK, so."
        ],
        [
            "The expected results we have this nontrivial data set.",
            "We have a bigger one in our final paper, but basically you have some training website with design templates to help you come up with these latent structure.",
            "Then we're going to evaluate based on the prediction at the base level, or maybe some higher level.",
            "I don't to be too explicit for that, but basically there's some score routinely used in the World Wide Web community to score."
        ],
        [
            "The later the webpage labeling here is again a performance comparison here.",
            "You know there are many things.",
            "There are still compare with because of solving the absolute network will be open problem if hidden variable is present.",
            "So we did some some of these homemade implementation we have.",
            "Hey, logical CRF, where we just assume everything is observed but you have not hierarchical features in the labeling space, and then because it is a CRF model which is based on likelihood, we can now train unsupervised fashion so that you can use the emerald hidden random variables.",
            "Then we also implement this hierarchical Ncube network trained fully supervised Lee and we have this partially observed M3, which is our model and you can see that the performance is what is.",
            "Average average score for.",
            "I think this one and that one is dominating over the others.",
            "Actually, it's a very interesting observation here.",
            "If you notice, these two things are supposedly to be different because they use different amount of informations.",
            "This hierarchical entry network fully supervised.",
            "OK, and this one is only partially supervised, but if you look at the performance they are not as good at the beginning, but when you have more and more training ratio, one more training examples actually pick up.",
            "That means that even without labeling in the intermediate layer, they can still achieve good performance.",
            "So that's a pretty nice result, and here is a detailed breakdown."
        ],
        [
            "Where you get the performance from on different labels.",
            "Since I'm running out of time, I'm good."
        ],
        [
            "To rank very, very quickly onto a non trivial extension of this model to because I claim that I want to learn maximum margin and maximum likely graphical MoD."
        ],
        [
            "Sort of just structure input all the models.",
            "So what do we make graphical model?",
            "So here I have a very familiar graphic model to all of you guys.",
            "Which is this topic model.",
            "Public model can be."
        ],
        [
            "He expressed by this related graph and you write on the usual likelihood function, and in this graph of course you have many hidden random variables.",
            "Everything has been observed if, except those awards.",
            "And how do we train that?",
            "Well, you really people optimize the likelihood of the tree of the words and they get something about topic."
        ],
        [
            "Now I want to resist the temptation of interpreting the parameters as topics that just parameters OK, and then people found out this is a bad idea when your goal is to do prediction.",
            "Because if you use this topic vector and then update you using to do SVM type of prediction, your performance is really not that great.",
            "It's worse than just putting a long word vector and the SVM directly, and people wonder why that's the case.",
            "Well, I think it's very natural because your training is not toward gearing towards optimizing the classification error, it's just the likelihood fitting of the data though.",
            "It's not.",
            "There's no real quick site is doing a good job in classification, therefore day Brian, his colleague, went on and introduced explicitly.",
            "This predicted label OK classification label or maybe regression score and labeled as a function of the word specific topic indicators and you can make this relation by a Gaussian type of model or some other logistic normal prediction model.",
            "Problems arise by certain way and then you can still run this fully supervised learning and get some division.",
            "Laundry to be determined.",
            "It turns out that the grid isn't good either for some unknown RBC coding is too complicated.",
            "What I mean by not good either is that if you turn the whole document into this framework and then make a influence on this way."
        ],
        [
            "The prediction label isn't as good as you would hope for."
        ],
        [
            "So we basically did you do things thing we write down this loss function.",
            "OK, using the same graphical model and the loss functions defined over cross entropy and the select variable select functions over the division and then these are all the complicated expression of this margin constraint.",
            "But at the end of the day you use the same procedure as presented before and you get the same estimator of all this latent parameters except."
        ],
        [
            "Now you are using a different division criteria or not the loss function and."
        ],
        [
            "Active.",
            "So without going through the technical details, I think it's."
        ],
        [
            "Good to finish the talk with some high level presentation of the results.",
            "Actually, before I go to the results, I want to basically highlight that you can use this idea to train.",
            "Very general topic models of arbitrary design.",
            "You can introduce more features, do according topic model or things like that.",
            "In the end, you basically do you know design on the loss function to be apart from model fitting.",
            "Apart from predictive accuracy, apart from the selecting random variable and then you somehow composed this expect margin constraint so everything has been pretty standard now.",
            "And."
        ],
        [
            "Look at the results.",
            "So this is the standard result used in the same paper that they wrote and."
        ],
        [
            "Firstly, we want to visualize topics well.",
            "It's hard to say which one is better, but at least in the Med LDA you'll see a slight less convolution of the topics in it, even if it's 2 dimensional space.",
            "And if you really."
        ],
        [
            "To the award in this topic, you know I couldn't detect any significant meaningful difference, but the."
        ],
        [
            "Performance actually shows some very nice insight.",
            "We evaluate this model in terms of the predictive accuracy, which is the classification label of every document using this criteria, and again we compare it with a number of other competing models.",
            "The baseline is very natural, you just trained SVM and then you use that topic vector freedom into a training LDA use topic to freedom into SVN.",
            "You get a prediction that is in here we set them to be 0.",
            "So she does the absolute difference of others.",
            "The second one is the supervised LDA index paper which is in here.",
            "This one is the disk ID, which is a different formulation by Simone and Fish are a few years ago and later than this, but still quite interesting model which promotes another way of discriminating between LDA model.",
            "And this is our model.",
            "This is our model.",
            "So the way I present this is the reason I presented this tool as follows.",
            "So we said that that mentality is trained based on a Max margin framework.",
            "So you can directly do classification by putting in the document and get the label from that, but also as a side product you also get a topic vector from that document.",
            "You can do another round of SVM to make a new prediction and in this case you can see these two things are not doing very similar things.",
            "Basically the.",
            "The performance came you would like to hope for in the SVM are already almost fully capitalized in the LDA framework, so that means the media is really doing what they're supposed to do by harvesting or leveraging on the maximum likelihood estimation and next margin estimation."
        ],
        [
            "OK, I think I'm almost there.",
            "You can train the same thing based on predictive labels such as movie scores, which is a real number and the same thing.",
            "We can again, you know, get this quite dramatic performance game over other standard implementations.",
            "I have to point out that there are some even nicer result because now the LDA them at LDA is a likelihood based model.",
            "You don't have to even train it in the fully supervised fashion.",
            "You can actually do cultural labeling and still get a nice performance.",
            "I know in this as indicated in this magenta line in the performance is still better than the baseline which is fully supervised.",
            "I think I think I'm running out of time."
        ],
        [
            "Efficiency is pretty good on this new model, and as a summary I hope I convey the message that."
        ],
        [
            "At least there is a way to combine the benefit of a likelihood based training and a margin based training and the the type of cost you have to pay in terms of inference and in terms of learning is is reasonable without the need of having too much demanding other cost on time and on the on the membrane.",
            "Things like that.",
            "And there are various interesting extension you can put to train model with.",
            "Even running variables on the waist very fancy sparsity factors in psychology.",
            "OK, I think I'm done.",
            "I just want to remind you this full framework we are in here now and I want to acknowledge the people who is involved in this work.",
            "The major piece of the work was done with a very capable postdoc right now in my group Ginger and Arm.",
            "It also helped with the design and implementation of the LDA model and here are the funding agencies that is responsible for supporting the work.",
            "OK, I think I'm done.",
            "Thank you very much for your patience."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you very much for the invitation that very nice introduction.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to share with you some of our recent work.",
                    "label": 0
                },
                {
                    "sent": "On a slightly fancier learning paradigm on a generic class of graphical models, mainly generalized linear models.",
                    "label": 1
                },
                {
                    "sent": "So the motivating prob.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is again this structured input output problem, which I'm going to introduce briefly, so in the general predicted problem which are unstructured, we basically call it a classification problem, and it is a very simple setting where you have a set of input features and then you're going to make a.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Prediction on that the structured input output prediction problem is the innocence similar, except that the output becomes a sequence or a matrix or predictors which are Inter correlated and we can find various reasons about why this is a good thing to do, because the labels such as labeling image parts or part of speech is should be dependent on each other.",
                    "label": 0
                },
                {
                    "sent": "If you want to make sense, they global sense out of the prediction.",
                    "label": 0
                },
                {
                    "sent": "So how people approach this problem again?",
                    "label": 0
                },
                {
                    "sent": "There are some classical approaches for unstructured prediction problem which is built on some particular function.",
                    "label": 0
                },
                {
                    "sent": "You're maximize that pretty function over your predictor and you get your prediction and you can have a very flexible design of this particular function.",
                    "label": 0
                },
                {
                    "sent": "In cases age linear model and you have different ways of training that and here I highlight two ways of training which has been frequently used in our domain.",
                    "label": 0
                },
                {
                    "sent": "One is to train a likelihood based loss function such as a conditional likelihood for the label given the input in something.",
                    "label": 0
                },
                {
                    "sent": "For example like a logistic regression setting.",
                    "label": 1
                },
                {
                    "sent": "Or you can train a base classifier.",
                    "label": 0
                },
                {
                    "sent": "And here is the training procedure and the other very favorite parity, maybe more favored in certain areas.",
                    "label": 0
                },
                {
                    "sent": "Training a discrete model based on a different loss function, known as the maximum margin, and by working through the algebra one can lend it himself at this loss function, which is the L2 norm of the weight vector of the decision boundary plus some slack variable.",
                    "label": 0
                },
                {
                    "sent": "And then you optimize this problem in the space of a margin space constraint where the product margin must be bigger than some lower bounds.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are pros and cons of these two approaches, and it is an extremely flexible design.",
                    "label": 0
                },
                {
                    "sent": "And that leads us a direct way of promoting these models to structure input problem.",
                    "label": 0
                },
                {
                    "sent": "So here again give you 2 very.",
                    "label": 0
                },
                {
                    "sent": "Popular models in the structure input upper learning domain and the first model is called a conditional random field where you build a random field model, often a chain Markov model over the predicted labels, and then you still train A.",
                    "label": 0
                },
                {
                    "sent": "A conditional probability solution of the output given the input.",
                    "label": 0
                },
                {
                    "sent": "And here you can also train the whole thing through a maximum margin procedure, and if you stare at all this equation you will see no difference, at least symbolically of this loss functions to the unstructured version, except that now your feature design become different you can design fencing features such as features defined on pairwise labels or potentials that reflects a Markov dependencies between nodes.",
                    "label": 0
                },
                {
                    "sent": "OK, you can even actually get some other.",
                    "label": 0
                },
                {
                    "sent": "Our dependencies between features, so that's why this model is more flexible in a sense.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Favored by many people, there are some debates in our community about which procedure is better.",
                    "label": 0
                },
                {
                    "sent": "Some people felt that if you know how to deploy the conditional model, you can get amazing results, for example by introducing various regularization terms to increase sparsity or promote some bias.",
                    "label": 0
                },
                {
                    "sent": "But some people believe that this so called maximum margin Markov network, which is structured version of SVM, has a very nice property 'cause they can be projected into a dual space where you.",
                    "label": 0
                },
                {
                    "sent": "Worker use the kernel tracks.",
                    "label": 0
                },
                {
                    "sent": "You can achieve something called the dual sparsity where your distributor is defined by a few support vectors.",
                    "label": 0
                },
                {
                    "sent": "So it is really debatable you know about which one is better, and that's not my interest here.",
                    "label": 0
                },
                {
                    "sent": "What I'm trying to promote is an approach which allow you to hopefully harvest the benefit of both approaches so that we can learn a model which is simultaneously pushing a version of a maximum margin type of loss function, but also doing this optimization in the space defined by margins so that.",
                    "label": 0
                },
                {
                    "sent": "You know you hopefully can get benefit from both approaches, and again, I don't think I need to really motivate this problem.",
                    "label": 0
                },
                {
                    "sent": "There are tons of problems of this nature, such as the people in computer vision doing image segmentation, or in my favorite area of population genetics where.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know you have this very high dimensional input of the human genome and you have this also very high dimensional output space, which is the disease phenotypes.",
                    "label": 0
                },
                {
                    "sent": "Say you are sick, you have high blood pressure.",
                    "label": 0
                },
                {
                    "sent": "You have all sorts of weird symptoms and the goal is to make these connections.",
                    "label": 0
                },
                {
                    "sent": "Maybe also predict how good you are in the next few days.",
                    "label": 0
                },
                {
                    "sent": "You know based on your genetic traits, again the key element here is that the prediction has to be structured because all these output vectors output variables are Inter dependent on each other.",
                    "label": 0
                },
                {
                    "sent": "So as I said, we.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've always nice foundation for training the model and but there are some challenges.",
                    "label": 0
                },
                {
                    "sent": "For example, one of the challenges that if you really want to go to high dimensional type of structure, input of model input output model, you have to worry about the sparsity because your data is never as many as I mentioned.",
                    "label": 0
                },
                {
                    "sent": "You have to deal with.",
                    "label": 0
                },
                {
                    "sent": "Therefore you need to regularize the model to achieve not only sparsity but also even model correctness and Secondly maybe in many domains you have prior information you know certain.",
                    "label": 0
                },
                {
                    "sent": "Features are more important over others.",
                    "label": 0
                },
                {
                    "sent": "How can you incorporate such features into the model?",
                    "label": 0
                },
                {
                    "sent": "I guess people would go for the likelihood based approach because you have a natural way to write down the posterior, but I will tell today that you can do the same for the margin based approach.",
                    "label": 0
                },
                {
                    "sent": "And so I can go on this list to list all these challenges.",
                    "label": 0
                },
                {
                    "sent": "I'm going to hopefully touch a few of this point in my following implementation.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the outline of my presentation.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you a general theory of the so called maximal entropy disclination Markov network and you will see why I give this weird very long term because it is really doing every element and listing here and I'm going to show you some special cases.",
                    "label": 0
                },
                {
                    "sent": "One of the special cases of this general model is to introduce a particular type of regularization, so that's the model reduces to a very simple model known as the maximizing Markov network weapon Tasker.",
                    "label": 0
                },
                {
                    "sent": "It's a very, very nice model.",
                    "label": 0
                },
                {
                    "sent": "And we show that we can reduce our model to that model.",
                    "label": 0
                },
                {
                    "sent": "Therefore using all the inference backwards over there.",
                    "label": 0
                },
                {
                    "sent": "But if you don't want to do a trivial regulation or do some fancy regulation, you actually achieve some.",
                    "label": 0
                },
                {
                    "sent": "You know maybe sparse version of the MCM cube network or some other desirable effects.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can also use this model to introduce hidden random variables.",
                    "label": 0
                },
                {
                    "sent": "People talking about training partially absorbed as structured SVM.",
                    "label": 0
                },
                {
                    "sent": "We can do it here as well, and by using a pretty flexible and generic algorithm.",
                    "label": 0
                },
                {
                    "sent": "And finally I'm going to show you a few examples of how to apply.",
                    "label": 0
                },
                {
                    "sent": "This approach is to tray really fancy models that will have to be Markov network.",
                    "label": 0
                },
                {
                    "sent": "It could be a directed graph model such as a topic model.",
                    "label": 1
                },
                {
                    "sent": "You can still trade topic modeling.",
                    "label": 0
                },
                {
                    "sent": "Distributed sense.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let's see, as I mentioned that the maximum likelihood estimator and the maximizing.",
                    "label": 0
                },
                {
                    "sent": "Estimator had this competing advantages and here I just list a few and want to point out the main reason people prefer one way or the other.",
                    "label": 0
                },
                {
                    "sent": "Or perhaps what I highlighted in red.",
                    "label": 0
                },
                {
                    "sent": "For example, in the likelihood based training you can do Bayesian inference, and that's a good thing because you know this Bayesian inference allow you to introduce these other priors.",
                    "label": 0
                },
                {
                    "sent": "Unfined and Secondly because it is a probability model.",
                    "label": 0
                },
                {
                    "sent": "It is very natural for you to introduce all the hidden random variables.",
                    "label": 0
                },
                {
                    "sent": "We can always write down a marginal likelihood over the over the observations and they're using the same type of approach to fill in the hidden random variables.",
                    "label": 0
                },
                {
                    "sent": "That's a very natural algorithm.",
                    "label": 0
                },
                {
                    "sent": "But it's not so easy to do the same thing in a maximizing Markov network type of framework, but here you have some other advantages.",
                    "label": 0
                },
                {
                    "sent": "In particular, you can do this kernel trick, cause the operation problem can be solved in the dual space where you see the product of decision boundary vectors of decision boundaries, and then you can replace this dot product with a kernel function.",
                    "label": 0
                },
                {
                    "sent": "You're suddenly in the area of nonlinear space.",
                    "label": 0
                },
                {
                    "sent": "I had mentioned reproducing kernel space.",
                    "label": 0
                },
                {
                    "sent": "Then you can do all these things, but you know doing all this kind of stuff, it's becoming more difficult.",
                    "label": 0
                },
                {
                    "sent": "So I notice that a couple of years ago there has been a very exciting line of work initiated by Tommy Accolade and his colleagues at MIT, which somehow get unnoticed in the following couple of years.",
                    "label": 0
                },
                {
                    "sent": "And this framework is called the maximum entropy discrimination model.",
                    "label": 1
                },
                {
                    "sent": "And here is the idea.",
                    "label": 0
                },
                {
                    "sent": "Basically, you can imagine that your model is all about learning the parameters of a model and therefore it is a vector of numbers, right?",
                    "label": 0
                },
                {
                    "sent": "And for this and that usually get a point estimation of all these numbers.",
                    "label": 0
                },
                {
                    "sent": "So that you can get the boundary or the conditional distribution in their framework instead, they want to go somewhat beige, and although it is not the true beige and work in the sense that they want to find a distribution of these parameters.",
                    "label": 0
                },
                {
                    "sent": "And their decision boundary will be made based on this distribution using a so-called model averaging framework.",
                    "label": 0
                },
                {
                    "sent": "Basically, do a weighted combination of all these predictors, maybe possibly infinite predictors.",
                    "label": 0
                },
                {
                    "sent": "And then you're going to get this single Arg Max type of prediction.",
                    "label": 0
                },
                {
                    "sent": "How to learn this?",
                    "label": 0
                },
                {
                    "sent": "What the learning criteria is kind of strange.",
                    "label": 0
                },
                {
                    "sent": "It is a learning criterion Bridge which is promoting a loss function defined on the KL divergences between the posterior distribution of the W over some.",
                    "label": 0
                },
                {
                    "sent": "Predefined prior distribution over double and you can imagine that as a minimum additional information you want to inject over the prior.",
                    "label": 0
                },
                {
                    "sent": "Something like that, but such optimization is not done in open space.",
                    "label": 0
                },
                {
                    "sent": "It is done in this space which is defined directly over this expectation division rule, which is called the expected magic OK and this framework has been used in training support vector machine and they show some very nice property.",
                    "label": 0
                },
                {
                    "sent": "So just to give you.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The full picture of my plan in the rest of the talk here is the learning parity that I'm going to pursue and which is the landmark that the landscape of this whole learning paradigm in SVM and make some margin.",
                    "label": 0
                },
                {
                    "sent": "So we start off with the SVM and we know that by introducing fancy features, structural features, you turn it into a cube network and you can train the whole thing as if you are training SVM using virtually the same criteria.",
                    "label": 0
                },
                {
                    "sent": "You go to the other direction and will now pursue a distribution.",
                    "label": 0
                },
                {
                    "sent": "W then you'll get the maximum entropy discrimination where you know you have a expected decision boundary for model averaging and you have this very strange loss function.",
                    "label": 1
                },
                {
                    "sent": "So once you see these two things in the map, it is very natural for you to consider maybe a combination of these two.",
                    "label": 0
                },
                {
                    "sent": "Check just to find a maximum entropy.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Version of the structure prediction model.",
                    "label": 0
                },
                {
                    "sent": "So it's a very very simple logic.",
                    "label": 0
                },
                {
                    "sent": "That's exactly what we pursued in this talk, and surprisingly it is not that difficult, but it gave us tremendous advantage in many of the learning problems.",
                    "label": 0
                },
                {
                    "sent": "OK, just to recap, in the structured SVM or some people prefer to create N cube network, it is often good to develop its good insight about the dual and primal form of a problem because you can see nice properties in either form, especially in this particular dual form of the of the.",
                    "label": 0
                },
                {
                    "sent": "M Cube network you start to see some nice insight of this model.",
                    "label": 0
                },
                {
                    "sent": "First of all, the decision boundary is now explicitly expressed as a linear combination of support vectors defined over the prediction rule and the coefficients of the support vectors are solved by a quadratic programming noticed complex operation problem and do this Katie condition is optimality condition.",
                    "label": 0
                },
                {
                    "sent": "You can show that many of the Alpha will be 0.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's why people call such model a dual sparse model because you will be using only a few support vectors, closer division boundary.",
                    "label": 0
                },
                {
                    "sent": "And obviously you can see other advantages because you can see this decision boundary appeared in the division Rule and in the loss function as a dot product.",
                    "label": 0
                },
                {
                    "sent": "Therefore you can replace it with kernels.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to slightly generalize this idea according to the road map I just outlined to make them a average model and using the maximum entropy idea.",
                    "label": 0
                },
                {
                    "sent": "So here is how I do that.",
                    "label": 0
                },
                {
                    "sent": "I'm going to now again pursue a distribution of the coefficient of this decision boundary, and you need to know what the form of that.",
                    "label": 0
                },
                {
                    "sent": "Let's imagine it's a symbolic abstract symbolic for maybe a log linear model, and then I'm going to use them to define a average division rule.",
                    "label": 0
                },
                {
                    "sent": "How do we achieve that role?",
                    "label": 0
                },
                {
                    "sent": "Or as usual, you minimize the KL divergences between the posterior of the W over a prior distribution of the W and here actually.",
                    "label": 0
                },
                {
                    "sent": "Explicit about your how they look like, because in fact you have the freedom of choice in various forms.",
                    "label": 0
                },
                {
                    "sent": "If you don't care about computational cost.",
                    "label": 0
                },
                {
                    "sent": "And of course, when important issues that you have to solve this P in the space of P which is defined by the margin constraint, so that if you are using this poster discussion to make your dish and they have the satisfied margin constraints, that doesn't have to be a hard constraint.",
                    "label": 0
                },
                {
                    "sent": "If your label are dirty, you can use a select function to allow mislabeled David data, so this is very much similar to the SVM setting, except that everything turned from a point estimator to a distribution of property.",
                    "label": 0
                },
                {
                    "sent": "So hopefully this idea is pretty simple and here is a. Geometric picture of how do you look at that?",
                    "label": 0
                },
                {
                    "sent": "You are going to have a reference point of P0.",
                    "label": 0
                },
                {
                    "sent": "You're going to search something which is closest to the zero in the space defined by this logic.",
                    "label": 0
                },
                {
                    "sent": "OK. And the.",
                    "label": 0
                },
                {
                    "sent": "How to solve that?",
                    "label": 0
                },
                {
                    "sent": "Well, people are familiar ways for solving maximum entropy models subject to constraints, and so, not surprisingly, is going to lead you a expansion family model distribution and lots.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A generic symbolic form of the solution, it looks like simple, but I'm going to tell you it's not my simple to obtain, at least in publicly you can prove very easily OK using a very calculus of variations that post distribution of W will be a log linear model, which is in the form of a product of your prior over a normalized form of this weighted combination of support vectors.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this part, this is very very similar to the dual form of the support vector machine.",
                    "label": 0
                },
                {
                    "sent": "The MQ network we just saw before and, but this is not the end of the story because they don't know how to solve the alphabet.",
                    "label": 0
                },
                {
                    "sent": "In fact, solving the value of the Alpha is actually the core piece of the difficulty of this problem, 'cause they have to be obtained by solving a dual optimization problem, which can be very nontrivial.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are how complex your partition function is.",
                    "label": 0
                },
                {
                    "sent": "So that's where we have rooms to play and trade off complexity and performance and all sorts of days.",
                    "label": 0
                },
                {
                    "sent": "Features, for example, may I suggest that I use a prior distribution defined as a trivial standard Gaussian like 0 mean and one variant dosing, and you plug in this P into this form.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and you plug the normalizer normalizer of the distribution into all this and that.",
                    "label": 0
                },
                {
                    "sent": "You can show to yourself that in the end.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the end of the day, you get a posterior distribution of PW, which is also a normal distribution with a shifted mean and a unit covariance is shifted.",
                    "label": 0
                },
                {
                    "sent": "Mean has a very interesting representation, which is actually this, which is the weighted sum of support vectors.",
                    "label": 0
                },
                {
                    "sent": "So if you remember our under support vectors, by the way, is supported is solved by solving this operation problem.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this form closely you will notice that it is exactly the same thing.",
                    "label": 0
                },
                {
                    "sent": "You used to solve the Ncube network.",
                    "label": 0
                },
                {
                    "sent": "Therefore, I claim that you know if you do a Gaussian maximum entropy from network, you're going to reduce yourself to M cubed network cause your division rule is going to be using directly the meaning of this.",
                    "label": 0
                },
                {
                    "sent": "Solution to make division by over inputs.",
                    "label": 1
                },
                {
                    "sent": "So that's a good thing.",
                    "label": 0
                },
                {
                    "sent": "What's the input?",
                    "label": 0
                },
                {
                    "sent": "What's the model here?",
                    "label": 0
                },
                {
                    "sent": "The model here is that since they did the same thing, you can solve this equation, for example, using whatever tool people developed for them cube network.",
                    "label": 0
                },
                {
                    "sent": "Therefore we were free of additional trouble in finding new inference algorithms for this.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "The covariance to be an identity.",
                    "label": 0
                },
                {
                    "sent": "OK, I believe you will get some complication, but it should be still a reasonably similar form of this one.",
                    "label": 0
                },
                {
                    "sent": "I didn't try here, I just want to show you a simplest reduction.",
                    "label": 0
                },
                {
                    "sent": "You will see a fancy reduction works which is.",
                    "label": 0
                },
                {
                    "sent": "Which is the?",
                    "label": 0
                },
                {
                    "sent": "How about we use the Laplace priors people expected, right so?",
                    "label": 0
                },
                {
                    "sent": "As I said, you can have this very simple, nice reduction, but of course that's not what I want.",
                    "label": 0
                },
                {
                    "sent": "We didn't get anything over what is existing, so if you are willing to.",
                    "label": 0
                },
                {
                    "sent": "Introduce more tricks and flexibility.",
                    "label": 0
                },
                {
                    "sent": "You actually found that this maximum entropy network will have at least three advantages.",
                    "label": 0
                },
                {
                    "sent": "First of all.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a generic advantage.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't want to make a big deal of that.",
                    "label": 0
                },
                {
                    "sent": "It is average model and naturally can prove APAC based bound to bound the convergence rate of the correct prediction.",
                    "label": 0
                },
                {
                    "sent": "And there is a lengthy proof of that which I'm not really quite accelerated standard proof.",
                    "label": 0
                },
                {
                    "sent": "The second one is a very useful one 'cause you need to supply a prior distribution of the coefficients and people call this technique essay Entropic regularization and not surprisingly by choosing some special priors such as those priors which might be introducing.",
                    "label": 0
                },
                {
                    "sent": "A concentration of shrinkage effect you may expect.",
                    "label": 0
                },
                {
                    "sent": "Expect to see some nice feature selection or sparsifying effect as I will show you in a second and Thirdly as not as you can expect it.",
                    "label": 0
                },
                {
                    "sent": "This model didn't say anything about how we need to come up with the predicted model.",
                    "label": 0
                },
                {
                    "sent": "In fact, pretty model can be a feature based model or can be a very generic descriptive alternative model.",
                    "label": 0
                },
                {
                    "sent": "All you do is to plug in all these models and use this.",
                    "label": 0
                },
                {
                    "sent": "Combination of training criteriums and you can get some very nice effects such as if the model is having hidden random variables.",
                    "label": 0
                },
                {
                    "sent": "You can handle that in in a principled way, so I've been very fast, but you will see some details for that.",
                    "label": 0
                },
                {
                    "sent": "I would like to.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Skip this generalized guarantee because that's the kind of the least pragmatically interesting thing, but still it is comforting because they can show that the convergence rate of making of the division is going to be as grow as expected in the rate of logging over.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the size of the of the party space.",
                    "label": 0
                },
                {
                    "sent": "The second part is what I want to play up, so we know that we have this intuition that by introducing a Laplace prior you will be hoping for some more shrinkage effect and this is true under this.",
                    "label": 0
                },
                {
                    "sent": "Under this particular model 'cause you can prove a Connery and show that if you use this Laplace prior you can derive a posterior mean of the coefficient to be of this form where each other is actually.",
                    "label": 0
                },
                {
                    "sent": "The linear combination of support vectors, so this form is very interesting because if you remember in the Gaussian empty net worth, the maximum entry network, our position is exactly this.",
                    "label": 0
                },
                {
                    "sent": "OK in here it is like this and this writer has a shrinkage effect.",
                    "label": 0
                },
                {
                    "sent": "If you have somewhat small gamma, this one will push the hose into smaller OK.",
                    "label": 0
                },
                {
                    "sent": "If you have a bigger then it's not getting the similar amount of shrinkage.",
                    "label": 0
                },
                {
                    "sent": "Therefore it is penalizing small which is almost like what you.",
                    "label": 0
                },
                {
                    "sent": "We expect to see in the L1 regularization to achieve a.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Denoising or maybe a sparsification effect and I will say a few more words about that in a second.",
                    "label": 0
                },
                {
                    "sent": "Actually, you can see the intuition from here.",
                    "label": 0
                },
                {
                    "sent": "So how to see them more closely?",
                    "label": 0
                },
                {
                    "sent": "A common example people used to motivate the sparsification of L1 regularization is to view them as a constraint operation problem, where you optimize a loss function and their inside inside a division know a feasibility space.",
                    "label": 0
                },
                {
                    "sent": "For example, people know that we under the L2 regularizer you'll have this circular feasible space and under the L1 regularization you have this diamond feasible space.",
                    "label": 0
                },
                {
                    "sent": "And if your loss function needs to touch this.",
                    "label": 0
                },
                {
                    "sent": "Feasibility space then if you have this out like it's going to achieve the nice sparsity effect because it can touch only at corner most likely.",
                    "label": 0
                },
                {
                    "sent": "So under our model you can show that you know one of the transformation.",
                    "label": 0
                },
                {
                    "sent": "This is equivalent to solving these constraints operation problem.",
                    "label": 0
                },
                {
                    "sent": "You have the usual margin constraint, but your loss function is summation of two terms.",
                    "label": 0
                },
                {
                    "sent": "The first term is a horribly complicated thing which I would call a KL norm.",
                    "label": 0
                },
                {
                    "sent": "Basically it is defining a feasible space of.",
                    "label": 0
                },
                {
                    "sent": "This type of shape is kind of a softer version of the diamond.",
                    "label": 0
                },
                {
                    "sent": "Depending on the value of the Lambda.",
                    "label": 0
                },
                {
                    "sent": "If you change the value of Lambda here, like from, I couldn't see the number here from maybe 10 to 100 you will see that you have a way to push this whole thing.",
                    "label": 0
                },
                {
                    "sent": "In the entire range from like something like a circle to something like a diamond.",
                    "label": 0
                },
                {
                    "sent": "Therefore you have these tweaking parameter to encourage a regularizer that is exactly the same as a L2 or L1 regularizer that would be within a second and then here is the usual slacking variable.",
                    "label": 0
                },
                {
                    "sent": "So in the end it is.",
                    "label": 0
                },
                {
                    "sent": "You can view them as a approximation to AL, one regularised support vector machine structure.",
                    "label": 0
                },
                {
                    "sent": "Oh, it's up.",
                    "label": 0
                },
                {
                    "sent": "There is no there is no reason for that.",
                    "label": 0
                },
                {
                    "sent": "It's just for convenience and he's actually not a norm.",
                    "label": 0
                },
                {
                    "sent": "Not not.",
                    "label": 0
                },
                {
                    "sent": "Not the technically and Norma just want to avoid using all these terms repeatedly.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's the kind of intuition, and if you compare again with this one, put out one, you can see that it is almost like a. IOS VM or L1 structure SVM, except that now you have some flexibility in controlling the shrinkage effect.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solving this kind of problem is actually not easy as you could expect.",
                    "label": 0
                },
                {
                    "sent": "So here I basically gave you both the primal and dual form of a Laplace maximum entropy screen network and you can see in either the prior and the dual you see something much more complicated than the Gaussian version.",
                    "label": 0
                },
                {
                    "sent": "Here you see this horrible KL norm and here you see this nonlinear effect on the support vectors.",
                    "label": 0
                },
                {
                    "sent": "So we actually get stuck with the available approach.",
                    "label": 0
                },
                {
                    "sent": "Find some trick.",
                    "label": 0
                },
                {
                    "sent": "So what are the trick?",
                    "label": 0
                },
                {
                    "sent": "What the trick is that people no one can write down a. Laplace distribution.",
                    "label": 0
                },
                {
                    "sent": "Using a you know a composite form, which is that you can view this prostitution as a product of a Gaussian distribution over the parameters and conditioning on a parameter which is which is the variance of Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Another itself is following another explosion.",
                    "label": 0
                },
                {
                    "sent": "If you marginalized out this call, you will recover the Laplace distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a tricky way of writing down the plus prior, but it turns out that once you use this representation you can introduce.",
                    "label": 0
                },
                {
                    "sent": "Extra auxiliary random variables, which makes the inference easy.",
                    "label": 0
                },
                {
                    "sent": "The trick is that once.",
                    "label": 0
                },
                {
                    "sent": "So every iteration you hold constant.",
                    "label": 0
                },
                {
                    "sent": "Visa.",
                    "label": 0
                },
                {
                    "sent": "People and cars are the ones you hold it constant.",
                    "label": 0
                },
                {
                    "sent": "You turn the whole thing into a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is the trick.",
                    "label": 0
                },
                {
                    "sent": "No, I'm going to minimize this scale, divergent as usual.",
                    "label": 0
                },
                {
                    "sent": "But what I do is to use a type of iterative operation.",
                    "label": 0
                },
                {
                    "sent": "Proper procedure in the first space I'm going to.",
                    "label": 0
                },
                {
                    "sent": "Ignore this stochastic city I'm holding to be constant and then as you can see, this way is a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "Therefore, I'm not back into a Gaussian maximum entropy Markov network and you can solve this problem trivially using the standard algorithm.",
                    "label": 0
                },
                {
                    "sent": "And Secondly, once you have this posterior PW to be to be solved in the previous iteration, you keep it constant and then try to solve appear off target Lambda and turns out that this is has a nice form you can come up with a code solution to that date to.",
                    "label": 0
                },
                {
                    "sent": "That computer is at all estimation, so we do this iteratively and use nothing but the algorithm used in the network and in the end you hopefully convergence.",
                    "label": 0
                },
                {
                    "sent": "Since this has a spirit of EM algorithm, it is a local operation algorithm, but as usual we can do the usual check of random restarts in order to achieve it.",
                    "label": 0
                },
                {
                    "sent": "Convergence speed is no slower than the stand.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, like when you would expect to see.",
                    "label": 0
                },
                {
                    "sent": "Alright, so now let's see some performance.",
                    "label": 0
                },
                {
                    "sent": "I think I'm verifying to the algorithm.",
                    "label": 0
                },
                {
                    "sent": "I hope to not boring you with more equations, and here are some results.",
                    "label": 0
                },
                {
                    "sent": "So I begin with this very standard.",
                    "label": 0
                },
                {
                    "sent": "Datasets that then used in his very groundbreaking Ncube network and let's see how at least our model compared to that one.",
                    "label": 0
                },
                {
                    "sent": "So we did a pretty exhaustive.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Harrison, on all possible variations you can imagine based on the current models.",
                    "label": 0
                },
                {
                    "sent": "So here we code up.",
                    "label": 0
                },
                {
                    "sent": "Of course, a traditional CRF.",
                    "label": 0
                },
                {
                    "sent": "We code up various ways of regularising the CRF, such as L1 norm and L2 norm, and we called up.",
                    "label": 0
                },
                {
                    "sent": "Of course, the M3 network and Q Network and also.",
                    "label": 0
                },
                {
                    "sent": "People may ask, why don't you just use a direct implementation of the L1 regularised?",
                    "label": 0
                },
                {
                    "sent": "No MQ network.",
                    "label": 0
                },
                {
                    "sent": "It turns out such sympathy exists in the literature, so we got a new paper just to develop that.",
                    "label": 0
                },
                {
                    "sent": "But in the end we actually made it happen and we also have this law plus maximum margin Markov network.",
                    "label": 0
                },
                {
                    "sent": "So here's the comparison and the various.",
                    "label": 0
                },
                {
                    "sent": "The training size.",
                    "label": 0
                },
                {
                    "sent": "You can see that you know this one seems to be constantly outperforming all the other alternatives over different conditions, which is quite comforting effect from the training under standard test data set we went.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this I don't want to show this is kind of the features that actually affect which is not showing significant differences between different algorithms.",
                    "label": 0
                },
                {
                    "sent": "Here is maybe something that is.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why we prefer one thing over the other?",
                    "label": 0
                },
                {
                    "sent": "For example, what about implement the L1M cube network?",
                    "label": 0
                },
                {
                    "sent": "Well, it costs us a new paper.",
                    "label": 0
                },
                {
                    "sent": "That means it's not trivial, but you can still do it.",
                    "label": 0
                },
                {
                    "sent": "The problem is that it turns out that all this appointed as matter of models are kind of sensitive to the magnitude of the regularization coefficients.",
                    "label": 0
                },
                {
                    "sent": "You have to be tricky in tuning that number, and for unknown reason we found that our model seems to be.",
                    "label": 0
                },
                {
                    "sent": "Less sensitive when you are training a training, different magnitude of the coefficients, we didn't get a good insight about that, but I believe maybe it has something to do with this model averaging.",
                    "label": 0
                },
                {
                    "sent": "In fact it is more tolerant to errors, but again, this is not something I'm sure about.",
                    "label": 0
                },
                {
                    "sent": "Affirmative the your students for their fire that's easier.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean that that that that extra parameter is equivalent to the Lambda in your L1 regularization in other models, right?",
                    "label": 0
                },
                {
                    "sent": "Just basing formal expressing that.",
                    "label": 0
                },
                {
                    "sent": "No, no, I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't have those.",
                    "label": 0
                },
                {
                    "sent": "And then when I use the prior I don't like you no longer have this Lambda to write because it's a product effect in the end.",
                    "label": 0
                },
                {
                    "sent": "All the excess is just a symbolic dimension where you change the value of the regulation constant.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm here is in fact all these are not even comparable because you cannot compare different models on the same scale of regularization.",
                    "label": 0
                },
                {
                    "sent": "It's just the way that we choose the 10 different values of regulation and we show you a range of the variance of their performance.",
                    "label": 0
                },
                {
                    "sent": "No, they're not, because the loss function is different.",
                    "label": 0
                },
                {
                    "sent": "Their income problem.",
                    "label": 0
                },
                {
                    "sent": "We basically fixed arrange a reasonable range that we expect to see the good numbers, and then we're going to change that and see how sensitive there.",
                    "label": 0
                },
                {
                    "sent": "So it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not a good graph, not good with autograph.",
                    "label": 0
                },
                {
                    "sent": "So in the paper we actually proud to five different graph.",
                    "label": 0
                },
                {
                    "sent": "So real range of the numbers here.",
                    "label": 0
                },
                {
                    "sent": "I just want to see this effect of how sensitive this whole thing is to changing values of the regularization.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so sensitivity to regular constant and then I want to went on.",
                    "label": 0
                },
                {
                    "sent": "I want to move on to show you some advantages of this model because this lasting arguably is OK Advantage.",
                    "label": 0
                },
                {
                    "sent": "But you can still achieve the same thing using a.",
                    "label": 0
                },
                {
                    "sent": "You know other approach but there are some other advantages which is not so easily achievable using other approaches such as a model with theoretical or hidden structures and this is often a good model to make complex predictions which contain internal.",
                    "label": 0
                },
                {
                    "sent": "You know a semantic structure such as you do Web web page segmentation.",
                    "label": 0
                },
                {
                    "sent": "If you want to just flexor plantation in here you know you lose the insight that the web is composed in.",
                    "label": 0
                },
                {
                    "sent": "A semantically informed way and also syntactically interesting and coherent way.",
                    "label": 0
                },
                {
                    "sent": "And one can easily come up with this labeling Iraqis of the web content.",
                    "label": 0
                },
                {
                    "sent": "The problem, that the reason people don't do that in learning is because getting training information for this internal layers are very, very tedious and you would want to do it.",
                    "label": 0
                },
                {
                    "sent": "Or it's very costly.",
                    "label": 0
                },
                {
                    "sent": "Great, so in our case we're going to treat all this as hidden random variables and we're going to build our structure prediction as usual by assuming that maybe they are no different from the observed features.",
                    "label": 0
                },
                {
                    "sent": "There we often define features based on this graphical model.",
                    "label": 0
                },
                {
                    "sent": "So what do you get in the?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when you have hidden hidden random variables in credit is something very simple becausw in your in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "When you solve this.",
                    "label": 0
                },
                {
                    "sent": "The maximum entropy from network you are already approaching a distribution, which is the distribution of the parameters.",
                    "label": 0
                },
                {
                    "sent": "Now we have more hidden random variables, it's just about.",
                    "label": 0
                },
                {
                    "sent": "Enriching the distribution to be continued, both the parameters and the C&C and your decision boundary has to be made by integrate out now, not only the parameters of the of the model, but also that hidden variables you don't care about.",
                    "label": 0
                },
                {
                    "sent": "OK, so we did nothing in changing the general framework, which is this maximum entropy switching call loss function plus some expected margin constraint on the distributions, except that now we are having one more loop of integration over a.",
                    "label": 0
                },
                {
                    "sent": "Extra subset of random variables.",
                    "label": 0
                },
                {
                    "sent": "So you do this and you imagine the predictive rule will be of course slightly more complicated.",
                    "label": 0
                },
                {
                    "sent": "Now you are going to make integration over the W, and also imagine the city to be discreet when you sum over all the possible these.",
                    "label": 0
                },
                {
                    "sent": "So nothing is significantly different than the training is again very very.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Expect it is going to be quite straightforward because all you did is that you need to introduce some prior now over 2 subsets of hidden random variables, say the WNZ, and you want to be simple by introducing a fully factorizable prior freedom, you can go fancier, but this fully feathered prior gives you the computational simplicity and then again you can imagine a type of EM approach where you keep hold one thing constant and optimize the other.",
                    "label": 0
                },
                {
                    "sent": "Therefore you are resolving a sub problem that we see before in the baseline.",
                    "label": 0
                },
                {
                    "sent": "Maximum entropy screen my whole network, so do this and and in every of this subroutine sub step you can basically using you use the existing standard algorithm of your favor to achieve the solution and you do it a couple of times.",
                    "label": 0
                },
                {
                    "sent": "So in the sense it is more expensive than any of this building block algorithm, But the additional price is roughly linear in a few more steps, more complex because your iteration will converge in a couple steps.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The expected results we have this nontrivial data set.",
                    "label": 0
                },
                {
                    "sent": "We have a bigger one in our final paper, but basically you have some training website with design templates to help you come up with these latent structure.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to evaluate based on the prediction at the base level, or maybe some higher level.",
                    "label": 0
                },
                {
                    "sent": "I don't to be too explicit for that, but basically there's some score routinely used in the World Wide Web community to score.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The later the webpage labeling here is again a performance comparison here.",
                    "label": 0
                },
                {
                    "sent": "You know there are many things.",
                    "label": 0
                },
                {
                    "sent": "There are still compare with because of solving the absolute network will be open problem if hidden variable is present.",
                    "label": 0
                },
                {
                    "sent": "So we did some some of these homemade implementation we have.",
                    "label": 0
                },
                {
                    "sent": "Hey, logical CRF, where we just assume everything is observed but you have not hierarchical features in the labeling space, and then because it is a CRF model which is based on likelihood, we can now train unsupervised fashion so that you can use the emerald hidden random variables.",
                    "label": 0
                },
                {
                    "sent": "Then we also implement this hierarchical Ncube network trained fully supervised Lee and we have this partially observed M3, which is our model and you can see that the performance is what is.",
                    "label": 0
                },
                {
                    "sent": "Average average score for.",
                    "label": 0
                },
                {
                    "sent": "I think this one and that one is dominating over the others.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's a very interesting observation here.",
                    "label": 0
                },
                {
                    "sent": "If you notice, these two things are supposedly to be different because they use different amount of informations.",
                    "label": 0
                },
                {
                    "sent": "This hierarchical entry network fully supervised.",
                    "label": 0
                },
                {
                    "sent": "OK, and this one is only partially supervised, but if you look at the performance they are not as good at the beginning, but when you have more and more training ratio, one more training examples actually pick up.",
                    "label": 0
                },
                {
                    "sent": "That means that even without labeling in the intermediate layer, they can still achieve good performance.",
                    "label": 0
                },
                {
                    "sent": "So that's a pretty nice result, and here is a detailed breakdown.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where you get the performance from on different labels.",
                    "label": 0
                },
                {
                    "sent": "Since I'm running out of time, I'm good.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To rank very, very quickly onto a non trivial extension of this model to because I claim that I want to learn maximum margin and maximum likely graphical MoD.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sort of just structure input all the models.",
                    "label": 0
                },
                {
                    "sent": "So what do we make graphical model?",
                    "label": 0
                },
                {
                    "sent": "So here I have a very familiar graphic model to all of you guys.",
                    "label": 0
                },
                {
                    "sent": "Which is this topic model.",
                    "label": 1
                },
                {
                    "sent": "Public model can be.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He expressed by this related graph and you write on the usual likelihood function, and in this graph of course you have many hidden random variables.",
                    "label": 0
                },
                {
                    "sent": "Everything has been observed if, except those awards.",
                    "label": 0
                },
                {
                    "sent": "And how do we train that?",
                    "label": 0
                },
                {
                    "sent": "Well, you really people optimize the likelihood of the tree of the words and they get something about topic.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I want to resist the temptation of interpreting the parameters as topics that just parameters OK, and then people found out this is a bad idea when your goal is to do prediction.",
                    "label": 0
                },
                {
                    "sent": "Because if you use this topic vector and then update you using to do SVM type of prediction, your performance is really not that great.",
                    "label": 0
                },
                {
                    "sent": "It's worse than just putting a long word vector and the SVM directly, and people wonder why that's the case.",
                    "label": 0
                },
                {
                    "sent": "Well, I think it's very natural because your training is not toward gearing towards optimizing the classification error, it's just the likelihood fitting of the data though.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "There's no real quick site is doing a good job in classification, therefore day Brian, his colleague, went on and introduced explicitly.",
                    "label": 0
                },
                {
                    "sent": "This predicted label OK classification label or maybe regression score and labeled as a function of the word specific topic indicators and you can make this relation by a Gaussian type of model or some other logistic normal prediction model.",
                    "label": 0
                },
                {
                    "sent": "Problems arise by certain way and then you can still run this fully supervised learning and get some division.",
                    "label": 0
                },
                {
                    "sent": "Laundry to be determined.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the grid isn't good either for some unknown RBC coding is too complicated.",
                    "label": 0
                },
                {
                    "sent": "What I mean by not good either is that if you turn the whole document into this framework and then make a influence on this way.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The prediction label isn't as good as you would hope for.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we basically did you do things thing we write down this loss function.",
                    "label": 0
                },
                {
                    "sent": "OK, using the same graphical model and the loss functions defined over cross entropy and the select variable select functions over the division and then these are all the complicated expression of this margin constraint.",
                    "label": 0
                },
                {
                    "sent": "But at the end of the day you use the same procedure as presented before and you get the same estimator of all this latent parameters except.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now you are using a different division criteria or not the loss function and.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Active.",
                    "label": 0
                },
                {
                    "sent": "So without going through the technical details, I think it's.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good to finish the talk with some high level presentation of the results.",
                    "label": 0
                },
                {
                    "sent": "Actually, before I go to the results, I want to basically highlight that you can use this idea to train.",
                    "label": 0
                },
                {
                    "sent": "Very general topic models of arbitrary design.",
                    "label": 0
                },
                {
                    "sent": "You can introduce more features, do according topic model or things like that.",
                    "label": 0
                },
                {
                    "sent": "In the end, you basically do you know design on the loss function to be apart from model fitting.",
                    "label": 0
                },
                {
                    "sent": "Apart from predictive accuracy, apart from the selecting random variable and then you somehow composed this expect margin constraint so everything has been pretty standard now.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at the results.",
                    "label": 0
                },
                {
                    "sent": "So this is the standard result used in the same paper that they wrote and.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Firstly, we want to visualize topics well.",
                    "label": 0
                },
                {
                    "sent": "It's hard to say which one is better, but at least in the Med LDA you'll see a slight less convolution of the topics in it, even if it's 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And if you really.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the award in this topic, you know I couldn't detect any significant meaningful difference, but the.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Performance actually shows some very nice insight.",
                    "label": 0
                },
                {
                    "sent": "We evaluate this model in terms of the predictive accuracy, which is the classification label of every document using this criteria, and again we compare it with a number of other competing models.",
                    "label": 0
                },
                {
                    "sent": "The baseline is very natural, you just trained SVM and then you use that topic vector freedom into a training LDA use topic to freedom into SVN.",
                    "label": 0
                },
                {
                    "sent": "You get a prediction that is in here we set them to be 0.",
                    "label": 0
                },
                {
                    "sent": "So she does the absolute difference of others.",
                    "label": 0
                },
                {
                    "sent": "The second one is the supervised LDA index paper which is in here.",
                    "label": 0
                },
                {
                    "sent": "This one is the disk ID, which is a different formulation by Simone and Fish are a few years ago and later than this, but still quite interesting model which promotes another way of discriminating between LDA model.",
                    "label": 0
                },
                {
                    "sent": "And this is our model.",
                    "label": 0
                },
                {
                    "sent": "This is our model.",
                    "label": 0
                },
                {
                    "sent": "So the way I present this is the reason I presented this tool as follows.",
                    "label": 0
                },
                {
                    "sent": "So we said that that mentality is trained based on a Max margin framework.",
                    "label": 0
                },
                {
                    "sent": "So you can directly do classification by putting in the document and get the label from that, but also as a side product you also get a topic vector from that document.",
                    "label": 0
                },
                {
                    "sent": "You can do another round of SVM to make a new prediction and in this case you can see these two things are not doing very similar things.",
                    "label": 0
                },
                {
                    "sent": "Basically the.",
                    "label": 0
                },
                {
                    "sent": "The performance came you would like to hope for in the SVM are already almost fully capitalized in the LDA framework, so that means the media is really doing what they're supposed to do by harvesting or leveraging on the maximum likelihood estimation and next margin estimation.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I think I'm almost there.",
                    "label": 0
                },
                {
                    "sent": "You can train the same thing based on predictive labels such as movie scores, which is a real number and the same thing.",
                    "label": 0
                },
                {
                    "sent": "We can again, you know, get this quite dramatic performance game over other standard implementations.",
                    "label": 0
                },
                {
                    "sent": "I have to point out that there are some even nicer result because now the LDA them at LDA is a likelihood based model.",
                    "label": 0
                },
                {
                    "sent": "You don't have to even train it in the fully supervised fashion.",
                    "label": 0
                },
                {
                    "sent": "You can actually do cultural labeling and still get a nice performance.",
                    "label": 0
                },
                {
                    "sent": "I know in this as indicated in this magenta line in the performance is still better than the baseline which is fully supervised.",
                    "label": 0
                },
                {
                    "sent": "I think I think I'm running out of time.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Efficiency is pretty good on this new model, and as a summary I hope I convey the message that.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At least there is a way to combine the benefit of a likelihood based training and a margin based training and the the type of cost you have to pay in terms of inference and in terms of learning is is reasonable without the need of having too much demanding other cost on time and on the on the membrane.",
                    "label": 0
                },
                {
                    "sent": "Things like that.",
                    "label": 0
                },
                {
                    "sent": "And there are various interesting extension you can put to train model with.",
                    "label": 0
                },
                {
                    "sent": "Even running variables on the waist very fancy sparsity factors in psychology.",
                    "label": 0
                },
                {
                    "sent": "OK, I think I'm done.",
                    "label": 0
                },
                {
                    "sent": "I just want to remind you this full framework we are in here now and I want to acknowledge the people who is involved in this work.",
                    "label": 0
                },
                {
                    "sent": "The major piece of the work was done with a very capable postdoc right now in my group Ginger and Arm.",
                    "label": 0
                },
                {
                    "sent": "It also helped with the design and implementation of the LDA model and here are the funding agencies that is responsible for supporting the work.",
                    "label": 0
                },
                {
                    "sent": "OK, I think I'm done.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much for your patience.",
                    "label": 0
                }
            ]
        }
    }
}