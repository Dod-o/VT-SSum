{
    "id": "mqdapgz6fifjdqvdztscrotsfy4ut6dx",
    "title": "Fast first-order methods for convex optimization with line search",
    "info": {
        "author": [
            "Katya Scheinberg, Lehigh University"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Optimization Methods->Convex Optimization"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_scheinberg_convexoptimization/",
    "segmentation": [
        [
            "So I am going basically to.",
            "Try to smooth the difference between smooth and non smooth.",
            "In this talk."
        ],
        [
            "OK, but I'm not going to do it in obvious ways.",
            "1st I'll just talk about very classical setting that you know have been probably.",
            "Got a lot of attention, probably next next couple of years ago.",
            "Now everybody's stochastic so I'm not going to be stochastic.",
            "Will just be deterministic.",
            "So composite function that we have and we're going to apply a standard 1st order algorithm to it.",
            "So one of the functions is assumed to be smooth for now.",
            "Well, it will be assumed to be smooth and the other is not smooth, but somewhat nice and one of the easiest things to use, which again a couple of years ago.",
            "Everybody was basically using for it is East Office algorithm, which is what I will be talking about so."
        ],
        [
            "Basically, the applications are very numerous, less so.",
            "The group plus, so the matrix completion robust PCA, which actually both these functions are not smooth here.",
            "But again you can smooth one of them.",
            "Sparse inverse covariance selections and so on.",
            "So lots of money."
        ],
        [
            "Learning problems fall into this category, So what are the basics?"
        ],
        [
            "Algorithms that we do, so the basic algorithm just has been already mentioned several times here.",
            "If you have a smooth function, what you can do is you basically build a quadratic over approximation of it by taking basically the linear approximation and adding the prox term, and so you're hoping that you will have something that sits above your nice and smooth term, which is, you know, been mentioned in the previous talk and in others and the other one that is kind of the non smooth term, but the easy one we just keep as is.",
            "Right?",
            "And then what we're going to do is basically optimize this quadratic.",
            "Well, it's not a quadratic function because this is not quadratic, but we'll call it quadratic because it's a quadratic approximation of 1 and then."
        ],
        [
            "You know the other is, it is so we take this quadratic, we minimize it.",
            "And you know this is our next sitter it right?",
            "And then there is this parameter which still Boyd was calling row, which nobody knows how to set.",
            "And this is what I'll be talking about exactly.",
            "What do we do with this parameter?",
            "So there is this parameter which is really crucial because first of all we need certain assumptions on it to prove rates of convergence.",
            "But on the other hand it really it is the step size in many ways it's a little bit less obvious, so if I didn't have this term at all.",
            "That would be exactly the step size that would be the learning rate, right?",
            "Because we would just be making gradient steps, but because I have this extra term now, things are a little bit more complicated.",
            "It's just it's a gradient step with some kind of shrinking.",
            "But still this mu determines basically.",
            "So if this new is very small, my steps are very small.",
            "If I multiplied if I managed to make my mu twice as big and still get good results, sufficient decrease whatever, I basically will converge twice as fast.",
            "And the the classical results say, well, if mu is less than 1 / L, where L is that Lipschitz constant.",
            "Then we can guarantee that this isn't over relaxation of the function, so we guarantee this basically here that this is quadratic.",
            "Estimated that the next iterate.",
            "This is the original function at the next iterate and the quadratic lies above the next function.",
            "So everything is great will converge with the rate over."
        ],
        [
            "Of over epsilon and then there is Nestor Ized version.",
            "And it's Pfister basically this particular one which says OK, will do the same thing.",
            "But now we're not going to actually build the approximations at the new iterates.",
            "We're going to make some kind of extra steps.",
            "Will compute the neutered, and then we're going to take the linear combination between the past it and then you iterate OK, Anne will move a little bit, and that would be actually where we're building our approximations.",
            "I assume most people are here familiar with this, and by doing this we actually buy ourselves a much better rate of convergence.",
            "So we get this square root of L over EPS.",
            "OK, and again, the mu is assumed to be small enough.",
            "OK, so very often people don't write this particular form of Pfister.",
            "They put basically K plus one or whatever came with plus 3 something like that.",
            "But I'll show why I want this particular form because it really works here for me.",
            "OK, so this is."
        ],
        [
            "Basic setting.",
            "Now of course, you know for."
        ],
        [
            "For awhile already, it's been well known that just first of all, you don't know that L. So you can't set mu one over less than 1 / L and 2nd.",
            "Doing that is a nasty thing because you're not going to really converge very fast.",
            "So in Pfister basically, and is that there's been a proposed away very simple to do.",
            "Basically a backtracking strategy.",
            "I wouldn't, I was.",
            "I used to call it line search, but it's not really a line search because you're not back tracking along the line.",
            "You're more like changing this parameter, and each time you re optimizing, so you're not exactly doing align search, but basically what you're doing is you're So what you need is that this condition that this quadratic lies above.",
            "The function not everywhere, because this is the condition."
        ],
        [
            "For this one guarantees really that this quadratic thing lies above the function everywhere, but we really don't care about everywhere what we care about is at the next literate, so we really care about what happens.",
            "Then this condition we can even check, right?",
            "This is nothing.",
            "I mean, it's computationally maybe expensive.",
            "Depends, but we can check this condition.",
            "So there is a backtracking idea.",
            "So try some UK here.",
            "You know, compute your, you know, do your minimization check this condition.",
            "If it works, continue.",
            "OK, and then of course in the basic case it will still converge with 1 / L epsilon because you know that for a fact that new K is not going to be less than one over epsilon, or maybe a constant times one over epsilon, so you'll converge.",
            "Same thing was true for Pfister.",
            "The backtracking condition was still the same.",
            "The problem was there is 1 little problem is that all the same thing works except where you can start.",
            "So this backtracking you have to.",
            "Yeah, the first guess of what the parameter should be, and then you have to maybe shrink it.",
            "The problem is that for Pfister you had to keep on shrinking it, so if you previous iterate had a small step size, all the others after that will have a small step size, and as it's a known kind of you know bad thing about 15 people just ignoring it.",
            "But if you just ignore it, actually in theory it doesn't work, so you're not going to get this convergence rates.",
            "So what I'll show you now is basically that we can get the same convergence rates if we're just a little bit more carefully and allow you to be big.",
            "Now why am I talking about this big?",
            "You know?",
            "In terms of smooth and nonsmooth, so, if I have a nonsmooth function I can smooth it right, and it's actually really no difference, not smooth function or smooth nonsmooth function, so, but now I'm in a smooth case, so I can do all this stuff except for, of course, my problem is not Now this L is going to be very large, because at the places where I smoothed, my Lipschitz constant is going to be very large, OK, but but then OK.",
            "So if I really if this is my convergence rates indeed, then the algorithm would not work very well for nonsmooth.",
            "For a smooth nonsmooth case.",
            "However, basically what I'm trying to say here is if we allow ourselves to make large steps, maybe we'll be lucky.",
            "Maybe most of the cases will be making fairly large steps OK, and then if occasionally we meet the place where things are really non smooth and we really have to make small steps will recover from that eventually, so it's not going to affect our whole algorithm.",
            "Our steps are not going to be bounded from above by 1 / L, so we're going to do better.",
            "At least you know we're hoping so.",
            "It's kind of a.",
            "Analysis What I'm going to do is an analysis that the way I think one should look at these methods, and I think it explains their behavior much better than this kind of constant, because in this kind of concept, when you have an awesome function, you smooth it, you really get a lousy converge."
        ],
        [
            "But in practice, it doesn't.",
            "It's not necessarily so.",
            "OK so back to illuminating this condition.",
            "So what what's happening right now in Vista?",
            "Anna backtracking is that you have this condition.",
            "After that you pick some UK.",
            "You do the minimization, you check the condition and you keep in.",
            "Keep iterating here maybe a few times if you have to.",
            "And then you find your mu, K and then you compute this parameter which basically tells you how far you're going to move.",
            "From your current iterates, you know along the kind of the previous direction.",
            "So how much you're going to apply this Nesterov acceleration, and then again then you continue OK, and if you do that, you basically going to get this.",
            "This is where the convergence rate comes from, right?",
            "You're going to get the difference between the current function of the current iteration, the function at the optimum is bounded by two L times this classical distance Bregman turn.",
            "Over K ^2.",
            "OK so this is this is classical fist analysis.",
            "Now The thing is, so we're trying to get rid of this.",
            "Point is that we don't really need this.",
            "We don't really need this.",
            "What we act."
        ],
        [
            "Really need is this.",
            "We need this condition.",
            "It's a condition on both stepsize anti K squared.",
            "If you just go through Vista analysis and look at it slightly differently.",
            "This is exactly what we need.",
            "So these two things are very connected and basically so this is the step size that we managed to make.",
            "This is the parameter.",
            "Which dictates how much we are, you know, deviating from standard gradient to the accelerated gradient.",
            "Basically in some sense.",
            "OK, and now what what?",
            "Let's just look at it for a little bit this product.",
            "This funny product has to be less or equal than this, so if we want to if we keep reducing the step size then TK squared can equal to TK.",
            "I mean ticket plus whatever blah blah from which we are getting the condition that."
        ],
        [
            "OK, let me go back here.",
            "This case squared actually is the."
        ],
        [
            "OK, now I can do it from here.",
            "Sorry.",
            "So if we do this if we keep this maintaining this condition then we get this bound.",
            "Before we had this two L / K ^2, now we have two mu Katie K squared here.",
            "OK so if you keep doing this to the way it was done then TK squared.",
            "You can show that TK grows basically SK does.",
            "It's actually something like a / 2 OK it grows with K but now if we want to make.",
            "These steps larger than these, so if you want to increase this step size, we have to decrease TK plus one.",
            "So basically, if we have a small step to allow for the next step to be better, we have to make this smaller, but that kind of contradicts the fact that we want these guys to grow fast because this is where we get our case squared term.",
            "But on the other hand, when would we need to make this step bigger than this?",
            "Only when this stuff happened to be small, which means that we actually reduced the step before that.",
            "So sometime before we happened to shrink our step size, we happen to make backtracking and we didn't.",
            "We couldn't go very far because of some conditions.",
            "OK, so in that case we actually can make this much larger.",
            "We can bank on that and make this much larger than we do right now.",
            "So it's basically a balance between the two things that you can do OK and.",
            "And it it actually has a nice kind of intuition.",
            "So if your step size had reduced significantly, you're going to make this thing larger.",
            "And what does it mean?",
            "You make this thing large.",
            "It means you're basically moving away from your current iterate less aggressively, basically OK.",
            "So alright, so this is basically this is what we get.",
            "Now we have.",
            "The only thing is the only thing is we can't compute this without knowing this.",
            "And this thing we actually only know.",
            "At the next iteration, so."
        ],
        [
            "Now we just have an extra loop, not an extra loop.",
            "We expand our loop so instead of just looping with these two things were now looping through the whole thing, but the number of loops we have to do actually is logarithmic because every time we're shrinking the parameter by a constant.",
            "So if you guessed it wrong, it is really logarithmic.",
            "Moreover, you can do some kind of more adaptive clever strategies, in which case the number of such loops is going to be at most twice as much as the number of iterates, so it's really you not increasing the number with this very significantly.",
            "Here, and you're allowing yourself to make much bigger steps.",
            "And actually, it's funny because in particular case when you for example if your F of X is quadratic like in law saw group lasso, then this actually doesn't cost you anything.",
            "Recomputing this in this because you're changing is only this parameter and your gradient is linear in terms of your.",
            "Basically in terms of your X in terms of you iterate.",
            "So re combining things with the different linear combinations does not cost you any extra gradient computation.",
            "I don't have time to.",
            "Describe this, but it's really."
        ],
        [
            "It's not not a big deal quite often.",
            "OK, so here is another.",
            "Key thing.",
            "So now what we can do is we can do the analysis so we have this different term here.",
            "And basically we can bound this term in general with this.",
            "Now what is this?",
            "This is an average of the step sizes.",
            "It's a little bit more complicated than average.",
            "It's an average of.",
            "It's an average of square roots, whatever, but basically it's the average of the step sizes you've taken along the way."
        ],
        [
            "So now we have a.",
            "Basically we have the following convergence rate here.",
            "So basically the number of iterations it takes is a square root of some number times epsilon, and that number is actually it's kind of a posteriori analysis.",
            "It's an average of the step sizes you've taken, but The thing is, we just there was just discussion about this.",
            "If you function is non smooth at current step in some directions it might be very smooth in some other direction in which you obtain a very good dissent.",
            "So if you can analyze which you know, I don't know how to do that, but I think in certain cases you can analyze that this step is going to be much better than basically the smoothing you know then one over epsilon or 1 / L, right?",
            "So if you can analyze this, you're getting basically a potential for much better convergence rates for nonsmooth functions, OK or not, just non smooth functions, but general functions with."
        ],
        [
            "Bad Lipschitz constants.",
            "OK, there is some computational results.",
            "I want to get a little bit to the next topic, just very quickly.",
            "The computational results show that on some standard last source type problems we do get gain in number of multiplications, clearly iterations, but mostly we are comparing number of multiplications.",
            "Most cases we do get again from doing this strategy OK."
        ],
        [
            "Don't have, I mean mostly it's interesting from theoretical point of view I think OK. Now I have 3 minutes and I just wanted to talk about."
        ],
        [
            "Because the void was talking bout alternating directions, there's one particular case where these alternating directions can be treated exactly the same way as Eastern Vista, and that's when you have two functions.",
            "So when you have just two functions, so I have no time, but hopefully those of you have been to Steve, I'd like to remember this if I have two functions that I'm optimizing, I now can basically split them with two variables right?",
            "This condition and Adam mounted Lagrangian term here.",
            "And the classical."
        ],
        [
            "Alternating direction methods then does this minimize this function now?"
        ],
        [
            "Notice I called the same function again Q. OK, it's not a quadratic function generally, but again, call it Q.",
            "It's an invented log."
        ],
        [
            "Function here and I minimize this Q in one variable and minimize it in another variable on another set of variables and then I am updating my LaGrange multiplier.",
            "OK, so if we now switch.",
            "Here what we do.",
            "Minimizing one variable minimizing another update LaGrange multiplier.",
            "But then why do we update model against multiple after this variable?",
            "Why don't we updated?"
        ],
        [
            "So this variable.",
            "So if we do this, if we updated every time what we're going?"
        ],
        [
            "To get is basically kind of double Fister if you want or double Easter in this particular case, so you treat first F of X as you did in Pfister.",
            "You linearize it approximated and do minimization, and then you treat the other one the same way, so you basically alternating between two easterfest steps."
        ],
        [
            "And it really actually is very powerful and I don't have time to do this, but you do get accelerated version of that and you just gain more.",
            "By doing this because very often for the applications that I've talked about, computing one gradient is the same as minimizing over."
        ],
        [
            "Basically, it's the same as doing this extra step and you kind of get the gradient for free there so."
        ],
        [
            "Again, I don't have."
        ],
        [
            "Thanks for this and you basically have some kind of similar convergence rates and again the same things with the backtracking and everything else applies there."
        ],
        [
            "And actually just one thing is things.",
            "So the step sizes average out even more, so the steps that you can make different step sizes for one function on the other four.",
            "So each of your alternating steps, that role that Steve was talking about can be different for every function, and then the road that goes into convergence theory is kind of the average on them.",
            "You in my case it's the average.",
            "So basically if one function behaves really well and the other really badly together they'll behave sort of better than the bad ones.",
            "So the bad one don't hurt you as long as you have one good one.",
            "In a way.",
            "OK alright sorry, I've tried to cram a lot into this talk.",
            "OK."
        ],
        [
            "The only thing is here you have a problem.",
            "So without the fast method with, when you don't do the fast method, it's fine.",
            "When you do the fast method then the gradient you have to.",
            "I mean you have to compute it because this Z is not this or this, so you don't actually get the update.",
            "Maybe one can figure it out how to do it.",
            "I don't know how to do it, but it doesn't.",
            "So here you would need."
        ],
        [
            "The gradient, but here yes, I write the gradient for the standard kind of Vista, but it actually comes to you for free.",
            "You just get it and then you can do exact methods this way and compute the gradient, but I just didn't understand right now you don't need, you don't need to.",
            "I mean, you actually know the important thing is why?",
            "So if your function is not smooth you do get subgradient here.",
            "Yeah, you do an.",
            "I mean whether it's a useful one, I don't know, but it just works.",
            "I mean most of the time, it just works."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I am going basically to.",
                    "label": 0
                },
                {
                    "sent": "Try to smooth the difference between smooth and non smooth.",
                    "label": 0
                },
                {
                    "sent": "In this talk.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, but I'm not going to do it in obvious ways.",
                    "label": 0
                },
                {
                    "sent": "1st I'll just talk about very classical setting that you know have been probably.",
                    "label": 0
                },
                {
                    "sent": "Got a lot of attention, probably next next couple of years ago.",
                    "label": 0
                },
                {
                    "sent": "Now everybody's stochastic so I'm not going to be stochastic.",
                    "label": 0
                },
                {
                    "sent": "Will just be deterministic.",
                    "label": 0
                },
                {
                    "sent": "So composite function that we have and we're going to apply a standard 1st order algorithm to it.",
                    "label": 0
                },
                {
                    "sent": "So one of the functions is assumed to be smooth for now.",
                    "label": 0
                },
                {
                    "sent": "Well, it will be assumed to be smooth and the other is not smooth, but somewhat nice and one of the easiest things to use, which again a couple of years ago.",
                    "label": 0
                },
                {
                    "sent": "Everybody was basically using for it is East Office algorithm, which is what I will be talking about so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically, the applications are very numerous, less so.",
                    "label": 0
                },
                {
                    "sent": "The group plus, so the matrix completion robust PCA, which actually both these functions are not smooth here.",
                    "label": 1
                },
                {
                    "sent": "But again you can smooth one of them.",
                    "label": 0
                },
                {
                    "sent": "Sparse inverse covariance selections and so on.",
                    "label": 0
                },
                {
                    "sent": "So lots of money.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning problems fall into this category, So what are the basics?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithms that we do, so the basic algorithm just has been already mentioned several times here.",
                    "label": 0
                },
                {
                    "sent": "If you have a smooth function, what you can do is you basically build a quadratic over approximation of it by taking basically the linear approximation and adding the prox term, and so you're hoping that you will have something that sits above your nice and smooth term, which is, you know, been mentioned in the previous talk and in others and the other one that is kind of the non smooth term, but the easy one we just keep as is.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And then what we're going to do is basically optimize this quadratic.",
                    "label": 0
                },
                {
                    "sent": "Well, it's not a quadratic function because this is not quadratic, but we'll call it quadratic because it's a quadratic approximation of 1 and then.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know the other is, it is so we take this quadratic, we minimize it.",
                    "label": 0
                },
                {
                    "sent": "And you know this is our next sitter it right?",
                    "label": 0
                },
                {
                    "sent": "And then there is this parameter which still Boyd was calling row, which nobody knows how to set.",
                    "label": 0
                },
                {
                    "sent": "And this is what I'll be talking about exactly.",
                    "label": 0
                },
                {
                    "sent": "What do we do with this parameter?",
                    "label": 0
                },
                {
                    "sent": "So there is this parameter which is really crucial because first of all we need certain assumptions on it to prove rates of convergence.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand it really it is the step size in many ways it's a little bit less obvious, so if I didn't have this term at all.",
                    "label": 0
                },
                {
                    "sent": "That would be exactly the step size that would be the learning rate, right?",
                    "label": 0
                },
                {
                    "sent": "Because we would just be making gradient steps, but because I have this extra term now, things are a little bit more complicated.",
                    "label": 0
                },
                {
                    "sent": "It's just it's a gradient step with some kind of shrinking.",
                    "label": 0
                },
                {
                    "sent": "But still this mu determines basically.",
                    "label": 0
                },
                {
                    "sent": "So if this new is very small, my steps are very small.",
                    "label": 0
                },
                {
                    "sent": "If I multiplied if I managed to make my mu twice as big and still get good results, sufficient decrease whatever, I basically will converge twice as fast.",
                    "label": 0
                },
                {
                    "sent": "And the the classical results say, well, if mu is less than 1 / L, where L is that Lipschitz constant.",
                    "label": 0
                },
                {
                    "sent": "Then we can guarantee that this isn't over relaxation of the function, so we guarantee this basically here that this is quadratic.",
                    "label": 0
                },
                {
                    "sent": "Estimated that the next iterate.",
                    "label": 0
                },
                {
                    "sent": "This is the original function at the next iterate and the quadratic lies above the next function.",
                    "label": 0
                },
                {
                    "sent": "So everything is great will converge with the rate over.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of over epsilon and then there is Nestor Ized version.",
                    "label": 0
                },
                {
                    "sent": "And it's Pfister basically this particular one which says OK, will do the same thing.",
                    "label": 0
                },
                {
                    "sent": "But now we're not going to actually build the approximations at the new iterates.",
                    "label": 0
                },
                {
                    "sent": "We're going to make some kind of extra steps.",
                    "label": 0
                },
                {
                    "sent": "Will compute the neutered, and then we're going to take the linear combination between the past it and then you iterate OK, Anne will move a little bit, and that would be actually where we're building our approximations.",
                    "label": 0
                },
                {
                    "sent": "I assume most people are here familiar with this, and by doing this we actually buy ourselves a much better rate of convergence.",
                    "label": 0
                },
                {
                    "sent": "So we get this square root of L over EPS.",
                    "label": 0
                },
                {
                    "sent": "OK, and again, the mu is assumed to be small enough.",
                    "label": 0
                },
                {
                    "sent": "OK, so very often people don't write this particular form of Pfister.",
                    "label": 0
                },
                {
                    "sent": "They put basically K plus one or whatever came with plus 3 something like that.",
                    "label": 0
                },
                {
                    "sent": "But I'll show why I want this particular form because it really works here for me.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basic setting.",
                    "label": 0
                },
                {
                    "sent": "Now of course, you know for.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For awhile already, it's been well known that just first of all, you don't know that L. So you can't set mu one over less than 1 / L and 2nd.",
                    "label": 0
                },
                {
                    "sent": "Doing that is a nasty thing because you're not going to really converge very fast.",
                    "label": 0
                },
                {
                    "sent": "So in Pfister basically, and is that there's been a proposed away very simple to do.",
                    "label": 0
                },
                {
                    "sent": "Basically a backtracking strategy.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't, I was.",
                    "label": 0
                },
                {
                    "sent": "I used to call it line search, but it's not really a line search because you're not back tracking along the line.",
                    "label": 0
                },
                {
                    "sent": "You're more like changing this parameter, and each time you re optimizing, so you're not exactly doing align search, but basically what you're doing is you're So what you need is that this condition that this quadratic lies above.",
                    "label": 0
                },
                {
                    "sent": "The function not everywhere, because this is the condition.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this one guarantees really that this quadratic thing lies above the function everywhere, but we really don't care about everywhere what we care about is at the next literate, so we really care about what happens.",
                    "label": 0
                },
                {
                    "sent": "Then this condition we can even check, right?",
                    "label": 0
                },
                {
                    "sent": "This is nothing.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's computationally maybe expensive.",
                    "label": 0
                },
                {
                    "sent": "Depends, but we can check this condition.",
                    "label": 0
                },
                {
                    "sent": "So there is a backtracking idea.",
                    "label": 0
                },
                {
                    "sent": "So try some UK here.",
                    "label": 0
                },
                {
                    "sent": "You know, compute your, you know, do your minimization check this condition.",
                    "label": 0
                },
                {
                    "sent": "If it works, continue.",
                    "label": 0
                },
                {
                    "sent": "OK, and then of course in the basic case it will still converge with 1 / L epsilon because you know that for a fact that new K is not going to be less than one over epsilon, or maybe a constant times one over epsilon, so you'll converge.",
                    "label": 0
                },
                {
                    "sent": "Same thing was true for Pfister.",
                    "label": 0
                },
                {
                    "sent": "The backtracking condition was still the same.",
                    "label": 0
                },
                {
                    "sent": "The problem was there is 1 little problem is that all the same thing works except where you can start.",
                    "label": 0
                },
                {
                    "sent": "So this backtracking you have to.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the first guess of what the parameter should be, and then you have to maybe shrink it.",
                    "label": 0
                },
                {
                    "sent": "The problem is that for Pfister you had to keep on shrinking it, so if you previous iterate had a small step size, all the others after that will have a small step size, and as it's a known kind of you know bad thing about 15 people just ignoring it.",
                    "label": 0
                },
                {
                    "sent": "But if you just ignore it, actually in theory it doesn't work, so you're not going to get this convergence rates.",
                    "label": 0
                },
                {
                    "sent": "So what I'll show you now is basically that we can get the same convergence rates if we're just a little bit more carefully and allow you to be big.",
                    "label": 0
                },
                {
                    "sent": "Now why am I talking about this big?",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "In terms of smooth and nonsmooth, so, if I have a nonsmooth function I can smooth it right, and it's actually really no difference, not smooth function or smooth nonsmooth function, so, but now I'm in a smooth case, so I can do all this stuff except for, of course, my problem is not Now this L is going to be very large, because at the places where I smoothed, my Lipschitz constant is going to be very large, OK, but but then OK.",
                    "label": 0
                },
                {
                    "sent": "So if I really if this is my convergence rates indeed, then the algorithm would not work very well for nonsmooth.",
                    "label": 0
                },
                {
                    "sent": "For a smooth nonsmooth case.",
                    "label": 0
                },
                {
                    "sent": "However, basically what I'm trying to say here is if we allow ourselves to make large steps, maybe we'll be lucky.",
                    "label": 0
                },
                {
                    "sent": "Maybe most of the cases will be making fairly large steps OK, and then if occasionally we meet the place where things are really non smooth and we really have to make small steps will recover from that eventually, so it's not going to affect our whole algorithm.",
                    "label": 0
                },
                {
                    "sent": "Our steps are not going to be bounded from above by 1 / L, so we're going to do better.",
                    "label": 0
                },
                {
                    "sent": "At least you know we're hoping so.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a.",
                    "label": 0
                },
                {
                    "sent": "Analysis What I'm going to do is an analysis that the way I think one should look at these methods, and I think it explains their behavior much better than this kind of constant, because in this kind of concept, when you have an awesome function, you smooth it, you really get a lousy converge.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in practice, it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It's not necessarily so.",
                    "label": 0
                },
                {
                    "sent": "OK so back to illuminating this condition.",
                    "label": 0
                },
                {
                    "sent": "So what what's happening right now in Vista?",
                    "label": 0
                },
                {
                    "sent": "Anna backtracking is that you have this condition.",
                    "label": 0
                },
                {
                    "sent": "After that you pick some UK.",
                    "label": 0
                },
                {
                    "sent": "You do the minimization, you check the condition and you keep in.",
                    "label": 0
                },
                {
                    "sent": "Keep iterating here maybe a few times if you have to.",
                    "label": 0
                },
                {
                    "sent": "And then you find your mu, K and then you compute this parameter which basically tells you how far you're going to move.",
                    "label": 0
                },
                {
                    "sent": "From your current iterates, you know along the kind of the previous direction.",
                    "label": 0
                },
                {
                    "sent": "So how much you're going to apply this Nesterov acceleration, and then again then you continue OK, and if you do that, you basically going to get this.",
                    "label": 0
                },
                {
                    "sent": "This is where the convergence rate comes from, right?",
                    "label": 0
                },
                {
                    "sent": "You're going to get the difference between the current function of the current iteration, the function at the optimum is bounded by two L times this classical distance Bregman turn.",
                    "label": 0
                },
                {
                    "sent": "Over K ^2.",
                    "label": 0
                },
                {
                    "sent": "OK so this is this is classical fist analysis.",
                    "label": 0
                },
                {
                    "sent": "Now The thing is, so we're trying to get rid of this.",
                    "label": 0
                },
                {
                    "sent": "Point is that we don't really need this.",
                    "label": 0
                },
                {
                    "sent": "We don't really need this.",
                    "label": 0
                },
                {
                    "sent": "What we act.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really need is this.",
                    "label": 0
                },
                {
                    "sent": "We need this condition.",
                    "label": 0
                },
                {
                    "sent": "It's a condition on both stepsize anti K squared.",
                    "label": 0
                },
                {
                    "sent": "If you just go through Vista analysis and look at it slightly differently.",
                    "label": 0
                },
                {
                    "sent": "This is exactly what we need.",
                    "label": 0
                },
                {
                    "sent": "So these two things are very connected and basically so this is the step size that we managed to make.",
                    "label": 0
                },
                {
                    "sent": "This is the parameter.",
                    "label": 0
                },
                {
                    "sent": "Which dictates how much we are, you know, deviating from standard gradient to the accelerated gradient.",
                    "label": 0
                },
                {
                    "sent": "Basically in some sense.",
                    "label": 0
                },
                {
                    "sent": "OK, and now what what?",
                    "label": 0
                },
                {
                    "sent": "Let's just look at it for a little bit this product.",
                    "label": 0
                },
                {
                    "sent": "This funny product has to be less or equal than this, so if we want to if we keep reducing the step size then TK squared can equal to TK.",
                    "label": 0
                },
                {
                    "sent": "I mean ticket plus whatever blah blah from which we are getting the condition that.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let me go back here.",
                    "label": 0
                },
                {
                    "sent": "This case squared actually is the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now I can do it from here.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So if we do this if we keep this maintaining this condition then we get this bound.",
                    "label": 0
                },
                {
                    "sent": "Before we had this two L / K ^2, now we have two mu Katie K squared here.",
                    "label": 0
                },
                {
                    "sent": "OK so if you keep doing this to the way it was done then TK squared.",
                    "label": 0
                },
                {
                    "sent": "You can show that TK grows basically SK does.",
                    "label": 0
                },
                {
                    "sent": "It's actually something like a / 2 OK it grows with K but now if we want to make.",
                    "label": 0
                },
                {
                    "sent": "These steps larger than these, so if you want to increase this step size, we have to decrease TK plus one.",
                    "label": 0
                },
                {
                    "sent": "So basically, if we have a small step to allow for the next step to be better, we have to make this smaller, but that kind of contradicts the fact that we want these guys to grow fast because this is where we get our case squared term.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, when would we need to make this step bigger than this?",
                    "label": 0
                },
                {
                    "sent": "Only when this stuff happened to be small, which means that we actually reduced the step before that.",
                    "label": 0
                },
                {
                    "sent": "So sometime before we happened to shrink our step size, we happen to make backtracking and we didn't.",
                    "label": 0
                },
                {
                    "sent": "We couldn't go very far because of some conditions.",
                    "label": 0
                },
                {
                    "sent": "OK, so in that case we actually can make this much larger.",
                    "label": 0
                },
                {
                    "sent": "We can bank on that and make this much larger than we do right now.",
                    "label": 0
                },
                {
                    "sent": "So it's basically a balance between the two things that you can do OK and.",
                    "label": 0
                },
                {
                    "sent": "And it it actually has a nice kind of intuition.",
                    "label": 0
                },
                {
                    "sent": "So if your step size had reduced significantly, you're going to make this thing larger.",
                    "label": 0
                },
                {
                    "sent": "And what does it mean?",
                    "label": 0
                },
                {
                    "sent": "You make this thing large.",
                    "label": 0
                },
                {
                    "sent": "It means you're basically moving away from your current iterate less aggressively, basically OK.",
                    "label": 0
                },
                {
                    "sent": "So alright, so this is basically this is what we get.",
                    "label": 0
                },
                {
                    "sent": "Now we have.",
                    "label": 0
                },
                {
                    "sent": "The only thing is the only thing is we can't compute this without knowing this.",
                    "label": 0
                },
                {
                    "sent": "And this thing we actually only know.",
                    "label": 0
                },
                {
                    "sent": "At the next iteration, so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we just have an extra loop, not an extra loop.",
                    "label": 0
                },
                {
                    "sent": "We expand our loop so instead of just looping with these two things were now looping through the whole thing, but the number of loops we have to do actually is logarithmic because every time we're shrinking the parameter by a constant.",
                    "label": 0
                },
                {
                    "sent": "So if you guessed it wrong, it is really logarithmic.",
                    "label": 0
                },
                {
                    "sent": "Moreover, you can do some kind of more adaptive clever strategies, in which case the number of such loops is going to be at most twice as much as the number of iterates, so it's really you not increasing the number with this very significantly.",
                    "label": 0
                },
                {
                    "sent": "Here, and you're allowing yourself to make much bigger steps.",
                    "label": 0
                },
                {
                    "sent": "And actually, it's funny because in particular case when you for example if your F of X is quadratic like in law saw group lasso, then this actually doesn't cost you anything.",
                    "label": 0
                },
                {
                    "sent": "Recomputing this in this because you're changing is only this parameter and your gradient is linear in terms of your.",
                    "label": 0
                },
                {
                    "sent": "Basically in terms of your X in terms of you iterate.",
                    "label": 0
                },
                {
                    "sent": "So re combining things with the different linear combinations does not cost you any extra gradient computation.",
                    "label": 0
                },
                {
                    "sent": "I don't have time to.",
                    "label": 0
                },
                {
                    "sent": "Describe this, but it's really.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's not not a big deal quite often.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is another.",
                    "label": 0
                },
                {
                    "sent": "Key thing.",
                    "label": 0
                },
                {
                    "sent": "So now what we can do is we can do the analysis so we have this different term here.",
                    "label": 0
                },
                {
                    "sent": "And basically we can bound this term in general with this.",
                    "label": 0
                },
                {
                    "sent": "Now what is this?",
                    "label": 0
                },
                {
                    "sent": "This is an average of the step sizes.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit more complicated than average.",
                    "label": 0
                },
                {
                    "sent": "It's an average of.",
                    "label": 0
                },
                {
                    "sent": "It's an average of square roots, whatever, but basically it's the average of the step sizes you've taken along the way.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we have a.",
                    "label": 0
                },
                {
                    "sent": "Basically we have the following convergence rate here.",
                    "label": 0
                },
                {
                    "sent": "So basically the number of iterations it takes is a square root of some number times epsilon, and that number is actually it's kind of a posteriori analysis.",
                    "label": 1
                },
                {
                    "sent": "It's an average of the step sizes you've taken, but The thing is, we just there was just discussion about this.",
                    "label": 0
                },
                {
                    "sent": "If you function is non smooth at current step in some directions it might be very smooth in some other direction in which you obtain a very good dissent.",
                    "label": 0
                },
                {
                    "sent": "So if you can analyze which you know, I don't know how to do that, but I think in certain cases you can analyze that this step is going to be much better than basically the smoothing you know then one over epsilon or 1 / L, right?",
                    "label": 0
                },
                {
                    "sent": "So if you can analyze this, you're getting basically a potential for much better convergence rates for nonsmooth functions, OK or not, just non smooth functions, but general functions with.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bad Lipschitz constants.",
                    "label": 0
                },
                {
                    "sent": "OK, there is some computational results.",
                    "label": 1
                },
                {
                    "sent": "I want to get a little bit to the next topic, just very quickly.",
                    "label": 0
                },
                {
                    "sent": "The computational results show that on some standard last source type problems we do get gain in number of multiplications, clearly iterations, but mostly we are comparing number of multiplications.",
                    "label": 0
                },
                {
                    "sent": "Most cases we do get again from doing this strategy OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Don't have, I mean mostly it's interesting from theoretical point of view I think OK. Now I have 3 minutes and I just wanted to talk about.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because the void was talking bout alternating directions, there's one particular case where these alternating directions can be treated exactly the same way as Eastern Vista, and that's when you have two functions.",
                    "label": 0
                },
                {
                    "sent": "So when you have just two functions, so I have no time, but hopefully those of you have been to Steve, I'd like to remember this if I have two functions that I'm optimizing, I now can basically split them with two variables right?",
                    "label": 0
                },
                {
                    "sent": "This condition and Adam mounted Lagrangian term here.",
                    "label": 0
                },
                {
                    "sent": "And the classical.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alternating direction methods then does this minimize this function now?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Notice I called the same function again Q. OK, it's not a quadratic function generally, but again, call it Q.",
                    "label": 0
                },
                {
                    "sent": "It's an invented log.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function here and I minimize this Q in one variable and minimize it in another variable on another set of variables and then I am updating my LaGrange multiplier.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we now switch.",
                    "label": 0
                },
                {
                    "sent": "Here what we do.",
                    "label": 0
                },
                {
                    "sent": "Minimizing one variable minimizing another update LaGrange multiplier.",
                    "label": 0
                },
                {
                    "sent": "But then why do we update model against multiple after this variable?",
                    "label": 0
                },
                {
                    "sent": "Why don't we updated?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this variable.",
                    "label": 0
                },
                {
                    "sent": "So if we do this, if we updated every time what we're going?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To get is basically kind of double Fister if you want or double Easter in this particular case, so you treat first F of X as you did in Pfister.",
                    "label": 0
                },
                {
                    "sent": "You linearize it approximated and do minimization, and then you treat the other one the same way, so you basically alternating between two easterfest steps.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it really actually is very powerful and I don't have time to do this, but you do get accelerated version of that and you just gain more.",
                    "label": 0
                },
                {
                    "sent": "By doing this because very often for the applications that I've talked about, computing one gradient is the same as minimizing over.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, it's the same as doing this extra step and you kind of get the gradient for free there so.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, I don't have.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks for this and you basically have some kind of similar convergence rates and again the same things with the backtracking and everything else applies there.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And actually just one thing is things.",
                    "label": 0
                },
                {
                    "sent": "So the step sizes average out even more, so the steps that you can make different step sizes for one function on the other four.",
                    "label": 0
                },
                {
                    "sent": "So each of your alternating steps, that role that Steve was talking about can be different for every function, and then the road that goes into convergence theory is kind of the average on them.",
                    "label": 0
                },
                {
                    "sent": "You in my case it's the average.",
                    "label": 0
                },
                {
                    "sent": "So basically if one function behaves really well and the other really badly together they'll behave sort of better than the bad ones.",
                    "label": 0
                },
                {
                    "sent": "So the bad one don't hurt you as long as you have one good one.",
                    "label": 0
                },
                {
                    "sent": "In a way.",
                    "label": 0
                },
                {
                    "sent": "OK alright sorry, I've tried to cram a lot into this talk.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The only thing is here you have a problem.",
                    "label": 0
                },
                {
                    "sent": "So without the fast method with, when you don't do the fast method, it's fine.",
                    "label": 0
                },
                {
                    "sent": "When you do the fast method then the gradient you have to.",
                    "label": 0
                },
                {
                    "sent": "I mean you have to compute it because this Z is not this or this, so you don't actually get the update.",
                    "label": 0
                },
                {
                    "sent": "Maybe one can figure it out how to do it.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to do it, but it doesn't.",
                    "label": 0
                },
                {
                    "sent": "So here you would need.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The gradient, but here yes, I write the gradient for the standard kind of Vista, but it actually comes to you for free.",
                    "label": 0
                },
                {
                    "sent": "You just get it and then you can do exact methods this way and compute the gradient, but I just didn't understand right now you don't need, you don't need to.",
                    "label": 0
                },
                {
                    "sent": "I mean, you actually know the important thing is why?",
                    "label": 0
                },
                {
                    "sent": "So if your function is not smooth you do get subgradient here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you do an.",
                    "label": 0
                },
                {
                    "sent": "I mean whether it's a useful one, I don't know, but it just works.",
                    "label": 0
                },
                {
                    "sent": "I mean most of the time, it just works.",
                    "label": 0
                }
            ]
        }
    }
}