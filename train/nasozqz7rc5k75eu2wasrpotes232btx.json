{
    "id": "nasozqz7rc5k75eu2wasrpotes232btx",
    "title": "Dimensionality Reduction",
    "info": {
        "author": [
            "Neil D. Lawrence, Department of Computer Science, University of Sheffield"
        ],
        "published": "Jan. 25, 2013",
        "recorded": "April 2012",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss2012_lawrence_dimensionality_reduction/",
    "segmentation": [
        [
            "I'm talking about dimensionality reduction.",
            "Which is something that interested me, I guess when I started I mean, oh, I should say it's very true what Bernard says about meeting people places.",
            "I think I met Bernard or saw Bernard for the first time in 1997 at the machine learning and generalization school.",
            "So we've known each other I guess for quite a long time and the same thing.",
            "Kind of course happened at these summer schools.",
            "You know, you can build relationships with people you meet 15 years later, organize your own summer schools.",
            "And have people attend.",
            "OK, so I'm going to talk about dimensionality reduction, which is something I would became interested in as a PhD student.",
            "I didn't do my thesis on it, but it struck me as something that was interesting and important, and.",
            "Shortly after that amount of work happened in spectral methods in machine learning, which I'm going to try and view.",
            "Part of what I say is is tutorial, but part of it is polemic.",
            "I'm trying to push a certain point of view.",
            "You're entitled to disagree with that point of view.",
            "Not everything I say is has to be considered right.",
            "I think it's right.",
            "That's why I say it, but it's also to encourage thought in yourselves.",
            "Other people take different perspectives from the one I present."
        ],
        [
            "What I wanted to first of all do is start.",
            "With something I think I struggled with, which is actually what it means to think in high dimensions.",
            "When people tell you these things about how high dimensional spaces are, what that means in practice and why, perhaps your intuitions aren't quite right.",
            "And then I want in this section to talk about linear dimensionality reduction and probabilistic linear dimensionality reduction.",
            "I'll also move onto sort of non probabilistic perspective in terms of distant matching given time in this session.",
            "Otherwise I'll push that into the next session.",
            "I haven't calibrated the speed of this, so depending on how fast I'll go, different material will appear."
        ],
        [
            "Open places.",
            "So, so just a bit of notation, so I'm following the statisticians in when I talk about data it will be of dimensionality little P, which I think is confusing because it's a probability distribution, but that's what statisticians.",
            "So if ever I kick that that's a thing goes off, that's great.",
            "That's causality inference there, by the way.",
            "That's the second time I did that.",
            "But that's advanced stuff.",
            "We're not talking about that here.",
            "I.",
            "Latent dimensionality I'm going to use Q.",
            "An will be the number of data points and my data matrix will always be Y.",
            "So whenever you see why that's a design matrix containing data, so it's got N Rosen, P columns.",
            "So that's the definition of a design matrix.",
            "Again, it's statistics inspired, and I'm going to use the reduced dimensional representation, which I call the latent variables, but it's the low dimensional representation will be X.",
            "These guys will come in later there, more for future slides.",
            "So just a little bit of notation.",
            "And then I'm going to use this Matlab style notation to try and differentiate between columns from a Ann Rosa Vaso AI: is a row of a, but as a column vector and this is a column of a as a column vector and then the element of A is AIJ."
        ],
        [
            "OK, so the source code and stuff is online, so why high dimensional data?"
        ],
        [
            "It really dominates in machine learning, partially for the some of the reasons Bernard was talking about earlier.",
            "I mean not just the number of data points.",
            "As you increase that, it becomes hard for a human to look at a large number of data points.",
            "It also becomes hard to look at high dimensionality, so examples of high dimensional data could be a customer in a database and the features might include their purchase history where they live, their sex, their age, their film preference, what tests have been done on them from a medical perspective.",
            "If you start think trying to think of characterizing an individual in numbers, you're going to have a very large set of numbers.",
            "So digitized photograph or video gotta pixel intensities, time, date, location of the photograph.",
            "One thing I like to play with is motion capture data.",
            "I like playing with motion capture data 'cause it's very high dimensional.",
            "But if you have a good model of it, humans can recognize it.",
            "So it's something you're quite trained to perceive.",
            "If you see a model of a stick man moving, you know whether the motions natural or not.",
            "Now that's 100 and something dimensional.",
            "Time series very often, but you can look at it and say.",
            "Yeah, that's correct, so you're capable of dealing with high dimensional data when it's presented in the correct form.",
            "That's something that shows that so motion capture data is something that I guess I started working with, and I've created the MATLAB mocap toolbox.",
            "So if you wanted to use it yourself, you can just move it into Matlab and Jeff Hinton student did that as well.",
            "So he demonstrates with mocap data 'cause it shows you when you've got a good model of high dimensional data.",
            "It's very clear to a human when the humans moving naturally.",
            "So human speech when I started in machine learning 15 years ago or more, there was a big overlap between machine learning and speech, so nowadays that seems to be smaller, but I think it's coming back.",
            "In fact, people are doing sort of deep learning in speech recognition.",
            "It's a very interesting application domain and there's a lot of opportunity, I think.",
            "Well, it's difficult because they do it very well, but I think it's an interesting set of data, but it's also high dimensional an it's a time series, so web pages.",
            "Or other documents.",
            "So features could consist of frequencies of given words.",
            "That's bag of words, models, other features could be how those documents are linked.",
            "So I have colleagues who are in the language group in Sheffield that are analyzing Twitter feeds, and so each tweet.",
            "It, even though it's only 5 or 6 words, it's potentially from a language of maybe 16,000 words and it's associated with information about where the tweet was given.",
            "Who's following that Twitter?",
            "Is that the right word?",
            "Twitter these young people in the audience, you should know.",
            "And all this additional information, so that's high dimensional data as well.",
            "Another area I work is computational biology gene expression, so you can characterize the state of a cell by effectively the concentration and location of all the chemical compounds that are in that cell.",
            "If you think of the size of that state space you have, I don't know.",
            "My memory is something like 30,000 genes in your body.",
            "Each of those genes has products which may be in the form of proteins.",
            "Many, many more proteins than there are genes.",
            "So if we think of the state of an individual one of yourselves.",
            "You need, you know millions of variables to characterize its state at any given time, plus position velocity of each thing very very very high dimensional."
        ],
        [
            "OK.",
            "So how do we deal with complex data?",
            "So I've tried to simulate some complex data here and this is very much starting from what goes on in a low dimensional because typically people don't show you images of high dimensional data 'cause you can't visualize it.",
            "So when I first started doing machine learning, if you looked at data like this it looks complicated, but people had techniques for dealing with this.",
            "They had mixtures of Gaussians MoD."
        ],
        [
            "And here with a circle located in the center, Anna Little ellipsoid denoting.",
            "The sort of covariance of the model you've got a mixture of Gaussians fit to that data, which you must think that's really good, because that looks quite complex in terms of data and density.",
            "Modeling is something we want to do.",
            "Understand the density of the data, and I tell you that fit is actually perfect because it's the model I generated the data from, so this isn't even a fit from the data.",
            "This is the true model.",
            "So once you've seen that, you sort of think complex structure isn't a problem.",
            "For mixtures of Gaussians, yeah.",
            "I mean, it can model these things quite well.",
            "People do a lot of effort into selecting the number of components into dealing with the fact that there's multiple local minima, but fundamentally surely that's the problem solved.",
            "Well, that's not the case, and it's not the case.",
            "I'd argue because of the way you look at data in two dimensions is somehow sort of wrong and a Gaussian.",
            "Maybe a nice model in two dimensions, but a straightforward Gaussian.",
            "A Gaussian where you're not thinking about what you're doing.",
            "So if you just apply a Gaussian and fit its covariance in high dimensions, it's going to be quite a bad model.",
            "Very often.",
            "I still argue that Gaussians can be very useful, and you can do a range of things with Gaussians, but you have to do things like structure the covariance matrix in order to fit them.",
            "So why is it you know this is the sort of thing that I came into machine learning from a fairly naive perspective in those days, machine learning was about how the brain worked, and neural networks with how the brain works.",
            "And then these bunch of people came along with support vector machines and messed it all up.",
            "Finally with deep learning they're going back to doing what we were doing 15 years ago.",
            "Back to these sort of drawing.",
            "These circles and putting lines between them.",
            "I came in from a mechanical engineering background.",
            "With this naive you that neural networks is where it's at and then when I saw these statistical models of mixes Gaussians, I thought well surely they are very powerful too.",
            "But I think the reason that they aren't as widely used as they should be is because in high dimensional data.",
            "They're typically don't work very well."
        ],
        [
            "So.",
            "These two dimensional plots of Gaussians can be misleading and our low dimensional intuitions can fail dramatically, and there's a couple of major issues.",
            "In high dimensions, in a Gaussian density, all that data actually moves to a shell.",
            "Now I'm going to try and show you visualize what I mean by moving to a shell.",
            "You may have heard this before.",
            "There is no data near the mean of the Gaussian, so."
        ],
        [
            "One way of thinking about a clustering model like this is what you're saying about your data, and I like this way of thinking about it is that each data point is really a prototype.",
            "That's the mean of the Gaussian that has then been corrupted in some way according to the covariance of the Gaussian.",
            "So you're saying you represent your data by prototypes that are the mean plus corruption, which is the covariance.",
            "How they move away from the mean."
        ],
        [
            "But what this high dimensional data stuff is saying that in high dimensions?",
            "The prototype is never going to be observed because you'll always be corrupted in a shell some distance away from the prototype, which is kind of an odd thing.",
            "So how does that go on?",
            "So I sort of used to use to hear this, and I used to think, OK, yeah, fine people would prove things about it, but I never had the intuition."
        ],
        [
            "So this is an exercise in trying to understand the intuition and it's based around a Gaussian egg.",
            "OK, there's another exercise.",
            "These are curse of dimensionality exercises in the old Bishop book and what I want you to see in this egg.",
            "This is a 1 dimensional egg and then I'm showing a density across this egg.",
            "The yellow bit in the middle is the yolk.",
            "And then the white bit on the outside is the sort of album, and then I put a green bit.",
            "Right at the interface between the yolk and the and the white of the egg so that green bit is if you over boil your egg like some places.",
            "Do you see on the outside of it there's a green covering its parity iron sulfide and it's my egg smell bad because they've got sulfur in them, so that green bit there is.",
            "This is an over boiled egg, but it's got a relatively thin layer that makes an egg taste bad actually, so you shouldn't over boil your eggs, but here it's been over boiled and the way we've set it up is that.",
            "The the iron sulfide is between .95 and 1.05 standard deviations from the mean.",
            "Yeah, so between 95% and 105% of a standard deviation from the mean.",
            "So the yolks up to .95 of the standard deviation and the white goes from 1.05 to Infinity.",
            "This is a big egg.",
            "I mean, well, a real egg would be a square density, right?",
            "And so here we are sort of fitting a Gaussian density across it.",
            "So that's how 1 dimensional leg.",
            "So if we look at the 1 dimensional leg, we find it's really nice 'cause the yolk is my favorite bit of the egg.",
            "That's nice and runny and tasty, and 65.8% of your egg is the tasty bit.",
            "OK, the white is sort of more healthy.",
            "It's got protein in it.",
            "29% of your egg is so nearly close to 30% of your egg is protein and 5% of the egg is this nasty iron sulfide bit that you don't like."
        ],
        [
            "OK, So what happens in two dimensions?",
            "You can compute these volumes again.",
            "The volume of the obviously probability distribution, so it's the total volume 100% and in two dimensions you get 60% of your eggs, so you've lost 5% of your yolk, yeah?",
            "And seven point 4% of your egg is now the iron sulfide and 33% a third of the egg is now in the white.",
            "OK, the reason?"
        ],
        [
            "Obviously, because this high density here in lower density here so."
        ],
        [
            "So you're sort of seeing this is now the view above of the egg, and this is a Gaussian egg, so this is higher peak.",
            "There's a sort of mountain of egg there, so there's more density in the middle and there is on the outside."
        ],
        [
            "So OK, but now if we go to three dimensions, the density on the outside goes up a bit.",
            "This 10% of the density is now in this iron sulfide 56%, so we've lost some of the yolk, but how does it behave as we go higher and higher dimension?",
            "Well, the nice thing so."
        ],
        [
            "I've seen a sort of order that seems to be."
        ],
        [
            "Bring up white goes."
        ],
        [
            "Up, yeah, yolk goes down.",
            "An iron sulfide goes up, so does that continue to be the case?"
        ],
        [
            "Well, what it turns out is we can actually compute the density of where the probability mass is so.",
            "If we're thinking of this egg in high dimensions, all I'm doing is I'm thinking of a Gaussian distribution where I'm sampling independently across axis with a fixed variance.",
            "So if the variance of the Gaussian is Sigma squared, what it turns out is the distance from the mean, which is obviously from Pythagoras Theorem the square root of Y I-1 plus Y I2 squared.",
            "So here's sample one here, sample two.",
            "That's where we end up.",
            "That's our distance from the mean.",
            "Yeah, so the distance is given by the square root.",
            "Now I can't do much with the square root.",
            "Well, you can, but it's actually easier to work with the square of the distance, and that's going to be an ongoing theme in everything I do.",
            "So it turns out that the square of this distance is chi squared distributed.",
            "Yeah, so that's a statistical known thing, used quite a lot.",
            "So each of these samples their length is chi squared distributed.",
            "So if you want the square of the distance of the whole thing, it's just the square of that.",
            "Then it's the sum.",
            "Have some chi squared distributed variables, so that's rather nice."
        ],
        [
            "Because it's analytics, so I like to think of the chi squared distribution as a gamma, because that way I don't have to remember lots of different distributions, but it's a gamma distribution in the way I parameterise a gamma, which is this form here.",
            "This is this density that's the normalizer, and the important part RX to the a -- 1 E to the minus BX.",
            "That's the form of the distribution in X.",
            "In this form, the mean of this distribution is a / B.",
            "Sometimes people set it so that the bees underneath X in the main is.",
            "A time is being, but this is the way I like."
        ],
        [
            "Do it writing of the chi squared is showing that it's scaled by Sigma squared, yeah?"
        ],
        [
            "So this is actually a scale parameter and so the Sigma squared appears underneath here.",
            "Yeah, so you've got a gamma distribution.",
            "The square of this value here is distributed as a gamma with a parameter.",
            "The shape parameter set to half, and what I think it was the rate parameter set to 1 / 2 Sigma squared.",
            "So that."
        ],
        [
            "Nice thing about that is what you're interested in is the distance from the actual origin of.",
            "A sample after you increase the dimensions, yeah, and what it turns out is the gamma rather nicely.",
            "If we want to add 2 gamma variables here and ask what's inside this square root here, that's also gamma distributed.",
            "As long as the scales of these two gammas are the same which they are.",
            "If we're assuming the same variance, it's gamma distributed with the shape parameter, which is the sum of the shape parameters of these guys.",
            "So in this case it's one and a scale rate parameter, which is the same as before.",
            "That's if these guys are independent, which I'm assuming they are, so that's important.",
            "We're assuming with sampling each of our features of our data in this Gaussian egg is being independently sampled from a Gaussian distribution which is spherical.",
            "Yeah, so we can compute this cumulative density function, and indeed we can do it.",
            "For arm P dimensional data.",
            "So P dimensional data the sum over these iy squared.",
            "These YYIK squared.",
            "So this is the I TH data point in each dimension of it is distributed as a gamma with the shape parameter P / 2.",
            "Now that means that the expected value under this distribution is P times Sigma squared."
        ],
        [
            "So what we'll typically do is we'll ask about 1 / P times the expected value so that we keep the expected value of the squared distance.",
            "This is in P dimensions.",
            "Now from the origin.",
            "To be Sigma squared, so that means that in the case of a standard normal, it's going to be one.",
            "Yeah, so we'll be working with standard normals.",
            "So the scaling of that scales the rate parameter and we've got now a density formula that says what will the distance, the squared distance.",
            "How is the squared distance from the mean distributed?",
            "Now that's nice because in our Gaussian egg model we can now ask the question, how does that square distance from the mean vary with the number of dimensions you see?",
            "This density is a function of P, the mean distance from the.",
            "So the mean distance from the origin?",
            "Which is the mean in this case?"
        ],
        [
            "That's the entire volume of the egg and you started out with the yolk, which was 65% of the egg and a white, which was I think 28% now initially the volume of the white portion goes up, but then actually it starts to drop down.",
            "And then you see the yolk drops down as well, until when you get to 1000 dimensions with what I've defined to be the alluminium, you basically have only.",
            "See points which are that distance from the mean.",
            "All your data sits in that shell.",
            "Does that make sense?",
            "So that's very counter intuitive verse."
        ],
        [
            "What you think?",
            "If you look at a Gaussian because you can see this Gaussian, the data is close to the mean, right?",
            "You know the Gaussian the date is close to the mean.",
            "It's very obvious.",
            "Right now there's a fun fact here that every marginal of a Gaussian is Gaussian.",
            "So this is a 2 dimensional joint Gaussian with variance of 1.",
            "OK.",
            "But actually, no, it's not.",
            "It's a 3 dimensional Gaussian.",
            "I'm just showing you the marginal of two points 210 to two variables in that Gaussian.",
            "Yeah, so it's a 3 dimensional Gaussian.",
            "That means that when you look in these two dimensions, we're projecting the third dimension down onto the 1st two.",
            "Yeah, so there's an amount of data that appears close to the mean here, but really in the third dimension, it's sitting away from the mean, 'cause it's a 3 dimensional Gaussian.",
            "Oh no, it's a four dimensional Gaussian.",
            "So there are two dimensions, right?",
            "Being projected onto the mean so the bit you see in the mean is really two other dimensions where the data is away from the mean and it's being projected down to the mean.",
            "No, no, no.",
            "It was a 5 dimensional Gaussian.",
            "OK, I can go on like that for awhile and we can go up to 1000 dimensions.",
            "Let's say it's 1000 dimensions.",
            "This is a marginal distribution from 1000 dimensional Gaussian 998 dimensions.",
            "You're not visualizing, you're just seeing projected down to the mean, yeah?",
            "So that means that there's no data near the mean.",
            "If we go to, we could even a Gaussian process is an infinite dimensional Gaussian, so you could even talk about an infinite dimensional Gaussian and look at these two parts of it.",
            "Yeah, so then there's definitely no day today.",
            "That mean because we've projected infinite, denser dimensions down onto the mean.",
            "So when you look at a Gaussian, the way I used to look at a Gaussian isar look, there is data.",
            "This mean whatever people say about what's happening in high dimensional spaces, I can see for myself that this data near the mean.",
            "But remember you're looking at potentially a projection of a very, very high dimensional that Gaussian down onto those two dimensions, which means that all this data in that high dimensional Gaussian must be nowhere near the mean, yeah?",
            "So in fact this approximation is so good that in many areas they will use a Gaussian distribution, taking it to high dimensions as an approximation for a sphere.",
            "Yeah, so in very high dimensions.",
            "If you want to have uniform data distributed over a sphere, you can approximate that with the Gaussian, and that's a sort of known physics trick.",
            "Works very well as dimensions go in the thousands.",
            "So Gaussians are bad distributions apparently."
        ],
        [
            "Haha so.",
            "The other thing that happens is interpoint distances.",
            "So in one slide this is sort of trying to show that the same thing happens with the distance between data points.",
            "If they're sampled from this Gaussian density for very similar reasons, it comes out very quickly if you think about it.",
            "So if you've got two data points, both sample from the same Gaussian, then subtracting one off the other leads to another Gaussian density.",
            "And squaring it then again leads to this gamma density.",
            "So that's just the only new step is subtracting one from the other, and Sigma squared becomes two Sigma squared.",
            "Everything else stays the same and you can see that these data points are gamma distributed.",
            "The interpoint distances gamma distributed with the parameters of the gamma being P / 2 and P / 4 Sigma squared rather than two Sigma squared.",
            "As we saw before.",
            "So I like this representation 'cause it's very easy and very common to compute.",
            "Interdata point distances.",
            "Yeah to do K nearest neighbors or whatever else, and in fact I'm going to base a lot of material on computing distances like this.",
            "So.",
            "The dimension normalized between points is drawn from a gamma an the mean is two Sigma squared.",
            "Now the interesting thing about this, the variance of this distribution is eight Sigma squared over P. So as P goes high, this variance goes to 0.",
            "Yeah.",
            "Certainly relative.",
            "If you look at the mean divided by the square root of the variance you got this scaling of P that as you go to very high dimensions what you'll see is that all the interpoint distances of your data set are.",
            "The same distance apart.",
            "Yeah, in this Gaussian egg, so any sample you take will be equidistant from every other sample.",
            "Which is annoying characteristic if you're trying to do things like K nearest neighbors, because as you change K, you'll find that no well if you look at the volume of neighbors you're considering as you increase the nearness of the neighbor, you suddenly go from no neighbors to all your points and neighbors.",
            "This basically says that for this density every data point is a neighbor of each other in high dimensions."
        ],
        [
            "OK, so now that seems to be a bad thing about Gaussian, so you shouldn't use Gaussians.",
            "You might think, but actually for any data where you've got independence over features, the central Limit Theorem applies to these sums we're taking, and the same results come out.",
            "They just don't come out quite so nicely and analytically in the low sample area.",
            "You get exactly the same thing going on the variance about the mean scales as P 1, /, P, so any.",
            "Distribution you sample from independently will lead to these different effects.",
            "If you want to know how it differs, well the Gaussian in this sphere.",
            "It's like if you are one of the things one of the reasons I believe we're here, it's because there's a large telescope here, and Bernard likes large telescopes.",
            "So, but it's useful for analogies as well.",
            "If you're standing on the earth looking out at this hyper sphere.",
            "OK, so it's high dimensional space of these Gaussian samples.",
            "How will it look?",
            "Well, all the samples will be uniformly distributed across space, abit like the stars.",
            "They're not quite uniformly distributed.",
            "Yeah, but if you're standing at the center of this hypersphere, so it's a 3 dimensional sphere.",
            "But it's a hypersphere in general looking out at these data points, they look like stars distributed over space will be beautiful.",
            "If you use non Gaussian densities, sub Gaussian or super Gaussian in the hypersphere, the points will cluster at either alongside.",
            "The axes are at 45 degrees to the axis, depending on whether you've used super or sub Gaussians, so they just won't be uniformly distributed over this hyper sphere anymore.",
            "It won't be quite as pretty, but the same problems will exist."
        ],
        [
            "So.",
            "These distributions behave very counterintuitively in high dimensions.",
            "We can compute these densities of squared distances analytically.",
            "For the Gaussian case and for non Gaussian independent systems we can invoke the central Limit theorem.",
            "So let's see how this applies for some real data.",
            "Let's what we're going to do now is.",
            "We see that this is problem and we can compute the theoretical interpoint distance between two data and it's valid because of the central limit theorem for any data set.",
            "If the features are independent.",
            "So here's some."
        ],
        [
            "Example datasets, well, let's just test our intuition.",
            "So this is samples from 1000 dimensional Gaussian and what I'm showing is the distribution of squared distances between each data point.",
            "Yeah, and what I'm showing you then is a histogram and then bang directly over it is the fit of that gamma distribution we derive before look how nicely the theory fits to data so.",
            "Um, that's good.",
            "We can sample from a Gaussian, create a data, set thousand samples, so there's a thousand data points.",
            "So around a million interpoint distances in this histogram."
        ],
        [
            "OK, now I've used fewer samples here in the fit.",
            "Still good.",
            "So this is 10,000 interpoint distances.",
            "The fit still excellent.",
            "OK, so let's take that and let's apply it to real."
        ],
        [
            "Data so that we can get our NIPS paper so we have to have a little artificial example, then a real data example.",
            "OK, the real data here is a little bit artificial as well, but this is a famous."
        ],
        [
            "Dataset, which is a so called oil data.",
            "It's oil flowing in a pipeline.",
            "And it's from Bishop and James.",
            "This 93 paper.",
            "It's widely used as a benchmark in dimensionality reduction, and it's oil flowing from a well.",
            "So it's got oil, gas and water in it.",
            "They according to the speed.",
            "I guess Reynolds number and stuff like that they'll mix in different ways.",
            "In turbulent flow.",
            "You basically get this homogeneous everything mixed together, but then there's two types of lamina flow that occur.",
            "One is gas, oil, water and then the other one I think is gas, oil, water.",
            "I hope I've got the right way around.",
            "It might be gas or not.",
            "Must be gas or who knows.",
            "Anyway, it's like that, sort of.",
            "Now you've got 12 measurements, which are gamma Ray dense geometry measurements, so these are measuring the density across here.",
            "And this is a classic learning problem because although clearly if you see the density on each of these, you should be able to determine which domain you're in.",
            "The physics of The thing is a little bit complicated, so in fact, turns out this data simulated if you read carefully the original paper and it's simulated by understanding the physics of these systems and what these things would read so little bit of a warning there it's simulated data, but it's from a realistic physical system.",
            "So what you can see here is you've got, I think, 12 sensors in total, measuring the density in the cross section of these pipes."
        ],
        [
            "So that's 12 readings.",
            "So if we now compare the truth to what we theoretically expect to happen, we see that this is the theoretical curve for the Inter data point distances, and this is the actual histograms.",
            "OK, so that's not too bad.",
            "There's a little bit of a mismatch.",
            "P is 12.",
            "The predicted variance of these interpoint distances is .66, and the actual variance we've observed is closer to two, so it's quite a lot larger than we would expect.",
            "The variance of interpoint distances.",
            "So anyway, OK, let's not worry."
        ],
        [
            "Too much about that.",
            "Let's look at another data set I told you I like motion capture data.",
            "This is motion capture data where the data is only 55 frames and there's 102 data points 'cause there's 34 points on the body, so they put balls on the body at 34 places, and there's an XY zed location for each of these balls.",
            "Yeah, so that's 3 * 34 One 2 dimensional data, 55 data points, and it's from Ohio State University.",
            "I use this data, little data set a lot.",
            "It's a nice small data set and one thing I like about it is the guy changes angle as he runs, so it's two things going on.",
            "This one is it's periodic 'cause he runs.",
            "It takes about 3 paces and the other one is some sort of change in his angle, which is sort of overlaid on top of the periodicity."
        ],
        [
            "So let's do the distances for this data and the theoretical curve.",
            "OK, P is now much larger, so the expected variance around the mean distance, which is 2.",
            "I've normalized all this data, so each dimension is variance one, so the expected squared distances 2.",
            "But what we observe here is that the variance is 1 versus the predicted .07."
        ],
        [
            "Mike radiator, so this is 6000 genes.",
            "And 77 experiments.",
            "It's a classic early microarray cell cycle data set.",
            "Its measurements in yeast of.",
            "These 6000 genes across different time series with different things being done to the yeast.",
            "So the 77 experiments are done each in different time series.",
            "And 6000 genes."
        ],
        [
            "And now if we look at the theory, the effect is even worse.",
            "Look at this guy up here.",
            "He's predicted variance is less than one percent.",
            ".0129, but the actual variance of the sum of the squared distances is .694."
        ],
        [
            "So that lesson goes."
        ],
        [
            "John, here's a valid data set.",
            "We see the same thing less pronounced than the gene expression, so very high dimensional data."
        ],
        [
            "Basically, not reflecting what we saw in the theory.",
            "The situation for real data does not reflect what we expect, So what?",
            "What's going on?",
            "Why does real data not conform to maths?",
            "It's very naughty.",
            "OK, something's gone wrong in our assumptions about what's going on in the mass.",
            "So let's look at P being 1000, again thousand dimension."
        ],
        [
            "Dataset artificial.",
            "But we get a similar effect.",
            "This is a Gaussian with P being 1000.",
            "But I've given this Gaussian a very specific low rank covariance where W. Is 1000 by 2.",
            "And then I've corrupted it by a little bit of noise.",
            "Yeah, OK, sorry, that's a bit low.",
            "So the form here is the covariance of this.",
            "Gaussian is WW transpose plus Sigma squared I where is a matrix of 1000 long and two columns.",
            "So it's basically this is rank 2, the covariance plus some diagonal terms to make it full rank.",
            "Now I've taken a noise to be .01 and sampled this matrix W from a standard normal."
        ],
        [
            "Now, Interestingly, if I now apply the theoretical curve assuming a dimensionality of two for the data, the fit is bang on.",
            "What's going on?",
            "All our assumptions in that material I gave you at the beginning was that the features are independent.",
            "If you have 1000 independent features about something, let's consider the case where they are binary.",
            "1000 independent binary facts about someone that covers 2 to the thousand people.",
            "The space that it's covering is absolutely enormous.",
            "If you have that degree of specification on the data, something really weird is happening.",
            "In practice, real features are massively correlated.",
            "Yeah, so all this cursor dimensionality stuff is actually rubbish.",
            "It's rubbish from the fact of when you're thinking about the data.",
            "If it was true in your data, there would be no information in your data.",
            "It's what we call noise.",
            "The only thing we assume full independence for across all data points and across all features is noise.",
            "What we do in practice is we assume underlying regularity's in our data, particularly, I mean for features and everything else.",
            "If we didn't, we would not be able to learn from the data.",
            "The worst case predictions from learning theory would apply, but in practice they don't because real data is correlated like this.",
            "This is a simple correlation and we're going to talk about this correlation first.",
            "How you fit models that assume this correlation?",
            "There's not an invalid lesson, though it doesn't mean that you don't have to worry about the curse dimensionality because it's actually associated with your model, not your data.",
            "Yeah.",
            "We would sampling from a model that had this independence characteristic.",
            "If you fit such a model that has this independence characteristic, your model is covering a vast space of data.",
            "It's going to be difficult to extract information from the data, so there's something wrong with models like that, but I think it's endemic in machine learning and statistics.",
            "To claim this is a characteristic of the space, it's a characteristic of the model plus the space.",
            "Yeah, it's the worst case thing that could happen in the space.",
            "But then if someone brought you that data which has that nasty characteristic.",
            "You really want to be asking them what rubbish have you just given me.",
            "There's no information in this data because there really isn't.",
            "That's what these lessons are about.",
            "If this information is this regularity in the data that you can extract, those things won't be true.",
            "OK, so.",
            "One thing I hope to do is put some of the.",
            "I mean the curse of dimensionality.",
            "Ooh, I mean, I just think it's a large sense.",
            "It's a myth.",
            "Alot of the people things people extract from it Bernard touched on it earlier saying you shouldn't expand things into high dimensions, that's what the statisticians say because it's a bad idea, right?",
            "Making your feature size Infinity?",
            "That's always going to lead to problems no no it's not.",
            "Because of the structure of the model because of the capacity control.",
            "That's the way Bernard was talking about it.",
            "Yeah, I'm saying the same thing.",
            "It's about your model.",
            "Yeah, not about your your data set size.",
            "It's a very bizarre thing.",
            "If you think about it for someone to come to you, you're going to model in someone's bought you.",
            "Here's your client.",
            "He's walked into the room and he says there's the data set.",
            "I found out 100 things about these 50 people.",
            "Please can you fit your model like you think?",
            "OK, that's difficult because there's P is large and N is smallish.",
            "But I'll try and fit a model and then the guy goes.",
            "Oh, I just heard I can tell you 100 more things about those people.",
            "Would you like to know?",
            "And then you said no don't tell me because then I'll have an even larger P. I don't want to know anymore 'cause it will mess with me.",
            "The data is worse.",
            "How can that be true?",
            "'cause it's not true?",
            "Those hundred things will not be independent of the previous hundred things.",
            "You'll get a richer model of the person, so and so and so on.",
            "OK, they'll be more stuff about that later on."
        ],
        [
            "OK, so where does this low rank covariance come from?",
            "It comes from a low dimensional approximation for the data set.",
            "That's the connection with dimensionality reduction, a simple low dimensional approximation and linear low dimensional approximation.",
            "If we made this C WW transpose plus DI made D Sigma squared I before then that would be known as factor analysis for my used is known as probabilistic PCA.",
            "OK, probabilistic principal component analysis and it's a linear mapping from a Q dimensional latent space.",
            "Because I'm sort of build probabilistic models, I think of the low dimensional thing as latent variables and observe things to a P dimensional.",
            "Dataspace.",
            "And then we corrupt that mapping.",
            "So we do a linear mapping from that and then we corrupted by independent Gaussian noise.",
            "That noise is high dimensional.",
            "That is something we can't.",
            "I mean, that's almost like it's one way of thinking about noise.",
            "It's the nasty difficult stuff that we can't extract information from.",
            "The earliest people who define noise were people like Laplace and Gauss who were looking at planetary motions through the atmosphere and in their telescopes.",
            "And they said these are things that can't account for.",
            "Let's assume they are independent random things being added to each.",
            "Observation and there are instructions that something that they can't deal with in the structure structure.",
            "Noise is very interesting, but it's not something we're assuming here.",
            "And then what we do is we marginalized these latent variables using a Gaussian prior.",
            "That's how we do probabilistic PCA.",
            "So just to remind."
        ],
        [
            "On the notation.",
            "So Q dimensional latent space, P dimensional data space and the number of data points.",
            "Why is our data in this end by P matrix that's important because it turns out if I write YY transpose, that's an inner product matrix.",
            "Center data is like our data with the mean subtracted off, so that's something we'll use a lot, and then the latent variables are X, and then we're going to use this mapping matrix Maps from AP dimension to a Q dimensional data space.",
            "So if."
        ],
        [
            "In effect, when I go from OK, so just about the notation, why am I making a fuss of this notation?",
            "Because when you see this form arising, the censored data set in this form, that's a covariance, yeah?",
            "So I sometimes write that is S so that is equal to that and then if you see this inner product matrix.",
            "This why?",
            "Why transpose?",
            "That's an inner product matrix which has this interpretation as a linear kernel sometimes, so those two features of the way I've defined the day."
        ],
        [
            "And this is how we're going to model the data set.",
            "So we find the lower dimensional plane embedded in high dimensional space.",
            "The player is described by this matrix W, which is a.",
            "In that space there.",
            "So we go from this low dimensional Space 2 dimensional in this case to a 3 dimensional space by multiplying the vector X by W, which gives us a 3 dimensional response and adding a mean to it."
        ],
        [
            "Um?",
            "If we also add noise then we have this sort of form.",
            "So for each data point we are assuming the relationship between the data and the latent variable is given by this equation.",
            "Now, the way we write that probabilistically.",
            "Is a Gaussian density.",
            "If this is Gaussian distributed.",
            "So if this is Gaussian distributed with variant Sigma squared.",
            "So this is that nasty high dimensional thing.",
            "It's independent across features.",
            "Then what we get is.",
            "The probability of Y given X has a mean well independent across data points, we assume, and it has a mean given by WXI plus mu.",
            "And then it's corrupted by some noise of covariance Sigma squared.",
            "Yeah, that's the likelihood.",
            "Now these X are things we don't know, so the parameters Now graph RW I. I know there's a big tendency to use graphical models, a lot of machine learning.",
            "I used to like it too, but now I don't know if it's something with age.",
            "I just look at them and I can't understand them because there's lots of circles and plates and things and I find it hard to see what the conditional in dependencies are.",
            "But this is an easy one.",
            "OK, so why is dependent on X&W?",
            "It also depends on you, but I didn't include it in the graph 'cause I didn't want to confuse you.",
            "So the latent variable approach is that these acts are nuisance variables and we want to get rid of them 'cause we don't know what they are.",
            "We don't know what the underlying causes are.",
            "I mean historically principle component analysis was invented by who anyone?",
            "Hotel in good good man.",
            "If someone said Pearson they would have been wrong.",
            "Pearson invented the same algorithm but for something different and these two ideas are being conflated.",
            "I want to deflate them.",
            "I want to inflate them.",
            "I want to play them.",
            "Hotel in was following the ideas in uh, in psychology at the time, which were known as factor analysis.",
            "Yeah, so her telling was trying to put on what he thought was a sound mathematical basis factor analysis and this is 1930s.",
            "I think 33 or 36 and his paper is really clear if you read it today and he thinks of principle component analysis 100% in this way as you've got some latent factors, which was Bernard saying that you had these intelligence tests, what comes next in the series?",
            "Those are the sort of things they were scoring and then they try and workout the latent causes that you're a psychopath and your IQ is 5.",
            "Yeah, so those are the X is.",
            "The underlying variables in psychology this was very important and hotel in came along and said this is very nice what you're doing, but here's a mathematical way of doing it.",
            "I don't want to call it factor analysis because factor means something else in maths, so I'm going to call it principle component analysis, But it's inspired by the social Sciences and he thinks of these types of models.",
            "So principle components is another type of factor analysis and I'll still explain it.",
            "I think it's a shame he changed the name because."
        ],
        [
            "Well, as you'll see from the mass.",
            "So we define a Gaussian prior over this latent.",
            "SpaceX and this is what.",
            "Oddly, although this is what hotel ING and?",
            "Did describe he only looked at the noiseless case, in effect now it took till 1996 when the maximum likelihood solution for the noisy case was given by tipping and Bishop and roll ice.",
            "Also proposed model around the same time 9690 seven, 98 their papers come out different time but it was all contemporaneous.",
            "So if you've got this prior distribution what you do is you use integration."
        ],
        [
            "To get rid of it.",
            "So we take the prior, we multiply it by the likelihood and we integrate over X and we get this marginal likelihood, which is the form of the Gaussian we used to sample from.",
            "So it's just a Gaussian with a mean of mu Anna covariance, which has that structure.",
            "Now remember what I said in the beginning?",
            "Be aware of Gaussians, but Gaussians with structure are good.",
            "In fact, that's all I do.",
            "Gaussians with structured covariance is different.",
            "Structures in the covariance, and this structure is a low rank plus a diagonal."
        ],
        [
            "And now you need to optimize the likelihood of this.",
            "So way we indicate that as a graph is we remove the marginalized variable and there it is.",
            "Now, why do I think it's a shame he changed the name from principle component analysis to from factor analysis to principle component analysis?",
            "Because it is a factorization of your data.",
            "It's factorization of covariance.",
            "Yeah.",
            "WW transpose is a factorization, so even though that wasn't factor in the sense the psychologist meant, the actual underlying algorithm has this factorization of the covariance in it, yeah?",
            "We'll see what factorization that is in a moment."
        ],
        [
            "I'm going to assume that you know that fitting the mean of a Gaussian is given by, well, the mean of the Gaussian is given by the sample mean, and so I'm going to do that.",
            "Let's me fitting the mean OK.",
            "So I'm centering my data.",
            "So common trick you can do it if the noise is constant, you can't do it.",
            "If the noise is varying or if the student T noise or something like that for Gaussian noise, constant variance, Gaussian noise.",
            "You can do that trick, so we'll."
        ],
        [
            "Dielectric generally, So what tipping and Bishop showed the papers.",
            "99 but the work was earlier, takes awhile to publish in."
        ],
        [
            "That's generals is that the maximum likelihood solution is given by.",
            "This form here, so if sees that covariance, then the log likelihood is given like this.",
            "So we rewrite the form of.",
            "This is a way you can rewrite the argument of the exponential in the Gaussian.",
            "You can turn it into this trace times the inverse covariance times Y, transpose Y, and the reason for doing that is that that guy there is the covariance.",
            "And it turns out that the maximum likelihood solution is given by.",
            "The eigenvalues that covariance, so let's just go through a little bit on the details for that.",
            "So how do you solve this sort of thing where you just take the gradient of the log likelihood?",
            "And you do matrix derivatives so the gradient of the log likelihood is given by this form.",
            "Where the covariance is appearing in the sample covariance is appearing in signing the covariance of the Gaussian.",
            "In a normal case, this W wouldn't be there.",
            "You would be taking the gradients with respect to see and you just recover that the covariance of the Gaussian is equal to the sample covariance.",
            "But here, because we've got this W constraining the covariance, it appears on this side.",
            "Of each of these C minus ones.",
            "Um?",
            "Oh, so I don't know how that extra slide came in.",
            "If we pre multiply that, what we're going to do is look for fixed point of that.",
            "So we set that to zero and then we can pre multiply it by two C which gives us the W is equal to this form.",
            "Bringing that on the other side and moving the minus an bringing the end by the covariance.",
            "So that is now the sample covariance.",
            "Yeah 'cause it's 1 / N. Now this you still can't solve and the trick that might tipping notice and this is important.",
            "I want you to know that this trick is in there because it's what prevents factor analysis being solved in this way.",
            "There's a fixed point solution for principle component analysis.",
            "And there's no single one set fixed point solution for factor analysis.",
            "It's an iterative algorithm factor analysis, whereas PCA is a one step.",
            "So you substitute W with it's singular value decomposition, so I've written that in ULR transpose, so as a rotation matrix, and it's going to turn out that you can't identify this rotation matrix for PCA.",
            "So that would imply if we substitute that in the covariance and This is why you can't identify R is WW, transpose is ULR transpose R which is 'cause it's a rotation matrix of square rotation matrix just becomes the identity.",
            "So you get UL squared U transpose.",
            "You can then use the matrix inversion lemma to show that this is the case.",
            "The inverse of the model covariance times W is equal to this form that's being expressed in terms of re expressing this W and the C -- 1 in terms of this eigenvalue decomposition.",
            "And once you've got that guy that allows you to substitute into the original data and show that the solution the maximum likelihood solution is given by this sample covariance times UE being equal to U times this Sigma squared plus L squared, where L ^2.",
            "Where those singular values from the singular value decomposition.",
            "Now that is recognized as an eigenvalue problem.",
            "Yeah, and it's an eigenvalue problem on the covariance of the data.",
            "So I've moved through that quite quickly, but the key point here is you can't express this guy for factor analysis.",
            "You can't do this rewrite.",
            "You can try it at home.",
            "The reason is because this instead of being plus Sigma squared I it's plus D times I, so the effect that has.",
            "See if I can do this.",
            "I forgot to plan this bit so.",
            "If I mess it up.",
            "So if you've got WW transpose plus Sigma squared I.",
            "Versus WW transpose plus D, so that's factor analysis.",
            "That's PCA.",
            "You think it was a lot more complicated than that?",
            "If you read a lot of text on it, but that's basically it, that's diagonal.",
            "And you want to fit all these things well when you do the single value decomposition you so I had it as L deny U L ^2.",
            "You transpose plus Sigma squared I.",
            "So the trick that's often used here is that this is also true.",
            "Yeah.",
            "So that works, but this doesn't.",
            "Sorry.",
            "So it's annoying, isn't it?",
            "I mean, how little simple things cause problems?",
            "You can't write this.",
            "So this is now Sigma.",
            "Yeah, that's full covariance.",
            "This guy here is full covariance.",
            "In order to write it in that way.",
            "Because you rotated in one OK another way.",
            "What does this mean physically?",
            "What are eigenvalues?",
            "Eigenvalues are simply the.",
            "Axis aligned so the Gaussian noise of the factor analysis is not axis aligned.",
            "Sorry is actually aligned and the Gaussian noise for PCA.",
            "Is spherical like that?",
            "So this is factor analysis and here you've got one Sigma.",
            "I squared Sigma one squared and you've got Sigma 2 squared.",
            "Yeah, different in different dimensions.",
            "PCA is circular.",
            "What happens if you rotate a circle?",
            "Yeah, nice isn't it?",
            "What happens if you rotate an Oval?",
            "It develops into a full covariance Gaussian.",
            "Yeah, so the fact that that problem exists.",
            "This trick of being able to rotate a circle and it still you can't change.",
            "You don't change the nature of the spherical noise if you rotate it.",
            "That's why you can do this nice simple eigenvalue decomposition on the.",
            "Principal component if you can't do it for factor analysis.",
            "OK, good.",
            "So that leads to this eigenvalue decomposition.",
            "And actually, while we're at it, since I've got the.",
            "What do I interpret this as?",
            "Well, what's going on is you've got.",
            "Your covariance of your data.",
            "What you're trying to find is the principle directions you're trying to find.",
            "You're trying to.",
            "You is the directions that will allow you to access align this data set.",
            "Yeah, so this is like you won.",
            "And this is like you 2.",
            "So that's what you're looking for.",
            "It said this, covariance the sample covariance from your data is given by this circle here.",
            "And then you do an eigen decomposition on that which gives you the principle axes.",
            "Prince Max is comes from principal component analysis of that covariance.",
            "Now the rotational invariances very interesting because the rotational invariant we had before remember it was ULR transpose, RLU transpose, right, right, right?",
            "Yeah.",
            "OK, so these things convert to WI.",
            "Should have said that's you because it's actually W 1 because it's multiplied by its eigenvalue to give it its length.",
            "You gives the direction like that and the length is given by the eigenvalue.",
            "This rotation guy.",
            "What he does is things like this.",
            "All these are valid solutions for W. Yeah, so these two I should have different colors.",
            "They shouldn't look.",
            "So let me try that.",
            "These two would be one set of solutions and these two would be another set of solutions.",
            "They rotated versions of the orthogonal set, so we tend to use the orthogonal set they're known to be orthogonal because in the eigenvalue decomposition that's a set of what I think of as orthonormal vectors, and that's a set of scales.",
            "Yeah, so these guys must be at right angles to each other.",
            "Good.",
            "So hopefully that's given some of the intuitions I have, or I think of myself about principle component analysis, but just to finish what's going on in the solution.",
            "Um?",
            "You are the eigenvectors of the covariance and Sigma squared plus L squared are the eigenvalues.",
            "Now in traditional principle component analysis you just need L squared Sigma.",
            "Squared is taken to zero, that's the noise.",
            "That is, in hotel is work.",
            "The noise was considered at 0.",
            "But in Prince probabilistic principal component alloces you add this Sigma squared onto the eigen values.",
            "Now what's that saying?",
            "Is that means the you're constantly adding to your data.",
            "A sort of your smudge in it so.",
            "If we were doing 1 dimensional principle component analysis like this.",
            "OK, so we're looking for the first principle direction.",
            "Then when you add the noise to it, what you actually do is you add a second component to it.",
            "A little bit of noise out in that direction and you add a little bit on the end as well.",
            "Yeah, so the eigenvalues you get in principle, component analysis, probabilistic principal component analysis, are slightly different from the ones you were getting.",
            "If you get PCA because you add this little bit of noise at the end, but you also it means it's a proper probabilistic setup, because if you just want to take the vector WW transpose the one dimensional vector, that's not a valid covariance, it just defines well, it's semi definite covariance.",
            "It defines a line.",
            "So adding the little bit of noise across the diagonal adds a little bit of there and a little bit there, so you end up having to subtract that off.",
            "In your solution for PCA that comes out of the maths, but that's my intuition.",
            "The mass I think you can see in the papers.",
            "I tried to give you intuition because that's what it took me longer to workout, so further manipulation shows that we can constrain this.",
            "Then the solution is given by the largest eigenvalues.",
            "What you don't see here is that the eigenvalues you need to retain.",
            "If you're reducing the dimension of, you are the largest ones, but that's you can show that as well.",
            "So the Q largest eigenvalues.",
            "So.",
            "So some further work shows the principle eigenvectors and then it turns out you can also workout.",
            "This requires quite a lot of further manipulation that the maximum likelihood value for Sigma squared is given by the average of the discarded eigenvalues.",
            "OK, so if you want to know what Sigma squared is, you have to take the average of the discarded eigen values and that's Sigma squared.",
            "So this is some intuition people have always had about PCA that somehow they look for these elbows.",
            "People talk about elbows finding dimensionality.",
            "These are an amazing thing that don't actually exist in real data, But the idea is if you do PCA, you look at the eigen values along this axis here.",
            "And you see some sort of elbow like that, so the eigenvalues go down, and then this is the elbow, the mythical elbow.",
            "And the idea is that this is the noise level and then you retain any of those components.",
            "OK, so PCA is making this more explicit if you really took the.",
            "I can decomposition of that matrix form WW transpose plus Sigma squared.",
            "I you really would see this.",
            "This would be Sigma squared.",
            "Yeah, and there would be a constant eigenvalue and this would be the dimensionality queue of these matrices here.",
            "Yeah, so it's encoded in this model that's being fitted to the data.",
            "But in fact, if you too if you try and fit the number of components, it will always say that everything is a component.",
            "The maximum likelihood is found by including all components.",
            "There's no magical fit where it will find this noise for you, so you either have to do one of two things.",
            "One is the classic thing we all know and love, which is to select the latent dimensionality Q and then fit Sigma squared.",
            "It's the average of the discarded eigenvalues, but the other cool thing you can do, which people often don't understand with PCA 'cause they may not understand probabilistic PCA, is you can set Sigma squared.",
            "To sum known value, let's call it A and if you do that then that will determine the dimensionality of the latent space for you automatically.",
            "Fact what it turns out is.",
            "Bear in mind this so this is UQ.",
            "The first Q eigenvectors.",
            "Over the sample covariance elyza Q by Q matrix is given by the 1st Q eigenvalues minus Sigma squared I.",
            "If I set Sigma squared.",
            "To some value a then this will be complex.",
            "For any.",
            "Eigenvalue which is smaller than a.",
            "That doesn't mean there's a complex solution.",
            "What you can prove is that you should discard all dimensions who have a smaller eigenvalue than that of Sigma.",
            "Yeah, so in that case it's somehow saying that the physical analogy of that is that your data looks like.",
            "This.",
            "It's only covariance that looks like that you're setting the variance, the noise variance, something like that larger than that eigenvalue.",
            "And it will say no, there's only one dimension in this data.",
            "It will switch off.",
            "That dimension which is smaller in standard deviation than your noise variance.",
            "So that's something you can do, and we ourselves use that to effect in a model where we estimated the noise level by separate means on gene expression data.",
            "So we were doing PCA on gene expression data and we had a way of estimating the noise in each gene expression measurement, and then that means that the dimensionality of our PCA was automatically determined.",
            "So if you sort of aware of that, that can be a useful trick.",
            "OK."
        ],
        [
            "So that's principle component analysis and those datasets I mentioned earlier.",
            "I'm just going to sort of quickly show you PCA on those different data set, and then we'll do a 5 minute break.",
            "So this is that stick man walking.",
            "I told you he did three strides.",
            "He actually starts from a strange pose.",
            "He starts from this pose here, and it's actually if you look at the third principle component, you'll see that disappearing off into the distance.",
            "So we're just looking at the first 2, the first 2 represent the strides so.",
            "These circles here.",
            "He's actually changing the angle of its run.",
            "That's why they don't lie on top of each other.",
            "The data is set so he runs in place.",
            "Yeah, so I removed the mean so he just runs in place otherwise it would just be a long stream of data.",
            "And what you can see is basically that because of the changing angle of run, the different strides don't quite align on top of each other.",
            "I'll show you this data set in the next session as well.",
            "I probably should have shown you this one, but I forgot to do that."
        ],
        [
            "So here is that all data and these are the model doesn't know the different flow regimes, But this is the homogeneous flow regime, the blue.",
            "And these are one of these is annular and one of these is.",
            "Yeah lamb layered stratified.",
            "Yeah so they look very similar because actually the readings the reflections that occur in the readings of the surfaces are kind of the same.",
            "Here you get lots of little reflections of little bubbles so they tend to live in these different regimes.",
            "So you can't quite separate the blue under here.",
            "You don't quite separate."
        ],
        [
            "With two principal components, this is data.",
            "This was the Val data that I didn't talk much about.",
            "How I've separated these is into men and women, so red crosses are men.",
            "Green circles are women.",
            "This is data.",
            "I used a tutorial interspeech, so I actually had some of the names.",
            "Here are people who are known in their community 'cause it's speech people, but I also labeled there's these three accents.",
            "Here are all South Yorkshire accents there from our admin staff in Sheffield and they sort of come closest together.",
            "Some of the undergraduates are close to the mothers are some far away, so you see there there's some correlations and structure in the data.",
            "This is me here.",
            "I'm worryingly close to the women.",
            "But that's 'cause PC's are limited algorithm really.",
            "My voice is very masculine, it just didn't pick that up.",
            "And John Barker, who I gave that tutorial into speech with his over here.",
            "So we are apparently very close together, so this is based on the valves.",
            "We make it some it's a hidden Markov model fit to our speech and then I just took the valves and the parameters of the hidden Markov models associated with the valves and doing PCA on that."
        ],
        [
            "This is the gene expression data.",
            "I don't know if this should separate or not, but what I did is I label the different time series.",
            "I told you it's this yeast cell data and it's some different time series conflated together.",
            "So each time series I've actually labeled with a different thing.",
            "So this is one of the time series.",
            "Blue Cross is the purple squares or another time series the red crosses.",
            "I think aren't even time series, they just one off experiments and the cyan actresses are a separate time series.",
            "The overlaying a bit but blue seems to separate on two sides and is the structure there.",
            "I don't know, but it's one way of visualizing it."
        ],
        [
            "OK, so what's the point in the probabilistic approach?",
            "Why wouldn't we just project with regular PCA?",
            "So why do probability?",
            "This is a standard question again again, and this is the standard answer that you always put in your papers.",
            "It's easy to extend to mixture models, so instead of doing mixtures of Gaussians.",
            "The first thing you should be doing is either mixtures of PCA or mix is a factor analyzers.",
            "If you've got a lot of data and you do a lot of restarts, you'll find that makes it a factor, analyzes proposed by Zubin and Matt Beal sometime ago, is very good.",
            "The Bayesian mixture factor analyzes that they use which also learn so mentality is a good model.",
            "And Mike Typings was really good at fitting these mixtures of PCA and you could get some really good density models because what you're doing there.",
            "Is your lining up little linear manifolds?",
            "Yeah, so each of these things is you do a mixture model of this.",
            "If there's enough data it fits these small correlated areas.",
            "So what you're saying in those cases, and this is a much more reasonable model in your high dimensional space, you've got a prototype and the prototype is corrupted locally in a linear way.",
            "Yeah, with some dimensionality.",
            "That's a very reasonable model.",
            "You may need a lot of data to fit all those local linear patches, and that's the weakness of it, but it's a very good way of modeling data, so that's less than one if you're going to mix the Gaussians in peace in high dimensions, don't.",
            "Two mixtures of probabilistic PCA or Bayesian factor analysis.",
            "Excellent standard model for comparison against.",
            "You can do model selection with Bayesian treatment parameters.",
            "So learning the dimensionality if this time in the third session, I'm going to show you that for nonlinear dimensionality reduction.",
            "You can deal with missing data, so missing data or these all factor analysis as well.",
            "I want to emphasize that I look at PCA, I was asked and when these guys did this work.",
            "So I'm Chris Bishop was my PhD supervisor, so I tend to focus on probabilistic PCA, but there's a whole load of parallel work in factor analysis which I want to emphasize exist."
        ],
        [
            "As well.",
            "So here's a oil data.",
            "In this case, we've removed 10% of the values by missing at random, and you can do various things.",
            "You can do literally EM algorithm.",
            "There's a few other things you can do to fill in these missing at random guys, and you get a corruption of the original plot, but you can still see the structure that's 10."
        ],
        [
            "Sent missing 20% missing."
        ],
        [
            "30% missing you can still see the structure.",
            "That's quite a lot of missing data.",
            "In fact.",
            "A probabilistic matrix factorization can just be seen as Bayesian PCA.",
            "It's exactly the same model structurally, and that's being applied to deal with large datasets of preference data.",
            "Collaborative filtering, where it's almost all missing data.",
            "Yeah, you've only got 5 values per person and.",
            "That's if in effect this model with very large amount of missing data, so probabilistic modeling."
        ],
        [
            "Gives you the ability to deal with that.",
            "Or here's 50% missing.",
            "Still some of the structure there greens overlapping with red completely, but you can still see these separate islands."
        ],
        [
            "So factor analysis is I've said is the same model but with this noise variances Now D. So each each output now has its own variance.",
            "But it leads to this problem that you can't optimize this likelihood in one step.",
            "You have to.",
            "You can use an EM algorithm, but it's not the fastest way and I did do a little bit of reading about it, it's just a massive field fitting these models.",
            "I mean, you'd really like a simple paper that just says what the best way is.",
            "I recommend the thing I read that perhaps made most sense.",
            "R is the main statistical package.",
            "So a good way of understanding what the statisticians are doing is finding what command they've put in R. And there's a way of doing factor analysis in R which describes.",
            "How it's doing it, which doesn't fit easily.",
            "How you're doing the maximum likelihood, so I can't exactly tell you how they're doing that, but I would have a look at that if you're interested in good factor analysis fits I tend not to do factor analysis, I tend to do PCA as I've said."
        ],
        [
            "Um?",
            "Independent component analysis, so it's worth briefly mentioning that, so again, the same linear Gaussian relationship, but now these source distributions are non Gaussian and this has the effect of removing.",
            "Well, it has the effect of messing up your nice analytical model so you can't anymore right down easily.",
            "The marginal likelihood and just do the fits.",
            "That's effect number.",
            "One effect #2."
        ],
        [
            "Two is instead of having a source distribution like this.",
            "This is like the example where remember earlier I said with Super Gaussian.",
            "If this is if I said this is 1000 dimensional Gaussian, all those 999 dimensions are being projected on top of the origin here well."
        ],
        [
            "This is the equivalent plot for a Super Gaussian source.",
            "Remember I said the same thing can occur.",
            "Central limit Theorem shows you the distances of the same, but the points will lie along the axis.",
            "Yeah, so this is the classical ICA type thing.",
            "If we were trying to do separation of sound sources there Super Gaussian in nature.",
            "If you were to plot them, you would see these sort of crosses like this.",
            "The source distribution they use then has this cross form.",
            "Now if I rotate it when I rotated a sphere.",
            "It didn't change that."
        ],
        [
            "Here, so the density here, when rotated, doesn't change."
        ],
        [
            "But this density does a cool thing to do if you're playing.",
            "If you want to do ICA and you've got some standard ICAO data set which you know has independent components, just do PCA on it, projecting to the latent space, you will see a rotated cross.",
            "And I see a is just finding that rotation to UN rotate it or."
        ],
        [
            "You will see a rotated square sub.",
            "Gaussian sources do the same thing.",
            "So what they do there is in this latent space, the probabilistic interpretation.",
            "They do other ways of fitting ICA, but the probabilistic interpretation is your fitting.",
            "Density which is non Gaussian in this latent space.",
            "To get your independent components out.",
            "OK, so that's going to be it for the first session, so unlike Bernard, I'm not going to court make you vote on make the lazy ones say yes, I want a break.",
            "You're all going to have a break.",
            "Or you can ask me questions and I'll drink some water or something.",
            "OK so will restart in 5 minutes."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm talking about dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "Which is something that interested me, I guess when I started I mean, oh, I should say it's very true what Bernard says about meeting people places.",
                    "label": 0
                },
                {
                    "sent": "I think I met Bernard or saw Bernard for the first time in 1997 at the machine learning and generalization school.",
                    "label": 0
                },
                {
                    "sent": "So we've known each other I guess for quite a long time and the same thing.",
                    "label": 0
                },
                {
                    "sent": "Kind of course happened at these summer schools.",
                    "label": 0
                },
                {
                    "sent": "You know, you can build relationships with people you meet 15 years later, organize your own summer schools.",
                    "label": 0
                },
                {
                    "sent": "And have people attend.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to talk about dimensionality reduction, which is something I would became interested in as a PhD student.",
                    "label": 0
                },
                {
                    "sent": "I didn't do my thesis on it, but it struck me as something that was interesting and important, and.",
                    "label": 0
                },
                {
                    "sent": "Shortly after that amount of work happened in spectral methods in machine learning, which I'm going to try and view.",
                    "label": 0
                },
                {
                    "sent": "Part of what I say is is tutorial, but part of it is polemic.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to push a certain point of view.",
                    "label": 0
                },
                {
                    "sent": "You're entitled to disagree with that point of view.",
                    "label": 0
                },
                {
                    "sent": "Not everything I say is has to be considered right.",
                    "label": 0
                },
                {
                    "sent": "I think it's right.",
                    "label": 0
                },
                {
                    "sent": "That's why I say it, but it's also to encourage thought in yourselves.",
                    "label": 0
                },
                {
                    "sent": "Other people take different perspectives from the one I present.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I wanted to first of all do is start.",
                    "label": 0
                },
                {
                    "sent": "With something I think I struggled with, which is actually what it means to think in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "When people tell you these things about how high dimensional spaces are, what that means in practice and why, perhaps your intuitions aren't quite right.",
                    "label": 0
                },
                {
                    "sent": "And then I want in this section to talk about linear dimensionality reduction and probabilistic linear dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "I'll also move onto sort of non probabilistic perspective in terms of distant matching given time in this session.",
                    "label": 0
                },
                {
                    "sent": "Otherwise I'll push that into the next session.",
                    "label": 0
                },
                {
                    "sent": "I haven't calibrated the speed of this, so depending on how fast I'll go, different material will appear.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Open places.",
                    "label": 0
                },
                {
                    "sent": "So, so just a bit of notation, so I'm following the statisticians in when I talk about data it will be of dimensionality little P, which I think is confusing because it's a probability distribution, but that's what statisticians.",
                    "label": 0
                },
                {
                    "sent": "So if ever I kick that that's a thing goes off, that's great.",
                    "label": 0
                },
                {
                    "sent": "That's causality inference there, by the way.",
                    "label": 0
                },
                {
                    "sent": "That's the second time I did that.",
                    "label": 0
                },
                {
                    "sent": "But that's advanced stuff.",
                    "label": 0
                },
                {
                    "sent": "We're not talking about that here.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Latent dimensionality I'm going to use Q.",
                    "label": 0
                },
                {
                    "sent": "An will be the number of data points and my data matrix will always be Y.",
                    "label": 1
                },
                {
                    "sent": "So whenever you see why that's a design matrix containing data, so it's got N Rosen, P columns.",
                    "label": 0
                },
                {
                    "sent": "So that's the definition of a design matrix.",
                    "label": 0
                },
                {
                    "sent": "Again, it's statistics inspired, and I'm going to use the reduced dimensional representation, which I call the latent variables, but it's the low dimensional representation will be X.",
                    "label": 0
                },
                {
                    "sent": "These guys will come in later there, more for future slides.",
                    "label": 0
                },
                {
                    "sent": "So just a little bit of notation.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to use this Matlab style notation to try and differentiate between columns from a Ann Rosa Vaso AI: is a row of a, but as a column vector and this is a column of a as a column vector and then the element of A is AIJ.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the source code and stuff is online, so why high dimensional data?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It really dominates in machine learning, partially for the some of the reasons Bernard was talking about earlier.",
                    "label": 1
                },
                {
                    "sent": "I mean not just the number of data points.",
                    "label": 0
                },
                {
                    "sent": "As you increase that, it becomes hard for a human to look at a large number of data points.",
                    "label": 0
                },
                {
                    "sent": "It also becomes hard to look at high dimensionality, so examples of high dimensional data could be a customer in a database and the features might include their purchase history where they live, their sex, their age, their film preference, what tests have been done on them from a medical perspective.",
                    "label": 1
                },
                {
                    "sent": "If you start think trying to think of characterizing an individual in numbers, you're going to have a very large set of numbers.",
                    "label": 1
                },
                {
                    "sent": "So digitized photograph or video gotta pixel intensities, time, date, location of the photograph.",
                    "label": 1
                },
                {
                    "sent": "One thing I like to play with is motion capture data.",
                    "label": 0
                },
                {
                    "sent": "I like playing with motion capture data 'cause it's very high dimensional.",
                    "label": 0
                },
                {
                    "sent": "But if you have a good model of it, humans can recognize it.",
                    "label": 0
                },
                {
                    "sent": "So it's something you're quite trained to perceive.",
                    "label": 0
                },
                {
                    "sent": "If you see a model of a stick man moving, you know whether the motions natural or not.",
                    "label": 0
                },
                {
                    "sent": "Now that's 100 and something dimensional.",
                    "label": 0
                },
                {
                    "sent": "Time series very often, but you can look at it and say.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's correct, so you're capable of dealing with high dimensional data when it's presented in the correct form.",
                    "label": 0
                },
                {
                    "sent": "That's something that shows that so motion capture data is something that I guess I started working with, and I've created the MATLAB mocap toolbox.",
                    "label": 0
                },
                {
                    "sent": "So if you wanted to use it yourself, you can just move it into Matlab and Jeff Hinton student did that as well.",
                    "label": 0
                },
                {
                    "sent": "So he demonstrates with mocap data 'cause it shows you when you've got a good model of high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "It's very clear to a human when the humans moving naturally.",
                    "label": 0
                },
                {
                    "sent": "So human speech when I started in machine learning 15 years ago or more, there was a big overlap between machine learning and speech, so nowadays that seems to be smaller, but I think it's coming back.",
                    "label": 0
                },
                {
                    "sent": "In fact, people are doing sort of deep learning in speech recognition.",
                    "label": 0
                },
                {
                    "sent": "It's a very interesting application domain and there's a lot of opportunity, I think.",
                    "label": 0
                },
                {
                    "sent": "Well, it's difficult because they do it very well, but I think it's an interesting set of data, but it's also high dimensional an it's a time series, so web pages.",
                    "label": 0
                },
                {
                    "sent": "Or other documents.",
                    "label": 1
                },
                {
                    "sent": "So features could consist of frequencies of given words.",
                    "label": 0
                },
                {
                    "sent": "That's bag of words, models, other features could be how those documents are linked.",
                    "label": 0
                },
                {
                    "sent": "So I have colleagues who are in the language group in Sheffield that are analyzing Twitter feeds, and so each tweet.",
                    "label": 0
                },
                {
                    "sent": "It, even though it's only 5 or 6 words, it's potentially from a language of maybe 16,000 words and it's associated with information about where the tweet was given.",
                    "label": 0
                },
                {
                    "sent": "Who's following that Twitter?",
                    "label": 0
                },
                {
                    "sent": "Is that the right word?",
                    "label": 0
                },
                {
                    "sent": "Twitter these young people in the audience, you should know.",
                    "label": 0
                },
                {
                    "sent": "And all this additional information, so that's high dimensional data as well.",
                    "label": 0
                },
                {
                    "sent": "Another area I work is computational biology gene expression, so you can characterize the state of a cell by effectively the concentration and location of all the chemical compounds that are in that cell.",
                    "label": 0
                },
                {
                    "sent": "If you think of the size of that state space you have, I don't know.",
                    "label": 0
                },
                {
                    "sent": "My memory is something like 30,000 genes in your body.",
                    "label": 0
                },
                {
                    "sent": "Each of those genes has products which may be in the form of proteins.",
                    "label": 0
                },
                {
                    "sent": "Many, many more proteins than there are genes.",
                    "label": 0
                },
                {
                    "sent": "So if we think of the state of an individual one of yourselves.",
                    "label": 0
                },
                {
                    "sent": "You need, you know millions of variables to characterize its state at any given time, plus position velocity of each thing very very very high dimensional.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So how do we deal with complex data?",
                    "label": 0
                },
                {
                    "sent": "So I've tried to simulate some complex data here and this is very much starting from what goes on in a low dimensional because typically people don't show you images of high dimensional data 'cause you can't visualize it.",
                    "label": 0
                },
                {
                    "sent": "So when I first started doing machine learning, if you looked at data like this it looks complicated, but people had techniques for dealing with this.",
                    "label": 0
                },
                {
                    "sent": "They had mixtures of Gaussians MoD.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here with a circle located in the center, Anna Little ellipsoid denoting.",
                    "label": 0
                },
                {
                    "sent": "The sort of covariance of the model you've got a mixture of Gaussians fit to that data, which you must think that's really good, because that looks quite complex in terms of data and density.",
                    "label": 0
                },
                {
                    "sent": "Modeling is something we want to do.",
                    "label": 0
                },
                {
                    "sent": "Understand the density of the data, and I tell you that fit is actually perfect because it's the model I generated the data from, so this isn't even a fit from the data.",
                    "label": 0
                },
                {
                    "sent": "This is the true model.",
                    "label": 0
                },
                {
                    "sent": "So once you've seen that, you sort of think complex structure isn't a problem.",
                    "label": 0
                },
                {
                    "sent": "For mixtures of Gaussians, yeah.",
                    "label": 1
                },
                {
                    "sent": "I mean, it can model these things quite well.",
                    "label": 0
                },
                {
                    "sent": "People do a lot of effort into selecting the number of components into dealing with the fact that there's multiple local minima, but fundamentally surely that's the problem solved.",
                    "label": 0
                },
                {
                    "sent": "Well, that's not the case, and it's not the case.",
                    "label": 0
                },
                {
                    "sent": "I'd argue because of the way you look at data in two dimensions is somehow sort of wrong and a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Maybe a nice model in two dimensions, but a straightforward Gaussian.",
                    "label": 0
                },
                {
                    "sent": "A Gaussian where you're not thinking about what you're doing.",
                    "label": 0
                },
                {
                    "sent": "So if you just apply a Gaussian and fit its covariance in high dimensions, it's going to be quite a bad model.",
                    "label": 0
                },
                {
                    "sent": "Very often.",
                    "label": 0
                },
                {
                    "sent": "I still argue that Gaussians can be very useful, and you can do a range of things with Gaussians, but you have to do things like structure the covariance matrix in order to fit them.",
                    "label": 0
                },
                {
                    "sent": "So why is it you know this is the sort of thing that I came into machine learning from a fairly naive perspective in those days, machine learning was about how the brain worked, and neural networks with how the brain works.",
                    "label": 0
                },
                {
                    "sent": "And then these bunch of people came along with support vector machines and messed it all up.",
                    "label": 0
                },
                {
                    "sent": "Finally with deep learning they're going back to doing what we were doing 15 years ago.",
                    "label": 0
                },
                {
                    "sent": "Back to these sort of drawing.",
                    "label": 0
                },
                {
                    "sent": "These circles and putting lines between them.",
                    "label": 0
                },
                {
                    "sent": "I came in from a mechanical engineering background.",
                    "label": 0
                },
                {
                    "sent": "With this naive you that neural networks is where it's at and then when I saw these statistical models of mixes Gaussians, I thought well surely they are very powerful too.",
                    "label": 0
                },
                {
                    "sent": "But I think the reason that they aren't as widely used as they should be is because in high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "They're typically don't work very well.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "These two dimensional plots of Gaussians can be misleading and our low dimensional intuitions can fail dramatically, and there's a couple of major issues.",
                    "label": 1
                },
                {
                    "sent": "In high dimensions, in a Gaussian density, all that data actually moves to a shell.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to try and show you visualize what I mean by moving to a shell.",
                    "label": 1
                },
                {
                    "sent": "You may have heard this before.",
                    "label": 0
                },
                {
                    "sent": "There is no data near the mean of the Gaussian, so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One way of thinking about a clustering model like this is what you're saying about your data, and I like this way of thinking about it is that each data point is really a prototype.",
                    "label": 0
                },
                {
                    "sent": "That's the mean of the Gaussian that has then been corrupted in some way according to the covariance of the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So you're saying you represent your data by prototypes that are the mean plus corruption, which is the covariance.",
                    "label": 0
                },
                {
                    "sent": "How they move away from the mean.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But what this high dimensional data stuff is saying that in high dimensions?",
                    "label": 1
                },
                {
                    "sent": "The prototype is never going to be observed because you'll always be corrupted in a shell some distance away from the prototype, which is kind of an odd thing.",
                    "label": 0
                },
                {
                    "sent": "So how does that go on?",
                    "label": 0
                },
                {
                    "sent": "So I sort of used to use to hear this, and I used to think, OK, yeah, fine people would prove things about it, but I never had the intuition.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is an exercise in trying to understand the intuition and it's based around a Gaussian egg.",
                    "label": 0
                },
                {
                    "sent": "OK, there's another exercise.",
                    "label": 0
                },
                {
                    "sent": "These are curse of dimensionality exercises in the old Bishop book and what I want you to see in this egg.",
                    "label": 0
                },
                {
                    "sent": "This is a 1 dimensional egg and then I'm showing a density across this egg.",
                    "label": 0
                },
                {
                    "sent": "The yellow bit in the middle is the yolk.",
                    "label": 0
                },
                {
                    "sent": "And then the white bit on the outside is the sort of album, and then I put a green bit.",
                    "label": 0
                },
                {
                    "sent": "Right at the interface between the yolk and the and the white of the egg so that green bit is if you over boil your egg like some places.",
                    "label": 0
                },
                {
                    "sent": "Do you see on the outside of it there's a green covering its parity iron sulfide and it's my egg smell bad because they've got sulfur in them, so that green bit there is.",
                    "label": 0
                },
                {
                    "sent": "This is an over boiled egg, but it's got a relatively thin layer that makes an egg taste bad actually, so you shouldn't over boil your eggs, but here it's been over boiled and the way we've set it up is that.",
                    "label": 0
                },
                {
                    "sent": "The the iron sulfide is between .95 and 1.05 standard deviations from the mean.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so between 95% and 105% of a standard deviation from the mean.",
                    "label": 0
                },
                {
                    "sent": "So the yolks up to .95 of the standard deviation and the white goes from 1.05 to Infinity.",
                    "label": 0
                },
                {
                    "sent": "This is a big egg.",
                    "label": 0
                },
                {
                    "sent": "I mean, well, a real egg would be a square density, right?",
                    "label": 0
                },
                {
                    "sent": "And so here we are sort of fitting a Gaussian density across it.",
                    "label": 0
                },
                {
                    "sent": "So that's how 1 dimensional leg.",
                    "label": 0
                },
                {
                    "sent": "So if we look at the 1 dimensional leg, we find it's really nice 'cause the yolk is my favorite bit of the egg.",
                    "label": 0
                },
                {
                    "sent": "That's nice and runny and tasty, and 65.8% of your egg is the tasty bit.",
                    "label": 0
                },
                {
                    "sent": "OK, the white is sort of more healthy.",
                    "label": 0
                },
                {
                    "sent": "It's got protein in it.",
                    "label": 0
                },
                {
                    "sent": "29% of your egg is so nearly close to 30% of your egg is protein and 5% of the egg is this nasty iron sulfide bit that you don't like.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what happens in two dimensions?",
                    "label": 0
                },
                {
                    "sent": "You can compute these volumes again.",
                    "label": 0
                },
                {
                    "sent": "The volume of the obviously probability distribution, so it's the total volume 100% and in two dimensions you get 60% of your eggs, so you've lost 5% of your yolk, yeah?",
                    "label": 0
                },
                {
                    "sent": "And seven point 4% of your egg is now the iron sulfide and 33% a third of the egg is now in the white.",
                    "label": 0
                },
                {
                    "sent": "OK, the reason?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Obviously, because this high density here in lower density here so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you're sort of seeing this is now the view above of the egg, and this is a Gaussian egg, so this is higher peak.",
                    "label": 0
                },
                {
                    "sent": "There's a sort of mountain of egg there, so there's more density in the middle and there is on the outside.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK, but now if we go to three dimensions, the density on the outside goes up a bit.",
                    "label": 0
                },
                {
                    "sent": "This 10% of the density is now in this iron sulfide 56%, so we've lost some of the yolk, but how does it behave as we go higher and higher dimension?",
                    "label": 0
                },
                {
                    "sent": "Well, the nice thing so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've seen a sort of order that seems to be.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bring up white goes.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Up, yeah, yolk goes down.",
                    "label": 0
                },
                {
                    "sent": "An iron sulfide goes up, so does that continue to be the case?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, what it turns out is we can actually compute the density of where the probability mass is so.",
                    "label": 1
                },
                {
                    "sent": "If we're thinking of this egg in high dimensions, all I'm doing is I'm thinking of a Gaussian distribution where I'm sampling independently across axis with a fixed variance.",
                    "label": 1
                },
                {
                    "sent": "So if the variance of the Gaussian is Sigma squared, what it turns out is the distance from the mean, which is obviously from Pythagoras Theorem the square root of Y I-1 plus Y I2 squared.",
                    "label": 0
                },
                {
                    "sent": "So here's sample one here, sample two.",
                    "label": 0
                },
                {
                    "sent": "That's where we end up.",
                    "label": 0
                },
                {
                    "sent": "That's our distance from the mean.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the distance is given by the square root.",
                    "label": 0
                },
                {
                    "sent": "Now I can't do much with the square root.",
                    "label": 0
                },
                {
                    "sent": "Well, you can, but it's actually easier to work with the square of the distance, and that's going to be an ongoing theme in everything I do.",
                    "label": 1
                },
                {
                    "sent": "So it turns out that the square of this distance is chi squared distributed.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's a statistical known thing, used quite a lot.",
                    "label": 0
                },
                {
                    "sent": "So each of these samples their length is chi squared distributed.",
                    "label": 0
                },
                {
                    "sent": "So if you want the square of the distance of the whole thing, it's just the square of that.",
                    "label": 0
                },
                {
                    "sent": "Then it's the sum.",
                    "label": 0
                },
                {
                    "sent": "Have some chi squared distributed variables, so that's rather nice.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because it's analytics, so I like to think of the chi squared distribution as a gamma, because that way I don't have to remember lots of different distributions, but it's a gamma distribution in the way I parameterise a gamma, which is this form here.",
                    "label": 1
                },
                {
                    "sent": "This is this density that's the normalizer, and the important part RX to the a -- 1 E to the minus BX.",
                    "label": 0
                },
                {
                    "sent": "That's the form of the distribution in X.",
                    "label": 1
                },
                {
                    "sent": "In this form, the mean of this distribution is a / B.",
                    "label": 1
                },
                {
                    "sent": "Sometimes people set it so that the bees underneath X in the main is.",
                    "label": 0
                },
                {
                    "sent": "A time is being, but this is the way I like.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do it writing of the chi squared is showing that it's scaled by Sigma squared, yeah?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is actually a scale parameter and so the Sigma squared appears underneath here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you've got a gamma distribution.",
                    "label": 0
                },
                {
                    "sent": "The square of this value here is distributed as a gamma with a parameter.",
                    "label": 0
                },
                {
                    "sent": "The shape parameter set to half, and what I think it was the rate parameter set to 1 / 2 Sigma squared.",
                    "label": 1
                },
                {
                    "sent": "So that.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nice thing about that is what you're interested in is the distance from the actual origin of.",
                    "label": 0
                },
                {
                    "sent": "A sample after you increase the dimensions, yeah, and what it turns out is the gamma rather nicely.",
                    "label": 0
                },
                {
                    "sent": "If we want to add 2 gamma variables here and ask what's inside this square root here, that's also gamma distributed.",
                    "label": 0
                },
                {
                    "sent": "As long as the scales of these two gammas are the same which they are.",
                    "label": 0
                },
                {
                    "sent": "If we're assuming the same variance, it's gamma distributed with the shape parameter, which is the sum of the shape parameters of these guys.",
                    "label": 1
                },
                {
                    "sent": "So in this case it's one and a scale rate parameter, which is the same as before.",
                    "label": 1
                },
                {
                    "sent": "That's if these guys are independent, which I'm assuming they are, so that's important.",
                    "label": 0
                },
                {
                    "sent": "We're assuming with sampling each of our features of our data in this Gaussian egg is being independently sampled from a Gaussian distribution which is spherical.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we can compute this cumulative density function, and indeed we can do it.",
                    "label": 0
                },
                {
                    "sent": "For arm P dimensional data.",
                    "label": 0
                },
                {
                    "sent": "So P dimensional data the sum over these iy squared.",
                    "label": 0
                },
                {
                    "sent": "These YYIK squared.",
                    "label": 0
                },
                {
                    "sent": "So this is the I TH data point in each dimension of it is distributed as a gamma with the shape parameter P / 2.",
                    "label": 0
                },
                {
                    "sent": "Now that means that the expected value under this distribution is P times Sigma squared.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we'll typically do is we'll ask about 1 / P times the expected value so that we keep the expected value of the squared distance.",
                    "label": 0
                },
                {
                    "sent": "This is in P dimensions.",
                    "label": 0
                },
                {
                    "sent": "Now from the origin.",
                    "label": 0
                },
                {
                    "sent": "To be Sigma squared, so that means that in the case of a standard normal, it's going to be one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we'll be working with standard normals.",
                    "label": 0
                },
                {
                    "sent": "So the scaling of that scales the rate parameter and we've got now a density formula that says what will the distance, the squared distance.",
                    "label": 1
                },
                {
                    "sent": "How is the squared distance from the mean distributed?",
                    "label": 0
                },
                {
                    "sent": "Now that's nice because in our Gaussian egg model we can now ask the question, how does that square distance from the mean vary with the number of dimensions you see?",
                    "label": 0
                },
                {
                    "sent": "This density is a function of P, the mean distance from the.",
                    "label": 0
                },
                {
                    "sent": "So the mean distance from the origin?",
                    "label": 1
                },
                {
                    "sent": "Which is the mean in this case?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's the entire volume of the egg and you started out with the yolk, which was 65% of the egg and a white, which was I think 28% now initially the volume of the white portion goes up, but then actually it starts to drop down.",
                    "label": 0
                },
                {
                    "sent": "And then you see the yolk drops down as well, until when you get to 1000 dimensions with what I've defined to be the alluminium, you basically have only.",
                    "label": 0
                },
                {
                    "sent": "See points which are that distance from the mean.",
                    "label": 0
                },
                {
                    "sent": "All your data sits in that shell.",
                    "label": 0
                },
                {
                    "sent": "Does that make sense?",
                    "label": 0
                },
                {
                    "sent": "So that's very counter intuitive verse.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What you think?",
                    "label": 0
                },
                {
                    "sent": "If you look at a Gaussian because you can see this Gaussian, the data is close to the mean, right?",
                    "label": 0
                },
                {
                    "sent": "You know the Gaussian the date is close to the mean.",
                    "label": 0
                },
                {
                    "sent": "It's very obvious.",
                    "label": 0
                },
                {
                    "sent": "Right now there's a fun fact here that every marginal of a Gaussian is Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So this is a 2 dimensional joint Gaussian with variance of 1.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But actually, no, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's a 3 dimensional Gaussian.",
                    "label": 0
                },
                {
                    "sent": "I'm just showing you the marginal of two points 210 to two variables in that Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it's a 3 dimensional Gaussian.",
                    "label": 0
                },
                {
                    "sent": "That means that when you look in these two dimensions, we're projecting the third dimension down onto the 1st two.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so there's an amount of data that appears close to the mean here, but really in the third dimension, it's sitting away from the mean, 'cause it's a 3 dimensional Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Oh no, it's a four dimensional Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So there are two dimensions, right?",
                    "label": 0
                },
                {
                    "sent": "Being projected onto the mean so the bit you see in the mean is really two other dimensions where the data is away from the mean and it's being projected down to the mean.",
                    "label": 0
                },
                {
                    "sent": "No, no, no.",
                    "label": 0
                },
                {
                    "sent": "It was a 5 dimensional Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK, I can go on like that for awhile and we can go up to 1000 dimensions.",
                    "label": 0
                },
                {
                    "sent": "Let's say it's 1000 dimensions.",
                    "label": 0
                },
                {
                    "sent": "This is a marginal distribution from 1000 dimensional Gaussian 998 dimensions.",
                    "label": 0
                },
                {
                    "sent": "You're not visualizing, you're just seeing projected down to the mean, yeah?",
                    "label": 0
                },
                {
                    "sent": "So that means that there's no data near the mean.",
                    "label": 0
                },
                {
                    "sent": "If we go to, we could even a Gaussian process is an infinite dimensional Gaussian, so you could even talk about an infinite dimensional Gaussian and look at these two parts of it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so then there's definitely no day today.",
                    "label": 0
                },
                {
                    "sent": "That mean because we've projected infinite, denser dimensions down onto the mean.",
                    "label": 0
                },
                {
                    "sent": "So when you look at a Gaussian, the way I used to look at a Gaussian isar look, there is data.",
                    "label": 0
                },
                {
                    "sent": "This mean whatever people say about what's happening in high dimensional spaces, I can see for myself that this data near the mean.",
                    "label": 0
                },
                {
                    "sent": "But remember you're looking at potentially a projection of a very, very high dimensional that Gaussian down onto those two dimensions, which means that all this data in that high dimensional Gaussian must be nowhere near the mean, yeah?",
                    "label": 0
                },
                {
                    "sent": "So in fact this approximation is so good that in many areas they will use a Gaussian distribution, taking it to high dimensions as an approximation for a sphere.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in very high dimensions.",
                    "label": 0
                },
                {
                    "sent": "If you want to have uniform data distributed over a sphere, you can approximate that with the Gaussian, and that's a sort of known physics trick.",
                    "label": 0
                },
                {
                    "sent": "Works very well as dimensions go in the thousands.",
                    "label": 0
                },
                {
                    "sent": "So Gaussians are bad distributions apparently.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Haha so.",
                    "label": 0
                },
                {
                    "sent": "The other thing that happens is interpoint distances.",
                    "label": 1
                },
                {
                    "sent": "So in one slide this is sort of trying to show that the same thing happens with the distance between data points.",
                    "label": 0
                },
                {
                    "sent": "If they're sampled from this Gaussian density for very similar reasons, it comes out very quickly if you think about it.",
                    "label": 0
                },
                {
                    "sent": "So if you've got two data points, both sample from the same Gaussian, then subtracting one off the other leads to another Gaussian density.",
                    "label": 0
                },
                {
                    "sent": "And squaring it then again leads to this gamma density.",
                    "label": 0
                },
                {
                    "sent": "So that's just the only new step is subtracting one from the other, and Sigma squared becomes two Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "Everything else stays the same and you can see that these data points are gamma distributed.",
                    "label": 0
                },
                {
                    "sent": "The interpoint distances gamma distributed with the parameters of the gamma being P / 2 and P / 4 Sigma squared rather than two Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "As we saw before.",
                    "label": 0
                },
                {
                    "sent": "So I like this representation 'cause it's very easy and very common to compute.",
                    "label": 0
                },
                {
                    "sent": "Interdata point distances.",
                    "label": 0
                },
                {
                    "sent": "Yeah to do K nearest neighbors or whatever else, and in fact I'm going to base a lot of material on computing distances like this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The dimension normalized between points is drawn from a gamma an the mean is two Sigma squared.",
                    "label": 1
                },
                {
                    "sent": "Now the interesting thing about this, the variance of this distribution is eight Sigma squared over P. So as P goes high, this variance goes to 0.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Certainly relative.",
                    "label": 0
                },
                {
                    "sent": "If you look at the mean divided by the square root of the variance you got this scaling of P that as you go to very high dimensions what you'll see is that all the interpoint distances of your data set are.",
                    "label": 0
                },
                {
                    "sent": "The same distance apart.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in this Gaussian egg, so any sample you take will be equidistant from every other sample.",
                    "label": 0
                },
                {
                    "sent": "Which is annoying characteristic if you're trying to do things like K nearest neighbors, because as you change K, you'll find that no well if you look at the volume of neighbors you're considering as you increase the nearness of the neighbor, you suddenly go from no neighbors to all your points and neighbors.",
                    "label": 0
                },
                {
                    "sent": "This basically says that for this density every data point is a neighbor of each other in high dimensions.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now that seems to be a bad thing about Gaussian, so you shouldn't use Gaussians.",
                    "label": 0
                },
                {
                    "sent": "You might think, but actually for any data where you've got independence over features, the central Limit Theorem applies to these sums we're taking, and the same results come out.",
                    "label": 1
                },
                {
                    "sent": "They just don't come out quite so nicely and analytically in the low sample area.",
                    "label": 0
                },
                {
                    "sent": "You get exactly the same thing going on the variance about the mean scales as P 1, /, P, so any.",
                    "label": 1
                },
                {
                    "sent": "Distribution you sample from independently will lead to these different effects.",
                    "label": 0
                },
                {
                    "sent": "If you want to know how it differs, well the Gaussian in this sphere.",
                    "label": 0
                },
                {
                    "sent": "It's like if you are one of the things one of the reasons I believe we're here, it's because there's a large telescope here, and Bernard likes large telescopes.",
                    "label": 0
                },
                {
                    "sent": "So, but it's useful for analogies as well.",
                    "label": 0
                },
                {
                    "sent": "If you're standing on the earth looking out at this hyper sphere.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's high dimensional space of these Gaussian samples.",
                    "label": 0
                },
                {
                    "sent": "How will it look?",
                    "label": 0
                },
                {
                    "sent": "Well, all the samples will be uniformly distributed across space, abit like the stars.",
                    "label": 0
                },
                {
                    "sent": "They're not quite uniformly distributed.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but if you're standing at the center of this hypersphere, so it's a 3 dimensional sphere.",
                    "label": 0
                },
                {
                    "sent": "But it's a hypersphere in general looking out at these data points, they look like stars distributed over space will be beautiful.",
                    "label": 0
                },
                {
                    "sent": "If you use non Gaussian densities, sub Gaussian or super Gaussian in the hypersphere, the points will cluster at either alongside.",
                    "label": 0
                },
                {
                    "sent": "The axes are at 45 degrees to the axis, depending on whether you've used super or sub Gaussians, so they just won't be uniformly distributed over this hyper sphere anymore.",
                    "label": 0
                },
                {
                    "sent": "It won't be quite as pretty, but the same problems will exist.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "These distributions behave very counterintuitively in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "We can compute these densities of squared distances analytically.",
                    "label": 0
                },
                {
                    "sent": "For the Gaussian case and for non Gaussian independent systems we can invoke the central Limit theorem.",
                    "label": 1
                },
                {
                    "sent": "So let's see how this applies for some real data.",
                    "label": 0
                },
                {
                    "sent": "Let's what we're going to do now is.",
                    "label": 0
                },
                {
                    "sent": "We see that this is problem and we can compute the theoretical interpoint distance between two data and it's valid because of the central limit theorem for any data set.",
                    "label": 0
                },
                {
                    "sent": "If the features are independent.",
                    "label": 0
                },
                {
                    "sent": "So here's some.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example datasets, well, let's just test our intuition.",
                    "label": 0
                },
                {
                    "sent": "So this is samples from 1000 dimensional Gaussian and what I'm showing is the distribution of squared distances between each data point.",
                    "label": 1
                },
                {
                    "sent": "Yeah, and what I'm showing you then is a histogram and then bang directly over it is the fit of that gamma distribution we derive before look how nicely the theory fits to data so.",
                    "label": 0
                },
                {
                    "sent": "Um, that's good.",
                    "label": 0
                },
                {
                    "sent": "We can sample from a Gaussian, create a data, set thousand samples, so there's a thousand data points.",
                    "label": 0
                },
                {
                    "sent": "So around a million interpoint distances in this histogram.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now I've used fewer samples here in the fit.",
                    "label": 0
                },
                {
                    "sent": "Still good.",
                    "label": 0
                },
                {
                    "sent": "So this is 10,000 interpoint distances.",
                    "label": 0
                },
                {
                    "sent": "The fit still excellent.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's take that and let's apply it to real.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data so that we can get our NIPS paper so we have to have a little artificial example, then a real data example.",
                    "label": 0
                },
                {
                    "sent": "OK, the real data here is a little bit artificial as well, but this is a famous.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dataset, which is a so called oil data.",
                    "label": 0
                },
                {
                    "sent": "It's oil flowing in a pipeline.",
                    "label": 0
                },
                {
                    "sent": "And it's from Bishop and James.",
                    "label": 1
                },
                {
                    "sent": "This 93 paper.",
                    "label": 0
                },
                {
                    "sent": "It's widely used as a benchmark in dimensionality reduction, and it's oil flowing from a well.",
                    "label": 0
                },
                {
                    "sent": "So it's got oil, gas and water in it.",
                    "label": 0
                },
                {
                    "sent": "They according to the speed.",
                    "label": 0
                },
                {
                    "sent": "I guess Reynolds number and stuff like that they'll mix in different ways.",
                    "label": 0
                },
                {
                    "sent": "In turbulent flow.",
                    "label": 0
                },
                {
                    "sent": "You basically get this homogeneous everything mixed together, but then there's two types of lamina flow that occur.",
                    "label": 0
                },
                {
                    "sent": "One is gas, oil, water and then the other one I think is gas, oil, water.",
                    "label": 1
                },
                {
                    "sent": "I hope I've got the right way around.",
                    "label": 0
                },
                {
                    "sent": "It might be gas or not.",
                    "label": 0
                },
                {
                    "sent": "Must be gas or who knows.",
                    "label": 0
                },
                {
                    "sent": "Anyway, it's like that, sort of.",
                    "label": 0
                },
                {
                    "sent": "Now you've got 12 measurements, which are gamma Ray dense geometry measurements, so these are measuring the density across here.",
                    "label": 0
                },
                {
                    "sent": "And this is a classic learning problem because although clearly if you see the density on each of these, you should be able to determine which domain you're in.",
                    "label": 0
                },
                {
                    "sent": "The physics of The thing is a little bit complicated, so in fact, turns out this data simulated if you read carefully the original paper and it's simulated by understanding the physics of these systems and what these things would read so little bit of a warning there it's simulated data, but it's from a realistic physical system.",
                    "label": 0
                },
                {
                    "sent": "So what you can see here is you've got, I think, 12 sensors in total, measuring the density in the cross section of these pipes.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's 12 readings.",
                    "label": 0
                },
                {
                    "sent": "So if we now compare the truth to what we theoretically expect to happen, we see that this is the theoretical curve for the Inter data point distances, and this is the actual histograms.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's not too bad.",
                    "label": 0
                },
                {
                    "sent": "There's a little bit of a mismatch.",
                    "label": 0
                },
                {
                    "sent": "P is 12.",
                    "label": 0
                },
                {
                    "sent": "The predicted variance of these interpoint distances is .66, and the actual variance we've observed is closer to two, so it's quite a lot larger than we would expect.",
                    "label": 0
                },
                {
                    "sent": "The variance of interpoint distances.",
                    "label": 0
                },
                {
                    "sent": "So anyway, OK, let's not worry.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Too much about that.",
                    "label": 0
                },
                {
                    "sent": "Let's look at another data set I told you I like motion capture data.",
                    "label": 0
                },
                {
                    "sent": "This is motion capture data where the data is only 55 frames and there's 102 data points 'cause there's 34 points on the body, so they put balls on the body at 34 places, and there's an XY zed location for each of these balls.",
                    "label": 1
                },
                {
                    "sent": "Yeah, so that's 3 * 34 One 2 dimensional data, 55 data points, and it's from Ohio State University.",
                    "label": 0
                },
                {
                    "sent": "I use this data, little data set a lot.",
                    "label": 0
                },
                {
                    "sent": "It's a nice small data set and one thing I like about it is the guy changes angle as he runs, so it's two things going on.",
                    "label": 0
                },
                {
                    "sent": "This one is it's periodic 'cause he runs.",
                    "label": 0
                },
                {
                    "sent": "It takes about 3 paces and the other one is some sort of change in his angle, which is sort of overlaid on top of the periodicity.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's do the distances for this data and the theoretical curve.",
                    "label": 0
                },
                {
                    "sent": "OK, P is now much larger, so the expected variance around the mean distance, which is 2.",
                    "label": 0
                },
                {
                    "sent": "I've normalized all this data, so each dimension is variance one, so the expected squared distances 2.",
                    "label": 0
                },
                {
                    "sent": "But what we observe here is that the variance is 1 versus the predicted .07.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mike radiator, so this is 6000 genes.",
                    "label": 0
                },
                {
                    "sent": "And 77 experiments.",
                    "label": 0
                },
                {
                    "sent": "It's a classic early microarray cell cycle data set.",
                    "label": 1
                },
                {
                    "sent": "Its measurements in yeast of.",
                    "label": 1
                },
                {
                    "sent": "These 6000 genes across different time series with different things being done to the yeast.",
                    "label": 0
                },
                {
                    "sent": "So the 77 experiments are done each in different time series.",
                    "label": 0
                },
                {
                    "sent": "And 6000 genes.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now if we look at the theory, the effect is even worse.",
                    "label": 0
                },
                {
                    "sent": "Look at this guy up here.",
                    "label": 0
                },
                {
                    "sent": "He's predicted variance is less than one percent.",
                    "label": 0
                },
                {
                    "sent": ".0129, but the actual variance of the sum of the squared distances is .694.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that lesson goes.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "John, here's a valid data set.",
                    "label": 0
                },
                {
                    "sent": "We see the same thing less pronounced than the gene expression, so very high dimensional data.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically, not reflecting what we saw in the theory.",
                    "label": 0
                },
                {
                    "sent": "The situation for real data does not reflect what we expect, So what?",
                    "label": 1
                },
                {
                    "sent": "What's going on?",
                    "label": 0
                },
                {
                    "sent": "Why does real data not conform to maths?",
                    "label": 0
                },
                {
                    "sent": "It's very naughty.",
                    "label": 0
                },
                {
                    "sent": "OK, something's gone wrong in our assumptions about what's going on in the mass.",
                    "label": 0
                },
                {
                    "sent": "So let's look at P being 1000, again thousand dimension.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dataset artificial.",
                    "label": 0
                },
                {
                    "sent": "But we get a similar effect.",
                    "label": 0
                },
                {
                    "sent": "This is a Gaussian with P being 1000.",
                    "label": 1
                },
                {
                    "sent": "But I've given this Gaussian a very specific low rank covariance where W. Is 1000 by 2.",
                    "label": 1
                },
                {
                    "sent": "And then I've corrupted it by a little bit of noise.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, sorry, that's a bit low.",
                    "label": 0
                },
                {
                    "sent": "So the form here is the covariance of this.",
                    "label": 0
                },
                {
                    "sent": "Gaussian is WW transpose plus Sigma squared I where is a matrix of 1000 long and two columns.",
                    "label": 0
                },
                {
                    "sent": "So it's basically this is rank 2, the covariance plus some diagonal terms to make it full rank.",
                    "label": 0
                },
                {
                    "sent": "Now I've taken a noise to be .01 and sampled this matrix W from a standard normal.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, Interestingly, if I now apply the theoretical curve assuming a dimensionality of two for the data, the fit is bang on.",
                    "label": 0
                },
                {
                    "sent": "What's going on?",
                    "label": 0
                },
                {
                    "sent": "All our assumptions in that material I gave you at the beginning was that the features are independent.",
                    "label": 0
                },
                {
                    "sent": "If you have 1000 independent features about something, let's consider the case where they are binary.",
                    "label": 0
                },
                {
                    "sent": "1000 independent binary facts about someone that covers 2 to the thousand people.",
                    "label": 0
                },
                {
                    "sent": "The space that it's covering is absolutely enormous.",
                    "label": 0
                },
                {
                    "sent": "If you have that degree of specification on the data, something really weird is happening.",
                    "label": 0
                },
                {
                    "sent": "In practice, real features are massively correlated.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so all this cursor dimensionality stuff is actually rubbish.",
                    "label": 0
                },
                {
                    "sent": "It's rubbish from the fact of when you're thinking about the data.",
                    "label": 0
                },
                {
                    "sent": "If it was true in your data, there would be no information in your data.",
                    "label": 0
                },
                {
                    "sent": "It's what we call noise.",
                    "label": 0
                },
                {
                    "sent": "The only thing we assume full independence for across all data points and across all features is noise.",
                    "label": 0
                },
                {
                    "sent": "What we do in practice is we assume underlying regularity's in our data, particularly, I mean for features and everything else.",
                    "label": 0
                },
                {
                    "sent": "If we didn't, we would not be able to learn from the data.",
                    "label": 0
                },
                {
                    "sent": "The worst case predictions from learning theory would apply, but in practice they don't because real data is correlated like this.",
                    "label": 0
                },
                {
                    "sent": "This is a simple correlation and we're going to talk about this correlation first.",
                    "label": 0
                },
                {
                    "sent": "How you fit models that assume this correlation?",
                    "label": 0
                },
                {
                    "sent": "There's not an invalid lesson, though it doesn't mean that you don't have to worry about the curse dimensionality because it's actually associated with your model, not your data.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "We would sampling from a model that had this independence characteristic.",
                    "label": 0
                },
                {
                    "sent": "If you fit such a model that has this independence characteristic, your model is covering a vast space of data.",
                    "label": 0
                },
                {
                    "sent": "It's going to be difficult to extract information from the data, so there's something wrong with models like that, but I think it's endemic in machine learning and statistics.",
                    "label": 0
                },
                {
                    "sent": "To claim this is a characteristic of the space, it's a characteristic of the model plus the space.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's the worst case thing that could happen in the space.",
                    "label": 0
                },
                {
                    "sent": "But then if someone brought you that data which has that nasty characteristic.",
                    "label": 0
                },
                {
                    "sent": "You really want to be asking them what rubbish have you just given me.",
                    "label": 0
                },
                {
                    "sent": "There's no information in this data because there really isn't.",
                    "label": 0
                },
                {
                    "sent": "That's what these lessons are about.",
                    "label": 0
                },
                {
                    "sent": "If this information is this regularity in the data that you can extract, those things won't be true.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "One thing I hope to do is put some of the.",
                    "label": 0
                },
                {
                    "sent": "I mean the curse of dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Ooh, I mean, I just think it's a large sense.",
                    "label": 0
                },
                {
                    "sent": "It's a myth.",
                    "label": 0
                },
                {
                    "sent": "Alot of the people things people extract from it Bernard touched on it earlier saying you shouldn't expand things into high dimensions, that's what the statisticians say because it's a bad idea, right?",
                    "label": 0
                },
                {
                    "sent": "Making your feature size Infinity?",
                    "label": 0
                },
                {
                    "sent": "That's always going to lead to problems no no it's not.",
                    "label": 0
                },
                {
                    "sent": "Because of the structure of the model because of the capacity control.",
                    "label": 0
                },
                {
                    "sent": "That's the way Bernard was talking about it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm saying the same thing.",
                    "label": 0
                },
                {
                    "sent": "It's about your model.",
                    "label": 0
                },
                {
                    "sent": "Yeah, not about your your data set size.",
                    "label": 0
                },
                {
                    "sent": "It's a very bizarre thing.",
                    "label": 0
                },
                {
                    "sent": "If you think about it for someone to come to you, you're going to model in someone's bought you.",
                    "label": 0
                },
                {
                    "sent": "Here's your client.",
                    "label": 0
                },
                {
                    "sent": "He's walked into the room and he says there's the data set.",
                    "label": 0
                },
                {
                    "sent": "I found out 100 things about these 50 people.",
                    "label": 0
                },
                {
                    "sent": "Please can you fit your model like you think?",
                    "label": 0
                },
                {
                    "sent": "OK, that's difficult because there's P is large and N is smallish.",
                    "label": 0
                },
                {
                    "sent": "But I'll try and fit a model and then the guy goes.",
                    "label": 0
                },
                {
                    "sent": "Oh, I just heard I can tell you 100 more things about those people.",
                    "label": 0
                },
                {
                    "sent": "Would you like to know?",
                    "label": 0
                },
                {
                    "sent": "And then you said no don't tell me because then I'll have an even larger P. I don't want to know anymore 'cause it will mess with me.",
                    "label": 0
                },
                {
                    "sent": "The data is worse.",
                    "label": 0
                },
                {
                    "sent": "How can that be true?",
                    "label": 0
                },
                {
                    "sent": "'cause it's not true?",
                    "label": 0
                },
                {
                    "sent": "Those hundred things will not be independent of the previous hundred things.",
                    "label": 0
                },
                {
                    "sent": "You'll get a richer model of the person, so and so and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, they'll be more stuff about that later on.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so where does this low rank covariance come from?",
                    "label": 1
                },
                {
                    "sent": "It comes from a low dimensional approximation for the data set.",
                    "label": 1
                },
                {
                    "sent": "That's the connection with dimensionality reduction, a simple low dimensional approximation and linear low dimensional approximation.",
                    "label": 0
                },
                {
                    "sent": "If we made this C WW transpose plus DI made D Sigma squared I before then that would be known as factor analysis for my used is known as probabilistic PCA.",
                    "label": 0
                },
                {
                    "sent": "OK, probabilistic principal component analysis and it's a linear mapping from a Q dimensional latent space.",
                    "label": 0
                },
                {
                    "sent": "Because I'm sort of build probabilistic models, I think of the low dimensional thing as latent variables and observe things to a P dimensional.",
                    "label": 0
                },
                {
                    "sent": "Dataspace.",
                    "label": 1
                },
                {
                    "sent": "And then we corrupt that mapping.",
                    "label": 0
                },
                {
                    "sent": "So we do a linear mapping from that and then we corrupted by independent Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "That noise is high dimensional.",
                    "label": 0
                },
                {
                    "sent": "That is something we can't.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's almost like it's one way of thinking about noise.",
                    "label": 0
                },
                {
                    "sent": "It's the nasty difficult stuff that we can't extract information from.",
                    "label": 0
                },
                {
                    "sent": "The earliest people who define noise were people like Laplace and Gauss who were looking at planetary motions through the atmosphere and in their telescopes.",
                    "label": 0
                },
                {
                    "sent": "And they said these are things that can't account for.",
                    "label": 0
                },
                {
                    "sent": "Let's assume they are independent random things being added to each.",
                    "label": 0
                },
                {
                    "sent": "Observation and there are instructions that something that they can't deal with in the structure structure.",
                    "label": 0
                },
                {
                    "sent": "Noise is very interesting, but it's not something we're assuming here.",
                    "label": 0
                },
                {
                    "sent": "And then what we do is we marginalized these latent variables using a Gaussian prior.",
                    "label": 0
                },
                {
                    "sent": "That's how we do probabilistic PCA.",
                    "label": 0
                },
                {
                    "sent": "So just to remind.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the notation.",
                    "label": 0
                },
                {
                    "sent": "So Q dimensional latent space, P dimensional data space and the number of data points.",
                    "label": 1
                },
                {
                    "sent": "Why is our data in this end by P matrix that's important because it turns out if I write YY transpose, that's an inner product matrix.",
                    "label": 0
                },
                {
                    "sent": "Center data is like our data with the mean subtracted off, so that's something we'll use a lot, and then the latent variables are X, and then we're going to use this mapping matrix Maps from AP dimension to a Q dimensional data space.",
                    "label": 0
                },
                {
                    "sent": "So if.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In effect, when I go from OK, so just about the notation, why am I making a fuss of this notation?",
                    "label": 0
                },
                {
                    "sent": "Because when you see this form arising, the censored data set in this form, that's a covariance, yeah?",
                    "label": 0
                },
                {
                    "sent": "So I sometimes write that is S so that is equal to that and then if you see this inner product matrix.",
                    "label": 1
                },
                {
                    "sent": "This why?",
                    "label": 0
                },
                {
                    "sent": "Why transpose?",
                    "label": 0
                },
                {
                    "sent": "That's an inner product matrix which has this interpretation as a linear kernel sometimes, so those two features of the way I've defined the day.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is how we're going to model the data set.",
                    "label": 0
                },
                {
                    "sent": "So we find the lower dimensional plane embedded in high dimensional space.",
                    "label": 1
                },
                {
                    "sent": "The player is described by this matrix W, which is a.",
                    "label": 0
                },
                {
                    "sent": "In that space there.",
                    "label": 0
                },
                {
                    "sent": "So we go from this low dimensional Space 2 dimensional in this case to a 3 dimensional space by multiplying the vector X by W, which gives us a 3 dimensional response and adding a mean to it.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "If we also add noise then we have this sort of form.",
                    "label": 0
                },
                {
                    "sent": "So for each data point we are assuming the relationship between the data and the latent variable is given by this equation.",
                    "label": 1
                },
                {
                    "sent": "Now, the way we write that probabilistically.",
                    "label": 0
                },
                {
                    "sent": "Is a Gaussian density.",
                    "label": 0
                },
                {
                    "sent": "If this is Gaussian distributed.",
                    "label": 0
                },
                {
                    "sent": "So if this is Gaussian distributed with variant Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "So this is that nasty high dimensional thing.",
                    "label": 0
                },
                {
                    "sent": "It's independent across features.",
                    "label": 0
                },
                {
                    "sent": "Then what we get is.",
                    "label": 1
                },
                {
                    "sent": "The probability of Y given X has a mean well independent across data points, we assume, and it has a mean given by WXI plus mu.",
                    "label": 0
                },
                {
                    "sent": "And then it's corrupted by some noise of covariance Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Now these X are things we don't know, so the parameters Now graph RW I. I know there's a big tendency to use graphical models, a lot of machine learning.",
                    "label": 0
                },
                {
                    "sent": "I used to like it too, but now I don't know if it's something with age.",
                    "label": 0
                },
                {
                    "sent": "I just look at them and I can't understand them because there's lots of circles and plates and things and I find it hard to see what the conditional in dependencies are.",
                    "label": 0
                },
                {
                    "sent": "But this is an easy one.",
                    "label": 0
                },
                {
                    "sent": "OK, so why is dependent on X&W?",
                    "label": 1
                },
                {
                    "sent": "It also depends on you, but I didn't include it in the graph 'cause I didn't want to confuse you.",
                    "label": 0
                },
                {
                    "sent": "So the latent variable approach is that these acts are nuisance variables and we want to get rid of them 'cause we don't know what they are.",
                    "label": 0
                },
                {
                    "sent": "We don't know what the underlying causes are.",
                    "label": 0
                },
                {
                    "sent": "I mean historically principle component analysis was invented by who anyone?",
                    "label": 0
                },
                {
                    "sent": "Hotel in good good man.",
                    "label": 0
                },
                {
                    "sent": "If someone said Pearson they would have been wrong.",
                    "label": 0
                },
                {
                    "sent": "Pearson invented the same algorithm but for something different and these two ideas are being conflated.",
                    "label": 0
                },
                {
                    "sent": "I want to deflate them.",
                    "label": 0
                },
                {
                    "sent": "I want to inflate them.",
                    "label": 0
                },
                {
                    "sent": "I want to play them.",
                    "label": 0
                },
                {
                    "sent": "Hotel in was following the ideas in uh, in psychology at the time, which were known as factor analysis.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so her telling was trying to put on what he thought was a sound mathematical basis factor analysis and this is 1930s.",
                    "label": 0
                },
                {
                    "sent": "I think 33 or 36 and his paper is really clear if you read it today and he thinks of principle component analysis 100% in this way as you've got some latent factors, which was Bernard saying that you had these intelligence tests, what comes next in the series?",
                    "label": 0
                },
                {
                    "sent": "Those are the sort of things they were scoring and then they try and workout the latent causes that you're a psychopath and your IQ is 5.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so those are the X is.",
                    "label": 0
                },
                {
                    "sent": "The underlying variables in psychology this was very important and hotel in came along and said this is very nice what you're doing, but here's a mathematical way of doing it.",
                    "label": 0
                },
                {
                    "sent": "I don't want to call it factor analysis because factor means something else in maths, so I'm going to call it principle component analysis, But it's inspired by the social Sciences and he thinks of these types of models.",
                    "label": 1
                },
                {
                    "sent": "So principle components is another type of factor analysis and I'll still explain it.",
                    "label": 0
                },
                {
                    "sent": "I think it's a shame he changed the name because.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, as you'll see from the mass.",
                    "label": 0
                },
                {
                    "sent": "So we define a Gaussian prior over this latent.",
                    "label": 0
                },
                {
                    "sent": "SpaceX and this is what.",
                    "label": 0
                },
                {
                    "sent": "Oddly, although this is what hotel ING and?",
                    "label": 0
                },
                {
                    "sent": "Did describe he only looked at the noiseless case, in effect now it took till 1996 when the maximum likelihood solution for the noisy case was given by tipping and Bishop and roll ice.",
                    "label": 0
                },
                {
                    "sent": "Also proposed model around the same time 9690 seven, 98 their papers come out different time but it was all contemporaneous.",
                    "label": 0
                },
                {
                    "sent": "So if you've got this prior distribution what you do is you use integration.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To get rid of it.",
                    "label": 0
                },
                {
                    "sent": "So we take the prior, we multiply it by the likelihood and we integrate over X and we get this marginal likelihood, which is the form of the Gaussian we used to sample from.",
                    "label": 0
                },
                {
                    "sent": "So it's just a Gaussian with a mean of mu Anna covariance, which has that structure.",
                    "label": 0
                },
                {
                    "sent": "Now remember what I said in the beginning?",
                    "label": 0
                },
                {
                    "sent": "Be aware of Gaussians, but Gaussians with structure are good.",
                    "label": 0
                },
                {
                    "sent": "In fact, that's all I do.",
                    "label": 0
                },
                {
                    "sent": "Gaussians with structured covariance is different.",
                    "label": 0
                },
                {
                    "sent": "Structures in the covariance, and this structure is a low rank plus a diagonal.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now you need to optimize the likelihood of this.",
                    "label": 0
                },
                {
                    "sent": "So way we indicate that as a graph is we remove the marginalized variable and there it is.",
                    "label": 0
                },
                {
                    "sent": "Now, why do I think it's a shame he changed the name from principle component analysis to from factor analysis to principle component analysis?",
                    "label": 0
                },
                {
                    "sent": "Because it is a factorization of your data.",
                    "label": 0
                },
                {
                    "sent": "It's factorization of covariance.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "WW transpose is a factorization, so even though that wasn't factor in the sense the psychologist meant, the actual underlying algorithm has this factorization of the covariance in it, yeah?",
                    "label": 0
                },
                {
                    "sent": "We'll see what factorization that is in a moment.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to assume that you know that fitting the mean of a Gaussian is given by, well, the mean of the Gaussian is given by the sample mean, and so I'm going to do that.",
                    "label": 0
                },
                {
                    "sent": "Let's me fitting the mean OK.",
                    "label": 0
                },
                {
                    "sent": "So I'm centering my data.",
                    "label": 0
                },
                {
                    "sent": "So common trick you can do it if the noise is constant, you can't do it.",
                    "label": 0
                },
                {
                    "sent": "If the noise is varying or if the student T noise or something like that for Gaussian noise, constant variance, Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "You can do that trick, so we'll.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dielectric generally, So what tipping and Bishop showed the papers.",
                    "label": 0
                },
                {
                    "sent": "99 but the work was earlier, takes awhile to publish in.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's generals is that the maximum likelihood solution is given by.",
                    "label": 0
                },
                {
                    "sent": "This form here, so if sees that covariance, then the log likelihood is given like this.",
                    "label": 0
                },
                {
                    "sent": "So we rewrite the form of.",
                    "label": 0
                },
                {
                    "sent": "This is a way you can rewrite the argument of the exponential in the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "You can turn it into this trace times the inverse covariance times Y, transpose Y, and the reason for doing that is that that guy there is the covariance.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that the maximum likelihood solution is given by.",
                    "label": 0
                },
                {
                    "sent": "The eigenvalues that covariance, so let's just go through a little bit on the details for that.",
                    "label": 0
                },
                {
                    "sent": "So how do you solve this sort of thing where you just take the gradient of the log likelihood?",
                    "label": 0
                },
                {
                    "sent": "And you do matrix derivatives so the gradient of the log likelihood is given by this form.",
                    "label": 0
                },
                {
                    "sent": "Where the covariance is appearing in the sample covariance is appearing in signing the covariance of the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "In a normal case, this W wouldn't be there.",
                    "label": 0
                },
                {
                    "sent": "You would be taking the gradients with respect to see and you just recover that the covariance of the Gaussian is equal to the sample covariance.",
                    "label": 0
                },
                {
                    "sent": "But here, because we've got this W constraining the covariance, it appears on this side.",
                    "label": 0
                },
                {
                    "sent": "Of each of these C minus ones.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Oh, so I don't know how that extra slide came in.",
                    "label": 0
                },
                {
                    "sent": "If we pre multiply that, what we're going to do is look for fixed point of that.",
                    "label": 0
                },
                {
                    "sent": "So we set that to zero and then we can pre multiply it by two C which gives us the W is equal to this form.",
                    "label": 0
                },
                {
                    "sent": "Bringing that on the other side and moving the minus an bringing the end by the covariance.",
                    "label": 0
                },
                {
                    "sent": "So that is now the sample covariance.",
                    "label": 0
                },
                {
                    "sent": "Yeah 'cause it's 1 / N. Now this you still can't solve and the trick that might tipping notice and this is important.",
                    "label": 0
                },
                {
                    "sent": "I want you to know that this trick is in there because it's what prevents factor analysis being solved in this way.",
                    "label": 0
                },
                {
                    "sent": "There's a fixed point solution for principle component analysis.",
                    "label": 0
                },
                {
                    "sent": "And there's no single one set fixed point solution for factor analysis.",
                    "label": 0
                },
                {
                    "sent": "It's an iterative algorithm factor analysis, whereas PCA is a one step.",
                    "label": 0
                },
                {
                    "sent": "So you substitute W with it's singular value decomposition, so I've written that in ULR transpose, so as a rotation matrix, and it's going to turn out that you can't identify this rotation matrix for PCA.",
                    "label": 0
                },
                {
                    "sent": "So that would imply if we substitute that in the covariance and This is why you can't identify R is WW, transpose is ULR transpose R which is 'cause it's a rotation matrix of square rotation matrix just becomes the identity.",
                    "label": 0
                },
                {
                    "sent": "So you get UL squared U transpose.",
                    "label": 0
                },
                {
                    "sent": "You can then use the matrix inversion lemma to show that this is the case.",
                    "label": 0
                },
                {
                    "sent": "The inverse of the model covariance times W is equal to this form that's being expressed in terms of re expressing this W and the C -- 1 in terms of this eigenvalue decomposition.",
                    "label": 0
                },
                {
                    "sent": "And once you've got that guy that allows you to substitute into the original data and show that the solution the maximum likelihood solution is given by this sample covariance times UE being equal to U times this Sigma squared plus L squared, where L ^2.",
                    "label": 0
                },
                {
                    "sent": "Where those singular values from the singular value decomposition.",
                    "label": 0
                },
                {
                    "sent": "Now that is recognized as an eigenvalue problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and it's an eigenvalue problem on the covariance of the data.",
                    "label": 0
                },
                {
                    "sent": "So I've moved through that quite quickly, but the key point here is you can't express this guy for factor analysis.",
                    "label": 0
                },
                {
                    "sent": "You can't do this rewrite.",
                    "label": 0
                },
                {
                    "sent": "You can try it at home.",
                    "label": 0
                },
                {
                    "sent": "The reason is because this instead of being plus Sigma squared I it's plus D times I, so the effect that has.",
                    "label": 0
                },
                {
                    "sent": "See if I can do this.",
                    "label": 0
                },
                {
                    "sent": "I forgot to plan this bit so.",
                    "label": 0
                },
                {
                    "sent": "If I mess it up.",
                    "label": 0
                },
                {
                    "sent": "So if you've got WW transpose plus Sigma squared I.",
                    "label": 0
                },
                {
                    "sent": "Versus WW transpose plus D, so that's factor analysis.",
                    "label": 0
                },
                {
                    "sent": "That's PCA.",
                    "label": 0
                },
                {
                    "sent": "You think it was a lot more complicated than that?",
                    "label": 0
                },
                {
                    "sent": "If you read a lot of text on it, but that's basically it, that's diagonal.",
                    "label": 0
                },
                {
                    "sent": "And you want to fit all these things well when you do the single value decomposition you so I had it as L deny U L ^2.",
                    "label": 0
                },
                {
                    "sent": "You transpose plus Sigma squared I.",
                    "label": 0
                },
                {
                    "sent": "So the trick that's often used here is that this is also true.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So that works, but this doesn't.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So it's annoying, isn't it?",
                    "label": 0
                },
                {
                    "sent": "I mean, how little simple things cause problems?",
                    "label": 0
                },
                {
                    "sent": "You can't write this.",
                    "label": 0
                },
                {
                    "sent": "So this is now Sigma.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's full covariance.",
                    "label": 0
                },
                {
                    "sent": "This guy here is full covariance.",
                    "label": 0
                },
                {
                    "sent": "In order to write it in that way.",
                    "label": 0
                },
                {
                    "sent": "Because you rotated in one OK another way.",
                    "label": 0
                },
                {
                    "sent": "What does this mean physically?",
                    "label": 0
                },
                {
                    "sent": "What are eigenvalues?",
                    "label": 0
                },
                {
                    "sent": "Eigenvalues are simply the.",
                    "label": 0
                },
                {
                    "sent": "Axis aligned so the Gaussian noise of the factor analysis is not axis aligned.",
                    "label": 0
                },
                {
                    "sent": "Sorry is actually aligned and the Gaussian noise for PCA.",
                    "label": 0
                },
                {
                    "sent": "Is spherical like that?",
                    "label": 0
                },
                {
                    "sent": "So this is factor analysis and here you've got one Sigma.",
                    "label": 0
                },
                {
                    "sent": "I squared Sigma one squared and you've got Sigma 2 squared.",
                    "label": 0
                },
                {
                    "sent": "Yeah, different in different dimensions.",
                    "label": 0
                },
                {
                    "sent": "PCA is circular.",
                    "label": 0
                },
                {
                    "sent": "What happens if you rotate a circle?",
                    "label": 0
                },
                {
                    "sent": "Yeah, nice isn't it?",
                    "label": 0
                },
                {
                    "sent": "What happens if you rotate an Oval?",
                    "label": 0
                },
                {
                    "sent": "It develops into a full covariance Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the fact that that problem exists.",
                    "label": 0
                },
                {
                    "sent": "This trick of being able to rotate a circle and it still you can't change.",
                    "label": 0
                },
                {
                    "sent": "You don't change the nature of the spherical noise if you rotate it.",
                    "label": 0
                },
                {
                    "sent": "That's why you can do this nice simple eigenvalue decomposition on the.",
                    "label": 0
                },
                {
                    "sent": "Principal component if you can't do it for factor analysis.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "So that leads to this eigenvalue decomposition.",
                    "label": 0
                },
                {
                    "sent": "And actually, while we're at it, since I've got the.",
                    "label": 0
                },
                {
                    "sent": "What do I interpret this as?",
                    "label": 0
                },
                {
                    "sent": "Well, what's going on is you've got.",
                    "label": 0
                },
                {
                    "sent": "Your covariance of your data.",
                    "label": 0
                },
                {
                    "sent": "What you're trying to find is the principle directions you're trying to find.",
                    "label": 0
                },
                {
                    "sent": "You're trying to.",
                    "label": 0
                },
                {
                    "sent": "You is the directions that will allow you to access align this data set.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is like you won.",
                    "label": 0
                },
                {
                    "sent": "And this is like you 2.",
                    "label": 0
                },
                {
                    "sent": "So that's what you're looking for.",
                    "label": 0
                },
                {
                    "sent": "It said this, covariance the sample covariance from your data is given by this circle here.",
                    "label": 0
                },
                {
                    "sent": "And then you do an eigen decomposition on that which gives you the principle axes.",
                    "label": 0
                },
                {
                    "sent": "Prince Max is comes from principal component analysis of that covariance.",
                    "label": 0
                },
                {
                    "sent": "Now the rotational invariances very interesting because the rotational invariant we had before remember it was ULR transpose, RLU transpose, right, right, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so these things convert to WI.",
                    "label": 0
                },
                {
                    "sent": "Should have said that's you because it's actually W 1 because it's multiplied by its eigenvalue to give it its length.",
                    "label": 0
                },
                {
                    "sent": "You gives the direction like that and the length is given by the eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "This rotation guy.",
                    "label": 0
                },
                {
                    "sent": "What he does is things like this.",
                    "label": 0
                },
                {
                    "sent": "All these are valid solutions for W. Yeah, so these two I should have different colors.",
                    "label": 0
                },
                {
                    "sent": "They shouldn't look.",
                    "label": 0
                },
                {
                    "sent": "So let me try that.",
                    "label": 0
                },
                {
                    "sent": "These two would be one set of solutions and these two would be another set of solutions.",
                    "label": 0
                },
                {
                    "sent": "They rotated versions of the orthogonal set, so we tend to use the orthogonal set they're known to be orthogonal because in the eigenvalue decomposition that's a set of what I think of as orthonormal vectors, and that's a set of scales.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so these guys must be at right angles to each other.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "So hopefully that's given some of the intuitions I have, or I think of myself about principle component analysis, but just to finish what's going on in the solution.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You are the eigenvectors of the covariance and Sigma squared plus L squared are the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Now in traditional principle component analysis you just need L squared Sigma.",
                    "label": 0
                },
                {
                    "sent": "Squared is taken to zero, that's the noise.",
                    "label": 0
                },
                {
                    "sent": "That is, in hotel is work.",
                    "label": 0
                },
                {
                    "sent": "The noise was considered at 0.",
                    "label": 0
                },
                {
                    "sent": "But in Prince probabilistic principal component alloces you add this Sigma squared onto the eigen values.",
                    "label": 0
                },
                {
                    "sent": "Now what's that saying?",
                    "label": 0
                },
                {
                    "sent": "Is that means the you're constantly adding to your data.",
                    "label": 0
                },
                {
                    "sent": "A sort of your smudge in it so.",
                    "label": 0
                },
                {
                    "sent": "If we were doing 1 dimensional principle component analysis like this.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're looking for the first principle direction.",
                    "label": 0
                },
                {
                    "sent": "Then when you add the noise to it, what you actually do is you add a second component to it.",
                    "label": 0
                },
                {
                    "sent": "A little bit of noise out in that direction and you add a little bit on the end as well.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the eigenvalues you get in principle, component analysis, probabilistic principal component analysis, are slightly different from the ones you were getting.",
                    "label": 0
                },
                {
                    "sent": "If you get PCA because you add this little bit of noise at the end, but you also it means it's a proper probabilistic setup, because if you just want to take the vector WW transpose the one dimensional vector, that's not a valid covariance, it just defines well, it's semi definite covariance.",
                    "label": 0
                },
                {
                    "sent": "It defines a line.",
                    "label": 0
                },
                {
                    "sent": "So adding the little bit of noise across the diagonal adds a little bit of there and a little bit there, so you end up having to subtract that off.",
                    "label": 0
                },
                {
                    "sent": "In your solution for PCA that comes out of the maths, but that's my intuition.",
                    "label": 0
                },
                {
                    "sent": "The mass I think you can see in the papers.",
                    "label": 0
                },
                {
                    "sent": "I tried to give you intuition because that's what it took me longer to workout, so further manipulation shows that we can constrain this.",
                    "label": 0
                },
                {
                    "sent": "Then the solution is given by the largest eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "What you don't see here is that the eigenvalues you need to retain.",
                    "label": 0
                },
                {
                    "sent": "If you're reducing the dimension of, you are the largest ones, but that's you can show that as well.",
                    "label": 0
                },
                {
                    "sent": "So the Q largest eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So some further work shows the principle eigenvectors and then it turns out you can also workout.",
                    "label": 0
                },
                {
                    "sent": "This requires quite a lot of further manipulation that the maximum likelihood value for Sigma squared is given by the average of the discarded eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you want to know what Sigma squared is, you have to take the average of the discarded eigen values and that's Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "So this is some intuition people have always had about PCA that somehow they look for these elbows.",
                    "label": 0
                },
                {
                    "sent": "People talk about elbows finding dimensionality.",
                    "label": 0
                },
                {
                    "sent": "These are an amazing thing that don't actually exist in real data, But the idea is if you do PCA, you look at the eigen values along this axis here.",
                    "label": 0
                },
                {
                    "sent": "And you see some sort of elbow like that, so the eigenvalues go down, and then this is the elbow, the mythical elbow.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that this is the noise level and then you retain any of those components.",
                    "label": 0
                },
                {
                    "sent": "OK, so PCA is making this more explicit if you really took the.",
                    "label": 0
                },
                {
                    "sent": "I can decomposition of that matrix form WW transpose plus Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "I you really would see this.",
                    "label": 0
                },
                {
                    "sent": "This would be Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and there would be a constant eigenvalue and this would be the dimensionality queue of these matrices here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it's encoded in this model that's being fitted to the data.",
                    "label": 0
                },
                {
                    "sent": "But in fact, if you too if you try and fit the number of components, it will always say that everything is a component.",
                    "label": 0
                },
                {
                    "sent": "The maximum likelihood is found by including all components.",
                    "label": 0
                },
                {
                    "sent": "There's no magical fit where it will find this noise for you, so you either have to do one of two things.",
                    "label": 0
                },
                {
                    "sent": "One is the classic thing we all know and love, which is to select the latent dimensionality Q and then fit Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "It's the average of the discarded eigenvalues, but the other cool thing you can do, which people often don't understand with PCA 'cause they may not understand probabilistic PCA, is you can set Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "To sum known value, let's call it A and if you do that then that will determine the dimensionality of the latent space for you automatically.",
                    "label": 0
                },
                {
                    "sent": "Fact what it turns out is.",
                    "label": 0
                },
                {
                    "sent": "Bear in mind this so this is UQ.",
                    "label": 0
                },
                {
                    "sent": "The first Q eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Over the sample covariance elyza Q by Q matrix is given by the 1st Q eigenvalues minus Sigma squared I.",
                    "label": 0
                },
                {
                    "sent": "If I set Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "To some value a then this will be complex.",
                    "label": 0
                },
                {
                    "sent": "For any.",
                    "label": 0
                },
                {
                    "sent": "Eigenvalue which is smaller than a.",
                    "label": 0
                },
                {
                    "sent": "That doesn't mean there's a complex solution.",
                    "label": 0
                },
                {
                    "sent": "What you can prove is that you should discard all dimensions who have a smaller eigenvalue than that of Sigma.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in that case it's somehow saying that the physical analogy of that is that your data looks like.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "It's only covariance that looks like that you're setting the variance, the noise variance, something like that larger than that eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "And it will say no, there's only one dimension in this data.",
                    "label": 0
                },
                {
                    "sent": "It will switch off.",
                    "label": 0
                },
                {
                    "sent": "That dimension which is smaller in standard deviation than your noise variance.",
                    "label": 0
                },
                {
                    "sent": "So that's something you can do, and we ourselves use that to effect in a model where we estimated the noise level by separate means on gene expression data.",
                    "label": 0
                },
                {
                    "sent": "So we were doing PCA on gene expression data and we had a way of estimating the noise in each gene expression measurement, and then that means that the dimensionality of our PCA was automatically determined.",
                    "label": 0
                },
                {
                    "sent": "So if you sort of aware of that, that can be a useful trick.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's principle component analysis and those datasets I mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to sort of quickly show you PCA on those different data set, and then we'll do a 5 minute break.",
                    "label": 0
                },
                {
                    "sent": "So this is that stick man walking.",
                    "label": 0
                },
                {
                    "sent": "I told you he did three strides.",
                    "label": 0
                },
                {
                    "sent": "He actually starts from a strange pose.",
                    "label": 0
                },
                {
                    "sent": "He starts from this pose here, and it's actually if you look at the third principle component, you'll see that disappearing off into the distance.",
                    "label": 0
                },
                {
                    "sent": "So we're just looking at the first 2, the first 2 represent the strides so.",
                    "label": 0
                },
                {
                    "sent": "These circles here.",
                    "label": 0
                },
                {
                    "sent": "He's actually changing the angle of its run.",
                    "label": 0
                },
                {
                    "sent": "That's why they don't lie on top of each other.",
                    "label": 0
                },
                {
                    "sent": "The data is set so he runs in place.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I removed the mean so he just runs in place otherwise it would just be a long stream of data.",
                    "label": 0
                },
                {
                    "sent": "And what you can see is basically that because of the changing angle of run, the different strides don't quite align on top of each other.",
                    "label": 0
                },
                {
                    "sent": "I'll show you this data set in the next session as well.",
                    "label": 0
                },
                {
                    "sent": "I probably should have shown you this one, but I forgot to do that.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is that all data and these are the model doesn't know the different flow regimes, But this is the homogeneous flow regime, the blue.",
                    "label": 0
                },
                {
                    "sent": "And these are one of these is annular and one of these is.",
                    "label": 0
                },
                {
                    "sent": "Yeah lamb layered stratified.",
                    "label": 0
                },
                {
                    "sent": "Yeah so they look very similar because actually the readings the reflections that occur in the readings of the surfaces are kind of the same.",
                    "label": 0
                },
                {
                    "sent": "Here you get lots of little reflections of little bubbles so they tend to live in these different regimes.",
                    "label": 0
                },
                {
                    "sent": "So you can't quite separate the blue under here.",
                    "label": 0
                },
                {
                    "sent": "You don't quite separate.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With two principal components, this is data.",
                    "label": 0
                },
                {
                    "sent": "This was the Val data that I didn't talk much about.",
                    "label": 0
                },
                {
                    "sent": "How I've separated these is into men and women, so red crosses are men.",
                    "label": 0
                },
                {
                    "sent": "Green circles are women.",
                    "label": 0
                },
                {
                    "sent": "This is data.",
                    "label": 0
                },
                {
                    "sent": "I used a tutorial interspeech, so I actually had some of the names.",
                    "label": 0
                },
                {
                    "sent": "Here are people who are known in their community 'cause it's speech people, but I also labeled there's these three accents.",
                    "label": 0
                },
                {
                    "sent": "Here are all South Yorkshire accents there from our admin staff in Sheffield and they sort of come closest together.",
                    "label": 0
                },
                {
                    "sent": "Some of the undergraduates are close to the mothers are some far away, so you see there there's some correlations and structure in the data.",
                    "label": 0
                },
                {
                    "sent": "This is me here.",
                    "label": 0
                },
                {
                    "sent": "I'm worryingly close to the women.",
                    "label": 0
                },
                {
                    "sent": "But that's 'cause PC's are limited algorithm really.",
                    "label": 0
                },
                {
                    "sent": "My voice is very masculine, it just didn't pick that up.",
                    "label": 0
                },
                {
                    "sent": "And John Barker, who I gave that tutorial into speech with his over here.",
                    "label": 0
                },
                {
                    "sent": "So we are apparently very close together, so this is based on the valves.",
                    "label": 0
                },
                {
                    "sent": "We make it some it's a hidden Markov model fit to our speech and then I just took the valves and the parameters of the hidden Markov models associated with the valves and doing PCA on that.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the gene expression data.",
                    "label": 1
                },
                {
                    "sent": "I don't know if this should separate or not, but what I did is I label the different time series.",
                    "label": 0
                },
                {
                    "sent": "I told you it's this yeast cell data and it's some different time series conflated together.",
                    "label": 0
                },
                {
                    "sent": "So each time series I've actually labeled with a different thing.",
                    "label": 0
                },
                {
                    "sent": "So this is one of the time series.",
                    "label": 0
                },
                {
                    "sent": "Blue Cross is the purple squares or another time series the red crosses.",
                    "label": 1
                },
                {
                    "sent": "I think aren't even time series, they just one off experiments and the cyan actresses are a separate time series.",
                    "label": 0
                },
                {
                    "sent": "The overlaying a bit but blue seems to separate on two sides and is the structure there.",
                    "label": 0
                },
                {
                    "sent": "I don't know, but it's one way of visualizing it.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so what's the point in the probabilistic approach?",
                    "label": 1
                },
                {
                    "sent": "Why wouldn't we just project with regular PCA?",
                    "label": 1
                },
                {
                    "sent": "So why do probability?",
                    "label": 0
                },
                {
                    "sent": "This is a standard question again again, and this is the standard answer that you always put in your papers.",
                    "label": 0
                },
                {
                    "sent": "It's easy to extend to mixture models, so instead of doing mixtures of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "The first thing you should be doing is either mixtures of PCA or mix is a factor analyzers.",
                    "label": 0
                },
                {
                    "sent": "If you've got a lot of data and you do a lot of restarts, you'll find that makes it a factor, analyzes proposed by Zubin and Matt Beal sometime ago, is very good.",
                    "label": 0
                },
                {
                    "sent": "The Bayesian mixture factor analyzes that they use which also learn so mentality is a good model.",
                    "label": 0
                },
                {
                    "sent": "And Mike Typings was really good at fitting these mixtures of PCA and you could get some really good density models because what you're doing there.",
                    "label": 0
                },
                {
                    "sent": "Is your lining up little linear manifolds?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so each of these things is you do a mixture model of this.",
                    "label": 0
                },
                {
                    "sent": "If there's enough data it fits these small correlated areas.",
                    "label": 0
                },
                {
                    "sent": "So what you're saying in those cases, and this is a much more reasonable model in your high dimensional space, you've got a prototype and the prototype is corrupted locally in a linear way.",
                    "label": 0
                },
                {
                    "sent": "Yeah, with some dimensionality.",
                    "label": 0
                },
                {
                    "sent": "That's a very reasonable model.",
                    "label": 0
                },
                {
                    "sent": "You may need a lot of data to fit all those local linear patches, and that's the weakness of it, but it's a very good way of modeling data, so that's less than one if you're going to mix the Gaussians in peace in high dimensions, don't.",
                    "label": 1
                },
                {
                    "sent": "Two mixtures of probabilistic PCA or Bayesian factor analysis.",
                    "label": 0
                },
                {
                    "sent": "Excellent standard model for comparison against.",
                    "label": 0
                },
                {
                    "sent": "You can do model selection with Bayesian treatment parameters.",
                    "label": 0
                },
                {
                    "sent": "So learning the dimensionality if this time in the third session, I'm going to show you that for nonlinear dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "You can deal with missing data, so missing data or these all factor analysis as well.",
                    "label": 0
                },
                {
                    "sent": "I want to emphasize that I look at PCA, I was asked and when these guys did this work.",
                    "label": 0
                },
                {
                    "sent": "So I'm Chris Bishop was my PhD supervisor, so I tend to focus on probabilistic PCA, but there's a whole load of parallel work in factor analysis which I want to emphasize exist.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "So here's a oil data.",
                    "label": 1
                },
                {
                    "sent": "In this case, we've removed 10% of the values by missing at random, and you can do various things.",
                    "label": 1
                },
                {
                    "sent": "You can do literally EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "There's a few other things you can do to fill in these missing at random guys, and you get a corruption of the original plot, but you can still see the structure that's 10.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sent missing 20% missing.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "30% missing you can still see the structure.",
                    "label": 0
                },
                {
                    "sent": "That's quite a lot of missing data.",
                    "label": 0
                },
                {
                    "sent": "In fact.",
                    "label": 0
                },
                {
                    "sent": "A probabilistic matrix factorization can just be seen as Bayesian PCA.",
                    "label": 0
                },
                {
                    "sent": "It's exactly the same model structurally, and that's being applied to deal with large datasets of preference data.",
                    "label": 0
                },
                {
                    "sent": "Collaborative filtering, where it's almost all missing data.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you've only got 5 values per person and.",
                    "label": 0
                },
                {
                    "sent": "That's if in effect this model with very large amount of missing data, so probabilistic modeling.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gives you the ability to deal with that.",
                    "label": 0
                },
                {
                    "sent": "Or here's 50% missing.",
                    "label": 0
                },
                {
                    "sent": "Still some of the structure there greens overlapping with red completely, but you can still see these separate islands.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So factor analysis is I've said is the same model but with this noise variances Now D. So each each output now has its own variance.",
                    "label": 0
                },
                {
                    "sent": "But it leads to this problem that you can't optimize this likelihood in one step.",
                    "label": 0
                },
                {
                    "sent": "You have to.",
                    "label": 0
                },
                {
                    "sent": "You can use an EM algorithm, but it's not the fastest way and I did do a little bit of reading about it, it's just a massive field fitting these models.",
                    "label": 0
                },
                {
                    "sent": "I mean, you'd really like a simple paper that just says what the best way is.",
                    "label": 0
                },
                {
                    "sent": "I recommend the thing I read that perhaps made most sense.",
                    "label": 0
                },
                {
                    "sent": "R is the main statistical package.",
                    "label": 0
                },
                {
                    "sent": "So a good way of understanding what the statisticians are doing is finding what command they've put in R. And there's a way of doing factor analysis in R which describes.",
                    "label": 0
                },
                {
                    "sent": "How it's doing it, which doesn't fit easily.",
                    "label": 0
                },
                {
                    "sent": "How you're doing the maximum likelihood, so I can't exactly tell you how they're doing that, but I would have a look at that if you're interested in good factor analysis fits I tend not to do factor analysis, I tend to do PCA as I've said.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Independent component analysis, so it's worth briefly mentioning that, so again, the same linear Gaussian relationship, but now these source distributions are non Gaussian and this has the effect of removing.",
                    "label": 0
                },
                {
                    "sent": "Well, it has the effect of messing up your nice analytical model so you can't anymore right down easily.",
                    "label": 0
                },
                {
                    "sent": "The marginal likelihood and just do the fits.",
                    "label": 0
                },
                {
                    "sent": "That's effect number.",
                    "label": 0
                },
                {
                    "sent": "One effect #2.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two is instead of having a source distribution like this.",
                    "label": 0
                },
                {
                    "sent": "This is like the example where remember earlier I said with Super Gaussian.",
                    "label": 0
                },
                {
                    "sent": "If this is if I said this is 1000 dimensional Gaussian, all those 999 dimensions are being projected on top of the origin here well.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the equivalent plot for a Super Gaussian source.",
                    "label": 0
                },
                {
                    "sent": "Remember I said the same thing can occur.",
                    "label": 0
                },
                {
                    "sent": "Central limit Theorem shows you the distances of the same, but the points will lie along the axis.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is the classical ICA type thing.",
                    "label": 0
                },
                {
                    "sent": "If we were trying to do separation of sound sources there Super Gaussian in nature.",
                    "label": 0
                },
                {
                    "sent": "If you were to plot them, you would see these sort of crosses like this.",
                    "label": 0
                },
                {
                    "sent": "The source distribution they use then has this cross form.",
                    "label": 0
                },
                {
                    "sent": "Now if I rotate it when I rotated a sphere.",
                    "label": 0
                },
                {
                    "sent": "It didn't change that.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, so the density here, when rotated, doesn't change.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this density does a cool thing to do if you're playing.",
                    "label": 0
                },
                {
                    "sent": "If you want to do ICA and you've got some standard ICAO data set which you know has independent components, just do PCA on it, projecting to the latent space, you will see a rotated cross.",
                    "label": 0
                },
                {
                    "sent": "And I see a is just finding that rotation to UN rotate it or.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You will see a rotated square sub.",
                    "label": 0
                },
                {
                    "sent": "Gaussian sources do the same thing.",
                    "label": 0
                },
                {
                    "sent": "So what they do there is in this latent space, the probabilistic interpretation.",
                    "label": 0
                },
                {
                    "sent": "They do other ways of fitting ICA, but the probabilistic interpretation is your fitting.",
                    "label": 0
                },
                {
                    "sent": "Density which is non Gaussian in this latent space.",
                    "label": 0
                },
                {
                    "sent": "To get your independent components out.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's going to be it for the first session, so unlike Bernard, I'm not going to court make you vote on make the lazy ones say yes, I want a break.",
                    "label": 0
                },
                {
                    "sent": "You're all going to have a break.",
                    "label": 0
                },
                {
                    "sent": "Or you can ask me questions and I'll drink some water or something.",
                    "label": 0
                },
                {
                    "sent": "OK so will restart in 5 minutes.",
                    "label": 0
                }
            ]
        }
    }
}