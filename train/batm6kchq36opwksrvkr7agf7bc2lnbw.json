{
    "id": "batm6kchq36opwksrvkr7agf7bc2lnbw",
    "title": "The Infinite Factorial Hidden Markov Model",
    "info": {
        "author": [
            "Jurgen Van Gael, Computer Laboratory, University of Cambridge"
        ],
        "published": "Aug. 4, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/icml08_vangael_ifh/",
    "segmentation": [
        [
            "Thank you very much.",
            "Right, so I'll be talking about the mark of Indian buffet process, an effect or hidden Markov models.",
            "And this is work together with the UK and Zoom in."
        ],
        [
            "Money.",
            "So here's sort of a picture of.",
            "The models that we probably all know.",
            "Very long time ago we we came up with people, came up with a mixture model and that was sort of generalized in for time series Model 2 hidden Markov models and on the other hand there was another kind of generalization in terms of making these into effect more expressive factorial models.",
            "And those two are.",
            "These were combined into factorial hidden Markov models which had more expressiveness than hidden Markov models and a little bit later, when they originally processed were sort of exploited, people came up with infinite mixtures and that leads you not so long ago to developing infinite hidden Markov models.",
            "Another development was the introduction of another non parametric building block which was the ICP which led to non parametric factor models and what we want.",
            "What I want to do in this talk is tell you a little bit about some work we did to complete the square or complete the cube in this picture and we want to talk about a new building block for nonparametric models which we call the mark of Indian buffet process and this will allow us to construct a.",
            "An infinite capacity version of the factorial hidden Markov model."
        ],
        [
            "So why do we want to do this?",
            "We want to build factorial models because of the following.",
            "So these are two applications, part of speech tagging and dialogue segmentation.",
            "Now sometimes we might not just be interested in, well, often these are observed an we want to infer, say, part of speech labels or we get a speech signal and we want to know which speaker was talking when.",
            "Now this is possible with hidden Markov models but some."
        ],
        [
            "Times we want to do more than that and we want to say things or have a more expressive, latent space.",
            "For example, we want to maybe do cello parsing at the same time.",
            "Or we want speakers might be overlapping and we want to say which speaker is talking at which point in time.",
            "But as you can see, you know they might be overlapping, and that might be much harder to model with a hidden Markov model.",
            "So factorial models are interesting objects to study."
        ],
        [
            "I think.",
            "Just to recap and get the notation straight.",
            "This is the actual hidden Markov model and the way this is OK.",
            "There's two components to it.",
            "First, there's a finite number of hidden chains.",
            "Let's say big em of them, and each chain has a variable S which is the latent state variable, which we don't observe.",
            "And we will assume that the chains all start in this dummy one state.",
            "Now the second component of the OH and, by the way, So what will say is that every chain has a transition matrix paisa by J, which the nodes.",
            "What's the probability of moving from state I to stay J in that particular chain?",
            "And there's these transition matrices, and this is a little mistake.",
            "This transition matrices don't have to be the same for all all chains.",
            "Then the second component of the spectral him market model is an observation model and you know this could be.",
            "This could be a wide variety of things, and so we're going to talk about it in the abstract, the observations Y at time T are generated from some.",
            "Likelihood model that's parameterized by some parameters and the states at time T for all the different chains.",
            "So the parameters of this model here are the number of chains the set of transition matrices, and the parameters of our likelihood model."
        ],
        [
            "So what we want to do now is we want to say, well maybe we don't want to specify the number of chains in, you know, fix them, maybe want to learn the more we want to model explicitly model the uncertainty in the number of chains.",
            "So we want to have a model where the number or we want to take this previous small.",
            "This factory in market Mall and take the limit as N goes to Infinity.",
            "Now the strategy that will use for that is the following.",
            "So first of all we'll just consider binary binary change.",
            "So change that are in state zero or one.",
            "And then we'll have a three step procedure.",
            "First of all, will describe a finite model and will choose the priors.",
            "Carefully so that this infinite limit is well defined.",
            "Second step will be to marginalized out these parameters and the parameters will turn out to be the transition matrices.",
            "And finally, we'll take the infinite limit and there."
        ],
        [
            "Be a little bit of algebra involved in that, so here's the finite model that we'll be looking at, so.",
            "For it will be fine one Markov chain as follows, so one single binary Markov chain as follows.",
            "We start in this dummy states a state one and.",
            "Then we'll let the Markov chain follow a dynamics with a transition matrix WM that has the following parametrization.",
            "So there is one parameter ASA BEM, which tells you what's the probability of moving from state zero to state one, and then there's a second parameter beta sub M, which is the probability that you're moving from or staying in state one OK. And because they have to sum up to one, this is.",
            "This transition matrix is fully specified.",
            "Now what we'll do is we'll put prior distributions on these asmbs and will make sure, and this this is critical in the analysis is that the probability of switching from zero to one depends on the number of chains.",
            "And then we also and we have this parameter Alpha and gamma and Delta which will have you know, which allow us to tune the number of chains etc.",
            "So this is 1 chain."
        ],
        [
            "And what we'll do next is, well, we'll add will glue a bunch of these guys together.",
            "And have a big well represented as a matrix.",
            "So this is, you know, each of these guys is 1 binary Markov chain and there's M of those chains and we'll call that matrix X to represent all those latent variables.",
            "And what will then do is marginalized out all these transition matrices and compute the probability of the matrix S given our hyperparameters.",
            "Now there's a bit of algebra involved here, but one thing that I want to point out is that this expression here.",
            "Only depends on C. These number C which I define as well C sub say 01 sub M are the counts of the number of 01 transitions in chain M-00R00 transitions etc.",
            "So once we have this expression, we can."
        ],
        [
            "Think about taking the infinite limit and just naively taking the infinite limit won't work as such.",
            "But what we're going to do is we're going to look at equivalence classes of these latent state sequences.",
            "Biggest and so we did.",
            "We did.",
            "Note that with the square brackets, so once we look at equivalence classes and more in particular, we'll look at left ordered form equivalence classes.",
            "Then we can take the infinite limit of these these.",
            "Matrices or we can take the infinite limit of the expression previous we had previously and will get the following expression.",
            "Now again, this only depends on these numbers C, so the transition counts and this is interesting because.",
            "What that means is that the probability of this matrix only depends on the transition counts means that this distribution over our latent state is mark of exchangeable.",
            "OK, so once now we have this infinite limit and so this this distribution over."
        ],
        [
            "State sequence is well defined and now we can use that as a building block in what we call the infinite factual hidden Markov model.",
            "So if you look at this part here.",
            "The essence and then the Alpha Gamma Delta is in a zombies.",
            "Then this is what we previously defined as the mark of Indian buffet process.",
            "So this is our non parametric prior distribution.",
            "Now by adding parameters which we give a certain prior an an observation model which will again talk in the abstract, it's some distribution F. If we put all these things together then this is what we call the infinite vector or hidden Markov model and I'll give you 2 examples.",
            "That we've you know, concrete examples that we've built and that we've experimented with."
        ],
        [
            "OK, so now the question is how do we do inference in these models?",
            "And this is how we go about it.",
            "Well, first of all, it turns out that this the constructions that the construction that we gave we can we can.",
            "Port the stick breaking construction for the Indian buffet process to our model.",
            "So the I mean I can talk about the technical details, but essentially we have we have a stick breaking construction over for this model and so we can think of the eys as being little sticks here.",
            "So the inference starts as follows.",
            "We have a current sample of latent states here and now.",
            "I've transposed the matrix so time goes down and the number of chains goes to the right.",
            "And let's assume in this case our current sample only has five states.",
            "So then our sampling algorithm works as follows.",
            "First we sample a slice variable uniformly.",
            "Between all between zero and the smallest stick that that's currently used.",
            "And we potentially add some new some new unused chains.",
            "And then we run the forward filtering backwards sampling algorithm on these chains, which essentially means that we can use a dynamic program to re sample the hidden states for our latent chains.",
            "And finally we might have to also re sample the parameters of the model so the Phis that I."
        ],
        [
            "From the previous slide.",
            "So let me just give you 2 examples of concrete examples of infinite factorial hidden Markov model which we constructed.",
            "So the first one is a linear Gaussian model where we we make this parameter Theta just a. Gaussian matrix A and what we do is we combine our or states at time T, which is just a vector of binary on off switches and we just take the inner product with this matrix.",
            "This feature matrix A and add some some Gaussian or."
        ],
        [
            "So what this is look like?",
            "Well, we did this experiment and let me quickly show you.",
            "The video, so we create an experiment where we have this bars in time data set.",
            "So we have 5 features which are these bars and so you see one here.",
            "You see one here and they switch on and off overtime and the question is given that we have such a data set can we recover these features?",
            "The A is it has a prior distribution, but the.",
            "So the.",
            "Number.",
            "Oh well, when we generate data, we choose a fixed number of chains to generate the data, yeah?",
            "So we in this case we generated data with from 5 features.",
            "But when we learn to model, you know we want to learn that there are five features and not surprising or as a sanity check will recover these five features so that."
        ],
        [
            "Seems the same.",
            "Seems to work now.",
            "More interesting experiment is this independent component analysis, infinite vector hidden Markov model where we do the following.",
            "We introduce a as parameters.",
            "We introduce these.",
            "A signal matrix which is has the same dimensionality's of our latent space.",
            "So for each latent value in each state in each chain, there is a an independent sample sample from a Laplace prior.",
            "And there is then a mixing matrix and we combine all these random variables according to the following model.",
            "So we take the pointwise product of our latent space with these heavy tailed Laplace distribution.",
            "We mix them all together and we add Gaussian noise.",
            "So in this model is."
        ],
        [
            "Useful for an experiment which we which we did on doing the speech, the blinds speaker separation.",
            "So what we did is the following.",
            "We took sentences from the speech Separation Challenge Challenge.",
            "We took five random speakers an five sentence for each speaker.",
            "We took four sentence is and appended them.",
            "Would random random intervals in between an.",
            "We did two experiments, one where we had 10 random microphones and 2nd experiment where we had three random microphones so we had.",
            "Linear mixtures of these speech signals."
        ],
        [
            "And we sampled 20 mixing matrices and so we ran the inference algorithm, took 20 samples and here you can see the ground truth.",
            "So each column here is a speaker and the white denotes if the speaker was.",
            "Producing a sentence or producing a signal and the time goes down.",
            "And what we find is when we have this infinite factorial hidden Markov model, well, we seem to recover the five speakers and.",
            "Relatively nicely, while if we have a prior distribution which is just the standard ICP that doesn't have these Markov dependencies, then as you can see I mean it recovers some of the speakers.",
            "What recovers something, but it you know it has it recovers 9 speakers, which is which is which is."
        ],
        [
            "A bit much same same experiment with less microphones then speakers and again the.",
            "Mark of Pryor seems to do seems to do well or imp."
        ],
        [
            "Move over the ICP.",
            "So here's the thing that I'd like you to take away from this from this.",
            "This talk is that this market by BP is just a new nonparametric building block and that we that we have in our Arsenal of techniques and.",
            "Well, I would say that one of the crucial things that sort of open now is the inference, which can sometimes be a little tedious, right?",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Right, so I'll be talking about the mark of Indian buffet process, an effect or hidden Markov models.",
                    "label": 1
                },
                {
                    "sent": "And this is work together with the UK and Zoom in.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Money.",
                    "label": 0
                },
                {
                    "sent": "So here's sort of a picture of.",
                    "label": 0
                },
                {
                    "sent": "The models that we probably all know.",
                    "label": 0
                },
                {
                    "sent": "Very long time ago we we came up with people, came up with a mixture model and that was sort of generalized in for time series Model 2 hidden Markov models and on the other hand there was another kind of generalization in terms of making these into effect more expressive factorial models.",
                    "label": 0
                },
                {
                    "sent": "And those two are.",
                    "label": 0
                },
                {
                    "sent": "These were combined into factorial hidden Markov models which had more expressiveness than hidden Markov models and a little bit later, when they originally processed were sort of exploited, people came up with infinite mixtures and that leads you not so long ago to developing infinite hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "Another development was the introduction of another non parametric building block which was the ICP which led to non parametric factor models and what we want.",
                    "label": 0
                },
                {
                    "sent": "What I want to do in this talk is tell you a little bit about some work we did to complete the square or complete the cube in this picture and we want to talk about a new building block for nonparametric models which we call the mark of Indian buffet process and this will allow us to construct a.",
                    "label": 0
                },
                {
                    "sent": "An infinite capacity version of the factorial hidden Markov model.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why do we want to do this?",
                    "label": 0
                },
                {
                    "sent": "We want to build factorial models because of the following.",
                    "label": 0
                },
                {
                    "sent": "So these are two applications, part of speech tagging and dialogue segmentation.",
                    "label": 0
                },
                {
                    "sent": "Now sometimes we might not just be interested in, well, often these are observed an we want to infer, say, part of speech labels or we get a speech signal and we want to know which speaker was talking when.",
                    "label": 0
                },
                {
                    "sent": "Now this is possible with hidden Markov models but some.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Times we want to do more than that and we want to say things or have a more expressive, latent space.",
                    "label": 0
                },
                {
                    "sent": "For example, we want to maybe do cello parsing at the same time.",
                    "label": 0
                },
                {
                    "sent": "Or we want speakers might be overlapping and we want to say which speaker is talking at which point in time.",
                    "label": 0
                },
                {
                    "sent": "But as you can see, you know they might be overlapping, and that might be much harder to model with a hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "So factorial models are interesting objects to study.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "Just to recap and get the notation straight.",
                    "label": 0
                },
                {
                    "sent": "This is the actual hidden Markov model and the way this is OK.",
                    "label": 0
                },
                {
                    "sent": "There's two components to it.",
                    "label": 0
                },
                {
                    "sent": "First, there's a finite number of hidden chains.",
                    "label": 0
                },
                {
                    "sent": "Let's say big em of them, and each chain has a variable S which is the latent state variable, which we don't observe.",
                    "label": 0
                },
                {
                    "sent": "And we will assume that the chains all start in this dummy one state.",
                    "label": 0
                },
                {
                    "sent": "Now the second component of the OH and, by the way, So what will say is that every chain has a transition matrix paisa by J, which the nodes.",
                    "label": 0
                },
                {
                    "sent": "What's the probability of moving from state I to stay J in that particular chain?",
                    "label": 0
                },
                {
                    "sent": "And there's these transition matrices, and this is a little mistake.",
                    "label": 0
                },
                {
                    "sent": "This transition matrices don't have to be the same for all all chains.",
                    "label": 0
                },
                {
                    "sent": "Then the second component of the spectral him market model is an observation model and you know this could be.",
                    "label": 0
                },
                {
                    "sent": "This could be a wide variety of things, and so we're going to talk about it in the abstract, the observations Y at time T are generated from some.",
                    "label": 0
                },
                {
                    "sent": "Likelihood model that's parameterized by some parameters and the states at time T for all the different chains.",
                    "label": 0
                },
                {
                    "sent": "So the parameters of this model here are the number of chains the set of transition matrices, and the parameters of our likelihood model.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we want to do now is we want to say, well maybe we don't want to specify the number of chains in, you know, fix them, maybe want to learn the more we want to model explicitly model the uncertainty in the number of chains.",
                    "label": 0
                },
                {
                    "sent": "So we want to have a model where the number or we want to take this previous small.",
                    "label": 1
                },
                {
                    "sent": "This factory in market Mall and take the limit as N goes to Infinity.",
                    "label": 1
                },
                {
                    "sent": "Now the strategy that will use for that is the following.",
                    "label": 0
                },
                {
                    "sent": "So first of all we'll just consider binary binary change.",
                    "label": 0
                },
                {
                    "sent": "So change that are in state zero or one.",
                    "label": 0
                },
                {
                    "sent": "And then we'll have a three step procedure.",
                    "label": 0
                },
                {
                    "sent": "First of all, will describe a finite model and will choose the priors.",
                    "label": 1
                },
                {
                    "sent": "Carefully so that this infinite limit is well defined.",
                    "label": 0
                },
                {
                    "sent": "Second step will be to marginalized out these parameters and the parameters will turn out to be the transition matrices.",
                    "label": 0
                },
                {
                    "sent": "And finally, we'll take the infinite limit and there.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Be a little bit of algebra involved in that, so here's the finite model that we'll be looking at, so.",
                    "label": 0
                },
                {
                    "sent": "For it will be fine one Markov chain as follows, so one single binary Markov chain as follows.",
                    "label": 1
                },
                {
                    "sent": "We start in this dummy states a state one and.",
                    "label": 0
                },
                {
                    "sent": "Then we'll let the Markov chain follow a dynamics with a transition matrix WM that has the following parametrization.",
                    "label": 0
                },
                {
                    "sent": "So there is one parameter ASA BEM, which tells you what's the probability of moving from state zero to state one, and then there's a second parameter beta sub M, which is the probability that you're moving from or staying in state one OK. And because they have to sum up to one, this is.",
                    "label": 0
                },
                {
                    "sent": "This transition matrix is fully specified.",
                    "label": 0
                },
                {
                    "sent": "Now what we'll do is we'll put prior distributions on these asmbs and will make sure, and this this is critical in the analysis is that the probability of switching from zero to one depends on the number of chains.",
                    "label": 0
                },
                {
                    "sent": "And then we also and we have this parameter Alpha and gamma and Delta which will have you know, which allow us to tune the number of chains etc.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 chain.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we'll do next is, well, we'll add will glue a bunch of these guys together.",
                    "label": 0
                },
                {
                    "sent": "And have a big well represented as a matrix.",
                    "label": 0
                },
                {
                    "sent": "So this is, you know, each of these guys is 1 binary Markov chain and there's M of those chains and we'll call that matrix X to represent all those latent variables.",
                    "label": 0
                },
                {
                    "sent": "And what will then do is marginalized out all these transition matrices and compute the probability of the matrix S given our hyperparameters.",
                    "label": 1
                },
                {
                    "sent": "Now there's a bit of algebra involved here, but one thing that I want to point out is that this expression here.",
                    "label": 0
                },
                {
                    "sent": "Only depends on C. These number C which I define as well C sub say 01 sub M are the counts of the number of 01 transitions in chain M-00R00 transitions etc.",
                    "label": 0
                },
                {
                    "sent": "So once we have this expression, we can.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Think about taking the infinite limit and just naively taking the infinite limit won't work as such.",
                    "label": 1
                },
                {
                    "sent": "But what we're going to do is we're going to look at equivalence classes of these latent state sequences.",
                    "label": 0
                },
                {
                    "sent": "Biggest and so we did.",
                    "label": 0
                },
                {
                    "sent": "We did.",
                    "label": 0
                },
                {
                    "sent": "Note that with the square brackets, so once we look at equivalence classes and more in particular, we'll look at left ordered form equivalence classes.",
                    "label": 0
                },
                {
                    "sent": "Then we can take the infinite limit of these these.",
                    "label": 0
                },
                {
                    "sent": "Matrices or we can take the infinite limit of the expression previous we had previously and will get the following expression.",
                    "label": 0
                },
                {
                    "sent": "Now again, this only depends on these numbers C, so the transition counts and this is interesting because.",
                    "label": 1
                },
                {
                    "sent": "What that means is that the probability of this matrix only depends on the transition counts means that this distribution over our latent state is mark of exchangeable.",
                    "label": 0
                },
                {
                    "sent": "OK, so once now we have this infinite limit and so this this distribution over.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "State sequence is well defined and now we can use that as a building block in what we call the infinite factual hidden Markov model.",
                    "label": 1
                },
                {
                    "sent": "So if you look at this part here.",
                    "label": 0
                },
                {
                    "sent": "The essence and then the Alpha Gamma Delta is in a zombies.",
                    "label": 0
                },
                {
                    "sent": "Then this is what we previously defined as the mark of Indian buffet process.",
                    "label": 1
                },
                {
                    "sent": "So this is our non parametric prior distribution.",
                    "label": 1
                },
                {
                    "sent": "Now by adding parameters which we give a certain prior an an observation model which will again talk in the abstract, it's some distribution F. If we put all these things together then this is what we call the infinite vector or hidden Markov model and I'll give you 2 examples.",
                    "label": 0
                },
                {
                    "sent": "That we've you know, concrete examples that we've built and that we've experimented with.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now the question is how do we do inference in these models?",
                    "label": 0
                },
                {
                    "sent": "And this is how we go about it.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, it turns out that this the constructions that the construction that we gave we can we can.",
                    "label": 0
                },
                {
                    "sent": "Port the stick breaking construction for the Indian buffet process to our model.",
                    "label": 0
                },
                {
                    "sent": "So the I mean I can talk about the technical details, but essentially we have we have a stick breaking construction over for this model and so we can think of the eys as being little sticks here.",
                    "label": 0
                },
                {
                    "sent": "So the inference starts as follows.",
                    "label": 0
                },
                {
                    "sent": "We have a current sample of latent states here and now.",
                    "label": 1
                },
                {
                    "sent": "I've transposed the matrix so time goes down and the number of chains goes to the right.",
                    "label": 0
                },
                {
                    "sent": "And let's assume in this case our current sample only has five states.",
                    "label": 0
                },
                {
                    "sent": "So then our sampling algorithm works as follows.",
                    "label": 1
                },
                {
                    "sent": "First we sample a slice variable uniformly.",
                    "label": 0
                },
                {
                    "sent": "Between all between zero and the smallest stick that that's currently used.",
                    "label": 0
                },
                {
                    "sent": "And we potentially add some new some new unused chains.",
                    "label": 0
                },
                {
                    "sent": "And then we run the forward filtering backwards sampling algorithm on these chains, which essentially means that we can use a dynamic program to re sample the hidden states for our latent chains.",
                    "label": 0
                },
                {
                    "sent": "And finally we might have to also re sample the parameters of the model so the Phis that I.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the previous slide.",
                    "label": 0
                },
                {
                    "sent": "So let me just give you 2 examples of concrete examples of infinite factorial hidden Markov model which we constructed.",
                    "label": 0
                },
                {
                    "sent": "So the first one is a linear Gaussian model where we we make this parameter Theta just a. Gaussian matrix A and what we do is we combine our or states at time T, which is just a vector of binary on off switches and we just take the inner product with this matrix.",
                    "label": 0
                },
                {
                    "sent": "This feature matrix A and add some some Gaussian or.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what this is look like?",
                    "label": 0
                },
                {
                    "sent": "Well, we did this experiment and let me quickly show you.",
                    "label": 0
                },
                {
                    "sent": "The video, so we create an experiment where we have this bars in time data set.",
                    "label": 0
                },
                {
                    "sent": "So we have 5 features which are these bars and so you see one here.",
                    "label": 0
                },
                {
                    "sent": "You see one here and they switch on and off overtime and the question is given that we have such a data set can we recover these features?",
                    "label": 0
                },
                {
                    "sent": "The A is it has a prior distribution, but the.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Number.",
                    "label": 0
                },
                {
                    "sent": "Oh well, when we generate data, we choose a fixed number of chains to generate the data, yeah?",
                    "label": 0
                },
                {
                    "sent": "So we in this case we generated data with from 5 features.",
                    "label": 1
                },
                {
                    "sent": "But when we learn to model, you know we want to learn that there are five features and not surprising or as a sanity check will recover these five features so that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Seems the same.",
                    "label": 0
                },
                {
                    "sent": "Seems to work now.",
                    "label": 0
                },
                {
                    "sent": "More interesting experiment is this independent component analysis, infinite vector hidden Markov model where we do the following.",
                    "label": 1
                },
                {
                    "sent": "We introduce a as parameters.",
                    "label": 0
                },
                {
                    "sent": "We introduce these.",
                    "label": 0
                },
                {
                    "sent": "A signal matrix which is has the same dimensionality's of our latent space.",
                    "label": 1
                },
                {
                    "sent": "So for each latent value in each state in each chain, there is a an independent sample sample from a Laplace prior.",
                    "label": 0
                },
                {
                    "sent": "And there is then a mixing matrix and we combine all these random variables according to the following model.",
                    "label": 1
                },
                {
                    "sent": "So we take the pointwise product of our latent space with these heavy tailed Laplace distribution.",
                    "label": 0
                },
                {
                    "sent": "We mix them all together and we add Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "So in this model is.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Useful for an experiment which we which we did on doing the speech, the blinds speaker separation.",
                    "label": 0
                },
                {
                    "sent": "So what we did is the following.",
                    "label": 0
                },
                {
                    "sent": "We took sentences from the speech Separation Challenge Challenge.",
                    "label": 1
                },
                {
                    "sent": "We took five random speakers an five sentence for each speaker.",
                    "label": 0
                },
                {
                    "sent": "We took four sentence is and appended them.",
                    "label": 0
                },
                {
                    "sent": "Would random random intervals in between an.",
                    "label": 0
                },
                {
                    "sent": "We did two experiments, one where we had 10 random microphones and 2nd experiment where we had three random microphones so we had.",
                    "label": 0
                },
                {
                    "sent": "Linear mixtures of these speech signals.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we sampled 20 mixing matrices and so we ran the inference algorithm, took 20 samples and here you can see the ground truth.",
                    "label": 0
                },
                {
                    "sent": "So each column here is a speaker and the white denotes if the speaker was.",
                    "label": 0
                },
                {
                    "sent": "Producing a sentence or producing a signal and the time goes down.",
                    "label": 0
                },
                {
                    "sent": "And what we find is when we have this infinite factorial hidden Markov model, well, we seem to recover the five speakers and.",
                    "label": 0
                },
                {
                    "sent": "Relatively nicely, while if we have a prior distribution which is just the standard ICP that doesn't have these Markov dependencies, then as you can see I mean it recovers some of the speakers.",
                    "label": 0
                },
                {
                    "sent": "What recovers something, but it you know it has it recovers 9 speakers, which is which is which is.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A bit much same same experiment with less microphones then speakers and again the.",
                    "label": 0
                },
                {
                    "sent": "Mark of Pryor seems to do seems to do well or imp.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Move over the ICP.",
                    "label": 0
                },
                {
                    "sent": "So here's the thing that I'd like you to take away from this from this.",
                    "label": 0
                },
                {
                    "sent": "This talk is that this market by BP is just a new nonparametric building block and that we that we have in our Arsenal of techniques and.",
                    "label": 1
                },
                {
                    "sent": "Well, I would say that one of the crucial things that sort of open now is the inference, which can sometimes be a little tedious, right?",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}