{
    "id": "ajimjrqynbnwqp2qkbqwn3rrcvmvs5xn",
    "title": "Trade-Offs in Robot Skill Learning",
    "info": {
        "author": [
            "Jan Peters, Department of Computer Science, Darmstadt University of Technology"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_peters_robot/",
    "segmentation": [
        [
            "OK, today I will be talking about trade offs and robots kill running."
        ],
        [
            "And as somebody who did his PhD very close to Hollywood, allow me to start this talk.",
            "Nevertheless, with a very famous movie The movie I, robot and for us roboticists.",
            "So parimeter robotics.",
            "To one, this is the reason why we got into robotics.",
            "Most of the times, we really want to create robots, well, which resemble all the fantastic things you saw."
        ],
        [
            "Short clip, which means we want to have robots that can deal with the uncertainty and tasks of environment post for example by these docs, which can adapt to humans and are particularly also safe in the presence of humans, so that it's not like an industrial robot arm which would hug you and probably crush your bones.",
            "And we have scenarios where the programming complexity is incredibly hard, so that it's really, really difficult for us if you want to hack up a task.",
            "And it's not limited to an extremely specific scenario without any uncertainty.",
            "So how can we fulfill Hollywoods future vision of robotics for the classical solution is pretty straightforward.",
            "We take a couple of smart grad students.",
            "If you put them in a cellar, we give them pizza, you give Nuka Cola and at the end of the semester the effect of the task perfect, right?",
            "2nd but unfortunately next semester or next year or next project you starting exactly from Square one and you well, smart humans will just never get us to that level of autonomy which we see in these robots.",
            "Instead, we want to have robots that can learn the new tricks and refine their skills.",
            "Now that of course has been integer.",
            "Tried machine learning is try to address this for quite awhile and the classical neural networks community, for example promised us just throw some data, edit and it works in modern machine learning.",
            "No, it's never that easy.",
            "Instead of the shelf will most likely never scale to entropy.",
            "Morphic robots unless you give them something extra.",
            "This something extra is exactly what we are aiming at.",
            "Since we have to constantly do tradeoffs between the different insights we have from physics, from how much data we want to retain autonomously.",
            "But we really want our system to be and also on how much human insight we still want to engineer into this system in terms of good features or a good learning architecture.",
            "Until we can actually really put an algorithm in there."
        ],
        [
            "So.",
            "In terms of trade offs in robot learning very clearly have three, which are usually conflicting.",
            "One is the quality of the behavior which we want to get that since if we go for incredibly high precision, then usually it's no longer efficient to even obtain this behavior as doing, let's say a couple of 10s of thousands of trials is commonly done on reinforcement learning benchmark tasks on a real robot.",
            "Well, you don't really want to do that, since let's say you're.",
            "Just playing ping pong, you will have to reset the rub it all the time.",
            "You will have to give it the ball back.",
            "You will be having to do a lot of trouble just to get these many many scenarios.",
            "So the efficiency of movement generation is crucial.",
            "Obtaining giving it just as much data as you need to get an acceptable behavior.",
            "And finally of course.",
            "Sometimes we can give it a lot of extra knowledge which will limit our autonomy, which gives us the third kind of major trade off and I'll give you a couple of give you 3 very specific scenarios where we have traded off well."
        ],
        [
            "Friend and different ones of these.",
            "Quantities first though, and quick introduction to Robert skill Learning, then the first tradeoff between generality and physical knowledge, then on information loss versus self improvement, and finally a large repertoire versus parsimonious behavior, parsimonious behavior representation and then a conclusion.",
            "So.",
            "Let me bring you into the kind of frameworks we live in.",
            "Well, we very frequently think about scenarios where we have both external state like this ball, which is here connected by a string to the table tennis record.",
            "We have an internal state which may be the position of the record, but it may also include, for example, the proprioceptive state of the arm.",
            "And we have actions which in our case are actually high dimensional continuous vectors as containing torques which we need to adjust all the time.",
            "Just to give you a feel for such an arm, this you have state space which is 21 dimensional and an action space which is 7 dimensional which is a major problem if you would just be taking off the shelf reinforcement learning methods."
        ],
        [
            "So instead we of course need to do need to, well do certain tradeoffs, but we'll be getting to them in a moment.",
            "We obviously model.",
            "We obviously want to try to learn policy meaning mappings from states to actions, and we have a teacher who gives us rewards, and I guess I'm telling you guys use inverse M, saying they obviously transferring from one state to the next based on the action.",
            "No, we usually do not study deterministic policy's, but much rather, we want to focus on stochastic policies because for many reasons, for one is that in most of our human actions actually distrust ecity.",
            "This milestone still city, which we have is a tremor in our arms already gives us the exploration which we use for most minor learning exercises.",
            "When you, for example, for the first time, stepping on a new sailing boat, you just want to adjust.",
            "Your arm to that, the steering gear of the sailing boat.",
            "You will do this only based upon the expiration Asian coming from your tremor.",
            "But it's really very mild.",
            "You also need this trust ecity of course, to model the teacher.",
            "Then who has this tremor you needed for expert?",
            "Generally exploration and in some cases you can show that it breaks the curse of dimensionality while in cases where the imperfect information you will have.",
            "Yep, scenarios where the optimal solution.",
            "Maybe stochastic and we have a stochastic policy which has some para meters which we can use for both imitation and reinforcement learning."
        ],
        [
            "Just in terms of notation, we let this or less loop rollout.",
            "We've trajectories consisting out of States and actions.",
            "We have distributions about them and we have rewards as on these."
        ],
        [
            "What we want to do?",
            "Well, we want to improve and this is T-ball as we are not enough America.",
            "I probably do not need to explain this to everybody.",
            "For the Europeans this is how kids on this continent learn how to baseball.",
            "Here first the parent puts it on.",
            "This puts the ball in a stick and then shows the kid how to hit the ball.",
            "And the robot has had the same scenario, just and becomes better and better an what it basically does here is that it optimizes the expectations of the scores of the teacher.",
            "This is only every outtake of every 30th trial.",
            "And it gets a reward based on how far it shoots the ball, as well as on negative reward based on how fast it becomes during that trajectory, so that it doesn't go for the local minimum of fitting fast and faster."
        ],
        [
            "So.",
            "The first tradeoff is some which may not be natural to the machine learner, but is incredibly natural to the roboticists.",
            "That's one between generality."
        ],
        [
            "Physical knowledge.",
            "And classically, people who come from robotics would tell you why are you doing this learning stuff.",
            "Physics is."
        ],
        [
            "Great, so they would tell you use all the physics you can since physics knowledge is extremely powerful and if you have a physical solution and you really have a well modeled system, well in that case physics actually has a very general solution which works over the whole state space sometimes and you can even say why it works.",
            "And well, you have smart students.",
            "They can handcraft.",
            "This may be in this semester."
        ],
        [
            "However.",
            "In practice, this is not so easy since, well, first of all we have all the ones of you have studied classical AI have probably seen already that.",
            "Well, classically I failed it modeling the whole world similarly in robotics.",
            "We can usually only deal with this scenario if you've worked hard enough on the model and the physical model that there is no longer any uncertainty left.",
            "And really, all physical models are wrong so.",
            "We usually have to do a lot of fudging.",
            "Then in the end to actually get it to work.",
            "An learning will.",
            "It does allow for higher accuracy, higher robustness."
        ],
        [
            "Autonomy and give you again one example of how give an example of how learning can really beat the physical model.",
            "So this year is the robot arm with the best possible estimate of the rigid body dynamics para meters and it's told to follow this trajectory and this time it's not following the trajectory as an industrial output and gesture.",
            "You would have improved prize in very strong proprioception cocontraction so that the arm would be totally stiff.",
            "And you know, lower these gains like the humans are very, very low gains.",
            "That's why you can push human normally.",
            "It gets pretty large trekking around and only the feedback model can take care can only the feedback component can take care of that model anymore.",
            "Well, if you would train the robot on a different letter than the letter B here, but on a human drawn letter A let's say then you already get a much better tracking performance then you would get from a physical model.",
            "And if you allow for online training then you can even in this low gain mode getting near perfect tracking performance so.",
            "The key lesson is that we can usually when we have enough of a physics blueprint that we know what out of physics we give it.",
            "So what combination of variables and maybe what additional features we can give it?",
            "Then we can get a really, really good trade off of where we are much closer to the learning solution than to the actual physics solution.",
            "The second thing, of course, is we want to give it not only in this as rigid body in a model, but it's very very obvious, but much rather we want to extract all of the important knowledge from robotics about the blueprint."
        ],
        [
            "Behind things so that we can give it the right features for that.",
            "If you really ended up with a policy meaning state from well mapping from state to action which has three major components, the first one.",
            "Is the one which compensates for forces which allows execution.",
            "An it basically Maps.",
            "Primitive behaviors, which are usually represented by the dynamical slow dimensional dynamical systems and knows how to map them into torques and uses the context only to the right physical context, only to reuse them in a different scenario.",
            "And I'll give you an explain this a little bit more by diving deeper onto these primitives.",
            "Since."
        ],
        [
            "These primitives for these primitives.",
            "We basically have taken groups that should have appeared for these primitives.",
            "We've basically taking a nonlinear spring damper system which has two kinds of para meters, one kind of farm.",
            "It is a used for digesting to different tasks and generalizing one time of para meters is used for adapting the actual policy.",
            "Important part is this is a mechanical counterpart to nonlinear dynamical system.",
            "Which gives us the right features.",
            "In order to do learning."
        ],
        [
            "And I'll quickly show you how we can use this in an imitation learning scenario where we would just want to reproduce using our new path distribution parameterized by the policy parameters Theta reproduce the examples the teacher has given us.",
            "Here, it's symbolized by having the actions here states here in this state action pair here.",
            "We can do this in this imitation learning scenario simply by jumping onto this solution using these physics based features or physics inspired features.",
            "And well, if you have unknown variables, hidden variables, then would become more complicated and you would have known."
        ],
        [
            "Like I rhythm, but can we do with this?",
            "Well, we can learn already by imitation.",
            "Here.",
            "You see how the robot is taken by the hand in this scenario of having a string on the ball?",
            "That's why it's so fast.",
            "And you see how the Robert is taught this behavior and.",
            "You will also see right away that it managed to reproduce this extremely nicely.",
            "This is.",
            "This example is actually really good.",
            "One of the trade off of well using pure physics and then trying to optimize for the planner on it versus learning on the actual data.",
            "Because my student Jens Kober, with whom whom I did this exact example a couple years back, he said he's such a good control engineer and he wants to rather do this the right old-fashioned wafers and you spent three months for a really ugly looking behavior, very modeled everything by hand using physics which never worked really.",
            "Really, really, really well, and then I basically after the three month told him well, come on, now, let's do learning.",
            "This was actually the work of the work on the robot versus the one of an afternoon which."
        ],
        [
            "Shows you that learning can do well, so where are we in terms of the tradeoffs?",
            "Quite clearly for high quality behavior, be very frequently need online in learning robot learning we need physics based representations and but we want to generalize them sufficiently that they cannot just store one kind of behavior as you would do this when you would be using well enough would be coming from the pure robotics side instead of them.",
            "The machine learning or Rob."
        ],
        [
            "Morningside?",
            "Next thing we're going to focus on is how we can trade off information loss."
        ],
        [
            "In self improvement for us, it is incredibly important to retain experience we cannot like if you were dealing with a discrete simulated scenario, you can try out violent actions.",
            "If we try out violent actions on a robot, that means the cable drives break and I have to spend the next six hours putting a new cable drive in and having at least one student.",
            "Since you never can do this with the.",
            "Just two ends usually need for.",
            "You will have a lot of annoying work ahead.",
            "And clearly we wonder is nevertheless just like in the classical reinforcement learning case, have a high reward."
        ],
        [
            "So for that, let's have a quick look again at the cost function of reinforcement learning, then formulated over paths.",
            "In this case, well, we have the expected return, and it has no notion of data.",
            "So instead it's just basically an optimal control problem.",
            "Normally you would have to either make a heuristic so that your data is generated from the.",
            "Basically the same process that data is generated from another process, but you treat it on a previous policy, but you treat it like it wasn't true.",
            "Generated from the new policy.",
            "Alternatively, you can of course also say it's a policy gradient method, so we are so close to our previous policy that we can generate a gradient, right?"
        ],
        [
            "Yeah.",
            "This seems not to be what humans do in both decision scenarios as well as in motor learning.",
            "Instead, would humans appear to do is that they are rather match the reward, weighted previous experience and within you behavior which is.",
            "Mildly weird when you think about it initially, but what it actually means is that we want to create policies that match the reward, weighted previous policies.",
            "One very way of seeing this is that you would want to match the pluses when you have classes available, but also the minus is if they're a little bit outside.",
            "In many reinforcement learning frameworks, X related to this, the first time we applied this in robotics was when I was still with during my PhD at University of Southern California, where we had this little robot doc and it had to go over rough terrain and at that time was the roughest terrain we could do and it basically it was supposed to, well, had to learn not to place its feet into the canyons or under the cliffs or under any places where it would slip, or it would get a big negative reward.",
            "So this kind of reward weighted matching very quickly gave it a really good policy.",
            "The only unfortunate thing is like 1/2 years later we managed to handcraft haeften.",
            "Even better approach, which used then the idea of template matching.",
            "Rat.",
            "And beat this about 1 after."
        ],
        [
            "Years later.",
            "So what we have from this reward weighted matching is a lower bound with this lower bound core is the one of trying to minimize the distance.",
            "Between the policy between the path distribution to generate and the reward weighted one which we have observed so far.",
            "Which means we have directly created a tradeoff because we have the tradeoff now between the rewards that switch well.",
            "If you have a high reward.",
            "And we have taken this only once, then this is as valuable as if we had taken action many many times and gotten a low reward.",
            "But nevertheless we will be slowly moving towards the high reward one, so it gives us is a Safeway of dealing between our experience and our reward.",
            "However, how we exactly trade that off is not totally clear here from this description, yet nevertheless this say."
        ],
        [
            "For handling, our experience has allowed us to create algorithms which are obviously inspired by M like approaches, but we have a lower bound and always jump to the lower bound.",
            "The maximum of the lower bound until we achieve the optimal solution.",
            "But it was even cooler.",
            "Is that as we can choose our stochastic policy and we can use a physics based features in here and we are linear in these features, at least for their shape.",
            "Para meters we can reduce the whole problem of reinforcement learning onto reward weighted regression where we take the actions project onto the rewards, rebate them, but the rewards and project them down onto the features."
        ],
        [
            "And this has allowed us to do a lot of tasks and I'm just going to show you one example.",
            "The example of Paulina Cup, where we start again within the demonstration for initialization and subsequently do self improvement.",
            "As you see, the imitation fails to reproduce the behavior an as the robot has doesn't have the same accelerations as the human, and there was a little bit of noise in the demonstration.",
            "And hence it has to do self improvement.",
            "What you know, see is how how it becomes better.",
            "It receives a report only based upon the distance by which the ball misses the Cup.",
            "And it usually gets it into the Cup for the first time at 40 something trials and it gets it into the Cup all the time.",
            "After about 90 trials.",
            "Now you may of course ask yourself how this compares to humans.",
            "Well, we've only done a very selected study on the relatives of my PhD students in school, but on his extended family we could figure out at one of his Christmas breaks after Nips that his cousins at the age of five to six would not manage to learn this behavior at all.",
            "The cousins over grown up and were 16 to 18.",
            "They would learn it within very very few trials 345 vial the.",
            "Cousins who were at the age of 10 to 12 took 30 something trials, usually which is about this at the same magnitude as our algorithm.",
            "It seems it's only me who took three months to get it in the first time."
        ],
        [
            "Now the see M like methods have one major drawback and that is that this this clearly trading off experience in high reward in a way where it is very useful for robotics.",
            "But we cannot really control where we are actually between the experience in the high reward.",
            "So what we're currently working on is how we can get more control solution, thereby having an additional para meter which tells us how close we should stay to the training data.",
            "Instead of.",
            "Which gives us then a new class of algorithms which we call the relative entropy policy search methods, but to that will get more in more detail in the next trade off.",
            "The advantage here is that we have an adjustable, more consistent tradeoff than with the M like methods."
        ],
        [
            "Quickly summarizing, clearly we have tradeoffs here.",
            "One, the one between efficiency of the generation A.",
            "In the efficiency of generation that we do not want to lose our behavior too fast, but on the other hand we may lose bad behavior too slow.",
            "In return, we become relatively safe with the real robot, and we usually end up still in highly rewarded behaviors.",
            "While this these two of course fighting against this one.",
            "Somewhat interesting are the initial demonstrations, since they are clearly contradicting the aim of autonomy in generality, since if you always need a good generalization to, sorry, good initialization to get started, then this cannot be giving you totally different solutions.",
            "I'll give you an example from high jumping until 1964, everybody ran forward, jumped over the target.",
            "People learned this kind of jumping, but they would never, never came towards the Fosbury flop and will miss the Fosbury came around jump two came from the side, jumped over it backwards.",
            "Such kind of backwards jumps we cannot do when we have to start from initial demonstration.",
            "In this case you would need something like a forward model for example."
        ],
        [
            "Nevertheless, we are not going to get into forward models today, but rather we are talking about the loss of the tradeoffs.",
            "The one between in having a large repertoire versus parsimonious behavior represent."
        ],
        [
            "For that we we clearly have now a tradeoff between the versatility he which we can generate with our and we have now many primitives, not just few, not just have one primitive children and the person money which we need in order to learn if."
        ],
        [
            "Recently now let's quickly.",
            "So now for this time reuse, this scenario of the relative entropy policy search just will not only use it for the."
        ],
        [
            "Action part, but we will use it in the complete framework with many primitives in."
        ],
        [
            "Where the context part gives us a gating network which can activate different primitives which then act as policy's conditioned on this internal gate."
        ],
        [
            "Invariable.",
            "If you would do this naively, you would end up just putting an additional oh in here and get such kind of a decomp."
        ],
        [
            "Vision.",
            "However, this has a major disadvantage, since you would never with all the primitives, would share the same.",
            "For example, incoming balls let's.",
            "And would then focus on one solution, which would work very well together, but it would not be able to.",
            "Split up the space in such a way that there would be a parsimonious policy arising."
        ],
        [
            "Hence we need to form need to force the primitives such that they limit their responsibility.",
            "This can be done by looking at the entry of the entropy of the of the gating network, and this high entropy here would indicate a high overlap of."
        ],
        [
            "Primatives again the same example you can see in this case very nicely that the two primitives have separated and a covering different parts of the state action space.",
            "Hence we can learn these fewer these primitives sufficiently also eliminate."
        ],
        [
            "Some of them.",
            "One very quick example.",
            "There are basically multiple behaviors for solving the same tasks stored in there.",
            "This is a tetherball task where you have to hit the ball such that it ends up at the at the other side of the pole."
        ],
        [
            "In this case, this multi primitive approach yields a good performance.",
            "Where the spreading out results into quick increase in the reward quicker than if you were looking at me.",
            "Looking at the naive approach and also at the single primitive approach.",
            "At the same time, we managed to reduce the primitives to a number which is more reasonable and hence more parsimonious."
        ],
        [
            "We've also used the same framework in the context of table tennis, for we started out by having multiple demonstrations of different table tennis strokes."
        ],
        [
            "Subsequently used them in the context of.",
            "Of reproducing behavior by selecting and generalizing among these parameters with a gating network, and.",
            "What you see next is how behavior is composed by basically, well, the weights correspond to the gating network.",
            "Output and well from this by mutation running alone, you can already generate a relatively good policy.",
            "However, imitation learning is also in table tennis, not enough.",
            "Hence 69% of the return balls in the task based definitely not enough, so we."
        ],
        [
            "Focused on hitting regions where we had zero percent success before and with the ball gun shot into exactly these regions and use now the reinforcement learning and complete reinforcement in version of the framework and return balls in this context.",
            "Finally.",
            "At the end."
        ],
        [
            "This had in the zero percent region score of 79% and returns.",
            "Finally, hear the robot plays against its creator Katerina.",
            "Milling is about as good at table tennis as the robots.",
            "As you can observe from this video.",
            "She is a computer scientist after all.",
            "And you see that the robot plays quite well."
        ],
        [
            "Would that work here?",
            "I'll trade off was mainly between the versatility which you really need for the well for high quality behavior versus the parsimony parsimony which you would need in order to learn things if it."
        ],
        [
            "With that I'm at the conclusion."
        ],
        [
            "And I hope that I've kind of conveyed that we're dealing with a really exciting problem here.",
            "Motor skill learning and we're having also some tradeoffs, tradeoffs between physical learning and knowledge experience versus high rewards versatility versus parsimony.",
            "And there are many more.",
            "We never worked on, hence I did not really discuss them here."
        ],
        [
            "And the big things goes to all these people who have interacted with me on when creating all this research yet.",
            "For that, thank you for your attention and I hope you have lots of questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, today I will be talking about trade offs and robots kill running.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as somebody who did his PhD very close to Hollywood, allow me to start this talk.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, with a very famous movie The movie I, robot and for us roboticists.",
                    "label": 0
                },
                {
                    "sent": "So parimeter robotics.",
                    "label": 0
                },
                {
                    "sent": "To one, this is the reason why we got into robotics.",
                    "label": 0
                },
                {
                    "sent": "Most of the times, we really want to create robots, well, which resemble all the fantastic things you saw.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Short clip, which means we want to have robots that can deal with the uncertainty and tasks of environment post for example by these docs, which can adapt to humans and are particularly also safe in the presence of humans, so that it's not like an industrial robot arm which would hug you and probably crush your bones.",
                    "label": 0
                },
                {
                    "sent": "And we have scenarios where the programming complexity is incredibly hard, so that it's really, really difficult for us if you want to hack up a task.",
                    "label": 0
                },
                {
                    "sent": "And it's not limited to an extremely specific scenario without any uncertainty.",
                    "label": 0
                },
                {
                    "sent": "So how can we fulfill Hollywoods future vision of robotics for the classical solution is pretty straightforward.",
                    "label": 1
                },
                {
                    "sent": "We take a couple of smart grad students.",
                    "label": 0
                },
                {
                    "sent": "If you put them in a cellar, we give them pizza, you give Nuka Cola and at the end of the semester the effect of the task perfect, right?",
                    "label": 0
                },
                {
                    "sent": "2nd but unfortunately next semester or next year or next project you starting exactly from Square one and you well, smart humans will just never get us to that level of autonomy which we see in these robots.",
                    "label": 0
                },
                {
                    "sent": "Instead, we want to have robots that can learn the new tricks and refine their skills.",
                    "label": 0
                },
                {
                    "sent": "Now that of course has been integer.",
                    "label": 0
                },
                {
                    "sent": "Tried machine learning is try to address this for quite awhile and the classical neural networks community, for example promised us just throw some data, edit and it works in modern machine learning.",
                    "label": 0
                },
                {
                    "sent": "No, it's never that easy.",
                    "label": 0
                },
                {
                    "sent": "Instead of the shelf will most likely never scale to entropy.",
                    "label": 0
                },
                {
                    "sent": "Morphic robots unless you give them something extra.",
                    "label": 0
                },
                {
                    "sent": "This something extra is exactly what we are aiming at.",
                    "label": 0
                },
                {
                    "sent": "Since we have to constantly do tradeoffs between the different insights we have from physics, from how much data we want to retain autonomously.",
                    "label": 0
                },
                {
                    "sent": "But we really want our system to be and also on how much human insight we still want to engineer into this system in terms of good features or a good learning architecture.",
                    "label": 0
                },
                {
                    "sent": "Until we can actually really put an algorithm in there.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In terms of trade offs in robot learning very clearly have three, which are usually conflicting.",
                    "label": 1
                },
                {
                    "sent": "One is the quality of the behavior which we want to get that since if we go for incredibly high precision, then usually it's no longer efficient to even obtain this behavior as doing, let's say a couple of 10s of thousands of trials is commonly done on reinforcement learning benchmark tasks on a real robot.",
                    "label": 0
                },
                {
                    "sent": "Well, you don't really want to do that, since let's say you're.",
                    "label": 0
                },
                {
                    "sent": "Just playing ping pong, you will have to reset the rub it all the time.",
                    "label": 0
                },
                {
                    "sent": "You will have to give it the ball back.",
                    "label": 0
                },
                {
                    "sent": "You will be having to do a lot of trouble just to get these many many scenarios.",
                    "label": 1
                },
                {
                    "sent": "So the efficiency of movement generation is crucial.",
                    "label": 0
                },
                {
                    "sent": "Obtaining giving it just as much data as you need to get an acceptable behavior.",
                    "label": 0
                },
                {
                    "sent": "And finally of course.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we can give it a lot of extra knowledge which will limit our autonomy, which gives us the third kind of major trade off and I'll give you a couple of give you 3 very specific scenarios where we have traded off well.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Friend and different ones of these.",
                    "label": 0
                },
                {
                    "sent": "Quantities first though, and quick introduction to Robert skill Learning, then the first tradeoff between generality and physical knowledge, then on information loss versus self improvement, and finally a large repertoire versus parsimonious behavior, parsimonious behavior representation and then a conclusion.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let me bring you into the kind of frameworks we live in.",
                    "label": 0
                },
                {
                    "sent": "Well, we very frequently think about scenarios where we have both external state like this ball, which is here connected by a string to the table tennis record.",
                    "label": 0
                },
                {
                    "sent": "We have an internal state which may be the position of the record, but it may also include, for example, the proprioceptive state of the arm.",
                    "label": 0
                },
                {
                    "sent": "And we have actions which in our case are actually high dimensional continuous vectors as containing torques which we need to adjust all the time.",
                    "label": 0
                },
                {
                    "sent": "Just to give you a feel for such an arm, this you have state space which is 21 dimensional and an action space which is 7 dimensional which is a major problem if you would just be taking off the shelf reinforcement learning methods.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So instead we of course need to do need to, well do certain tradeoffs, but we'll be getting to them in a moment.",
                    "label": 0
                },
                {
                    "sent": "We obviously model.",
                    "label": 0
                },
                {
                    "sent": "We obviously want to try to learn policy meaning mappings from states to actions, and we have a teacher who gives us rewards, and I guess I'm telling you guys use inverse M, saying they obviously transferring from one state to the next based on the action.",
                    "label": 0
                },
                {
                    "sent": "No, we usually do not study deterministic policy's, but much rather, we want to focus on stochastic policies because for many reasons, for one is that in most of our human actions actually distrust ecity.",
                    "label": 0
                },
                {
                    "sent": "This milestone still city, which we have is a tremor in our arms already gives us the exploration which we use for most minor learning exercises.",
                    "label": 0
                },
                {
                    "sent": "When you, for example, for the first time, stepping on a new sailing boat, you just want to adjust.",
                    "label": 0
                },
                {
                    "sent": "Your arm to that, the steering gear of the sailing boat.",
                    "label": 0
                },
                {
                    "sent": "You will do this only based upon the expiration Asian coming from your tremor.",
                    "label": 0
                },
                {
                    "sent": "But it's really very mild.",
                    "label": 0
                },
                {
                    "sent": "You also need this trust ecity of course, to model the teacher.",
                    "label": 0
                },
                {
                    "sent": "Then who has this tremor you needed for expert?",
                    "label": 1
                },
                {
                    "sent": "Generally exploration and in some cases you can show that it breaks the curse of dimensionality while in cases where the imperfect information you will have.",
                    "label": 1
                },
                {
                    "sent": "Yep, scenarios where the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "Maybe stochastic and we have a stochastic policy which has some para meters which we can use for both imitation and reinforcement learning.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just in terms of notation, we let this or less loop rollout.",
                    "label": 0
                },
                {
                    "sent": "We've trajectories consisting out of States and actions.",
                    "label": 0
                },
                {
                    "sent": "We have distributions about them and we have rewards as on these.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we want to do?",
                    "label": 0
                },
                {
                    "sent": "Well, we want to improve and this is T-ball as we are not enough America.",
                    "label": 0
                },
                {
                    "sent": "I probably do not need to explain this to everybody.",
                    "label": 0
                },
                {
                    "sent": "For the Europeans this is how kids on this continent learn how to baseball.",
                    "label": 0
                },
                {
                    "sent": "Here first the parent puts it on.",
                    "label": 0
                },
                {
                    "sent": "This puts the ball in a stick and then shows the kid how to hit the ball.",
                    "label": 0
                },
                {
                    "sent": "And the robot has had the same scenario, just and becomes better and better an what it basically does here is that it optimizes the expectations of the scores of the teacher.",
                    "label": 1
                },
                {
                    "sent": "This is only every outtake of every 30th trial.",
                    "label": 0
                },
                {
                    "sent": "And it gets a reward based on how far it shoots the ball, as well as on negative reward based on how fast it becomes during that trajectory, so that it doesn't go for the local minimum of fitting fast and faster.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The first tradeoff is some which may not be natural to the machine learner, but is incredibly natural to the roboticists.",
                    "label": 0
                },
                {
                    "sent": "That's one between generality.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Physical knowledge.",
                    "label": 0
                },
                {
                    "sent": "And classically, people who come from robotics would tell you why are you doing this learning stuff.",
                    "label": 0
                },
                {
                    "sent": "Physics is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Great, so they would tell you use all the physics you can since physics knowledge is extremely powerful and if you have a physical solution and you really have a well modeled system, well in that case physics actually has a very general solution which works over the whole state space sometimes and you can even say why it works.",
                    "label": 1
                },
                {
                    "sent": "And well, you have smart students.",
                    "label": 0
                },
                {
                    "sent": "They can handcraft.",
                    "label": 0
                },
                {
                    "sent": "This may be in this semester.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "In practice, this is not so easy since, well, first of all we have all the ones of you have studied classical AI have probably seen already that.",
                    "label": 0
                },
                {
                    "sent": "Well, classically I failed it modeling the whole world similarly in robotics.",
                    "label": 1
                },
                {
                    "sent": "We can usually only deal with this scenario if you've worked hard enough on the model and the physical model that there is no longer any uncertainty left.",
                    "label": 0
                },
                {
                    "sent": "And really, all physical models are wrong so.",
                    "label": 1
                },
                {
                    "sent": "We usually have to do a lot of fudging.",
                    "label": 0
                },
                {
                    "sent": "Then in the end to actually get it to work.",
                    "label": 0
                },
                {
                    "sent": "An learning will.",
                    "label": 1
                },
                {
                    "sent": "It does allow for higher accuracy, higher robustness.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Autonomy and give you again one example of how give an example of how learning can really beat the physical model.",
                    "label": 0
                },
                {
                    "sent": "So this year is the robot arm with the best possible estimate of the rigid body dynamics para meters and it's told to follow this trajectory and this time it's not following the trajectory as an industrial output and gesture.",
                    "label": 0
                },
                {
                    "sent": "You would have improved prize in very strong proprioception cocontraction so that the arm would be totally stiff.",
                    "label": 0
                },
                {
                    "sent": "And you know, lower these gains like the humans are very, very low gains.",
                    "label": 0
                },
                {
                    "sent": "That's why you can push human normally.",
                    "label": 0
                },
                {
                    "sent": "It gets pretty large trekking around and only the feedback model can take care can only the feedback component can take care of that model anymore.",
                    "label": 0
                },
                {
                    "sent": "Well, if you would train the robot on a different letter than the letter B here, but on a human drawn letter A let's say then you already get a much better tracking performance then you would get from a physical model.",
                    "label": 0
                },
                {
                    "sent": "And if you allow for online training then you can even in this low gain mode getting near perfect tracking performance so.",
                    "label": 0
                },
                {
                    "sent": "The key lesson is that we can usually when we have enough of a physics blueprint that we know what out of physics we give it.",
                    "label": 0
                },
                {
                    "sent": "So what combination of variables and maybe what additional features we can give it?",
                    "label": 0
                },
                {
                    "sent": "Then we can get a really, really good trade off of where we are much closer to the learning solution than to the actual physics solution.",
                    "label": 0
                },
                {
                    "sent": "The second thing, of course, is we want to give it not only in this as rigid body in a model, but it's very very obvious, but much rather we want to extract all of the important knowledge from robotics about the blueprint.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Behind things so that we can give it the right features for that.",
                    "label": 0
                },
                {
                    "sent": "If you really ended up with a policy meaning state from well mapping from state to action which has three major components, the first one.",
                    "label": 0
                },
                {
                    "sent": "Is the one which compensates for forces which allows execution.",
                    "label": 0
                },
                {
                    "sent": "An it basically Maps.",
                    "label": 0
                },
                {
                    "sent": "Primitive behaviors, which are usually represented by the dynamical slow dimensional dynamical systems and knows how to map them into torques and uses the context only to the right physical context, only to reuse them in a different scenario.",
                    "label": 0
                },
                {
                    "sent": "And I'll give you an explain this a little bit more by diving deeper onto these primitives.",
                    "label": 0
                },
                {
                    "sent": "Since.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These primitives for these primitives.",
                    "label": 0
                },
                {
                    "sent": "We basically have taken groups that should have appeared for these primitives.",
                    "label": 0
                },
                {
                    "sent": "We've basically taking a nonlinear spring damper system which has two kinds of para meters, one kind of farm.",
                    "label": 0
                },
                {
                    "sent": "It is a used for digesting to different tasks and generalizing one time of para meters is used for adapting the actual policy.",
                    "label": 0
                },
                {
                    "sent": "Important part is this is a mechanical counterpart to nonlinear dynamical system.",
                    "label": 0
                },
                {
                    "sent": "Which gives us the right features.",
                    "label": 0
                },
                {
                    "sent": "In order to do learning.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'll quickly show you how we can use this in an imitation learning scenario where we would just want to reproduce using our new path distribution parameterized by the policy parameters Theta reproduce the examples the teacher has given us.",
                    "label": 1
                },
                {
                    "sent": "Here, it's symbolized by having the actions here states here in this state action pair here.",
                    "label": 0
                },
                {
                    "sent": "We can do this in this imitation learning scenario simply by jumping onto this solution using these physics based features or physics inspired features.",
                    "label": 0
                },
                {
                    "sent": "And well, if you have unknown variables, hidden variables, then would become more complicated and you would have known.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like I rhythm, but can we do with this?",
                    "label": 0
                },
                {
                    "sent": "Well, we can learn already by imitation.",
                    "label": 1
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "You see how the robot is taken by the hand in this scenario of having a string on the ball?",
                    "label": 0
                },
                {
                    "sent": "That's why it's so fast.",
                    "label": 0
                },
                {
                    "sent": "And you see how the Robert is taught this behavior and.",
                    "label": 0
                },
                {
                    "sent": "You will also see right away that it managed to reproduce this extremely nicely.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "This example is actually really good.",
                    "label": 0
                },
                {
                    "sent": "One of the trade off of well using pure physics and then trying to optimize for the planner on it versus learning on the actual data.",
                    "label": 0
                },
                {
                    "sent": "Because my student Jens Kober, with whom whom I did this exact example a couple years back, he said he's such a good control engineer and he wants to rather do this the right old-fashioned wafers and you spent three months for a really ugly looking behavior, very modeled everything by hand using physics which never worked really.",
                    "label": 0
                },
                {
                    "sent": "Really, really, really well, and then I basically after the three month told him well, come on, now, let's do learning.",
                    "label": 0
                },
                {
                    "sent": "This was actually the work of the work on the robot versus the one of an afternoon which.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shows you that learning can do well, so where are we in terms of the tradeoffs?",
                    "label": 0
                },
                {
                    "sent": "Quite clearly for high quality behavior, be very frequently need online in learning robot learning we need physics based representations and but we want to generalize them sufficiently that they cannot just store one kind of behavior as you would do this when you would be using well enough would be coming from the pure robotics side instead of them.",
                    "label": 0
                },
                {
                    "sent": "The machine learning or Rob.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Morningside?",
                    "label": 0
                },
                {
                    "sent": "Next thing we're going to focus on is how we can trade off information loss.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In self improvement for us, it is incredibly important to retain experience we cannot like if you were dealing with a discrete simulated scenario, you can try out violent actions.",
                    "label": 0
                },
                {
                    "sent": "If we try out violent actions on a robot, that means the cable drives break and I have to spend the next six hours putting a new cable drive in and having at least one student.",
                    "label": 0
                },
                {
                    "sent": "Since you never can do this with the.",
                    "label": 0
                },
                {
                    "sent": "Just two ends usually need for.",
                    "label": 0
                },
                {
                    "sent": "You will have a lot of annoying work ahead.",
                    "label": 0
                },
                {
                    "sent": "And clearly we wonder is nevertheless just like in the classical reinforcement learning case, have a high reward.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for that, let's have a quick look again at the cost function of reinforcement learning, then formulated over paths.",
                    "label": 1
                },
                {
                    "sent": "In this case, well, we have the expected return, and it has no notion of data.",
                    "label": 1
                },
                {
                    "sent": "So instead it's just basically an optimal control problem.",
                    "label": 1
                },
                {
                    "sent": "Normally you would have to either make a heuristic so that your data is generated from the.",
                    "label": 0
                },
                {
                    "sent": "Basically the same process that data is generated from another process, but you treat it on a previous policy, but you treat it like it wasn't true.",
                    "label": 0
                },
                {
                    "sent": "Generated from the new policy.",
                    "label": 0
                },
                {
                    "sent": "Alternatively, you can of course also say it's a policy gradient method, so we are so close to our previous policy that we can generate a gradient, right?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "This seems not to be what humans do in both decision scenarios as well as in motor learning.",
                    "label": 0
                },
                {
                    "sent": "Instead, would humans appear to do is that they are rather match the reward, weighted previous experience and within you behavior which is.",
                    "label": 1
                },
                {
                    "sent": "Mildly weird when you think about it initially, but what it actually means is that we want to create policies that match the reward, weighted previous policies.",
                    "label": 1
                },
                {
                    "sent": "One very way of seeing this is that you would want to match the pluses when you have classes available, but also the minus is if they're a little bit outside.",
                    "label": 0
                },
                {
                    "sent": "In many reinforcement learning frameworks, X related to this, the first time we applied this in robotics was when I was still with during my PhD at University of Southern California, where we had this little robot doc and it had to go over rough terrain and at that time was the roughest terrain we could do and it basically it was supposed to, well, had to learn not to place its feet into the canyons or under the cliffs or under any places where it would slip, or it would get a big negative reward.",
                    "label": 0
                },
                {
                    "sent": "So this kind of reward weighted matching very quickly gave it a really good policy.",
                    "label": 0
                },
                {
                    "sent": "The only unfortunate thing is like 1/2 years later we managed to handcraft haeften.",
                    "label": 0
                },
                {
                    "sent": "Even better approach, which used then the idea of template matching.",
                    "label": 0
                },
                {
                    "sent": "Rat.",
                    "label": 0
                },
                {
                    "sent": "And beat this about 1 after.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Years later.",
                    "label": 0
                },
                {
                    "sent": "So what we have from this reward weighted matching is a lower bound with this lower bound core is the one of trying to minimize the distance.",
                    "label": 0
                },
                {
                    "sent": "Between the policy between the path distribution to generate and the reward weighted one which we have observed so far.",
                    "label": 0
                },
                {
                    "sent": "Which means we have directly created a tradeoff because we have the tradeoff now between the rewards that switch well.",
                    "label": 0
                },
                {
                    "sent": "If you have a high reward.",
                    "label": 1
                },
                {
                    "sent": "And we have taken this only once, then this is as valuable as if we had taken action many many times and gotten a low reward.",
                    "label": 0
                },
                {
                    "sent": "But nevertheless we will be slowly moving towards the high reward one, so it gives us is a Safeway of dealing between our experience and our reward.",
                    "label": 0
                },
                {
                    "sent": "However, how we exactly trade that off is not totally clear here from this description, yet nevertheless this say.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For handling, our experience has allowed us to create algorithms which are obviously inspired by M like approaches, but we have a lower bound and always jump to the lower bound.",
                    "label": 1
                },
                {
                    "sent": "The maximum of the lower bound until we achieve the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "But it was even cooler.",
                    "label": 0
                },
                {
                    "sent": "Is that as we can choose our stochastic policy and we can use a physics based features in here and we are linear in these features, at least for their shape.",
                    "label": 0
                },
                {
                    "sent": "Para meters we can reduce the whole problem of reinforcement learning onto reward weighted regression where we take the actions project onto the rewards, rebate them, but the rewards and project them down onto the features.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this has allowed us to do a lot of tasks and I'm just going to show you one example.",
                    "label": 0
                },
                {
                    "sent": "The example of Paulina Cup, where we start again within the demonstration for initialization and subsequently do self improvement.",
                    "label": 0
                },
                {
                    "sent": "As you see, the imitation fails to reproduce the behavior an as the robot has doesn't have the same accelerations as the human, and there was a little bit of noise in the demonstration.",
                    "label": 0
                },
                {
                    "sent": "And hence it has to do self improvement.",
                    "label": 0
                },
                {
                    "sent": "What you know, see is how how it becomes better.",
                    "label": 0
                },
                {
                    "sent": "It receives a report only based upon the distance by which the ball misses the Cup.",
                    "label": 0
                },
                {
                    "sent": "And it usually gets it into the Cup for the first time at 40 something trials and it gets it into the Cup all the time.",
                    "label": 0
                },
                {
                    "sent": "After about 90 trials.",
                    "label": 0
                },
                {
                    "sent": "Now you may of course ask yourself how this compares to humans.",
                    "label": 0
                },
                {
                    "sent": "Well, we've only done a very selected study on the relatives of my PhD students in school, but on his extended family we could figure out at one of his Christmas breaks after Nips that his cousins at the age of five to six would not manage to learn this behavior at all.",
                    "label": 0
                },
                {
                    "sent": "The cousins over grown up and were 16 to 18.",
                    "label": 0
                },
                {
                    "sent": "They would learn it within very very few trials 345 vial the.",
                    "label": 0
                },
                {
                    "sent": "Cousins who were at the age of 10 to 12 took 30 something trials, usually which is about this at the same magnitude as our algorithm.",
                    "label": 0
                },
                {
                    "sent": "It seems it's only me who took three months to get it in the first time.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the see M like methods have one major drawback and that is that this this clearly trading off experience in high reward in a way where it is very useful for robotics.",
                    "label": 0
                },
                {
                    "sent": "But we cannot really control where we are actually between the experience in the high reward.",
                    "label": 1
                },
                {
                    "sent": "So what we're currently working on is how we can get more control solution, thereby having an additional para meter which tells us how close we should stay to the training data.",
                    "label": 0
                },
                {
                    "sent": "Instead of.",
                    "label": 0
                },
                {
                    "sent": "Which gives us then a new class of algorithms which we call the relative entropy policy search methods, but to that will get more in more detail in the next trade off.",
                    "label": 1
                },
                {
                    "sent": "The advantage here is that we have an adjustable, more consistent tradeoff than with the M like methods.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quickly summarizing, clearly we have tradeoffs here.",
                    "label": 0
                },
                {
                    "sent": "One, the one between efficiency of the generation A.",
                    "label": 0
                },
                {
                    "sent": "In the efficiency of generation that we do not want to lose our behavior too fast, but on the other hand we may lose bad behavior too slow.",
                    "label": 1
                },
                {
                    "sent": "In return, we become relatively safe with the real robot, and we usually end up still in highly rewarded behaviors.",
                    "label": 0
                },
                {
                    "sent": "While this these two of course fighting against this one.",
                    "label": 0
                },
                {
                    "sent": "Somewhat interesting are the initial demonstrations, since they are clearly contradicting the aim of autonomy in generality, since if you always need a good generalization to, sorry, good initialization to get started, then this cannot be giving you totally different solutions.",
                    "label": 0
                },
                {
                    "sent": "I'll give you an example from high jumping until 1964, everybody ran forward, jumped over the target.",
                    "label": 0
                },
                {
                    "sent": "People learned this kind of jumping, but they would never, never came towards the Fosbury flop and will miss the Fosbury came around jump two came from the side, jumped over it backwards.",
                    "label": 0
                },
                {
                    "sent": "Such kind of backwards jumps we cannot do when we have to start from initial demonstration.",
                    "label": 0
                },
                {
                    "sent": "In this case you would need something like a forward model for example.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nevertheless, we are not going to get into forward models today, but rather we are talking about the loss of the tradeoffs.",
                    "label": 0
                },
                {
                    "sent": "The one between in having a large repertoire versus parsimonious behavior represent.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For that we we clearly have now a tradeoff between the versatility he which we can generate with our and we have now many primitives, not just few, not just have one primitive children and the person money which we need in order to learn if.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recently now let's quickly.",
                    "label": 0
                },
                {
                    "sent": "So now for this time reuse, this scenario of the relative entropy policy search just will not only use it for the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Action part, but we will use it in the complete framework with many primitives in.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where the context part gives us a gating network which can activate different primitives which then act as policy's conditioned on this internal gate.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Invariable.",
                    "label": 0
                },
                {
                    "sent": "If you would do this naively, you would end up just putting an additional oh in here and get such kind of a decomp.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vision.",
                    "label": 0
                },
                {
                    "sent": "However, this has a major disadvantage, since you would never with all the primitives, would share the same.",
                    "label": 0
                },
                {
                    "sent": "For example, incoming balls let's.",
                    "label": 0
                },
                {
                    "sent": "And would then focus on one solution, which would work very well together, but it would not be able to.",
                    "label": 0
                },
                {
                    "sent": "Split up the space in such a way that there would be a parsimonious policy arising.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hence we need to form need to force the primitives such that they limit their responsibility.",
                    "label": 0
                },
                {
                    "sent": "This can be done by looking at the entry of the entropy of the of the gating network, and this high entropy here would indicate a high overlap of.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Primatives again the same example you can see in this case very nicely that the two primitives have separated and a covering different parts of the state action space.",
                    "label": 0
                },
                {
                    "sent": "Hence we can learn these fewer these primitives sufficiently also eliminate.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some of them.",
                    "label": 0
                },
                {
                    "sent": "One very quick example.",
                    "label": 0
                },
                {
                    "sent": "There are basically multiple behaviors for solving the same tasks stored in there.",
                    "label": 0
                },
                {
                    "sent": "This is a tetherball task where you have to hit the ball such that it ends up at the at the other side of the pole.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this case, this multi primitive approach yields a good performance.",
                    "label": 1
                },
                {
                    "sent": "Where the spreading out results into quick increase in the reward quicker than if you were looking at me.",
                    "label": 0
                },
                {
                    "sent": "Looking at the naive approach and also at the single primitive approach.",
                    "label": 0
                },
                {
                    "sent": "At the same time, we managed to reduce the primitives to a number which is more reasonable and hence more parsimonious.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've also used the same framework in the context of table tennis, for we started out by having multiple demonstrations of different table tennis strokes.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Subsequently used them in the context of.",
                    "label": 0
                },
                {
                    "sent": "Of reproducing behavior by selecting and generalizing among these parameters with a gating network, and.",
                    "label": 0
                },
                {
                    "sent": "What you see next is how behavior is composed by basically, well, the weights correspond to the gating network.",
                    "label": 0
                },
                {
                    "sent": "Output and well from this by mutation running alone, you can already generate a relatively good policy.",
                    "label": 0
                },
                {
                    "sent": "However, imitation learning is also in table tennis, not enough.",
                    "label": 1
                },
                {
                    "sent": "Hence 69% of the return balls in the task based definitely not enough, so we.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Focused on hitting regions where we had zero percent success before and with the ball gun shot into exactly these regions and use now the reinforcement learning and complete reinforcement in version of the framework and return balls in this context.",
                    "label": 0
                },
                {
                    "sent": "Finally.",
                    "label": 0
                },
                {
                    "sent": "At the end.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This had in the zero percent region score of 79% and returns.",
                    "label": 0
                },
                {
                    "sent": "Finally, hear the robot plays against its creator Katerina.",
                    "label": 0
                },
                {
                    "sent": "Milling is about as good at table tennis as the robots.",
                    "label": 1
                },
                {
                    "sent": "As you can observe from this video.",
                    "label": 0
                },
                {
                    "sent": "She is a computer scientist after all.",
                    "label": 0
                },
                {
                    "sent": "And you see that the robot plays quite well.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would that work here?",
                    "label": 0
                },
                {
                    "sent": "I'll trade off was mainly between the versatility which you really need for the well for high quality behavior versus the parsimony parsimony which you would need in order to learn things if it.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With that I'm at the conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I hope that I've kind of conveyed that we're dealing with a really exciting problem here.",
                    "label": 0
                },
                {
                    "sent": "Motor skill learning and we're having also some tradeoffs, tradeoffs between physical learning and knowledge experience versus high rewards versatility versus parsimony.",
                    "label": 1
                },
                {
                    "sent": "And there are many more.",
                    "label": 0
                },
                {
                    "sent": "We never worked on, hence I did not really discuss them here.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the big things goes to all these people who have interacted with me on when creating all this research yet.",
                    "label": 0
                },
                {
                    "sent": "For that, thank you for your attention and I hope you have lots of questions.",
                    "label": 0
                }
            ]
        }
    }
}