{
    "id": "5psek3kjeuqsoahphzmh4kbpouysrbx2",
    "title": "Analysing Non-stationarity in EEG-BCI",
    "info": {
        "author": [
            "Klaus-Robert M\u00fcller, Department of Software Engineering and Theoretical Computer Science, Technische Universit\u00e4t Berlin"
        ],
        "published": "April 3, 2014",
        "recorded": "February 2014",
        "category": [
            "Top->Technology->Neurotechnology",
            "Top->Medicine"
        ]
    },
    "url": "http://videolectures.net/bbci2014_mueller_non_stationarity/",
    "segmentation": [
        [
            "OK, so I think you you have heard already a lot of things on BC I, but it's still just have a couple of slides where I show you a bit of BCI.",
            "Other than that, my talk will be rather technically.",
            "About non stationary Tees and how to estimate them and what to do with them.",
            "So.",
            "This is joint work with a lot of people.",
            "And the principal components are Benjamin Blankets who's sick.",
            "Gobble cool who's the?",
            "Emdeon to neurologist in RCBC Iteam from charity.",
            "And many other people, and there's even more OK.",
            "So."
        ],
        [
            "Just to show you this this plot again we do non invasive brain computer interfacing and I will.",
            "I'm mainly interested in decoding becausw my.",
            "Anne.",
            "The chair that I have is this chair for machine learning, not for neuroscience, not for computational neuroscience, so I'm really interested in machine learning and I'm not only interested in machine learning in the context of PCI, but also in the context in genetics, web analysis, what you know, all these things that you would do as a machine learning.",
            "Professor.",
            "OK and mainly I I try to also contribute to new algorithms and new theory and you will have to suffer for this today.",
            "An OK so."
        ],
        [
            "We talk about rhythms.",
            "You've seen this slide before.",
            "And you've seen this slide many times.",
            "So.",
            "An where we will be using data mainly from the motor rhythm.",
            "You know this is a lateralized rhythm, if we are moving ahead and then this Idol decider rhythm is suppressed.",
            "So if I'm waving to you, an Idol rhythm is suppressed that is lateralized OK, right and left hemisphere left and right hemisphere.",
            "And also if I imagine that and so you've heard different.",
            "Tutorials about this already, so I don't really have to go into details.",
            "I'm happy I can save some time so this rhythm is also there if I just imagine."
        ],
        [
            "So just to remind you what the typical setup is.",
            "So and you know, I think Benjamin made this point probably before.",
            "That maybe 15 years ago or so.",
            "Before we started working on BCI, it was very common that the subjects would have to train for.",
            "100 or more hours to train the brain signals such that.",
            "Through biofeedback, such that they could be decoded and we tried to turn this around, had had the machines learn.",
            "So now the typical protocol would be that.",
            "Most of the time we have healthy subjects that come to our lab.",
            "There's also some patients studies, but I don't talk about them.",
            "Anne.",
            "So they would come to the lab get.",
            "The EG kept on.",
            "Then they would have to do some training and then go to an online feedback session.",
            "I'm just a bit slow here and I."
        ],
        [
            "I don't know whether you saw this video, but this is the typical training, so where people have to relax it.",
            "This is relax and then they have to assume some brain states.",
            "So left imagination of hand movements.",
            "Left imagination so you can try this.",
            "Relax, focus, do it while the letters there OK squeeze the ball.",
            "Throw up all something.",
            "OK, so you can.",
            "You can see that this is a bit demanding.",
            "You have to think to focus and relax.",
            "It's a very strange state.",
            "Also boring, but that's another story.",
            "So.",
            "I'm showing this to you because it shows.",
            "You know it puts you into this situation of the subject training.",
            "So you now know what it is.",
            "So."
        ],
        [
            "So you know that afterwards we do machine learning.",
            "So we we.",
            "From these cognitive states of left and right hand imaginations we take the matrices time channel and extract, for example through common spatial patterns we get some spatial patterns that allowed to decode the particular cognitive state.",
            "OK, can have spatial temporal patterns.",
            "We can have all sorts of things, but this is not the point of my talk.",
            "This was already discussed before in sessions before."
        ],
        [
            "So you can, if you look at the whole typical BBC I set up this this pre processing this this CSP stuff this artifact removal and this is stuffed into a big feature vector and then you have an LDA or some some mathematical program that finally decodes.",
            "OK, that's set up.",
            "So then you're done."
        ],
        [
            "And you can apply this.",
            "So for example this video shows.",
            "Spelling.",
            "Done it, save it.",
            "I don't recall when it was 2000.",
            "6 maybe I don't recall really, maybe 2004, I don't.",
            "So.",
            "But you can see that that there's a subject, and the subject is now assuming certain cognitive states.",
            "And this is being decoded in real time and what you see here is the nice hexa spell.",
            "An interface that that we came up with together with product Mary Smith's Group.",
            "And the idea here is that the bit of information left versus right imagination is translated into a controlled signal where the cursor is elongated and turned.",
            "So with two choices of these, heck, so it's because of sex spell of these hexagons.",
            "We can actually take any letter in the alphabet.",
            "OK, so this is quite nice and it's also reflecting very well what we know about BCI, namely that our decoding is not perfect.",
            "So if we do an unperfect decoding, the question is what kind of human machine interface are we using?",
            "And what what is good?",
            "What is optimal to reflect this uncertainty?",
            "And, you know, in a sense that the uncertainty is reflected by the design, so we don't have to put the cursor exactly on the on one particular spot.",
            "We can.",
            "We have some leeway here, OK, and if we cannot extend then we try again and then.",
            "OK, so you can see in real time decoding.",
            "After after cognitive state and you can see it.",
            "There's two thresholds and basically.",
            "See that the two states, one state when the the orange errors is all the way up and the other one.",
            "If it's all the way down, OK, so.",
            "So the subject is going to spell dishonest and cook for, which is one of the famous sentences first sentences transferred through the telephone by Rice.",
            "Some physical, but anyway, so the reason why I'm showing this to you is that you recall.",
            "The the training set up relaxed focused OK so this guy is sitting in the crowd of people that are gathering around him at see Bitsy bit's largest computer fair in Germany, which is going to be.",
            "Next next week, I think.",
            "Anne and maybe 40 people standing around.",
            "Maybe one or two camera teams there.",
            "A big noise.",
            "Acoustical noise, but also behind this wall you know behind the screen, this white wall and there's a main power supply of all 8 cable of this size.",
            "OK, so there's huge stray fields.",
            "So this is a real, you know this is PCI in the real world.",
            "OK, so so why am I explaining this so so much?",
            "So on one side we have this very relaxed and focused.",
            "Setting on the other side, we have the real world setting and.",
            "But my claim is between these two, the statistics of the brain Signal X Ray changes.",
            "Because we have more optical flow and in the feedback session without sleep threat.",
            "But there's also this psychological pressure that we have.",
            "And this also the cable that was not there.",
            "Right during training OK?",
            "So.",
            "OK."
        ],
        [
            "So.",
            "If I think about this, I could say, well, you could say, well you have to show right?",
            "So I'm I'm claiming yes there nonstationarity's.",
            "But you know, maybe we can be a bit more quantitative.",
            "How could we actually measure measure this?",
            "And this is not so trivial.",
            "Because you think about it, you have an easy bunch of EG signals.",
            "So you do some CSP.",
            "So you project this down, extract features there, a bit more low dimensional than 64 times some.",
            "You know?",
            "And so, but since they are still high dimensional, so now we have a probability distribution of 1 cognitive state and the probability distribution of another cognitive state in this feature space.",
            "And we go from training too.",
            "To feedback and and the question is, how can we actually measure these probability distributions whether they are on the same spot or whether they are somewhere else, right?",
            "And so this is a bit nontrivial because we have to estimate probability densities in some sense.",
            "And this is a notoriously hard thing, OK?",
            "So we can.",
            "But I can try to visualize this because I don't want to go with you through the art of estimating probability distributions in high dimensions, which is very much beyond anything that this school has in mind.",
            "We can sit on top of our classifier, which is a hyperplane, and we assume that the data of one class and the other class is Gaussian, and this is a quite good assumption.",
            "So we compute the means and covariances.",
            "And then we can see that.",
            "So this line here is where the hyperplane sits, and this is like one class of the one cognitive state and the other class of the other cognitive state.",
            "And now I'm.",
            "Showing you the video of which is the video now not from cbit, but from another experiment.",
            "So which is the video?",
            "Where a subject just does some typical spelling or some typical BCI experiment.",
            "OK, so you can see in this projection.",
            "This looks like hell.",
            "It's all with wiggling around.",
            "So this means that these probability distributions do what they want.",
            "So maybe I nobody so far told you that machine learners typically assume that the distribution underlying the training set and the distribution underlying the test set should be the same.",
            "So if this is not the case, then we don't know how to do machine learning OK.",
            "So here it's clearly not the case, and this is, by the way, not the only kind of application where this is happening, so.",
            "So this is Felix is not there anymore.",
            "So for example, if you if you look at web data OK.",
            "So you may be interested in the latest trends.",
            "OK, what people?",
            "You know I interested in or.",
            "What are the news?",
            "And of course the very definition of this is that there's something changing OK.",
            "So it's not the only BC eyes where things change, OK?"
        ],
        [
            "So I would like to talk about this.",
            "In this tutorial.",
            "So I'm just not sure what was exactly the distribution that you just show.",
            "OK, so I showed the probability distribution in feature space.",
            "So I take the G signal, do the usual thing that you have already learned on day one or two, day two like 2 CSP and then get it to this feature space.",
            "OK, so then every chunk of data that you put through UCSB as a projector, right?",
            "So if you take 3 CSP filters here on the top and on the bottom, and then you.",
            "Run the the continuous EG data through these these filters.",
            "So which means that you for every you know kind of time slice you get.",
            "One point in the six dimensional space in this case OK. And then you plot put all these points.",
            "You know that's a cloud and we know that.",
            "This is approximately Gaussian distributed.",
            "For the left class and for the right class of right and left imagination.",
            "And I'm just what I just showed was.",
            "So we know that these are Gaussians, or we assume that they are goal since at least so we can estimate the mean and the covariance and the mean and covariance are shown as these discussed blobs that moved around and so we can.",
            "We can take a window of estimation.",
            "I don't know in this thing.",
            "This is a couple I don't.",
            "Maybe this was a minute or something like that I have.",
            "I would have to look this up.",
            "This is old, you know, really old paper from 2005, so it's nine years ago so, but we take a reasonable time to estimate these and then we go from step to step and then think it must have been below a minute.",
            "OK, but this is longer experiment so we slide it across the experiment, OK?",
            "Avoid.",
            "I'm just going to discuss it here on this slide.",
            "OK, so.",
            "Of course, the point of this talk is to show you how to avoid this.",
            "OK, but now let's do the baby thing first.",
            "OK, So what everybody knows is when we go from this.",
            "I'm training experiment to feedback.",
            "Then you know if you have done any G experiments, you realize that somehow while in them in the training case you may be able to to move your cursor, or you may be able to to assume some classes.",
            "Then if you then go to feedback into real time feedback, then then all of a sudden.",
            "You know you start to be biased.",
            "Right, so so a very simple thing that you can do is just mean if this is your classifier in.",
            "I mean there should be a sign over this, but this is essentially your classifier access the features.",
            "Doubl is the filter and this threshold.",
            "So basically what people always do, which is you know, taking care of most of the non stationarity is to do an adaptation of the bias OK?",
            "So in other words.",
            "What happens is that during this training phase, the data is here and then there's some kind of.",
            "Bias OK?",
            "So this is all I want to say about this because it's so easy.",
            "You just change the be OK.",
            "So the question is then, So what do you generally do if you do pattern recognition?",
            "If you have a nonstationary data or if you have some.",
            "Non stationary data.",
            "So OK.",
            "Typically this is exactly the question that you had.",
            "Can we not be invariant somehow so so people do invariant pattern recognition?",
            "So they try to come up with some features that that, you know, make the nonstationarity go away, or that make all the variance go away.",
            "OK, so it's.",
            "This is very useful.",
            "For example, if you want to recognize optical characters.",
            "So you think about optical characters like a zero or one?",
            "Then you know sometimes they're written very straight.",
            "Sometimes they have it's slanted, sometimes they're thicker or thinner.",
            "You want to be invariant against all these things, OK?",
            "In the case of optical character recognition, people have thought a lot about this, but in the case of BCI, what are invariants?",
            "Is there but you know this is a bit unclear.",
            "There are many cases where it's a bit unclear, OK?",
            "So ideally we would like to have some magic feature that gets everything into the stationary world, and that's done OK.",
            "But unfortunately we don't have that.",
            "CSP does actually a great deal too to get invariants.",
            "But it's not.",
            "OK, so the next thing that you could do is covariate shift OK. Anne.",
            "So let's.",
            "Think about this model for a moment.",
            "I mean this is more like a theoretical model from statistics, so you assume the following, so you assume that the density of the data.",
            "Between training and test set changes.",
            "But the conditional density doesn't.",
            "So this is a bit strong assumption.",
            "So, but you know, this is what the coverage shift model is about and you can deal with this.",
            "And we will.",
            "See some slides in one minute about that.",
            "Anne.",
            "The other possibility is to say, well, why not put everything on a subspace that is actually stationary.",
            "Which is again trying to get some invariant features.",
            "But then.",
            "I will also talk about other things, but these are the 1st two things on our program."
        ],
        [
            "So if we look at the neurophysiology.",
            "And you look at, you know the.",
            "The basic Maps of training and our feedback, and we take some selected channels on this.",
            "Then we can see that.",
            "You know the feedback is the solid curves and the other ones are the light curves, so you can see that on a single channel basis.",
            "This is an average.",
            "You already see some differences it must pronounce here dozet.",
            "No wonder because you have some optical flow.",
            "When you do feedback at.",
            "But there's also some other differences, so there's a difference pattern that you can.",
            "You can subtract these two, and so there's a clear difference.",
            "OK, and it makes some physiological sense.",
            "So.",
            "Me."
        ],
        [
            "So let's do some math, because I already promised this to you, so I have to keep my promises.",
            "So in the typical let's look at the very simple regression problem.",
            "So you have some kind of basis functions.",
            "You combine them somehow, and that's your function.",
            "OK, that's the kernel Ridge regression say OK. F could be linear as well.",
            "OK.",
            "So your data is is basically why given X plus some noise.",
            "You have this this basis function.",
            "They can be linear.",
            "There can be nonlinear and you try to solve this optimization problem.",
            "So now.",
            "That's just for a second.",
            "Ignore the W and ignore this part over here, OK?",
            "Then you see that you this is like the mean squared error.",
            "That's like just minimizing a quadratic error function.",
            "To get the alphas that are, you know.",
            "Reflecting the data best.",
            "So if you have nonstationarity.",
            "You can prove that this model will be biased.",
            "Which means that no matter how much data you have.",
            "He will always fail to estimate the truth.",
            "OK. Now you can think of how maybe if I regularize ha then I can solve the problem.",
            "No, you can't OK.",
            "But you can if if you doing this thing.",
            "And that's the covariate shift.",
            "So you basically weight the data that you have.",
            "You wait your training data by some factor and this factor is just given by.",
            "Did the ratio between the probability.",
            "Of XI.",
            "Feedback over the probability of XI training.",
            "So in other words.",
            "OK, so we think about it.",
            "So your data is shifted away OK.",
            "So then.",
            "The idea is you may not.",
            "I want to take a data from here to here.",
            "OK, so this is the data that you would have that you would encounter in your test set or in your other scenario.",
            "In the second scenario.",
            "So this means that all this part of the data is not cannot be relevant because this has been.",
            "This is far away from from this type of data, OK?",
            "So the idea of this factor is to say we are waiting.",
            "The these data points in the training set higher.",
            "That are closer that I have a high probability to to be there also in the in the test set OK.",
            "So in other words, if the probability to get a data point in the test set is 0.",
            "You know, because it's here at the edge, it's not here anymore.",
            "Then it's weighted down OK.",
            "In this way we can actually get an unbiased estimator.",
            "So this is all theory.",
            "And the nice thing is that we control that it works also for BCO, not really good because the covariate shift model is a very.",
            "No limiting strict one.",
            "OK, because we assume that the densities change but not the conditionals.",
            "So we actually saw in the data that we had moving around that also the conditionals change a bit.",
            "OK.",
            "So it's actually not the right assumption, but it already works better.",
            "Or would it be used useful to apply and normalization process to the trails?",
            "For example, variance normalization something like this, sorry, standard deviation normalization process, yeah, but OK.",
            "So.",
            "But maybe this time if the densities are not the same right?",
            "Then this means that also you know if I normalize, if I use the training data to normalize.",
            "Then this will not help to get them to match.",
            "So normalization is not a solution, so so can I just say one more thing before I give.",
            "So the interesting thing about this covariate shift model is if you think about your cross validation procedure that you always like to use to choose your model parameters, then of course also cross validation will be biased.",
            "So you have to include a factor.",
            "This factor also in the cross validation procedures.",
            "Then it becomes unbiased.",
            "So it this is, I mean you can read this up.",
            "This is learning theoretical but has also some PCI application.",
            "But again this is not the Golden bullet, it's just I'm just throwing concepts at you.",
            "So you will 1st and then Michael.",
            "Basically this density ratio estimation that is yeah, this density ratio estimation that is.",
            "Means feedback upon the training so that happens only once.",
            "You know your what will be.",
            "Your feedback session will be.",
            "But suppose in the case of testing scenario you don't know what will be your distribution.",
            "So how would you calculate your importance?",
            "Yes, so very true.",
            "So you assume that you have part of the test data already, not just of the densities unlabeled.",
            "OK, so if we don't have that, we cannot apply the model.",
            "OK, OK, good so.",
            "I mean, I should say also it's a bit hairy to to estimate.",
            "The densities in high dimensions.",
            "So a much better thing is to estimate the ratios.",
            "OK, because density ratio estimation is much more stable.",
            "Then density estimation and taking the ratio.",
            "But that's a side remark for the connoisseur."
        ],
        [
            "So OK, now we are at the point where what about taking some projections and getting rid of this nonstationarity?"
        ],
        [
            "OK, so let's take some candidates.",
            "PCA.",
            "Don't think how we can get rid of the nonstationarity with PCA."
        ],
        [
            "I see a.",
            "Same thing.",
            "So.",
            "PCA tricks tries to extract the direction of largest variance.",
            "ICA tries to.",
            "Look for kurtosis or.",
            "Whatever, OK, the millions of variants.",
            "So, so the we can we can think of these algorithms always as projection algorithms that that optimize for some.",
            "I'm kind of loss function in the first case.",
            "What statisticians call this index function?",
            "So in one case it's variance in the other case it's cortosis or other things in like in ICA.",
            "So we came up with the idea in fact."
        ],
        [
            "Polyphon Bruno came up with this idea.",
            "To say, why not take as an index function as a loss function of our projection that we would like to be stationary.",
            "And then we thought this is so simple.",
            "You know somebody has to be have done it right, and so because you know you just start with a linear thing.",
            "First we thought, OK, let's do it linearly, but it turned out that nobody had done it.",
            "It's quite surprising, and so the idea would be that you decompose the data X into, so this is what you measure and you try to estimate it matrix A that actually decomposes this X into one part that is stationary and the other part it is nonstationary.",
            "As simple as that and I'll tell you how."
        ],
        [
            "So.",
            "So this is some D variant data and this is again what I what I had on the last slide and a is now a matrix that has one part of the matrix that projects to this non stick to the stationary part and one to the non stationary part.",
            "Let's say let's define what stationary means.",
            "In this case we take stationarity to be.",
            "I mean.",
            "It's the same.",
            "And covariance is the same OK?",
            "Overtime.",
            "So.",
            "Recall."
        ],
        [
            "Like before, we've seen this before in in, in, in, in, in a sense, in the movie, right?",
            "So the probability distribution is wiggling around if I take several time Windows the distributions change OK.",
            "But we would like to have the means and the covariances be the same this."
        ],
        [
            "Is what we.",
            "This is what the stationarity assumption is about."
        ],
        [
            "Now this is a plot that tries to show everything OK, so.",
            "You get an intuition from that, so assume that you have a stationary, non stationary source in the stationary source.",
            "So I mean just to go back one step.",
            "OK, so there's a stationary subspace and the non stationary subspace they make it may be high dimensional, but this is very hard to draw so we take this non stationary subspace to be 1 dimensional in this stationary to be 1 dimensional.",
            "OK, now.",
            "Can we mix them together?",
            "OK. And look at their.",
            "Distribution.",
            "Overtime.",
            "OK.",
            "So, but so, so let's just.",
            "OK, let's first look at these these two.",
            "Let's look at this non stationary data stream of the stationary data stream.",
            "So if we are taking any kind of window and take.",
            "A point here.",
            "So in time OK and.",
            "Make a dot here this is.",
            "One axis, and this is the second time series is the other axis, so you just it's a 2 dimensional thing, so you have a bunch of dots, right?",
            "So that's the density in this window.",
            "And that's the density in this window.",
            "That's the density in this window, and you can see you know this is very low variance.",
            "And so basically One Direction is completely gone and the variance is only in this signal.",
            "OK, so you can get a feeling for this, so if I now have a signal like that, then this is highly non stationary, because this is wiggling all over the place and so the idea would be to say, can we get a projection?",
            "In this space.",
            "That we project.",
            "Along One Direction.",
            "It's stationary and with project and everything else is nonstationary.",
            "So of course you know the pictures in the sense misleading because it's just two dimensional.",
            "So in this case, by construction, if we project in this direction then this is going to be stationary in this direction it's nonstationary, OK?",
            "It's axis parallel, but if we think about some higher dimensional example then we can project obliquely.",
            "Not along the axis.",
            "And then there may be the stationary space in the non stationary space.",
            "So that's the basic idea.",
            "So this a matrix would be the projector that allows us to get.",
            "You know decompose the data.",
            "Like that OK, if you have an assumption that any space can be decomposed into stationary and non stationary or can you tell looking just at the data that it can be.",
            "Composed into stationary non stationary well, I mean the.",
            "The question so I have a microphone so I don't need to microphone.",
            "So the question.",
            "Is I would like to ask, it's slightly different so assume that the whole thing is non stationary or assume that the whole thing is stationary.",
            "So what will our algorithm do?",
            "And I will come to this.",
            "Please be patient.",
            "OK, so.",
            "This is our model and this is like a blind source separation model, except that we have another index function.",
            "Pro."
        ],
        [
            "That is, how do we get this?",
            "So just to wrap this up, we have a bunch of epochs in data of data points like that and we would like to find some linear subspace such that marginalized projected to the subspace.",
            "It's all the same, it's stationary.",
            "OK, that's"
        ],
        [
            "Stationary projection.",
            "Now we have to be a bit.",
            "Anne.",
            "Clear so we have X = 8 * S, so we have to invert that a. OK. Then to get S. Of course, we don't know a so.",
            "So 8 to the minus one.",
            "Would this be be this projection to the stationary sources or the non stationary sources?",
            "And so this whole thing is the observed data and we would like to get this part.",
            "And estimated so that we get the estimated sources.",
            "Now we can multiply things because we know what the structure of this is, and we know what the structure of this is, and that's what we have OK. OK, so now comes the interesting thing, Whoops."
        ],
        [
            "Indirection.",
            "This must be 0.",
            "OK, why because if we multiply this part.",
            "This multiplies with this one and.",
            "If this is non zero then we also have some non stationary component in this stationary subspace.",
            "So this must be 0.",
            "OK."
        ],
        [
            "It can be arbitrary.",
            "Because if we think about the nonstationary part, it's already nonstationary.",
            "Mixing some state stationary stuff too.",
            "It will still have it.",
            "Non station."
        ],
        [
            "Free.",
            "So.",
            "Same thing.",
            "OK."
        ],
        [
            "Then this this is a technical.",
            "Technicality the question is what can we identify?",
            "We can identify the true non stationary space and the true sources.",
            "Stationary sources but."
        ],
        [
            "Not the true stationary space and then through non stationary sources.",
            "This is a bit technical to get this, but."
        ],
        [
            "OK, so now now about the algorithm.",
            "So the idea is we just.",
            "Take the whole data set.",
            "Split it up into some epochs, compute means and covariance matrices in these epochs.",
            "And then.",
            "The idea is simple, so assume that we have this magic projection that gets everything stationary.",
            "Then the projection applied to each of these epochs should make all their means and covariances queenside OK."
        ],
        [
            "So let's be more former.",
            "So we can either compare.",
            "The Gaussian approximations BB was the projection.",
            "Remember that.",
            "So we have the.",
            "Projected data in dipakai and we take the difference to the projected data either in a PAC, J or because we can.",
            "We are slightly smarter.",
            "We can take the difference to the mean.",
            "Because we can take either all possible combinations with switches.",
            "And squared, if we have an apacs or by some nice trick we can just compare to the mean.",
            "That's an OK.",
            "But that's only an algorithmic OK, so but you're familiar with the KL Divergent.",
            "I've seen it yesterday on slides.",
            "The KL Divergent is the Canonical.",
            "An measure to compare probability distributions.",
            "We could in principle also take some squared error or something like that, but you know, it's a probability distribution, so we should better compare it in the way you should be doing this.",
            "OK, so.",
            "The idea would be to change the projection such that this becomes minimal.",
            "OK."
        ],
        [
            "Now a bit more technical, so that's this is what we had on the last slide.",
            "We can actually do some.",
            "Remove the mean OK?",
            "Without of loss of generality.",
            "Then we can whiten the data.",
            "OK, so we make the data lie on the sphere.",
            "This is typically also done in ICA.",
            "First step, OK. As we talk about in terms of ICA, the ICS means basically work up on 3rd and four order status sticks, and here we are talking about first and 2nd order statistics.",
            "So yeah, thank you for bringing this up because it's a good point.",
            "So I tried to say this in the beginning.",
            "So in ICA we are trying to get.",
            "Optimize a certain index function.",
            "4th order statistics, something right.",
            "Entropy mutual information, you name it.",
            "But here we are doing something different.",
            "We are trying to make things stationary.",
            "That's another index function.",
            "So there's a huge difference between that so.",
            "But this is just some mathematical trick to get rid of this complicated stuff, you know, because we have B sitting all over the place and we're trying to optimize for be OK.",
            "So if we can get the mean 20 if we can get everything on the ball.",
            "Then the only thing that remains as a possible transformation is a rotation.",
            "Because the data is on the ball, the only degree of freedom we have is is a rotation.",
            "So these are now rotation matrices OK?",
            "And then everything can be written nicely in simply OK. And then.",
            "A is basically a concatenation of the whitening matrix and the rotation with the whitening is this.",
            "Matlab's square root matrix.",
            "OK. OK, so.",
            "OK, just go back because just to catch some breath for the next slide.",
            "So now we can start doing great in Descent on this.",
            "And in order say we take some usually G data and we're trying to do gradient descent.",
            "We're looking for this projection B and with the typical amount of data this can take.",
            "A day of computing time.",
            "OK so but there is quite some some structure in this because we're looking for rotation matrices and so.",
            "The this is.",
            "So if we have these complicated optimization problems, we can actually."
        ],
        [
            "Solve them.",
            "In the algebraic group of rotation matrices I don't expect you to understand this at the moment, but the point is the solution that we have has a particular group structure because these are rotation matrices.",
            "So if we take this group structure into account, then everything can be done much more easily, and if we do this then our computation time goes down to less than a second.",
            "OK, so it actually.",
            "If you have very structured optimization problems, then it's always good to go to your local guy who knows about optimization and a particular algebraic optimization to get.",
            "This done OK so.",
            "But this is technical."
        ],
        [
            "Now you can ask how many epochs do you need to be finding this projection.",
            "OK, because you can choose very many apacs you can choose choose very large apacs so.",
            "So what?",
            "What is good?",
            "OK.",
            "So if these are the marginalized Gaussians.",
            "So for example.",
            "If we project in this direction these two Gaussians that are clearly not the same.",
            "They will appear to be the same.",
            "So what I'm just trying to say is.",
            "You know, if we have too few of these covariance matrices, too few of the blocks we will not get the right solution.",
            "There will be some arbitrariness in this.",
            "So in this case, if we have a.",
            "3rd matrix.",
            "Then we cannot.",
            "You know, have this arbitrariness.",
            "OK.",
            "So another word."
        ],
        [
            "We can then we can prove a theorem that says if we have the we need the ND minus large uses the stationary and non stationary.",
            "So this is the whole space and that's the number of stationary directions.",
            "So that's number of nonstationary directions.",
            "Half plus one.",
            "Should be the number of epochs at least.",
            "Then we can.",
            "We can identify this.",
            "Ideally.",
            "Now I'm saying this becausw people apply these things and then they complain that these algorithms don't work.",
            "These algorithms always work under some conditions, so we can only identify if we have enough epochs.",
            "That's my message here.",
            "It's what the theory says."
        ],
        [
            "But we can test this and we can make some toy experiment and basically what the theory says is that that somewhere here should be somewhere here should be the the minimum number of epochs and so you can you can see in this case.",
            "So that's the angle that are the error that we make in approximating the subspace.",
            "So we have a subspace that we try to estimate and we measure the difference between the angle of what we have estimated and the truth.",
            "Because this is a simulation.",
            "So if we have very few epox, then of course we get nonsense because of the we cannot identify.",
            "If we have too many epochs, then we cannot estimate the covariance matrix as well.",
            "So this the truth is somewhere in the middle and the theory.",
            "Kiss you nice hold on this.",
            "So I think in practice it's pretty unknown how many stationary or non stationary sources to expect so, but you always can do even if you have a small number of fatal.",
            "You can make many pokes out of it, so like going more to the right hand side, could we do something with the regularised version of that covariance estimate to solve the small sampler?",
            "OK, so you can do like like like you suggest, but there's a in fact if you download the package on SSA there's some.",
            "Other trick in this that tries to estimate this automatically and that's about the IT uses the algebraic properties of this whole problem, so it's a. OK, so just so this is actually very beautiful from the machine learning POV.",
            "For those of your machine learners, this is interesting for those others just, you know, take a rest for two minutes.",
            "OK, so.",
            "So if you have means and covariances, these are polynomials.",
            "OK, so we are we try.",
            "We can also view the problem from the point of view of polynomials and from algebra.",
            "OK, so think about now the following.",
            "So we have a bunch of epochs and in the Chair Park we have the polynomials that correspond to this epoc mean and covariance.",
            "OK, and then we'd like to make them all be the same.",
            "So this means that that.",
            "There's a certain algebraic structure.",
            "That that we have and we would like to find a solution within this algebraic structure in this polynomial space.",
            "So we're looking in some polynomial ring and the good thing is that there are some beautiful.",
            "Results from algebra that that say that basically describe the system very well, and then they can actually also infer the number of dimensionality that you would expect.",
            "So I leave it at that, OK.",
            "So there's a very nice in long paper if you want to read this that says everything about the algebra.",
            "So just a small question.",
            "The transformation you did.",
            "The beginning to have the means zero and covariance they identity so well.",
            "I mean, the the mean is clear why you can make it 0, but the covariance.",
            "But I'm so I'm not saying in every epoc I'm saying over the whole data, so there's this is sorry there's a.",
            "Degree of freedom that that you have so is so it doesn't matter whether you're in the covariances one or 10,000, you know this.",
            "We have to fix a scale.",
            "OK, but that's."
        ],
        [
            "Move forward so we can now apply this to BCI.",
            "And we can.",
            "So we can do the following trick.",
            "We take the BCI data, apply this stationary subspace analysis.",
            "And go to the stationary subspace.",
            "And classify there.",
            "OK. And hope that it's still that the stationary part is also discriminative.",
            "Which is a hope.",
            "So.",
            "If we don't do SSA in this particular data set this on average you can see that we get 85% classification rates.",
            "And we get a nice reduction with SSA.",
            "This is quite a substantial reduction and.",
            "And we were quite happy about it.",
            "So we can now do some.",
            "Toy simulation this is on this side.",
            "Which is.",
            "We take an EG data set and we start adding to it Alpha so we so additional Alpha power OK.",
            "Some more and more which.",
            "Basically drives the classification nuts because you know it adds a lot of.",
            "At problems so if you do essay then this basically even though you add a lot of Alpha power, this is not really changing a lot, but without SSA this this classifier goes to chance level OK. Now comes the interesting question.",
            "So if you do SSA, you can project to the stationary into the non stationary subspace.",
            "So a. S&AN they are.",
            "You can.",
            "Look at the patterns that they have because it's a linear projection.",
            "Similarly, as you can look at the pattern of CSP and ICA and you name it.",
            "And so we can do this.",
            "For PCI, so we can look at the stationary subspace and we can look at the non stationary subspace and we can see that there's a lot of nonstationarity around the rim.",
            "OK, which is something that we would expect because there's some some moving of the electrodes and stuff like that and drying and so on.",
            "Now we could say, well, OK, let's just remove these electrodes and just do the thing over.",
            "And then we hope that our classification rates go go.",
            "You know?",
            "Become better.",
            "And this is not the case.",
            "Because removing these electrodes would mean that we are ex is we do an axis parallel projection out.",
            "SSA does no axis parallel projection but bleak projections.",
            "It uses all the electrodes.",
            "OK, and so this is actually quite nice.",
            "Now we can.",
            "Continue."
        ],
        [
            "So.",
            "Can just look at some imagined movement data.",
            "Takes a subject data set with 40 subjects left, right foot classification, 125 trials for classes in 88 EG channels so they have some baseline, some motor imagery and some rest.",
            "And this is Michael."
        ],
        [
            "Yeah, I mean.",
            "Now you can ask.",
            "Now you can ask what is.",
            "Changing most.",
            "So so far I've always said hey, it's nice to have this projection to the stationary subspace because we can classify there.",
            "But many people are actually more interested in the nonstationary subspace because this is where for example plasticity and learning happens.",
            "This is where change happens and change can be most interesting to some people.",
            "Also, the people who do trend detection, by the way in the web OK?",
            "So.",
            "Let's do the following setup so we.",
            "We ask the question whether the strongest changes and we concatenate all the trials.",
            "And divide the data into a box.",
            "And then we have we compute the means and covariances as we have done.",
            "So we do our beautiful algebraic optimization thing.",
            "And then.",
            "Anne."
        ],
        [
            "Now we can look at.",
            "Directions sorted by nonstationarity.",
            "And Nonstationarity, and we can compare.",
            "ICA PCA and SSA?",
            "And so clearly because SSA has been constructed to do this, we find the best non stationary directions with SSA.",
            "These are clearly the most non stationary ones.",
            "And then if we look at them then we can see, for example this one.",
            "This direction and essay was a loose electrode.",
            "And this one was some eye movement.",
            "And this one was some muscle activity.",
            "So.",
            "It's interesting because those are the signals that you would expect in the nonstationary part.",
            "Also, I see a has a bunch of non stationary directions also artifacts.",
            "Now we can.",
            "Removed."
        ],
        [
            "Right, we can reject all the artifacts.",
            "Then you know, we still in SSA becausw.",
            "By construction we still have the monster most nonstationary directions nicely sorted, and there we can.",
            "Learn from then we can think about.",
            "Learning plasticity of our experiments, whatever.",
            "OK."
        ],
        [
            "Now.",
            "Anne.",
            "So so.",
            "The point that I would like to make is that.",
            "Of course, PCA and ICA.",
            "This was also one of the questions before, right?",
            "So why not just use them?",
            "So PCA and ICA may find some artifacts, but they're not made to find these nonstationarity's.",
            "If we are interested in nonstationarity's, then we better use SSA.",
            "So if we just look at.",
            "The PCA components.",
            "Sort them by variance and look at the nonstationarity after artifact rejection so we can see that this basically flat OK. Anne.",
            "So the PCA basis is clearly not optimal with respect to non stationarity.",
            "The same thing is happening in the ICA basis.",
            "So you will have the same kind of thing.",
            "So if you compare.",
            "The essay thing next to this you can see that you can find much more non stationary T with these methods and of course it's clear why because you have constructed the algorithm like that."
        ],
        [
            "So now we can.",
            "We can start, you know, classifying these this essay directions.",
            "So here's to say, if we look at now, the data overall, 40 subjects where we extracted 1600 SSA components.",
            "OK of all these subjects.",
            "Then we can look at the degree of non stationarity that we can measure quantitatively.",
            "And then we can see what are these that are very high.",
            "And we can.",
            "Also plot the artifact likelihood so we can see that from a certain point on we just find artifacts.",
            "So, So what we?",
            "We can say is that non stationarity is correlated with artifact likelihood, but we cannot say that it's causal, right?",
            "Just don't want to go into this direction.",
            "OK, so so it's actually also a nice way of getting artifacts.",
            "OK."
        ],
        [
            "So then let's let's look at another thing.",
            "So we have concatenated many trials and did this one study.",
            "Now we are we are looking at something else.",
            "We're looking at.",
            "This is the typical typical trial.",
            "So baseline motor.",
            "Metairie rest OK, and this is.",
            "Many trials and so now we can.",
            "We can look at.",
            "One subject and we can try to see.",
            "So what is most nonstationary during a trial?",
            "OK?",
            "So what do we expect to be most non stationary during a trial where people do motor Metairie?",
            "So what do you expect?",
            "Any idea?",
            "There you go."
        ],
        [
            "So.",
            "These are the most non stationary sources from left and right class.",
            "So we haven't told this.",
            "Our system that this is 2 classes we haven't told them anything.",
            "Just said please find the most non stationary directions.",
            "And we can look at the.",
            "The nice the patterns that are associated to that, and this is quite neat.",
            "Because.",
            "Yeah, it's a linear projection, so it's interpretable."
        ],
        [
            "Everything.",
            "OK, so let's wrap up this part.",
            "So SSA finds stationary and non stationary subspace is question of course that are opened.",
            "What if we have higher order moments or temporal structure?",
            "Maybe we could change the SSA algorithm.",
            "We could put convolution in this there's a wide range of stuff that we can still do.",
            "And then there's like issue of model selection, and I think that from the algebraic point of view we already have quite good solution to this and it's included in our package.",
            "So if you if you look in jam alarm.",
            "Merlos section then you can find the essay toolbox.",
            "Or you can just type essay toolbox and they find it."
        ],
        [
            "OK."
        ],
        [
            "Then Anne.",
            "Let's talk about another aspect of Nonstationarity.",
            "Which is.",
            "Multi."
        ],
        [
            "Almost I would like to make this interpretation so if we have a bunch of different subjects, we can also think of them as being.",
            "You know?",
            "Not being part of 1 distribution, but you know maybe this is 1 big non stationary distributions.",
            "The the number of subjects they they are different right?",
            "And so one if we want to find a universal classifier, then we may have to have two.",
            "Some subject independent PCI.",
            "We may have to deal with this.",
            "So this is another view about Nonstationarity and BCI.",
            "I just want to remind you of that.",
            "And there's different solutions.",
            "Of course, I cannot pull out essay and say this is the solution to that, because that's another story.",
            "OK, so."
        ],
        [
            "I'm just move."
        ],
        [
            "Going forward and."
        ],
        [
            "Skipping this"
        ],
        [
            "So.",
            "Now comes an interesting question, so it's also.",
            "You have seen this video.",
            "And the question is an if you think about different subjects that do in BCI experiment and you think about that distributions.",
            "So quite often, if we want to build universal classifiers, we try to say maybe we can find some aspects some CSP pattern that is universal to all of them, OK?",
            "In that sense, we are trying to see is there anything in this stationary part.",
            "That we can transfer.",
            "So now we're asking the question.",
            "Bit differently, so we are saying is the stationary part or is this non stationary part?",
            "In subjects, is it actually?",
            "Different between subjects?",
            "Or is it similar?",
            "OK.",
            "So."
        ],
        [
            "So.",
            "In other words, if we do BCI experiments.",
            "We do training and we go to feedback.",
            "There's a nonstationarity the question is, is this nonstationarity similar across subject, or is the?",
            "The the way we can decode.",
            "The stationary part is that similar OK?",
            "End.",
            "We can, so in that sense we have a bunch of modalities.",
            "These are the subjects and we look at the changes between training and test set.",
            "Anne and.",
            "So so now we can look at the.",
            "This sub spaces between the subjects that are discriminative.",
            "And the subspaces that are.",
            "Non stationary OK.",
            "So what we find is So what is the discriminative subject space we we're taking a classifier for every subject and look at what is the subspace in this.",
            "Discriminative, part and how strong do they coincide OK?",
            "And we look at the similarity, and there's basically little similarity.",
            "It's .2 and this is as a function of the size of the subspace.",
            "The largest subspace.",
            "Of course, the more similarity there is.",
            "Other for the non stationary subspace and that is quite surprising in the first moment.",
            "So this non stationary subspace that reflects this change from training to.",
            "Feedback session this is actually very similar.",
            ".8 is quite high.",
            "In terms of the space.",
            "So.",
            "Isn't it?",
            "This isn't it bit strange or?",
            "I mean, I, I when I saw this first, I thought what the heck is going on.",
            "So this is like we consider this nonstationary part as nonsense.",
            "It's nuisance.",
            "It's something that prevents us from learning good.",
            "So all of a sudden we can learn from noise.",
            "I thought noise never helps.",
            "At.",
            "So this is."
        ],
        [
            "Interesting, so just to give the picture.",
            "So assume that these are the dimensions.",
            "Then you know for the subject 1, the discriminative subspace is here subject to its here an and so on so forth.",
            "And this upper part is the non stationary subspace.",
            "So the.",
            "That so usually what people are always doing is trying to to transfer this bit of information between the subjects.",
            "We just saw that this, you know doesn't really match well.",
            "So what we're saying is maybe we should better transfer this bit of information in the smart way, OK?",
            "But there was a question, so I don't know if I get it right.",
            "The nonstationary thing I see it as is it and it is the noise and it's the noise or the thing that reflects the change from from training to feedback session.",
            "OK, because there's more optical flow as I pointed out in the first couple of slides, because when I think of the nonstationary thing and I think I think as a source from outside like the magnetic thing and then it would make sense that it is the same always.",
            "I, I think there's it's true, so there's different parts, some that come from outside, but some come also from the very fact that we're trying to.",
            "Go from a training to a feedback session.",
            "OK, so."
        ],
        [
            "Now is the algorithm.",
            "How can we get mileage from this insight?",
            "OK, so now it for every subject I we compute the eigenvectors of this beast here.",
            "So we take the covariance on the training and the covariance on the test set and we take we try to find the PCA of that subspace.",
            "Until Dimension D OK.",
            "So for each subject I will get L eigenvalues.",
            "Get the first L eigenvalues.",
            "Then we aggregate this into a huge matrix and then apply PCA.",
            "And then we can.",
            "Reduce the dimensionality of the nonstationary subspace and then we.",
            "Find a projection operator that is orthogonal to this non stationary subspace that is common to all.",
            "And then we can make our data invariant because we look at the projected the data only orthogonal to this non stationary subspace.",
            "And then we compute the CSP filters just to say this again.",
            "So OK. We have the training and the test covariances.",
            "So we subtract them.",
            "We do an eigen decomposition.",
            "Take the leading L values.",
            "Stack this into a large vector.",
            "And since we know that this is many.",
            "You know, vectors are actually very correlated as we have seen in the plot.",
            "We do a PCA.",
            "And since they're very correlated, we can actually get a very compact subspace.",
            "And to that compact subspace, we want to be orthogonal, OK?",
            "Because that compact subspace reflects the overall.",
            "An nonstationarity of the whole ensemble of subjects.",
            "And then we do CSP OK?",
            "Two slides before you have shown or one side before.",
            "Two slides here."
        ],
        [
            "That the similarity between the nonstationarity nonstationary subspace is higher.",
            "But I see that there's much more variance, probably between users that then in the discriminative subspace.",
            "You are you compensating for that variance by taking these Li vectors?",
            "Or how do you deal with?",
            "That said, I mean, in a sense I'm.",
            "I'm thinking of this as the following.",
            "So you have this.",
            "This data and this other subject, another subject subject, and so all these there's there's considerable variance in this.",
            "And this is the reason why you would finally have to do a PCA and find this.",
            "Robot space.",
            "So that's that would be OK.",
            "I take on it and what's your interpretation of that?",
            "The discriminative subspace is, despite they have a bad similarity.",
            "Have a low variance.",
            "That's a good question.",
            "I'm.",
            "I think that has also maybe it has a bit to do with the with what was also mentioned because there's external nonstationarity's.",
            "And then there's the nonstationary choose between training and and feedback.",
            "So I think that this part is not in the discriminative one.",
            "And this part made very strange strongly, but this is just.",
            "A conjecture OK, OK."
        ],
        [
            "So then we can we call this SCSP and we can see.",
            "OK, so so here's.",
            "Some.",
            "Toyish data set.",
            "So we have a.",
            "Visual cue and training and auditory cue in test or letters in training and moving objects in test.",
            "So there's a clear nonstationarity just to to show this fact, and then we can use standard CSP or.",
            "SS CSP, which is using this transfer of nonstationarity procedure.",
            "Anne.",
            "Yeah, and we can see that there is.",
            "We gain something with the stationary subspace, ESP."
        ],
        [
            "And we can now start looking at the.",
            "The pattern that we find.",
            "With respect to the so we have different different users and we can see.",
            "This unit was activity activity in the occipital and also temporal areas.",
            "That that is.",
            "Mainly responsible for the visual and audio processing, so you would expect to see that in the final classifier, and I think we can see this nice because there's always this this occipital components and also the temporal ones.",
            "OK, so this makes some sense."
        ],
        [
            "If we look at the feature distribution.",
            "This is just the typical.",
            "Training test data projected to some dimensions and you can see that there's a huge difference.",
            "Between the covariances.",
            "And if we do this procedure of the where we subtract this nonstationary space, match is much better.",
            "In other words, the whole whole thing becomes more stationary, which means that anything that you can apply to it will work better."
        ],
        [
            "OK.",
            "So think the.",
            "The idea here is that we try to reduce nonstationarity in data, but we doing this in the multimodal way, meaning that we're trying to make use of what we know from other subjects.",
            "And in some strange way we can.",
            "We can learn from.",
            "This nuisance OK?",
            "Which this is the non stationary subspace.",
            "Which I think is quite nice.",
            "OK."
        ],
        [
            "Continue here and.",
            "This is another.",
            "I mean, it should be faster now, I believe.",
            "So this is a very recent piece of work where we try to put, you know, many of these things together.",
            "Under one roof.",
            "Mathematical proof, of course."
        ],
        [
            "OK, so this is the typical BCI pipeline.",
            "This is CSP."
        ],
        [
            "OK, so.",
            "There's a very interesting theorem that we can prove.",
            "And."
        ],
        [
            "So we know that CSP is non robust so any kind of electron artifacts or muscle activity or something like that will just basically make CSP find nonsensical directions.",
            "So I think this has been written by many people and experienced by many people.",
            "So if you have artifacts and you use ESPN, no good.",
            "OK, so we have to do something against it."
        ],
        [
            "And of course, your machine learner you.",
            "I mean, you do something against it by proving a theorem.",
            "I guess so.",
            "Here's the theorem.",
            "So if W is our.",
            "CS P filter.",
            "Anne.",
            "And this V is a is a matrix.",
            "Then we can decompose this into lightning projection and orthogonal projection and we can show the following.",
            "We can show that the span of the CSP and the span of.",
            "What comes now is the same.",
            "OK, so now comes the interesting thing.",
            "So we've seen the Kullback Leibler divergent before.",
            "So if we take.",
            "Gaussian.",
            "So so OK.",
            "So the first thing is, so we take a projection.",
            "We are searching for a projection V with respect to one class.",
            "And we've.",
            "Also searching for the same projection with respect to the second class such that this becomes far apart.",
            "In other words.",
            "So the callback livelier divergent in SSP.",
            "We try to try to match them.",
            "We try to make them as close as possible.",
            "But in CSP we tried to distinguish between two states.",
            "Would like to make them as far as possible apart.",
            "OK, so now what is the projection in this high dimensional space such that this becomes far as the part?",
            "And we can.",
            "Write this down as as this kind of optimization problem as a cool bug library divergent.",
            "OK, and if we do so then we can show that the final.",
            "Projections that we get are the same as the CSP projections.",
            "OK, So what So what does it mean?",
            "It just means that we have found another odd way of computing CSP.",
            "OK, this is all we have.",
            "However, while you may think just come on, I mean density some some eigenvalue problem and some some relic coefficient, and this this KL divergent.",
            "Anyway, who cares anyway?",
            "Right now there's a.",
            "There's a very fundamental advantage of having this as a KL divergent.",
            "An which is that there has been, you know, loads of beautiful mathematical papers that have taught us how to make this KL divergent's a robust one OK?",
            "So recall in CSP, if we do have our artifacts, this goes wrong OK.",
            "So now we have some mathematical framework in the optimization framework.",
            "Whether we can we have some other things that we can do?"
        ],
        [
            "And there's the beautiful, better divergents.",
            "And I actually stole these slides from Voytek.",
            "M. So he's been very optimistic, so so basically for woytek.",
            "AM.",
            "The use of a bitter divergent which is a robust color grabber divergences.",
            "This is like as illuminating as the kernel trick.",
            "I'm I see whether this finally ends up there, but but this is the killed.",
            "The kale divergent with a little beta in this.",
            "So basically you're waiting the.",
            "You know this distance between puke and queue.",
            "You waiting it by this factor of beta.",
            "Um?",
            "And subtracting something.",
            "Just take this as a mathematical fact, OK?",
            "Not invented by us."
        ],
        [
            "So why would this be interesting?",
            "So robustness is, I think, quite a useful thing.",
            "So if you recall, you have some 1 dimensional data and you estimate the mean, then it's here.",
            "If you have this outlier, so we learn that that it would be nice to actually have something like robust selected the median.",
            "So we can also.",
            "Estimate this is the two men without the outlier, so we could.",
            "Think of estimating the mean also with a bitter divergent.",
            "And the bitter divergents will get something very close to the true mean, because it's waiting down this point, OK?",
            "So just this should give you an intuition.",
            "So what the idea now is?",
            "If we are using the bitter divergents we're not.",
            "Basically looking.",
            "Taking every trial in our BCI experiment similarly.",
            "Important, but we can wait down some artifactual trials, OK?",
            "And I will expand on this a bit."
        ],
        [
            "So just a bit of a wrap up, so this is CSP.",
            "We can prove the similarity of the CSP by this KL divergent CSP.",
            "And now comes a very beautiful little trick.",
            "Which says we can, instead of computing the CSP at the KL divergent, we can also take compute sum over.",
            "The trial was clear care liver chances.",
            "That's a difference.",
            "It's mathematically not the same, but it's approximately the same.",
            "And when we do that then we can take the better divergences because then the beta divergences will allow us to wait down a certain trial.",
            "OK, so this is our program now."
        ],
        [
            "So we can do simulations and show that the beta divergences really, really robust."
        ],
        [
            "But this picture is much nice.",
            "So assume that there is an artifact.",
            "This is typically in data, right in EG data.",
            "So this is what CS P gives us, because there's one electrode that is loose, so CP jumps on this electrode.",
            "Now, if you do the better divergents thing.",
            "That is formally equivalent to CSP recall.",
            "Then we can look at.",
            "A ratio which is the divergance term with artifact and the average divergance term without artifact.",
            "And as a function of beta OK?",
            "And we can see that.",
            "You know there may be.",
            "Some artifacts that that you know are.",
            "A factor of 1000.",
            "Larger.",
            "But then some average divergent steam term without artifact, which means that if you think about the mean, it's like putting a point all the way.",
            "You know on the other side of the city, OK?",
            "So.",
            "But if we are ramping up better the better value of the divergent.",
            "Then all of a sudden this is weighted down almost completely.",
            "OK.",
            "Which then makes a huge difference in."
        ],
        [
            "In robustness, but we can move in one step further so."
        ],
        [
            "With better divergance.",
            "This doesn't happen.",
            "This kind of CSP.",
            "Pattern would not happen.",
            "OK, so that's the good news.",
            "We just waiting it down.",
            "To the extent that this influence will be marginalized."
        ],
        [
            "Now, once we have things in there kind of in written as a as a beta in terms of a divergance, we can start playing with this because we have the mathematical framework too.",
            "You know, this part is the CSP part, but remember we in lots of people have played all sorts of things with CSP.",
            "We have heard in this talk stationary CSP we have, you know, some robust CSP invariances P whatever.",
            "I mean there's a big literature on CSP versions.",
            "So now we can.",
            "We can look at this from the machine learning POV, so we have an error function that is CSP and we can add a regular regularizer.",
            "And then we can.",
            "See whether all these CSP methods can basically be be considered as part of the regularizers and we can see.",
            "See, hey, maybe there are some regularizers that we haven't tried."
        ],
        [
            "So what are typical regularizers?",
            "So, for example, we could have.",
            "A regularizer that regularizes within session changes or between session changes or across subject changes or multi subject stuff.",
            "So big.",
            "Basically all that can be done in one framework.",
            "And."
        ],
        [
            "So you can look at the difference between normal CSP and these divergencies peas and you can see that you get some statistically significant improvements in different scenarios so we can impose within's test set session stationarity between session stationarity across subject stationarity.",
            "So.",
            "So basically you can ask different questions and just implement this as a particular regularizer.",
            "So you can have something stationary.",
            "You can have something smooth.",
            "You can have some whatever you actually asked the data to be."
        ],
        [
            "So.",
            "So this is the.",
            "CSP, when we have training and test and we see that this it says use huge difference and this is with the divergencies P."
        ],
        [
            "Again, and.",
            "If we.",
            "Now look at this particular example that we had before with this one electrode going beserk, right, and we are increasing regularization.",
            "In this case, we are not increasing beta.",
            "But we are increasing regularizer that regularizers towards other subjects.",
            "So we have a bunch of subjects that are doing the same experiment and we say yeah, what we want to have is something that should be a bit more similar to the other subject.",
            "Then we can see that this goes away.",
            "OK, as if we regularize stronger and the nice thing is that we can do cross validation to get the value of this regularization."
        ],
        [
            "So.",
            "The nice thing about what I just showed the divergencies P framework is that it integrates many CSP variants in a different manner, under different regularization terms.",
            "An we have a common optimization method so so in other words, we you know the nice thing about this is."
        ],
        [
            "Is that the optimization that we can do is always the same?",
            "We can do some gradient descent and otherwise we would always have to implement different things for the different CSP algorithms."
        ],
        [
            "So we can.",
            "This way various simply develop new CSP variants.",
            "The divergents trick allows us to make it simultaneously robust.",
            "And.",
            "Voytik is put up a home page which says which is divergentmethods.org where you can find code so you can just.",
            "Download the MATLAB code that this also regularize is implemented and you can just play around.",
            "And there's a review paper that just appeared, or it's actually advanced online that has, you know, quite lengthy descriptions of the whole part of this talk."
        ],
        [
            "An I think I'm I'm I'm going overtime."
        ],
        [
            "As a. I have two.",
            "As a chair I have to stop myself.",
            "Just hard.",
            "And so I think basically what I'm what I.",
            "Try to say and what has been always a common denominator of many of the talks.",
            "Is that machine learning and modern data analysis are very important for BCI and they are important to move the field forward.",
            "I have focused very strongly on technical parts online stationarity.",
            "And try to put this into a, you know joint understanding.",
            "Anne.",
            "And.",
            "This the SSA method that can project to some subspaces we can have.",
            "But we talked about the covariance shift or I mentioned covariate shift.",
            "Class cross validation.",
            "We didn't have time to talk about this.",
            "We didn't have time to talk about this, so.",
            "11 final other remark that I would like to make is like of course we've now seen lots of ways to deal with non stationarity.",
            "Being invariant, projecting out.",
            "Looking at the covariate shift model.",
            "Having some nice divergents framework with appropriate regularization.",
            "We can also track nonstationarity.",
            "And people in invasive studies do this quite quite heavily.",
            "They have some Kalman filters and they they use them.",
            "But this requires the nonstationarity to be rather slow moving, and we've seen quite fast moving nonstationarity's here.",
            "OK, so I think this brings me to the end of my talk.",
            "I thank you for your patience and your stamina to go through this.",
            "You know highly technical stuff that is fun for me.",
            "So thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I think you you have heard already a lot of things on BC I, but it's still just have a couple of slides where I show you a bit of BCI.",
                    "label": 0
                },
                {
                    "sent": "Other than that, my talk will be rather technically.",
                    "label": 0
                },
                {
                    "sent": "About non stationary Tees and how to estimate them and what to do with them.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with a lot of people.",
                    "label": 0
                },
                {
                    "sent": "And the principal components are Benjamin Blankets who's sick.",
                    "label": 0
                },
                {
                    "sent": "Gobble cool who's the?",
                    "label": 0
                },
                {
                    "sent": "Emdeon to neurologist in RCBC Iteam from charity.",
                    "label": 0
                },
                {
                    "sent": "And many other people, and there's even more OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to show you this this plot again we do non invasive brain computer interfacing and I will.",
                    "label": 0
                },
                {
                    "sent": "I'm mainly interested in decoding becausw my.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The chair that I have is this chair for machine learning, not for neuroscience, not for computational neuroscience, so I'm really interested in machine learning and I'm not only interested in machine learning in the context of PCI, but also in the context in genetics, web analysis, what you know, all these things that you would do as a machine learning.",
                    "label": 0
                },
                {
                    "sent": "Professor.",
                    "label": 0
                },
                {
                    "sent": "OK and mainly I I try to also contribute to new algorithms and new theory and you will have to suffer for this today.",
                    "label": 0
                },
                {
                    "sent": "An OK so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We talk about rhythms.",
                    "label": 0
                },
                {
                    "sent": "You've seen this slide before.",
                    "label": 0
                },
                {
                    "sent": "And you've seen this slide many times.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "An where we will be using data mainly from the motor rhythm.",
                    "label": 0
                },
                {
                    "sent": "You know this is a lateralized rhythm, if we are moving ahead and then this Idol decider rhythm is suppressed.",
                    "label": 0
                },
                {
                    "sent": "So if I'm waving to you, an Idol rhythm is suppressed that is lateralized OK, right and left hemisphere left and right hemisphere.",
                    "label": 0
                },
                {
                    "sent": "And also if I imagine that and so you've heard different.",
                    "label": 0
                },
                {
                    "sent": "Tutorials about this already, so I don't really have to go into details.",
                    "label": 0
                },
                {
                    "sent": "I'm happy I can save some time so this rhythm is also there if I just imagine.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to remind you what the typical setup is.",
                    "label": 0
                },
                {
                    "sent": "So and you know, I think Benjamin made this point probably before.",
                    "label": 0
                },
                {
                    "sent": "That maybe 15 years ago or so.",
                    "label": 0
                },
                {
                    "sent": "Before we started working on BCI, it was very common that the subjects would have to train for.",
                    "label": 0
                },
                {
                    "sent": "100 or more hours to train the brain signals such that.",
                    "label": 0
                },
                {
                    "sent": "Through biofeedback, such that they could be decoded and we tried to turn this around, had had the machines learn.",
                    "label": 1
                },
                {
                    "sent": "So now the typical protocol would be that.",
                    "label": 1
                },
                {
                    "sent": "Most of the time we have healthy subjects that come to our lab.",
                    "label": 0
                },
                {
                    "sent": "There's also some patients studies, but I don't talk about them.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So they would come to the lab get.",
                    "label": 0
                },
                {
                    "sent": "The EG kept on.",
                    "label": 0
                },
                {
                    "sent": "Then they would have to do some training and then go to an online feedback session.",
                    "label": 1
                },
                {
                    "sent": "I'm just a bit slow here and I.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't know whether you saw this video, but this is the typical training, so where people have to relax it.",
                    "label": 0
                },
                {
                    "sent": "This is relax and then they have to assume some brain states.",
                    "label": 0
                },
                {
                    "sent": "So left imagination of hand movements.",
                    "label": 0
                },
                {
                    "sent": "Left imagination so you can try this.",
                    "label": 0
                },
                {
                    "sent": "Relax, focus, do it while the letters there OK squeeze the ball.",
                    "label": 0
                },
                {
                    "sent": "Throw up all something.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can.",
                    "label": 0
                },
                {
                    "sent": "You can see that this is a bit demanding.",
                    "label": 0
                },
                {
                    "sent": "You have to think to focus and relax.",
                    "label": 0
                },
                {
                    "sent": "It's a very strange state.",
                    "label": 0
                },
                {
                    "sent": "Also boring, but that's another story.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm showing this to you because it shows.",
                    "label": 0
                },
                {
                    "sent": "You know it puts you into this situation of the subject training.",
                    "label": 0
                },
                {
                    "sent": "So you now know what it is.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you know that afterwards we do machine learning.",
                    "label": 1
                },
                {
                    "sent": "So we we.",
                    "label": 0
                },
                {
                    "sent": "From these cognitive states of left and right hand imaginations we take the matrices time channel and extract, for example through common spatial patterns we get some spatial patterns that allowed to decode the particular cognitive state.",
                    "label": 0
                },
                {
                    "sent": "OK, can have spatial temporal patterns.",
                    "label": 0
                },
                {
                    "sent": "We can have all sorts of things, but this is not the point of my talk.",
                    "label": 0
                },
                {
                    "sent": "This was already discussed before in sessions before.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can, if you look at the whole typical BBC I set up this this pre processing this this CSP stuff this artifact removal and this is stuffed into a big feature vector and then you have an LDA or some some mathematical program that finally decodes.",
                    "label": 0
                },
                {
                    "sent": "OK, that's set up.",
                    "label": 0
                },
                {
                    "sent": "So then you're done.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can apply this.",
                    "label": 0
                },
                {
                    "sent": "So for example this video shows.",
                    "label": 0
                },
                {
                    "sent": "Spelling.",
                    "label": 0
                },
                {
                    "sent": "Done it, save it.",
                    "label": 0
                },
                {
                    "sent": "I don't recall when it was 2000.",
                    "label": 0
                },
                {
                    "sent": "6 maybe I don't recall really, maybe 2004, I don't.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "But you can see that that there's a subject, and the subject is now assuming certain cognitive states.",
                    "label": 0
                },
                {
                    "sent": "And this is being decoded in real time and what you see here is the nice hexa spell.",
                    "label": 0
                },
                {
                    "sent": "An interface that that we came up with together with product Mary Smith's Group.",
                    "label": 0
                },
                {
                    "sent": "And the idea here is that the bit of information left versus right imagination is translated into a controlled signal where the cursor is elongated and turned.",
                    "label": 0
                },
                {
                    "sent": "So with two choices of these, heck, so it's because of sex spell of these hexagons.",
                    "label": 0
                },
                {
                    "sent": "We can actually take any letter in the alphabet.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is quite nice and it's also reflecting very well what we know about BCI, namely that our decoding is not perfect.",
                    "label": 0
                },
                {
                    "sent": "So if we do an unperfect decoding, the question is what kind of human machine interface are we using?",
                    "label": 0
                },
                {
                    "sent": "And what what is good?",
                    "label": 0
                },
                {
                    "sent": "What is optimal to reflect this uncertainty?",
                    "label": 0
                },
                {
                    "sent": "And, you know, in a sense that the uncertainty is reflected by the design, so we don't have to put the cursor exactly on the on one particular spot.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "We have some leeway here, OK, and if we cannot extend then we try again and then.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can see in real time decoding.",
                    "label": 0
                },
                {
                    "sent": "After after cognitive state and you can see it.",
                    "label": 0
                },
                {
                    "sent": "There's two thresholds and basically.",
                    "label": 0
                },
                {
                    "sent": "See that the two states, one state when the the orange errors is all the way up and the other one.",
                    "label": 0
                },
                {
                    "sent": "If it's all the way down, OK, so.",
                    "label": 0
                },
                {
                    "sent": "So the subject is going to spell dishonest and cook for, which is one of the famous sentences first sentences transferred through the telephone by Rice.",
                    "label": 0
                },
                {
                    "sent": "Some physical, but anyway, so the reason why I'm showing this to you is that you recall.",
                    "label": 0
                },
                {
                    "sent": "The the training set up relaxed focused OK so this guy is sitting in the crowd of people that are gathering around him at see Bitsy bit's largest computer fair in Germany, which is going to be.",
                    "label": 0
                },
                {
                    "sent": "Next next week, I think.",
                    "label": 0
                },
                {
                    "sent": "Anne and maybe 40 people standing around.",
                    "label": 0
                },
                {
                    "sent": "Maybe one or two camera teams there.",
                    "label": 0
                },
                {
                    "sent": "A big noise.",
                    "label": 0
                },
                {
                    "sent": "Acoustical noise, but also behind this wall you know behind the screen, this white wall and there's a main power supply of all 8 cable of this size.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's huge stray fields.",
                    "label": 0
                },
                {
                    "sent": "So this is a real, you know this is PCI in the real world.",
                    "label": 0
                },
                {
                    "sent": "OK, so so why am I explaining this so so much?",
                    "label": 0
                },
                {
                    "sent": "So on one side we have this very relaxed and focused.",
                    "label": 0
                },
                {
                    "sent": "Setting on the other side, we have the real world setting and.",
                    "label": 0
                },
                {
                    "sent": "But my claim is between these two, the statistics of the brain Signal X Ray changes.",
                    "label": 0
                },
                {
                    "sent": "Because we have more optical flow and in the feedback session without sleep threat.",
                    "label": 0
                },
                {
                    "sent": "But there's also this psychological pressure that we have.",
                    "label": 0
                },
                {
                    "sent": "And this also the cable that was not there.",
                    "label": 0
                },
                {
                    "sent": "Right during training OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If I think about this, I could say, well, you could say, well you have to show right?",
                    "label": 0
                },
                {
                    "sent": "So I'm I'm claiming yes there nonstationarity's.",
                    "label": 0
                },
                {
                    "sent": "But you know, maybe we can be a bit more quantitative.",
                    "label": 0
                },
                {
                    "sent": "How could we actually measure measure this?",
                    "label": 0
                },
                {
                    "sent": "And this is not so trivial.",
                    "label": 0
                },
                {
                    "sent": "Because you think about it, you have an easy bunch of EG signals.",
                    "label": 0
                },
                {
                    "sent": "So you do some CSP.",
                    "label": 0
                },
                {
                    "sent": "So you project this down, extract features there, a bit more low dimensional than 64 times some.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "And so, but since they are still high dimensional, so now we have a probability distribution of 1 cognitive state and the probability distribution of another cognitive state in this feature space.",
                    "label": 0
                },
                {
                    "sent": "And we go from training too.",
                    "label": 0
                },
                {
                    "sent": "To feedback and and the question is, how can we actually measure these probability distributions whether they are on the same spot or whether they are somewhere else, right?",
                    "label": 0
                },
                {
                    "sent": "And so this is a bit nontrivial because we have to estimate probability densities in some sense.",
                    "label": 0
                },
                {
                    "sent": "And this is a notoriously hard thing, OK?",
                    "label": 0
                },
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "But I can try to visualize this because I don't want to go with you through the art of estimating probability distributions in high dimensions, which is very much beyond anything that this school has in mind.",
                    "label": 0
                },
                {
                    "sent": "We can sit on top of our classifier, which is a hyperplane, and we assume that the data of one class and the other class is Gaussian, and this is a quite good assumption.",
                    "label": 0
                },
                {
                    "sent": "So we compute the means and covariances.",
                    "label": 0
                },
                {
                    "sent": "And then we can see that.",
                    "label": 0
                },
                {
                    "sent": "So this line here is where the hyperplane sits, and this is like one class of the one cognitive state and the other class of the other cognitive state.",
                    "label": 0
                },
                {
                    "sent": "And now I'm.",
                    "label": 0
                },
                {
                    "sent": "Showing you the video of which is the video now not from cbit, but from another experiment.",
                    "label": 0
                },
                {
                    "sent": "So which is the video?",
                    "label": 0
                },
                {
                    "sent": "Where a subject just does some typical spelling or some typical BCI experiment.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can see in this projection.",
                    "label": 0
                },
                {
                    "sent": "This looks like hell.",
                    "label": 0
                },
                {
                    "sent": "It's all with wiggling around.",
                    "label": 0
                },
                {
                    "sent": "So this means that these probability distributions do what they want.",
                    "label": 0
                },
                {
                    "sent": "So maybe I nobody so far told you that machine learners typically assume that the distribution underlying the training set and the distribution underlying the test set should be the same.",
                    "label": 0
                },
                {
                    "sent": "So if this is not the case, then we don't know how to do machine learning OK.",
                    "label": 0
                },
                {
                    "sent": "So here it's clearly not the case, and this is, by the way, not the only kind of application where this is happening, so.",
                    "label": 0
                },
                {
                    "sent": "So this is Felix is not there anymore.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you if you look at web data OK.",
                    "label": 0
                },
                {
                    "sent": "So you may be interested in the latest trends.",
                    "label": 0
                },
                {
                    "sent": "OK, what people?",
                    "label": 0
                },
                {
                    "sent": "You know I interested in or.",
                    "label": 0
                },
                {
                    "sent": "What are the news?",
                    "label": 0
                },
                {
                    "sent": "And of course the very definition of this is that there's something changing OK.",
                    "label": 0
                },
                {
                    "sent": "So it's not the only BC eyes where things change, OK?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I would like to talk about this.",
                    "label": 0
                },
                {
                    "sent": "In this tutorial.",
                    "label": 0
                },
                {
                    "sent": "So I'm just not sure what was exactly the distribution that you just show.",
                    "label": 0
                },
                {
                    "sent": "OK, so I showed the probability distribution in feature space.",
                    "label": 0
                },
                {
                    "sent": "So I take the G signal, do the usual thing that you have already learned on day one or two, day two like 2 CSP and then get it to this feature space.",
                    "label": 0
                },
                {
                    "sent": "OK, so then every chunk of data that you put through UCSB as a projector, right?",
                    "label": 0
                },
                {
                    "sent": "So if you take 3 CSP filters here on the top and on the bottom, and then you.",
                    "label": 0
                },
                {
                    "sent": "Run the the continuous EG data through these these filters.",
                    "label": 0
                },
                {
                    "sent": "So which means that you for every you know kind of time slice you get.",
                    "label": 0
                },
                {
                    "sent": "One point in the six dimensional space in this case OK. And then you plot put all these points.",
                    "label": 0
                },
                {
                    "sent": "You know that's a cloud and we know that.",
                    "label": 0
                },
                {
                    "sent": "This is approximately Gaussian distributed.",
                    "label": 0
                },
                {
                    "sent": "For the left class and for the right class of right and left imagination.",
                    "label": 0
                },
                {
                    "sent": "And I'm just what I just showed was.",
                    "label": 0
                },
                {
                    "sent": "So we know that these are Gaussians, or we assume that they are goal since at least so we can estimate the mean and the covariance and the mean and covariance are shown as these discussed blobs that moved around and so we can.",
                    "label": 0
                },
                {
                    "sent": "We can take a window of estimation.",
                    "label": 0
                },
                {
                    "sent": "I don't know in this thing.",
                    "label": 0
                },
                {
                    "sent": "This is a couple I don't.",
                    "label": 0
                },
                {
                    "sent": "Maybe this was a minute or something like that I have.",
                    "label": 0
                },
                {
                    "sent": "I would have to look this up.",
                    "label": 0
                },
                {
                    "sent": "This is old, you know, really old paper from 2005, so it's nine years ago so, but we take a reasonable time to estimate these and then we go from step to step and then think it must have been below a minute.",
                    "label": 0
                },
                {
                    "sent": "OK, but this is longer experiment so we slide it across the experiment, OK?",
                    "label": 0
                },
                {
                    "sent": "Avoid.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to discuss it here on this slide.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Of course, the point of this talk is to show you how to avoid this.",
                    "label": 0
                },
                {
                    "sent": "OK, but now let's do the baby thing first.",
                    "label": 0
                },
                {
                    "sent": "OK, So what everybody knows is when we go from this.",
                    "label": 0
                },
                {
                    "sent": "I'm training experiment to feedback.",
                    "label": 0
                },
                {
                    "sent": "Then you know if you have done any G experiments, you realize that somehow while in them in the training case you may be able to to move your cursor, or you may be able to to assume some classes.",
                    "label": 0
                },
                {
                    "sent": "Then if you then go to feedback into real time feedback, then then all of a sudden.",
                    "label": 0
                },
                {
                    "sent": "You know you start to be biased.",
                    "label": 0
                },
                {
                    "sent": "Right, so so a very simple thing that you can do is just mean if this is your classifier in.",
                    "label": 0
                },
                {
                    "sent": "I mean there should be a sign over this, but this is essentially your classifier access the features.",
                    "label": 0
                },
                {
                    "sent": "Doubl is the filter and this threshold.",
                    "label": 0
                },
                {
                    "sent": "So basically what people always do, which is you know, taking care of most of the non stationarity is to do an adaptation of the bias OK?",
                    "label": 0
                },
                {
                    "sent": "So in other words.",
                    "label": 0
                },
                {
                    "sent": "What happens is that during this training phase, the data is here and then there's some kind of.",
                    "label": 0
                },
                {
                    "sent": "Bias OK?",
                    "label": 0
                },
                {
                    "sent": "So this is all I want to say about this because it's so easy.",
                    "label": 0
                },
                {
                    "sent": "You just change the be OK.",
                    "label": 0
                },
                {
                    "sent": "So the question is then, So what do you generally do if you do pattern recognition?",
                    "label": 0
                },
                {
                    "sent": "If you have a nonstationary data or if you have some.",
                    "label": 0
                },
                {
                    "sent": "Non stationary data.",
                    "label": 0
                },
                {
                    "sent": "So OK.",
                    "label": 0
                },
                {
                    "sent": "Typically this is exactly the question that you had.",
                    "label": 0
                },
                {
                    "sent": "Can we not be invariant somehow so so people do invariant pattern recognition?",
                    "label": 0
                },
                {
                    "sent": "So they try to come up with some features that that, you know, make the nonstationarity go away, or that make all the variance go away.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's.",
                    "label": 0
                },
                {
                    "sent": "This is very useful.",
                    "label": 0
                },
                {
                    "sent": "For example, if you want to recognize optical characters.",
                    "label": 0
                },
                {
                    "sent": "So you think about optical characters like a zero or one?",
                    "label": 0
                },
                {
                    "sent": "Then you know sometimes they're written very straight.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they have it's slanted, sometimes they're thicker or thinner.",
                    "label": 0
                },
                {
                    "sent": "You want to be invariant against all these things, OK?",
                    "label": 0
                },
                {
                    "sent": "In the case of optical character recognition, people have thought a lot about this, but in the case of BCI, what are invariants?",
                    "label": 0
                },
                {
                    "sent": "Is there but you know this is a bit unclear.",
                    "label": 0
                },
                {
                    "sent": "There are many cases where it's a bit unclear, OK?",
                    "label": 0
                },
                {
                    "sent": "So ideally we would like to have some magic feature that gets everything into the stationary world, and that's done OK.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately we don't have that.",
                    "label": 0
                },
                {
                    "sent": "CSP does actually a great deal too to get invariants.",
                    "label": 0
                },
                {
                    "sent": "But it's not.",
                    "label": 0
                },
                {
                    "sent": "OK, so the next thing that you could do is covariate shift OK. Anne.",
                    "label": 0
                },
                {
                    "sent": "So let's.",
                    "label": 0
                },
                {
                    "sent": "Think about this model for a moment.",
                    "label": 0
                },
                {
                    "sent": "I mean this is more like a theoretical model from statistics, so you assume the following, so you assume that the density of the data.",
                    "label": 0
                },
                {
                    "sent": "Between training and test set changes.",
                    "label": 0
                },
                {
                    "sent": "But the conditional density doesn't.",
                    "label": 0
                },
                {
                    "sent": "So this is a bit strong assumption.",
                    "label": 0
                },
                {
                    "sent": "So, but you know, this is what the coverage shift model is about and you can deal with this.",
                    "label": 0
                },
                {
                    "sent": "And we will.",
                    "label": 0
                },
                {
                    "sent": "See some slides in one minute about that.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The other possibility is to say, well, why not put everything on a subspace that is actually stationary.",
                    "label": 0
                },
                {
                    "sent": "Which is again trying to get some invariant features.",
                    "label": 0
                },
                {
                    "sent": "But then.",
                    "label": 0
                },
                {
                    "sent": "I will also talk about other things, but these are the 1st two things on our program.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we look at the neurophysiology.",
                    "label": 0
                },
                {
                    "sent": "And you look at, you know the.",
                    "label": 0
                },
                {
                    "sent": "The basic Maps of training and our feedback, and we take some selected channels on this.",
                    "label": 0
                },
                {
                    "sent": "Then we can see that.",
                    "label": 0
                },
                {
                    "sent": "You know the feedback is the solid curves and the other ones are the light curves, so you can see that on a single channel basis.",
                    "label": 0
                },
                {
                    "sent": "This is an average.",
                    "label": 0
                },
                {
                    "sent": "You already see some differences it must pronounce here dozet.",
                    "label": 0
                },
                {
                    "sent": "No wonder because you have some optical flow.",
                    "label": 0
                },
                {
                    "sent": "When you do feedback at.",
                    "label": 0
                },
                {
                    "sent": "But there's also some other differences, so there's a difference pattern that you can.",
                    "label": 0
                },
                {
                    "sent": "You can subtract these two, and so there's a clear difference.",
                    "label": 0
                },
                {
                    "sent": "OK, and it makes some physiological sense.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Me.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's do some math, because I already promised this to you, so I have to keep my promises.",
                    "label": 0
                },
                {
                    "sent": "So in the typical let's look at the very simple regression problem.",
                    "label": 0
                },
                {
                    "sent": "So you have some kind of basis functions.",
                    "label": 0
                },
                {
                    "sent": "You combine them somehow, and that's your function.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the kernel Ridge regression say OK. F could be linear as well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So your data is is basically why given X plus some noise.",
                    "label": 0
                },
                {
                    "sent": "You have this this basis function.",
                    "label": 0
                },
                {
                    "sent": "They can be linear.",
                    "label": 0
                },
                {
                    "sent": "There can be nonlinear and you try to solve this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "That's just for a second.",
                    "label": 0
                },
                {
                    "sent": "Ignore the W and ignore this part over here, OK?",
                    "label": 0
                },
                {
                    "sent": "Then you see that you this is like the mean squared error.",
                    "label": 0
                },
                {
                    "sent": "That's like just minimizing a quadratic error function.",
                    "label": 0
                },
                {
                    "sent": "To get the alphas that are, you know.",
                    "label": 0
                },
                {
                    "sent": "Reflecting the data best.",
                    "label": 0
                },
                {
                    "sent": "So if you have nonstationarity.",
                    "label": 0
                },
                {
                    "sent": "You can prove that this model will be biased.",
                    "label": 0
                },
                {
                    "sent": "Which means that no matter how much data you have.",
                    "label": 0
                },
                {
                    "sent": "He will always fail to estimate the truth.",
                    "label": 0
                },
                {
                    "sent": "OK. Now you can think of how maybe if I regularize ha then I can solve the problem.",
                    "label": 0
                },
                {
                    "sent": "No, you can't OK.",
                    "label": 0
                },
                {
                    "sent": "But you can if if you doing this thing.",
                    "label": 0
                },
                {
                    "sent": "And that's the covariate shift.",
                    "label": 1
                },
                {
                    "sent": "So you basically weight the data that you have.",
                    "label": 0
                },
                {
                    "sent": "You wait your training data by some factor and this factor is just given by.",
                    "label": 0
                },
                {
                    "sent": "Did the ratio between the probability.",
                    "label": 0
                },
                {
                    "sent": "Of XI.",
                    "label": 0
                },
                {
                    "sent": "Feedback over the probability of XI training.",
                    "label": 0
                },
                {
                    "sent": "So in other words.",
                    "label": 0
                },
                {
                    "sent": "OK, so we think about it.",
                    "label": 0
                },
                {
                    "sent": "So your data is shifted away OK.",
                    "label": 0
                },
                {
                    "sent": "So then.",
                    "label": 0
                },
                {
                    "sent": "The idea is you may not.",
                    "label": 0
                },
                {
                    "sent": "I want to take a data from here to here.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the data that you would have that you would encounter in your test set or in your other scenario.",
                    "label": 0
                },
                {
                    "sent": "In the second scenario.",
                    "label": 0
                },
                {
                    "sent": "So this means that all this part of the data is not cannot be relevant because this has been.",
                    "label": 0
                },
                {
                    "sent": "This is far away from from this type of data, OK?",
                    "label": 0
                },
                {
                    "sent": "So the idea of this factor is to say we are waiting.",
                    "label": 0
                },
                {
                    "sent": "The these data points in the training set higher.",
                    "label": 0
                },
                {
                    "sent": "That are closer that I have a high probability to to be there also in the in the test set OK.",
                    "label": 0
                },
                {
                    "sent": "So in other words, if the probability to get a data point in the test set is 0.",
                    "label": 0
                },
                {
                    "sent": "You know, because it's here at the edge, it's not here anymore.",
                    "label": 0
                },
                {
                    "sent": "Then it's weighted down OK.",
                    "label": 1
                },
                {
                    "sent": "In this way we can actually get an unbiased estimator.",
                    "label": 0
                },
                {
                    "sent": "So this is all theory.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing is that we control that it works also for BCO, not really good because the covariate shift model is a very.",
                    "label": 0
                },
                {
                    "sent": "No limiting strict one.",
                    "label": 0
                },
                {
                    "sent": "OK, because we assume that the densities change but not the conditionals.",
                    "label": 0
                },
                {
                    "sent": "So we actually saw in the data that we had moving around that also the conditionals change a bit.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So it's actually not the right assumption, but it already works better.",
                    "label": 0
                },
                {
                    "sent": "Or would it be used useful to apply and normalization process to the trails?",
                    "label": 0
                },
                {
                    "sent": "For example, variance normalization something like this, sorry, standard deviation normalization process, yeah, but OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "But maybe this time if the densities are not the same right?",
                    "label": 0
                },
                {
                    "sent": "Then this means that also you know if I normalize, if I use the training data to normalize.",
                    "label": 0
                },
                {
                    "sent": "Then this will not help to get them to match.",
                    "label": 0
                },
                {
                    "sent": "So normalization is not a solution, so so can I just say one more thing before I give.",
                    "label": 0
                },
                {
                    "sent": "So the interesting thing about this covariate shift model is if you think about your cross validation procedure that you always like to use to choose your model parameters, then of course also cross validation will be biased.",
                    "label": 0
                },
                {
                    "sent": "So you have to include a factor.",
                    "label": 0
                },
                {
                    "sent": "This factor also in the cross validation procedures.",
                    "label": 0
                },
                {
                    "sent": "Then it becomes unbiased.",
                    "label": 0
                },
                {
                    "sent": "So it this is, I mean you can read this up.",
                    "label": 0
                },
                {
                    "sent": "This is learning theoretical but has also some PCI application.",
                    "label": 0
                },
                {
                    "sent": "But again this is not the Golden bullet, it's just I'm just throwing concepts at you.",
                    "label": 0
                },
                {
                    "sent": "So you will 1st and then Michael.",
                    "label": 0
                },
                {
                    "sent": "Basically this density ratio estimation that is yeah, this density ratio estimation that is.",
                    "label": 0
                },
                {
                    "sent": "Means feedback upon the training so that happens only once.",
                    "label": 0
                },
                {
                    "sent": "You know your what will be.",
                    "label": 0
                },
                {
                    "sent": "Your feedback session will be.",
                    "label": 0
                },
                {
                    "sent": "But suppose in the case of testing scenario you don't know what will be your distribution.",
                    "label": 0
                },
                {
                    "sent": "So how would you calculate your importance?",
                    "label": 0
                },
                {
                    "sent": "Yes, so very true.",
                    "label": 0
                },
                {
                    "sent": "So you assume that you have part of the test data already, not just of the densities unlabeled.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we don't have that, we cannot apply the model.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, good so.",
                    "label": 0
                },
                {
                    "sent": "I mean, I should say also it's a bit hairy to to estimate.",
                    "label": 0
                },
                {
                    "sent": "The densities in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "So a much better thing is to estimate the ratios.",
                    "label": 0
                },
                {
                    "sent": "OK, because density ratio estimation is much more stable.",
                    "label": 0
                },
                {
                    "sent": "Then density estimation and taking the ratio.",
                    "label": 0
                },
                {
                    "sent": "But that's a side remark for the connoisseur.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK, now we are at the point where what about taking some projections and getting rid of this nonstationarity?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's take some candidates.",
                    "label": 0
                },
                {
                    "sent": "PCA.",
                    "label": 0
                },
                {
                    "sent": "Don't think how we can get rid of the nonstationarity with PCA.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I see a.",
                    "label": 0
                },
                {
                    "sent": "Same thing.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "PCA tricks tries to extract the direction of largest variance.",
                    "label": 0
                },
                {
                    "sent": "ICA tries to.",
                    "label": 0
                },
                {
                    "sent": "Look for kurtosis or.",
                    "label": 0
                },
                {
                    "sent": "Whatever, OK, the millions of variants.",
                    "label": 0
                },
                {
                    "sent": "So, so the we can we can think of these algorithms always as projection algorithms that that optimize for some.",
                    "label": 0
                },
                {
                    "sent": "I'm kind of loss function in the first case.",
                    "label": 0
                },
                {
                    "sent": "What statisticians call this index function?",
                    "label": 0
                },
                {
                    "sent": "So in one case it's variance in the other case it's cortosis or other things in like in ICA.",
                    "label": 0
                },
                {
                    "sent": "So we came up with the idea in fact.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Polyphon Bruno came up with this idea.",
                    "label": 0
                },
                {
                    "sent": "To say, why not take as an index function as a loss function of our projection that we would like to be stationary.",
                    "label": 0
                },
                {
                    "sent": "And then we thought this is so simple.",
                    "label": 0
                },
                {
                    "sent": "You know somebody has to be have done it right, and so because you know you just start with a linear thing.",
                    "label": 0
                },
                {
                    "sent": "First we thought, OK, let's do it linearly, but it turned out that nobody had done it.",
                    "label": 0
                },
                {
                    "sent": "It's quite surprising, and so the idea would be that you decompose the data X into, so this is what you measure and you try to estimate it matrix A that actually decomposes this X into one part that is stationary and the other part it is nonstationary.",
                    "label": 0
                },
                {
                    "sent": "As simple as that and I'll tell you how.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this is some D variant data and this is again what I what I had on the last slide and a is now a matrix that has one part of the matrix that projects to this non stick to the stationary part and one to the non stationary part.",
                    "label": 0
                },
                {
                    "sent": "Let's say let's define what stationary means.",
                    "label": 0
                },
                {
                    "sent": "In this case we take stationarity to be.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "It's the same.",
                    "label": 0
                },
                {
                    "sent": "And covariance is the same OK?",
                    "label": 0
                },
                {
                    "sent": "Overtime.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Recall.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like before, we've seen this before in in, in, in, in, in a sense, in the movie, right?",
                    "label": 0
                },
                {
                    "sent": "So the probability distribution is wiggling around if I take several time Windows the distributions change OK.",
                    "label": 0
                },
                {
                    "sent": "But we would like to have the means and the covariances be the same this.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is what we.",
                    "label": 0
                },
                {
                    "sent": "This is what the stationarity assumption is about.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now this is a plot that tries to show everything OK, so.",
                    "label": 0
                },
                {
                    "sent": "You get an intuition from that, so assume that you have a stationary, non stationary source in the stationary source.",
                    "label": 0
                },
                {
                    "sent": "So I mean just to go back one step.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a stationary subspace and the non stationary subspace they make it may be high dimensional, but this is very hard to draw so we take this non stationary subspace to be 1 dimensional in this stationary to be 1 dimensional.",
                    "label": 0
                },
                {
                    "sent": "OK, now.",
                    "label": 0
                },
                {
                    "sent": "Can we mix them together?",
                    "label": 0
                },
                {
                    "sent": "OK. And look at their.",
                    "label": 0
                },
                {
                    "sent": "Distribution.",
                    "label": 0
                },
                {
                    "sent": "Overtime.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So, but so, so let's just.",
                    "label": 0
                },
                {
                    "sent": "OK, let's first look at these these two.",
                    "label": 0
                },
                {
                    "sent": "Let's look at this non stationary data stream of the stationary data stream.",
                    "label": 0
                },
                {
                    "sent": "So if we are taking any kind of window and take.",
                    "label": 0
                },
                {
                    "sent": "A point here.",
                    "label": 0
                },
                {
                    "sent": "So in time OK and.",
                    "label": 0
                },
                {
                    "sent": "Make a dot here this is.",
                    "label": 0
                },
                {
                    "sent": "One axis, and this is the second time series is the other axis, so you just it's a 2 dimensional thing, so you have a bunch of dots, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the density in this window.",
                    "label": 0
                },
                {
                    "sent": "And that's the density in this window.",
                    "label": 0
                },
                {
                    "sent": "That's the density in this window, and you can see you know this is very low variance.",
                    "label": 0
                },
                {
                    "sent": "And so basically One Direction is completely gone and the variance is only in this signal.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can get a feeling for this, so if I now have a signal like that, then this is highly non stationary, because this is wiggling all over the place and so the idea would be to say, can we get a projection?",
                    "label": 0
                },
                {
                    "sent": "In this space.",
                    "label": 0
                },
                {
                    "sent": "That we project.",
                    "label": 0
                },
                {
                    "sent": "Along One Direction.",
                    "label": 0
                },
                {
                    "sent": "It's stationary and with project and everything else is nonstationary.",
                    "label": 1
                },
                {
                    "sent": "So of course you know the pictures in the sense misleading because it's just two dimensional.",
                    "label": 0
                },
                {
                    "sent": "So in this case, by construction, if we project in this direction then this is going to be stationary in this direction it's nonstationary, OK?",
                    "label": 0
                },
                {
                    "sent": "It's axis parallel, but if we think about some higher dimensional example then we can project obliquely.",
                    "label": 0
                },
                {
                    "sent": "Not along the axis.",
                    "label": 0
                },
                {
                    "sent": "And then there may be the stationary space in the non stationary space.",
                    "label": 0
                },
                {
                    "sent": "So that's the basic idea.",
                    "label": 0
                },
                {
                    "sent": "So this a matrix would be the projector that allows us to get.",
                    "label": 0
                },
                {
                    "sent": "You know decompose the data.",
                    "label": 0
                },
                {
                    "sent": "Like that OK, if you have an assumption that any space can be decomposed into stationary and non stationary or can you tell looking just at the data that it can be.",
                    "label": 0
                },
                {
                    "sent": "Composed into stationary non stationary well, I mean the.",
                    "label": 1
                },
                {
                    "sent": "The question so I have a microphone so I don't need to microphone.",
                    "label": 0
                },
                {
                    "sent": "So the question.",
                    "label": 0
                },
                {
                    "sent": "Is I would like to ask, it's slightly different so assume that the whole thing is non stationary or assume that the whole thing is stationary.",
                    "label": 0
                },
                {
                    "sent": "So what will our algorithm do?",
                    "label": 0
                },
                {
                    "sent": "And I will come to this.",
                    "label": 0
                },
                {
                    "sent": "Please be patient.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This is our model and this is like a blind source separation model, except that we have another index function.",
                    "label": 0
                },
                {
                    "sent": "Pro.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is, how do we get this?",
                    "label": 0
                },
                {
                    "sent": "So just to wrap this up, we have a bunch of epochs in data of data points like that and we would like to find some linear subspace such that marginalized projected to the subspace.",
                    "label": 0
                },
                {
                    "sent": "It's all the same, it's stationary.",
                    "label": 0
                },
                {
                    "sent": "OK, that's",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stationary projection.",
                    "label": 0
                },
                {
                    "sent": "Now we have to be a bit.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Clear so we have X = 8 * S, so we have to invert that a. OK. Then to get S. Of course, we don't know a so.",
                    "label": 0
                },
                {
                    "sent": "So 8 to the minus one.",
                    "label": 0
                },
                {
                    "sent": "Would this be be this projection to the stationary sources or the non stationary sources?",
                    "label": 0
                },
                {
                    "sent": "And so this whole thing is the observed data and we would like to get this part.",
                    "label": 0
                },
                {
                    "sent": "And estimated so that we get the estimated sources.",
                    "label": 0
                },
                {
                    "sent": "Now we can multiply things because we know what the structure of this is, and we know what the structure of this is, and that's what we have OK. OK, so now comes the interesting thing, Whoops.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Indirection.",
                    "label": 0
                },
                {
                    "sent": "This must be 0.",
                    "label": 0
                },
                {
                    "sent": "OK, why because if we multiply this part.",
                    "label": 0
                },
                {
                    "sent": "This multiplies with this one and.",
                    "label": 0
                },
                {
                    "sent": "If this is non zero then we also have some non stationary component in this stationary subspace.",
                    "label": 0
                },
                {
                    "sent": "So this must be 0.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It can be arbitrary.",
                    "label": 0
                },
                {
                    "sent": "Because if we think about the nonstationary part, it's already nonstationary.",
                    "label": 0
                },
                {
                    "sent": "Mixing some state stationary stuff too.",
                    "label": 0
                },
                {
                    "sent": "It will still have it.",
                    "label": 0
                },
                {
                    "sent": "Non station.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Free.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Same thing.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then this this is a technical.",
                    "label": 0
                },
                {
                    "sent": "Technicality the question is what can we identify?",
                    "label": 0
                },
                {
                    "sent": "We can identify the true non stationary space and the true sources.",
                    "label": 0
                },
                {
                    "sent": "Stationary sources but.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not the true stationary space and then through non stationary sources.",
                    "label": 0
                },
                {
                    "sent": "This is a bit technical to get this, but.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now now about the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the idea is we just.",
                    "label": 0
                },
                {
                    "sent": "Take the whole data set.",
                    "label": 0
                },
                {
                    "sent": "Split it up into some epochs, compute means and covariance matrices in these epochs.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "The idea is simple, so assume that we have this magic projection that gets everything stationary.",
                    "label": 0
                },
                {
                    "sent": "Then the projection applied to each of these epochs should make all their means and covariances queenside OK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's be more former.",
                    "label": 0
                },
                {
                    "sent": "So we can either compare.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian approximations BB was the projection.",
                    "label": 0
                },
                {
                    "sent": "Remember that.",
                    "label": 0
                },
                {
                    "sent": "So we have the.",
                    "label": 0
                },
                {
                    "sent": "Projected data in dipakai and we take the difference to the projected data either in a PAC, J or because we can.",
                    "label": 0
                },
                {
                    "sent": "We are slightly smarter.",
                    "label": 0
                },
                {
                    "sent": "We can take the difference to the mean.",
                    "label": 0
                },
                {
                    "sent": "Because we can take either all possible combinations with switches.",
                    "label": 0
                },
                {
                    "sent": "And squared, if we have an apacs or by some nice trick we can just compare to the mean.",
                    "label": 0
                },
                {
                    "sent": "That's an OK.",
                    "label": 0
                },
                {
                    "sent": "But that's only an algorithmic OK, so but you're familiar with the KL Divergent.",
                    "label": 0
                },
                {
                    "sent": "I've seen it yesterday on slides.",
                    "label": 0
                },
                {
                    "sent": "The KL Divergent is the Canonical.",
                    "label": 0
                },
                {
                    "sent": "An measure to compare probability distributions.",
                    "label": 0
                },
                {
                    "sent": "We could in principle also take some squared error or something like that, but you know, it's a probability distribution, so we should better compare it in the way you should be doing this.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The idea would be to change the projection such that this becomes minimal.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now a bit more technical, so that's this is what we had on the last slide.",
                    "label": 0
                },
                {
                    "sent": "We can actually do some.",
                    "label": 0
                },
                {
                    "sent": "Remove the mean OK?",
                    "label": 0
                },
                {
                    "sent": "Without of loss of generality.",
                    "label": 0
                },
                {
                    "sent": "Then we can whiten the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so we make the data lie on the sphere.",
                    "label": 0
                },
                {
                    "sent": "This is typically also done in ICA.",
                    "label": 0
                },
                {
                    "sent": "First step, OK. As we talk about in terms of ICA, the ICS means basically work up on 3rd and four order status sticks, and here we are talking about first and 2nd order statistics.",
                    "label": 0
                },
                {
                    "sent": "So yeah, thank you for bringing this up because it's a good point.",
                    "label": 0
                },
                {
                    "sent": "So I tried to say this in the beginning.",
                    "label": 0
                },
                {
                    "sent": "So in ICA we are trying to get.",
                    "label": 0
                },
                {
                    "sent": "Optimize a certain index function.",
                    "label": 0
                },
                {
                    "sent": "4th order statistics, something right.",
                    "label": 0
                },
                {
                    "sent": "Entropy mutual information, you name it.",
                    "label": 0
                },
                {
                    "sent": "But here we are doing something different.",
                    "label": 0
                },
                {
                    "sent": "We are trying to make things stationary.",
                    "label": 0
                },
                {
                    "sent": "That's another index function.",
                    "label": 0
                },
                {
                    "sent": "So there's a huge difference between that so.",
                    "label": 0
                },
                {
                    "sent": "But this is just some mathematical trick to get rid of this complicated stuff, you know, because we have B sitting all over the place and we're trying to optimize for be OK.",
                    "label": 0
                },
                {
                    "sent": "So if we can get the mean 20 if we can get everything on the ball.",
                    "label": 0
                },
                {
                    "sent": "Then the only thing that remains as a possible transformation is a rotation.",
                    "label": 0
                },
                {
                    "sent": "Because the data is on the ball, the only degree of freedom we have is is a rotation.",
                    "label": 0
                },
                {
                    "sent": "So these are now rotation matrices OK?",
                    "label": 0
                },
                {
                    "sent": "And then everything can be written nicely in simply OK. And then.",
                    "label": 0
                },
                {
                    "sent": "A is basically a concatenation of the whitening matrix and the rotation with the whitening is this.",
                    "label": 0
                },
                {
                    "sent": "Matlab's square root matrix.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, so.",
                    "label": 0
                },
                {
                    "sent": "OK, just go back because just to catch some breath for the next slide.",
                    "label": 0
                },
                {
                    "sent": "So now we can start doing great in Descent on this.",
                    "label": 0
                },
                {
                    "sent": "And in order say we take some usually G data and we're trying to do gradient descent.",
                    "label": 0
                },
                {
                    "sent": "We're looking for this projection B and with the typical amount of data this can take.",
                    "label": 0
                },
                {
                    "sent": "A day of computing time.",
                    "label": 0
                },
                {
                    "sent": "OK so but there is quite some some structure in this because we're looking for rotation matrices and so.",
                    "label": 0
                },
                {
                    "sent": "The this is.",
                    "label": 0
                },
                {
                    "sent": "So if we have these complicated optimization problems, we can actually.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solve them.",
                    "label": 0
                },
                {
                    "sent": "In the algebraic group of rotation matrices I don't expect you to understand this at the moment, but the point is the solution that we have has a particular group structure because these are rotation matrices.",
                    "label": 0
                },
                {
                    "sent": "So if we take this group structure into account, then everything can be done much more easily, and if we do this then our computation time goes down to less than a second.",
                    "label": 0
                },
                {
                    "sent": "OK, so it actually.",
                    "label": 0
                },
                {
                    "sent": "If you have very structured optimization problems, then it's always good to go to your local guy who knows about optimization and a particular algebraic optimization to get.",
                    "label": 0
                },
                {
                    "sent": "This done OK so.",
                    "label": 0
                },
                {
                    "sent": "But this is technical.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now you can ask how many epochs do you need to be finding this projection.",
                    "label": 0
                },
                {
                    "sent": "OK, because you can choose very many apacs you can choose choose very large apacs so.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "What is good?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So if these are the marginalized Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "If we project in this direction these two Gaussians that are clearly not the same.",
                    "label": 0
                },
                {
                    "sent": "They will appear to be the same.",
                    "label": 0
                },
                {
                    "sent": "So what I'm just trying to say is.",
                    "label": 0
                },
                {
                    "sent": "You know, if we have too few of these covariance matrices, too few of the blocks we will not get the right solution.",
                    "label": 0
                },
                {
                    "sent": "There will be some arbitrariness in this.",
                    "label": 0
                },
                {
                    "sent": "So in this case, if we have a.",
                    "label": 0
                },
                {
                    "sent": "3rd matrix.",
                    "label": 0
                },
                {
                    "sent": "Then we cannot.",
                    "label": 0
                },
                {
                    "sent": "You know, have this arbitrariness.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So another word.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can then we can prove a theorem that says if we have the we need the ND minus large uses the stationary and non stationary.",
                    "label": 0
                },
                {
                    "sent": "So this is the whole space and that's the number of stationary directions.",
                    "label": 0
                },
                {
                    "sent": "So that's number of nonstationary directions.",
                    "label": 0
                },
                {
                    "sent": "Half plus one.",
                    "label": 0
                },
                {
                    "sent": "Should be the number of epochs at least.",
                    "label": 0
                },
                {
                    "sent": "Then we can.",
                    "label": 0
                },
                {
                    "sent": "We can identify this.",
                    "label": 0
                },
                {
                    "sent": "Ideally.",
                    "label": 0
                },
                {
                    "sent": "Now I'm saying this becausw people apply these things and then they complain that these algorithms don't work.",
                    "label": 0
                },
                {
                    "sent": "These algorithms always work under some conditions, so we can only identify if we have enough epochs.",
                    "label": 0
                },
                {
                    "sent": "That's my message here.",
                    "label": 0
                },
                {
                    "sent": "It's what the theory says.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we can test this and we can make some toy experiment and basically what the theory says is that that somewhere here should be somewhere here should be the the minimum number of epochs and so you can you can see in this case.",
                    "label": 0
                },
                {
                    "sent": "So that's the angle that are the error that we make in approximating the subspace.",
                    "label": 0
                },
                {
                    "sent": "So we have a subspace that we try to estimate and we measure the difference between the angle of what we have estimated and the truth.",
                    "label": 0
                },
                {
                    "sent": "Because this is a simulation.",
                    "label": 0
                },
                {
                    "sent": "So if we have very few epox, then of course we get nonsense because of the we cannot identify.",
                    "label": 0
                },
                {
                    "sent": "If we have too many epochs, then we cannot estimate the covariance matrix as well.",
                    "label": 0
                },
                {
                    "sent": "So this the truth is somewhere in the middle and the theory.",
                    "label": 0
                },
                {
                    "sent": "Kiss you nice hold on this.",
                    "label": 0
                },
                {
                    "sent": "So I think in practice it's pretty unknown how many stationary or non stationary sources to expect so, but you always can do even if you have a small number of fatal.",
                    "label": 0
                },
                {
                    "sent": "You can make many pokes out of it, so like going more to the right hand side, could we do something with the regularised version of that covariance estimate to solve the small sampler?",
                    "label": 0
                },
                {
                    "sent": "OK, so you can do like like like you suggest, but there's a in fact if you download the package on SSA there's some.",
                    "label": 0
                },
                {
                    "sent": "Other trick in this that tries to estimate this automatically and that's about the IT uses the algebraic properties of this whole problem, so it's a. OK, so just so this is actually very beautiful from the machine learning POV.",
                    "label": 0
                },
                {
                    "sent": "For those of your machine learners, this is interesting for those others just, you know, take a rest for two minutes.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So if you have means and covariances, these are polynomials.",
                    "label": 0
                },
                {
                    "sent": "OK, so we are we try.",
                    "label": 0
                },
                {
                    "sent": "We can also view the problem from the point of view of polynomials and from algebra.",
                    "label": 0
                },
                {
                    "sent": "OK, so think about now the following.",
                    "label": 0
                },
                {
                    "sent": "So we have a bunch of epochs and in the Chair Park we have the polynomials that correspond to this epoc mean and covariance.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we'd like to make them all be the same.",
                    "label": 0
                },
                {
                    "sent": "So this means that that.",
                    "label": 0
                },
                {
                    "sent": "There's a certain algebraic structure.",
                    "label": 0
                },
                {
                    "sent": "That that we have and we would like to find a solution within this algebraic structure in this polynomial space.",
                    "label": 0
                },
                {
                    "sent": "So we're looking in some polynomial ring and the good thing is that there are some beautiful.",
                    "label": 0
                },
                {
                    "sent": "Results from algebra that that say that basically describe the system very well, and then they can actually also infer the number of dimensionality that you would expect.",
                    "label": 0
                },
                {
                    "sent": "So I leave it at that, OK.",
                    "label": 0
                },
                {
                    "sent": "So there's a very nice in long paper if you want to read this that says everything about the algebra.",
                    "label": 0
                },
                {
                    "sent": "So just a small question.",
                    "label": 0
                },
                {
                    "sent": "The transformation you did.",
                    "label": 0
                },
                {
                    "sent": "The beginning to have the means zero and covariance they identity so well.",
                    "label": 0
                },
                {
                    "sent": "I mean, the the mean is clear why you can make it 0, but the covariance.",
                    "label": 0
                },
                {
                    "sent": "But I'm so I'm not saying in every epoc I'm saying over the whole data, so there's this is sorry there's a.",
                    "label": 0
                },
                {
                    "sent": "Degree of freedom that that you have so is so it doesn't matter whether you're in the covariances one or 10,000, you know this.",
                    "label": 0
                },
                {
                    "sent": "We have to fix a scale.",
                    "label": 0
                },
                {
                    "sent": "OK, but that's.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Move forward so we can now apply this to BCI.",
                    "label": 0
                },
                {
                    "sent": "And we can.",
                    "label": 0
                },
                {
                    "sent": "So we can do the following trick.",
                    "label": 0
                },
                {
                    "sent": "We take the BCI data, apply this stationary subspace analysis.",
                    "label": 0
                },
                {
                    "sent": "And go to the stationary subspace.",
                    "label": 0
                },
                {
                    "sent": "And classify there.",
                    "label": 0
                },
                {
                    "sent": "OK. And hope that it's still that the stationary part is also discriminative.",
                    "label": 0
                },
                {
                    "sent": "Which is a hope.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we don't do SSA in this particular data set this on average you can see that we get 85% classification rates.",
                    "label": 0
                },
                {
                    "sent": "And we get a nice reduction with SSA.",
                    "label": 0
                },
                {
                    "sent": "This is quite a substantial reduction and.",
                    "label": 0
                },
                {
                    "sent": "And we were quite happy about it.",
                    "label": 0
                },
                {
                    "sent": "So we can now do some.",
                    "label": 0
                },
                {
                    "sent": "Toy simulation this is on this side.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "We take an EG data set and we start adding to it Alpha so we so additional Alpha power OK.",
                    "label": 0
                },
                {
                    "sent": "Some more and more which.",
                    "label": 0
                },
                {
                    "sent": "Basically drives the classification nuts because you know it adds a lot of.",
                    "label": 0
                },
                {
                    "sent": "At problems so if you do essay then this basically even though you add a lot of Alpha power, this is not really changing a lot, but without SSA this this classifier goes to chance level OK. Now comes the interesting question.",
                    "label": 0
                },
                {
                    "sent": "So if you do SSA, you can project to the stationary into the non stationary subspace.",
                    "label": 0
                },
                {
                    "sent": "So a. S&AN they are.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "Look at the patterns that they have because it's a linear projection.",
                    "label": 0
                },
                {
                    "sent": "Similarly, as you can look at the pattern of CSP and ICA and you name it.",
                    "label": 0
                },
                {
                    "sent": "And so we can do this.",
                    "label": 0
                },
                {
                    "sent": "For PCI, so we can look at the stationary subspace and we can look at the non stationary subspace and we can see that there's a lot of nonstationarity around the rim.",
                    "label": 0
                },
                {
                    "sent": "OK, which is something that we would expect because there's some some moving of the electrodes and stuff like that and drying and so on.",
                    "label": 0
                },
                {
                    "sent": "Now we could say, well, OK, let's just remove these electrodes and just do the thing over.",
                    "label": 0
                },
                {
                    "sent": "And then we hope that our classification rates go go.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "Become better.",
                    "label": 0
                },
                {
                    "sent": "And this is not the case.",
                    "label": 0
                },
                {
                    "sent": "Because removing these electrodes would mean that we are ex is we do an axis parallel projection out.",
                    "label": 0
                },
                {
                    "sent": "SSA does no axis parallel projection but bleak projections.",
                    "label": 0
                },
                {
                    "sent": "It uses all the electrodes.",
                    "label": 0
                },
                {
                    "sent": "OK, and so this is actually quite nice.",
                    "label": 0
                },
                {
                    "sent": "Now we can.",
                    "label": 0
                },
                {
                    "sent": "Continue.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Can just look at some imagined movement data.",
                    "label": 0
                },
                {
                    "sent": "Takes a subject data set with 40 subjects left, right foot classification, 125 trials for classes in 88 EG channels so they have some baseline, some motor imagery and some rest.",
                    "label": 0
                },
                {
                    "sent": "And this is Michael.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, I mean.",
                    "label": 0
                },
                {
                    "sent": "Now you can ask.",
                    "label": 0
                },
                {
                    "sent": "Now you can ask what is.",
                    "label": 0
                },
                {
                    "sent": "Changing most.",
                    "label": 0
                },
                {
                    "sent": "So so far I've always said hey, it's nice to have this projection to the stationary subspace because we can classify there.",
                    "label": 0
                },
                {
                    "sent": "But many people are actually more interested in the nonstationary subspace because this is where for example plasticity and learning happens.",
                    "label": 0
                },
                {
                    "sent": "This is where change happens and change can be most interesting to some people.",
                    "label": 0
                },
                {
                    "sent": "Also, the people who do trend detection, by the way in the web OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's do the following setup so we.",
                    "label": 0
                },
                {
                    "sent": "We ask the question whether the strongest changes and we concatenate all the trials.",
                    "label": 1
                },
                {
                    "sent": "And divide the data into a box.",
                    "label": 0
                },
                {
                    "sent": "And then we have we compute the means and covariances as we have done.",
                    "label": 0
                },
                {
                    "sent": "So we do our beautiful algebraic optimization thing.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we can look at.",
                    "label": 0
                },
                {
                    "sent": "Directions sorted by nonstationarity.",
                    "label": 0
                },
                {
                    "sent": "And Nonstationarity, and we can compare.",
                    "label": 0
                },
                {
                    "sent": "ICA PCA and SSA?",
                    "label": 0
                },
                {
                    "sent": "And so clearly because SSA has been constructed to do this, we find the best non stationary directions with SSA.",
                    "label": 0
                },
                {
                    "sent": "These are clearly the most non stationary ones.",
                    "label": 0
                },
                {
                    "sent": "And then if we look at them then we can see, for example this one.",
                    "label": 0
                },
                {
                    "sent": "This direction and essay was a loose electrode.",
                    "label": 0
                },
                {
                    "sent": "And this one was some eye movement.",
                    "label": 0
                },
                {
                    "sent": "And this one was some muscle activity.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's interesting because those are the signals that you would expect in the nonstationary part.",
                    "label": 0
                },
                {
                    "sent": "Also, I see a has a bunch of non stationary directions also artifacts.",
                    "label": 0
                },
                {
                    "sent": "Now we can.",
                    "label": 0
                },
                {
                    "sent": "Removed.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, we can reject all the artifacts.",
                    "label": 0
                },
                {
                    "sent": "Then you know, we still in SSA becausw.",
                    "label": 0
                },
                {
                    "sent": "By construction we still have the monster most nonstationary directions nicely sorted, and there we can.",
                    "label": 0
                },
                {
                    "sent": "Learn from then we can think about.",
                    "label": 0
                },
                {
                    "sent": "Learning plasticity of our experiments, whatever.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "The point that I would like to make is that.",
                    "label": 0
                },
                {
                    "sent": "Of course, PCA and ICA.",
                    "label": 1
                },
                {
                    "sent": "This was also one of the questions before, right?",
                    "label": 0
                },
                {
                    "sent": "So why not just use them?",
                    "label": 0
                },
                {
                    "sent": "So PCA and ICA may find some artifacts, but they're not made to find these nonstationarity's.",
                    "label": 0
                },
                {
                    "sent": "If we are interested in nonstationarity's, then we better use SSA.",
                    "label": 0
                },
                {
                    "sent": "So if we just look at.",
                    "label": 0
                },
                {
                    "sent": "The PCA components.",
                    "label": 0
                },
                {
                    "sent": "Sort them by variance and look at the nonstationarity after artifact rejection so we can see that this basically flat OK. Anne.",
                    "label": 0
                },
                {
                    "sent": "So the PCA basis is clearly not optimal with respect to non stationarity.",
                    "label": 0
                },
                {
                    "sent": "The same thing is happening in the ICA basis.",
                    "label": 0
                },
                {
                    "sent": "So you will have the same kind of thing.",
                    "label": 0
                },
                {
                    "sent": "So if you compare.",
                    "label": 0
                },
                {
                    "sent": "The essay thing next to this you can see that you can find much more non stationary T with these methods and of course it's clear why because you have constructed the algorithm like that.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we can.",
                    "label": 0
                },
                {
                    "sent": "We can start, you know, classifying these this essay directions.",
                    "label": 0
                },
                {
                    "sent": "So here's to say, if we look at now, the data overall, 40 subjects where we extracted 1600 SSA components.",
                    "label": 0
                },
                {
                    "sent": "OK of all these subjects.",
                    "label": 0
                },
                {
                    "sent": "Then we can look at the degree of non stationarity that we can measure quantitatively.",
                    "label": 0
                },
                {
                    "sent": "And then we can see what are these that are very high.",
                    "label": 0
                },
                {
                    "sent": "And we can.",
                    "label": 0
                },
                {
                    "sent": "Also plot the artifact likelihood so we can see that from a certain point on we just find artifacts.",
                    "label": 0
                },
                {
                    "sent": "So, So what we?",
                    "label": 0
                },
                {
                    "sent": "We can say is that non stationarity is correlated with artifact likelihood, but we cannot say that it's causal, right?",
                    "label": 0
                },
                {
                    "sent": "Just don't want to go into this direction.",
                    "label": 0
                },
                {
                    "sent": "OK, so so it's actually also a nice way of getting artifacts.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then let's let's look at another thing.",
                    "label": 0
                },
                {
                    "sent": "So we have concatenated many trials and did this one study.",
                    "label": 0
                },
                {
                    "sent": "Now we are we are looking at something else.",
                    "label": 0
                },
                {
                    "sent": "We're looking at.",
                    "label": 0
                },
                {
                    "sent": "This is the typical typical trial.",
                    "label": 0
                },
                {
                    "sent": "So baseline motor.",
                    "label": 0
                },
                {
                    "sent": "Metairie rest OK, and this is.",
                    "label": 0
                },
                {
                    "sent": "Many trials and so now we can.",
                    "label": 0
                },
                {
                    "sent": "We can look at.",
                    "label": 0
                },
                {
                    "sent": "One subject and we can try to see.",
                    "label": 0
                },
                {
                    "sent": "So what is most nonstationary during a trial?",
                    "label": 1
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "So what do we expect to be most non stationary during a trial where people do motor Metairie?",
                    "label": 0
                },
                {
                    "sent": "So what do you expect?",
                    "label": 0
                },
                {
                    "sent": "Any idea?",
                    "label": 0
                },
                {
                    "sent": "There you go.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "These are the most non stationary sources from left and right class.",
                    "label": 0
                },
                {
                    "sent": "So we haven't told this.",
                    "label": 0
                },
                {
                    "sent": "Our system that this is 2 classes we haven't told them anything.",
                    "label": 0
                },
                {
                    "sent": "Just said please find the most non stationary directions.",
                    "label": 0
                },
                {
                    "sent": "And we can look at the.",
                    "label": 0
                },
                {
                    "sent": "The nice the patterns that are associated to that, and this is quite neat.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a linear projection, so it's interpretable.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Everything.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's wrap up this part.",
                    "label": 0
                },
                {
                    "sent": "So SSA finds stationary and non stationary subspace is question of course that are opened.",
                    "label": 1
                },
                {
                    "sent": "What if we have higher order moments or temporal structure?",
                    "label": 1
                },
                {
                    "sent": "Maybe we could change the SSA algorithm.",
                    "label": 0
                },
                {
                    "sent": "We could put convolution in this there's a wide range of stuff that we can still do.",
                    "label": 0
                },
                {
                    "sent": "And then there's like issue of model selection, and I think that from the algebraic point of view we already have quite good solution to this and it's included in our package.",
                    "label": 0
                },
                {
                    "sent": "So if you if you look in jam alarm.",
                    "label": 0
                },
                {
                    "sent": "Merlos section then you can find the essay toolbox.",
                    "label": 0
                },
                {
                    "sent": "Or you can just type essay toolbox and they find it.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then Anne.",
                    "label": 0
                },
                {
                    "sent": "Let's talk about another aspect of Nonstationarity.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "Multi.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Almost I would like to make this interpretation so if we have a bunch of different subjects, we can also think of them as being.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "Not being part of 1 distribution, but you know maybe this is 1 big non stationary distributions.",
                    "label": 0
                },
                {
                    "sent": "The the number of subjects they they are different right?",
                    "label": 0
                },
                {
                    "sent": "And so one if we want to find a universal classifier, then we may have to have two.",
                    "label": 0
                },
                {
                    "sent": "Some subject independent PCI.",
                    "label": 0
                },
                {
                    "sent": "We may have to deal with this.",
                    "label": 0
                },
                {
                    "sent": "So this is another view about Nonstationarity and BCI.",
                    "label": 0
                },
                {
                    "sent": "I just want to remind you of that.",
                    "label": 0
                },
                {
                    "sent": "And there's different solutions.",
                    "label": 0
                },
                {
                    "sent": "Of course, I cannot pull out essay and say this is the solution to that, because that's another story.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm just move.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going forward and.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Skipping this",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now comes an interesting question, so it's also.",
                    "label": 0
                },
                {
                    "sent": "You have seen this video.",
                    "label": 0
                },
                {
                    "sent": "And the question is an if you think about different subjects that do in BCI experiment and you think about that distributions.",
                    "label": 0
                },
                {
                    "sent": "So quite often, if we want to build universal classifiers, we try to say maybe we can find some aspects some CSP pattern that is universal to all of them, OK?",
                    "label": 0
                },
                {
                    "sent": "In that sense, we are trying to see is there anything in this stationary part.",
                    "label": 0
                },
                {
                    "sent": "That we can transfer.",
                    "label": 0
                },
                {
                    "sent": "So now we're asking the question.",
                    "label": 0
                },
                {
                    "sent": "Bit differently, so we are saying is the stationary part or is this non stationary part?",
                    "label": 1
                },
                {
                    "sent": "In subjects, is it actually?",
                    "label": 0
                },
                {
                    "sent": "Different between subjects?",
                    "label": 0
                },
                {
                    "sent": "Or is it similar?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In other words, if we do BCI experiments.",
                    "label": 0
                },
                {
                    "sent": "We do training and we go to feedback.",
                    "label": 0
                },
                {
                    "sent": "There's a nonstationarity the question is, is this nonstationarity similar across subject, or is the?",
                    "label": 0
                },
                {
                    "sent": "The the way we can decode.",
                    "label": 0
                },
                {
                    "sent": "The stationary part is that similar OK?",
                    "label": 0
                },
                {
                    "sent": "End.",
                    "label": 0
                },
                {
                    "sent": "We can, so in that sense we have a bunch of modalities.",
                    "label": 0
                },
                {
                    "sent": "These are the subjects and we look at the changes between training and test set.",
                    "label": 1
                },
                {
                    "sent": "Anne and.",
                    "label": 0
                },
                {
                    "sent": "So so now we can look at the.",
                    "label": 0
                },
                {
                    "sent": "This sub spaces between the subjects that are discriminative.",
                    "label": 0
                },
                {
                    "sent": "And the subspaces that are.",
                    "label": 0
                },
                {
                    "sent": "Non stationary OK.",
                    "label": 0
                },
                {
                    "sent": "So what we find is So what is the discriminative subject space we we're taking a classifier for every subject and look at what is the subspace in this.",
                    "label": 0
                },
                {
                    "sent": "Discriminative, part and how strong do they coincide OK?",
                    "label": 0
                },
                {
                    "sent": "And we look at the similarity, and there's basically little similarity.",
                    "label": 0
                },
                {
                    "sent": "It's .2 and this is as a function of the size of the subspace.",
                    "label": 0
                },
                {
                    "sent": "The largest subspace.",
                    "label": 0
                },
                {
                    "sent": "Of course, the more similarity there is.",
                    "label": 0
                },
                {
                    "sent": "Other for the non stationary subspace and that is quite surprising in the first moment.",
                    "label": 0
                },
                {
                    "sent": "So this non stationary subspace that reflects this change from training to.",
                    "label": 0
                },
                {
                    "sent": "Feedback session this is actually very similar.",
                    "label": 0
                },
                {
                    "sent": ".8 is quite high.",
                    "label": 0
                },
                {
                    "sent": "In terms of the space.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Isn't it?",
                    "label": 0
                },
                {
                    "sent": "This isn't it bit strange or?",
                    "label": 0
                },
                {
                    "sent": "I mean, I, I when I saw this first, I thought what the heck is going on.",
                    "label": 0
                },
                {
                    "sent": "So this is like we consider this nonstationary part as nonsense.",
                    "label": 0
                },
                {
                    "sent": "It's nuisance.",
                    "label": 0
                },
                {
                    "sent": "It's something that prevents us from learning good.",
                    "label": 0
                },
                {
                    "sent": "So all of a sudden we can learn from noise.",
                    "label": 0
                },
                {
                    "sent": "I thought noise never helps.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interesting, so just to give the picture.",
                    "label": 0
                },
                {
                    "sent": "So assume that these are the dimensions.",
                    "label": 0
                },
                {
                    "sent": "Then you know for the subject 1, the discriminative subspace is here subject to its here an and so on so forth.",
                    "label": 0
                },
                {
                    "sent": "And this upper part is the non stationary subspace.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "That so usually what people are always doing is trying to to transfer this bit of information between the subjects.",
                    "label": 0
                },
                {
                    "sent": "We just saw that this, you know doesn't really match well.",
                    "label": 0
                },
                {
                    "sent": "So what we're saying is maybe we should better transfer this bit of information in the smart way, OK?",
                    "label": 0
                },
                {
                    "sent": "But there was a question, so I don't know if I get it right.",
                    "label": 0
                },
                {
                    "sent": "The nonstationary thing I see it as is it and it is the noise and it's the noise or the thing that reflects the change from from training to feedback session.",
                    "label": 0
                },
                {
                    "sent": "OK, because there's more optical flow as I pointed out in the first couple of slides, because when I think of the nonstationary thing and I think I think as a source from outside like the magnetic thing and then it would make sense that it is the same always.",
                    "label": 0
                },
                {
                    "sent": "I, I think there's it's true, so there's different parts, some that come from outside, but some come also from the very fact that we're trying to.",
                    "label": 0
                },
                {
                    "sent": "Go from a training to a feedback session.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now is the algorithm.",
                    "label": 0
                },
                {
                    "sent": "How can we get mileage from this insight?",
                    "label": 0
                },
                {
                    "sent": "OK, so now it for every subject I we compute the eigenvectors of this beast here.",
                    "label": 0
                },
                {
                    "sent": "So we take the covariance on the training and the covariance on the test set and we take we try to find the PCA of that subspace.",
                    "label": 0
                },
                {
                    "sent": "Until Dimension D OK.",
                    "label": 0
                },
                {
                    "sent": "So for each subject I will get L eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Get the first L eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Then we aggregate this into a huge matrix and then apply PCA.",
                    "label": 0
                },
                {
                    "sent": "And then we can.",
                    "label": 0
                },
                {
                    "sent": "Reduce the dimensionality of the nonstationary subspace and then we.",
                    "label": 0
                },
                {
                    "sent": "Find a projection operator that is orthogonal to this non stationary subspace that is common to all.",
                    "label": 0
                },
                {
                    "sent": "And then we can make our data invariant because we look at the projected the data only orthogonal to this non stationary subspace.",
                    "label": 0
                },
                {
                    "sent": "And then we compute the CSP filters just to say this again.",
                    "label": 0
                },
                {
                    "sent": "So OK. We have the training and the test covariances.",
                    "label": 0
                },
                {
                    "sent": "So we subtract them.",
                    "label": 0
                },
                {
                    "sent": "We do an eigen decomposition.",
                    "label": 0
                },
                {
                    "sent": "Take the leading L values.",
                    "label": 0
                },
                {
                    "sent": "Stack this into a large vector.",
                    "label": 0
                },
                {
                    "sent": "And since we know that this is many.",
                    "label": 0
                },
                {
                    "sent": "You know, vectors are actually very correlated as we have seen in the plot.",
                    "label": 0
                },
                {
                    "sent": "We do a PCA.",
                    "label": 0
                },
                {
                    "sent": "And since they're very correlated, we can actually get a very compact subspace.",
                    "label": 0
                },
                {
                    "sent": "And to that compact subspace, we want to be orthogonal, OK?",
                    "label": 0
                },
                {
                    "sent": "Because that compact subspace reflects the overall.",
                    "label": 0
                },
                {
                    "sent": "An nonstationarity of the whole ensemble of subjects.",
                    "label": 0
                },
                {
                    "sent": "And then we do CSP OK?",
                    "label": 0
                },
                {
                    "sent": "Two slides before you have shown or one side before.",
                    "label": 0
                },
                {
                    "sent": "Two slides here.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That the similarity between the nonstationarity nonstationary subspace is higher.",
                    "label": 0
                },
                {
                    "sent": "But I see that there's much more variance, probably between users that then in the discriminative subspace.",
                    "label": 1
                },
                {
                    "sent": "You are you compensating for that variance by taking these Li vectors?",
                    "label": 0
                },
                {
                    "sent": "Or how do you deal with?",
                    "label": 0
                },
                {
                    "sent": "That said, I mean, in a sense I'm.",
                    "label": 0
                },
                {
                    "sent": "I'm thinking of this as the following.",
                    "label": 0
                },
                {
                    "sent": "So you have this.",
                    "label": 0
                },
                {
                    "sent": "This data and this other subject, another subject subject, and so all these there's there's considerable variance in this.",
                    "label": 0
                },
                {
                    "sent": "And this is the reason why you would finally have to do a PCA and find this.",
                    "label": 0
                },
                {
                    "sent": "Robot space.",
                    "label": 0
                },
                {
                    "sent": "So that's that would be OK.",
                    "label": 0
                },
                {
                    "sent": "I take on it and what's your interpretation of that?",
                    "label": 0
                },
                {
                    "sent": "The discriminative subspace is, despite they have a bad similarity.",
                    "label": 0
                },
                {
                    "sent": "Have a low variance.",
                    "label": 0
                },
                {
                    "sent": "That's a good question.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "I think that has also maybe it has a bit to do with the with what was also mentioned because there's external nonstationarity's.",
                    "label": 0
                },
                {
                    "sent": "And then there's the nonstationary choose between training and and feedback.",
                    "label": 1
                },
                {
                    "sent": "So I think that this part is not in the discriminative one.",
                    "label": 0
                },
                {
                    "sent": "And this part made very strange strongly, but this is just.",
                    "label": 0
                },
                {
                    "sent": "A conjecture OK, OK.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then we can we call this SCSP and we can see.",
                    "label": 0
                },
                {
                    "sent": "OK, so so here's.",
                    "label": 0
                },
                {
                    "sent": "Some.",
                    "label": 0
                },
                {
                    "sent": "Toyish data set.",
                    "label": 0
                },
                {
                    "sent": "So we have a.",
                    "label": 0
                },
                {
                    "sent": "Visual cue and training and auditory cue in test or letters in training and moving objects in test.",
                    "label": 1
                },
                {
                    "sent": "So there's a clear nonstationarity just to to show this fact, and then we can use standard CSP or.",
                    "label": 0
                },
                {
                    "sent": "SS CSP, which is using this transfer of nonstationarity procedure.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and we can see that there is.",
                    "label": 0
                },
                {
                    "sent": "We gain something with the stationary subspace, ESP.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can now start looking at the.",
                    "label": 0
                },
                {
                    "sent": "The pattern that we find.",
                    "label": 0
                },
                {
                    "sent": "With respect to the so we have different different users and we can see.",
                    "label": 0
                },
                {
                    "sent": "This unit was activity activity in the occipital and also temporal areas.",
                    "label": 1
                },
                {
                    "sent": "That that is.",
                    "label": 0
                },
                {
                    "sent": "Mainly responsible for the visual and audio processing, so you would expect to see that in the final classifier, and I think we can see this nice because there's always this this occipital components and also the temporal ones.",
                    "label": 0
                },
                {
                    "sent": "OK, so this makes some sense.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look at the feature distribution.",
                    "label": 0
                },
                {
                    "sent": "This is just the typical.",
                    "label": 0
                },
                {
                    "sent": "Training test data projected to some dimensions and you can see that there's a huge difference.",
                    "label": 0
                },
                {
                    "sent": "Between the covariances.",
                    "label": 0
                },
                {
                    "sent": "And if we do this procedure of the where we subtract this nonstationary space, match is much better.",
                    "label": 0
                },
                {
                    "sent": "In other words, the whole whole thing becomes more stationary, which means that anything that you can apply to it will work better.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So think the.",
                    "label": 0
                },
                {
                    "sent": "The idea here is that we try to reduce nonstationarity in data, but we doing this in the multimodal way, meaning that we're trying to make use of what we know from other subjects.",
                    "label": 1
                },
                {
                    "sent": "And in some strange way we can.",
                    "label": 0
                },
                {
                    "sent": "We can learn from.",
                    "label": 0
                },
                {
                    "sent": "This nuisance OK?",
                    "label": 0
                },
                {
                    "sent": "Which this is the non stationary subspace.",
                    "label": 0
                },
                {
                    "sent": "Which I think is quite nice.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Continue here and.",
                    "label": 0
                },
                {
                    "sent": "This is another.",
                    "label": 0
                },
                {
                    "sent": "I mean, it should be faster now, I believe.",
                    "label": 0
                },
                {
                    "sent": "So this is a very recent piece of work where we try to put, you know, many of these things together.",
                    "label": 0
                },
                {
                    "sent": "Under one roof.",
                    "label": 0
                },
                {
                    "sent": "Mathematical proof, of course.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is the typical BCI pipeline.",
                    "label": 0
                },
                {
                    "sent": "This is CSP.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "There's a very interesting theorem that we can prove.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we know that CSP is non robust so any kind of electron artifacts or muscle activity or something like that will just basically make CSP find nonsensical directions.",
                    "label": 0
                },
                {
                    "sent": "So I think this has been written by many people and experienced by many people.",
                    "label": 0
                },
                {
                    "sent": "So if you have artifacts and you use ESPN, no good.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have to do something against it.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course, your machine learner you.",
                    "label": 0
                },
                {
                    "sent": "I mean, you do something against it by proving a theorem.",
                    "label": 0
                },
                {
                    "sent": "I guess so.",
                    "label": 0
                },
                {
                    "sent": "Here's the theorem.",
                    "label": 0
                },
                {
                    "sent": "So if W is our.",
                    "label": 0
                },
                {
                    "sent": "CS P filter.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And this V is a is a matrix.",
                    "label": 0
                },
                {
                    "sent": "Then we can decompose this into lightning projection and orthogonal projection and we can show the following.",
                    "label": 0
                },
                {
                    "sent": "We can show that the span of the CSP and the span of.",
                    "label": 0
                },
                {
                    "sent": "What comes now is the same.",
                    "label": 0
                },
                {
                    "sent": "OK, so now comes the interesting thing.",
                    "label": 0
                },
                {
                    "sent": "So we've seen the Kullback Leibler divergent before.",
                    "label": 0
                },
                {
                    "sent": "So if we take.",
                    "label": 0
                },
                {
                    "sent": "Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So so OK.",
                    "label": 0
                },
                {
                    "sent": "So the first thing is, so we take a projection.",
                    "label": 0
                },
                {
                    "sent": "We are searching for a projection V with respect to one class.",
                    "label": 0
                },
                {
                    "sent": "And we've.",
                    "label": 0
                },
                {
                    "sent": "Also searching for the same projection with respect to the second class such that this becomes far apart.",
                    "label": 0
                },
                {
                    "sent": "In other words.",
                    "label": 0
                },
                {
                    "sent": "So the callback livelier divergent in SSP.",
                    "label": 0
                },
                {
                    "sent": "We try to try to match them.",
                    "label": 0
                },
                {
                    "sent": "We try to make them as close as possible.",
                    "label": 0
                },
                {
                    "sent": "But in CSP we tried to distinguish between two states.",
                    "label": 0
                },
                {
                    "sent": "Would like to make them as far as possible apart.",
                    "label": 0
                },
                {
                    "sent": "OK, so now what is the projection in this high dimensional space such that this becomes far as the part?",
                    "label": 0
                },
                {
                    "sent": "And we can.",
                    "label": 0
                },
                {
                    "sent": "Write this down as as this kind of optimization problem as a cool bug library divergent.",
                    "label": 0
                },
                {
                    "sent": "OK, and if we do so then we can show that the final.",
                    "label": 0
                },
                {
                    "sent": "Projections that we get are the same as the CSP projections.",
                    "label": 0
                },
                {
                    "sent": "OK, So what So what does it mean?",
                    "label": 0
                },
                {
                    "sent": "It just means that we have found another odd way of computing CSP.",
                    "label": 0
                },
                {
                    "sent": "OK, this is all we have.",
                    "label": 0
                },
                {
                    "sent": "However, while you may think just come on, I mean density some some eigenvalue problem and some some relic coefficient, and this this KL divergent.",
                    "label": 0
                },
                {
                    "sent": "Anyway, who cares anyway?",
                    "label": 0
                },
                {
                    "sent": "Right now there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a very fundamental advantage of having this as a KL divergent.",
                    "label": 0
                },
                {
                    "sent": "An which is that there has been, you know, loads of beautiful mathematical papers that have taught us how to make this KL divergent's a robust one OK?",
                    "label": 0
                },
                {
                    "sent": "So recall in CSP, if we do have our artifacts, this goes wrong OK.",
                    "label": 0
                },
                {
                    "sent": "So now we have some mathematical framework in the optimization framework.",
                    "label": 0
                },
                {
                    "sent": "Whether we can we have some other things that we can do?",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's the beautiful, better divergents.",
                    "label": 0
                },
                {
                    "sent": "And I actually stole these slides from Voytek.",
                    "label": 0
                },
                {
                    "sent": "M. So he's been very optimistic, so so basically for woytek.",
                    "label": 0
                },
                {
                    "sent": "AM.",
                    "label": 0
                },
                {
                    "sent": "The use of a bitter divergent which is a robust color grabber divergences.",
                    "label": 0
                },
                {
                    "sent": "This is like as illuminating as the kernel trick.",
                    "label": 0
                },
                {
                    "sent": "I'm I see whether this finally ends up there, but but this is the killed.",
                    "label": 0
                },
                {
                    "sent": "The kale divergent with a little beta in this.",
                    "label": 0
                },
                {
                    "sent": "So basically you're waiting the.",
                    "label": 0
                },
                {
                    "sent": "You know this distance between puke and queue.",
                    "label": 0
                },
                {
                    "sent": "You waiting it by this factor of beta.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And subtracting something.",
                    "label": 0
                },
                {
                    "sent": "Just take this as a mathematical fact, OK?",
                    "label": 0
                },
                {
                    "sent": "Not invented by us.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why would this be interesting?",
                    "label": 0
                },
                {
                    "sent": "So robustness is, I think, quite a useful thing.",
                    "label": 0
                },
                {
                    "sent": "So if you recall, you have some 1 dimensional data and you estimate the mean, then it's here.",
                    "label": 0
                },
                {
                    "sent": "If you have this outlier, so we learn that that it would be nice to actually have something like robust selected the median.",
                    "label": 0
                },
                {
                    "sent": "So we can also.",
                    "label": 0
                },
                {
                    "sent": "Estimate this is the two men without the outlier, so we could.",
                    "label": 0
                },
                {
                    "sent": "Think of estimating the mean also with a bitter divergent.",
                    "label": 0
                },
                {
                    "sent": "And the bitter divergents will get something very close to the true mean, because it's waiting down this point, OK?",
                    "label": 0
                },
                {
                    "sent": "So just this should give you an intuition.",
                    "label": 0
                },
                {
                    "sent": "So what the idea now is?",
                    "label": 0
                },
                {
                    "sent": "If we are using the bitter divergents we're not.",
                    "label": 0
                },
                {
                    "sent": "Basically looking.",
                    "label": 0
                },
                {
                    "sent": "Taking every trial in our BCI experiment similarly.",
                    "label": 0
                },
                {
                    "sent": "Important, but we can wait down some artifactual trials, OK?",
                    "label": 0
                },
                {
                    "sent": "And I will expand on this a bit.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just a bit of a wrap up, so this is CSP.",
                    "label": 0
                },
                {
                    "sent": "We can prove the similarity of the CSP by this KL divergent CSP.",
                    "label": 0
                },
                {
                    "sent": "And now comes a very beautiful little trick.",
                    "label": 0
                },
                {
                    "sent": "Which says we can, instead of computing the CSP at the KL divergent, we can also take compute sum over.",
                    "label": 0
                },
                {
                    "sent": "The trial was clear care liver chances.",
                    "label": 0
                },
                {
                    "sent": "That's a difference.",
                    "label": 0
                },
                {
                    "sent": "It's mathematically not the same, but it's approximately the same.",
                    "label": 0
                },
                {
                    "sent": "And when we do that then we can take the better divergences because then the beta divergences will allow us to wait down a certain trial.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is our program now.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can do simulations and show that the beta divergences really, really robust.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this picture is much nice.",
                    "label": 0
                },
                {
                    "sent": "So assume that there is an artifact.",
                    "label": 0
                },
                {
                    "sent": "This is typically in data, right in EG data.",
                    "label": 0
                },
                {
                    "sent": "So this is what CS P gives us, because there's one electrode that is loose, so CP jumps on this electrode.",
                    "label": 0
                },
                {
                    "sent": "Now, if you do the better divergents thing.",
                    "label": 0
                },
                {
                    "sent": "That is formally equivalent to CSP recall.",
                    "label": 0
                },
                {
                    "sent": "Then we can look at.",
                    "label": 0
                },
                {
                    "sent": "A ratio which is the divergance term with artifact and the average divergance term without artifact.",
                    "label": 0
                },
                {
                    "sent": "And as a function of beta OK?",
                    "label": 0
                },
                {
                    "sent": "And we can see that.",
                    "label": 0
                },
                {
                    "sent": "You know there may be.",
                    "label": 0
                },
                {
                    "sent": "Some artifacts that that you know are.",
                    "label": 0
                },
                {
                    "sent": "A factor of 1000.",
                    "label": 0
                },
                {
                    "sent": "Larger.",
                    "label": 0
                },
                {
                    "sent": "But then some average divergent steam term without artifact, which means that if you think about the mean, it's like putting a point all the way.",
                    "label": 0
                },
                {
                    "sent": "You know on the other side of the city, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "But if we are ramping up better the better value of the divergent.",
                    "label": 0
                },
                {
                    "sent": "Then all of a sudden this is weighted down almost completely.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Which then makes a huge difference in.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In robustness, but we can move in one step further so.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With better divergance.",
                    "label": 0
                },
                {
                    "sent": "This doesn't happen.",
                    "label": 0
                },
                {
                    "sent": "This kind of CSP.",
                    "label": 0
                },
                {
                    "sent": "Pattern would not happen.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the good news.",
                    "label": 0
                },
                {
                    "sent": "We just waiting it down.",
                    "label": 0
                },
                {
                    "sent": "To the extent that this influence will be marginalized.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, once we have things in there kind of in written as a as a beta in terms of a divergance, we can start playing with this because we have the mathematical framework too.",
                    "label": 0
                },
                {
                    "sent": "You know, this part is the CSP part, but remember we in lots of people have played all sorts of things with CSP.",
                    "label": 0
                },
                {
                    "sent": "We have heard in this talk stationary CSP we have, you know, some robust CSP invariances P whatever.",
                    "label": 0
                },
                {
                    "sent": "I mean there's a big literature on CSP versions.",
                    "label": 0
                },
                {
                    "sent": "So now we can.",
                    "label": 0
                },
                {
                    "sent": "We can look at this from the machine learning POV, so we have an error function that is CSP and we can add a regular regularizer.",
                    "label": 0
                },
                {
                    "sent": "And then we can.",
                    "label": 0
                },
                {
                    "sent": "See whether all these CSP methods can basically be be considered as part of the regularizers and we can see.",
                    "label": 0
                },
                {
                    "sent": "See, hey, maybe there are some regularizers that we haven't tried.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what are typical regularizers?",
                    "label": 0
                },
                {
                    "sent": "So, for example, we could have.",
                    "label": 0
                },
                {
                    "sent": "A regularizer that regularizes within session changes or between session changes or across subject changes or multi subject stuff.",
                    "label": 0
                },
                {
                    "sent": "So big.",
                    "label": 0
                },
                {
                    "sent": "Basically all that can be done in one framework.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can look at the difference between normal CSP and these divergencies peas and you can see that you get some statistically significant improvements in different scenarios so we can impose within's test set session stationarity between session stationarity across subject stationarity.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So basically you can ask different questions and just implement this as a particular regularizer.",
                    "label": 0
                },
                {
                    "sent": "So you can have something stationary.",
                    "label": 0
                },
                {
                    "sent": "You can have something smooth.",
                    "label": 0
                },
                {
                    "sent": "You can have some whatever you actually asked the data to be.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "CSP, when we have training and test and we see that this it says use huge difference and this is with the divergencies P.",
                    "label": 1
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, and.",
                    "label": 0
                },
                {
                    "sent": "If we.",
                    "label": 0
                },
                {
                    "sent": "Now look at this particular example that we had before with this one electrode going beserk, right, and we are increasing regularization.",
                    "label": 0
                },
                {
                    "sent": "In this case, we are not increasing beta.",
                    "label": 0
                },
                {
                    "sent": "But we are increasing regularizer that regularizers towards other subjects.",
                    "label": 0
                },
                {
                    "sent": "So we have a bunch of subjects that are doing the same experiment and we say yeah, what we want to have is something that should be a bit more similar to the other subject.",
                    "label": 0
                },
                {
                    "sent": "Then we can see that this goes away.",
                    "label": 0
                },
                {
                    "sent": "OK, as if we regularize stronger and the nice thing is that we can do cross validation to get the value of this regularization.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about what I just showed the divergencies P framework is that it integrates many CSP variants in a different manner, under different regularization terms.",
                    "label": 1
                },
                {
                    "sent": "An we have a common optimization method so so in other words, we you know the nice thing about this is.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that the optimization that we can do is always the same?",
                    "label": 0
                },
                {
                    "sent": "We can do some gradient descent and otherwise we would always have to implement different things for the different CSP algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "This way various simply develop new CSP variants.",
                    "label": 0
                },
                {
                    "sent": "The divergents trick allows us to make it simultaneously robust.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Voytik is put up a home page which says which is divergentmethods.org where you can find code so you can just.",
                    "label": 0
                },
                {
                    "sent": "Download the MATLAB code that this also regularize is implemented and you can just play around.",
                    "label": 0
                },
                {
                    "sent": "And there's a review paper that just appeared, or it's actually advanced online that has, you know, quite lengthy descriptions of the whole part of this talk.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An I think I'm I'm I'm going overtime.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a. I have two.",
                    "label": 0
                },
                {
                    "sent": "As a chair I have to stop myself.",
                    "label": 0
                },
                {
                    "sent": "Just hard.",
                    "label": 0
                },
                {
                    "sent": "And so I think basically what I'm what I.",
                    "label": 0
                },
                {
                    "sent": "Try to say and what has been always a common denominator of many of the talks.",
                    "label": 0
                },
                {
                    "sent": "Is that machine learning and modern data analysis are very important for BCI and they are important to move the field forward.",
                    "label": 1
                },
                {
                    "sent": "I have focused very strongly on technical parts online stationarity.",
                    "label": 0
                },
                {
                    "sent": "And try to put this into a, you know joint understanding.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This the SSA method that can project to some subspaces we can have.",
                    "label": 0
                },
                {
                    "sent": "But we talked about the covariance shift or I mentioned covariate shift.",
                    "label": 0
                },
                {
                    "sent": "Class cross validation.",
                    "label": 0
                },
                {
                    "sent": "We didn't have time to talk about this.",
                    "label": 0
                },
                {
                    "sent": "We didn't have time to talk about this, so.",
                    "label": 0
                },
                {
                    "sent": "11 final other remark that I would like to make is like of course we've now seen lots of ways to deal with non stationarity.",
                    "label": 0
                },
                {
                    "sent": "Being invariant, projecting out.",
                    "label": 0
                },
                {
                    "sent": "Looking at the covariate shift model.",
                    "label": 0
                },
                {
                    "sent": "Having some nice divergents framework with appropriate regularization.",
                    "label": 0
                },
                {
                    "sent": "We can also track nonstationarity.",
                    "label": 0
                },
                {
                    "sent": "And people in invasive studies do this quite quite heavily.",
                    "label": 0
                },
                {
                    "sent": "They have some Kalman filters and they they use them.",
                    "label": 0
                },
                {
                    "sent": "But this requires the nonstationarity to be rather slow moving, and we've seen quite fast moving nonstationarity's here.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think this brings me to the end of my talk.",
                    "label": 0
                },
                {
                    "sent": "I thank you for your patience and your stamina to go through this.",
                    "label": 0
                },
                {
                    "sent": "You know highly technical stuff that is fun for me.",
                    "label": 0
                },
                {
                    "sent": "So thank you.",
                    "label": 0
                }
            ]
        }
    }
}