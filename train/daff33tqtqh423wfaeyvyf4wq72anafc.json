{
    "id": "daff33tqtqh423wfaeyvyf4wq72anafc",
    "title": "Maximum Likelihood Rule Ensembles",
    "info": {
        "author": [
            "Wojciech Kotlowski, Institute of Computing Science, Poznan University of Technology"
        ],
        "published": "Aug. 6, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_kotlowski_mlr/",
    "segmentation": [
        [
            "OK. Hello everybody.",
            "I'd like to present our.",
            "Our paper about maximum likelihood rule ensembles.",
            "This is joint work with shift of them change scheme and there must have been ski.",
            "I know the name, I quite quite hard.",
            "Mine is not better and generally this is work which consists in building rule ensembles using boosting."
        ],
        [
            "Well, let us start with far the motivations and what do the decision rules are and why do we care about using decision rules in in our algorithm?",
            "Well, decision rule is in general a simple logical expression of the form.",
            "Let's say if conditions, then decision.",
            "And if conditions are satisfied, then some kind of a decision is taken, usually in the classification problem decision is voting for some of the classes.",
            "Therefore, decision rule can be treated as a simple classifier that votes for some class when the condition and status are satisfied, and abstain abstains from vote when the conditions are not satisfied.",
            "Here's the example of.",
            "Decision rules for.",
            "Credit.",
            "Scoring problem.",
            "Anne.",
            "So Decisional consists in, let's say, elementary expressions.",
            "Each expression is expression of the form, let's say direct duration, greater, equal to some value or for a nominal attributes that might be.",
            "Equality or an equality sign and then the rule votes for a class low and the main advantage of decision rules is their simplicity and interpreter interpret ability.",
            "Especially the rule ensemble is much easier to interpret than the tree ensemble."
        ],
        [
            "However, most of the the popular algorithms for rule induction are based on sequential covering.",
            "This is, let's say, the old school of generating rules and well, most of the sequential covering algorithm suffered from the insufficient accuracy.",
            "However, there are some.",
            "There are some methods which use boosting approach and we also follow this boosting approach by treating each decision rule as a single classifiers based classifier in the ensemble.",
            "And there are some methods which use boosting for inducing decision rules and those methods are rooted, slipper and lri.",
            "And well, it was proved experimentally that boosting leads to powerful prediction model with.",
            "With high accuracy, however, in our case or contrary to previous approaches, we try to estimate the conditional class distribution.",
            "And we minimize the negative log likelihood.",
            "The loss function which was used for example in logic boots or Mart on Friedman Smart with our classification trees and regression trees.",
            "And estimating the class distribution allows us to work with any kind of loss matrix, so any kind of.",
            "Misclassification costs.",
            "And our approach can deal directly with the general K class problem.",
            "So we do not need to use.",
            "Let's say we need.",
            "We do not need to solve the general many class problems by using by solving several binary classification problems."
        ],
        [
            "OK, so we have data set D consisting of objects with their labels, an object with their labels labels and we have K classes.",
            "So why is the class label of of each object and we estimate the probabilities with the maximum likelihood estimation which is in general maximizing the likelihood or equivalently minimizing the negative log likelihood, which is easier.",
            "And well, probabilities.",
            "Are always positive and they must sum to one, so it's better to use unconstrained vector F. And we just say which is related to the probabilities by the logistic transform.",
            "And then by plugging this vector into the log likelihood, we minimize the following loss function.",
            "And now the coordinates of the vector are unconstrained, so they can.",
            "They can be any."
        ],
        [
            "Real values.",
            "How do we treat decision rule in our ensemble?",
            "Well, we treat each decision rule as a function which returns a vector.",
            "A constant vector in some subspace in some axis, parallel or rectangular region S. This is the region in which the conditions of the ruler satisfied and returns zero vector outside this outside this region.",
            "And what is this constant vector?",
            "This constant vector express the confidence of predicting the classes, so in particular, The Cave coordinate of the vector expresses the confidence of predicting The Cave class, and we also we also introduce an indicator function.",
            "See such that it equals to one if and only if the object satisfies the conditions of the rules.",
            "So if the objects is inside this region S and then decision rule can be written as.",
            "This vector Alpha times the value of the indicator function."
        ],
        [
            "And now we assume that the classification function is a linear combination of EM rules, which is typical for all boosting strategies, and we use the boosting strategy by adding the rules 1 by 1 to the ensemble Gridley, minimizing the loss function.",
            "There are many different points of view on boosting.",
            "One is introduced by Asti Friedman and tipsy running which says that.",
            "Boosting is nothing else than minimizing really minimizing some loss function.",
            "In our case, this is the negative log likelihood, so we try to.",
            "Generate the rule in M iteration which minimizes.",
            "The value of the loss function.",
            "Without changing the rules that have already been generated and the rules that have already been generated, is this F -- 1, so the assemble after ensemble after N -- 1 iterations?",
            "And to simplify the thing, we restrict the rules only to the rules which vote for only one plus, so this vote vectors is has the form the following form where the Alpha is the positive value and.",
            "The is some vector which has only which has zero and one at The Cave position, so it votes for The Cave plus and since there are no, there's no simple solution to this equation.",
            "We need to have some kind of approximation and we propose two approximations, gradient."
        ],
        [
            "Newton meters and 1st gradual method approximates the value of loss function for after N -- 1 iteration.",
            "After the first order with respect to the Alpha value, which is the response of the rule.",
            "And since the first term in the approximation is constant.",
            "It results in minimization of the second term."
        ],
        [
            "On the other hand, the Newton method approximates the loss functions up to this second order, so we obtain free terms.",
            "The first term is again this constant, but since the function is convex.",
            "We can use a Newton step.",
            "To minimize.",
            "These are this expansion.",
            "And by plugging this Newton step into the equation here, we end up with minimizing the following value.",
            "So in."
        ],
        [
            "Both cases to generate a rule we need to minimize some value.",
            "Some function alarm.",
            "And those expressions, though derivatives can easily be computed if we denote PIKD value of the current probability estimate using the ensemble in M -- 1 iteration.",
            "We obtained a very simple formulas.",
            "And they can be very easily determined once we obtain all the values of PIKIK by the logistic transfer.",
            "From from those expressions first that Newton method tends to generate rules with small coverage.",
            "This is because we have additional term in denominator here.",
            "That's why Newton method.",
            "That's why rules generated by Newton matches are usually smaller, so they cover less smaller number."
        ],
        [
            "Of examples.",
            "So how does the procedure of rule generation look like?",
            "Well, we obtain the value of C, which is in fact the set of conditions of the rule.",
            "Because this is this indicator function, by minimizing the value of L. In both Newton and gradient methods and we start with the most general rules.",
            "So the rule which covers the whole space and then we sequentially add conditions to the rule which is equivalent to trimming the rectangle as long as LM can be decreased.",
            "So in contrary to the tree induction, there is another natural stopping condition, so we add conditions until we reach the minimum of the value of L. Then the response of the rule Alpha M is obtained by the Newton step, but we use shrinking.",
            "Well, it it was proved experimentally that shrinking improves the accuracy.",
            "Because shrinking is connected with regularization path with L1 norm.",
            "We also apply regularization with the holdout set.",
            "Instead of generating the rule on the whole data set, we choose a random subsample of 50% of objects to generate the rule, and then the response is obtained using all objects.",
            "Since there are 50% of objects which were not seen during the rule generation, the calculating the response on the whole data set usually decrease, usually decreases the magnitude of the rule.",
            "And can be thought of as an alternative to pruning.",
            "And we also apply the simple procedure for stopping to assemble well on some cases.",
            "Rule ensembles tends to overfit as the number of rule increases, so we use the stopping condition by estimating the quality of the rule of the on the holdout set."
        ],
        [
            "Our ruling samples can also be used in ordinal classification or ordinal regression.",
            "Setting an and they can be easily adapted to capture the ordinal properties of the class labels, which is the essential thing in the ordinal regression.",
            "And we the main changes to instead of generate rules which vote for only one class, we generate rules which vote for let's say at least and at most class unions.",
            "So the sets.",
            "Of the form from K. To the large scale from 1 to K so.",
            "It says.",
            "So it is generating the rules which increases the probability not of a single class, but on the whole class Union an it corresponds to generating the rules with the vectors V to be of the form consisting of minus ones and ones, or the opposite.",
            "So at least K class or at most K class, and in fact the only change which we need to do in the algorithm is different different expressions for the derivatives.",
            "Of the loss.",
            "Now we need to we need to sum over the classes here."
        ],
        [
            "What related works in this area?",
            "There are three three main algorithms which consists in building rule ensembles using.",
            "Some kind of boosting strategy.",
            "First was the slipper and it was proposed within the Adaboost scheme with confidence rated predictions.",
            "So Slipper minimizes the exponential loss and slipper device.",
            "The training set into growing, improving parts so it does pruning well in our case we used different loss functions.",
            "We lost function.",
            "We try to obtain the probability estimates.",
            "And we also use this regularization instead of pruning procedures in our rule generation method.",
            "Then it was lri.",
            "So lightweight rule induction and it generates a bit different kind of rules rules which were in the form of DNF formulas and it uses used the different reweighting scheme based on cumulative error.",
            "This re weighting scheme can also be explained within the boosting.",
            "Within the boosting terms, by introducing a loss function which is a polynomial loss function.",
            "And finally, roll fit by Freeman and prepare school role.",
            "Fit is a method in which rules are not generated using boosting.",
            "First, the tree ensemble is learned.",
            "And only then the rules are produced from generated trees and the weights of the rules are fitted with L1 regularization."
        ],
        [
            "Well, we did the computational experiment, which by which we could compare or do results of our approach with already existing approach to rule induction using boosting.",
            "We used the 35 files from UCI repository.",
            "20 of them were binary and 50 of them were multiclass datasets and we use several.",
            "We set the following parameters in sleeper.",
            "We used 500 iterations.",
            "This is because sleeper has internal procedure to internal cross validation procedure which can detect the optimal number of rules.",
            "Anne.",
            "In other metals in Lri and roll feet we used.",
            "In general, we use rather default parameters.",
            "In our case, we use very small shrink shrinkage and large number of rules.",
            "And accuracy was estimated using the tenfold cross validation and following the the paper of themselves.",
            "We used nonparametric Friedman test to check whether all the algorithm algorithms perform equally well.",
            "So this is the null hypothesis and the Friedman test is generally based on the average ranks and in this presentation we only restrict the analysis of the experiment to do binary class."
        ],
        [
            "Problem.",
            "So here are the binary class problem results.",
            "Here are the results.",
            "The average.",
            "What's the average error?",
            "01 error and the best results within each data sets were marked with Bolt and.",
            "With each result there is a rank assigned.",
            "And well."
        ],
        [
            "Here are the the analysis of the results.",
            "Well, three month state test states that the classifiers are not equally equally good.",
            "So it means that we can proceed to a post hoc analysis.",
            "Which are consistent calculating the critical difference and the critical difference has the interpretation that algorithms with difference in average ranks more than the value of the critical difference significant significantly different, and then some in his work showed that values of that the analysis can be can be.",
            "Illustrated on such diagram.",
            "Where on the let's say average rank access, we put each classifier.",
            "And each classifier which is not significantly different.",
            "Do another classifier.",
            "Is joint.",
            "With this black line.",
            "So we see from this.",
            "So we see from from this diagram that for example.",
            "Our algorithms this is the Newton method and this is the grading method are not significantly different to the Elara Lri method.",
            "While those three existing methods, we cannot say that any of them are significantly better than the other methods.",
            "Well then we used also the multiclass problems and the results were roughly the same.",
            "We need to exclude those two algorithms from the tests and we didn't obtain a significant significant differences differences between our algorithms and lri."
        ],
        [
            "So in summary.",
            "We introduced a rule induction algorithm for solving classification problem by estimating the probabilities.",
            "So our algorithm can work with.",
            "In fact any loss matrix or enemies classification costs and our algorithm was shown to be competitive or outperforms the existing approaches.",
            "And as I mentioned, it can be simply adapted to the ordinal regression problems.",
            "OK, that's all, thank you.",
            "Questions.",
            "Here's my question.",
            "With respect to these probabilities that you have, did you try to get an idea as to how good they are?",
            "So now you face is notoriously.",
            "It's going to be notorious in estimating its probabilities.",
            "Did you try to see if maybe?",
            "Might introduce the change in there because I think measuring the accuracy is not necessarily fair in this case at least you would do some sort of ABC or something like that.",
            "Learning that Sunseeker also evaluated from probability estimation.",
            "While we measured the 01 last big cause, other algorithms which we compare with were not adapted to estimate the probabilities.",
            "That's why.",
            "Right?",
            "Is it very interesting?",
            "Compliance, etc.",
            "Questions.",
            "I have a couple questions actually.",
            "So one of the advantages of rule learning systems is that the OR at least one of the claimed advantages is that the results are interpretable and easy for people to look at and understand part of the basis for that claim is that they often result in compact.",
            "Don't do any studies to see what the size of the different rule sets are for your well, it depends.",
            "It very depends on the amount of shrinkage which is applied in this problem.",
            "So if we use small value of shrinkage then usually we need to generate let's say about 100 rules, which is quite a lot.",
            "But then the accuracy is really high, but increasing the value of shrinkage which means that I mean increasing up to the value of one I think.",
            "But there is, there is some kind of tradeoff between those two.",
            "Those two factors.",
            "So on the one hand, we can have very large, so let's say 100, two 100 rules, 1 sample and but its accuracy is very high.",
            "On the other hand, we can decrease a bit accuracy, but obtain and much more compact, much, much, much more compact than sample and.",
            "Well, we can.",
            "Yeah we can change the value of this parameter and also it is because all the.",
            "All the rules are weighted.",
            "They have their own, let's say responses which can be treated as weights, so we can also choose from the model.",
            "Let's say the rules which has the highest weights and show for example the.",
            "If one wants to interpret the model, we can show only the most important rules from the ensemble.",
            "So I mean it's just a conjecture I guess, but it seems like if you were trying to.",
            "If you're using the loss function that you're using, like a logic looks like loss function.",
            "We're really trying to accurately model the probabilities.",
            "It seems like that might tend to give you larger rulesets just because it requires more information to accurately model the contours of the probability space then just to you know, Model A01 option so.",
            "It seems like to some extent you're sort of.",
            "You're you're giving your system at a disadvantage if you are comparing full cycles with something like lri.",
            "So the second question I had was.",
            "Quite like around Slide 9 maybe?"
        ],
        [
            "OK. Kind of.",
            "Correction for small numbers for these.",
            "Probability, so in Dominator we used well, it's in fact not the Newton method, but Quasi Newton method in the in the sense that we put some small term in the denominator to remove the possibility that we could divide very very small number.",
            "So if second derivative is really small so the curvature is really small then.",
            "Then they did this step.",
            "The size of the step or the value of the value of the rule response cannot be arbitrarily large.",
            "Well, thank you very much, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Hello everybody.",
                    "label": 0
                },
                {
                    "sent": "I'd like to present our.",
                    "label": 0
                },
                {
                    "sent": "Our paper about maximum likelihood rule ensembles.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with shift of them change scheme and there must have been ski.",
                    "label": 0
                },
                {
                    "sent": "I know the name, I quite quite hard.",
                    "label": 0
                },
                {
                    "sent": "Mine is not better and generally this is work which consists in building rule ensembles using boosting.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, let us start with far the motivations and what do the decision rules are and why do we care about using decision rules in in our algorithm?",
                    "label": 0
                },
                {
                    "sent": "Well, decision rule is in general a simple logical expression of the form.",
                    "label": 1
                },
                {
                    "sent": "Let's say if conditions, then decision.",
                    "label": 0
                },
                {
                    "sent": "And if conditions are satisfied, then some kind of a decision is taken, usually in the classification problem decision is voting for some of the classes.",
                    "label": 0
                },
                {
                    "sent": "Therefore, decision rule can be treated as a simple classifier that votes for some class when the condition and status are satisfied, and abstain abstains from vote when the conditions are not satisfied.",
                    "label": 1
                },
                {
                    "sent": "Here's the example of.",
                    "label": 0
                },
                {
                    "sent": "Decision rules for.",
                    "label": 0
                },
                {
                    "sent": "Credit.",
                    "label": 0
                },
                {
                    "sent": "Scoring problem.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 1
                },
                {
                    "sent": "So Decisional consists in, let's say, elementary expressions.",
                    "label": 0
                },
                {
                    "sent": "Each expression is expression of the form, let's say direct duration, greater, equal to some value or for a nominal attributes that might be.",
                    "label": 0
                },
                {
                    "sent": "Equality or an equality sign and then the rule votes for a class low and the main advantage of decision rules is their simplicity and interpreter interpret ability.",
                    "label": 1
                },
                {
                    "sent": "Especially the rule ensemble is much easier to interpret than the tree ensemble.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, most of the the popular algorithms for rule induction are based on sequential covering.",
                    "label": 1
                },
                {
                    "sent": "This is, let's say, the old school of generating rules and well, most of the sequential covering algorithm suffered from the insufficient accuracy.",
                    "label": 0
                },
                {
                    "sent": "However, there are some.",
                    "label": 0
                },
                {
                    "sent": "There are some methods which use boosting approach and we also follow this boosting approach by treating each decision rule as a single classifiers based classifier in the ensemble.",
                    "label": 0
                },
                {
                    "sent": "And there are some methods which use boosting for inducing decision rules and those methods are rooted, slipper and lri.",
                    "label": 1
                },
                {
                    "sent": "And well, it was proved experimentally that boosting leads to powerful prediction model with.",
                    "label": 1
                },
                {
                    "sent": "With high accuracy, however, in our case or contrary to previous approaches, we try to estimate the conditional class distribution.",
                    "label": 0
                },
                {
                    "sent": "And we minimize the negative log likelihood.",
                    "label": 0
                },
                {
                    "sent": "The loss function which was used for example in logic boots or Mart on Friedman Smart with our classification trees and regression trees.",
                    "label": 1
                },
                {
                    "sent": "And estimating the class distribution allows us to work with any kind of loss matrix, so any kind of.",
                    "label": 0
                },
                {
                    "sent": "Misclassification costs.",
                    "label": 0
                },
                {
                    "sent": "And our approach can deal directly with the general K class problem.",
                    "label": 0
                },
                {
                    "sent": "So we do not need to use.",
                    "label": 0
                },
                {
                    "sent": "Let's say we need.",
                    "label": 0
                },
                {
                    "sent": "We do not need to solve the general many class problems by using by solving several binary classification problems.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we have data set D consisting of objects with their labels, an object with their labels labels and we have K classes.",
                    "label": 0
                },
                {
                    "sent": "So why is the class label of of each object and we estimate the probabilities with the maximum likelihood estimation which is in general maximizing the likelihood or equivalently minimizing the negative log likelihood, which is easier.",
                    "label": 1
                },
                {
                    "sent": "And well, probabilities.",
                    "label": 0
                },
                {
                    "sent": "Are always positive and they must sum to one, so it's better to use unconstrained vector F. And we just say which is related to the probabilities by the logistic transform.",
                    "label": 0
                },
                {
                    "sent": "And then by plugging this vector into the log likelihood, we minimize the following loss function.",
                    "label": 0
                },
                {
                    "sent": "And now the coordinates of the vector are unconstrained, so they can.",
                    "label": 0
                },
                {
                    "sent": "They can be any.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Real values.",
                    "label": 0
                },
                {
                    "sent": "How do we treat decision rule in our ensemble?",
                    "label": 0
                },
                {
                    "sent": "Well, we treat each decision rule as a function which returns a vector.",
                    "label": 1
                },
                {
                    "sent": "A constant vector in some subspace in some axis, parallel or rectangular region S. This is the region in which the conditions of the ruler satisfied and returns zero vector outside this outside this region.",
                    "label": 1
                },
                {
                    "sent": "And what is this constant vector?",
                    "label": 1
                },
                {
                    "sent": "This constant vector express the confidence of predicting the classes, so in particular, The Cave coordinate of the vector expresses the confidence of predicting The Cave class, and we also we also introduce an indicator function.",
                    "label": 1
                },
                {
                    "sent": "See such that it equals to one if and only if the object satisfies the conditions of the rules.",
                    "label": 0
                },
                {
                    "sent": "So if the objects is inside this region S and then decision rule can be written as.",
                    "label": 0
                },
                {
                    "sent": "This vector Alpha times the value of the indicator function.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now we assume that the classification function is a linear combination of EM rules, which is typical for all boosting strategies, and we use the boosting strategy by adding the rules 1 by 1 to the ensemble Gridley, minimizing the loss function.",
                    "label": 1
                },
                {
                    "sent": "There are many different points of view on boosting.",
                    "label": 0
                },
                {
                    "sent": "One is introduced by Asti Friedman and tipsy running which says that.",
                    "label": 0
                },
                {
                    "sent": "Boosting is nothing else than minimizing really minimizing some loss function.",
                    "label": 0
                },
                {
                    "sent": "In our case, this is the negative log likelihood, so we try to.",
                    "label": 0
                },
                {
                    "sent": "Generate the rule in M iteration which minimizes.",
                    "label": 0
                },
                {
                    "sent": "The value of the loss function.",
                    "label": 0
                },
                {
                    "sent": "Without changing the rules that have already been generated and the rules that have already been generated, is this F -- 1, so the assemble after ensemble after N -- 1 iterations?",
                    "label": 1
                },
                {
                    "sent": "And to simplify the thing, we restrict the rules only to the rules which vote for only one plus, so this vote vectors is has the form the following form where the Alpha is the positive value and.",
                    "label": 0
                },
                {
                    "sent": "The is some vector which has only which has zero and one at The Cave position, so it votes for The Cave plus and since there are no, there's no simple solution to this equation.",
                    "label": 1
                },
                {
                    "sent": "We need to have some kind of approximation and we propose two approximations, gradient.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Newton meters and 1st gradual method approximates the value of loss function for after N -- 1 iteration.",
                    "label": 0
                },
                {
                    "sent": "After the first order with respect to the Alpha value, which is the response of the rule.",
                    "label": 1
                },
                {
                    "sent": "And since the first term in the approximation is constant.",
                    "label": 0
                },
                {
                    "sent": "It results in minimization of the second term.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the other hand, the Newton method approximates the loss functions up to this second order, so we obtain free terms.",
                    "label": 1
                },
                {
                    "sent": "The first term is again this constant, but since the function is convex.",
                    "label": 0
                },
                {
                    "sent": "We can use a Newton step.",
                    "label": 0
                },
                {
                    "sent": "To minimize.",
                    "label": 0
                },
                {
                    "sent": "These are this expansion.",
                    "label": 0
                },
                {
                    "sent": "And by plugging this Newton step into the equation here, we end up with minimizing the following value.",
                    "label": 0
                },
                {
                    "sent": "So in.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Both cases to generate a rule we need to minimize some value.",
                    "label": 0
                },
                {
                    "sent": "Some function alarm.",
                    "label": 0
                },
                {
                    "sent": "And those expressions, though derivatives can easily be computed if we denote PIKD value of the current probability estimate using the ensemble in M -- 1 iteration.",
                    "label": 0
                },
                {
                    "sent": "We obtained a very simple formulas.",
                    "label": 0
                },
                {
                    "sent": "And they can be very easily determined once we obtain all the values of PIKIK by the logistic transfer.",
                    "label": 0
                },
                {
                    "sent": "From from those expressions first that Newton method tends to generate rules with small coverage.",
                    "label": 1
                },
                {
                    "sent": "This is because we have additional term in denominator here.",
                    "label": 0
                },
                {
                    "sent": "That's why Newton method.",
                    "label": 0
                },
                {
                    "sent": "That's why rules generated by Newton matches are usually smaller, so they cover less smaller number.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of examples.",
                    "label": 0
                },
                {
                    "sent": "So how does the procedure of rule generation look like?",
                    "label": 0
                },
                {
                    "sent": "Well, we obtain the value of C, which is in fact the set of conditions of the rule.",
                    "label": 0
                },
                {
                    "sent": "Because this is this indicator function, by minimizing the value of L. In both Newton and gradient methods and we start with the most general rules.",
                    "label": 0
                },
                {
                    "sent": "So the rule which covers the whole space and then we sequentially add conditions to the rule which is equivalent to trimming the rectangle as long as LM can be decreased.",
                    "label": 1
                },
                {
                    "sent": "So in contrary to the tree induction, there is another natural stopping condition, so we add conditions until we reach the minimum of the value of L. Then the response of the rule Alpha M is obtained by the Newton step, but we use shrinking.",
                    "label": 1
                },
                {
                    "sent": "Well, it it was proved experimentally that shrinking improves the accuracy.",
                    "label": 0
                },
                {
                    "sent": "Because shrinking is connected with regularization path with L1 norm.",
                    "label": 1
                },
                {
                    "sent": "We also apply regularization with the holdout set.",
                    "label": 1
                },
                {
                    "sent": "Instead of generating the rule on the whole data set, we choose a random subsample of 50% of objects to generate the rule, and then the response is obtained using all objects.",
                    "label": 1
                },
                {
                    "sent": "Since there are 50% of objects which were not seen during the rule generation, the calculating the response on the whole data set usually decrease, usually decreases the magnitude of the rule.",
                    "label": 0
                },
                {
                    "sent": "And can be thought of as an alternative to pruning.",
                    "label": 0
                },
                {
                    "sent": "And we also apply the simple procedure for stopping to assemble well on some cases.",
                    "label": 0
                },
                {
                    "sent": "Rule ensembles tends to overfit as the number of rule increases, so we use the stopping condition by estimating the quality of the rule of the on the holdout set.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our ruling samples can also be used in ordinal classification or ordinal regression.",
                    "label": 0
                },
                {
                    "sent": "Setting an and they can be easily adapted to capture the ordinal properties of the class labels, which is the essential thing in the ordinal regression.",
                    "label": 1
                },
                {
                    "sent": "And we the main changes to instead of generate rules which vote for only one class, we generate rules which vote for let's say at least and at most class unions.",
                    "label": 0
                },
                {
                    "sent": "So the sets.",
                    "label": 0
                },
                {
                    "sent": "Of the form from K. To the large scale from 1 to K so.",
                    "label": 0
                },
                {
                    "sent": "It says.",
                    "label": 1
                },
                {
                    "sent": "So it is generating the rules which increases the probability not of a single class, but on the whole class Union an it corresponds to generating the rules with the vectors V to be of the form consisting of minus ones and ones, or the opposite.",
                    "label": 1
                },
                {
                    "sent": "So at least K class or at most K class, and in fact the only change which we need to do in the algorithm is different different expressions for the derivatives.",
                    "label": 0
                },
                {
                    "sent": "Of the loss.",
                    "label": 0
                },
                {
                    "sent": "Now we need to we need to sum over the classes here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What related works in this area?",
                    "label": 0
                },
                {
                    "sent": "There are three three main algorithms which consists in building rule ensembles using.",
                    "label": 0
                },
                {
                    "sent": "Some kind of boosting strategy.",
                    "label": 0
                },
                {
                    "sent": "First was the slipper and it was proposed within the Adaboost scheme with confidence rated predictions.",
                    "label": 1
                },
                {
                    "sent": "So Slipper minimizes the exponential loss and slipper device.",
                    "label": 1
                },
                {
                    "sent": "The training set into growing, improving parts so it does pruning well in our case we used different loss functions.",
                    "label": 0
                },
                {
                    "sent": "We lost function.",
                    "label": 0
                },
                {
                    "sent": "We try to obtain the probability estimates.",
                    "label": 0
                },
                {
                    "sent": "And we also use this regularization instead of pruning procedures in our rule generation method.",
                    "label": 0
                },
                {
                    "sent": "Then it was lri.",
                    "label": 0
                },
                {
                    "sent": "So lightweight rule induction and it generates a bit different kind of rules rules which were in the form of DNF formulas and it uses used the different reweighting scheme based on cumulative error.",
                    "label": 1
                },
                {
                    "sent": "This re weighting scheme can also be explained within the boosting.",
                    "label": 0
                },
                {
                    "sent": "Within the boosting terms, by introducing a loss function which is a polynomial loss function.",
                    "label": 0
                },
                {
                    "sent": "And finally, roll fit by Freeman and prepare school role.",
                    "label": 0
                },
                {
                    "sent": "Fit is a method in which rules are not generated using boosting.",
                    "label": 1
                },
                {
                    "sent": "First, the tree ensemble is learned.",
                    "label": 1
                },
                {
                    "sent": "And only then the rules are produced from generated trees and the weights of the rules are fitted with L1 regularization.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, we did the computational experiment, which by which we could compare or do results of our approach with already existing approach to rule induction using boosting.",
                    "label": 0
                },
                {
                    "sent": "We used the 35 files from UCI repository.",
                    "label": 1
                },
                {
                    "sent": "20 of them were binary and 50 of them were multiclass datasets and we use several.",
                    "label": 0
                },
                {
                    "sent": "We set the following parameters in sleeper.",
                    "label": 0
                },
                {
                    "sent": "We used 500 iterations.",
                    "label": 0
                },
                {
                    "sent": "This is because sleeper has internal procedure to internal cross validation procedure which can detect the optimal number of rules.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "In other metals in Lri and roll feet we used.",
                    "label": 0
                },
                {
                    "sent": "In general, we use rather default parameters.",
                    "label": 1
                },
                {
                    "sent": "In our case, we use very small shrink shrinkage and large number of rules.",
                    "label": 0
                },
                {
                    "sent": "And accuracy was estimated using the tenfold cross validation and following the the paper of themselves.",
                    "label": 0
                },
                {
                    "sent": "We used nonparametric Friedman test to check whether all the algorithm algorithms perform equally well.",
                    "label": 1
                },
                {
                    "sent": "So this is the null hypothesis and the Friedman test is generally based on the average ranks and in this presentation we only restrict the analysis of the experiment to do binary class.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "So here are the binary class problem results.",
                    "label": 0
                },
                {
                    "sent": "Here are the results.",
                    "label": 0
                },
                {
                    "sent": "The average.",
                    "label": 0
                },
                {
                    "sent": "What's the average error?",
                    "label": 0
                },
                {
                    "sent": "01 error and the best results within each data sets were marked with Bolt and.",
                    "label": 0
                },
                {
                    "sent": "With each result there is a rank assigned.",
                    "label": 0
                },
                {
                    "sent": "And well.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are the the analysis of the results.",
                    "label": 0
                },
                {
                    "sent": "Well, three month state test states that the classifiers are not equally equally good.",
                    "label": 1
                },
                {
                    "sent": "So it means that we can proceed to a post hoc analysis.",
                    "label": 0
                },
                {
                    "sent": "Which are consistent calculating the critical difference and the critical difference has the interpretation that algorithms with difference in average ranks more than the value of the critical difference significant significantly different, and then some in his work showed that values of that the analysis can be can be.",
                    "label": 1
                },
                {
                    "sent": "Illustrated on such diagram.",
                    "label": 0
                },
                {
                    "sent": "Where on the let's say average rank access, we put each classifier.",
                    "label": 0
                },
                {
                    "sent": "And each classifier which is not significantly different.",
                    "label": 0
                },
                {
                    "sent": "Do another classifier.",
                    "label": 0
                },
                {
                    "sent": "Is joint.",
                    "label": 0
                },
                {
                    "sent": "With this black line.",
                    "label": 0
                },
                {
                    "sent": "So we see from this.",
                    "label": 0
                },
                {
                    "sent": "So we see from from this diagram that for example.",
                    "label": 0
                },
                {
                    "sent": "Our algorithms this is the Newton method and this is the grading method are not significantly different to the Elara Lri method.",
                    "label": 1
                },
                {
                    "sent": "While those three existing methods, we cannot say that any of them are significantly better than the other methods.",
                    "label": 0
                },
                {
                    "sent": "Well then we used also the multiclass problems and the results were roughly the same.",
                    "label": 0
                },
                {
                    "sent": "We need to exclude those two algorithms from the tests and we didn't obtain a significant significant differences differences between our algorithms and lri.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary.",
                    "label": 0
                },
                {
                    "sent": "We introduced a rule induction algorithm for solving classification problem by estimating the probabilities.",
                    "label": 1
                },
                {
                    "sent": "So our algorithm can work with.",
                    "label": 1
                },
                {
                    "sent": "In fact any loss matrix or enemies classification costs and our algorithm was shown to be competitive or outperforms the existing approaches.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned, it can be simply adapted to the ordinal regression problems.",
                    "label": 1
                },
                {
                    "sent": "OK, that's all, thank you.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Here's my question.",
                    "label": 0
                },
                {
                    "sent": "With respect to these probabilities that you have, did you try to get an idea as to how good they are?",
                    "label": 0
                },
                {
                    "sent": "So now you face is notoriously.",
                    "label": 0
                },
                {
                    "sent": "It's going to be notorious in estimating its probabilities.",
                    "label": 0
                },
                {
                    "sent": "Did you try to see if maybe?",
                    "label": 0
                },
                {
                    "sent": "Might introduce the change in there because I think measuring the accuracy is not necessarily fair in this case at least you would do some sort of ABC or something like that.",
                    "label": 0
                },
                {
                    "sent": "Learning that Sunseeker also evaluated from probability estimation.",
                    "label": 0
                },
                {
                    "sent": "While we measured the 01 last big cause, other algorithms which we compare with were not adapted to estimate the probabilities.",
                    "label": 0
                },
                {
                    "sent": "That's why.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Is it very interesting?",
                    "label": 0
                },
                {
                    "sent": "Compliance, etc.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "I have a couple questions actually.",
                    "label": 0
                },
                {
                    "sent": "So one of the advantages of rule learning systems is that the OR at least one of the claimed advantages is that the results are interpretable and easy for people to look at and understand part of the basis for that claim is that they often result in compact.",
                    "label": 0
                },
                {
                    "sent": "Don't do any studies to see what the size of the different rule sets are for your well, it depends.",
                    "label": 0
                },
                {
                    "sent": "It very depends on the amount of shrinkage which is applied in this problem.",
                    "label": 0
                },
                {
                    "sent": "So if we use small value of shrinkage then usually we need to generate let's say about 100 rules, which is quite a lot.",
                    "label": 0
                },
                {
                    "sent": "But then the accuracy is really high, but increasing the value of shrinkage which means that I mean increasing up to the value of one I think.",
                    "label": 0
                },
                {
                    "sent": "But there is, there is some kind of tradeoff between those two.",
                    "label": 0
                },
                {
                    "sent": "Those two factors.",
                    "label": 0
                },
                {
                    "sent": "So on the one hand, we can have very large, so let's say 100, two 100 rules, 1 sample and but its accuracy is very high.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we can decrease a bit accuracy, but obtain and much more compact, much, much, much more compact than sample and.",
                    "label": 0
                },
                {
                    "sent": "Well, we can.",
                    "label": 0
                },
                {
                    "sent": "Yeah we can change the value of this parameter and also it is because all the.",
                    "label": 0
                },
                {
                    "sent": "All the rules are weighted.",
                    "label": 0
                },
                {
                    "sent": "They have their own, let's say responses which can be treated as weights, so we can also choose from the model.",
                    "label": 0
                },
                {
                    "sent": "Let's say the rules which has the highest weights and show for example the.",
                    "label": 0
                },
                {
                    "sent": "If one wants to interpret the model, we can show only the most important rules from the ensemble.",
                    "label": 0
                },
                {
                    "sent": "So I mean it's just a conjecture I guess, but it seems like if you were trying to.",
                    "label": 0
                },
                {
                    "sent": "If you're using the loss function that you're using, like a logic looks like loss function.",
                    "label": 0
                },
                {
                    "sent": "We're really trying to accurately model the probabilities.",
                    "label": 0
                },
                {
                    "sent": "It seems like that might tend to give you larger rulesets just because it requires more information to accurately model the contours of the probability space then just to you know, Model A01 option so.",
                    "label": 0
                },
                {
                    "sent": "It seems like to some extent you're sort of.",
                    "label": 0
                },
                {
                    "sent": "You're you're giving your system at a disadvantage if you are comparing full cycles with something like lri.",
                    "label": 0
                },
                {
                    "sent": "So the second question I had was.",
                    "label": 0
                },
                {
                    "sent": "Quite like around Slide 9 maybe?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Kind of.",
                    "label": 0
                },
                {
                    "sent": "Correction for small numbers for these.",
                    "label": 0
                },
                {
                    "sent": "Probability, so in Dominator we used well, it's in fact not the Newton method, but Quasi Newton method in the in the sense that we put some small term in the denominator to remove the possibility that we could divide very very small number.",
                    "label": 0
                },
                {
                    "sent": "So if second derivative is really small so the curvature is really small then.",
                    "label": 0
                },
                {
                    "sent": "Then they did this step.",
                    "label": 0
                },
                {
                    "sent": "The size of the step or the value of the value of the rule response cannot be arbitrarily large.",
                    "label": 0
                },
                {
                    "sent": "Well, thank you very much, thank you.",
                    "label": 0
                }
            ]
        }
    }
}