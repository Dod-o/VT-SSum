{
    "id": "ftnv24pye4qwuwfhp54ryzyh5vprvbfu",
    "title": "The Neural Autoregressive Distribution Estimator, incl. discussion by Yoshua Bengio",
    "info": {
        "author": [
            "Yoshua Bengio, Department of Computer Science and Operations Research, University of Montreal",
            "Hugo Larochelle, Google, Inc."
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science"
        ]
    },
    "url": "http://videolectures.net/aistats2011_larochelle_neural/",
    "segmentation": [
        [
            "Alright, so yes, I'm going to talk about the neural autoregressive distribution estimator, so this is joint work with Ian."
        ],
        [
            "University of edinburg.",
            "So this is work about the problem of distribution estimation and so the task is to produce a good estimator of the probability distribution P of X based on samples from that distribution.",
            "And it's a very general problem.",
            "So if you have a good distribution estimator then you can solve a lot of different problems.",
            "For instance, if your observation separates into an input and a class label, then just using Bayes rule you can get a classifier.",
            "By first doing distribution estimation and then doing Bayes rule.",
            "Or if you have some observations in some, some of them are missing some of the elements are missing.",
            "Then we can try to fill those in and be somewhat more robust to filling inputs.",
            "It's also a very hard problem, and that's because as the dimensionality of the observation increases well effectively the problem sort of comes becomes exponentially harder because they are essentially more regions where you have to do a good prediction.",
            "And because we are distributing mass that needs to sum to one.",
            "If you make a bad prediction somewhere, say signing too much mass, then it means somewhere else.",
            "You'll be assignment assigning 2 little mass.",
            "So in a sense it's quite a hard problem as the dimensionality increases."
        ],
        [
            "So in this talk I'll be focusing mainly on estimating distributions of binary vectors, so observations where each element is either 01.",
            "I'll talk about how you could generalize that, but I would focus on this and then I'm going to review 2 successful approaches for modeling such data, so that include restricted Boltzmann machines, or BMS, an fully visible simulate belief network savvy SBN, and then I'll talk about is how we've built on these approaches in order to develop Nate.",
            "That is, neural aggressive distribution estimator, and then I'll talk about the experiments we ran and show it to state of the art."
        ],
        [
            "That problem?",
            "Alright, so first talk about restricted Boltzmann machine and so in an RBM you model the distribution of some input vector X with a layer of binary hidden units H. So we define a Markov random field over both, that's bipartite, and So what you do is that you define this energy function.",
            "We see here to assign an energy for some value, some configuration of X&H.",
            "And here you have interactions modeled by the W matrix, which sort of.",
            "Model the interactions between both as well as biases, C&B, that sort of expressed preferences for whether X or H should be more likely to be equal to 1 or 0.",
            "And then to get a distribution, we take the exponential of the negativity energy and then we just normalize by dividing by a normalization constant said which essentially sums over all possible values of X&H.",
            "This normalization constant is typically intractable because it's exponential really.",
            "To compute an, especially for very large hidden layers an so in particular, if you wanted to train this model to maximize the log likelihood of your data.",
            "Well, you'd like you're going to need to approximate somehow the gradient on the log of said.",
            "Fortunately, there's a lot of good work for doing this to get some sort of sampling approximations of that gradient, and then it's been shown that using these approximations you can get very good model of high dimensional binary observations.",
            "If anything, it's it's pretty much that the best you can get it gets really, really good.",
            "Performance is.",
            "Oddly enough, it's actually bad at estimating P of X, because the normalization constant is intractable, so you don't actually get technically speaking and distribution estimator.",
            "You get a good model another.",
            "Distribution estimator, because typically that is intractable, which is in this setting quite unfortunate."
        ],
        [
            "I'm going to discuss another type of approach, which is to try to fully visible Bayesian network.",
            "So in, then in a fully visible Bayesian network.",
            "What you do is you take your joint P of X and you decompose it into a product of conditionals, and these conditionals are going to be consistent with some dependency graph.",
            "So if you have some prior knowledge as to which variables depend on which others, you can use that, but typically you don't.",
            "So one thing that works surprisingly well in practice is that you just.",
            "Sample some random permutation for the observations and then you fix that and then you use the conditionals associated with that.",
            "So you just take your original observations and use a random permutation of them, and then you use a left to right dependency graph so you get them the product of the P of XK given X up to K. So that would give you the full distribution.",
            "No, there are a lot of different ways of getting a fully visible Bayesian network, and one of them is to replace P of XK given X up today by logistic regressors.",
            "So model these conditionals in the sort of lugging their family of distributions and so on the right.",
            "What you have is essentially forward propagation graph for computing these different conditionals, so I'm going to use X hat or X hat K for a shorthand meaning P of XK being equal to 1 given X up to K. And so in a sense, you can think of XC as some sort of predictive reconstruction of the observation X.",
            "Now, in practice this is a fairly good model actually of binary observations, and what's great is that it's tractable.",
            "That is, it is a distribution estimator.",
            "It normalizes, but it's not as good in practice, is in GBM, and so the question we tried to address in this in this work is to see how could we sort of combine both carbs in this framework of fully visible Bayesian network switch.",
            "Which gives gives us tractable distribution."
        ],
        [
            "Maters.",
            "Alright, so the question is, could we try to turn an RBM into a Bayesian network?",
            "Now, of course, any distribution P of X.",
            "We can just write it as it's the product of the left to right conditionals.",
            "So by doing this we don't really gain anything P of X is intractable, and then most of the conditionals also are going to be intractable.",
            "What we could do, however, is to actually approximate P of X given X up to K using some approximate inference procedure."
        ],
        [
            "And so with RBS, which often uses mean field.",
            "So what we'd be suggesting then is given X up to K, you do mean field over the remaining observation.",
            "That's X with index greater or equal than K an over the hidden units.",
            "Now if you derive mean field for this, you get the two equations you see here on the bottom and then feel would correspond to alternating the application of these two updates until convergence.",
            "So this procedure is guaranteed to converge, and then what you get in the end.",
            "Is an estimate of the probability of either the hidden units or the remaining observations being equal to 1 given X up to K. Those updates are also fairly simple.",
            "They're just linear transformations of which you have in the opposite layer pass through a non linear sigmoid OK."
        ],
        [
            "Now to make this approach more explicit, what we're proposing is that you could first do mean field over all the variables that includes all of XNH, and then once this has converged, you look at what's the value of the approximate approximated marginal at X1.",
            "And so we use that as our estimate of P of X1 given equal to 1 being equal."
        ],
        [
            "And then we will set the first observation to the X12 D actually observed value, and then will restart mean field and do meaningful overall remaining observations an H and so when this is, this has converged.",
            "You look at what's the approximated mean field output 4X2 and then will use that as our estimate of X to be equal to 1 given X one.",
            "And then you can."
        ],
        [
            "Use this procedure for all observations from left to right.",
            "OK."
        ],
        [
            "So how about this procedure?",
            "What's nice is that it's tractable, but it's actually still very inefficient, so typically with our BMS, mean field will require about 2030 iterations to converge an within GBM with a lot of hidden units.",
            "So we're talking about maybe 500 a few 1000 hidden units.",
            "This is going to be quite slow, 20 iterations, and so you can get an idea of what that means.",
            "It's kind of like doing forward propagation in the deep neural network with 20 hidden layers, or 30 hidden layers.",
            "That's quite expensive.",
            "The problem is that we actually have to do mean field for each conditional P of X given X up to K, so that makes it even more expensive.",
            "And sort of a separate issue is that this doesn't tell us how to improve training of carbs and our training carbs is, generally speaking, relatively hard problem.",
            "There's a lot of good research for doing this better, but most of them are based on essentially approximating the gradient of the negative log likelihood gradient, and those approximations are often sarcastic, and then it means that, for instance, we couldn't use a second order optimizer if you have for some data set, training RBM is actually.",
            "Our optimization problem.",
            "Then there might be issues.",
            "Also, there are no obvious stopping criteria since we can't evaluate P of X, then we can't really know when we started overfitting, for instance, which is in practice a little bit annoying.",
            "Now, it would seem that we didn't gain anything by sort of thinking about this procedure.",
            "This mean field procedure.",
            "But what we have learned is, what would the conditionals sort of look like in a fully Bayesian, fully visible Bayesian network?",
            "What kind of parametric form they would take for us to get good distribution estimators?",
            "Assuming of course that mean field is actually a good approximate inference procedure.",
            "So what we could do is just look at this sort of recursive mean field procedure.",
            "As just to some form some parametric form for getting the conditionals and try to use that as an inspiration for designing slightly different conditionals that we're going to look similar but are going to be more efficient and so."
        ],
        [
            "That's in effect what we did in this work, and that's how we got the new little aggressive distribution estimator.",
            "So, but briefly, what we did is that we took the.",
            "We took inspiration from the procedure I just described, and then based on this we derive an auto encoder where the output reconstruction can be used as the parameters of these conditionals and then obtain an actual distribution estimator.",
            "And so we did these three simple steps, which is first we looked at the procedure I just described.",
            "We only look at one iteration going up and then down.",
            "OK, so we actually compute the hidden units just once, and then a sort of reconstruction and predictive reconstruction for the remaining observations.",
            "We also untied the up and down wait, so that's going to give more parameters to our parametric form, and so presumably will be able to do better with that.",
            "And also instead of 1st training in GBM and then applying this procedure will actually fit these conditionals to our data.",
            "That is, we're going to maximize the log likelihood of our training data based on this functional form for P of X given X up ticking.",
            "So you have the two equations here.",
            "So in aid for the conditional P of X given X up to K, you first compute a hidden layer, HK, which is going to be simply the sigmoid of on the linear transformation of observation X up to K. And then from that hidden layer you'll literally linearly transform it with a different linear.",
            "But there is sort of separate set of parameters and then apply a signal in and that will give you X at K. So the probability of escaping equal to 1.",
            "So."
        ],
        [
            "Here I give you a more visual illustration of of this procedure, so as you can see.",
            "So this is essentially a forward propagation graph and it looks very similar to an auto encoder where at the output you get a reconstruction of the input.",
            "And you have this hidden layer which is separated, which is effectively just a bunch of hidden layers, smaller hidden layers, one for each observation.",
            "And also in blue you see which connections are tide together.",
            "That is, say the connection between X1 and all the hidden layers is the same for all the hidden layers.",
            "OK, so that's what the blue lines mean.",
            "And so if you were to do forward propagation in this grab graph in the somewhat naive way you would for each hidden layer, you would have computations in all of DH sode being the number of observations and H being the number of hidden units in each of these HKS.",
            "And then you need to do this for all hidden layers.",
            "There are as many hidden layers as observations, so this would all scale in all of the squared H. Which is a bit unfortunately means for very high dimensional problems then this by scaling with D squared.",
            "Somewhat unfortunate it's going to scale much less well."
        ],
        [
            "No, it turns out that we can leverage this weight sharing by noticing that if we look at the linear input of each hidden layer.",
            "So here you have the difference between the linear input for AHK plus one and HK.",
            "That difference is actually only the product of XK plus one times the column vector at position K plus one.",
            "So the difference between the computations you have to make at for The Cave conditional like plus one conditional only scales in all of H number of hidden units.",
            "So now it means that since we have the hidden layers to compute the whole encoding, computing all in layers can be done in oh of DH.",
            "So now it doesn't scale with the square of the, which is nice.",
            "And so in order to do for propagation in first computes H1 and then to compute H2, you'd look at the linear activation of H1.",
            "And then just add this column vector scaled by X2 and then you move like this.",
            "So on for all hidden layers."
        ],
        [
            "Alright, so to train this model you just do stochastic gradient descent.",
            "At least that's what we did in our experiments on the negative log likelihood of the training data.",
            "So like I said, computations are quite efficient.",
            "In particular, if you compare, say, with a fully visible simulate belief net, it actually scales in Ofd Square, and here we don't have this D squared term, we only have H * D and so syntactically for very large dimensional problems it would actually scale better.",
            "We can make use of 2nd order optimizers because we have a exact formula for the gradients, which is not true for our BMS.",
            "Typically, if you have a stochastic estimation of the gradients.",
            "And while quickly I can say that you could also generalize Nate for other types of distribution, say for real valued vectors, by instead of outputing at the output of the auto encoder.",
            "So instead of X had being the probability of XK being equal to 1, you could output say, I mean and then the standard deviation and use that for a Gaussian.",
            "So that would be a conditional Gaussian and then you would again train by the minimizing negative log likelihood of training.",
            "OK."
        ],
        [
            "No, before I move to the experiments.",
            "Want to mention a few related works.",
            "The first we're going to compare in our experiments.",
            "So one sort of obvious baseline is to compare with a mixture of multivariate Bernoulli's.",
            "There's some work that essentially showed that this performs better than different types of fully visible Bayesian Nets.",
            "So that's a pretty fair baseline.",
            "Another alternative is to actually use what I call small, tractable RPM's so it turns out that if you take in GBM and you restrict the size of the hidden layer, so say 25 hidden units, you can compute the partition function under normalization constant said fairly efficiently.",
            "It's going to take maybe a few seconds or more, maybe a few minutes, and since you only have to do this once, then once this has been done, this constant been has been computed, then you.",
            "Effectively have a distribution estimator, and so in previous work what we've looked at is whether we could maintain tractability of these smaller BMS by but increase the number of parameters by adding other constraints in the hidden layer.",
            "So instead of having individual binary, one thing we looked at is whether we could have groups of multinomial hidden units that is within one group you would impose that only one hidden hidden unit can be equal to 1 and the others need to be 0.",
            "We also looked at the restricted or develop the restricted Boltzmann forests, where instead of having groups of multinomial units you have St restructured hidden units and that particular model tended to do better than the mixture of multivariate Bernoulli.",
            "Of course we'll compare with a fully visible sigmoid belief net, but I want to talk about work by Yahoo and Sammy Benjo where they've looked at instead of using logistic regression for the conditionals they would use neural network conditional and now the sort of standard version of this model tended to over fit in some problems and will not do as well as fully visible sigmoid belief.",
            "That is, that's the main reason why we didn't explicitly compare with them, though in their paper they describe way of pruning the connections to avoid this overfitting.",
            "And so this pruning procedure is based on the juristic where you look at statistical tests between for identifying the dependencies between the observations and then based on this you could sort of pre prune the weights and set explicitly some weights to zero.",
            "And that solved most of the overfitting problems.",
            "Now we'd like to argue that Nathan is innocence simpler because as we'll see, we don't actually need to do this pruning in order to outperform fully visible sigmoid belief.",
            "Net and.",
            "And also presumably it would be possible.",
            "They would also benefit from the same pruning procedure."
        ],
        [
            "Alright, so experiments.",
            "So here what you have is experiments on eight different datasets, comparison with different baselines that I've just mentioned here.",
            "What you have is the average likelihood on the test data, and it's normalized by subtracting the log likelihood for the mixture of Bernoulli.",
            "So that's why the row for MLB mixture bernali is zero everywhere and well, the main point is that almost all across the board, the performance of Nate is better than the other approaches, and actually often it's quite significantly better with a large margin.",
            "I've mentioned before that when you use this fully visible basean net approach, you typically just use a random permutation of the observations.",
            "Now that might be a bit disturbing and you'd think that with different choices of this permutation, maybe the results could vary widely.",
            "Well, it turns out that in practice it doesn't, and so we did this with Nate and for four different data, three different datasets, we've changed our random permutations an looked at the standard deviation of the average test error would get.",
            "And as you can see, the standard deviation is quite small actually, so it doesn't seem really worth it to actually optimize this permutation."
        ],
        [
            "Now the final experiment.",
            "We decided we try to compare with an RBM where we can get an exact estimate of the partition function so we can get exact estimates of log likelihoods.",
            "So work by Russ Salakhutdinov, Annie and Marie are shown that you can get fairly good approximation, so we decided to compare with this.",
            "So this is results for carbs with 500 hidden units and their approximations.",
            "So those are numbers from their paper and these are BMS and the other models have been trained on a binarized version of Emnace where each pixel has been set to.",
            "Either 01 based on some threshold and so on.",
            "The left you have the test log likelihood for different methods.",
            "We've added the fully visible sigmoid belief net an Nate and as you can see, well again Nate outperforms FBN, but it's also quite close to the best RPM, so at minus 86 versus minus 88, and so it seems that we didn't sacrifice too much by gaining tractability, which is quite encouraging and on the right you have some samples from Nate which.",
            "For most of the truly are digits, and are fairly good."
        ],
        [
            "Alright, so I guess that's it, so I've presented made an estimator for distributions over vectors and as we can see it's pretty cool.",
            "It has very good performance on several datasets.",
            "If you want to try Nathan your own problem or actually want to compare with Nathan, these other baselines on the same data sets there is called available right now if you want to download into my web web page for future work, one of the things I want.",
            "One of the things I'd like to do is to.",
            "Star Trek tackling some supervised learning problems like classification and regression, and in particular with Nate.",
            "Since we define a distribution over the full joint, so all observations, it seems like the hybrid generative discriminative learning paradigm should be quite useful in order to obtain good performance.",
            "So in some previous work I did this with Joshua for our BMS and now I think I would like to do essentially a similar thing for classification, but also look at regression which here would be, I think, much simpler than within the PBM framework.",
            "And I guess that's it."
        ],
        [
            "Thank you."
        ],
        [
            "Right, so first I'd like to congratulate you level Shannon Ian Murray on this excellent paper.",
            "I'm going to try to.",
            "Give you a few comments.",
            "Oops, it starts."
        ],
        [
            "From PBM's which gives us a powerful distribution model.",
            "Right so um?",
            "The parametrization is inspired by RBM's, in particular by a mean field computation for the conditionals.",
            "And it's actually a structure very similar to left, right directed belief network that my brother, Sammy and I worked on 11 years ago."
        ],
        [
            "I'm just going to give you a flashback.",
            "So the idea was to decompose the joint with all the conditionals in some arbitrary order, an parameterized each of them with a neural net.",
            "The important idea.",
            "Is that?",
            "We reuse some of the heating units for all the conditionals, so if you look at predicting the second variable given the first variable with some hidden units.",
            "These hidden units provide a representation which can be exploited when you are going to compute the second variable given the third variable given the first 2 variables and that that sharing is buying something that you don't get.",
            "For example, with a sigmoid.",
            "Ull belief net that you mentioned.",
            "Now in need you have even."
        ],
        [
            "Or sharing.",
            "So.",
            "In addition to the same kind of sharing, you also have that the hidden units groups actually share a lot of share the same parameters, and in fact each of them is only looking at partial sums of the activations that you would get for the later groups, and that's why you can also get very efficient computation by exploiting these partial sums and not redoing them all the time."
        ],
        [
            "So when I read the paper, I was really impressed by the results.",
            "I mean, he just flashed this table of results, but I've actually worked with some of these datasets and try to get good performance and.",
            "Yeah, I think I think the results were very surprising and pretty good.",
            "There's also a thing he didn't mention, which is that on one of the datasets you can actually do a comparisons comparison with the results that we obtained 11 years ago, and so it looks like the PBM inspired constraints.",
            "These additional sharing seems to be very effective, so improves on these older results with regular SIG model neural Nets.",
            "Now to explain the other comparisons that you find in this paper.",
            "He was already mentioned that probably the reason it's working better than the fully visible sigmoidal belief net is we have these hidden units.",
            "We have these extra capacity.",
            "And then the other thing he didn't mention is that why is it working better than the PBM's in the comparisons?",
            "Well, in his comparisons, the PBM is that he had to choose where small to be tractable, so that you can compute the distribution function, and maybe that's the reason why you're getting better performance.",
            "So the."
        ],
        [
            "To get our BMS to have tractable distribution function, there just not enough capacity.",
            "Now, one thing that the reviewers of the paper were worried about is.",
            "You know we have to choose this ordering, so maybe that's the problem.",
            "It it doesn't sound very natural.",
            "One thing I thought about is that if you actually keep the output weights tide and I don't know what the experimental price of that would be in terms of accuracy, then you find that the parameters are invariant to the ordering.",
            "So you could reorder the inputs and use the same.",
            "Parameters so the same model could be used for any ordering.",
            "And that's great.",
            "It means, for example, you could train an for each example, use a different ordering, and so the model will be invariant to the ordering.",
            "I don't know if this would work in practice, but it's a nice feature."
        ],
        [
            "Of the model.",
            "One thing that I was a bit worried about also when I looked at the paper was that if you look at what the hidden units are doing as you go from the left to right to these groups there.",
            "Essentially have the same input but getting larger and larger as you consider more more of the variables.",
            "And so they will tend to saturate more and more.",
            "And in neural Nets research usually this is considered bad, so I don't know.",
            "Maybe it's natural that the probabilities become sharper because you get more evidence about what the hidden units should be.",
            "But actually there would be a way to get around that by adding extra degrees of freedom to scale the sums for each group."
        ],
        [
            "So that's it.",
            "I think it's a very interesting model.",
            "It computes tractably probably function that gradient is tractable and deterministic, and exact.",
            "You can sample from it exactly exactly.",
            "It can learn nonlinear relationships, it works.",
            "I'm not sure how to extend that to deeper models, though.",
            "That's an interesting question, so again, congratulations for your work."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so yes, I'm going to talk about the neural autoregressive distribution estimator, so this is joint work with Ian.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "University of edinburg.",
                    "label": 0
                },
                {
                    "sent": "So this is work about the problem of distribution estimation and so the task is to produce a good estimator of the probability distribution P of X based on samples from that distribution.",
                    "label": 1
                },
                {
                    "sent": "And it's a very general problem.",
                    "label": 0
                },
                {
                    "sent": "So if you have a good distribution estimator then you can solve a lot of different problems.",
                    "label": 0
                },
                {
                    "sent": "For instance, if your observation separates into an input and a class label, then just using Bayes rule you can get a classifier.",
                    "label": 0
                },
                {
                    "sent": "By first doing distribution estimation and then doing Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "Or if you have some observations in some, some of them are missing some of the elements are missing.",
                    "label": 0
                },
                {
                    "sent": "Then we can try to fill those in and be somewhat more robust to filling inputs.",
                    "label": 0
                },
                {
                    "sent": "It's also a very hard problem, and that's because as the dimensionality of the observation increases well effectively the problem sort of comes becomes exponentially harder because they are essentially more regions where you have to do a good prediction.",
                    "label": 0
                },
                {
                    "sent": "And because we are distributing mass that needs to sum to one.",
                    "label": 0
                },
                {
                    "sent": "If you make a bad prediction somewhere, say signing too much mass, then it means somewhere else.",
                    "label": 0
                },
                {
                    "sent": "You'll be assignment assigning 2 little mass.",
                    "label": 0
                },
                {
                    "sent": "So in a sense it's quite a hard problem as the dimensionality increases.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this talk I'll be focusing mainly on estimating distributions of binary vectors, so observations where each element is either 01.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about how you could generalize that, but I would focus on this and then I'm going to review 2 successful approaches for modeling such data, so that include restricted Boltzmann machines, or BMS, an fully visible simulate belief network savvy SBN, and then I'll talk about is how we've built on these approaches in order to develop Nate.",
                    "label": 1
                },
                {
                    "sent": "That is, neural aggressive distribution estimator, and then I'll talk about the experiments we ran and show it to state of the art.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That problem?",
                    "label": 0
                },
                {
                    "sent": "Alright, so first talk about restricted Boltzmann machine and so in an RBM you model the distribution of some input vector X with a layer of binary hidden units H. So we define a Markov random field over both, that's bipartite, and So what you do is that you define this energy function.",
                    "label": 1
                },
                {
                    "sent": "We see here to assign an energy for some value, some configuration of X&H.",
                    "label": 0
                },
                {
                    "sent": "And here you have interactions modeled by the W matrix, which sort of.",
                    "label": 0
                },
                {
                    "sent": "Model the interactions between both as well as biases, C&B, that sort of expressed preferences for whether X or H should be more likely to be equal to 1 or 0.",
                    "label": 0
                },
                {
                    "sent": "And then to get a distribution, we take the exponential of the negativity energy and then we just normalize by dividing by a normalization constant said which essentially sums over all possible values of X&H.",
                    "label": 0
                },
                {
                    "sent": "This normalization constant is typically intractable because it's exponential really.",
                    "label": 0
                },
                {
                    "sent": "To compute an, especially for very large hidden layers an so in particular, if you wanted to train this model to maximize the log likelihood of your data.",
                    "label": 0
                },
                {
                    "sent": "Well, you'd like you're going to need to approximate somehow the gradient on the log of said.",
                    "label": 0
                },
                {
                    "sent": "Fortunately, there's a lot of good work for doing this to get some sort of sampling approximations of that gradient, and then it's been shown that using these approximations you can get very good model of high dimensional binary observations.",
                    "label": 1
                },
                {
                    "sent": "If anything, it's it's pretty much that the best you can get it gets really, really good.",
                    "label": 0
                },
                {
                    "sent": "Performance is.",
                    "label": 0
                },
                {
                    "sent": "Oddly enough, it's actually bad at estimating P of X, because the normalization constant is intractable, so you don't actually get technically speaking and distribution estimator.",
                    "label": 0
                },
                {
                    "sent": "You get a good model another.",
                    "label": 0
                },
                {
                    "sent": "Distribution estimator, because typically that is intractable, which is in this setting quite unfortunate.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to discuss another type of approach, which is to try to fully visible Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "So in, then in a fully visible Bayesian network.",
                    "label": 1
                },
                {
                    "sent": "What you do is you take your joint P of X and you decompose it into a product of conditionals, and these conditionals are going to be consistent with some dependency graph.",
                    "label": 0
                },
                {
                    "sent": "So if you have some prior knowledge as to which variables depend on which others, you can use that, but typically you don't.",
                    "label": 0
                },
                {
                    "sent": "So one thing that works surprisingly well in practice is that you just.",
                    "label": 0
                },
                {
                    "sent": "Sample some random permutation for the observations and then you fix that and then you use the conditionals associated with that.",
                    "label": 0
                },
                {
                    "sent": "So you just take your original observations and use a random permutation of them, and then you use a left to right dependency graph so you get them the product of the P of XK given X up to K. So that would give you the full distribution.",
                    "label": 0
                },
                {
                    "sent": "No, there are a lot of different ways of getting a fully visible Bayesian network, and one of them is to replace P of XK given X up today by logistic regressors.",
                    "label": 0
                },
                {
                    "sent": "So model these conditionals in the sort of lugging their family of distributions and so on the right.",
                    "label": 0
                },
                {
                    "sent": "What you have is essentially forward propagation graph for computing these different conditionals, so I'm going to use X hat or X hat K for a shorthand meaning P of XK being equal to 1 given X up to K. And so in a sense, you can think of XC as some sort of predictive reconstruction of the observation X.",
                    "label": 0
                },
                {
                    "sent": "Now, in practice this is a fairly good model actually of binary observations, and what's great is that it's tractable.",
                    "label": 0
                },
                {
                    "sent": "That is, it is a distribution estimator.",
                    "label": 0
                },
                {
                    "sent": "It normalizes, but it's not as good in practice, is in GBM, and so the question we tried to address in this in this work is to see how could we sort of combine both carbs in this framework of fully visible Bayesian network switch.",
                    "label": 1
                },
                {
                    "sent": "Which gives gives us tractable distribution.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maters.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the question is, could we try to turn an RBM into a Bayesian network?",
                    "label": 1
                },
                {
                    "sent": "Now, of course, any distribution P of X.",
                    "label": 0
                },
                {
                    "sent": "We can just write it as it's the product of the left to right conditionals.",
                    "label": 0
                },
                {
                    "sent": "So by doing this we don't really gain anything P of X is intractable, and then most of the conditionals also are going to be intractable.",
                    "label": 0
                },
                {
                    "sent": "What we could do, however, is to actually approximate P of X given X up to K using some approximate inference procedure.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so with RBS, which often uses mean field.",
                    "label": 0
                },
                {
                    "sent": "So what we'd be suggesting then is given X up to K, you do mean field over the remaining observation.",
                    "label": 0
                },
                {
                    "sent": "That's X with index greater or equal than K an over the hidden units.",
                    "label": 0
                },
                {
                    "sent": "Now if you derive mean field for this, you get the two equations you see here on the bottom and then feel would correspond to alternating the application of these two updates until convergence.",
                    "label": 0
                },
                {
                    "sent": "So this procedure is guaranteed to converge, and then what you get in the end.",
                    "label": 0
                },
                {
                    "sent": "Is an estimate of the probability of either the hidden units or the remaining observations being equal to 1 given X up to K. Those updates are also fairly simple.",
                    "label": 0
                },
                {
                    "sent": "They're just linear transformations of which you have in the opposite layer pass through a non linear sigmoid OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now to make this approach more explicit, what we're proposing is that you could first do mean field over all the variables that includes all of XNH, and then once this has converged, you look at what's the value of the approximate approximated marginal at X1.",
                    "label": 0
                },
                {
                    "sent": "And so we use that as our estimate of P of X1 given equal to 1 being equal.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we will set the first observation to the X12 D actually observed value, and then will restart mean field and do meaningful overall remaining observations an H and so when this is, this has converged.",
                    "label": 0
                },
                {
                    "sent": "You look at what's the approximated mean field output 4X2 and then will use that as our estimate of X to be equal to 1 given X one.",
                    "label": 0
                },
                {
                    "sent": "And then you can.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use this procedure for all observations from left to right.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how about this procedure?",
                    "label": 0
                },
                {
                    "sent": "What's nice is that it's tractable, but it's actually still very inefficient, so typically with our BMS, mean field will require about 2030 iterations to converge an within GBM with a lot of hidden units.",
                    "label": 0
                },
                {
                    "sent": "So we're talking about maybe 500 a few 1000 hidden units.",
                    "label": 0
                },
                {
                    "sent": "This is going to be quite slow, 20 iterations, and so you can get an idea of what that means.",
                    "label": 0
                },
                {
                    "sent": "It's kind of like doing forward propagation in the deep neural network with 20 hidden layers, or 30 hidden layers.",
                    "label": 0
                },
                {
                    "sent": "That's quite expensive.",
                    "label": 0
                },
                {
                    "sent": "The problem is that we actually have to do mean field for each conditional P of X given X up to K, so that makes it even more expensive.",
                    "label": 1
                },
                {
                    "sent": "And sort of a separate issue is that this doesn't tell us how to improve training of carbs and our training carbs is, generally speaking, relatively hard problem.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of good research for doing this better, but most of them are based on essentially approximating the gradient of the negative log likelihood gradient, and those approximations are often sarcastic, and then it means that, for instance, we couldn't use a second order optimizer if you have for some data set, training RBM is actually.",
                    "label": 0
                },
                {
                    "sent": "Our optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Then there might be issues.",
                    "label": 1
                },
                {
                    "sent": "Also, there are no obvious stopping criteria since we can't evaluate P of X, then we can't really know when we started overfitting, for instance, which is in practice a little bit annoying.",
                    "label": 0
                },
                {
                    "sent": "Now, it would seem that we didn't gain anything by sort of thinking about this procedure.",
                    "label": 1
                },
                {
                    "sent": "This mean field procedure.",
                    "label": 0
                },
                {
                    "sent": "But what we have learned is, what would the conditionals sort of look like in a fully Bayesian, fully visible Bayesian network?",
                    "label": 0
                },
                {
                    "sent": "What kind of parametric form they would take for us to get good distribution estimators?",
                    "label": 0
                },
                {
                    "sent": "Assuming of course that mean field is actually a good approximate inference procedure.",
                    "label": 0
                },
                {
                    "sent": "So what we could do is just look at this sort of recursive mean field procedure.",
                    "label": 0
                },
                {
                    "sent": "As just to some form some parametric form for getting the conditionals and try to use that as an inspiration for designing slightly different conditionals that we're going to look similar but are going to be more efficient and so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's in effect what we did in this work, and that's how we got the new little aggressive distribution estimator.",
                    "label": 1
                },
                {
                    "sent": "So, but briefly, what we did is that we took the.",
                    "label": 0
                },
                {
                    "sent": "We took inspiration from the procedure I just described, and then based on this we derive an auto encoder where the output reconstruction can be used as the parameters of these conditionals and then obtain an actual distribution estimator.",
                    "label": 0
                },
                {
                    "sent": "And so we did these three simple steps, which is first we looked at the procedure I just described.",
                    "label": 0
                },
                {
                    "sent": "We only look at one iteration going up and then down.",
                    "label": 0
                },
                {
                    "sent": "OK, so we actually compute the hidden units just once, and then a sort of reconstruction and predictive reconstruction for the remaining observations.",
                    "label": 0
                },
                {
                    "sent": "We also untied the up and down wait, so that's going to give more parameters to our parametric form, and so presumably will be able to do better with that.",
                    "label": 1
                },
                {
                    "sent": "And also instead of 1st training in GBM and then applying this procedure will actually fit these conditionals to our data.",
                    "label": 0
                },
                {
                    "sent": "That is, we're going to maximize the log likelihood of our training data based on this functional form for P of X given X up ticking.",
                    "label": 0
                },
                {
                    "sent": "So you have the two equations here.",
                    "label": 0
                },
                {
                    "sent": "So in aid for the conditional P of X given X up to K, you first compute a hidden layer, HK, which is going to be simply the sigmoid of on the linear transformation of observation X up to K. And then from that hidden layer you'll literally linearly transform it with a different linear.",
                    "label": 0
                },
                {
                    "sent": "But there is sort of separate set of parameters and then apply a signal in and that will give you X at K. So the probability of escaping equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here I give you a more visual illustration of of this procedure, so as you can see.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially a forward propagation graph and it looks very similar to an auto encoder where at the output you get a reconstruction of the input.",
                    "label": 0
                },
                {
                    "sent": "And you have this hidden layer which is separated, which is effectively just a bunch of hidden layers, smaller hidden layers, one for each observation.",
                    "label": 0
                },
                {
                    "sent": "And also in blue you see which connections are tide together.",
                    "label": 0
                },
                {
                    "sent": "That is, say the connection between X1 and all the hidden layers is the same for all the hidden layers.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what the blue lines mean.",
                    "label": 0
                },
                {
                    "sent": "And so if you were to do forward propagation in this grab graph in the somewhat naive way you would for each hidden layer, you would have computations in all of DH sode being the number of observations and H being the number of hidden units in each of these HKS.",
                    "label": 0
                },
                {
                    "sent": "And then you need to do this for all hidden layers.",
                    "label": 0
                },
                {
                    "sent": "There are as many hidden layers as observations, so this would all scale in all of the squared H. Which is a bit unfortunately means for very high dimensional problems then this by scaling with D squared.",
                    "label": 0
                },
                {
                    "sent": "Somewhat unfortunate it's going to scale much less well.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, it turns out that we can leverage this weight sharing by noticing that if we look at the linear input of each hidden layer.",
                    "label": 0
                },
                {
                    "sent": "So here you have the difference between the linear input for AHK plus one and HK.",
                    "label": 0
                },
                {
                    "sent": "That difference is actually only the product of XK plus one times the column vector at position K plus one.",
                    "label": 0
                },
                {
                    "sent": "So the difference between the computations you have to make at for The Cave conditional like plus one conditional only scales in all of H number of hidden units.",
                    "label": 0
                },
                {
                    "sent": "So now it means that since we have the hidden layers to compute the whole encoding, computing all in layers can be done in oh of DH.",
                    "label": 0
                },
                {
                    "sent": "So now it doesn't scale with the square of the, which is nice.",
                    "label": 0
                },
                {
                    "sent": "And so in order to do for propagation in first computes H1 and then to compute H2, you'd look at the linear activation of H1.",
                    "label": 0
                },
                {
                    "sent": "And then just add this column vector scaled by X2 and then you move like this.",
                    "label": 0
                },
                {
                    "sent": "So on for all hidden layers.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so to train this model you just do stochastic gradient descent.",
                    "label": 1
                },
                {
                    "sent": "At least that's what we did in our experiments on the negative log likelihood of the training data.",
                    "label": 1
                },
                {
                    "sent": "So like I said, computations are quite efficient.",
                    "label": 0
                },
                {
                    "sent": "In particular, if you compare, say, with a fully visible simulate belief net, it actually scales in Ofd Square, and here we don't have this D squared term, we only have H * D and so syntactically for very large dimensional problems it would actually scale better.",
                    "label": 1
                },
                {
                    "sent": "We can make use of 2nd order optimizers because we have a exact formula for the gradients, which is not true for our BMS.",
                    "label": 0
                },
                {
                    "sent": "Typically, if you have a stochastic estimation of the gradients.",
                    "label": 0
                },
                {
                    "sent": "And while quickly I can say that you could also generalize Nate for other types of distribution, say for real valued vectors, by instead of outputing at the output of the auto encoder.",
                    "label": 0
                },
                {
                    "sent": "So instead of X had being the probability of XK being equal to 1, you could output say, I mean and then the standard deviation and use that for a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So that would be a conditional Gaussian and then you would again train by the minimizing negative log likelihood of training.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, before I move to the experiments.",
                    "label": 0
                },
                {
                    "sent": "Want to mention a few related works.",
                    "label": 0
                },
                {
                    "sent": "The first we're going to compare in our experiments.",
                    "label": 0
                },
                {
                    "sent": "So one sort of obvious baseline is to compare with a mixture of multivariate Bernoulli's.",
                    "label": 1
                },
                {
                    "sent": "There's some work that essentially showed that this performs better than different types of fully visible Bayesian Nets.",
                    "label": 0
                },
                {
                    "sent": "So that's a pretty fair baseline.",
                    "label": 0
                },
                {
                    "sent": "Another alternative is to actually use what I call small, tractable RPM's so it turns out that if you take in GBM and you restrict the size of the hidden layer, so say 25 hidden units, you can compute the partition function under normalization constant said fairly efficiently.",
                    "label": 0
                },
                {
                    "sent": "It's going to take maybe a few seconds or more, maybe a few minutes, and since you only have to do this once, then once this has been done, this constant been has been computed, then you.",
                    "label": 0
                },
                {
                    "sent": "Effectively have a distribution estimator, and so in previous work what we've looked at is whether we could maintain tractability of these smaller BMS by but increase the number of parameters by adding other constraints in the hidden layer.",
                    "label": 0
                },
                {
                    "sent": "So instead of having individual binary, one thing we looked at is whether we could have groups of multinomial hidden units that is within one group you would impose that only one hidden hidden unit can be equal to 1 and the others need to be 0.",
                    "label": 1
                },
                {
                    "sent": "We also looked at the restricted or develop the restricted Boltzmann forests, where instead of having groups of multinomial units you have St restructured hidden units and that particular model tended to do better than the mixture of multivariate Bernoulli.",
                    "label": 1
                },
                {
                    "sent": "Of course we'll compare with a fully visible sigmoid belief net, but I want to talk about work by Yahoo and Sammy Benjo where they've looked at instead of using logistic regression for the conditionals they would use neural network conditional and now the sort of standard version of this model tended to over fit in some problems and will not do as well as fully visible sigmoid belief.",
                    "label": 0
                },
                {
                    "sent": "That is, that's the main reason why we didn't explicitly compare with them, though in their paper they describe way of pruning the connections to avoid this overfitting.",
                    "label": 0
                },
                {
                    "sent": "And so this pruning procedure is based on the juristic where you look at statistical tests between for identifying the dependencies between the observations and then based on this you could sort of pre prune the weights and set explicitly some weights to zero.",
                    "label": 0
                },
                {
                    "sent": "And that solved most of the overfitting problems.",
                    "label": 0
                },
                {
                    "sent": "Now we'd like to argue that Nathan is innocence simpler because as we'll see, we don't actually need to do this pruning in order to outperform fully visible sigmoid belief.",
                    "label": 0
                },
                {
                    "sent": "Net and.",
                    "label": 0
                },
                {
                    "sent": "And also presumably it would be possible.",
                    "label": 0
                },
                {
                    "sent": "They would also benefit from the same pruning procedure.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so experiments.",
                    "label": 0
                },
                {
                    "sent": "So here what you have is experiments on eight different datasets, comparison with different baselines that I've just mentioned here.",
                    "label": 0
                },
                {
                    "sent": "What you have is the average likelihood on the test data, and it's normalized by subtracting the log likelihood for the mixture of Bernoulli.",
                    "label": 1
                },
                {
                    "sent": "So that's why the row for MLB mixture bernali is zero everywhere and well, the main point is that almost all across the board, the performance of Nate is better than the other approaches, and actually often it's quite significantly better with a large margin.",
                    "label": 0
                },
                {
                    "sent": "I've mentioned before that when you use this fully visible basean net approach, you typically just use a random permutation of the observations.",
                    "label": 1
                },
                {
                    "sent": "Now that might be a bit disturbing and you'd think that with different choices of this permutation, maybe the results could vary widely.",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out that in practice it doesn't, and so we did this with Nate and for four different data, three different datasets, we've changed our random permutations an looked at the standard deviation of the average test error would get.",
                    "label": 1
                },
                {
                    "sent": "And as you can see, the standard deviation is quite small actually, so it doesn't seem really worth it to actually optimize this permutation.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the final experiment.",
                    "label": 0
                },
                {
                    "sent": "We decided we try to compare with an RBM where we can get an exact estimate of the partition function so we can get exact estimates of log likelihoods.",
                    "label": 0
                },
                {
                    "sent": "So work by Russ Salakhutdinov, Annie and Marie are shown that you can get fairly good approximation, so we decided to compare with this.",
                    "label": 0
                },
                {
                    "sent": "So this is results for carbs with 500 hidden units and their approximations.",
                    "label": 0
                },
                {
                    "sent": "So those are numbers from their paper and these are BMS and the other models have been trained on a binarized version of Emnace where each pixel has been set to.",
                    "label": 1
                },
                {
                    "sent": "Either 01 based on some threshold and so on.",
                    "label": 0
                },
                {
                    "sent": "The left you have the test log likelihood for different methods.",
                    "label": 0
                },
                {
                    "sent": "We've added the fully visible sigmoid belief net an Nate and as you can see, well again Nate outperforms FBN, but it's also quite close to the best RPM, so at minus 86 versus minus 88, and so it seems that we didn't sacrifice too much by gaining tractability, which is quite encouraging and on the right you have some samples from Nate which.",
                    "label": 0
                },
                {
                    "sent": "For most of the truly are digits, and are fairly good.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so I guess that's it, so I've presented made an estimator for distributions over vectors and as we can see it's pretty cool.",
                    "label": 1
                },
                {
                    "sent": "It has very good performance on several datasets.",
                    "label": 1
                },
                {
                    "sent": "If you want to try Nathan your own problem or actually want to compare with Nathan, these other baselines on the same data sets there is called available right now if you want to download into my web web page for future work, one of the things I want.",
                    "label": 1
                },
                {
                    "sent": "One of the things I'd like to do is to.",
                    "label": 0
                },
                {
                    "sent": "Star Trek tackling some supervised learning problems like classification and regression, and in particular with Nate.",
                    "label": 0
                },
                {
                    "sent": "Since we define a distribution over the full joint, so all observations, it seems like the hybrid generative discriminative learning paradigm should be quite useful in order to obtain good performance.",
                    "label": 0
                },
                {
                    "sent": "So in some previous work I did this with Joshua for our BMS and now I think I would like to do essentially a similar thing for classification, but also look at regression which here would be, I think, much simpler than within the PBM framework.",
                    "label": 0
                },
                {
                    "sent": "And I guess that's it.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so first I'd like to congratulate you level Shannon Ian Murray on this excellent paper.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to.",
                    "label": 0
                },
                {
                    "sent": "Give you a few comments.",
                    "label": 0
                },
                {
                    "sent": "Oops, it starts.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From PBM's which gives us a powerful distribution model.",
                    "label": 1
                },
                {
                    "sent": "Right so um?",
                    "label": 1
                },
                {
                    "sent": "The parametrization is inspired by RBM's, in particular by a mean field computation for the conditionals.",
                    "label": 0
                },
                {
                    "sent": "And it's actually a structure very similar to left, right directed belief network that my brother, Sammy and I worked on 11 years ago.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm just going to give you a flashback.",
                    "label": 0
                },
                {
                    "sent": "So the idea was to decompose the joint with all the conditionals in some arbitrary order, an parameterized each of them with a neural net.",
                    "label": 0
                },
                {
                    "sent": "The important idea.",
                    "label": 0
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "We reuse some of the heating units for all the conditionals, so if you look at predicting the second variable given the first variable with some hidden units.",
                    "label": 0
                },
                {
                    "sent": "These hidden units provide a representation which can be exploited when you are going to compute the second variable given the third variable given the first 2 variables and that that sharing is buying something that you don't get.",
                    "label": 0
                },
                {
                    "sent": "For example, with a sigmoid.",
                    "label": 0
                },
                {
                    "sent": "Ull belief net that you mentioned.",
                    "label": 0
                },
                {
                    "sent": "Now in need you have even.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or sharing.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In addition to the same kind of sharing, you also have that the hidden units groups actually share a lot of share the same parameters, and in fact each of them is only looking at partial sums of the activations that you would get for the later groups, and that's why you can also get very efficient computation by exploiting these partial sums and not redoing them all the time.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when I read the paper, I was really impressed by the results.",
                    "label": 0
                },
                {
                    "sent": "I mean, he just flashed this table of results, but I've actually worked with some of these datasets and try to get good performance and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think I think the results were very surprising and pretty good.",
                    "label": 0
                },
                {
                    "sent": "There's also a thing he didn't mention, which is that on one of the datasets you can actually do a comparisons comparison with the results that we obtained 11 years ago, and so it looks like the PBM inspired constraints.",
                    "label": 0
                },
                {
                    "sent": "These additional sharing seems to be very effective, so improves on these older results with regular SIG model neural Nets.",
                    "label": 1
                },
                {
                    "sent": "Now to explain the other comparisons that you find in this paper.",
                    "label": 1
                },
                {
                    "sent": "He was already mentioned that probably the reason it's working better than the fully visible sigmoidal belief net is we have these hidden units.",
                    "label": 0
                },
                {
                    "sent": "We have these extra capacity.",
                    "label": 0
                },
                {
                    "sent": "And then the other thing he didn't mention is that why is it working better than the PBM's in the comparisons?",
                    "label": 0
                },
                {
                    "sent": "Well, in his comparisons, the PBM is that he had to choose where small to be tractable, so that you can compute the distribution function, and maybe that's the reason why you're getting better performance.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To get our BMS to have tractable distribution function, there just not enough capacity.",
                    "label": 0
                },
                {
                    "sent": "Now, one thing that the reviewers of the paper were worried about is.",
                    "label": 0
                },
                {
                    "sent": "You know we have to choose this ordering, so maybe that's the problem.",
                    "label": 0
                },
                {
                    "sent": "It it doesn't sound very natural.",
                    "label": 0
                },
                {
                    "sent": "One thing I thought about is that if you actually keep the output weights tide and I don't know what the experimental price of that would be in terms of accuracy, then you find that the parameters are invariant to the ordering.",
                    "label": 0
                },
                {
                    "sent": "So you could reorder the inputs and use the same.",
                    "label": 0
                },
                {
                    "sent": "Parameters so the same model could be used for any ordering.",
                    "label": 1
                },
                {
                    "sent": "And that's great.",
                    "label": 0
                },
                {
                    "sent": "It means, for example, you could train an for each example, use a different ordering, and so the model will be invariant to the ordering.",
                    "label": 1
                },
                {
                    "sent": "I don't know if this would work in practice, but it's a nice feature.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of the model.",
                    "label": 0
                },
                {
                    "sent": "One thing that I was a bit worried about also when I looked at the paper was that if you look at what the hidden units are doing as you go from the left to right to these groups there.",
                    "label": 0
                },
                {
                    "sent": "Essentially have the same input but getting larger and larger as you consider more more of the variables.",
                    "label": 1
                },
                {
                    "sent": "And so they will tend to saturate more and more.",
                    "label": 1
                },
                {
                    "sent": "And in neural Nets research usually this is considered bad, so I don't know.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's natural that the probabilities become sharper because you get more evidence about what the hidden units should be.",
                    "label": 0
                },
                {
                    "sent": "But actually there would be a way to get around that by adding extra degrees of freedom to scale the sums for each group.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's it.",
                    "label": 0
                },
                {
                    "sent": "I think it's a very interesting model.",
                    "label": 0
                },
                {
                    "sent": "It computes tractably probably function that gradient is tractable and deterministic, and exact.",
                    "label": 0
                },
                {
                    "sent": "You can sample from it exactly exactly.",
                    "label": 0
                },
                {
                    "sent": "It can learn nonlinear relationships, it works.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure how to extend that to deeper models, though.",
                    "label": 1
                },
                {
                    "sent": "That's an interesting question, so again, congratulations for your work.",
                    "label": 0
                }
            ]
        }
    }
}