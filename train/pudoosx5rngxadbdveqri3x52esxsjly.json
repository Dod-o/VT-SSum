{
    "id": "pudoosx5rngxadbdveqri3x52esxsjly",
    "title": "Perturbative Corrections to Expectation Consistent Approximate Inference",
    "info": {
        "author": [
            "Manfred Opper, TU Berlin"
        ],
        "published": "Dec. 31, 2007",
        "recorded": "December 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/abi07_opper_pce/",
    "segmentation": [
        [
            "I'd like to thank the organizers for inviting me and.",
            "This is joint work with Lori Canola and it's ongoing work on trying to compute corrections to expectation propagation, and.",
            "Right, OK, so the idea is of course, if we if we run an approximate inference algorithm, we don't know how close the answer is to the answer to the exact answer.",
            "I mean the exact answer means if we would be able to do exact inference and the question is can we sort of quantify what we have lost by using the approximation.",
            "Of course I can't give you the complete answer.",
            "That would mean I would be able to solve the complete inference problems.",
            "But we can do it in a certain what we call a perturbative correction.",
            "So it's in the spirit of.",
            "Let's say you wanted to compute.",
            "Orders gamma function, which is a simple integral using Laplace's method.",
            "Well you do some Taylor expansion somehow, and if you go to higher orders you would get something like a divergent serious.",
            "And if you sum up a couple of those terms you might get improving answers, but beyond that might be.",
            "Maybe you get some worse answers since it's an asymptotic series, but maybe such an approach might still be of use if we.",
            "Use a couple of terms only."
        ],
        [
            "Right, So what I want to talk about is first EP in a nutshell.",
            "Well, our understanding of EP.",
            "Of course there might be people who would see the slightly different then I would like to talk about the fixed points.",
            "I'm not talking about the algorithm as such.",
            "I'm not talking about how it converges when it converges.",
            "So let's say I have the fixed points and then I'm interested in how the fixed points are related to the exact inference.",
            "And then I would like to identify the correction to the partition functions.",
            "And I specialize two models with pairwise couplings that would also apply to Gaussian process classification.",
            "So now in our post that already has shown a slightly different route of doing these corrections that would apply to two other models.",
            "Yeah, then we would identifying this correction term.",
            "Of course we can't compute it.",
            "If we could, then we would have, then we would be able to solve the exact inference problem, but we go into a perturbative expansion, so we identify something that might be.",
            "Small and then do a Taylor expansion with respect to that small quantity, and we do some illustration on a couple of simple models like Gaussian classification and also some Ising networks for which nicely prizing networks for smaller ones.",
            "We can do exact inference just by enumeration and see if this actually gets you something and maybe you have some time for now."
        ],
        [
            "OK, so our idea I EP is the following.",
            "So we have variables X.",
            "These are the random variables that we try to make inference on and we have an intractable density, so it's made up of, let's say if not is kind of a prior term and say this is a tractable nice thing and we have a bunch of factors FN and those two together would make this intractable.",
            "So what we would do is we replace it by some other tractable density in a nice class.",
            "That has this prior and it the these likelihood terms are replaced by nice likelihood terms, maybe in an exponential family.",
            "So what's done recursively is so we choose.",
            "We take this nice density.",
            "Q Take out one of the factors G. One of the nice factors and replace it by one of the less noise factors.",
            "FJ and define a tilted.",
            "I think you call it a tilted distribution.",
            "QJ of X and.",
            "Which contains only one of the bad factors, and then you use that update in order to use that QJ in order to update the parameters in your approximating distribution Q by saying well, Q should be very close to that QJ and what we do in order to achieve that we do moment matching between Q&QJ.",
            "So I'm not talking about cool back Libra divergences or anything else.",
            "There will be some.",
            "Moment matching here and that will allow us also to treat models were actually kullback Leibler matching would not be possible, so it's a slightly different I."
        ],
        [
            "So yes, please right.",
            "So what are these are the fixed point equations, so let's assume this has converged to some fixed point and say we assume that these approximating factors live in an exponential families.",
            "So we would have some parameters lamp to we would have some sufficient some statistic firbeck, so these would be the factors GN an.",
            "We assume if note is also somehow in this family and then Q would be.",
            "This prior if not times such such a factor.",
            "So if we compute the tilted distribution, that means we take away one of the GS.",
            "So subtracting a lump to J and we would multiply in one of the.",
            "Bad factors, FJ.",
            "So that will be the QJ and the moment matching conditions would be we would would the flies shoot on average coincide?",
            "If I do the average by the Q distribution or by the QJ distribution.",
            "These are the fixed point can."
        ],
        [
            "Nations cakes please.",
            "Now what we can do?",
            "I mean very much now interested in these Q ends the tilted distribution and the approximating distribution Q and we are interested in the case where we believe maybe Q&Q and they are very close together because we do the moment matching and we hope they might be because of the moment matching in some sense close so.",
            "So we try to express everything that we want to know about that inference in terms of the Q&Q, and So what we can do.",
            "Going back to the basic definitions of these things, we can express these intractable terms in terms of the ratio of the queues, the queue ends, the queues and the corresponding normalizing factors.",
            "That's a very simple exercise.",
            "I write it just down, and now if we're interested in these partition function, well, I mean partition functions.",
            "This these are the.",
            "The normalizers of the probability distribution is one of the quantities that we're really interested in.",
            "Let's express the partition function in terms of these cues and the zeds so if we plug that in again, a very simple exercise shows that this is related to a factor that we would identify as the partition function in the EP approximation times.",
            "The term that we write as a product of 1 plus little epsilons and these little epsilons are just related to the difference between the approximating and the tilted distribution.",
            "So if the approximating one in the tilted are well equal, then this whole thing vanish is an we would get a one here, and at the end of the day.",
            "So there would be some that skew, so it would be a one.",
            "This correction are would be a one.",
            "So we were interested in saying, well, how can we say something?",
            "It's one plus something.",
            "Is it appears also in our poster and already has shown it mean.",
            "One simple thing you could do is, well, you make you make and expect an expansion with respect to these little end."
        ],
        [
            "Can you go to the next one?",
            "And if we look already at the 1st order expansion here, 1st order means we take the terms linear in N. Well, if you look at the log partition function then it's a little exercise to show that actually sort of the 1st order term in this approximation is exactly 0.",
            "So you could say well EP has a nice property.",
            "Well it makes the 1st order correction equal to 0 and I want to go to higher orders that something on the poster.",
            "But now I will go a different route in order to compute those or estimate those corrections."
        ],
        [
            "Right, so just a little summary.",
            "What we have is we have the partition function is the EP partition function time something this R and this are.",
            "This correction can be written in terms of the tilted distribution in the approximating distribution average over the approximation.",
            "You can also express the entire intractable distribution by this ratio of kuzan Q.",
            "Ends and in this talk I will only discuss this correction to the to the P per partition function and I don't want to go into details.",
            "I give also some examples about posterior moments because we want to make predictions we want to compute posterior means and things like that.",
            "You can use the same scheme also for doing these corrections, but I will only discuss the main idea on this."
        ],
        [
            "Object.",
            "How many times?",
            "OK, so I will specialize for the following class of models, so these are models that we call models with pairwise couplings.",
            "So in this case we would have a prior term if not, which is a quadratic form.",
            "And well, we might think that maybe draza negative definite matrix and then that would be like a Gaussian prior.",
            "But we might also think in terms of Ising models.",
            "Where Will Jay is a coupling matrix and X is kind of well, yeah.",
            "Variables there will be likelihood terms in well crucial for this type of calculation that I'm going to present is the fact that these terms will depend only on a single variable XN, so it's one of the variables that we have in in the vector X, so depends only on a single variable.",
            "Well, for this type of model we make likelihood approximation jeez.",
            "Which are sort of Gaussian terms so well, obviously Gaussian, and then the total approximating EP distribution is then a multivariate Gaussian, and if we sort of get the algorithm to converge, we would have a Gaussian with mean mu and covariance Sigma.",
            "OK, so let's say we run the algorithm.",
            "We get this.",
            "We would get values for gamma for Lambda from you in Sigma, and so on, all right now."
        ],
        [
            "Comes the correction.",
            "Please, it turns out that the correction to the partition function, the multiplicative correction to the partition function that involved the tilted distribution Q and the approximated distribution Q, and it turns out if all these factors depend only on single variables, then this whole thing boils down on having the product of the ratio of the marginals of Q and of the~ distribution.",
            "It's a slight simplification.",
            "And we're interested in what is this?",
            "How can we approximate this object here?",
            "So I call?",
            "Well, I write this as a P1 over a pinot.",
            "Dan P1 NP node are factorizing distributions that factorize in these variables XN.",
            "So here's the main idea.",
            "We know well if P1 and P note would be very close, well then the this whole thing.",
            "This correction would be or one.",
            "And so we want to measure closeness by the cumulants of these.",
            "These things and why I'm interested in cumulants, well humans, we know that the approximating distribution Q is a Gaussian in that case, so it has got only the 1st and 2nd cumulants.",
            "That would be non 0.",
            "But by the moment matching condition, that's the basic thing in the EP algorithm, the moment matching condition says, well this other distribution.",
            "Here the Q the Q ends.",
            "They have the same 1st and 2nd cumulants.",
            "So these share to the 1st and 2nd cumulants and there might be higher order terms.",
            "Maybe if I sort of specify these higher order cumulants they could give me a measure for how different is this from one?",
            "How different is this from one and could give me maybe?",
            "A correction that I can actually compute cheaply and see how big the correction is, indicating maybe EP was good.",
            "Or I could even improve on the result of EP by including higher order cumulants in the expansion, so the idea would be express the ratio of these two products of densities by the cumulants and the cumulants are cumulants for single variable densities, and so they are.",
            "Easy to calculate."
        ],
        [
            "Right, So what I mean by cumulants?",
            "Well, I introduce the characteristic function of a density Q.",
            "That's just the Fourier transform of the density, and this is the inversion.",
            "I can express the density by the characteristic function, and the cumulants are defined by an expansion of the logarithm of this characteristic function in essentially the coefficients in that expansion.",
            "In terms of case.",
            "Define the cumulants.",
            "Now what we can do is of course we can express those densities that we had on the previous transparency by the cumulants soapy nought.",
            "Well would be like that.",
            "Work I note is the characteristic function for a Gaussian.",
            "Well, we know for a Gaussian it's a very simple thing.",
            "There is something E to the K transpose mean, and since it's factorizing well in the cumulant expansion, you would have the variance, the local variance in the exponent, all they can do the same thing for the other non Gaussian distribution, and you would find out well of course it shares the same 1st and 2nd.",
            "Cumulants this distribution shares the same 1st and 2nd cumulants by the moment matching condition of AP, and this factor is essentially the rest that contains the higher order cumulants, so we would be interested in dealing with this guy.",
            "But"
        ],
        [
            "These guys.",
            "Now, of course, if everything would be trivial, you could all do it in in in two minutes.",
            "For this calculation you would you would need a couple of more minutes.",
            "It's a bit of magic at this comes out nicely and I didn't expect it before hand, but OK, so just there has always to be something that can't be understood by first looking at it.",
            "If you really sit down and do the calculation and do a bit of shifting.",
            "In in the integration you are able to express the ratio of the of the thing you want to sort of calculate as a Gaussian integral over independent Gaussians of the well.",
            "These cumulants these higher order cumulants and this argument is somehow a little bit in the complex plane.",
            "Well, you just have to do it, just you shifted a bit and then you end up I mean well something you have to do and then see what comes out.",
            "So surprise surprise now look, can you just go back to the."
        ],
        [
            "If one more So what you have to do is you have to plug this in and then average over a multivariate Gaussian.",
            "So that's the goal, so next."
        ],
        [
            "Yes, the next one.",
            "Now I have to average no the next yes no next yes.",
            "So the final thing you have to do is you have to compute this ratio.",
            "You have to average it over a multivariate Gaussian.",
            "So we have all."
        ],
        [
            "The scene, can you go to the previous one so you have one factorizing Gaussian here and here is this guy that has a multivariate Gaussian distribution.",
            "So the idea is make life simple.",
            "Introduce a complex Gaussian.",
            "I mean you can do that, it's just the Gaussian.",
            "It has a real, an imaginary part.",
            "Nothing.",
            "I mean nothing mystical, so here's the real part.",
            "Is the imaginary part and things like that to simplify the whole calcul."
        ],
        [
            "Patient announced the next step, so you say, well, this can be written as East to the higher order Cumulants and you have to average over zed.",
            "Zed is a multivariate complex Gaussian which has the following covariance structure.",
            "How the covariances are given by the covariance of the approximating Gaussian and the nice thing is the variance is 0.",
            "So you have a Gaussian which has zero variance and that makes it of the well.",
            "It's a complex number come on.",
            "It's not nothing really exciting, but the nice it has.",
            "I mean this this comes out is really nice because if you do, you're perturbation expansion.",
            "You see a lot of the bad terms that might things big just cancel and that is for me.",
            "It's an interpretation.",
            "If EP works well for these types of things, maybe that's one of the reasons.",
            "OK, so this is the important thing.",
            "This have to do this fancy average here and for these guys the variance.",
            "Is 0."
        ],
        [
            "OK, next please, so here's a here's Oh yes, so So what I'm trying to do now now comes to comes to question of perturbation expansion.",
            "What do I mean by that?",
            "Well, perturbation expansion, you say?",
            "Well, here's something, hopefully small.",
            "There are these higher order cumulants and well, if it's good, maybe there are small.",
            "So let's try to introduce kind of bookkeeping factor.",
            "Lampton said it equal to 1 at the end and do an expansion with respect to Lambda and just go to.",
            "Lower orders and maybe well in the future.",
            "You can do a bit higher orders.",
            "So what I'm doing is only the first 2 orders.",
            "So I do this expansion with respect to Lambda and so I get a first order Terminus, 2nd order term and surprise surprise, because this is this is a factor, well that contains only single variable averages over a Gaussian which has zero variance, so this is gone.",
            "Other first order is dead.",
            "And if I look well this is also dead.",
            "By the same reason, and if you take multiply out that square, you get also terms GMGM.",
            "Remember these are my order sums of higher order Cumulants.",
            "Well, when M is equal to N, again this is something you have to.",
            "You can calculate using the marginal Gaussian well, a Gaussian with zero variance.",
            "And so again this is dead, so it's only terms in not equal to N that survive here and then.",
            "At the end of the day I said number equal to 1.",
            "It was just a bookkeeping device, so all the single marginal terms nicely vanish.",
            "And remember what we're doing, we sum this over.",
            "We have an interpretation of this is a Gaussian process classifier.",
            "Things so end would run over the number of data points, so things that that scale with N with the number of data points.",
            "They are absent here.",
            "This am not equal to N. Well, if these if the covariances between these Gaussian random variables are small, then maybe this is term which is also not very big.",
            "So the conjecture is hopefully I remember the order of my slides correctly.",
            "Can you know?"
        ],
        [
            "Yes, next one.",
            "No, I don't remember that this is.",
            "This is just a comment, so you have to do Gaussian average just to remind you how to do Gaussian averages.",
            "It's an old story.",
            "I mean, if you have a product of Gaussians and some of the Gaussians might be the same, well you have to do it well.",
            "Gaussians are completely determined by their covariance, so you can write this as a sum of our overall pairings where you have you pair one of the variables with another variable and well, so it's a product of individual covariances and you can write it.",
            "Let's take this example.",
            "You have four Gaussians and while you just have to connect this to that, computers can variance multiply the covariance for that so well?",
            "There's way for ways for doing that.",
            "The only thing I'm saying for this nice example if two of the variables are the same, then you would have a closed loop and these closed loops are absent because we have a zero variance Gaussian complex."
        ],
        [
            "OK, so.",
            "So what we have to do is we have to compute this.",
            "We have to compute the correction and the correction is related to this cumulant expansion, and you go to the algebra you see which term survives.",
            "And what you do?",
            "Well, this is sort of the final result.",
            "And sort of in our approximation, we say well in practice we can truncate that.",
            "Well, we start with three, four and five and see if we get something reasonable.",
            "So you can easily calculate these cumulants for the problems that we were interested in.",
            "It cost you almost nothing, and then you have to plug them into the formula and you get something.",
            "Right, so the bed the complicated task has already been done.",
            "We have run the inference algorithm we have obtained fixed point equations and we do this on top and try to see if we can get a good."
        ],
        [
            "Erection.",
            "Right, so just stop a second.",
            "Here's the conjecture.",
            "When is EP Good, Well EP is good.",
            "If the corrections are small.",
            "When are the corrections possibly small?",
            "Well, if these cumulants are small and that might be actually true for the case of Gaussian process classification when the posterior variance is rather small compared to the mean, and you can show at these higher order cumulants are very small.",
            "Well, the other argument maybe both of them work together in practice.",
            "If the posterior covariances are small, can you go back?"
        ],
        [
            "Two again, so you see, there's a.",
            "These are the posterior covariances between variable N&M near the race to the power of L. So if these are also reasonably small and maybe in high dimensional spaces, you often have, you often have cases where they were actually variables are very weakly coupled, so probably also in the approximation their covariance would come out very small.",
            "Well also you know mean field would be reasonable.",
            "Maybe EP is a bit better.",
            "But in any case, these things if."
        ],
        [
            "These things are small.",
            "Yeah, then we yeah we would get small correction and EP would work well, OK?",
            "Right?"
        ],
        [
            "So yeah, these next one.",
            "So as an example, Gaussian process classification.",
            "Sorry.",
            "How many minutes was it then?",
            "OK yeah.",
            "So first example, Gaussian process classification.",
            "So we would have a prior precisely of that form.",
            "K is the inverse, K is the kernel matrix.",
            "And here we choose as the likelihood a simple noise free classification for simplicity.",
            "So these terms would be a unit step function.",
            "You have always FN is 1 if the label.",
            "Um, if XN and the label if their product is positive.",
            "Right in this case, then these terms that the the.",
            "Well, there should be a J Soria Q in an, so the tilted distributions would actually have such a form.",
            "It looks very much like a Gaussian and approximating a Gaussian by a Gaussian might give a good approximation, but this is not a Gaussian, it's a cut off Gaussian because we have a unit step function, so it's cut off as zero, so this is not the negative.",
            "If you go to the negative numbers, it's completely 0, so an indication if we really have a small variance.",
            "If we have a small variance compared to.",
            "The mean then?",
            "Well, we might be in business, and that's one.",
            "That's our explanation why in such a case, for Gaussian process classification EP."
        ],
        [
            "Well, maybe I skipped this a bit more detail about the cumulants, but I think."
        ],
        [
            "Don't have too much time, so this is an example where we have a simple just to illustrate things.",
            "We have.",
            "The input is 1 dimensional.",
            "We have a couple of data points with positive and negative labels.",
            "These are sort of the inputs.",
            "These are the different labels and what we do is we we take one of the inputs and move them around and compute for different positions of one of these inputs.",
            "The free energy, the.",
            "The correction to the marginal likelihood, the logarithm of it, and you see, of course, if if one data point is close to another point, the correlation increases and the since since the correction was related to the correlation to the covariance.",
            "So if we get we get a strong increase.",
            "But if you see on the next transparency the net."
        ],
        [
            "Effect.",
            "So yeah, the net effect is really very small, so you compare the EP partition function plus it's correction, so you're really see it's a very small effect that you get, and I think this is.",
            "This is in our in our point of view explains some of the experiments where people especially call.",
            "In his work where they have done extensive Monte Carlo studies on Gaussian process classification and have compared it to EP and got sort of excellent Queens."
        ],
        [
            "Yeah, excellent results, so it's very.",
            "Yes.",
            "So this is your parting the correction plus Love Z EP plus it's correction plus it's correction based on small on the 1st cumulants.",
            "As a function of one of the positions of one of the one of the inputs, we wanted to see how you know?",
            "I mean, otherwise we would just get a number, right?",
            "And what we do we try to get a bit structure into that and move one of the data points.",
            "The input points around and to see if one of if this input point is close to one of the other input points, then posterior covariance is probably you know more different from zero.",
            "And you would get sort of more structure, but.",
            "The days that you showed there plus one other inputs, yes yes.",
            "The label or it has a label, yes, yes, let's see.",
            "Yes it has.",
            "Yeah yeah, yeah yeah yeah.",
            "Well to be honest, I'm not fully sure how to interpret this.",
            "I mean the main message is well, there is some correction, but it's typically very small.",
            "It's going down those little spikes as you do, where it's right on the X. Yeah yeah yeah no.",
            "Seems that your correction doesn't scale with the size of the.",
            "I mean it's a.",
            "What do you mean?",
            "I mean, the size is fixed.",
            "Yeah right yeah.",
            "I mean.",
            "I would expect if it was a bad correction, you would expect that it goes off when yeah.",
            "Right yes.",
            "Is the intuition correct for that?",
            "For that data point?",
            "When you've moved it amongst the ex is Jessa Cumulants.",
            "The corrections should be larger for that data point, not sure.",
            "I mean, it's in.",
            "It's an effective no no no.",
            "The cumulants are OK. Of course you re run EP all the time, right?",
            "Yes, the cumulants would change, but also the covariances would change the posterior covariance.",
            "So it's two things that.",
            "I know Rich, can you say?",
            "That blood was to take a number of data points is fixed.",
            "Run optimize the log marginal likelihood perspective behind the breakfast in choosing kernel high privacy and then introduce an extra point that we run over this one dimensional.",
            "And we see that when the circle basically crosses the X is.",
            "They said double interpretation of labels.",
            "See you can expect a lot more than likely to death to get smaller and way the circle runs in the circles.",
            "Most likely it should be bigger, so that's that's the spirit guided.",
            "Interesting from what man?",
            "From the same before that I would expect the correction to be bigger.",
            "When the circle ran over the cross, which I don't really see in the plot, but maybe an illusion of the way though.",
            "Because your picture where you showed before the cumulus mismatch was for a well classified example, right?",
            "Or did I misunderstand?",
            "Will there?",
            "I mean on the previous uses."
        ],
        [
            "You see the correction alone, right?",
            "Yeah.",
            "Yeah."
        ],
        [
            "OK, so we we we we we would have to do a bit more, more detailed things, but this is very recent results so I wanted to show you."
        ],
        [
            "A second to yeah yeah, so you can also calculate.",
            "This is only for four data points.",
            "You can calculate the correction to the posterior mean and so you see here the relative correction is in the order 10 to the minus three.",
            "So I mean I haven't shown you, but it goes also with the cumulants slightly more involved, right?"
        ],
        [
            "Yeah, next please.",
            "So now we try to do also Ising cases.",
            "That's a nice things because we can get exact results to that, so it's the toy thing is you just have two Ising variables coupled together with the coupling J and in order to make this Ising we introduce terms that are actually some of 2D functions and that's what we sort of that wasn't meant to be doable, sort of.",
            "In the original EP type of approach, but in our type of approach where we say we do moment matching, it's it can be done.",
            "Without any problem.",
            "So you can do.",
            "You can compute this this lock partition function for this case and computed for a small JS and you see if you compare EP with the exact results.",
            "So Jason J squared term is OK, the J to the 4th doesn't work and if I.",
            "Sort of include the first nontrivial.",
            "Cumulant, which is the 4th, but by symmetry, the third one is 0.",
            "Then you get actually the correction to the 4th order, right?",
            "I haven't haven't done this.",
            "You know, for the rest of it, but now we go to we go to."
        ],
        [
            "Yes, the next one we go to now two networks randomizing networks.",
            "So again the same construction.",
            "Oh sorry, that should be all the zeiser plus minus one and we have now a network with also a bias in and BJ's were chosen to be plus minus.",
            "They don't have a bias.",
            "They were chosen to be positive or negative, and the variance scales with the parameter beta and we choose N = 10 and so we can get easily exact results for that, and I think also the gammas were were chosen random."
        ],
        [
            "And so the final type of plots.",
            "So this is the absolute deviation of the negative log partition function.",
            "There's a couple of things on this picture, so the Green 10 after remember what is, I think the the beta approximation.",
            "There was a Kikuchi approximation.",
            "There is the solid line is EP, the Triangles is EP with sort of the 3rd order cumulant only, but it turns out in this case you have of course.",
            "Include also the 4th order, which would be, which would be the circles and if you take the 5th into account you would get the crosses and you see this is.",
            "You know this is a log scale, you get really you get this much better by orders of magnitude but you see with increasing betas.",
            "It seems to be.",
            "A problem, but we would expect that there you know for these run for these random kind of things.",
            "If you have a large variance of the JS you you end up a different phase.",
            "They can.",
            "Yes yeah we know ground truth yes yes yes.",
            "So this is really the absolute.",
            "Deviation of the absolute error.",
            "We know the ground truth for that type of thing.",
            "Yes, and."
        ],
        [
            "And then the last you can also do the maximum absolute deviation of the first of the single node marginals.",
            "And here it's again it's.",
            "Belief propagation there was some type of Kikuchi.",
            "I mean this is sort of copied from a previous paper.",
            "These two things and this is this EP again and here this is EP with what was it for?",
            "3rd and 4th, I mean it includes sort of the first nontrivial corrections an you see.",
            "Also, you get you get a decent decrease in error, and this is a case where we know the ground truth.",
            "Yeah, this is more or less what I wanted to show you.",
            "Maybe there's a few."
        ],
        [
            "Ideas on the last transparency?",
            "What one should do?",
            "I mean, there's always ways of rearranging these perturbation expansions.",
            "The question, what do you believe is the small quantity that you might try to wish?",
            "Getting an expansion so you might say well in case where the sigmas are small, so I'd really do a perturbation expansion with respect to powers of sigmas and what we really would want to do is kind of developing a sanity check release because we've seen in the previous transparency there is a case where all these approximations don't really change the result and maybe also EP is not very good in that case, but there should be something.",
            "Should be something.",
            "Say we we have maybe idea, divergent, serious and we which we should see somehow in the behavior of consecutive terms in the perturbation theory.",
            "So if they're increasing or whatever?",
            "I mean this is something that that one should do.",
            "Get really a sanity check?",
            "When is it good and when it is it not good?",
            "Of course what we should also do?",
            "I mean, in some cases we know that EP doesn't work so well.",
            "Maybe the variational approximation or Power EP works better.",
            "And so one should be able to see that by developing a similar expansion for other, like the variational or Power EP, and that's something that we should.",
            "We should do.",
            "The question is this was very much for for the Gaussian approximating family.",
            "I don't know for other types of of exponential families, so I would have to think a bit more how to do that for other exponential families.",
            "Thank you.",
            "So with your motivation for the Gaussian classifier, you'd expect the corrections to be large if you happen to have data points near the decision boundary.",
            "Yes, that's what I would think.",
            "Yeah, so we should try and construct such a case.",
            "Yeah yeah, yeah, in this case we would really see that these cumulants are really.",
            "It's matching the marginal sales.",
            "You said it was different, but it's just the geometry of the data point distribution out of the decision boundary, but this one the example there's only like a couple of points which are actual decision boundaries.",
            "By moving your end state point around you can never get into a situation where you land on the decision boundary itself.",
            "Good question, I don't know.",
            "Yeah, I think it's helpful to think of this as just using a plan.",
            "Vinegar, not multiplying two signals of opposite signs, and visualizing when the product most gas in or not.",
            "Running time.",
            "After all my collaborators, I mean, you do EP.",
            "At the convergence you're tracking this and you saw it.",
            "It's it's simply matrix multiplication, so it's yeah, yes you have all the quantities that you need, you just you just add up a few millions and well, I mean you have to multiply probably.",
            "Yes, they are univariate cumulants and then you have to sort of do a double summation.",
            "Yes.",
            "Is it obvious how those competes?",
            "I mean, yes, I mean that yeah, if you think in terms of powers of C, we think with what we've done, we haven't neglected any.",
            "I mean this the 3rd order terms would come with higher.",
            "With how higher powers of see I think we're quite sure about that.",
            "Yeah it is.",
            "Yeah yeah.",
            "I mean, there might be also a way of doing a variational perturbation theory, that's another type of things that were you say where I don't only optimize on the on the sort of zero order term, but where I take the zero order term plus correction and then have another variational parameter.",
            "That's something that that often also works quite nicely.",
            "I mean, we haven't tried it here, but that would require then another optimization algorithm, and I don't know how costly that would be.",
            "Models that supposed to be simple like this POS linear one.",
            "And you could probably explain then why is it just causing process classification where he does so very well.",
            "Should be simple as well.",
            "And yeah, yeah, that's that's that's we want that's what we wanted to try.",
            "But I mean, yeah.",
            "Do always less than you expect.",
            "One last one thing on the things you had, yeah.",
            "The highest of the community.",
            "At one point it was losing out below accumulate.",
            "That's sort of what you were saying at the beginning.",
            "It's no guarantee as you increase the number of cumulants that will improve things, it can actually get worse.",
            "Yes.",
            "This yeah OK I'm.",
            "Well, I we have to investigate that that this is no problem.",
            "Yeah oh I don't know.",
            "Improve.",
            "Low temperatures, which is really nice, yeah.",
            "Yeah.",
            "PP is better.",
            "I don't know.",
            "I don't know.",
            "This is just very recent result, probably produced yesterday.",
            "That would be a very nice property, but I can't prove anything.",
            "Thank you yeah.",
            "Always.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'd like to thank the organizers for inviting me and.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Lori Canola and it's ongoing work on trying to compute corrections to expectation propagation, and.",
                    "label": 1
                },
                {
                    "sent": "Right, OK, so the idea is of course, if we if we run an approximate inference algorithm, we don't know how close the answer is to the answer to the exact answer.",
                    "label": 0
                },
                {
                    "sent": "I mean the exact answer means if we would be able to do exact inference and the question is can we sort of quantify what we have lost by using the approximation.",
                    "label": 0
                },
                {
                    "sent": "Of course I can't give you the complete answer.",
                    "label": 0
                },
                {
                    "sent": "That would mean I would be able to solve the complete inference problems.",
                    "label": 0
                },
                {
                    "sent": "But we can do it in a certain what we call a perturbative correction.",
                    "label": 0
                },
                {
                    "sent": "So it's in the spirit of.",
                    "label": 0
                },
                {
                    "sent": "Let's say you wanted to compute.",
                    "label": 0
                },
                {
                    "sent": "Orders gamma function, which is a simple integral using Laplace's method.",
                    "label": 0
                },
                {
                    "sent": "Well you do some Taylor expansion somehow, and if you go to higher orders you would get something like a divergent serious.",
                    "label": 0
                },
                {
                    "sent": "And if you sum up a couple of those terms you might get improving answers, but beyond that might be.",
                    "label": 0
                },
                {
                    "sent": "Maybe you get some worse answers since it's an asymptotic series, but maybe such an approach might still be of use if we.",
                    "label": 0
                },
                {
                    "sent": "Use a couple of terms only.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, So what I want to talk about is first EP in a nutshell.",
                    "label": 1
                },
                {
                    "sent": "Well, our understanding of EP.",
                    "label": 0
                },
                {
                    "sent": "Of course there might be people who would see the slightly different then I would like to talk about the fixed points.",
                    "label": 0
                },
                {
                    "sent": "I'm not talking about the algorithm as such.",
                    "label": 0
                },
                {
                    "sent": "I'm not talking about how it converges when it converges.",
                    "label": 1
                },
                {
                    "sent": "So let's say I have the fixed points and then I'm interested in how the fixed points are related to the exact inference.",
                    "label": 0
                },
                {
                    "sent": "And then I would like to identify the correction to the partition functions.",
                    "label": 1
                },
                {
                    "sent": "And I specialize two models with pairwise couplings that would also apply to Gaussian process classification.",
                    "label": 0
                },
                {
                    "sent": "So now in our post that already has shown a slightly different route of doing these corrections that would apply to two other models.",
                    "label": 0
                },
                {
                    "sent": "Yeah, then we would identifying this correction term.",
                    "label": 0
                },
                {
                    "sent": "Of course we can't compute it.",
                    "label": 0
                },
                {
                    "sent": "If we could, then we would have, then we would be able to solve the exact inference problem, but we go into a perturbative expansion, so we identify something that might be.",
                    "label": 0
                },
                {
                    "sent": "Small and then do a Taylor expansion with respect to that small quantity, and we do some illustration on a couple of simple models like Gaussian classification and also some Ising networks for which nicely prizing networks for smaller ones.",
                    "label": 0
                },
                {
                    "sent": "We can do exact inference just by enumeration and see if this actually gets you something and maybe you have some time for now.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so our idea I EP is the following.",
                    "label": 0
                },
                {
                    "sent": "So we have variables X.",
                    "label": 0
                },
                {
                    "sent": "These are the random variables that we try to make inference on and we have an intractable density, so it's made up of, let's say if not is kind of a prior term and say this is a tractable nice thing and we have a bunch of factors FN and those two together would make this intractable.",
                    "label": 0
                },
                {
                    "sent": "So what we would do is we replace it by some other tractable density in a nice class.",
                    "label": 1
                },
                {
                    "sent": "That has this prior and it the these likelihood terms are replaced by nice likelihood terms, maybe in an exponential family.",
                    "label": 0
                },
                {
                    "sent": "So what's done recursively is so we choose.",
                    "label": 0
                },
                {
                    "sent": "We take this nice density.",
                    "label": 0
                },
                {
                    "sent": "Q Take out one of the factors G. One of the nice factors and replace it by one of the less noise factors.",
                    "label": 0
                },
                {
                    "sent": "FJ and define a tilted.",
                    "label": 1
                },
                {
                    "sent": "I think you call it a tilted distribution.",
                    "label": 0
                },
                {
                    "sent": "QJ of X and.",
                    "label": 0
                },
                {
                    "sent": "Which contains only one of the bad factors, and then you use that update in order to use that QJ in order to update the parameters in your approximating distribution Q by saying well, Q should be very close to that QJ and what we do in order to achieve that we do moment matching between Q&QJ.",
                    "label": 0
                },
                {
                    "sent": "So I'm not talking about cool back Libra divergences or anything else.",
                    "label": 0
                },
                {
                    "sent": "There will be some.",
                    "label": 0
                },
                {
                    "sent": "Moment matching here and that will allow us also to treat models were actually kullback Leibler matching would not be possible, so it's a slightly different I.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yes, please right.",
                    "label": 0
                },
                {
                    "sent": "So what are these are the fixed point equations, so let's assume this has converged to some fixed point and say we assume that these approximating factors live in an exponential families.",
                    "label": 0
                },
                {
                    "sent": "So we would have some parameters lamp to we would have some sufficient some statistic firbeck, so these would be the factors GN an.",
                    "label": 0
                },
                {
                    "sent": "We assume if note is also somehow in this family and then Q would be.",
                    "label": 0
                },
                {
                    "sent": "This prior if not times such such a factor.",
                    "label": 0
                },
                {
                    "sent": "So if we compute the tilted distribution, that means we take away one of the GS.",
                    "label": 0
                },
                {
                    "sent": "So subtracting a lump to J and we would multiply in one of the.",
                    "label": 0
                },
                {
                    "sent": "Bad factors, FJ.",
                    "label": 0
                },
                {
                    "sent": "So that will be the QJ and the moment matching conditions would be we would would the flies shoot on average coincide?",
                    "label": 1
                },
                {
                    "sent": "If I do the average by the Q distribution or by the QJ distribution.",
                    "label": 1
                },
                {
                    "sent": "These are the fixed point can.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nations cakes please.",
                    "label": 0
                },
                {
                    "sent": "Now what we can do?",
                    "label": 0
                },
                {
                    "sent": "I mean very much now interested in these Q ends the tilted distribution and the approximating distribution Q and we are interested in the case where we believe maybe Q&Q and they are very close together because we do the moment matching and we hope they might be because of the moment matching in some sense close so.",
                    "label": 0
                },
                {
                    "sent": "So we try to express everything that we want to know about that inference in terms of the Q&Q, and So what we can do.",
                    "label": 0
                },
                {
                    "sent": "Going back to the basic definitions of these things, we can express these intractable terms in terms of the ratio of the queues, the queue ends, the queues and the corresponding normalizing factors.",
                    "label": 0
                },
                {
                    "sent": "That's a very simple exercise.",
                    "label": 0
                },
                {
                    "sent": "I write it just down, and now if we're interested in these partition function, well, I mean partition functions.",
                    "label": 0
                },
                {
                    "sent": "This these are the.",
                    "label": 0
                },
                {
                    "sent": "The normalizers of the probability distribution is one of the quantities that we're really interested in.",
                    "label": 0
                },
                {
                    "sent": "Let's express the partition function in terms of these cues and the zeds so if we plug that in again, a very simple exercise shows that this is related to a factor that we would identify as the partition function in the EP approximation times.",
                    "label": 1
                },
                {
                    "sent": "The term that we write as a product of 1 plus little epsilons and these little epsilons are just related to the difference between the approximating and the tilted distribution.",
                    "label": 0
                },
                {
                    "sent": "So if the approximating one in the tilted are well equal, then this whole thing vanish is an we would get a one here, and at the end of the day.",
                    "label": 0
                },
                {
                    "sent": "So there would be some that skew, so it would be a one.",
                    "label": 0
                },
                {
                    "sent": "This correction are would be a one.",
                    "label": 0
                },
                {
                    "sent": "So we were interested in saying, well, how can we say something?",
                    "label": 0
                },
                {
                    "sent": "It's one plus something.",
                    "label": 0
                },
                {
                    "sent": "Is it appears also in our poster and already has shown it mean.",
                    "label": 0
                },
                {
                    "sent": "One simple thing you could do is, well, you make you make and expect an expansion with respect to these little end.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can you go to the next one?",
                    "label": 0
                },
                {
                    "sent": "And if we look already at the 1st order expansion here, 1st order means we take the terms linear in N. Well, if you look at the log partition function then it's a little exercise to show that actually sort of the 1st order term in this approximation is exactly 0.",
                    "label": 1
                },
                {
                    "sent": "So you could say well EP has a nice property.",
                    "label": 0
                },
                {
                    "sent": "Well it makes the 1st order correction equal to 0 and I want to go to higher orders that something on the poster.",
                    "label": 0
                },
                {
                    "sent": "But now I will go a different route in order to compute those or estimate those corrections.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so just a little summary.",
                    "label": 0
                },
                {
                    "sent": "What we have is we have the partition function is the EP partition function time something this R and this are.",
                    "label": 0
                },
                {
                    "sent": "This correction can be written in terms of the tilted distribution in the approximating distribution average over the approximation.",
                    "label": 0
                },
                {
                    "sent": "You can also express the entire intractable distribution by this ratio of kuzan Q.",
                    "label": 0
                },
                {
                    "sent": "Ends and in this talk I will only discuss this correction to the to the P per partition function and I don't want to go into details.",
                    "label": 0
                },
                {
                    "sent": "I give also some examples about posterior moments because we want to make predictions we want to compute posterior means and things like that.",
                    "label": 0
                },
                {
                    "sent": "You can use the same scheme also for doing these corrections, but I will only discuss the main idea on this.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Object.",
                    "label": 0
                },
                {
                    "sent": "How many times?",
                    "label": 0
                },
                {
                    "sent": "OK, so I will specialize for the following class of models, so these are models that we call models with pairwise couplings.",
                    "label": 1
                },
                {
                    "sent": "So in this case we would have a prior term if not, which is a quadratic form.",
                    "label": 0
                },
                {
                    "sent": "And well, we might think that maybe draza negative definite matrix and then that would be like a Gaussian prior.",
                    "label": 0
                },
                {
                    "sent": "But we might also think in terms of Ising models.",
                    "label": 0
                },
                {
                    "sent": "Where Will Jay is a coupling matrix and X is kind of well, yeah.",
                    "label": 0
                },
                {
                    "sent": "Variables there will be likelihood terms in well crucial for this type of calculation that I'm going to present is the fact that these terms will depend only on a single variable XN, so it's one of the variables that we have in in the vector X, so depends only on a single variable.",
                    "label": 0
                },
                {
                    "sent": "Well, for this type of model we make likelihood approximation jeez.",
                    "label": 0
                },
                {
                    "sent": "Which are sort of Gaussian terms so well, obviously Gaussian, and then the total approximating EP distribution is then a multivariate Gaussian, and if we sort of get the algorithm to converge, we would have a Gaussian with mean mu and covariance Sigma.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say we run the algorithm.",
                    "label": 0
                },
                {
                    "sent": "We get this.",
                    "label": 0
                },
                {
                    "sent": "We would get values for gamma for Lambda from you in Sigma, and so on, all right now.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Comes the correction.",
                    "label": 0
                },
                {
                    "sent": "Please, it turns out that the correction to the partition function, the multiplicative correction to the partition function that involved the tilted distribution Q and the approximated distribution Q, and it turns out if all these factors depend only on single variables, then this whole thing boils down on having the product of the ratio of the marginals of Q and of the~ distribution.",
                    "label": 1
                },
                {
                    "sent": "It's a slight simplification.",
                    "label": 0
                },
                {
                    "sent": "And we're interested in what is this?",
                    "label": 0
                },
                {
                    "sent": "How can we approximate this object here?",
                    "label": 0
                },
                {
                    "sent": "So I call?",
                    "label": 0
                },
                {
                    "sent": "Well, I write this as a P1 over a pinot.",
                    "label": 0
                },
                {
                    "sent": "Dan P1 NP node are factorizing distributions that factorize in these variables XN.",
                    "label": 0
                },
                {
                    "sent": "So here's the main idea.",
                    "label": 0
                },
                {
                    "sent": "We know well if P1 and P note would be very close, well then the this whole thing.",
                    "label": 0
                },
                {
                    "sent": "This correction would be or one.",
                    "label": 0
                },
                {
                    "sent": "And so we want to measure closeness by the cumulants of these.",
                    "label": 0
                },
                {
                    "sent": "These things and why I'm interested in cumulants, well humans, we know that the approximating distribution Q is a Gaussian in that case, so it has got only the 1st and 2nd cumulants.",
                    "label": 0
                },
                {
                    "sent": "That would be non 0.",
                    "label": 0
                },
                {
                    "sent": "But by the moment matching condition, that's the basic thing in the EP algorithm, the moment matching condition says, well this other distribution.",
                    "label": 0
                },
                {
                    "sent": "Here the Q the Q ends.",
                    "label": 0
                },
                {
                    "sent": "They have the same 1st and 2nd cumulants.",
                    "label": 0
                },
                {
                    "sent": "So these share to the 1st and 2nd cumulants and there might be higher order terms.",
                    "label": 0
                },
                {
                    "sent": "Maybe if I sort of specify these higher order cumulants they could give me a measure for how different is this from one?",
                    "label": 0
                },
                {
                    "sent": "How different is this from one and could give me maybe?",
                    "label": 0
                },
                {
                    "sent": "A correction that I can actually compute cheaply and see how big the correction is, indicating maybe EP was good.",
                    "label": 0
                },
                {
                    "sent": "Or I could even improve on the result of EP by including higher order cumulants in the expansion, so the idea would be express the ratio of these two products of densities by the cumulants and the cumulants are cumulants for single variable densities, and so they are.",
                    "label": 0
                },
                {
                    "sent": "Easy to calculate.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, So what I mean by cumulants?",
                    "label": 1
                },
                {
                    "sent": "Well, I introduce the characteristic function of a density Q.",
                    "label": 0
                },
                {
                    "sent": "That's just the Fourier transform of the density, and this is the inversion.",
                    "label": 0
                },
                {
                    "sent": "I can express the density by the characteristic function, and the cumulants are defined by an expansion of the logarithm of this characteristic function in essentially the coefficients in that expansion.",
                    "label": 1
                },
                {
                    "sent": "In terms of case.",
                    "label": 0
                },
                {
                    "sent": "Define the cumulants.",
                    "label": 0
                },
                {
                    "sent": "Now what we can do is of course we can express those densities that we had on the previous transparency by the cumulants soapy nought.",
                    "label": 0
                },
                {
                    "sent": "Well would be like that.",
                    "label": 0
                },
                {
                    "sent": "Work I note is the characteristic function for a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Well, we know for a Gaussian it's a very simple thing.",
                    "label": 0
                },
                {
                    "sent": "There is something E to the K transpose mean, and since it's factorizing well in the cumulant expansion, you would have the variance, the local variance in the exponent, all they can do the same thing for the other non Gaussian distribution, and you would find out well of course it shares the same 1st and 2nd.",
                    "label": 0
                },
                {
                    "sent": "Cumulants this distribution shares the same 1st and 2nd cumulants by the moment matching condition of AP, and this factor is essentially the rest that contains the higher order cumulants, so we would be interested in dealing with this guy.",
                    "label": 1
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These guys.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, if everything would be trivial, you could all do it in in in two minutes.",
                    "label": 0
                },
                {
                    "sent": "For this calculation you would you would need a couple of more minutes.",
                    "label": 0
                },
                {
                    "sent": "It's a bit of magic at this comes out nicely and I didn't expect it before hand, but OK, so just there has always to be something that can't be understood by first looking at it.",
                    "label": 0
                },
                {
                    "sent": "If you really sit down and do the calculation and do a bit of shifting.",
                    "label": 0
                },
                {
                    "sent": "In in the integration you are able to express the ratio of the of the thing you want to sort of calculate as a Gaussian integral over independent Gaussians of the well.",
                    "label": 1
                },
                {
                    "sent": "These cumulants these higher order cumulants and this argument is somehow a little bit in the complex plane.",
                    "label": 0
                },
                {
                    "sent": "Well, you just have to do it, just you shifted a bit and then you end up I mean well something you have to do and then see what comes out.",
                    "label": 0
                },
                {
                    "sent": "So surprise surprise now look, can you just go back to the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If one more So what you have to do is you have to plug this in and then average over a multivariate Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So that's the goal, so next.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, the next one.",
                    "label": 0
                },
                {
                    "sent": "Now I have to average no the next yes no next yes.",
                    "label": 0
                },
                {
                    "sent": "So the final thing you have to do is you have to compute this ratio.",
                    "label": 0
                },
                {
                    "sent": "You have to average it over a multivariate Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So we have all.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The scene, can you go to the previous one so you have one factorizing Gaussian here and here is this guy that has a multivariate Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So the idea is make life simple.",
                    "label": 0
                },
                {
                    "sent": "Introduce a complex Gaussian.",
                    "label": 0
                },
                {
                    "sent": "I mean you can do that, it's just the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It has a real, an imaginary part.",
                    "label": 0
                },
                {
                    "sent": "Nothing.",
                    "label": 0
                },
                {
                    "sent": "I mean nothing mystical, so here's the real part.",
                    "label": 0
                },
                {
                    "sent": "Is the imaginary part and things like that to simplify the whole calcul.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Patient announced the next step, so you say, well, this can be written as East to the higher order Cumulants and you have to average over zed.",
                    "label": 0
                },
                {
                    "sent": "Zed is a multivariate complex Gaussian which has the following covariance structure.",
                    "label": 1
                },
                {
                    "sent": "How the covariances are given by the covariance of the approximating Gaussian and the nice thing is the variance is 0.",
                    "label": 0
                },
                {
                    "sent": "So you have a Gaussian which has zero variance and that makes it of the well.",
                    "label": 0
                },
                {
                    "sent": "It's a complex number come on.",
                    "label": 0
                },
                {
                    "sent": "It's not nothing really exciting, but the nice it has.",
                    "label": 0
                },
                {
                    "sent": "I mean this this comes out is really nice because if you do, you're perturbation expansion.",
                    "label": 0
                },
                {
                    "sent": "You see a lot of the bad terms that might things big just cancel and that is for me.",
                    "label": 0
                },
                {
                    "sent": "It's an interpretation.",
                    "label": 0
                },
                {
                    "sent": "If EP works well for these types of things, maybe that's one of the reasons.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the important thing.",
                    "label": 0
                },
                {
                    "sent": "This have to do this fancy average here and for these guys the variance.",
                    "label": 0
                },
                {
                    "sent": "Is 0.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, next please, so here's a here's Oh yes, so So what I'm trying to do now now comes to comes to question of perturbation expansion.",
                    "label": 0
                },
                {
                    "sent": "What do I mean by that?",
                    "label": 0
                },
                {
                    "sent": "Well, perturbation expansion, you say?",
                    "label": 1
                },
                {
                    "sent": "Well, here's something, hopefully small.",
                    "label": 0
                },
                {
                    "sent": "There are these higher order cumulants and well, if it's good, maybe there are small.",
                    "label": 0
                },
                {
                    "sent": "So let's try to introduce kind of bookkeeping factor.",
                    "label": 0
                },
                {
                    "sent": "Lampton said it equal to 1 at the end and do an expansion with respect to Lambda and just go to.",
                    "label": 1
                },
                {
                    "sent": "Lower orders and maybe well in the future.",
                    "label": 0
                },
                {
                    "sent": "You can do a bit higher orders.",
                    "label": 0
                },
                {
                    "sent": "So what I'm doing is only the first 2 orders.",
                    "label": 0
                },
                {
                    "sent": "So I do this expansion with respect to Lambda and so I get a first order Terminus, 2nd order term and surprise surprise, because this is this is a factor, well that contains only single variable averages over a Gaussian which has zero variance, so this is gone.",
                    "label": 0
                },
                {
                    "sent": "Other first order is dead.",
                    "label": 0
                },
                {
                    "sent": "And if I look well this is also dead.",
                    "label": 0
                },
                {
                    "sent": "By the same reason, and if you take multiply out that square, you get also terms GMGM.",
                    "label": 0
                },
                {
                    "sent": "Remember these are my order sums of higher order Cumulants.",
                    "label": 0
                },
                {
                    "sent": "Well, when M is equal to N, again this is something you have to.",
                    "label": 0
                },
                {
                    "sent": "You can calculate using the marginal Gaussian well, a Gaussian with zero variance.",
                    "label": 0
                },
                {
                    "sent": "And so again this is dead, so it's only terms in not equal to N that survive here and then.",
                    "label": 0
                },
                {
                    "sent": "At the end of the day I said number equal to 1.",
                    "label": 1
                },
                {
                    "sent": "It was just a bookkeeping device, so all the single marginal terms nicely vanish.",
                    "label": 0
                },
                {
                    "sent": "And remember what we're doing, we sum this over.",
                    "label": 0
                },
                {
                    "sent": "We have an interpretation of this is a Gaussian process classifier.",
                    "label": 0
                },
                {
                    "sent": "Things so end would run over the number of data points, so things that that scale with N with the number of data points.",
                    "label": 0
                },
                {
                    "sent": "They are absent here.",
                    "label": 0
                },
                {
                    "sent": "This am not equal to N. Well, if these if the covariances between these Gaussian random variables are small, then maybe this is term which is also not very big.",
                    "label": 0
                },
                {
                    "sent": "So the conjecture is hopefully I remember the order of my slides correctly.",
                    "label": 0
                },
                {
                    "sent": "Can you know?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, next one.",
                    "label": 0
                },
                {
                    "sent": "No, I don't remember that this is.",
                    "label": 0
                },
                {
                    "sent": "This is just a comment, so you have to do Gaussian average just to remind you how to do Gaussian averages.",
                    "label": 1
                },
                {
                    "sent": "It's an old story.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you have a product of Gaussians and some of the Gaussians might be the same, well you have to do it well.",
                    "label": 0
                },
                {
                    "sent": "Gaussians are completely determined by their covariance, so you can write this as a sum of our overall pairings where you have you pair one of the variables with another variable and well, so it's a product of individual covariances and you can write it.",
                    "label": 0
                },
                {
                    "sent": "Let's take this example.",
                    "label": 0
                },
                {
                    "sent": "You have four Gaussians and while you just have to connect this to that, computers can variance multiply the covariance for that so well?",
                    "label": 0
                },
                {
                    "sent": "There's way for ways for doing that.",
                    "label": 0
                },
                {
                    "sent": "The only thing I'm saying for this nice example if two of the variables are the same, then you would have a closed loop and these closed loops are absent because we have a zero variance Gaussian complex.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So what we have to do is we have to compute this.",
                    "label": 0
                },
                {
                    "sent": "We have to compute the correction and the correction is related to this cumulant expansion, and you go to the algebra you see which term survives.",
                    "label": 0
                },
                {
                    "sent": "And what you do?",
                    "label": 0
                },
                {
                    "sent": "Well, this is sort of the final result.",
                    "label": 0
                },
                {
                    "sent": "And sort of in our approximation, we say well in practice we can truncate that.",
                    "label": 1
                },
                {
                    "sent": "Well, we start with three, four and five and see if we get something reasonable.",
                    "label": 0
                },
                {
                    "sent": "So you can easily calculate these cumulants for the problems that we were interested in.",
                    "label": 0
                },
                {
                    "sent": "It cost you almost nothing, and then you have to plug them into the formula and you get something.",
                    "label": 0
                },
                {
                    "sent": "Right, so the bed the complicated task has already been done.",
                    "label": 0
                },
                {
                    "sent": "We have run the inference algorithm we have obtained fixed point equations and we do this on top and try to see if we can get a good.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Erection.",
                    "label": 0
                },
                {
                    "sent": "Right, so just stop a second.",
                    "label": 0
                },
                {
                    "sent": "Here's the conjecture.",
                    "label": 0
                },
                {
                    "sent": "When is EP Good, Well EP is good.",
                    "label": 0
                },
                {
                    "sent": "If the corrections are small.",
                    "label": 0
                },
                {
                    "sent": "When are the corrections possibly small?",
                    "label": 0
                },
                {
                    "sent": "Well, if these cumulants are small and that might be actually true for the case of Gaussian process classification when the posterior variance is rather small compared to the mean, and you can show at these higher order cumulants are very small.",
                    "label": 1
                },
                {
                    "sent": "Well, the other argument maybe both of them work together in practice.",
                    "label": 1
                },
                {
                    "sent": "If the posterior covariances are small, can you go back?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two again, so you see, there's a.",
                    "label": 0
                },
                {
                    "sent": "These are the posterior covariances between variable N&M near the race to the power of L. So if these are also reasonably small and maybe in high dimensional spaces, you often have, you often have cases where they were actually variables are very weakly coupled, so probably also in the approximation their covariance would come out very small.",
                    "label": 0
                },
                {
                    "sent": "Well also you know mean field would be reasonable.",
                    "label": 0
                },
                {
                    "sent": "Maybe EP is a bit better.",
                    "label": 0
                },
                {
                    "sent": "But in any case, these things if.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These things are small.",
                    "label": 0
                },
                {
                    "sent": "Yeah, then we yeah we would get small correction and EP would work well, OK?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, these next one.",
                    "label": 0
                },
                {
                    "sent": "So as an example, Gaussian process classification.",
                    "label": 1
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "How many minutes was it then?",
                    "label": 0
                },
                {
                    "sent": "OK yeah.",
                    "label": 0
                },
                {
                    "sent": "So first example, Gaussian process classification.",
                    "label": 1
                },
                {
                    "sent": "So we would have a prior precisely of that form.",
                    "label": 0
                },
                {
                    "sent": "K is the inverse, K is the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "And here we choose as the likelihood a simple noise free classification for simplicity.",
                    "label": 1
                },
                {
                    "sent": "So these terms would be a unit step function.",
                    "label": 0
                },
                {
                    "sent": "You have always FN is 1 if the label.",
                    "label": 0
                },
                {
                    "sent": "Um, if XN and the label if their product is positive.",
                    "label": 0
                },
                {
                    "sent": "Right in this case, then these terms that the the.",
                    "label": 0
                },
                {
                    "sent": "Well, there should be a J Soria Q in an, so the tilted distributions would actually have such a form.",
                    "label": 0
                },
                {
                    "sent": "It looks very much like a Gaussian and approximating a Gaussian by a Gaussian might give a good approximation, but this is not a Gaussian, it's a cut off Gaussian because we have a unit step function, so it's cut off as zero, so this is not the negative.",
                    "label": 0
                },
                {
                    "sent": "If you go to the negative numbers, it's completely 0, so an indication if we really have a small variance.",
                    "label": 0
                },
                {
                    "sent": "If we have a small variance compared to.",
                    "label": 0
                },
                {
                    "sent": "The mean then?",
                    "label": 0
                },
                {
                    "sent": "Well, we might be in business, and that's one.",
                    "label": 0
                },
                {
                    "sent": "That's our explanation why in such a case, for Gaussian process classification EP.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, maybe I skipped this a bit more detail about the cumulants, but I think.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Don't have too much time, so this is an example where we have a simple just to illustrate things.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "The input is 1 dimensional.",
                    "label": 0
                },
                {
                    "sent": "We have a couple of data points with positive and negative labels.",
                    "label": 0
                },
                {
                    "sent": "These are sort of the inputs.",
                    "label": 0
                },
                {
                    "sent": "These are the different labels and what we do is we we take one of the inputs and move them around and compute for different positions of one of these inputs.",
                    "label": 0
                },
                {
                    "sent": "The free energy, the.",
                    "label": 0
                },
                {
                    "sent": "The correction to the marginal likelihood, the logarithm of it, and you see, of course, if if one data point is close to another point, the correlation increases and the since since the correction was related to the correlation to the covariance.",
                    "label": 0
                },
                {
                    "sent": "So if we get we get a strong increase.",
                    "label": 0
                },
                {
                    "sent": "But if you see on the next transparency the net.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Effect.",
                    "label": 0
                },
                {
                    "sent": "So yeah, the net effect is really very small, so you compare the EP partition function plus it's correction, so you're really see it's a very small effect that you get, and I think this is.",
                    "label": 0
                },
                {
                    "sent": "This is in our in our point of view explains some of the experiments where people especially call.",
                    "label": 0
                },
                {
                    "sent": "In his work where they have done extensive Monte Carlo studies on Gaussian process classification and have compared it to EP and got sort of excellent Queens.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, excellent results, so it's very.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So this is your parting the correction plus Love Z EP plus it's correction plus it's correction based on small on the 1st cumulants.",
                    "label": 0
                },
                {
                    "sent": "As a function of one of the positions of one of the one of the inputs, we wanted to see how you know?",
                    "label": 0
                },
                {
                    "sent": "I mean, otherwise we would just get a number, right?",
                    "label": 0
                },
                {
                    "sent": "And what we do we try to get a bit structure into that and move one of the data points.",
                    "label": 0
                },
                {
                    "sent": "The input points around and to see if one of if this input point is close to one of the other input points, then posterior covariance is probably you know more different from zero.",
                    "label": 0
                },
                {
                    "sent": "And you would get sort of more structure, but.",
                    "label": 0
                },
                {
                    "sent": "The days that you showed there plus one other inputs, yes yes.",
                    "label": 0
                },
                {
                    "sent": "The label or it has a label, yes, yes, let's see.",
                    "label": 0
                },
                {
                    "sent": "Yes it has.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, yeah yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Well to be honest, I'm not fully sure how to interpret this.",
                    "label": 0
                },
                {
                    "sent": "I mean the main message is well, there is some correction, but it's typically very small.",
                    "label": 0
                },
                {
                    "sent": "It's going down those little spikes as you do, where it's right on the X. Yeah yeah yeah no.",
                    "label": 0
                },
                {
                    "sent": "Seems that your correction doesn't scale with the size of the.",
                    "label": 0
                },
                {
                    "sent": "I mean it's a.",
                    "label": 0
                },
                {
                    "sent": "What do you mean?",
                    "label": 0
                },
                {
                    "sent": "I mean, the size is fixed.",
                    "label": 0
                },
                {
                    "sent": "Yeah right yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "I would expect if it was a bad correction, you would expect that it goes off when yeah.",
                    "label": 0
                },
                {
                    "sent": "Right yes.",
                    "label": 0
                },
                {
                    "sent": "Is the intuition correct for that?",
                    "label": 0
                },
                {
                    "sent": "For that data point?",
                    "label": 0
                },
                {
                    "sent": "When you've moved it amongst the ex is Jessa Cumulants.",
                    "label": 0
                },
                {
                    "sent": "The corrections should be larger for that data point, not sure.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's in.",
                    "label": 0
                },
                {
                    "sent": "It's an effective no no no.",
                    "label": 0
                },
                {
                    "sent": "The cumulants are OK. Of course you re run EP all the time, right?",
                    "label": 0
                },
                {
                    "sent": "Yes, the cumulants would change, but also the covariances would change the posterior covariance.",
                    "label": 0
                },
                {
                    "sent": "So it's two things that.",
                    "label": 0
                },
                {
                    "sent": "I know Rich, can you say?",
                    "label": 0
                },
                {
                    "sent": "That blood was to take a number of data points is fixed.",
                    "label": 0
                },
                {
                    "sent": "Run optimize the log marginal likelihood perspective behind the breakfast in choosing kernel high privacy and then introduce an extra point that we run over this one dimensional.",
                    "label": 0
                },
                {
                    "sent": "And we see that when the circle basically crosses the X is.",
                    "label": 0
                },
                {
                    "sent": "They said double interpretation of labels.",
                    "label": 0
                },
                {
                    "sent": "See you can expect a lot more than likely to death to get smaller and way the circle runs in the circles.",
                    "label": 0
                },
                {
                    "sent": "Most likely it should be bigger, so that's that's the spirit guided.",
                    "label": 0
                },
                {
                    "sent": "Interesting from what man?",
                    "label": 0
                },
                {
                    "sent": "From the same before that I would expect the correction to be bigger.",
                    "label": 0
                },
                {
                    "sent": "When the circle ran over the cross, which I don't really see in the plot, but maybe an illusion of the way though.",
                    "label": 0
                },
                {
                    "sent": "Because your picture where you showed before the cumulus mismatch was for a well classified example, right?",
                    "label": 0
                },
                {
                    "sent": "Or did I misunderstand?",
                    "label": 0
                },
                {
                    "sent": "Will there?",
                    "label": 0
                },
                {
                    "sent": "I mean on the previous uses.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You see the correction alone, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we we we we we would have to do a bit more, more detailed things, but this is very recent results so I wanted to show you.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A second to yeah yeah, so you can also calculate.",
                    "label": 0
                },
                {
                    "sent": "This is only for four data points.",
                    "label": 0
                },
                {
                    "sent": "You can calculate the correction to the posterior mean and so you see here the relative correction is in the order 10 to the minus three.",
                    "label": 0
                },
                {
                    "sent": "So I mean I haven't shown you, but it goes also with the cumulants slightly more involved, right?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, next please.",
                    "label": 0
                },
                {
                    "sent": "So now we try to do also Ising cases.",
                    "label": 0
                },
                {
                    "sent": "That's a nice things because we can get exact results to that, so it's the toy thing is you just have two Ising variables coupled together with the coupling J and in order to make this Ising we introduce terms that are actually some of 2D functions and that's what we sort of that wasn't meant to be doable, sort of.",
                    "label": 0
                },
                {
                    "sent": "In the original EP type of approach, but in our type of approach where we say we do moment matching, it's it can be done.",
                    "label": 0
                },
                {
                    "sent": "Without any problem.",
                    "label": 0
                },
                {
                    "sent": "So you can do.",
                    "label": 0
                },
                {
                    "sent": "You can compute this this lock partition function for this case and computed for a small JS and you see if you compare EP with the exact results.",
                    "label": 0
                },
                {
                    "sent": "So Jason J squared term is OK, the J to the 4th doesn't work and if I.",
                    "label": 0
                },
                {
                    "sent": "Sort of include the first nontrivial.",
                    "label": 0
                },
                {
                    "sent": "Cumulant, which is the 4th, but by symmetry, the third one is 0.",
                    "label": 0
                },
                {
                    "sent": "Then you get actually the correction to the 4th order, right?",
                    "label": 0
                },
                {
                    "sent": "I haven't haven't done this.",
                    "label": 0
                },
                {
                    "sent": "You know, for the rest of it, but now we go to we go to.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, the next one we go to now two networks randomizing networks.",
                    "label": 0
                },
                {
                    "sent": "So again the same construction.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, that should be all the zeiser plus minus one and we have now a network with also a bias in and BJ's were chosen to be plus minus.",
                    "label": 0
                },
                {
                    "sent": "They don't have a bias.",
                    "label": 0
                },
                {
                    "sent": "They were chosen to be positive or negative, and the variance scales with the parameter beta and we choose N = 10 and so we can get easily exact results for that, and I think also the gammas were were chosen random.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the final type of plots.",
                    "label": 0
                },
                {
                    "sent": "So this is the absolute deviation of the negative log partition function.",
                    "label": 0
                },
                {
                    "sent": "There's a couple of things on this picture, so the Green 10 after remember what is, I think the the beta approximation.",
                    "label": 0
                },
                {
                    "sent": "There was a Kikuchi approximation.",
                    "label": 0
                },
                {
                    "sent": "There is the solid line is EP, the Triangles is EP with sort of the 3rd order cumulant only, but it turns out in this case you have of course.",
                    "label": 0
                },
                {
                    "sent": "Include also the 4th order, which would be, which would be the circles and if you take the 5th into account you would get the crosses and you see this is.",
                    "label": 0
                },
                {
                    "sent": "You know this is a log scale, you get really you get this much better by orders of magnitude but you see with increasing betas.",
                    "label": 0
                },
                {
                    "sent": "It seems to be.",
                    "label": 0
                },
                {
                    "sent": "A problem, but we would expect that there you know for these run for these random kind of things.",
                    "label": 0
                },
                {
                    "sent": "If you have a large variance of the JS you you end up a different phase.",
                    "label": 0
                },
                {
                    "sent": "They can.",
                    "label": 0
                },
                {
                    "sent": "Yes yeah we know ground truth yes yes yes.",
                    "label": 0
                },
                {
                    "sent": "So this is really the absolute.",
                    "label": 0
                },
                {
                    "sent": "Deviation of the absolute error.",
                    "label": 0
                },
                {
                    "sent": "We know the ground truth for that type of thing.",
                    "label": 0
                },
                {
                    "sent": "Yes, and.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the last you can also do the maximum absolute deviation of the first of the single node marginals.",
                    "label": 0
                },
                {
                    "sent": "And here it's again it's.",
                    "label": 0
                },
                {
                    "sent": "Belief propagation there was some type of Kikuchi.",
                    "label": 0
                },
                {
                    "sent": "I mean this is sort of copied from a previous paper.",
                    "label": 0
                },
                {
                    "sent": "These two things and this is this EP again and here this is EP with what was it for?",
                    "label": 0
                },
                {
                    "sent": "3rd and 4th, I mean it includes sort of the first nontrivial corrections an you see.",
                    "label": 0
                },
                {
                    "sent": "Also, you get you get a decent decrease in error, and this is a case where we know the ground truth.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is more or less what I wanted to show you.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's a few.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ideas on the last transparency?",
                    "label": 0
                },
                {
                    "sent": "What one should do?",
                    "label": 0
                },
                {
                    "sent": "I mean, there's always ways of rearranging these perturbation expansions.",
                    "label": 0
                },
                {
                    "sent": "The question, what do you believe is the small quantity that you might try to wish?",
                    "label": 0
                },
                {
                    "sent": "Getting an expansion so you might say well in case where the sigmas are small, so I'd really do a perturbation expansion with respect to powers of sigmas and what we really would want to do is kind of developing a sanity check release because we've seen in the previous transparency there is a case where all these approximations don't really change the result and maybe also EP is not very good in that case, but there should be something.",
                    "label": 0
                },
                {
                    "sent": "Should be something.",
                    "label": 0
                },
                {
                    "sent": "Say we we have maybe idea, divergent, serious and we which we should see somehow in the behavior of consecutive terms in the perturbation theory.",
                    "label": 0
                },
                {
                    "sent": "So if they're increasing or whatever?",
                    "label": 0
                },
                {
                    "sent": "I mean this is something that that one should do.",
                    "label": 0
                },
                {
                    "sent": "Get really a sanity check?",
                    "label": 1
                },
                {
                    "sent": "When is it good and when it is it not good?",
                    "label": 0
                },
                {
                    "sent": "Of course what we should also do?",
                    "label": 0
                },
                {
                    "sent": "I mean, in some cases we know that EP doesn't work so well.",
                    "label": 1
                },
                {
                    "sent": "Maybe the variational approximation or Power EP works better.",
                    "label": 0
                },
                {
                    "sent": "And so one should be able to see that by developing a similar expansion for other, like the variational or Power EP, and that's something that we should.",
                    "label": 0
                },
                {
                    "sent": "We should do.",
                    "label": 0
                },
                {
                    "sent": "The question is this was very much for for the Gaussian approximating family.",
                    "label": 0
                },
                {
                    "sent": "I don't know for other types of of exponential families, so I would have to think a bit more how to do that for other exponential families.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So with your motivation for the Gaussian classifier, you'd expect the corrections to be large if you happen to have data points near the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's what I would think.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we should try and construct such a case.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, yeah, in this case we would really see that these cumulants are really.",
                    "label": 0
                },
                {
                    "sent": "It's matching the marginal sales.",
                    "label": 0
                },
                {
                    "sent": "You said it was different, but it's just the geometry of the data point distribution out of the decision boundary, but this one the example there's only like a couple of points which are actual decision boundaries.",
                    "label": 0
                },
                {
                    "sent": "By moving your end state point around you can never get into a situation where you land on the decision boundary itself.",
                    "label": 0
                },
                {
                    "sent": "Good question, I don't know.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think it's helpful to think of this as just using a plan.",
                    "label": 0
                },
                {
                    "sent": "Vinegar, not multiplying two signals of opposite signs, and visualizing when the product most gas in or not.",
                    "label": 0
                },
                {
                    "sent": "Running time.",
                    "label": 0
                },
                {
                    "sent": "After all my collaborators, I mean, you do EP.",
                    "label": 0
                },
                {
                    "sent": "At the convergence you're tracking this and you saw it.",
                    "label": 0
                },
                {
                    "sent": "It's it's simply matrix multiplication, so it's yeah, yes you have all the quantities that you need, you just you just add up a few millions and well, I mean you have to multiply probably.",
                    "label": 0
                },
                {
                    "sent": "Yes, they are univariate cumulants and then you have to sort of do a double summation.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Is it obvious how those competes?",
                    "label": 0
                },
                {
                    "sent": "I mean, yes, I mean that yeah, if you think in terms of powers of C, we think with what we've done, we haven't neglected any.",
                    "label": 0
                },
                {
                    "sent": "I mean this the 3rd order terms would come with higher.",
                    "label": 0
                },
                {
                    "sent": "With how higher powers of see I think we're quite sure about that.",
                    "label": 0
                },
                {
                    "sent": "Yeah it is.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean, there might be also a way of doing a variational perturbation theory, that's another type of things that were you say where I don't only optimize on the on the sort of zero order term, but where I take the zero order term plus correction and then have another variational parameter.",
                    "label": 0
                },
                {
                    "sent": "That's something that that often also works quite nicely.",
                    "label": 0
                },
                {
                    "sent": "I mean, we haven't tried it here, but that would require then another optimization algorithm, and I don't know how costly that would be.",
                    "label": 0
                },
                {
                    "sent": "Models that supposed to be simple like this POS linear one.",
                    "label": 0
                },
                {
                    "sent": "And you could probably explain then why is it just causing process classification where he does so very well.",
                    "label": 0
                },
                {
                    "sent": "Should be simple as well.",
                    "label": 0
                },
                {
                    "sent": "And yeah, yeah, that's that's that's we want that's what we wanted to try.",
                    "label": 0
                },
                {
                    "sent": "But I mean, yeah.",
                    "label": 0
                },
                {
                    "sent": "Do always less than you expect.",
                    "label": 0
                },
                {
                    "sent": "One last one thing on the things you had, yeah.",
                    "label": 0
                },
                {
                    "sent": "The highest of the community.",
                    "label": 0
                },
                {
                    "sent": "At one point it was losing out below accumulate.",
                    "label": 0
                },
                {
                    "sent": "That's sort of what you were saying at the beginning.",
                    "label": 0
                },
                {
                    "sent": "It's no guarantee as you increase the number of cumulants that will improve things, it can actually get worse.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "This yeah OK I'm.",
                    "label": 0
                },
                {
                    "sent": "Well, I we have to investigate that that this is no problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah oh I don't know.",
                    "label": 0
                },
                {
                    "sent": "Improve.",
                    "label": 0
                },
                {
                    "sent": "Low temperatures, which is really nice, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "PP is better.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "This is just very recent result, probably produced yesterday.",
                    "label": 0
                },
                {
                    "sent": "That would be a very nice property, but I can't prove anything.",
                    "label": 0
                },
                {
                    "sent": "Thank you yeah.",
                    "label": 0
                },
                {
                    "sent": "Always.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}