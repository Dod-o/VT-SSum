{
    "id": "ceem7j2wagwsx3sdcuo23perr4bnan3d",
    "title": "Large Scale Multilingual and Multimodal Integration",
    "info": {
        "author": [
            "Jan Rupnik, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "Feb. 15, 2008",
        "recorded": "January 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/psm08_rupnik_lsm/",
    "segmentation": [
        [
            "Huh?"
        ],
        [
            "And so one of the main goals was to increase the scalability of CCA in respect to how many documents or text or data points it can handle, and to how many views it can handle because the baseline was to view CCA.",
            "So there have been different approaches to tackle this problem.",
            "So some some some work has been done on extending the classical definitions and Institute George of Steven have been more working mainly on that and John Short Taylor from UCL and equal or consider from Xerox on.",
            "We've been working on some of correlation, definition of problem and Max Min formulation.",
            "And there there have also been some other approaches, for instance probabilistic approaches by Ray Bontine.",
            "He's doing some probabilistic PCA extensions to multiview learning.",
            "And there's also been work on sparsity done by the Korea from UCL, and he was mainly working on a matching matching pursuit type of algorithm.",
            "Greedy, greedy algorithm that was trying to find the sparse sparse representation of the mappings that CCA finds.",
            "So the outcome of the project, though, is that the collaboration still continues within the smart project and results or results are just being put into deliverables.",
            "And also we're preparing some further publications.",
            "And so now I will mainly focus on the thing that I was working on and that is implementing the sum of correlation formulation for text mining."
        ],
        [
            "So this is just a just a brief introduction to bag of words or we have documents that are points in the and we represent them by points in an N dimensional vector space.",
            "And this is just how we construct the vectors.",
            "Each feature is a word and we just count the number of occurrences and these are vectors, so.",
            "From this moment on, we're just working on.",
            "On the lesson linear algebra problems."
        ],
        [
            "So, and the similarity between documents is just the inner product between them.",
            "So."
        ],
        [
            "OK, the Canonical correlation analysis has been alive for quite some time and its main its main goal is to find 2 vectors in each space.",
            "So that when we project all the data to those two vectors, those two directions we get, we get vectors that are maximally correlated.",
            "And that can be that can be formulated in this equation.",
            "With the covariance matrix and solve the eigenvalue problem so."
        ],
        [
            "This is the CCA.",
            "And."
        ],
        [
            "This is just the graphical view, visualization and interpretation, so we have this documents in both spaces.",
            "And we see that this is the.",
            "This is the direction we have two directions on.",
            "If we so that if we project the documents onto them, we get kind of linear relation between them.",
            "Let's see, yeah."
        ],
        [
            "This this is this is supposed to be 2 points with the same meaning.",
            "You wanna next space in one in white space?",
            "And so on.",
            "So this is."
        ],
        [
            "The first and the longer arrow is the direction of maximal correlation, and we also have a."
        ],
        [
            "2nd second dimension, but the documents are less correlated in that direction."
        ],
        [
            "So one of the possible formulations of the problem of scale when you want to go to more than two views is to try to find the projections so that the sum of pairwise correlations is maximized.",
            "So this is just a generalization of the two view CCA.",
            "Instead of just correlation, we have some of our pairwise correlations.",
            "Um?",
            "So this is the primal optimization with covariance matrices."
        ],
        [
            "But we.",
            "It's it's usually very useful to go to the dual formulation because we can use the kernel trick and also the number of features is usually higher than number of documents and this is then more efficient.",
            "To work in the dual and we also need to regularize becausw, otherwise this algorithm or overfits quickly.",
            "When regularization is we just instead of having normal normal kernels, we have this regular rice kernel.",
            "Here."
        ],
        [
            "So."
        ],
        [
            "Once we have that this formulation, we can just use the standard Lagrangian techniques."
        ],
        [
            "And we end up with a problem that is called the multiparameter eigenvalue problem.",
            "It's it just seem it's very similar to standard eigenvalue problem, except that there's not one eigenvalue Lambda here, but we have a whole vector of eigenvalue, so it's a.",
            "It's a bit different.",
            "And.",
            "There there has been an algorithm around for quite some time, but was in was proved to converge.",
            "If this giant block matrix is positive and positive definite and some."
        ],
        [
            "And this is the simple algorithm.",
            "And the I I you can you can reformulate, or by using some substitutions.",
            "The regional optimization into this scenario and just use this algorithm.",
            "So here the block matrices were this LLIKIKJLJ on, so I could prove that this was a symmetric and positive definite.",
            "But this is just solving it for one dimensional representation."
        ],
        [
            "So if you want to and this is usually not not sufficient to really capture all the information.",
            "So we usually want to find a higher dimensional representation and we end up with a similar optimization problem, except we have some extra constraints that when we're looking for this new WS that are projection vectors, we want them to be uncorrelated with the ones that we've already found, because we want the solutions to really give us some new information.",
            "And we can use some projection matrices and orthogonalization techniques to also transform this problem into a standard multi parameter eigen problem and solve it as in the algorithm that was on the previous slide.",
            "And the main."
        ],
        [
            "Point is that by using linear kernels and avoiding multiplying matrices or computing inverses, always just doing matrix vectors multiplications, we can.",
            "We can make this method quite scalable because it's linear in the size of the data or in the number of views.",
            "But we also get a squared KK squared factor that is the number of the dimensions that we want to compute the dimensionality of the representation, and this comes from the orthogonalization.",
            "Because we have to always orthogonalize against full vectors and the more we have, the more costly it gets.",
            "So and this is."
        ],
        [
            "442 view CCA.",
            "This is how when you run it on unaligned corpus of two languages.",
            "Slovenian in English.",
            "These are.",
            "This is how the these vectors look like.",
            "Weather the the words with the highest.",
            "The weight in the vector star.",
            "Written in the order descending order here."
        ],
        [
            "So you can use this.",
            "This common space representation to do multilingual search or classification and I was mainly testing it on information retrieval task.",
            "So the the idea is to project all the all the all the corpus is in the into the common space and project the query there and then find which documents are the most similar to the query and.",
            "Return the results.",
            "So."
        ],
        [
            "We have documents in X view and documents and why you and the first thing is to map the first view into the common space, then second view.",
            "Then we are presented with a query.",
            "We map it.",
            "And look for the similar documents and retrieve.",
            "So no matter what language the queries in the result results will be documents of both languages."
        ],
        [
            "So I was experimenting mainly with European Parliament corpus europarl with 10 languages and 100,000 documents per language.",
            "I was I kept 8000 documents for testing and was doing mainly mate retrieval and Sue the query retrieval where mate retrieval is just because you already know which documents are translations of each other.",
            "So you take one document is a query projected and you know which which document is it's made, document it's translation and you look how similar they are.",
            "If they are the most similar to documents then this is supposed to be.",
            "Good.",
            "But a little bit more realistic scenario is to do observe the query mate retrieval where we just cut most of the words in the query document off and we just.",
            "We're just left with five, 2 or 10 words and this is supposed to be a bit more realistic, more similar to a typical query.",
            "And I was testing average precision, so I I.",
            "When I projected the query in I, I ranked all the documents with respect to similarity that to the query and then I computed the score.",
            "The rank of the mate document one over this rank and I averaged over all the queries.",
            "And."
        ],
        [
            "I was comparing my method with the with two to other approaches.",
            "One is to do K means clustering.",
            "You concatenate all the translations together to get one large vector.",
            "And to get Lelong long vectors and then just do clustering on this space.",
            "And after you got, let's say 100 clusters and their centroids, you chop those vectors apart and consider those components as this concept vectors that are that can be used in a similar way to the CCA vectors."
        ],
        [
            "And also compared it with LSI.",
            "I just concatenated all the documents and did a singular value the composition.",
            "Then took the first 100 single left singular vectors and or did the split and got the components."
        ],
        [
            "So this is a mate retrieval, the first, the green color means that the score is high.",
            "So the first table is is my MCC a method.",
            "This is LSI which is which is comperable and this is clustering that didn't do so well.",
            "And these are the retreiving from one, let's say.",
            "Spanish to German and we look at the.",
            "Look at the number.",
            "It's a 0.5.",
            "Oh"
        ],
        [
            "OK so I am this is pseudo query.",
            "Here the numbers are a lot lower.",
            "Instead of 90 we've got 0.2, but still the method outperforms the other two.",
            "And."
        ],
        [
            "This is the five words through the query retrieval, and we also see that in with Finnish language it seems to be doing a bit worse, and I think it has to do something with the preprocessing because I was doing very simple preprocessing."
        ],
        [
            "And so that's it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Huh?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so one of the main goals was to increase the scalability of CCA in respect to how many documents or text or data points it can handle, and to how many views it can handle because the baseline was to view CCA.",
                    "label": 1
                },
                {
                    "sent": "So there have been different approaches to tackle this problem.",
                    "label": 0
                },
                {
                    "sent": "So some some some work has been done on extending the classical definitions and Institute George of Steven have been more working mainly on that and John Short Taylor from UCL and equal or consider from Xerox on.",
                    "label": 0
                },
                {
                    "sent": "We've been working on some of correlation, definition of problem and Max Min formulation.",
                    "label": 0
                },
                {
                    "sent": "And there there have also been some other approaches, for instance probabilistic approaches by Ray Bontine.",
                    "label": 0
                },
                {
                    "sent": "He's doing some probabilistic PCA extensions to multiview learning.",
                    "label": 0
                },
                {
                    "sent": "And there's also been work on sparsity done by the Korea from UCL, and he was mainly working on a matching matching pursuit type of algorithm.",
                    "label": 0
                },
                {
                    "sent": "Greedy, greedy algorithm that was trying to find the sparse sparse representation of the mappings that CCA finds.",
                    "label": 0
                },
                {
                    "sent": "So the outcome of the project, though, is that the collaboration still continues within the smart project and results or results are just being put into deliverables.",
                    "label": 0
                },
                {
                    "sent": "And also we're preparing some further publications.",
                    "label": 0
                },
                {
                    "sent": "And so now I will mainly focus on the thing that I was working on and that is implementing the sum of correlation formulation for text mining.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is just a just a brief introduction to bag of words or we have documents that are points in the and we represent them by points in an N dimensional vector space.",
                    "label": 1
                },
                {
                    "sent": "And this is just how we construct the vectors.",
                    "label": 0
                },
                {
                    "sent": "Each feature is a word and we just count the number of occurrences and these are vectors, so.",
                    "label": 0
                },
                {
                    "sent": "From this moment on, we're just working on.",
                    "label": 1
                },
                {
                    "sent": "On the lesson linear algebra problems.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, and the similarity between documents is just the inner product between them.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the Canonical correlation analysis has been alive for quite some time and its main its main goal is to find 2 vectors in each space.",
                    "label": 0
                },
                {
                    "sent": "So that when we project all the data to those two vectors, those two directions we get, we get vectors that are maximally correlated.",
                    "label": 1
                },
                {
                    "sent": "And that can be that can be formulated in this equation.",
                    "label": 0
                },
                {
                    "sent": "With the covariance matrix and solve the eigenvalue problem so.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the CCA.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is just the graphical view, visualization and interpretation, so we have this documents in both spaces.",
                    "label": 0
                },
                {
                    "sent": "And we see that this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the direction we have two directions on.",
                    "label": 0
                },
                {
                    "sent": "If we so that if we project the documents onto them, we get kind of linear relation between them.",
                    "label": 0
                },
                {
                    "sent": "Let's see, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This this is this is supposed to be 2 points with the same meaning.",
                    "label": 0
                },
                {
                    "sent": "You wanna next space in one in white space?",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first and the longer arrow is the direction of maximal correlation, and we also have a.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2nd second dimension, but the documents are less correlated in that direction.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of the possible formulations of the problem of scale when you want to go to more than two views is to try to find the projections so that the sum of pairwise correlations is maximized.",
                    "label": 1
                },
                {
                    "sent": "So this is just a generalization of the two view CCA.",
                    "label": 0
                },
                {
                    "sent": "Instead of just correlation, we have some of our pairwise correlations.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this is the primal optimization with covariance matrices.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we.",
                    "label": 0
                },
                {
                    "sent": "It's it's usually very useful to go to the dual formulation because we can use the kernel trick and also the number of features is usually higher than number of documents and this is then more efficient.",
                    "label": 1
                },
                {
                    "sent": "To work in the dual and we also need to regularize becausw, otherwise this algorithm or overfits quickly.",
                    "label": 0
                },
                {
                    "sent": "When regularization is we just instead of having normal normal kernels, we have this regular rice kernel.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once we have that this formulation, we can just use the standard Lagrangian techniques.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we end up with a problem that is called the multiparameter eigenvalue problem.",
                    "label": 1
                },
                {
                    "sent": "It's it just seem it's very similar to standard eigenvalue problem, except that there's not one eigenvalue Lambda here, but we have a whole vector of eigenvalue, so it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a bit different.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "There there has been an algorithm around for quite some time, but was in was proved to converge.",
                    "label": 0
                },
                {
                    "sent": "If this giant block matrix is positive and positive definite and some.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is the simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "And the I I you can you can reformulate, or by using some substitutions.",
                    "label": 0
                },
                {
                    "sent": "The regional optimization into this scenario and just use this algorithm.",
                    "label": 0
                },
                {
                    "sent": "So here the block matrices were this LLIKIKJLJ on, so I could prove that this was a symmetric and positive definite.",
                    "label": 1
                },
                {
                    "sent": "But this is just solving it for one dimensional representation.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you want to and this is usually not not sufficient to really capture all the information.",
                    "label": 0
                },
                {
                    "sent": "So we usually want to find a higher dimensional representation and we end up with a similar optimization problem, except we have some extra constraints that when we're looking for this new WS that are projection vectors, we want them to be uncorrelated with the ones that we've already found, because we want the solutions to really give us some new information.",
                    "label": 1
                },
                {
                    "sent": "And we can use some projection matrices and orthogonalization techniques to also transform this problem into a standard multi parameter eigen problem and solve it as in the algorithm that was on the previous slide.",
                    "label": 1
                },
                {
                    "sent": "And the main.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Point is that by using linear kernels and avoiding multiplying matrices or computing inverses, always just doing matrix vectors multiplications, we can.",
                    "label": 0
                },
                {
                    "sent": "We can make this method quite scalable because it's linear in the size of the data or in the number of views.",
                    "label": 1
                },
                {
                    "sent": "But we also get a squared KK squared factor that is the number of the dimensions that we want to compute the dimensionality of the representation, and this comes from the orthogonalization.",
                    "label": 1
                },
                {
                    "sent": "Because we have to always orthogonalize against full vectors and the more we have, the more costly it gets.",
                    "label": 0
                },
                {
                    "sent": "So and this is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "442 view CCA.",
                    "label": 0
                },
                {
                    "sent": "This is how when you run it on unaligned corpus of two languages.",
                    "label": 0
                },
                {
                    "sent": "Slovenian in English.",
                    "label": 0
                },
                {
                    "sent": "These are.",
                    "label": 0
                },
                {
                    "sent": "This is how the these vectors look like.",
                    "label": 0
                },
                {
                    "sent": "Weather the the words with the highest.",
                    "label": 0
                },
                {
                    "sent": "The weight in the vector star.",
                    "label": 0
                },
                {
                    "sent": "Written in the order descending order here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you can use this.",
                    "label": 0
                },
                {
                    "sent": "This common space representation to do multilingual search or classification and I was mainly testing it on information retrieval task.",
                    "label": 0
                },
                {
                    "sent": "So the the idea is to project all the all the all the corpus is in the into the common space and project the query there and then find which documents are the most similar to the query and.",
                    "label": 1
                },
                {
                    "sent": "Return the results.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have documents in X view and documents and why you and the first thing is to map the first view into the common space, then second view.",
                    "label": 0
                },
                {
                    "sent": "Then we are presented with a query.",
                    "label": 0
                },
                {
                    "sent": "We map it.",
                    "label": 0
                },
                {
                    "sent": "And look for the similar documents and retrieve.",
                    "label": 0
                },
                {
                    "sent": "So no matter what language the queries in the result results will be documents of both languages.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I was experimenting mainly with European Parliament corpus europarl with 10 languages and 100,000 documents per language.",
                    "label": 0
                },
                {
                    "sent": "I was I kept 8000 documents for testing and was doing mainly mate retrieval and Sue the query retrieval where mate retrieval is just because you already know which documents are translations of each other.",
                    "label": 0
                },
                {
                    "sent": "So you take one document is a query projected and you know which which document is it's made, document it's translation and you look how similar they are.",
                    "label": 0
                },
                {
                    "sent": "If they are the most similar to documents then this is supposed to be.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "But a little bit more realistic scenario is to do observe the query mate retrieval where we just cut most of the words in the query document off and we just.",
                    "label": 1
                },
                {
                    "sent": "We're just left with five, 2 or 10 words and this is supposed to be a bit more realistic, more similar to a typical query.",
                    "label": 1
                },
                {
                    "sent": "And I was testing average precision, so I I.",
                    "label": 0
                },
                {
                    "sent": "When I projected the query in I, I ranked all the documents with respect to similarity that to the query and then I computed the score.",
                    "label": 1
                },
                {
                    "sent": "The rank of the mate document one over this rank and I averaged over all the queries.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I was comparing my method with the with two to other approaches.",
                    "label": 0
                },
                {
                    "sent": "One is to do K means clustering.",
                    "label": 0
                },
                {
                    "sent": "You concatenate all the translations together to get one large vector.",
                    "label": 0
                },
                {
                    "sent": "And to get Lelong long vectors and then just do clustering on this space.",
                    "label": 1
                },
                {
                    "sent": "And after you got, let's say 100 clusters and their centroids, you chop those vectors apart and consider those components as this concept vectors that are that can be used in a similar way to the CCA vectors.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And also compared it with LSI.",
                    "label": 1
                },
                {
                    "sent": "I just concatenated all the documents and did a singular value the composition.",
                    "label": 0
                },
                {
                    "sent": "Then took the first 100 single left singular vectors and or did the split and got the components.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a mate retrieval, the first, the green color means that the score is high.",
                    "label": 0
                },
                {
                    "sent": "So the first table is is my MCC a method.",
                    "label": 0
                },
                {
                    "sent": "This is LSI which is which is comperable and this is clustering that didn't do so well.",
                    "label": 0
                },
                {
                    "sent": "And these are the retreiving from one, let's say.",
                    "label": 0
                },
                {
                    "sent": "Spanish to German and we look at the.",
                    "label": 0
                },
                {
                    "sent": "Look at the number.",
                    "label": 0
                },
                {
                    "sent": "It's a 0.5.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I am this is pseudo query.",
                    "label": 0
                },
                {
                    "sent": "Here the numbers are a lot lower.",
                    "label": 0
                },
                {
                    "sent": "Instead of 90 we've got 0.2, but still the method outperforms the other two.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the five words through the query retrieval, and we also see that in with Finnish language it seems to be doing a bit worse, and I think it has to do something with the preprocessing because I was doing very simple preprocessing.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so that's it.",
                    "label": 0
                }
            ]
        }
    }
}