{
    "id": "uoddrwtzsahr75basimlnig7psv5cfyp",
    "title": "Machine Learning Methods For Protein Analyses",
    "info": {
        "author": [
            "William Stafford Noble, University of Washington"
        ],
        "published": "Oct. 5, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Computational Biology",
            "Top->Computer Science->Bioinformatics"
        ]
    },
    "url": "http://videolectures.net/mlsb09_noble_mlmfpa/",
    "segmentation": [
        [
            "Thank you for the invitation and thank you all for being here on Sunday morning at 9:00 AM.",
            "I will.",
            "I'll try to keep it lively so you can.",
            "Be glad that you got yourselves out of bed.",
            "I've got three different projects to talk about today.",
            "They all have to do with proteins to one of them is actually sort of an older sort of more traditional bioinformatics problem, finding evolutionary relationships among proteins and the other two are actually two quite different approaches to the same problem that have nothing to do with proteins."
        ],
        [
            "Sequence per say, but more with identifying proteins from mass spectrometry data.",
            "The thing that ties them together is the use of machine learning to."
        ],
        [
            "I was from these different types of data.",
            "I want to acknowledge that all of these projects are joint work with various people, so this is work with Christina Leslie, who's at Memorial Sloan Kettering Cancer Center and Ian Melvin and Jason Weston, who are at NEC Research in New Jersey.",
            "This first project is is about finding evolutionary relationships among proteins, and I want to start with an analogy which I'm sure some of you have heard before, which is.",
            "Most of you are familiar with the black."
        ],
        [
            "Best services offered by the NCBI.",
            "This is just a screenshot of the interface.",
            "The idea is that you have a query protein sequence which you put into this web interface.",
            "You click, it goes and does a database search and returns a ranked list of proteins where the ranking reflects the probability of evolutionary relationship or the evolutionary distance from the query to each target.",
            "In the ranked list."
        ],
        [
            "You can draw a pretty close analogue between that process and something like the Google search engine where you take a sequence of English or Slovenian text, put it into the database into the search engine and it gives you a ranked list of documents ranked by relevance at a conceptual level."
        ],
        [
            "Thing that these two search engines share is there is an underlying network.",
            "In the case of the Web, this is a picture of the part of the World Wide Web where nodes are computers are computers and edges are connections between them.",
            "But you could think of it as a picture of the of the.",
            "Web database that Google searches or as a picture of proteins where edges represent similarities among the proteins."
        ],
        [
            "Now.",
            "If we think about the protein homology detection problem that the problem has a very long history dating back one of the most widely cited algorithms is the Smith Waterman algorithm, which is a dynamic programming algorithm just for comparing pairs of sequences, and this was followed by BLAST which was published in 1990 and is become the most widely cited paper, and all of the scientific literature, which is essentially just a heuristic approximation of the Smith Waterman algorithm.",
            "In the 90s there were a number of different methods, probably best exemplified by PSI Blast for taking these pairwise comparisons and building iteratively building up models, profiles or profile hidden Markov models that allow you to search in sort of a larger area of the network space that I showed you in the previous slide.",
            "We published in 2004 a slightly different style of solution to this problem.",
            "An algorithm called rank prop, which was explicitly based on the page rank algorithm that's used by the Google search engine.",
            "The idea being that you rather than building an explicit model of the query, you perform a diffusion operation over an explicit protein similarity network and the amount of activation that flows in this diffusion process from the query to any target.",
            "Induces a ranking that gives you an idea of who is homologous to whom.",
            "The next year, H Search was published, which I think is now one of the best search.",
            "Search engines that's out there.",
            "H search is more expensive because what it's doing is building models not just of the query, but of all of the targets in the database and then doing hidden Markov model versus hidden Markov model style comparisons.",
            "What I want to talk to you about today is using."
        ],
        [
            "A different style of search, called supervised semantic indexing.",
            "The idea for this project grew out of work.",
            "The citations here that Jason Weston and some of his colleagues did in the context of natural language processing, the idea was to take a big compendium of documents, in this case 1.8 million Wikipedia documents and try to learn to induce a ranking on these documents where the training set consists of pairs of documents.",
            "And you're learning to rank by putting for any given target, you actually have one example that's linked, and one example that's not linked and you want to learn a ranking so that the linked ones tend to be closer to the target than the unlinked ones.",
            "And this works by embedding all of the documents into a lower dimensional space, learning the structure of that space, and then you can induce a ranking just by doing a sort of a Euclidean distance in that low rank.",
            "Embedding one of the keys to making this work is that there is a.",
            "Highly scalable optimizer that allows you to do the learning even when you have a data set of almost 2 million documents.",
            "So we wanted to try this idea in the context of proteins.",
            "So the key idea is to."
        ],
        [
            "Do something similar to what I just described, but with proteins we're going to take a very large database of proteins and we're going to embed it into a low dimensional space where by low dimensional I mean about 200 dimensions.",
            "Is what we've been using.",
            "And we're going to learn the structure of this space so that if two proteins are homologous, they tend to be close to one another and vice versa.",
            "After you've done this, learning which is relatively expensive, the actual classification querying is quite cheap because all you're going to do is compute the distances in this embedded space.",
            "So to make this work, we need 3 different things.",
            "I'm going to tell you about each one in turn.",
            "First, how we represent the proteins initially, so an initial feature representation, what how we train this thing?",
            "How we learn this embedding and then some signal to learn from an algorithm that can learn from that signal."
        ],
        [
            "So for the initial embedding for the higher dimensional space that we start from, we use essentially the same protein similarity network that we used in the rank prop algorithm.",
            "So the idea is you take some database, usually a precomputed database of all versus all this can be a PSI blast similarity network or an H search similarity network.",
            "Take all of the expectation values.",
            "The statistical values that come out convert them into sort of arbitrary weights using this exponential.",
            "Transfer function and then normalize those weights so that you essentially get something like a stochastic connectivity matrix.",
            "You can think of it as in the context of the rank prop algorithm.",
            "You could think of it as sort of a random surfer along the protein similarity network, and the weights give you your probability of taking any outgoing edge from the node that you're at currently.",
            "So mathematically, the representation looks like this.",
            "You have some."
        ],
        [
            "Very protein, P prime and you're going to embed this into this feature representation where the dimensionality of the feature space, which is the script L. There is just the number of proteins in the database that you're comparing against.",
            "So a key key property of this representation is that it's sparse, because of course if you take one query, most of the things in the database have no recognizable similarity, so they're very values are undefined, so they get.",
            "Sign them up.",
            "Probability of zero in this in this representation.",
            "So we're using either side Blaster."
        ],
        [
            "HH search as the representation, but we also use it in a second way which is as a training signal, so we're going to essentially take pairs of proteins and we're going to call each pair either positive or negative based on whether that pair is assigned a smallie value by the training signal.",
            "So in this case we use it.",
            "Any value of .01 and essentially the training procedure consists of picking pairs of examples and saying yes or no.",
            "Does the training signal the algorithm that we're learning from say that these are similar to each other or not?",
            "So our goal, given those pairs, is to learn."
        ],
        [
            "Some embedding, so you have this W matrix and then by L matrix which is going to give you an end dimensional embedding.",
            "So you're projecting from this higher L dimensional space down into an end dimensional space, and we're going to rank the database using this function function here, which is just the one norm of the embedded representations of the query and some protein from the database that you're interested in.",
            "So our goal is going to be to learn W. This is the only thing that we're learning.",
            "We're trying to.",
            "People this property holds.",
            "In other words, if we have a positive and a negative to positive is more highly ranked than the negative example."
        ],
        [
            "So graphically you can think of it like this.",
            "Here's a query protein.",
            "This is a good example where the positive is close in the embedded space and the negative as far.",
            "And here's a bad example over here, and one of the keys to making this work is that we use a margin based training procedure, so we want to enforce that the negative examples are farther away from the query than the positives by some margin, and so that can be presented this way where one is the size of the margin and these are the differences that you're interested in so.",
            "If it obeys this rule, if the query the positive is closer than the negative, there's no problem and you just have a zero loss.",
            "Otherwise you have this loss associated with the difference between the two.",
            "So it turns out that this is the same function I just showed you, it's."
        ],
        [
            "Lative Lee straightforward.",
            "Given pairs two different pairs of positive and negative pair to update the the parameters of your W matrix in such a way that you sort of push.",
            "There's four different operations.",
            "There are sort of two complementary pairs you're pushing the query away from the negative.",
            "You're pushing the negative toward the query, and then similarly for the positives.",
            "And this Lambda.",
            "Here is a hyperparameter that just specifies the."
        ],
        [
            "Great.",
            "So we're testing this in the context.",
            "We've tried several different settings, but I'm going to tell you about is this remote homology detection task.",
            "So this is a standard setting for this problem.",
            "The idea is that if you have protein structure that tells you a lot more about whether to proteins are evolutionarily related, so we can use the structural information to provide a gold standard to evaluate against.",
            "So we use the SCOP database, the structural classification of proteins, and we do super family recognition.",
            "The idea being that we're actually interested in homologs, and so we're going to say members of this class are considered positive examples.",
            "Things that are in the same fold, but different super families may actually be homologous to each other, so we don't consider those as positives or negatives, but things that are in other folds are considered negative examples, and so we're going to then do a semi supervised setting where the feature vectors are drawn from the entire database.",
            "But we only have labels.",
            "Instead of those and, we're going to use performance metrics being the area under the RC curve up to either the 1st or the 50th.",
            "False positive, averaged over different queries."
        ],
        [
            "So these are results averaged over 100 different queries.",
            "We've been careful to make sure that the the labels for the for that queries class are not used in the training procedure, so you have to retrain for each one of these queries.",
            "But essentially what you can see is the rock one and the Rock 50 performance for various methods.",
            "So PSI blast rank prop the proton bed.",
            "That's the name of the algorithm that we've come up with.",
            "The proton bad version of PSI Blast and then H predan approach.",
            "Bed version of HH Pred and what you can see is that in each case the the learning this embedding across all of the queries improves over just using the original training signal alone.",
            "Another nice feature of."
        ],
        [
            "This is that you can.",
            "You can use this for visualization.",
            "I know this is hard to see, but conceptually if you do this down into a 2 dimensional space you can actually look and see where the proteins are.",
            "So for example, here's just a zoom in on one section and the colors represent different Scott.",
            "I think they're super families actually.",
            "And then you can see that there are clumping together based on what this categories are."
        ],
        [
            "Now we wanted to take this one step further, because of course the fact that structure gives you more information about homology implies that you might want to use that information during the training procedure.",
            "So the nice thing about this setup is that we can use structural information during training to learn a good embedding even when structure is not necessarily available during the query time.",
            "So the idea is that we're going to come up with additional constraints.",
            "On our proteins, based on structural information and use those during the learning phase so that we get a good embedding and then we won't require structural information during classification.",
            "So to do that we're going to multitask learning and essentially learn the embedding both from the sequence information and from the structure information.",
            "So there's a couple different ways that you can image."
        ],
        [
            "Engine representing structural information.",
            "One simple way is if you have, for example this cop hierarchy, you could introduce what we've done is introduce sort of pseudo proteins representing the centroid of each Scott category, whether it's a class or a family or a super family, and we want to enforce something so that examples that are members of that class, proteins that are members of that class are closer to their centroid then proteins that are not members of that class and that gives you.",
            "Rank based constraints that you can just add into the training procedure.",
            "Sort of interleaving those with the sequence based constraints.",
            "Similarly, if you don't have a human curated database, you can use as a training signal, something analogous to side Blaster HH Pride, but that actually uses the structural information, so you could do this with any structural."
        ],
        [
            "Molarity metric we used.",
            "We chose one called Mammoth, but essentially the form of the constraints is the same.",
            "But now these constraints are coming from the structural information."
        ],
        [
            "And what you can see here in red are the versions of this algorithm that take into account structure during the training.",
            "But don't use structure during the classification.",
            "And again you see significant improvements relative to the methods that were only trained sequence alone."
        ],
        [
            "Another way of looking at this same data just to give you a qualitative sense for the difference is this is a plot that's showing the arosi one metric on this axis."
        ],
        [
            "And the number of queries out of 100 that achieve that score or better.",
            "And so you can see the different algorithms there are actually two different of the SCOP.",
            "Using either constraint based on classes like I described, or constraints based on more similar to the mammoth style constraints that I described and what you can see is that all all three of those structural based ones are above the other curves, and you get a similar kind of phenomenon if you look at the RFC 50 scores.",
            "And then finally one thing that we just figured out yester."
        ],
        [
            "I actually was that.",
            "We'd struggle along a lot with the rank prop algorithm in the sense that the scores that it returned were not well calibrated from 11 query to the next, and so we had to have a separate learning phase that would then map those scores on to something that was interpretable from one query to the next and a nice side effect of learning a single embedding for all proteins is that it turns out that the scores from this new algorithm program bed are actually well calibrated, so we haven't yet figured out a way to.",
            "To assign a semantics to them, although I think there are straightforward ways to do that, so we can't say what the value is.",
            "Yet at this point, but at least we can say that a score of .75 from 1 query corresponds to a .75 from a different query, and that's shown by taking all of your results from all 100 queries, sorting them into one single list, and making our OC curve.",
            "And this shows that our OC curve for the different methods and the improvement is still good in that in that sort of calibration.",
            "Setting."
        ],
        [
            "OK, so that's the first project.",
            "The key idea here again, was to use this semi supervised semi supervised or supervised semantic indexing that bootstraps from unlabeled data and a training signal and that allows you to incorporate structural information via multitask learning.",
            "For the other two projects that I want to talk about."
        ],
        [
            "We're shifting gears to talk about mass spectrometry data and this may be less familiar terrain to some of you.",
            "I should say the graduate student that did most of this work is Oliver Serang and Mike MCAS is a mass spec trauma tryst in my Department."
        ],
        [
            "Help us with the mass spectrometry aspects, so in case you're not familiar with this problem, let me try to give you a little background of what the task is that we're trying to solve.",
            "So the goal of shotgun proteomics or tandem mass spectrometry is to take a complex biological sample an characterize the proteins that exist in that sample.",
            "So in its simplest form you just want to identify them, ideally eventually would like to be able to quantify each of those proteins, but for now we're focusing just on figuring out what's there.",
            "Drop or some saliva or what have you.",
            "And so the way that the device works is there's a preprocessing step where the proteins are digested into peptides.",
            "On those peptides are then subjected to tandem tandem mass spectrometry and that gives rise to these observations.",
            "These Spectra, which I'll describe in a little more detail later, but for the purposes of this problem we assume that there is some database search algorithm that gives you an initial noisy mapping from the Spectra.",
            "Two possible peptides that might have produced them, so the idea is that there was one homogeneous population of peptides that is responsible for producing each spectrum, but it's difficult to figure out what that what the identity of that peptide was.",
            "So you have scores that represent how well each spectrum matches each peptide, and typically you take the best matching peptide for each spectrum.",
            "And then there's also usually a procedure where you take the best matching spectrum for each peptide."
        ],
        [
            "So in the standard setting of this problem, you sort of collapsed the right hand side of that tripartite graph down into these things, which I'll refer to as PSMS or peptide spectrum matches, and each one has a probability associated with it saying how good is this match for this particular peptide?",
            "These edges represent whether this peptide occurs in this protein, so of course it's a many to many mapping.",
            "'cause of peptide can be in more than one protein.",
            "And of course each protein has many peptides in it, but our task is to take these probabilities, take into account the structure of this bipartite graph and induce a ranking on proteins, ideally with probability associated with each one, saying how confident are we that this particular protein was in the sample?",
            "So there are several existing methods.",
            "I think the 1st first systematic treatment of this problem was with protein profit paper which was published in 2003.",
            "This used sort of heuristic M like algorithm to learn a set of weights on that bipartite graph and a set of protein probabilities or pseudo probabilities.",
            "And this is now I think the most widely used tool for solving this particular problem.",
            "We also in this work compare against an algorithm."
        ],
        [
            "LMS Bayes, which was published in Recom last year, which uses a more fully probabilistic model, but it has lots of parameters and requires a sampling procedure to get the posterior estimates."
        ],
        [
            "So our question was going into this as a starting place for doing more sophisticated modeling later.",
            "Could we come up with the simplest model that we could and still try to get exact posteriors?",
            "Could we do marginalization with respect to a simple model just by doing some graph manipulations on that bipartite graph?"
        ],
        [
            "And rather than having hundreds of parameters, we decided to boil it down to three parameters.",
            "The model that we have essentially assumes that you have some probability Alpha that if a protein is in the sample, it's going to produce any given peptide within it.",
            "Some noise model characterized by a parameter beta, that sometimes you'll see a peptide even though the protein isn't in the sample, and then a prior probability gamma on proteins that are present in the sample and for the prior.",
            "We just use a uniform prior, so there's really just two free parameters that we have to.",
            "Learn for this saw this model.",
            "To make this simple of a model."
        ],
        [
            "We have to make quite a few assumptions.",
            "Most of them are relatively.",
            "Non controversial, I tried to summarize them all here.",
            "The idea is there's a list of assumptions here.",
            "This is a small tripartite graph and each of the little numbers here corresponds to one of the listed assumptions.",
            "So for example, you have a conditional independence of peptides given proteins, and that's represented by this link here, saying that if you know this information that you don't have to include a directed edge in the probability model between two different peptides, and similarly there's a conditional independence of Spectra given peptides.",
            "So that's shown over here.",
            "The the different parameters are represented by 3, four and five.",
            "The assumptions 3, four and five of peptides being produced either by proteins or by the noise model.",
            "And then you also have that the proteins themselves are independent of each other and then the dependence.",
            "The idea that if you know something about these peptides you don't have direct dependencies between the proteins and the Spectra."
        ],
        [
            "I'm not going to go over the details of how you write this out, but here is a one equation that sort of summarizes the model rather than try to explain all of the nomenclature, the main thing to focus on is there's a big summation and multiplication there that are that are expensive.",
            "Essentially, if you really wanted to compute this directly, it requires marginalizing over a very large set.",
            "You can think of it conceptually as if you have, you know.",
            "6000 proteins in your database?",
            "You have to enumerate the powerset of all possible subsets of that 6000, which is a big number to say the least."
        ],
        [
            "So Oliver sat looking at this problem for awhile and came up with several different ways to try to make it tractable or approachable computationally.",
            "The first one is relatively straightforward to notice that the bipartite graph that we're given is actually consists of a bunch of connected components are unconnected components and so you can divide.",
            "You can partition the bipartite graph into the connected components and then say, well, if we draw an inference about these things because of our independence assumptions, we.",
            "We don't have to worry about any of these, so it breaks down into.",
            "In this case, 3 smaller subproblems, which that by itself gives you a big big improvement because the main thing you have to worry about then is whichever one of these is the largest connected."
        ],
        [
            "Covenant.",
            "The next step is to do a clustering.",
            "So the idea here is that we're going to collapse proteins that have the same connectivity into a sort of a super protein.",
            "So if you look at protein one and protein 2, they're both connected to both of these peptides, and so you can think of it as well.",
            "If in this model you have sort of four possibilities, there is either they're both.",
            "There they are neither there only protein one is there only protein, two is there, and the key observation is that those last two are.",
            "Indistinguishable from the models perspective, so you'd like to say rather than having four probable possibilities, there's just.",
            "You can either have one protein there, two proteins.",
            "There are no proteins there at all, and so this supernode here has essentially end different states rather than two to the end different states.",
            "After you've done this collapsing, and you can prove that you haven't changed the problem at all, that it's exactly the same solution."
        ],
        [
            "And then finally one of the one of the other features of these datasets is that a lot of the probabilities assigned by by the precursor software that we call it.",
            "There's a piece of software called peptide profit that actually produces the probabilities.",
            "Alot of those probabilities are zero.",
            "Saying there is a match here, but it's so bad that we don't really believe that this is a strong match, and so the observation is that if you have something like this where the probability is 0.",
            "It doesn't hurt you to just split it into two separate.",
            "You've sensually doubled this node, and that allows you to break this bigger component into two smaller components, and again, you can prove that you can get exactly the same answer as you would have gotten if you'd enumerated that entire component.",
            "Now, in practice, sometimes after those three optimizations, it's still not computationally feasible to do the marginalization.",
            "So in practice, what we have to do is sometimes we get a component where we have we have to take some of the probabilities that are small and force them to zero in order to do more pruning to make it computationally feasible."
        ],
        [
            "But even for the for the datasets that we looked at, so there's four different datasets that we examined in this study from H. Influenzae from a yeast whole cell lysate and then from 2 prepared samples of purified proteins.",
            "First one has 18 known proteins in it, and the other one has 49 known proteins in it.",
            "And what we're showing on the top is the size of the problem, so the number of nodes on each side of the bipartite graph, and the number of edges, and on the bottom is the size of the problem.",
            "This is the log base two of the size of the problem, so.",
            "Two to the 33,000 is a pretty big number for the initial problem, but of course there's a lot of of separate components, and so as you apply each of these three, you get down to two to the 60th, two to the 47, and so on.",
            "So it makes this much more feasible."
        ],
        [
            "Now, in terms of performance, the model that we're using, we expected to be maybe do as well as some of the existing methods, but it's such a simple model that we didn't expect it to actually do better, and in some cases it actually seems to be doing a little bit better.",
            "So especially on like this smaller data set here we have the green line.",
            "Here is the model that we've produced in the Red line is protein profit.",
            "The diagonals here correspond to ties essentially.",
            "Protein profit gives a lot of things probability of 1.0 and this is just showing number of false positives versus number of true positives.",
            "So sort of an unnormalized arosi curve in some of these cases the all the different variants, the two different variants essentially agree with each other completely, with some minor differences.",
            "This is the only data set where we had results from this, this other Bayesian sampling algorithm and they seem to agree fairly well, but in the crucial part we get a little bit of improvement.",
            "Up there in the corner."
        ],
        [
            "There's also, of course, there's a learning procedure.",
            "We essentially do cross validation to select those two parameters, and we wanted to show that it doesn't really matter that much.",
            "Those two parameters generalize pretty well from one data set to another, so this is just showing if we take parameters that we learn on the first data set.",
            "Second, data set is actually a bunch of different labs have taken the same physical sample and analyze it using a variety of different mass spectrometry platforms in different experimental settings.",
            "So there's actually, you know, I think, on the order of 100 different.",
            "Datasets that comprise that second, the ISP 18 data set and what you can see is that if you compare the area under the RC curve up to the 50th false positive from protein profit versus our method, we're almost always doing better even though the parameters were learned from a completely different data set."
        ],
        [
            "So this was a good start.",
            "It seems like we've introduced the ability to actually compute protein posteriors exactly with a simple model and what we're working on now is to try to learn a more sophisticated protein specific priors, and actually more specifically to learn sort of a variant of that peptide emission probabilities so that you can say, given this peptide sequence, I'm more likely to observe it and this other peptide sequence is less likely to be observed.",
            "So that's what will next time you hear me talk.",
            "That's what I'll be talking about.",
            "Huh?",
            "OK, so then the third project that I."
        ],
        [
            "Wanted to tell you about is actually solving the same problem, but in a quite different kind of approach, so the previous one we were using a very simple generative, fully probabilistic model.",
            "This one again, it's a collaboration with Mike Makas, Jason Weston, and Marina who is a graduate student in why you and also works at NEC Research with Jason this is more.",
            "This solution is more in the style of the first problem that I told you about where rather than build up an explicit model where essentially going to try to design.",
            "A learning algorithm that directly maximizes the thing that we're interested in maximizing."
        ],
        [
            "So the problem is the same where we've got a bunch of Spectra with identifications to peptides and then mapping on to proteins.",
            "In this case, we're not going to.",
            "We're going to do a little bit of simplification, but."
        ],
        [
            "Not as much so.",
            "The key idea is there's a couple of different key ideas.",
            "So first of all, we're going to get a single probability per peptide spectrum match, and then do protein level inference.",
            "So in other words.",
            "Going to get."
        ],
        [
            "Probabilities for all of these, and we're not going to collapse this down to a bipartite graph.",
            "We will actually enforce that there's only one mapping per spectrum, so for instance, for instance, this .06 would be illuminated, but you could have multiple spectrum mapping to a single peptide.",
            "And then what we're going to do is.",
            "Oh sorry, I'm reading this backwards though.",
            "What what people have done previously is to do that and then say.",
            "We're going to take all of this information, solve it all, give us 11 number like like I did in the previous solution that characterizes sort of our belief that this peptide is present in the mixture and then just use that single number in a second processing stage to try to make inferences about the protein level and the idea here is twofold.",
            "We want to first of all, not put it through this bottleneck, so we want to instead of having all everything get boiled down to a single probability.",
            "To maintain a rich feature representation and then do a single joint inference to try to maximize the thing we're interested in, which is really protein level identification rates."
        ],
        [
            "So what we did was, rather than using, here's one probability that tells us whether we believe this peptide is here.",
            "We instead use a richer feature representation.",
            "There's a lot of jargon on this slide, but I'll just give you a couple examples so this cross correlation score is the main score that's used in the initial search.",
            "So remember, each spectrum is initially searched against the database, and the thing that it's doing the searching with this is this cross correlation score, and then there's some other variant scores that have to do with.",
            "How well what's the difference between the best and the second best score?",
            "Or there's another simpler score that's supposed to be sort of complementary to the cross correlation?",
            "But then we also use properties, not just of the quality of the match between the peptide in the spectrum, but also properties of the peptide itself.",
            "So what's the observed mass of the peptide?",
            "There's also the difference between the observed and theoretical, so when you observe a spectrum, the device actually says.",
            "Here's a spectrum, and here is the mass of the thing that produced this.",
            "So you can look at the difference between that.",
            "And what you actually matched against to give you an idea of whether you're correct or not, and then other properties over here that have to do with whether or not, for example, the ends of the peptide have the expected amino acids.",
            "So if you use trips and as the enzyme to cleave the proteins into peptides, you should have a K or an R at the end you know.",
            "And if you don't, that's evidenced that maybe this isn't a good match.",
            "And then also information about how much charge each peptide was associated with.",
            "The spectrum was associated with.",
            "So in order to take all of these 17 features and boil them all down into a single peptide level score, we use a two layer neural network.",
            "We actually experimented with."
        ],
        [
            "Using a simpler representation, sort of like I did in the first thing, just using a simple linear embedding, but the two layer network gave us better performance, so you've got.",
            "These are the 17 features, and in a neural network you have some transfer function down here they have weight stored on each of these edges there hidden nodes.",
            "I drew 5 but I think we actually used three in this work.",
            "We tried 3, four and five and by the time we got to five we saw evidence of overfitting.",
            "So we just fixed it at three after that and then you get some output.",
            "Which is essentially a score that tells you the quality of this peptide spectrum match.",
            "Now, this wouldn't be that different from what a lot of other people have done in the context of trying to evaluate peptide level scores, but the key to making this."
        ],
        [
            "Station different from previous things that we then embed the training of that neural network into a higher order structure that represents the entire problem jointly.",
            "So here's the function that represents that neural network, and this you can think as scoring each one of the edges here from each spectrum to one peptide OK, and so now you've got three different possible scores for each of these different PSMS, and then you're going to say there's a score for the PEP side, which is the maximum of all the PSM scores that came into it.",
            "So you can imagine that maybe your initial neural network says S1 is the best match to E one, but then you in the next phase of learning you say Oh well, actually it changed and now I think S 2 is better.",
            "So we we haven't collapsed this down into a single layer and then similarly you have a summation for each protein over all of the peptides that it contains an critically, there's a normalization here that depends not on the number of peptides that you observed, but the number of peptides that actually exist.",
            "In the protein, so one of the things one of the drawbacks to existing methods is that things like protein profit will look at and say I saw four matches and four seems like a pretty good number, but it might.",
            "It doesn't take into account the fact that maybe this is a very large protein which contains 100 theoretical peptides and four matches.",
            "In that case is not as compelling information as four matches when your proteins only 57 amino acids long.",
            "So we're going to do a stochastic gradient descent."
        ],
        [
            "Training and the idea is we're going to essentially pick a random protein, and we've got a protein and we've got a label.",
            "So the key to making this work is this idea which has been used before in mass spectrometry that you can introduce to your database so called decoy proteins or decoy peptides.",
            "So imagine that you're searching the database of all proteins human.",
            "You can then take each of those proteins and shuffle them and search against the target decoy database, and the idea is that if your algorithm pulls out one of those shuffled ones, you know for sure that that is an incorrect match, and so we treat the label target versus decoy.",
            "We treat that as a label and then we can just compute this Fr, which is that the output of that entire three stage calculation that I showed you in the previous slide and just basically check whether or not we've got.",
            "We've got a good score.",
            "For the Fr.",
            "So we're using this hinge loss function, so again, there's a margin here, and if your score is good enough, then you're fine.",
            "And if it's bad, then there's some loss associated with it and you do a gradient update essentially to try to change the parameters of your neural network to try to minimize that hinge loss function.",
            "So there's a couple of different ways that we can evaluate how well this is working, the simplest one."
        ],
        [
            "Is to use a target decoy evaluation, so these are different decoys.",
            "Then we use during the actual training procedure, but the idea is again we have false positive proteins.",
            "Here true positive proteins here, and we've compared it now against two other methods.",
            "So Barrista is the name of the algorithm that I described before protein profit and ID Picker are two competing methods.",
            "Protein profit is that heuristic.",
            "M like procedure ID picker is a much more.",
            "It's a sort of a parsimonious procedure.",
            "You essentially say.",
            "I'm going to choose the best match for each, for each peptide, and then try to find a set of smaller set of proteins that explains all of those matches.",
            "An ID picture doesn't actually give you a full ranking, so we had to just run it with a couple of different thresholds.",
            "That's why there's only dots for those, but you can see that protein profit and ID picker are quite consistent with each other, and that the barrista by doing this joint optimization is getting considerably more positive.",
            "So for example, at a fixed false positive rate.",
            "You know, rather than getting say for example about 1200 proteins, we're getting more on the order of 1500 proteins identified from the same sample, and for someone like Mike Mcas who spends a lot of time trying to identify samples, this is a very significant difference in the rate of identification.",
            "So this is we did this on several different datasets, so this is the one I just showed you.",
            "These are different cleavage agents, so the experiment was repeated but digesting the protein initially with a different type of enzyme and."
        ],
        [
            "This is a different data set from the C. Elegans, and you get a similar kind of improvement in each case."
        ],
        [
            "It's also possible, although a little bit dicey or to try to use some other kind of experimental method as an evaluation.",
            "So here we use.",
            "There were I think, 3 different experimental methods that are complementary to mass spectrometry, where they had taken yeast cells in similar, although not the same conditions and identified proteins from those.",
            "So we we took the intersection of those sets and said, well, anything that was identified in these published studies as being present.",
            "In these yeast cells will call that a true positive and other ones.",
            "We just list the total number of positive examples here.",
            "And so again you see a difference between protein profit and ID picture.",
            "On the one hand and barrista on the other.",
            "The ranking is not as long for protein profit, simply 'cause it doesn't.",
            "It gives lots of things with probability of one and lots with probability 0."
        ],
        [
            "And again, we can do this for all the different yeast datasets, and in each case we get an improvement."
        ],
        [
            "So we wanted to understand why Beresta was making better identifications, and so this is sort of a graphical depiction of what we think is going on.",
            "It's a little bit confusing, so let me try to explain it.",
            "This is the length of each protein and then each row in this picture corresponds to one protein that was identified by one method and not the other at a fixed false positive rate.",
            "I think we had 10.",
            "We allowed ten false positives and the coloring is it might be hard to see on here, but.",
            "A lot of the region in between is Gray, which means there was a peptide that was never observed, and then the other ones that have color have probabilities from zero to 1.",
            "So a couple of important things to note.",
            "First of all, these probabilities are not actually given to Barrista as part of its input.",
            "It's getting that 17 dimensional feature vector, but just for comparison, we colored them all in the same scale and then also protein profit.",
            "One of the things that it requires is that you threshold the probabilities, so actually anything below .05 is actually not being provided as input to protein profit, but we included it here just again for comparison.",
            "So there's a couple things to notice.",
            "The most striking of which is that.",
            "There's a lot of much shorter guys here.",
            "In fact, there's four up in that top picture that were more than 2000.",
            "I just truncated them to make the picture slightly more legible, and so on.",
            "Average protein profit.",
            "And this is something that Mike told us.",
            "A priore protein profit has this tendency to have very large false positives because it doesn't successfully penalize for for having very long length.",
            "It says, well, you've got some good matches and doesn't.",
            "Doesn't take into account the fact that those are sort of weighted against the length of the protein.",
            "Another thing that has come up a lot in the literature is that up in this region there's a lot of very short proteins that have just a few.",
            "Identifications by by the initial search and in fact there's been a rule that's been in the literature for awhile, which is that in order to do a positive identification of a protein, you need at least two peptides to identify.",
            "It is called the two peptide rule."
        ],
        [
            "And so we went and looked at our results and said how many of these guys are actually what in the literature referred to as one hit wonders, that is, proteins that we identified on the basis of a single peptide.",
            "If you use the .05 threshold that that protein profit uses, there are actually quite a few.",
            "There's on the order of 100 that are one hit wonders from the perspective of protein profit, but a lot of those have many many week matches, so innocence baristas able to say well, you don't have any single peptide that has strong evidence, but you've got 22 out of 25 where we saw weak evidence in favor of them, and so it can integrate that and.",
            "Still, make a positive call that were actually 9 cases where protein profit or sorry where barrista made a one hit wonder call on the basis of justice.",
            "Single peptide identification.",
            "This is one of them just to show you what we're looking at.",
            "This is the mass to charge axis of the spectrum and this is just the intensity.",
            "This is the actual peptide sequence that was identified and what we've done is labeled.",
            "Each of these peaks according to which cleavage location it corresponds to.",
            "So any particular cleavage location like.",
            "After the E here gives you two ions, the B ion corresponding to VE, which is this red peak here and the Y on corresponding to all of these guys, which is going to be probably Y11Y12 down there.",
            "The key observation here, I realized that most people haven't spent a lot of time gazing at Spectra, but for anybody who's looked at a lot of Spectra, they immediately say this is a very compelling identification.",
            "The fact that you see all of these peptides, all of the ions that you expect there are no very tall peaks that aren't accounted for.",
            "Most of the remaining peaks are quite small.",
            "Seems quite clear that this peptide is actually present in the mixture, and Furthermore, this is the protein sequence down here.",
            "It has this huge.",
            "Peptide so these vertical bars represent the cleavage locations for trips and so some of them are.",
            "The tryptic peptides are so small that they could never be observed in the Mass Effect there outside of the mass rate mass range and similarly this one is so large that it could never be observed, so this entire protein only has two observable peptides and one of them is observed with very strong probability.",
            "So this is a case where I would say I believe this thing is here on the basis of justice, single identification because we're taking into account.",
            "This additional information as well."
        ],
        [
            "And then finally, sort of.",
            "As an aside, we also wanted to try doing a multi task version of this optimization so.",
            "In a lot of protein identification, experiments are a lot of mass spectrometry experiments.",
            "The goal is just to find out what proteins are there, but sometimes you are actually interested in, for example, different splice forms of a protein.",
            "So you want to know things about which particular peptides have been observed as well.",
            "So in those settings you could make the argument that just optimizing for protein level identification is not ideal.",
            "You'd like to both identify proteins and identify peptides in the same optimization.",
            "And so we show in the in this submitted paper that it's possible to essentially include terms both for maximizing protein identification, identification rates, and peptide identification rates, and this is just a comparison either at the peptide level or the protein level of protein profit, and then barrista when you're either doing 1 task, the other task, or both tasks together.",
            "Now what you can see is that at the peptide level, multitasking improves relative to either single task optimization.",
            "So a little bit surprising because you would think it would be intermediate between the two, but the fact is that if you use protein level information that helps you to do peptide level identification and the results at the protein level are sort of more in line with what you'd expect a priore that you can sort of say, well if we just optimize for proteins we do well, and if we just optimized for peptides, we do worsen.",
            "If we optimize for both simultaneously.",
            "We do somewhere in the middle of the two.",
            "So."
        ],
        [
            "The key here and this is similar to the first problem that I talked about is that we've formulated this as a single direct optimization of the thing that the mass spectrometry actually wants to solve, which is find me in many proteins in this data set as possible, and we've tried to build into the algorithm some some smarts to take into account.",
            "For example, example many weak matches or normalizing for the total number of peptides that are in the protein, and then we also showed that you can embed this in a multi task learning framework.",
            "To do simultaneous joint."
        ],
        [
            "Optimization.",
            "So.",
            "Overall, these three different stories.",
            "I think one of the there's a couple of different take home messages.",
            "One of them is that sometimes you want a discriminative model.",
            "Sometimes you want a generative model.",
            "It really depends on what the task is at hand and what your goals are.",
            "But that in any case developing application specific algorithms that take into account what the problem that you really want to solve is often going to give you better results than just using an out of the box algorithm."
        ],
        [
            "I want to thank you and also point out that in addition to the MSB, we organize each year in Vancouver, the MLC be, which is sort of a similar workshop, but it takes place.",
            "It's affiliated with the Nips Machine Learning Workshop.",
            "It's in Whistler, BC in December.",
            "These rules and you can submit unpublished or recently published work, and there are six page abstracts due September 27th.",
            "So I encourage you to submit there as well.",
            "Thank you very much.",
            "I'll be happy to take questions if there are any.",
            "Correctly, you can take in a mass spectrometer.",
            "And then get out both.",
            "Identification.",
            "Well, so this is not multi orselli, so these are tandem mass Spectra, so we're taking in.",
            "You know, maybe 10,000 fragmentation Spectra, each one corresponding to a single peptide and jointly optimizing over all of those.",
            "Sorry if I didn't make that clear.",
            "So in MALDI or seldi you are taking a sample and sort of you know I think there's a matrix and then there's a laser and you're essentially looking at.",
            "You're getting one large spectrum that tells you a lot of information, whereas in the in this framework you're actually taking the proteins first, digesting them, doing a liquid chromatography, and then over the overtime, maybe half hour an hour.",
            "The device is pumping out five or six Spectra per second.",
            "And giving you what is hopefully for each spectrum the results of a homogeneous isolation of similar or identical peptides.",
            "You get much better.",
            "That's the I think that's right, yeah.",
            "How?",
            "How much is the?",
            "Completely worth it isn't.",
            "It's not.",
            "I think they're complementary.",
            "Unfortunately, I haven't worked directly with multi cell D data myself.",
            "My impression is that it's more popular in the context of things like biomarker discovery as opposed to just characterizing a complex mixture so, but I really.",
            "I'm beyond the edge of my expertise, so I start comparing and contrasting them.",
            "Yeah.",
            "So science correct also includes the expected sequences.",
            "Which problem we talk about them.",
            "That's right, so.",
            "That's right, so there's two different variants of this task, so there are so called de Novo approaches where you only provide the Spectra, and you, say, find what are the the peptides that generated them.",
            "In general, because your search space is so much larger, you that has much lower success rate.",
            "On the other hand, of course, if there are post translational modifications or proteins that have not been identified in this genome, the database search approach is obviously not ideal.",
            "That's right, that's right.",
            "Don't use.",
            "Any inquiry information about what you?",
            "You take our Easter.",
            "Certainly going to be there.",
            "Yeah, yeah, so we.",
            "We don't, but you certainly could.",
            "In fact, there was a paper lately.",
            "I think it was in bioinformatics from Edmark ATS group where they do essentially that where they say we take all of these existing resources and we build a protein level prior and then we can put that prior in and say, well, we've got weak evidence that this protein is here, but we've seen it 47 times before, so that boosts our belief that it's there.",
            "So that's a reasonable thing to do.",
            "We just haven't done it in this setting.",
            "Yeah.",
            "Yeah, there's a lot of different ways that people generate decoy sequences, so I mean the simplest way you can just take the proteins and reverse them, although of course sometimes you have palindromes, you have to treat those specially, or you can just shuffle them.",
            "Or you can generate them from Markov chain.",
            "In this case, I think we just shuffled and then try it and then check and make sure that it actually got shuffled that we don't get the same peptide as a target Anna Decoy.",
            "Yeah.",
            "And about the algorithm in the first problem.",
            "Because it relies on fair looks quite computationally expensive.",
            "Can you tell something more about computational complexity?",
            "I don't know that much about the computational complexity, other than that it's surprisingly cheap.",
            "I don't know the the innards of how the optimization works, so I'm afraid I'll have to.",
            "I can certainly give you a site for the LP version that talks about that in more detail, but it relies on the fact that this representation is sparse and these updates are individually quite cheap.",
            "One of the keys is that you choose.",
            "During training you choose pairs of positive and negative pairs, so you're essentially balancing the optimization.",
            "So even though there are many, many more negative pairs, in general you're going to focus on saying, well, I'm going to try to.",
            "I really don't care if this guy is 4 miles away or 10 miles away, he's just far and not interesting, so you the optimization proceeds more quickly because you're balancing the training set.",
            "Well, So what you do is you're doing this on line training over the entire all possible pairs, but just randomly choosing with replacement each time.",
            "So in principle, if you continue the training long enough, you'll eventually use all of them.",
            "But there's no preselection of downsampling that happens.",
            "Use only features.",
            "Try to use features on the person's bag.",
            "Oh, I see no.",
            "The only features that we've used are the training, the feature representation that I showed you, which is those.",
            "Y values provided either by PSI Blast or HH search or Mammoth or Scott.",
            "You certainly could try to use additional information and there's no conceptual reason other than just the you have to somehow normalize them to be similar to one another.",
            "But again, it's a non parametric method so you could imagine taking the feature representation I showed you and then appending a vector of expression values that were normalized in a similar fashion.",
            "Victor let you get again sparse in some sense.",
            "Or do you think that's a good point?",
            "So that would make if you didn't, you would either have to somehow enforce sparsity on those additional features, or pay a big cost in computation.",
            "And then of course, the other drawback would be to do a query.",
            "Then you would require.",
            "All those other features, probably a more reasonable way to do it, would be to introduce 1/3 task into the multitasking and say, OK, we've got this.",
            "These were still going to use the sequence based features, but we use a training signal that says things that are similarly regulated should also be close in this embedded space.",
            "A little bit too.",
            "You constructed so these these are constructed based on prior knowledge about what peptides occur in which proteins, right?",
            "So first you select a set of proteins that you expect somehow to be there.",
            "All the ones that are so you have to have the genome and you have all the inferred Orfs from that genome.",
            "And then you just exhaustively say find all the tryptic peptides and so it's a deterministic procedure to find that bipartite graph.",
            "Coco.",
            "That procedure of very straightforward.",
            "I mean, you know there's as long as somebody hands you the set of proteins you just go through and find kayson ours and introduce the cleavage breaks.",
            "And then just ask you know how many times did I see this peptide in different proteins so?",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for the invitation and thank you all for being here on Sunday morning at 9:00 AM.",
                    "label": 0
                },
                {
                    "sent": "I will.",
                    "label": 0
                },
                {
                    "sent": "I'll try to keep it lively so you can.",
                    "label": 0
                },
                {
                    "sent": "Be glad that you got yourselves out of bed.",
                    "label": 0
                },
                {
                    "sent": "I've got three different projects to talk about today.",
                    "label": 0
                },
                {
                    "sent": "They all have to do with proteins to one of them is actually sort of an older sort of more traditional bioinformatics problem, finding evolutionary relationships among proteins and the other two are actually two quite different approaches to the same problem that have nothing to do with proteins.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sequence per say, but more with identifying proteins from mass spectrometry data.",
                    "label": 0
                },
                {
                    "sent": "The thing that ties them together is the use of machine learning to.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I was from these different types of data.",
                    "label": 0
                },
                {
                    "sent": "I want to acknowledge that all of these projects are joint work with various people, so this is work with Christina Leslie, who's at Memorial Sloan Kettering Cancer Center and Ian Melvin and Jason Weston, who are at NEC Research in New Jersey.",
                    "label": 0
                },
                {
                    "sent": "This first project is is about finding evolutionary relationships among proteins, and I want to start with an analogy which I'm sure some of you have heard before, which is.",
                    "label": 1
                },
                {
                    "sent": "Most of you are familiar with the black.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Best services offered by the NCBI.",
                    "label": 0
                },
                {
                    "sent": "This is just a screenshot of the interface.",
                    "label": 0
                },
                {
                    "sent": "The idea is that you have a query protein sequence which you put into this web interface.",
                    "label": 0
                },
                {
                    "sent": "You click, it goes and does a database search and returns a ranked list of proteins where the ranking reflects the probability of evolutionary relationship or the evolutionary distance from the query to each target.",
                    "label": 0
                },
                {
                    "sent": "In the ranked list.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can draw a pretty close analogue between that process and something like the Google search engine where you take a sequence of English or Slovenian text, put it into the database into the search engine and it gives you a ranked list of documents ranked by relevance at a conceptual level.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing that these two search engines share is there is an underlying network.",
                    "label": 0
                },
                {
                    "sent": "In the case of the Web, this is a picture of the part of the World Wide Web where nodes are computers are computers and edges are connections between them.",
                    "label": 0
                },
                {
                    "sent": "But you could think of it as a picture of the of the.",
                    "label": 0
                },
                {
                    "sent": "Web database that Google searches or as a picture of proteins where edges represent similarities among the proteins.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "If we think about the protein homology detection problem that the problem has a very long history dating back one of the most widely cited algorithms is the Smith Waterman algorithm, which is a dynamic programming algorithm just for comparing pairs of sequences, and this was followed by BLAST which was published in 1990 and is become the most widely cited paper, and all of the scientific literature, which is essentially just a heuristic approximation of the Smith Waterman algorithm.",
                    "label": 0
                },
                {
                    "sent": "In the 90s there were a number of different methods, probably best exemplified by PSI Blast for taking these pairwise comparisons and building iteratively building up models, profiles or profile hidden Markov models that allow you to search in sort of a larger area of the network space that I showed you in the previous slide.",
                    "label": 1
                },
                {
                    "sent": "We published in 2004 a slightly different style of solution to this problem.",
                    "label": 0
                },
                {
                    "sent": "An algorithm called rank prop, which was explicitly based on the page rank algorithm that's used by the Google search engine.",
                    "label": 0
                },
                {
                    "sent": "The idea being that you rather than building an explicit model of the query, you perform a diffusion operation over an explicit protein similarity network and the amount of activation that flows in this diffusion process from the query to any target.",
                    "label": 0
                },
                {
                    "sent": "Induces a ranking that gives you an idea of who is homologous to whom.",
                    "label": 0
                },
                {
                    "sent": "The next year, H Search was published, which I think is now one of the best search.",
                    "label": 0
                },
                {
                    "sent": "Search engines that's out there.",
                    "label": 0
                },
                {
                    "sent": "H search is more expensive because what it's doing is building models not just of the query, but of all of the targets in the database and then doing hidden Markov model versus hidden Markov model style comparisons.",
                    "label": 0
                },
                {
                    "sent": "What I want to talk to you about today is using.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A different style of search, called supervised semantic indexing.",
                    "label": 1
                },
                {
                    "sent": "The idea for this project grew out of work.",
                    "label": 0
                },
                {
                    "sent": "The citations here that Jason Weston and some of his colleagues did in the context of natural language processing, the idea was to take a big compendium of documents, in this case 1.8 million Wikipedia documents and try to learn to induce a ranking on these documents where the training set consists of pairs of documents.",
                    "label": 0
                },
                {
                    "sent": "And you're learning to rank by putting for any given target, you actually have one example that's linked, and one example that's not linked and you want to learn a ranking so that the linked ones tend to be closer to the target than the unlinked ones.",
                    "label": 0
                },
                {
                    "sent": "And this works by embedding all of the documents into a lower dimensional space, learning the structure of that space, and then you can induce a ranking just by doing a sort of a Euclidean distance in that low rank.",
                    "label": 0
                },
                {
                    "sent": "Embedding one of the keys to making this work is that there is a.",
                    "label": 1
                },
                {
                    "sent": "Highly scalable optimizer that allows you to do the learning even when you have a data set of almost 2 million documents.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to try this idea in the context of proteins.",
                    "label": 0
                },
                {
                    "sent": "So the key idea is to.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do something similar to what I just described, but with proteins we're going to take a very large database of proteins and we're going to embed it into a low dimensional space where by low dimensional I mean about 200 dimensions.",
                    "label": 0
                },
                {
                    "sent": "Is what we've been using.",
                    "label": 0
                },
                {
                    "sent": "And we're going to learn the structure of this space so that if two proteins are homologous, they tend to be close to one another and vice versa.",
                    "label": 0
                },
                {
                    "sent": "After you've done this, learning which is relatively expensive, the actual classification querying is quite cheap because all you're going to do is compute the distances in this embedded space.",
                    "label": 0
                },
                {
                    "sent": "So to make this work, we need 3 different things.",
                    "label": 1
                },
                {
                    "sent": "I'm going to tell you about each one in turn.",
                    "label": 0
                },
                {
                    "sent": "First, how we represent the proteins initially, so an initial feature representation, what how we train this thing?",
                    "label": 0
                },
                {
                    "sent": "How we learn this embedding and then some signal to learn from an algorithm that can learn from that signal.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the initial embedding for the higher dimensional space that we start from, we use essentially the same protein similarity network that we used in the rank prop algorithm.",
                    "label": 1
                },
                {
                    "sent": "So the idea is you take some database, usually a precomputed database of all versus all this can be a PSI blast similarity network or an H search similarity network.",
                    "label": 0
                },
                {
                    "sent": "Take all of the expectation values.",
                    "label": 0
                },
                {
                    "sent": "The statistical values that come out convert them into sort of arbitrary weights using this exponential.",
                    "label": 1
                },
                {
                    "sent": "Transfer function and then normalize those weights so that you essentially get something like a stochastic connectivity matrix.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as in the context of the rank prop algorithm.",
                    "label": 0
                },
                {
                    "sent": "You could think of it as sort of a random surfer along the protein similarity network, and the weights give you your probability of taking any outgoing edge from the node that you're at currently.",
                    "label": 0
                },
                {
                    "sent": "So mathematically, the representation looks like this.",
                    "label": 0
                },
                {
                    "sent": "You have some.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very protein, P prime and you're going to embed this into this feature representation where the dimensionality of the feature space, which is the script L. There is just the number of proteins in the database that you're comparing against.",
                    "label": 1
                },
                {
                    "sent": "So a key key property of this representation is that it's sparse, because of course if you take one query, most of the things in the database have no recognizable similarity, so they're very values are undefined, so they get.",
                    "label": 0
                },
                {
                    "sent": "Sign them up.",
                    "label": 0
                },
                {
                    "sent": "Probability of zero in this in this representation.",
                    "label": 0
                },
                {
                    "sent": "So we're using either side Blaster.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "HH search as the representation, but we also use it in a second way which is as a training signal, so we're going to essentially take pairs of proteins and we're going to call each pair either positive or negative based on whether that pair is assigned a smallie value by the training signal.",
                    "label": 1
                },
                {
                    "sent": "So in this case we use it.",
                    "label": 1
                },
                {
                    "sent": "Any value of .01 and essentially the training procedure consists of picking pairs of examples and saying yes or no.",
                    "label": 0
                },
                {
                    "sent": "Does the training signal the algorithm that we're learning from say that these are similar to each other or not?",
                    "label": 0
                },
                {
                    "sent": "So our goal, given those pairs, is to learn.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some embedding, so you have this W matrix and then by L matrix which is going to give you an end dimensional embedding.",
                    "label": 1
                },
                {
                    "sent": "So you're projecting from this higher L dimensional space down into an end dimensional space, and we're going to rank the database using this function function here, which is just the one norm of the embedded representations of the query and some protein from the database that you're interested in.",
                    "label": 1
                },
                {
                    "sent": "So our goal is going to be to learn W. This is the only thing that we're learning.",
                    "label": 0
                },
                {
                    "sent": "We're trying to.",
                    "label": 0
                },
                {
                    "sent": "People this property holds.",
                    "label": 0
                },
                {
                    "sent": "In other words, if we have a positive and a negative to positive is more highly ranked than the negative example.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So graphically you can think of it like this.",
                    "label": 0
                },
                {
                    "sent": "Here's a query protein.",
                    "label": 0
                },
                {
                    "sent": "This is a good example where the positive is close in the embedded space and the negative as far.",
                    "label": 0
                },
                {
                    "sent": "And here's a bad example over here, and one of the keys to making this work is that we use a margin based training procedure, so we want to enforce that the negative examples are farther away from the query than the positives by some margin, and so that can be presented this way where one is the size of the margin and these are the differences that you're interested in so.",
                    "label": 1
                },
                {
                    "sent": "If it obeys this rule, if the query the positive is closer than the negative, there's no problem and you just have a zero loss.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you have this loss associated with the difference between the two.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that this is the same function I just showed you, it's.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lative Lee straightforward.",
                    "label": 0
                },
                {
                    "sent": "Given pairs two different pairs of positive and negative pair to update the the parameters of your W matrix in such a way that you sort of push.",
                    "label": 0
                },
                {
                    "sent": "There's four different operations.",
                    "label": 0
                },
                {
                    "sent": "There are sort of two complementary pairs you're pushing the query away from the negative.",
                    "label": 0
                },
                {
                    "sent": "You're pushing the negative toward the query, and then similarly for the positives.",
                    "label": 0
                },
                {
                    "sent": "And this Lambda.",
                    "label": 0
                },
                {
                    "sent": "Here is a hyperparameter that just specifies the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Great.",
                    "label": 0
                },
                {
                    "sent": "So we're testing this in the context.",
                    "label": 0
                },
                {
                    "sent": "We've tried several different settings, but I'm going to tell you about is this remote homology detection task.",
                    "label": 0
                },
                {
                    "sent": "So this is a standard setting for this problem.",
                    "label": 0
                },
                {
                    "sent": "The idea is that if you have protein structure that tells you a lot more about whether to proteins are evolutionarily related, so we can use the structural information to provide a gold standard to evaluate against.",
                    "label": 0
                },
                {
                    "sent": "So we use the SCOP database, the structural classification of proteins, and we do super family recognition.",
                    "label": 0
                },
                {
                    "sent": "The idea being that we're actually interested in homologs, and so we're going to say members of this class are considered positive examples.",
                    "label": 0
                },
                {
                    "sent": "Things that are in the same fold, but different super families may actually be homologous to each other, so we don't consider those as positives or negatives, but things that are in other folds are considered negative examples, and so we're going to then do a semi supervised setting where the feature vectors are drawn from the entire database.",
                    "label": 0
                },
                {
                    "sent": "But we only have labels.",
                    "label": 0
                },
                {
                    "sent": "Instead of those and, we're going to use performance metrics being the area under the RC curve up to either the 1st or the 50th.",
                    "label": 1
                },
                {
                    "sent": "False positive, averaged over different queries.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are results averaged over 100 different queries.",
                    "label": 1
                },
                {
                    "sent": "We've been careful to make sure that the the labels for the for that queries class are not used in the training procedure, so you have to retrain for each one of these queries.",
                    "label": 0
                },
                {
                    "sent": "But essentially what you can see is the rock one and the Rock 50 performance for various methods.",
                    "label": 0
                },
                {
                    "sent": "So PSI blast rank prop the proton bed.",
                    "label": 0
                },
                {
                    "sent": "That's the name of the algorithm that we've come up with.",
                    "label": 0
                },
                {
                    "sent": "The proton bad version of PSI Blast and then H predan approach.",
                    "label": 0
                },
                {
                    "sent": "Bed version of HH Pred and what you can see is that in each case the the learning this embedding across all of the queries improves over just using the original training signal alone.",
                    "label": 0
                },
                {
                    "sent": "Another nice feature of.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is that you can.",
                    "label": 0
                },
                {
                    "sent": "You can use this for visualization.",
                    "label": 0
                },
                {
                    "sent": "I know this is hard to see, but conceptually if you do this down into a 2 dimensional space you can actually look and see where the proteins are.",
                    "label": 0
                },
                {
                    "sent": "So for example, here's just a zoom in on one section and the colors represent different Scott.",
                    "label": 0
                },
                {
                    "sent": "I think they're super families actually.",
                    "label": 0
                },
                {
                    "sent": "And then you can see that there are clumping together based on what this categories are.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we wanted to take this one step further, because of course the fact that structure gives you more information about homology implies that you might want to use that information during the training procedure.",
                    "label": 0
                },
                {
                    "sent": "So the nice thing about this setup is that we can use structural information during training to learn a good embedding even when structure is not necessarily available during the query time.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that we're going to come up with additional constraints.",
                    "label": 0
                },
                {
                    "sent": "On our proteins, based on structural information and use those during the learning phase so that we get a good embedding and then we won't require structural information during classification.",
                    "label": 1
                },
                {
                    "sent": "So to do that we're going to multitask learning and essentially learn the embedding both from the sequence information and from the structure information.",
                    "label": 0
                },
                {
                    "sent": "So there's a couple different ways that you can image.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Engine representing structural information.",
                    "label": 0
                },
                {
                    "sent": "One simple way is if you have, for example this cop hierarchy, you could introduce what we've done is introduce sort of pseudo proteins representing the centroid of each Scott category, whether it's a class or a family or a super family, and we want to enforce something so that examples that are members of that class, proteins that are members of that class are closer to their centroid then proteins that are not members of that class and that gives you.",
                    "label": 0
                },
                {
                    "sent": "Rank based constraints that you can just add into the training procedure.",
                    "label": 0
                },
                {
                    "sent": "Sort of interleaving those with the sequence based constraints.",
                    "label": 0
                },
                {
                    "sent": "Similarly, if you don't have a human curated database, you can use as a training signal, something analogous to side Blaster HH Pride, but that actually uses the structural information, so you could do this with any structural.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Molarity metric we used.",
                    "label": 0
                },
                {
                    "sent": "We chose one called Mammoth, but essentially the form of the constraints is the same.",
                    "label": 0
                },
                {
                    "sent": "But now these constraints are coming from the structural information.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what you can see here in red are the versions of this algorithm that take into account structure during the training.",
                    "label": 0
                },
                {
                    "sent": "But don't use structure during the classification.",
                    "label": 0
                },
                {
                    "sent": "And again you see significant improvements relative to the methods that were only trained sequence alone.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another way of looking at this same data just to give you a qualitative sense for the difference is this is a plot that's showing the arosi one metric on this axis.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the number of queries out of 100 that achieve that score or better.",
                    "label": 0
                },
                {
                    "sent": "And so you can see the different algorithms there are actually two different of the SCOP.",
                    "label": 0
                },
                {
                    "sent": "Using either constraint based on classes like I described, or constraints based on more similar to the mammoth style constraints that I described and what you can see is that all all three of those structural based ones are above the other curves, and you get a similar kind of phenomenon if you look at the RFC 50 scores.",
                    "label": 0
                },
                {
                    "sent": "And then finally one thing that we just figured out yester.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I actually was that.",
                    "label": 0
                },
                {
                    "sent": "We'd struggle along a lot with the rank prop algorithm in the sense that the scores that it returned were not well calibrated from 11 query to the next, and so we had to have a separate learning phase that would then map those scores on to something that was interpretable from one query to the next and a nice side effect of learning a single embedding for all proteins is that it turns out that the scores from this new algorithm program bed are actually well calibrated, so we haven't yet figured out a way to.",
                    "label": 0
                },
                {
                    "sent": "To assign a semantics to them, although I think there are straightforward ways to do that, so we can't say what the value is.",
                    "label": 0
                },
                {
                    "sent": "Yet at this point, but at least we can say that a score of .75 from 1 query corresponds to a .75 from a different query, and that's shown by taking all of your results from all 100 queries, sorting them into one single list, and making our OC curve.",
                    "label": 0
                },
                {
                    "sent": "And this shows that our OC curve for the different methods and the improvement is still good in that in that sort of calibration.",
                    "label": 0
                },
                {
                    "sent": "Setting.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's the first project.",
                    "label": 0
                },
                {
                    "sent": "The key idea here again, was to use this semi supervised semi supervised or supervised semantic indexing that bootstraps from unlabeled data and a training signal and that allows you to incorporate structural information via multitask learning.",
                    "label": 0
                },
                {
                    "sent": "For the other two projects that I want to talk about.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're shifting gears to talk about mass spectrometry data and this may be less familiar terrain to some of you.",
                    "label": 0
                },
                {
                    "sent": "I should say the graduate student that did most of this work is Oliver Serang and Mike MCAS is a mass spec trauma tryst in my Department.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Help us with the mass spectrometry aspects, so in case you're not familiar with this problem, let me try to give you a little background of what the task is that we're trying to solve.",
                    "label": 0
                },
                {
                    "sent": "So the goal of shotgun proteomics or tandem mass spectrometry is to take a complex biological sample an characterize the proteins that exist in that sample.",
                    "label": 0
                },
                {
                    "sent": "So in its simplest form you just want to identify them, ideally eventually would like to be able to quantify each of those proteins, but for now we're focusing just on figuring out what's there.",
                    "label": 0
                },
                {
                    "sent": "Drop or some saliva or what have you.",
                    "label": 0
                },
                {
                    "sent": "And so the way that the device works is there's a preprocessing step where the proteins are digested into peptides.",
                    "label": 0
                },
                {
                    "sent": "On those peptides are then subjected to tandem tandem mass spectrometry and that gives rise to these observations.",
                    "label": 0
                },
                {
                    "sent": "These Spectra, which I'll describe in a little more detail later, but for the purposes of this problem we assume that there is some database search algorithm that gives you an initial noisy mapping from the Spectra.",
                    "label": 0
                },
                {
                    "sent": "Two possible peptides that might have produced them, so the idea is that there was one homogeneous population of peptides that is responsible for producing each spectrum, but it's difficult to figure out what that what the identity of that peptide was.",
                    "label": 0
                },
                {
                    "sent": "So you have scores that represent how well each spectrum matches each peptide, and typically you take the best matching peptide for each spectrum.",
                    "label": 0
                },
                {
                    "sent": "And then there's also usually a procedure where you take the best matching spectrum for each peptide.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the standard setting of this problem, you sort of collapsed the right hand side of that tripartite graph down into these things, which I'll refer to as PSMS or peptide spectrum matches, and each one has a probability associated with it saying how good is this match for this particular peptide?",
                    "label": 0
                },
                {
                    "sent": "These edges represent whether this peptide occurs in this protein, so of course it's a many to many mapping.",
                    "label": 0
                },
                {
                    "sent": "'cause of peptide can be in more than one protein.",
                    "label": 0
                },
                {
                    "sent": "And of course each protein has many peptides in it, but our task is to take these probabilities, take into account the structure of this bipartite graph and induce a ranking on proteins, ideally with probability associated with each one, saying how confident are we that this particular protein was in the sample?",
                    "label": 1
                },
                {
                    "sent": "So there are several existing methods.",
                    "label": 0
                },
                {
                    "sent": "I think the 1st first systematic treatment of this problem was with protein profit paper which was published in 2003.",
                    "label": 0
                },
                {
                    "sent": "This used sort of heuristic M like algorithm to learn a set of weights on that bipartite graph and a set of protein probabilities or pseudo probabilities.",
                    "label": 0
                },
                {
                    "sent": "And this is now I think the most widely used tool for solving this particular problem.",
                    "label": 0
                },
                {
                    "sent": "We also in this work compare against an algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "LMS Bayes, which was published in Recom last year, which uses a more fully probabilistic model, but it has lots of parameters and requires a sampling procedure to get the posterior estimates.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our question was going into this as a starting place for doing more sophisticated modeling later.",
                    "label": 0
                },
                {
                    "sent": "Could we come up with the simplest model that we could and still try to get exact posteriors?",
                    "label": 0
                },
                {
                    "sent": "Could we do marginalization with respect to a simple model just by doing some graph manipulations on that bipartite graph?",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And rather than having hundreds of parameters, we decided to boil it down to three parameters.",
                    "label": 0
                },
                {
                    "sent": "The model that we have essentially assumes that you have some probability Alpha that if a protein is in the sample, it's going to produce any given peptide within it.",
                    "label": 0
                },
                {
                    "sent": "Some noise model characterized by a parameter beta, that sometimes you'll see a peptide even though the protein isn't in the sample, and then a prior probability gamma on proteins that are present in the sample and for the prior.",
                    "label": 1
                },
                {
                    "sent": "We just use a uniform prior, so there's really just two free parameters that we have to.",
                    "label": 0
                },
                {
                    "sent": "Learn for this saw this model.",
                    "label": 0
                },
                {
                    "sent": "To make this simple of a model.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have to make quite a few assumptions.",
                    "label": 0
                },
                {
                    "sent": "Most of them are relatively.",
                    "label": 0
                },
                {
                    "sent": "Non controversial, I tried to summarize them all here.",
                    "label": 0
                },
                {
                    "sent": "The idea is there's a list of assumptions here.",
                    "label": 0
                },
                {
                    "sent": "This is a small tripartite graph and each of the little numbers here corresponds to one of the listed assumptions.",
                    "label": 0
                },
                {
                    "sent": "So for example, you have a conditional independence of peptides given proteins, and that's represented by this link here, saying that if you know this information that you don't have to include a directed edge in the probability model between two different peptides, and similarly there's a conditional independence of Spectra given peptides.",
                    "label": 1
                },
                {
                    "sent": "So that's shown over here.",
                    "label": 0
                },
                {
                    "sent": "The the different parameters are represented by 3, four and five.",
                    "label": 0
                },
                {
                    "sent": "The assumptions 3, four and five of peptides being produced either by proteins or by the noise model.",
                    "label": 0
                },
                {
                    "sent": "And then you also have that the proteins themselves are independent of each other and then the dependence.",
                    "label": 0
                },
                {
                    "sent": "The idea that if you know something about these peptides you don't have direct dependencies between the proteins and the Spectra.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm not going to go over the details of how you write this out, but here is a one equation that sort of summarizes the model rather than try to explain all of the nomenclature, the main thing to focus on is there's a big summation and multiplication there that are that are expensive.",
                    "label": 0
                },
                {
                    "sent": "Essentially, if you really wanted to compute this directly, it requires marginalizing over a very large set.",
                    "label": 0
                },
                {
                    "sent": "You can think of it conceptually as if you have, you know.",
                    "label": 0
                },
                {
                    "sent": "6000 proteins in your database?",
                    "label": 0
                },
                {
                    "sent": "You have to enumerate the powerset of all possible subsets of that 6000, which is a big number to say the least.",
                    "label": 1
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Oliver sat looking at this problem for awhile and came up with several different ways to try to make it tractable or approachable computationally.",
                    "label": 0
                },
                {
                    "sent": "The first one is relatively straightforward to notice that the bipartite graph that we're given is actually consists of a bunch of connected components are unconnected components and so you can divide.",
                    "label": 0
                },
                {
                    "sent": "You can partition the bipartite graph into the connected components and then say, well, if we draw an inference about these things because of our independence assumptions, we.",
                    "label": 0
                },
                {
                    "sent": "We don't have to worry about any of these, so it breaks down into.",
                    "label": 0
                },
                {
                    "sent": "In this case, 3 smaller subproblems, which that by itself gives you a big big improvement because the main thing you have to worry about then is whichever one of these is the largest connected.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Covenant.",
                    "label": 0
                },
                {
                    "sent": "The next step is to do a clustering.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that we're going to collapse proteins that have the same connectivity into a sort of a super protein.",
                    "label": 1
                },
                {
                    "sent": "So if you look at protein one and protein 2, they're both connected to both of these peptides, and so you can think of it as well.",
                    "label": 0
                },
                {
                    "sent": "If in this model you have sort of four possibilities, there is either they're both.",
                    "label": 0
                },
                {
                    "sent": "There they are neither there only protein one is there only protein, two is there, and the key observation is that those last two are.",
                    "label": 0
                },
                {
                    "sent": "Indistinguishable from the models perspective, so you'd like to say rather than having four probable possibilities, there's just.",
                    "label": 0
                },
                {
                    "sent": "You can either have one protein there, two proteins.",
                    "label": 0
                },
                {
                    "sent": "There are no proteins there at all, and so this supernode here has essentially end different states rather than two to the end different states.",
                    "label": 0
                },
                {
                    "sent": "After you've done this collapsing, and you can prove that you haven't changed the problem at all, that it's exactly the same solution.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then finally one of the one of the other features of these datasets is that a lot of the probabilities assigned by by the precursor software that we call it.",
                    "label": 0
                },
                {
                    "sent": "There's a piece of software called peptide profit that actually produces the probabilities.",
                    "label": 0
                },
                {
                    "sent": "Alot of those probabilities are zero.",
                    "label": 0
                },
                {
                    "sent": "Saying there is a match here, but it's so bad that we don't really believe that this is a strong match, and so the observation is that if you have something like this where the probability is 0.",
                    "label": 0
                },
                {
                    "sent": "It doesn't hurt you to just split it into two separate.",
                    "label": 0
                },
                {
                    "sent": "You've sensually doubled this node, and that allows you to break this bigger component into two smaller components, and again, you can prove that you can get exactly the same answer as you would have gotten if you'd enumerated that entire component.",
                    "label": 0
                },
                {
                    "sent": "Now, in practice, sometimes after those three optimizations, it's still not computationally feasible to do the marginalization.",
                    "label": 0
                },
                {
                    "sent": "So in practice, what we have to do is sometimes we get a component where we have we have to take some of the probabilities that are small and force them to zero in order to do more pruning to make it computationally feasible.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But even for the for the datasets that we looked at, so there's four different datasets that we examined in this study from H. Influenzae from a yeast whole cell lysate and then from 2 prepared samples of purified proteins.",
                    "label": 0
                },
                {
                    "sent": "First one has 18 known proteins in it, and the other one has 49 known proteins in it.",
                    "label": 0
                },
                {
                    "sent": "And what we're showing on the top is the size of the problem, so the number of nodes on each side of the bipartite graph, and the number of edges, and on the bottom is the size of the problem.",
                    "label": 0
                },
                {
                    "sent": "This is the log base two of the size of the problem, so.",
                    "label": 1
                },
                {
                    "sent": "Two to the 33,000 is a pretty big number for the initial problem, but of course there's a lot of of separate components, and so as you apply each of these three, you get down to two to the 60th, two to the 47, and so on.",
                    "label": 0
                },
                {
                    "sent": "So it makes this much more feasible.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, in terms of performance, the model that we're using, we expected to be maybe do as well as some of the existing methods, but it's such a simple model that we didn't expect it to actually do better, and in some cases it actually seems to be doing a little bit better.",
                    "label": 0
                },
                {
                    "sent": "So especially on like this smaller data set here we have the green line.",
                    "label": 0
                },
                {
                    "sent": "Here is the model that we've produced in the Red line is protein profit.",
                    "label": 0
                },
                {
                    "sent": "The diagonals here correspond to ties essentially.",
                    "label": 0
                },
                {
                    "sent": "Protein profit gives a lot of things probability of 1.0 and this is just showing number of false positives versus number of true positives.",
                    "label": 1
                },
                {
                    "sent": "So sort of an unnormalized arosi curve in some of these cases the all the different variants, the two different variants essentially agree with each other completely, with some minor differences.",
                    "label": 0
                },
                {
                    "sent": "This is the only data set where we had results from this, this other Bayesian sampling algorithm and they seem to agree fairly well, but in the crucial part we get a little bit of improvement.",
                    "label": 0
                },
                {
                    "sent": "Up there in the corner.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's also, of course, there's a learning procedure.",
                    "label": 0
                },
                {
                    "sent": "We essentially do cross validation to select those two parameters, and we wanted to show that it doesn't really matter that much.",
                    "label": 0
                },
                {
                    "sent": "Those two parameters generalize pretty well from one data set to another, so this is just showing if we take parameters that we learn on the first data set.",
                    "label": 0
                },
                {
                    "sent": "Second, data set is actually a bunch of different labs have taken the same physical sample and analyze it using a variety of different mass spectrometry platforms in different experimental settings.",
                    "label": 0
                },
                {
                    "sent": "So there's actually, you know, I think, on the order of 100 different.",
                    "label": 0
                },
                {
                    "sent": "Datasets that comprise that second, the ISP 18 data set and what you can see is that if you compare the area under the RC curve up to the 50th false positive from protein profit versus our method, we're almost always doing better even though the parameters were learned from a completely different data set.",
                    "label": 1
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this was a good start.",
                    "label": 0
                },
                {
                    "sent": "It seems like we've introduced the ability to actually compute protein posteriors exactly with a simple model and what we're working on now is to try to learn a more sophisticated protein specific priors, and actually more specifically to learn sort of a variant of that peptide emission probabilities so that you can say, given this peptide sequence, I'm more likely to observe it and this other peptide sequence is less likely to be observed.",
                    "label": 0
                },
                {
                    "sent": "So that's what will next time you hear me talk.",
                    "label": 0
                },
                {
                    "sent": "That's what I'll be talking about.",
                    "label": 0
                },
                {
                    "sent": "Huh?",
                    "label": 0
                },
                {
                    "sent": "OK, so then the third project that I.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wanted to tell you about is actually solving the same problem, but in a quite different kind of approach, so the previous one we were using a very simple generative, fully probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "This one again, it's a collaboration with Mike Makas, Jason Weston, and Marina who is a graduate student in why you and also works at NEC Research with Jason this is more.",
                    "label": 0
                },
                {
                    "sent": "This solution is more in the style of the first problem that I told you about where rather than build up an explicit model where essentially going to try to design.",
                    "label": 0
                },
                {
                    "sent": "A learning algorithm that directly maximizes the thing that we're interested in maximizing.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the problem is the same where we've got a bunch of Spectra with identifications to peptides and then mapping on to proteins.",
                    "label": 0
                },
                {
                    "sent": "In this case, we're not going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to do a little bit of simplification, but.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not as much so.",
                    "label": 0
                },
                {
                    "sent": "The key idea is there's a couple of different key ideas.",
                    "label": 1
                },
                {
                    "sent": "So first of all, we're going to get a single probability per peptide spectrum match, and then do protein level inference.",
                    "label": 1
                },
                {
                    "sent": "So in other words.",
                    "label": 0
                },
                {
                    "sent": "Going to get.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probabilities for all of these, and we're not going to collapse this down to a bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "We will actually enforce that there's only one mapping per spectrum, so for instance, for instance, this .06 would be illuminated, but you could have multiple spectrum mapping to a single peptide.",
                    "label": 0
                },
                {
                    "sent": "And then what we're going to do is.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, I'm reading this backwards though.",
                    "label": 0
                },
                {
                    "sent": "What what people have done previously is to do that and then say.",
                    "label": 0
                },
                {
                    "sent": "We're going to take all of this information, solve it all, give us 11 number like like I did in the previous solution that characterizes sort of our belief that this peptide is present in the mixture and then just use that single number in a second processing stage to try to make inferences about the protein level and the idea here is twofold.",
                    "label": 0
                },
                {
                    "sent": "We want to first of all, not put it through this bottleneck, so we want to instead of having all everything get boiled down to a single probability.",
                    "label": 0
                },
                {
                    "sent": "To maintain a rich feature representation and then do a single joint inference to try to maximize the thing we're interested in, which is really protein level identification rates.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we did was, rather than using, here's one probability that tells us whether we believe this peptide is here.",
                    "label": 0
                },
                {
                    "sent": "We instead use a richer feature representation.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of jargon on this slide, but I'll just give you a couple examples so this cross correlation score is the main score that's used in the initial search.",
                    "label": 0
                },
                {
                    "sent": "So remember, each spectrum is initially searched against the database, and the thing that it's doing the searching with this is this cross correlation score, and then there's some other variant scores that have to do with.",
                    "label": 0
                },
                {
                    "sent": "How well what's the difference between the best and the second best score?",
                    "label": 0
                },
                {
                    "sent": "Or there's another simpler score that's supposed to be sort of complementary to the cross correlation?",
                    "label": 0
                },
                {
                    "sent": "But then we also use properties, not just of the quality of the match between the peptide in the spectrum, but also properties of the peptide itself.",
                    "label": 1
                },
                {
                    "sent": "So what's the observed mass of the peptide?",
                    "label": 0
                },
                {
                    "sent": "There's also the difference between the observed and theoretical, so when you observe a spectrum, the device actually says.",
                    "label": 0
                },
                {
                    "sent": "Here's a spectrum, and here is the mass of the thing that produced this.",
                    "label": 0
                },
                {
                    "sent": "So you can look at the difference between that.",
                    "label": 0
                },
                {
                    "sent": "And what you actually matched against to give you an idea of whether you're correct or not, and then other properties over here that have to do with whether or not, for example, the ends of the peptide have the expected amino acids.",
                    "label": 0
                },
                {
                    "sent": "So if you use trips and as the enzyme to cleave the proteins into peptides, you should have a K or an R at the end you know.",
                    "label": 0
                },
                {
                    "sent": "And if you don't, that's evidenced that maybe this isn't a good match.",
                    "label": 0
                },
                {
                    "sent": "And then also information about how much charge each peptide was associated with.",
                    "label": 0
                },
                {
                    "sent": "The spectrum was associated with.",
                    "label": 0
                },
                {
                    "sent": "So in order to take all of these 17 features and boil them all down into a single peptide level score, we use a two layer neural network.",
                    "label": 0
                },
                {
                    "sent": "We actually experimented with.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using a simpler representation, sort of like I did in the first thing, just using a simple linear embedding, but the two layer network gave us better performance, so you've got.",
                    "label": 0
                },
                {
                    "sent": "These are the 17 features, and in a neural network you have some transfer function down here they have weight stored on each of these edges there hidden nodes.",
                    "label": 0
                },
                {
                    "sent": "I drew 5 but I think we actually used three in this work.",
                    "label": 0
                },
                {
                    "sent": "We tried 3, four and five and by the time we got to five we saw evidence of overfitting.",
                    "label": 0
                },
                {
                    "sent": "So we just fixed it at three after that and then you get some output.",
                    "label": 0
                },
                {
                    "sent": "Which is essentially a score that tells you the quality of this peptide spectrum match.",
                    "label": 0
                },
                {
                    "sent": "Now, this wouldn't be that different from what a lot of other people have done in the context of trying to evaluate peptide level scores, but the key to making this.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Station different from previous things that we then embed the training of that neural network into a higher order structure that represents the entire problem jointly.",
                    "label": 0
                },
                {
                    "sent": "So here's the function that represents that neural network, and this you can think as scoring each one of the edges here from each spectrum to one peptide OK, and so now you've got three different possible scores for each of these different PSMS, and then you're going to say there's a score for the PEP side, which is the maximum of all the PSM scores that came into it.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine that maybe your initial neural network says S1 is the best match to E one, but then you in the next phase of learning you say Oh well, actually it changed and now I think S 2 is better.",
                    "label": 0
                },
                {
                    "sent": "So we we haven't collapsed this down into a single layer and then similarly you have a summation for each protein over all of the peptides that it contains an critically, there's a normalization here that depends not on the number of peptides that you observed, but the number of peptides that actually exist.",
                    "label": 0
                },
                {
                    "sent": "In the protein, so one of the things one of the drawbacks to existing methods is that things like protein profit will look at and say I saw four matches and four seems like a pretty good number, but it might.",
                    "label": 0
                },
                {
                    "sent": "It doesn't take into account the fact that maybe this is a very large protein which contains 100 theoretical peptides and four matches.",
                    "label": 0
                },
                {
                    "sent": "In that case is not as compelling information as four matches when your proteins only 57 amino acids long.",
                    "label": 0
                },
                {
                    "sent": "So we're going to do a stochastic gradient descent.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Training and the idea is we're going to essentially pick a random protein, and we've got a protein and we've got a label.",
                    "label": 1
                },
                {
                    "sent": "So the key to making this work is this idea which has been used before in mass spectrometry that you can introduce to your database so called decoy proteins or decoy peptides.",
                    "label": 0
                },
                {
                    "sent": "So imagine that you're searching the database of all proteins human.",
                    "label": 0
                },
                {
                    "sent": "You can then take each of those proteins and shuffle them and search against the target decoy database, and the idea is that if your algorithm pulls out one of those shuffled ones, you know for sure that that is an incorrect match, and so we treat the label target versus decoy.",
                    "label": 0
                },
                {
                    "sent": "We treat that as a label and then we can just compute this Fr, which is that the output of that entire three stage calculation that I showed you in the previous slide and just basically check whether or not we've got.",
                    "label": 0
                },
                {
                    "sent": "We've got a good score.",
                    "label": 0
                },
                {
                    "sent": "For the Fr.",
                    "label": 0
                },
                {
                    "sent": "So we're using this hinge loss function, so again, there's a margin here, and if your score is good enough, then you're fine.",
                    "label": 1
                },
                {
                    "sent": "And if it's bad, then there's some loss associated with it and you do a gradient update essentially to try to change the parameters of your neural network to try to minimize that hinge loss function.",
                    "label": 0
                },
                {
                    "sent": "So there's a couple of different ways that we can evaluate how well this is working, the simplest one.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is to use a target decoy evaluation, so these are different decoys.",
                    "label": 0
                },
                {
                    "sent": "Then we use during the actual training procedure, but the idea is again we have false positive proteins.",
                    "label": 0
                },
                {
                    "sent": "Here true positive proteins here, and we've compared it now against two other methods.",
                    "label": 0
                },
                {
                    "sent": "So Barrista is the name of the algorithm that I described before protein profit and ID Picker are two competing methods.",
                    "label": 0
                },
                {
                    "sent": "Protein profit is that heuristic.",
                    "label": 0
                },
                {
                    "sent": "M like procedure ID picker is a much more.",
                    "label": 0
                },
                {
                    "sent": "It's a sort of a parsimonious procedure.",
                    "label": 0
                },
                {
                    "sent": "You essentially say.",
                    "label": 0
                },
                {
                    "sent": "I'm going to choose the best match for each, for each peptide, and then try to find a set of smaller set of proteins that explains all of those matches.",
                    "label": 0
                },
                {
                    "sent": "An ID picture doesn't actually give you a full ranking, so we had to just run it with a couple of different thresholds.",
                    "label": 0
                },
                {
                    "sent": "That's why there's only dots for those, but you can see that protein profit and ID picker are quite consistent with each other, and that the barrista by doing this joint optimization is getting considerably more positive.",
                    "label": 0
                },
                {
                    "sent": "So for example, at a fixed false positive rate.",
                    "label": 0
                },
                {
                    "sent": "You know, rather than getting say for example about 1200 proteins, we're getting more on the order of 1500 proteins identified from the same sample, and for someone like Mike Mcas who spends a lot of time trying to identify samples, this is a very significant difference in the rate of identification.",
                    "label": 0
                },
                {
                    "sent": "So this is we did this on several different datasets, so this is the one I just showed you.",
                    "label": 0
                },
                {
                    "sent": "These are different cleavage agents, so the experiment was repeated but digesting the protein initially with a different type of enzyme and.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a different data set from the C. Elegans, and you get a similar kind of improvement in each case.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's also possible, although a little bit dicey or to try to use some other kind of experimental method as an evaluation.",
                    "label": 0
                },
                {
                    "sent": "So here we use.",
                    "label": 0
                },
                {
                    "sent": "There were I think, 3 different experimental methods that are complementary to mass spectrometry, where they had taken yeast cells in similar, although not the same conditions and identified proteins from those.",
                    "label": 0
                },
                {
                    "sent": "So we we took the intersection of those sets and said, well, anything that was identified in these published studies as being present.",
                    "label": 0
                },
                {
                    "sent": "In these yeast cells will call that a true positive and other ones.",
                    "label": 0
                },
                {
                    "sent": "We just list the total number of positive examples here.",
                    "label": 0
                },
                {
                    "sent": "And so again you see a difference between protein profit and ID picture.",
                    "label": 0
                },
                {
                    "sent": "On the one hand and barrista on the other.",
                    "label": 0
                },
                {
                    "sent": "The ranking is not as long for protein profit, simply 'cause it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It gives lots of things with probability of one and lots with probability 0.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, we can do this for all the different yeast datasets, and in each case we get an improvement.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we wanted to understand why Beresta was making better identifications, and so this is sort of a graphical depiction of what we think is going on.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit confusing, so let me try to explain it.",
                    "label": 0
                },
                {
                    "sent": "This is the length of each protein and then each row in this picture corresponds to one protein that was identified by one method and not the other at a fixed false positive rate.",
                    "label": 0
                },
                {
                    "sent": "I think we had 10.",
                    "label": 0
                },
                {
                    "sent": "We allowed ten false positives and the coloring is it might be hard to see on here, but.",
                    "label": 0
                },
                {
                    "sent": "A lot of the region in between is Gray, which means there was a peptide that was never observed, and then the other ones that have color have probabilities from zero to 1.",
                    "label": 0
                },
                {
                    "sent": "So a couple of important things to note.",
                    "label": 0
                },
                {
                    "sent": "First of all, these probabilities are not actually given to Barrista as part of its input.",
                    "label": 0
                },
                {
                    "sent": "It's getting that 17 dimensional feature vector, but just for comparison, we colored them all in the same scale and then also protein profit.",
                    "label": 0
                },
                {
                    "sent": "One of the things that it requires is that you threshold the probabilities, so actually anything below .05 is actually not being provided as input to protein profit, but we included it here just again for comparison.",
                    "label": 0
                },
                {
                    "sent": "So there's a couple things to notice.",
                    "label": 0
                },
                {
                    "sent": "The most striking of which is that.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of much shorter guys here.",
                    "label": 0
                },
                {
                    "sent": "In fact, there's four up in that top picture that were more than 2000.",
                    "label": 0
                },
                {
                    "sent": "I just truncated them to make the picture slightly more legible, and so on.",
                    "label": 0
                },
                {
                    "sent": "Average protein profit.",
                    "label": 0
                },
                {
                    "sent": "And this is something that Mike told us.",
                    "label": 0
                },
                {
                    "sent": "A priore protein profit has this tendency to have very large false positives because it doesn't successfully penalize for for having very long length.",
                    "label": 0
                },
                {
                    "sent": "It says, well, you've got some good matches and doesn't.",
                    "label": 0
                },
                {
                    "sent": "Doesn't take into account the fact that those are sort of weighted against the length of the protein.",
                    "label": 0
                },
                {
                    "sent": "Another thing that has come up a lot in the literature is that up in this region there's a lot of very short proteins that have just a few.",
                    "label": 0
                },
                {
                    "sent": "Identifications by by the initial search and in fact there's been a rule that's been in the literature for awhile, which is that in order to do a positive identification of a protein, you need at least two peptides to identify.",
                    "label": 0
                },
                {
                    "sent": "It is called the two peptide rule.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we went and looked at our results and said how many of these guys are actually what in the literature referred to as one hit wonders, that is, proteins that we identified on the basis of a single peptide.",
                    "label": 0
                },
                {
                    "sent": "If you use the .05 threshold that that protein profit uses, there are actually quite a few.",
                    "label": 0
                },
                {
                    "sent": "There's on the order of 100 that are one hit wonders from the perspective of protein profit, but a lot of those have many many week matches, so innocence baristas able to say well, you don't have any single peptide that has strong evidence, but you've got 22 out of 25 where we saw weak evidence in favor of them, and so it can integrate that and.",
                    "label": 0
                },
                {
                    "sent": "Still, make a positive call that were actually 9 cases where protein profit or sorry where barrista made a one hit wonder call on the basis of justice.",
                    "label": 0
                },
                {
                    "sent": "Single peptide identification.",
                    "label": 0
                },
                {
                    "sent": "This is one of them just to show you what we're looking at.",
                    "label": 0
                },
                {
                    "sent": "This is the mass to charge axis of the spectrum and this is just the intensity.",
                    "label": 0
                },
                {
                    "sent": "This is the actual peptide sequence that was identified and what we've done is labeled.",
                    "label": 0
                },
                {
                    "sent": "Each of these peaks according to which cleavage location it corresponds to.",
                    "label": 0
                },
                {
                    "sent": "So any particular cleavage location like.",
                    "label": 0
                },
                {
                    "sent": "After the E here gives you two ions, the B ion corresponding to VE, which is this red peak here and the Y on corresponding to all of these guys, which is going to be probably Y11Y12 down there.",
                    "label": 0
                },
                {
                    "sent": "The key observation here, I realized that most people haven't spent a lot of time gazing at Spectra, but for anybody who's looked at a lot of Spectra, they immediately say this is a very compelling identification.",
                    "label": 0
                },
                {
                    "sent": "The fact that you see all of these peptides, all of the ions that you expect there are no very tall peaks that aren't accounted for.",
                    "label": 0
                },
                {
                    "sent": "Most of the remaining peaks are quite small.",
                    "label": 0
                },
                {
                    "sent": "Seems quite clear that this peptide is actually present in the mixture, and Furthermore, this is the protein sequence down here.",
                    "label": 0
                },
                {
                    "sent": "It has this huge.",
                    "label": 0
                },
                {
                    "sent": "Peptide so these vertical bars represent the cleavage locations for trips and so some of them are.",
                    "label": 0
                },
                {
                    "sent": "The tryptic peptides are so small that they could never be observed in the Mass Effect there outside of the mass rate mass range and similarly this one is so large that it could never be observed, so this entire protein only has two observable peptides and one of them is observed with very strong probability.",
                    "label": 0
                },
                {
                    "sent": "So this is a case where I would say I believe this thing is here on the basis of justice, single identification because we're taking into account.",
                    "label": 0
                },
                {
                    "sent": "This additional information as well.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then finally, sort of.",
                    "label": 0
                },
                {
                    "sent": "As an aside, we also wanted to try doing a multi task version of this optimization so.",
                    "label": 0
                },
                {
                    "sent": "In a lot of protein identification, experiments are a lot of mass spectrometry experiments.",
                    "label": 0
                },
                {
                    "sent": "The goal is just to find out what proteins are there, but sometimes you are actually interested in, for example, different splice forms of a protein.",
                    "label": 0
                },
                {
                    "sent": "So you want to know things about which particular peptides have been observed as well.",
                    "label": 0
                },
                {
                    "sent": "So in those settings you could make the argument that just optimizing for protein level identification is not ideal.",
                    "label": 0
                },
                {
                    "sent": "You'd like to both identify proteins and identify peptides in the same optimization.",
                    "label": 0
                },
                {
                    "sent": "And so we show in the in this submitted paper that it's possible to essentially include terms both for maximizing protein identification, identification rates, and peptide identification rates, and this is just a comparison either at the peptide level or the protein level of protein profit, and then barrista when you're either doing 1 task, the other task, or both tasks together.",
                    "label": 0
                },
                {
                    "sent": "Now what you can see is that at the peptide level, multitasking improves relative to either single task optimization.",
                    "label": 1
                },
                {
                    "sent": "So a little bit surprising because you would think it would be intermediate between the two, but the fact is that if you use protein level information that helps you to do peptide level identification and the results at the protein level are sort of more in line with what you'd expect a priore that you can sort of say, well if we just optimize for proteins we do well, and if we just optimized for peptides, we do worsen.",
                    "label": 0
                },
                {
                    "sent": "If we optimize for both simultaneously.",
                    "label": 0
                },
                {
                    "sent": "We do somewhere in the middle of the two.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The key here and this is similar to the first problem that I talked about is that we've formulated this as a single direct optimization of the thing that the mass spectrometry actually wants to solve, which is find me in many proteins in this data set as possible, and we've tried to build into the algorithm some some smarts to take into account.",
                    "label": 0
                },
                {
                    "sent": "For example, example many weak matches or normalizing for the total number of peptides that are in the protein, and then we also showed that you can embed this in a multi task learning framework.",
                    "label": 1
                },
                {
                    "sent": "To do simultaneous joint.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Optimization.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Overall, these three different stories.",
                    "label": 0
                },
                {
                    "sent": "I think one of the there's a couple of different take home messages.",
                    "label": 0
                },
                {
                    "sent": "One of them is that sometimes you want a discriminative model.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you want a generative model.",
                    "label": 0
                },
                {
                    "sent": "It really depends on what the task is at hand and what your goals are.",
                    "label": 0
                },
                {
                    "sent": "But that in any case developing application specific algorithms that take into account what the problem that you really want to solve is often going to give you better results than just using an out of the box algorithm.",
                    "label": 1
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to thank you and also point out that in addition to the MSB, we organize each year in Vancouver, the MLC be, which is sort of a similar workshop, but it takes place.",
                    "label": 0
                },
                {
                    "sent": "It's affiliated with the Nips Machine Learning Workshop.",
                    "label": 0
                },
                {
                    "sent": "It's in Whistler, BC in December.",
                    "label": 0
                },
                {
                    "sent": "These rules and you can submit unpublished or recently published work, and there are six page abstracts due September 27th.",
                    "label": 1
                },
                {
                    "sent": "So I encourage you to submit there as well.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "I'll be happy to take questions if there are any.",
                    "label": 0
                },
                {
                    "sent": "Correctly, you can take in a mass spectrometer.",
                    "label": 0
                },
                {
                    "sent": "And then get out both.",
                    "label": 0
                },
                {
                    "sent": "Identification.",
                    "label": 0
                },
                {
                    "sent": "Well, so this is not multi orselli, so these are tandem mass Spectra, so we're taking in.",
                    "label": 0
                },
                {
                    "sent": "You know, maybe 10,000 fragmentation Spectra, each one corresponding to a single peptide and jointly optimizing over all of those.",
                    "label": 0
                },
                {
                    "sent": "Sorry if I didn't make that clear.",
                    "label": 0
                },
                {
                    "sent": "So in MALDI or seldi you are taking a sample and sort of you know I think there's a matrix and then there's a laser and you're essentially looking at.",
                    "label": 0
                },
                {
                    "sent": "You're getting one large spectrum that tells you a lot of information, whereas in the in this framework you're actually taking the proteins first, digesting them, doing a liquid chromatography, and then over the overtime, maybe half hour an hour.",
                    "label": 0
                },
                {
                    "sent": "The device is pumping out five or six Spectra per second.",
                    "label": 0
                },
                {
                    "sent": "And giving you what is hopefully for each spectrum the results of a homogeneous isolation of similar or identical peptides.",
                    "label": 0
                },
                {
                    "sent": "You get much better.",
                    "label": 0
                },
                {
                    "sent": "That's the I think that's right, yeah.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "How much is the?",
                    "label": 0
                },
                {
                    "sent": "Completely worth it isn't.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "I think they're complementary.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, I haven't worked directly with multi cell D data myself.",
                    "label": 0
                },
                {
                    "sent": "My impression is that it's more popular in the context of things like biomarker discovery as opposed to just characterizing a complex mixture so, but I really.",
                    "label": 0
                },
                {
                    "sent": "I'm beyond the edge of my expertise, so I start comparing and contrasting them.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So science correct also includes the expected sequences.",
                    "label": 0
                },
                {
                    "sent": "Which problem we talk about them.",
                    "label": 0
                },
                {
                    "sent": "That's right, so.",
                    "label": 0
                },
                {
                    "sent": "That's right, so there's two different variants of this task, so there are so called de Novo approaches where you only provide the Spectra, and you, say, find what are the the peptides that generated them.",
                    "label": 0
                },
                {
                    "sent": "In general, because your search space is so much larger, you that has much lower success rate.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, of course, if there are post translational modifications or proteins that have not been identified in this genome, the database search approach is obviously not ideal.",
                    "label": 0
                },
                {
                    "sent": "That's right, that's right.",
                    "label": 0
                },
                {
                    "sent": "Don't use.",
                    "label": 0
                },
                {
                    "sent": "Any inquiry information about what you?",
                    "label": 0
                },
                {
                    "sent": "You take our Easter.",
                    "label": 0
                },
                {
                    "sent": "Certainly going to be there.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, so we.",
                    "label": 0
                },
                {
                    "sent": "We don't, but you certainly could.",
                    "label": 0
                },
                {
                    "sent": "In fact, there was a paper lately.",
                    "label": 0
                },
                {
                    "sent": "I think it was in bioinformatics from Edmark ATS group where they do essentially that where they say we take all of these existing resources and we build a protein level prior and then we can put that prior in and say, well, we've got weak evidence that this protein is here, but we've seen it 47 times before, so that boosts our belief that it's there.",
                    "label": 0
                },
                {
                    "sent": "So that's a reasonable thing to do.",
                    "label": 0
                },
                {
                    "sent": "We just haven't done it in this setting.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's a lot of different ways that people generate decoy sequences, so I mean the simplest way you can just take the proteins and reverse them, although of course sometimes you have palindromes, you have to treat those specially, or you can just shuffle them.",
                    "label": 0
                },
                {
                    "sent": "Or you can generate them from Markov chain.",
                    "label": 0
                },
                {
                    "sent": "In this case, I think we just shuffled and then try it and then check and make sure that it actually got shuffled that we don't get the same peptide as a target Anna Decoy.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And about the algorithm in the first problem.",
                    "label": 0
                },
                {
                    "sent": "Because it relies on fair looks quite computationally expensive.",
                    "label": 0
                },
                {
                    "sent": "Can you tell something more about computational complexity?",
                    "label": 0
                },
                {
                    "sent": "I don't know that much about the computational complexity, other than that it's surprisingly cheap.",
                    "label": 0
                },
                {
                    "sent": "I don't know the the innards of how the optimization works, so I'm afraid I'll have to.",
                    "label": 0
                },
                {
                    "sent": "I can certainly give you a site for the LP version that talks about that in more detail, but it relies on the fact that this representation is sparse and these updates are individually quite cheap.",
                    "label": 0
                },
                {
                    "sent": "One of the keys is that you choose.",
                    "label": 0
                },
                {
                    "sent": "During training you choose pairs of positive and negative pairs, so you're essentially balancing the optimization.",
                    "label": 0
                },
                {
                    "sent": "So even though there are many, many more negative pairs, in general you're going to focus on saying, well, I'm going to try to.",
                    "label": 0
                },
                {
                    "sent": "I really don't care if this guy is 4 miles away or 10 miles away, he's just far and not interesting, so you the optimization proceeds more quickly because you're balancing the training set.",
                    "label": 0
                },
                {
                    "sent": "Well, So what you do is you're doing this on line training over the entire all possible pairs, but just randomly choosing with replacement each time.",
                    "label": 0
                },
                {
                    "sent": "So in principle, if you continue the training long enough, you'll eventually use all of them.",
                    "label": 0
                },
                {
                    "sent": "But there's no preselection of downsampling that happens.",
                    "label": 0
                },
                {
                    "sent": "Use only features.",
                    "label": 0
                },
                {
                    "sent": "Try to use features on the person's bag.",
                    "label": 0
                },
                {
                    "sent": "Oh, I see no.",
                    "label": 0
                },
                {
                    "sent": "The only features that we've used are the training, the feature representation that I showed you, which is those.",
                    "label": 0
                },
                {
                    "sent": "Y values provided either by PSI Blast or HH search or Mammoth or Scott.",
                    "label": 0
                },
                {
                    "sent": "You certainly could try to use additional information and there's no conceptual reason other than just the you have to somehow normalize them to be similar to one another.",
                    "label": 0
                },
                {
                    "sent": "But again, it's a non parametric method so you could imagine taking the feature representation I showed you and then appending a vector of expression values that were normalized in a similar fashion.",
                    "label": 0
                },
                {
                    "sent": "Victor let you get again sparse in some sense.",
                    "label": 0
                },
                {
                    "sent": "Or do you think that's a good point?",
                    "label": 0
                },
                {
                    "sent": "So that would make if you didn't, you would either have to somehow enforce sparsity on those additional features, or pay a big cost in computation.",
                    "label": 0
                },
                {
                    "sent": "And then of course, the other drawback would be to do a query.",
                    "label": 0
                },
                {
                    "sent": "Then you would require.",
                    "label": 0
                },
                {
                    "sent": "All those other features, probably a more reasonable way to do it, would be to introduce 1/3 task into the multitasking and say, OK, we've got this.",
                    "label": 0
                },
                {
                    "sent": "These were still going to use the sequence based features, but we use a training signal that says things that are similarly regulated should also be close in this embedded space.",
                    "label": 0
                },
                {
                    "sent": "A little bit too.",
                    "label": 0
                },
                {
                    "sent": "You constructed so these these are constructed based on prior knowledge about what peptides occur in which proteins, right?",
                    "label": 0
                },
                {
                    "sent": "So first you select a set of proteins that you expect somehow to be there.",
                    "label": 0
                },
                {
                    "sent": "All the ones that are so you have to have the genome and you have all the inferred Orfs from that genome.",
                    "label": 0
                },
                {
                    "sent": "And then you just exhaustively say find all the tryptic peptides and so it's a deterministic procedure to find that bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "Coco.",
                    "label": 0
                },
                {
                    "sent": "That procedure of very straightforward.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know there's as long as somebody hands you the set of proteins you just go through and find kayson ours and introduce the cleavage breaks.",
                    "label": 0
                },
                {
                    "sent": "And then just ask you know how many times did I see this peptide in different proteins so?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}