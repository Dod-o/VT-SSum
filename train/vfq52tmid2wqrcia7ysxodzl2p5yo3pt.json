{
    "id": "vfq52tmid2wqrcia7ysxodzl2p5yo3pt",
    "title": "Sparse modeling: some unifying theory and \u201ctopic-imaging\u201d",
    "info": {
        "author": [
            "Bin Yu, Department of Statistics, UC Berkeley"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science->Artificial Intelligence",
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/aistats2011_yu_modeling/",
    "segmentation": [
        [
            "Well, thank you very much for the invitation.",
            "It's my first time here, so can you hear me?"
        ],
        [
            "Doesn't stay here.",
            "So here I would like to report some recent work on sparse modeling and in particular, or first go over some unifying theory which I have done with a couple calls.",
            "I will tell you their names when the time comes and also I want to report on going project with also colleagues on topic Image Ng.",
            "So kind of bring sparse modeling to social science.",
            "First, I would like to dedicate this talk to my former colleague, the late professor David Blackwell, who passed away last year, and in particular, I'm like to remind ourselves this famous quote.",
            "David said many years ago.",
            "Basically, I'm not interested in doing research.",
            "I don't know how many of you can claim that, and I have never been an interesting understanding, which is a quite different thing.",
            "If you haven't heard about David Blackwell, encourage you just search."
        ],
        [
            "I'm on the web.",
            "And there are many other interviews and movies, and so I think you enjoy knowing him a bit.",
            "So statistics or machine learning or in general data driven science is increasing a great error now because we have this site evolution and we have lots and lots of data here.",
            "Just very very few areas I happen to have touched.",
            "Genomics, neuroscience, remote sensing and computer vision, but I think it would be interesting for all of us to pay more attention to the emerging field of computational social science and kind of natural language processing.",
            "Maybe sister, but definitely social networks when you try to understand human interactions organizaci.",
            "And tasks.",
            "And also what I will talk about is trying to use a machine learning methods, but go beyond that with human validation.",
            "With media analysis an we're tossing the term topic image Ng, but it's kind of not the best distinguishing from the topic modeling from LDA, so we may be switching to something less exciting, but document subject specific summarization so it's a mouthful, so we haven't settled on a good topic yet.",
            "All of these problems.",
            "Represent high dimensional data with large P and small.",
            "An answer, sample size and P is the number attributes with predictors.",
            "Yeah, because we depends on how you take the document unit.",
            "If you take articles depends also the document corpus.",
            "So if you take paragraphs is huge so you have a point that sometimes they're allowed by piece large so.",
            "But if you take document, sample size is not too large.",
            "Depends on the corpus you take.",
            "But and could be large.",
            "And here is where I idealized where how we see information flows.",
            "Right, you have investigating reporting because I'm doing media analysis, I'll start from there and that impacts.",
            "You know agents and actors and decision makers would impact the States and you have more reporting, so there's this feedback loop.",
            "An historical data has been."
        ],
        [
            "Spectrum of media reporting right you have on one end you have the hero from the Second World War.",
            "Who wants to be very very faithful to be persuasive.",
            "We must be believable to be believable.",
            "We must be credible to be credible.",
            "It must be truthful, right?",
            "This one and as Edward Murrow with word time Reporter and you have the other strength spectrum which Hurst.",
            "You supply the photographs and or supply the war so you basically can make up anything.",
            "I think the truth probably somewhere in the middle.",
            "Everybody kind of make up a bed and there's a whole spectrum, so we want to be on this and on the left hand."
        ],
        [
            "Put the photos in the right position.",
            "And so far the media analysis has been very very labor intensive, so you have a graduate student who are ready to corpus of documents and then she or he will write a report based on their rating.",
            "And also this person's particular experience and views will impact on how they're going to summarize certain coverage, say China.",
            "On the other hand, we have seen a great advance of statistical machine learning.",
            "Most of the time, I mean, we hope it's fast and scalable.",
            "And we hope it's kind of objective in the sense that when the method is specified, including preprocessing stage, then she reproducible.",
            "So doesn't who's running the algorithm?",
            "What we should get the same thing?",
            "And but it's very much designed so far around predictive task.",
            "Of course, sparse modeling is a step forward to move to interpretability.",
            "So our goal is to harness this power from machine learning to enhance media studies."
        ],
        [
            "And we're going to use the predictive and sparse framework as surrogates, so we want to predict it.",
            "But we also want to be sparse, so we now hitting the top is predicting performance as I will show you.",
            "However, we also want to validate because we are after summarization and meaning.",
            "We want to validate things through human experiments, not just the methods but also robust pre processing.",
            "So we basically want the method to be relative robust.",
            "Oh oh, sensible ways of normalizing's remove words so there's less obituary choices.",
            "So let's just take the data.",
            "I will show you all the results, show you will be based on the business section of New York.",
            "I'm in 2009."
        ],
        [
            "And the idea is that supposed to be interesting topic actually that the project has involved has evolved in the beginning.",
            "We're just using a query word now and moving into like topic so.",
            "Of connection thank you.",
            "Oops, now I cut it off.",
            "So we want to say China.",
            "We want to take.",
            "Use predictive framework to find a set of words or phrases.",
            "We begin with the token like bag of words and we moved to like bigrams and trigrams so it's kind of evolving.",
            "Which best distinguish China from the articles not having anything to do?",
            "I mean not mentioning China, I shouldn't claim that.",
            "Have nothing to do with China, just didn't mention the word China.",
            "And as I said, we want our model to be pretty good."
        ],
        [
            "Interpretable, which is a much much broader concept than sparsity, but right now we're using sparsity as a surrogate to leaders there, but we don't really.",
            "We cannot trust any sparse model necessarily integrable, so we.",
            "Miranda human experiment to validate what we find.",
            "So we want things to be fast because of huge documents, but database and we understand that our sparse model is not true right?",
            "We don't have any illusion that the sparse model is now generating how the documents are generated, but it's a useful approximation so all models are wrong by some models are useful in the spirit of George Box.",
            "I will call the selected set of phrases or words at the topic imaging of the topic of course relative to particular document, carpets, right?",
            "If you take a different corpus, say you take news from China, which we're doing now too.",
            "Is cross language.",
            "Then you might have very different impression, right?",
            "So suppose you take the coverage of the recent nuclear plant accident in Japan.",
            "In China, Friends came back.",
            "It's quite different from what we hear here.",
            "And we want the topic image to be valued at least in two ways.",
            "So we wanted to be predictive.",
            "So if prediction is very very low, that's now transferred to either.",
            "But we also understand that to make the prediction as as possible doesn't mean it's the most meaningful.",
            "And later we discovered that.",
            "So a certain point up to a certain point, like 70% predictability is very differentiating with meanings and then the meaning and predictability kind of decouples.",
            "So they don't go always together to the end, and we wanted the words to be meaningful and also robust to pre processing.",
            "So here just a run of."
        ],
        [
            "I see us like there's a lot of work to do.",
            "Data processing for most of the non modern data problems.",
            "And subject knowledge also very important.",
            "The earlier you bring that in, the better you off.",
            "Even we have a lot of good automatic methods.",
            "And here I'm not actually developing any new methodology in this talk.",
            "I do believe strongly that theory comes in.",
            "Necessarily for high dimensional problems, because it's very hard for us to understand things without.",
            "Sometimes the mental abstraction.",
            "So I see theory as a useful.",
            "Two to help us to do good work in practice.",
            "And validation, so we cannot just run away from models.",
            "We really have to know bring the problem back to the context and have some other outside statistics framework to validate what we see.",
            "So."
        ],
        [
            "Here's a road map for today's talk.",
            "I'll give very brief introduction, probably unnecessary for sparse modeling.",
            "I couldn't help keep bring back the 70s model selection saying that model selections process is not really just knew.",
            "Now it's around for a long time.",
            "And then I will cover some.",
            "I'll try to give you a geometric view of a recent work with.",
            "Anne Megabowl Ravikumar actually is here today and also Wainwright were right on Wednesday.",
            "And then I'll come back to the topic image Ng problem show you what we did.",
            "Mainly it's actually not approved pre processing and also human validation."
        ],
        [
            "So this is courtesy of Wikipedia is I don't know whether it's really described anything about all come but so nice photo to show who is.",
            "14th century English logician and Franciscan Friar.",
            "He is named well.",
            "We attribute this principle parsimony to him called outcomes.",
            "Razor entities must not be multiplied beyond necessity.",
            "And I gave this this slide a couple years ago in a workshop in Europe.",
            "Friend on Paul.",
            "Mentioned that actually you can trace this back to Greek writing, so I'm sure if you go further anytime when you start it's very simple.",
            "Reasonable criteria, so if I want to be a little mathematical and then give you some notation."
        ],
        [
            "I have a script XN tell me the end typos of my data XYZ appear dimensional vector which is a predictor.",
            "So in my.",
            "Topic image Ng problem.",
            "If I take China out or everything relate to China and then.",
            "The XI will contain either.",
            "Yeah, so now the other words exist or not, or some word counts.",
            "You can take different versions and why would be this document is an has contained word relate to China or not.",
            "Annina matrix notation.",
            "I can write a linear model as white to Exeter, Platycerium, and people realize that if you do maximum likelihood, you do these squares and the Gaussian assumption you end up with the largest model.",
            "If you want to select, you have two to the power P sub models for me depends on how much light do trigram."
        ],
        [
            "Diagram my pee is like in the millions.",
            "And you don't want to go for the largest model first.",
            "You cannot fit it.",
            "In the 70s, people already realized the problem.",
            "Actually, if you read features paper in 1921, he was already doing some model selection or he was already aware of the model selection problem by the formal Dragons and terminology didn't really came about until the 70s, and you have a key key.",
            "It's a Japanese statistician who started the Institute of Statistical Mathematics and Metal CP at Bell Labs.",
            "So the idea is that you actually try to estimate the prediction error using model.",
            "We nowadays use to use.",
            "To use a cross validation empirically estimated estimation error.",
            "By that time the data was not as abandoned, so people are using estimation of prediction error as the criteria to select the model.",
            "Naturally, this leads to a penalized maximum, likelihood or penalize these squares and AIC, which is a key key information criteria, although he will insist, is our information criteria that you penalized by two times the dimension of the model.",
            "Usually we were feeding.",
            "Audio only squares.",
            "I'm not dealing with the Sigma Scarping estimated, just make the presenter easier and a few years later Schwartz did abbasian version and he basically took the vision framework and did a Laplacian approximation to the posterior probabilities and discovered that to the 1st order the priorities and manner.",
            "This is very much related to the syntactic normality of Asian estimate under good conditions, so it's kind of the same analytical tools and the prize disappear because syntactically you data override your prior.",
            "And you end up penalized the dimension by log in, which depends on the sample size than two, which AIC, AIC.",
            "We know more or less is very closely related to cross validation and gives you.",
            "Good predictions MBC.",
            "When you have major predictors and less important ones, you have nice two groups, be icy, cuts it better and then there's always coding criteria which happened to work on many years ago which will coincide with the IC41 four man has many other connection with coding and also Bayesian statistics.",
            "So both if you use more."
        ],
        [
            "In terms we say that that criteria penalized with L0, which is because you're counting.",
            "If you go back to my account, my calculation of P is like in the millions.",
            "You will have two to the power P or 10 to the power 30,000.",
            "Um?",
            "Models.",
            "To choose from OK and for me, my end is probably thousands or 10s of thousands.",
            "Most of time will say that this convent or research is too expensive, which are definitely true to possibly run over this possibilities, but I want to make an argument that is also not necessary.",
            "Because for statistics, we're really not doing infinite precision optimization we have.",
            "Only Say 10s of thousands of observations.",
            "And the number of submodels is astronomical compared with the sample size.",
            "I don't think anybody in this room would believe that we actually have the power to tell these models apart.",
            "Basically this is something which we can take advantage of when the noise or the data is random.",
            "We really kind of using a surrogate right?",
            "If you go to the classical framework, we really want the Oracle.",
            "Likelihood, which will give you the true distribution because we true parameter because we don't, we use random surrogate or sampling surrogate and that surrogate wobbles right?",
            "Because depends on what sample you get.",
            "It's a random approximation and therefore we don't have to be very precise.",
            "We just have to be as precise as the data allows us and this kind of roughly says that there's no need to go through this sub models because we don't have the power to tell them apart.",
            "And lot of models basically.",
            "Equivalent from the point of the model, the data, because we cannot tell them apart.",
            "So if you do the convex relaxation of this L zero problem to go to lesu, you basically just cover the path in all this possible models.",
            "As long as your pass goes through this equivalent classes for predicted propers, you basically engage."
        ],
        [
            "And This is why lifestyle.",
            "I'm sure I already heard about Lesu is.",
            "Basically it's all one penalized least squares.",
            "You are have this counters if P is 2, the circle represents a counter line of the first term, which is the least squares.",
            "And then you have L1 panelization, which is equivalent to L1 constraint.",
            "And if you think about you have this diamond and you through like a nice smooth ellipsoid type of thing at this most like you end up hitting.",
            "The diamond at the corner and the color you have sparsity.",
            "It doesn't always happen that way, but most sometimes it does and nobody has really articulate exactly what you mean by, you know, most of the time.",
            "You will have to introduce some probability distribution.",
            "And again, this message now in most time."
        ],
        [
            "Attribute to the name came from TV show newspaper 1996.",
            "He calls the SU means at least absolute selection and shrinkage operator.",
            "There's two as might be permitted.",
            "They were already works.",
            "There was basis pursuit in signal processing by turn down the Hall and then there is this non negative garage by bryman.",
            "And.",
            "It's kind of very nice mathematic.",
            "Lee Ann.",
            "In practice, there's also in the beginning quadratic programming tool, so people really have been loving it.",
            "In the beginning, people using QP to solve it for each Lambda, because the quadratic programming problem.",
            "But soon people realize that well in mathematics people don't usually select Lambda, but when you select Lambda you have to calculate it for a bunch of lambdas you should really tie order solution together.",
            "So there was a second generation which is past following algorithms which work pretty well too, like 2 million predictors.",
            "And there's lots there.",
            "Couldn't buy anything at all, and there was an earlier work actually, not quite interstitial literature by offspring at all in 2000 and then recently.",
            "Somebody told me that there was even earlier paper 1957 in kind of mathematical finance by Mark, which he solved whole salute of quadratic programming problem use pass following.",
            "So I'm sure there would be earlier reference if we dig further.",
            "Right now, the four really, really large P. The gradient type of methods to the leading the 1st order method is still very much active risk."
        ],
        [
            "And there's a lot of active research on extending to various sparsity to being bringing prior information.",
            "I'll just add more regularity, even lawsuits, regularised alot of times on peace.",
            "Really large is not enough regularization.",
            "So I'm now going to the agreement and the sparse modeling so and also, of course there's a parallel field called sparse sensing, a compressed sensing, and there they emphasize more the design of X.",
            "So sparse modeling we tend to use it when we kind of passively observe X&Y, without worrying about how to collect taxes.",
            "But compressed sensing is signal processing engineering.",
            "There's more emphasis on, but how to choose X or randomize X, but the mathematics are very much the same.",
            "So for the last 10 or 15 years, statistical theoretical research have really reviewed that.",
            "Indiana One piece very large.",
            "If you don't have low dimensional structures, hopeless right?",
            "That's kind of lost generation.",
            "You go back to 2030 years.",
            "We have this recent convergence and people look at it.",
            "If your functions actually ambient so it's really impede dimension, you quickly run out.",
            "The samples are even for an equal to like 100.",
            "But Luckily I think lot of times I wouldn't say all the time nature does give us some structure to work with and we do have approximate low dimensional structure and therefore we pay like price for search in this high dimensional dimension, but with something not too bad.",
            "If the tales of the noise in the linear model discussion, but when the tails is not Gaussian you can see that things are a lot harder.",
            "So we have this log P kind of term.",
            "Which is very tight to the thin tail of Gaussian distribution.",
            "An sparse sparsity.",
            "It's really want the simplest low dimensional structure and linear model.",
            "Again is the simplest model, so it's very nice to analyze that because mathematically you can go far further and there's a lot of work analyzing the soup.",
            "You can use prediction error, you can use L2 error.",
            "You can also use model selection, consistency and most of the time we assume the.",
            "The true model is sparse, right?",
            "So it's kind of there's a nice match between the model and the Mansard.",
            "Recently we have done some work.",
            "Assume the noise is not really Gaussian, but pursue amazingly less.",
            "You actually also came up pretty nicely.",
            "We kind of motivated at proper medical image in where you cannot assume Gaussian ality, but amazingly well imagine signal to noise ratio.",
            "The SU actually turned out to be pretty good.",
            "So there's been lots of lots of lots of work taking while two or three criteria to analyze the.",
            "So would I want to present.",
            "Next is some recent will try to unify a lot of the works and hopefully bring more clarity into what's going on in high dimension.",
            "So this is a."
        ],
        [
            "First was the NIPS paper 209 and we submitted Journal paper.",
            "I think now we we invited to do one part of the paper to Institute science.",
            "So, um.",
            "Let me go to a bit abstraction so.",
            "This is a general problem.",
            "If you leave the squares, so my first blue expression L is a loss function could be the minus log of the log likelihood function from general idea squares like logistic regression.",
            "And R doesn't have to be.",
            "L1 could be some grouped lawsuit.",
            "OK, but nevertheless you have a goodness of fit term.",
            "The first term and you have a regularization term second term.",
            "And the goal is to see whether if there's a true model which generate a theater star.",
            "If City stars structure matches the regularizer, and then there's hope that we hope the estimator the difference will be small and the proper conditions so usually sit.",
            "A star is kind of sparse.",
            "Just think about a suit for this purpose, it's good enough, but I'll give you the general geometric interpretation of our result."
        ],
        [
            "OK, any question.",
            "So from now I will get a little heavy before was kind of just talking.",
            "OK, so notation wise it's OK.",
            "Here's a better view of the linear under sparse modeling, so there was the similarity between my slide and Martin slide, because we kind of.",
            "Then you have this why which is in dimension.",
            "And you have a rectangle X matrix that's kind of the feature of modern statistical problems piece larger than.",
            "And then you can permute all the theaters so that non 0 ones come up in the pink block and kind of blueish.",
            "Block is all the zeros.",
            "OK, so big S is the non 0 status.",
            "And the magenta is.",
            "The noise I want to emphasize again later, you're going to see something like log P appearing.",
            "There has a lot to do with.",
            "Like Omega being short tails of sub Gaussian.",
            "OK, so it's now like if I give you.",
            "Along like a heavy tailed distribution, things just automatic go through some.",
            "My son might not.",
            "So so far, so don't think so.",
            "High dimensional problems will be this nice search term log P. That's kind of very nice.",
            "It's because you think about if Omega is closely."
        ],
        [
            "We have a lot of outliers going to be mixed up with the nonzeros, either because you can have huge observation why and it's not coming from the Cedar stars coming from the noise and you can see the problem gets harder OK and I just listed you know lots of papers.",
            "If I'm not including work on my politics because there's no.",
            "So lots and lots of work."
        ],
        [
            "OK. And then there's another problem people have started, which is sparse graphical models for Gaussian likelihood, right?",
            "So if you have multivariate Gaussian, you have a nice nice correspondence between conditional independence, say between 1, four.",
            "And the inverse covariance matrix one full right?",
            "So my black actually means zero, my white means higher.",
            "OK, you see one and four.",
            "Those like smoothing is because presentation.",
            "This shouldn't be this like blurring.",
            "It should be just zero.",
            "Oh no 0.",
            "So then you can formulate estimation of the sparse graphical models through.",
            "Also at one panelization.",
            "Of course, this is only one particular one.",
            "Doesn't mean that we couldn't find better ones later, but that's what we're doing now and also work in there.",
            "So again, you have a blue form, which is the loss function fit, and then you have a red which is a penalty and one personality doesn't hold.",
            "The blue part can be interpreted as a Bregman divergences, so there's still some interpretation.",
            "You're matching the coherence even you lose the conditional independence.",
            "And the next one I'll just do a ad for Martin on Wednesday morning.",
            "Actually, it's not tomorrow.",
            "I'm sorry.",
            "It's Wednesday morning at 8:15 so he will talk more about that so I won't go into that."
        ],
        [
            "So about almost like a year and half ago, we're wondering that we did a couple examples ourselves and we see other people's works and we start wondering.",
            "Maybe there's something.",
            "Unifying then doing case by case.",
            "Right.",
            "Right now you take statistics.",
            "I mean if you go to the 50s and 40s and 30s people approving maximum likelihood, assembly morality, case by case and after a while there's a unifying framework which cover lots of cases.",
            "So that's what we wanted to do.",
            "We saw some similarity in doing linear squares, low rank and also.",
            "Graphic, sparse graphical model estimation and the question is, can we fish out something essential to the good performance of this L1 penalized estimators and make it?",
            "Actually we wanted to teach in graduate courses.",
            "So it turns out is such indeed the case.",
            "We cannot handle any regularization, was a little disappointment.",
            "Actually, we're hoping that will cover a broader class of regularizers, but it didn't happen, at least with the techniques we used.",
            "You need the regularizer 90 decompose relative to some subspaces, like the non 0100 ones.",
            "They have to separate otherwise, or some triangle inequality argument just wouldn't go through.",
            "And the second one is very classic in some sense.",
            "We know that we need some convexity for the loss function, right?",
            "If things are flat in loss function, there's no way you can cover the parameter, but to us, nice surprise, us already seen previous works.",
            "You don't need the strong convexity over the whole space because we don't have it right.",
            "P is larger than N, you won't have it, so we only need restricted strong convexity."
        ],
        [
            "So let's let me just go back to maximum likelihood estimation under the bed.",
            "Right, the reason so?",
            "If you take my.",
            "Solid curve as the Oracle.",
            "I could function basically the likely function with expectation always infinite sample size.",
            "Right and then you have truth.",
            "Well maybe the other way around.",
            "You have to imagine the article now because I have my minimize.",
            "That's it ahead.",
            "If it's either would be fine.",
            "But as I said, right?",
            "You have basically one work schedule, any kind of disturbance to this article loss function by Central Limit Theorem by concentration inequality, whatever you want to think, then you really don't have the the article you have random approximation and because the random browser is wildly occur of Delta L in terms of how close you are to the truth of the likehood function, when you do the minimization that introduce an error.",
            "In terms of estimation, right?",
            "You'll see the ancetre hat.",
            "While you."
        ],
        [
            "Oh, good curvature.",
            "Then you can pin it down precisely.",
            "The wobbly in the vertical direction translates pretty nicely, or even get reduced contracted too.",
            "The difference between the estimator and the truth, but you have a flat or low curvature like could function, then a little disturbance in the vertical scale get translate to a huge disturbance.",
            "So basically that's what the Fisher information tells us, is the curvature right?",
            "So hashing you have good hashing.",
            "Strong curvature you can estimate well and when you don't, you can't.",
            "OK and then.",
            "There's some target goal space.",
            "They use central limit theorem since I've had to kill the sampling noise an you see fish information.",
            "What happens in high dimension in high dimension?",
            "Let's think about this crash lawsuit.",
            "So how changes X prime X and if P is larger than N, your Hessian matrix is forever degenerate.",
            "You always want to have.",
            "Flight directions it doesn't matter.",
            "What you do, doesn't measure how wide your axis is.",
            "There's no escaping.",
            "So if you do this square, that's kind of indirectly says.",
            "Therefore, these squares you you have to do general inverse do have many many solutions because all the data points all the parameter values kind of lying on the flight direction will give you the same discourse and you don't know which one to choose.",
            "However, it turns out if you look at if we import look at item 2.",
            "If you look at the compass based the decomposable regularizers like L1.",
            "While there's a match of the sparsity in the true model and L1.",
            "If you do the SU this matching.",
            "And Decomposability will force the difference of your true estimator, your estimator and truth to a cone which I use green see.",
            "OK, and this is not the most general case, but it's good enough for the talk.",
            "But now what you need is that.",
            "You actually only need.",
            "The strong convexity on this cone.",
            "Right, because assuming the sparse is small, this cone relative the whole ambient space is a small set.",
            "And therefore.",
            "You OK and you can prove if your design matrix now to change degenerate.",
            "Then you can have this strong convexity on this small cone.",
            "And therefore the old old, like a classical astronautics, will go through on this call.",
            "Basically that's what's going on in high dimension.",
            "In this simple case.",
            "And, um.",
            "Going through this exercise really made us ourselves very clear what needed.",
            "Otherwise the deeper example use special properties.",
            "You kind of get it, but you don't see the common structure.",
            "So this is a mathematical result."
        ],
        [
            "Which.",
            "I think I will probably don't want to go into becoming instrumentation.",
            "Basically that's just writing out what I said in mathematical form.",
            "So the work for different problems is to prove the strong convexity.",
            "Over the cone for different problems and that can be challenging for general linear models.",
            "Actually we needed a more refined result than what I'm presenting below in terms of graphics.",
            "Actually we don't have a cone, we have something.",
            "In cruise at the origin and then more work needed so the work basics being pushed to proving the strongest strong convexity, which is like you have to do for maximum likely to you have all this regularity conditions on the different partial derivatives of likelihood so similarly work there, but you basically need something so you can go back from the likelihood to the parameter."
        ],
        [
            "So what do I mean by decompose decomposable shoot?",
            "Somehow couldn't make it stay.",
            "I just said you take 2 subclasses subspaces and then the null nicely."
        ],
        [
            "Become a part one part and the other.",
            "So for the Sioux we have non zero part and zero part for low rank which you hear from marching on Wednesday.",
            "You have a nice decomposition to buy more complex.",
            "So we recover whatever we have gotten in the literature in the suitcase, so exact sparsity or hard sparsity you have S non zeros and you have zeros.",
            "The rest 0.",
            "So you see this quelaag P coming up.",
            "So without the search, if you know the dimensions K you know which key parameters then you basically square root K overage, the right rate.",
            "Because we don't know what it is you pay a penalty of log P. OK, so if you know your noise tail is Gaussian or sub Gaussian, then the effective sample size become over log P. There's still a nice little backup at envelope calculation."
        ],
        [
            "Because very conservative.",
            "So at least you lost that much sample size.",
            "OK, so should you do this search, you should just reduce your sample size by factor one over log and sometimes you do small.",
            "But this is conservative.",
            "So we recovered that and four week sparsity is that you do assume your parameters are exactly in the sparse.",
            "Non zero, you have a little bit tail like say Q equal to half, so things kind of die out pretty fast.",
            "And from practical point of view this is very very important is that you need your result today aggregate gracefully when the exact Sumption doesn't hold.",
            "So you do have that policy.",
            "OK, but I'm not addressing the other.",
            "You know you have missing.",
            "Predictors."
        ],
        [
            "So to summarize that we can recover lots of.",
            "Quite a few existing results and the at the time they kind of weak sparse linear model.",
            "Kind of knew, but now it's been done with the form students, which kind of concurrent work and low rank estimation was also margins.",
            "Work with the student, but the general idea models truly adding you.",
            "But actually I didn't give you the hard part of her proof, the results so you won't have a cone there."
        ],
        [
            "So what we learned is that you really need convexity over small part of sample space.",
            "Now the samples with parameter space an depend alization these you there if there's a good match between the regularizer and your model assumption.",
            "OK, so that's kind of take hold message.",
            "It's not nothing.",
            "Two out of the ordinary and I've been working on some FMR problem.",
            "Sparsity can make an argument, holds there it's a good approximation, but for the media analysis it's completely just."
        ],
        [
            "A necessity we have to look at sparse presentations and this is joint work with Lorenda guy who is a copy of the grant we have together.",
            "NSF CDI and we have a student Luke Meretrix, Anna, former Postal, now went back to China.",
            "Ginger Jayanna student Laura and Brian Coward."
        ],
        [
            "So still very much going on going so the the.",
            "The project actually started by a question of of Lawrence, a social scientist from UC Davis.",
            "The question will choose studying Muslim, how she was interested in seeing how mostly have been portrayed before after September 11 in US media.",
            "And, um.",
            "You can ask this question special small, interesting going overtime, which we're moving there, but I don't have a result to show yet.",
            "So you want to see how particular topic is changed over a particular newspaper website.",
            "And we want some automation of the corpus because."
        ],
        [
            "We couldn't possibly read all the.",
            "Documents out there.",
            "As I said, since I'm kind of a little short of time, I'll just say that we want predictive and sparsity already, said Anne.",
            "We want human validation, so here's a result from our current state of the method.",
            "So we.",
            "Came up with Beijing contributed research.",
            "So do you know anybody have a guess why contributed research come up their global Hu Jintao, the President Impulse in John Promise, Xinjiang People Liberation Army because they were there because India had protests in Shanghai.",
            "So trying province earthquake anti bad trade Cougars who the people live in Sinjang.",
            "And when Java the Prime Minister in Shanghai news?",
            "But why contributed research is there?",
            "Yes, because it doesn't happen with you know.",
            "Yeah, you got it.",
            "So it's kind of like us so we didn't tease out that part because we put everything together first.",
            "We were positive, but you absolutely right.",
            "That's why it's there.",
            "So we should probably get rid of it."
        ],
        [
            "Point, so here's the data.",
            "We have the international section of New York Times.",
            "And we have 130,000 paragraphs of about 10,000 articles.",
            "So depends on what you need you take.",
            "You can have big and or you can have relatively small and.",
            "And we.",
            "Did we started with single word but now when we went to some phrases just to bring some more meaning so we end up with.",
            "216 thousand after deleting some real words.",
            "OK, we truncated and there are some replicates.",
            "Is about data and we."
        ],
        [
            "Move those.",
            "Here's but interest time.",
            "You have alot of things just before you can do the thing we do.",
            "We didn't do stemming because we talked about it, decided not to because sometimes the two words actually have different meanings.",
            "And we did bagger phrases vectorization.",
            "And."
        ],
        [
            "We dropped the real words and you end up with a very sparse matrix.",
            "OK, so XIJK is the number of counts of the JS JS document.",
            "The case term.",
            "OK. And then we."
        ],
        [
            "We we actually.",
            "This is the first time we did it.",
            "We didn't worry too much about preprocessing, but later realize that you have to normalize a little bit.",
            "So we compared the remove spot word or rescaling by TFT.",
            "FDIOU do what's mathematician do use our two normalization and then."
        ],
        [
            "Labeling because the data just there.",
            "Why we call one class this or that is up to us.",
            "So we did different labeling whether we have when the China appeared."
        ],
        [
            "Voice of China.",
            "Chinese or China's.",
            "We did like 5 different possibilities just to see whether that's sensitive.",
            "We don't want our result to be very sensitive to our pre processing because that just means you're not capturing anything.",
            "And we compared the measure we compare relative or fast.",
            "So we will be able to run.",
            "So this is the matter after you do all the pre processing Co occur, you don't use negative example.",
            "Just look at the old articles which labeled China and.",
            "You select phrases.",
            "Correlation is a simple Stew measured.",
            "An you do use the negative examples unless Sue and logistic regression with L1 penalty.",
            "OK, so we're running our method of modifications of Madigan groups BBR.",
            "Take into account sparsity.",
            "OK, here's a flow chart of what we do right.",
            "You have corpus.",
            "And then you make a data matrix and you do different way of to prepare the matrix and then for the labeling we also have different options and then for select races we have 4 right?",
            "So in the end we get 120 possibilities we did."
        ],
        [
            "Over factorial possibility.",
            "OK, so this is a second round for our conference paper.",
            "We didn't go so do such a sorrow.",
            "Investigation.",
            "I emphasize again that prediction is not a goal.",
            "So we like prediction well, but we cannot really take it as seriously as used to.",
            "If you predict predictions that go.",
            "And we want to do summarization of human like raging.",
            "Basically we want to replace human rating.",
            "Response variable.",
            "Open the document has China in it or not.",
            "So two classes, so the last.",
            "So you use the linear regression for the yes.",
            "So what happened was that we began with logistics rationale penalty because that was more natural.",
            "And later I suggested that why don't you do assume?",
            "Because faster I know that from previous work for two classes that suit give pretty good result not more than two classes because the scaling will kill you.",
            "Yeah, so that's why we find out.",
            "Actually that's really good.",
            "And it's faster."
        ],
        [
            "We did actually.",
            "We started with logistic regression.",
            "We did it for a long time but then.",
            "For computational reasons, so it was actually just running human experiment.",
            "It's the first time we did it.",
            "Took awhile to get approved.",
            "And then nicely has business going.",
            "Berkeley has a X lab.",
            "They can run this experiment value and we design web survey which can be talking itself.",
            "How we put it, but I won't have time.",
            "But I can give you the paper and then the library could do 36 subjects right?",
            "So this is not random sample.",
            "This is either staff or students.",
            "I don't think Professors wanted to do that.",
            "It's too little pain.",
            "So it's the Berkeley community, right?",
            "So we basically validated some of methods to the Berkeley Community and we actually want to try a mechanic.",
            "Turk fantasy, whether there's any difference.",
            "But this human experiment takes time and we did a lot of randomization to make sure that we don't create systematic."
        ],
        [
            "Errors and then we.",
            "I'll just show you the result.",
            "So this is kind of.",
            "I'll just show you one panel and then just give you the summary.",
            "Otherwise it's painful to go.",
            "So the first panel, the top one you have on the vertical and horizontal axis you have for method cooccurrence correlation R1LR, which is logistics.",
            "One panel of the regression analysis and you aggregated over all other one.",
            "You have three curves.",
            "Is we basically the first kind of look at the pre processing method as forced upward and R for rescaling with L2.",
            "And then T is TF IDF.",
            "OK, so you can see that the higher the score the better.",
            "So T. FIDF, which is which is a measured from the literature of the field.",
            "Information retrieval works really well, which is not surprising because that's something impurity people ratifying that and across different methods.",
            "So the first roll panel is we use documents as the unit.",
            "If you move to the panel below it.",
            "You have still have the same file measures called current correlation, LLR and Sue and you see.",
            "The order.",
            "Of how good."
        ],
        [
            "Things are in terms of re scaling changed, so R which is re skating use L2.",
            "Becomes.",
            "The best.",
            "You can see that the SU.",
            "And also one of our actually for this panels work better than the two other measures across the different.",
            "It's not overall you would think that Sue is pretty robust, but.",
            "L1 how are if you know which rescaling to use?",
            "L1R is pretty good, but we worry that with the new units not newspaper or some other writing, you don't know whether how things work, so we kind of prefer the shoe and the other panels also shows the sushi for his lawsuit goes pretty well, but.",
            "Summarize in the high level summaries."
        ],
        [
            "That TFIDF, if you looking at articles, it's very good measure progress scaling, which is not surprising because that's what we took from the information retrieval and LY rescaling.",
            "Oh, I'm sorry, was not L 281, we tried out too, but I think I won't spread out.",
            "We didn't do our two or wireless killing is the best for paragraphs and assume if you consider all factors it seems to be the best overall and not just intern performance, but relative to performance measures and labeling options.",
            "And OLR is not bad either but it's slower.",
            "So that's why we prefer the SU.",
            "And the algorithms are modified modifications of the BBR by magic in Group.",
            "So we just they give us the source code with some modification."
        ],
        [
            "So the summary is that we really feel like sometimes not always abstraction, complaints, clarity.",
            "But you have to start with.",
            "The examples would have gone there.",
            "Always have some example, you know what's going on and then you can do the general and for the social science problem media analysis.",
            "I'm tossing the document XQ but I don't know whether my calls will go with it.",
            "We did many processing schemes and predicting the guy we didn't develop any new algorithms, but I think our contribution is more formulating the problem into machine learning framework and also validation through human validation.",
            "So why are we going in terms of the?",
            "They are standings because Daniels the topic Image Ng Project we try to get more data from New York Time and the data actually is not the easiest and as you scoop it yourself.",
            "And we haven't done crosstime analysis.",
            "We also want to do analysis across different geographic regions and different time periods.",
            "And because the message quite automatic, can we produce serious about finding something robusta?",
            "Preprocessing we think might be possible to do different languages and then the Chinese visiting scholar is trying out with People's Daily and doing Chinese newspaper and so far we haven't read human experiment yet but looks reasonable."
        ],
        [
            "So we hope this is a step forward for some sensitive social science study.",
            "And right now we're talking to a political scientist to maybe compared the two party platform documents, Republican and Democrat parties, and to see whether those documents actually annotated so they Republican Democrat, tend to use different terminologies."
        ],
        [
            "And see whether we can automatically.",
            "Car is kind of still more validation.",
            "And also we're talking to a heist Business School professor who wants to study the British Parliament documents in terms of tax laws over since 1860, or something to see whether people allow the British laws have changed overtime.",
            "So in the beginning, tax laws property laws were written by for particular Duke or Baron, and now it's more common.",
            "So see whether we can capture that.",
            "That's also something he had a conjecture to see whether we can.",
            "Really recovered that he was actually also interested in analyzing Chinese textbooks over the years.",
            "I said I'll stay out of that project.",
            "And in terms of research side, we kind of want from the human data.",
            "Where can we predict human performance?",
            "We know prediction."
        ],
        [
            "There is not enough because of different methods.",
            "Looking at prediction errors about 70%.",
            "Is predictive and then after that you cannot help.",
            "Can we find other features in data to predict human performance?",
            "The other thing is can we go beyond just name entities, right?",
            "You saw the latest on China is mainly names nouns.",
            "Can we go to work?",
            "So I really want to see we can actually give sentences and to help.",
            "What did People's Republic?"
        ],
        [
            "Like People's Liberation Army do in Xinjiang, for example.",
            "Right now there just say they were there so and then with visualization, I'm thinking that you can click on the name entity and see the next layer and the next layer and we're moving towards that with some initial ideas.",
            "Let me thank my coauthors again and without them wouldn't have been possible an my funding agencies.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, thank you very much for the invitation.",
                    "label": 0
                },
                {
                    "sent": "It's my first time here, so can you hear me?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doesn't stay here.",
                    "label": 0
                },
                {
                    "sent": "So here I would like to report some recent work on sparse modeling and in particular, or first go over some unifying theory which I have done with a couple calls.",
                    "label": 0
                },
                {
                    "sent": "I will tell you their names when the time comes and also I want to report on going project with also colleagues on topic Image Ng.",
                    "label": 0
                },
                {
                    "sent": "So kind of bring sparse modeling to social science.",
                    "label": 0
                },
                {
                    "sent": "First, I would like to dedicate this talk to my former colleague, the late professor David Blackwell, who passed away last year, and in particular, I'm like to remind ourselves this famous quote.",
                    "label": 0
                },
                {
                    "sent": "David said many years ago.",
                    "label": 0
                },
                {
                    "sent": "Basically, I'm not interested in doing research.",
                    "label": 0
                },
                {
                    "sent": "I don't know how many of you can claim that, and I have never been an interesting understanding, which is a quite different thing.",
                    "label": 0
                },
                {
                    "sent": "If you haven't heard about David Blackwell, encourage you just search.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm on the web.",
                    "label": 0
                },
                {
                    "sent": "And there are many other interviews and movies, and so I think you enjoy knowing him a bit.",
                    "label": 0
                },
                {
                    "sent": "So statistics or machine learning or in general data driven science is increasing a great error now because we have this site evolution and we have lots and lots of data here.",
                    "label": 0
                },
                {
                    "sent": "Just very very few areas I happen to have touched.",
                    "label": 0
                },
                {
                    "sent": "Genomics, neuroscience, remote sensing and computer vision, but I think it would be interesting for all of us to pay more attention to the emerging field of computational social science and kind of natural language processing.",
                    "label": 1
                },
                {
                    "sent": "Maybe sister, but definitely social networks when you try to understand human interactions organizaci.",
                    "label": 0
                },
                {
                    "sent": "And tasks.",
                    "label": 0
                },
                {
                    "sent": "And also what I will talk about is trying to use a machine learning methods, but go beyond that with human validation.",
                    "label": 0
                },
                {
                    "sent": "With media analysis an we're tossing the term topic image Ng, but it's kind of not the best distinguishing from the topic modeling from LDA, so we may be switching to something less exciting, but document subject specific summarization so it's a mouthful, so we haven't settled on a good topic yet.",
                    "label": 0
                },
                {
                    "sent": "All of these problems.",
                    "label": 0
                },
                {
                    "sent": "Represent high dimensional data with large P and small.",
                    "label": 0
                },
                {
                    "sent": "An answer, sample size and P is the number attributes with predictors.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because we depends on how you take the document unit.",
                    "label": 0
                },
                {
                    "sent": "If you take articles depends also the document corpus.",
                    "label": 0
                },
                {
                    "sent": "So if you take paragraphs is huge so you have a point that sometimes they're allowed by piece large so.",
                    "label": 0
                },
                {
                    "sent": "But if you take document, sample size is not too large.",
                    "label": 0
                },
                {
                    "sent": "Depends on the corpus you take.",
                    "label": 0
                },
                {
                    "sent": "But and could be large.",
                    "label": 0
                },
                {
                    "sent": "And here is where I idealized where how we see information flows.",
                    "label": 0
                },
                {
                    "sent": "Right, you have investigating reporting because I'm doing media analysis, I'll start from there and that impacts.",
                    "label": 0
                },
                {
                    "sent": "You know agents and actors and decision makers would impact the States and you have more reporting, so there's this feedback loop.",
                    "label": 0
                },
                {
                    "sent": "An historical data has been.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Spectrum of media reporting right you have on one end you have the hero from the Second World War.",
                    "label": 1
                },
                {
                    "sent": "Who wants to be very very faithful to be persuasive.",
                    "label": 0
                },
                {
                    "sent": "We must be believable to be believable.",
                    "label": 0
                },
                {
                    "sent": "We must be credible to be credible.",
                    "label": 0
                },
                {
                    "sent": "It must be truthful, right?",
                    "label": 0
                },
                {
                    "sent": "This one and as Edward Murrow with word time Reporter and you have the other strength spectrum which Hurst.",
                    "label": 0
                },
                {
                    "sent": "You supply the photographs and or supply the war so you basically can make up anything.",
                    "label": 0
                },
                {
                    "sent": "I think the truth probably somewhere in the middle.",
                    "label": 0
                },
                {
                    "sent": "Everybody kind of make up a bed and there's a whole spectrum, so we want to be on this and on the left hand.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Put the photos in the right position.",
                    "label": 0
                },
                {
                    "sent": "And so far the media analysis has been very very labor intensive, so you have a graduate student who are ready to corpus of documents and then she or he will write a report based on their rating.",
                    "label": 0
                },
                {
                    "sent": "And also this person's particular experience and views will impact on how they're going to summarize certain coverage, say China.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we have seen a great advance of statistical machine learning.",
                    "label": 0
                },
                {
                    "sent": "Most of the time, I mean, we hope it's fast and scalable.",
                    "label": 0
                },
                {
                    "sent": "And we hope it's kind of objective in the sense that when the method is specified, including preprocessing stage, then she reproducible.",
                    "label": 0
                },
                {
                    "sent": "So doesn't who's running the algorithm?",
                    "label": 0
                },
                {
                    "sent": "What we should get the same thing?",
                    "label": 0
                },
                {
                    "sent": "And but it's very much designed so far around predictive task.",
                    "label": 0
                },
                {
                    "sent": "Of course, sparse modeling is a step forward to move to interpretability.",
                    "label": 0
                },
                {
                    "sent": "So our goal is to harness this power from machine learning to enhance media studies.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we're going to use the predictive and sparse framework as surrogates, so we want to predict it.",
                    "label": 1
                },
                {
                    "sent": "But we also want to be sparse, so we now hitting the top is predicting performance as I will show you.",
                    "label": 1
                },
                {
                    "sent": "However, we also want to validate because we are after summarization and meaning.",
                    "label": 0
                },
                {
                    "sent": "We want to validate things through human experiments, not just the methods but also robust pre processing.",
                    "label": 0
                },
                {
                    "sent": "So we basically want the method to be relative robust.",
                    "label": 0
                },
                {
                    "sent": "Oh oh, sensible ways of normalizing's remove words so there's less obituary choices.",
                    "label": 0
                },
                {
                    "sent": "So let's just take the data.",
                    "label": 0
                },
                {
                    "sent": "I will show you all the results, show you will be based on the business section of New York.",
                    "label": 0
                },
                {
                    "sent": "I'm in 2009.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the idea is that supposed to be interesting topic actually that the project has involved has evolved in the beginning.",
                    "label": 0
                },
                {
                    "sent": "We're just using a query word now and moving into like topic so.",
                    "label": 1
                },
                {
                    "sent": "Of connection thank you.",
                    "label": 0
                },
                {
                    "sent": "Oops, now I cut it off.",
                    "label": 0
                },
                {
                    "sent": "So we want to say China.",
                    "label": 0
                },
                {
                    "sent": "We want to take.",
                    "label": 0
                },
                {
                    "sent": "Use predictive framework to find a set of words or phrases.",
                    "label": 1
                },
                {
                    "sent": "We begin with the token like bag of words and we moved to like bigrams and trigrams so it's kind of evolving.",
                    "label": 1
                },
                {
                    "sent": "Which best distinguish China from the articles not having anything to do?",
                    "label": 0
                },
                {
                    "sent": "I mean not mentioning China, I shouldn't claim that.",
                    "label": 0
                },
                {
                    "sent": "Have nothing to do with China, just didn't mention the word China.",
                    "label": 0
                },
                {
                    "sent": "And as I said, we want our model to be pretty good.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interpretable, which is a much much broader concept than sparsity, but right now we're using sparsity as a surrogate to leaders there, but we don't really.",
                    "label": 0
                },
                {
                    "sent": "We cannot trust any sparse model necessarily integrable, so we.",
                    "label": 0
                },
                {
                    "sent": "Miranda human experiment to validate what we find.",
                    "label": 0
                },
                {
                    "sent": "So we want things to be fast because of huge documents, but database and we understand that our sparse model is not true right?",
                    "label": 0
                },
                {
                    "sent": "We don't have any illusion that the sparse model is now generating how the documents are generated, but it's a useful approximation so all models are wrong by some models are useful in the spirit of George Box.",
                    "label": 0
                },
                {
                    "sent": "I will call the selected set of phrases or words at the topic imaging of the topic of course relative to particular document, carpets, right?",
                    "label": 0
                },
                {
                    "sent": "If you take a different corpus, say you take news from China, which we're doing now too.",
                    "label": 0
                },
                {
                    "sent": "Is cross language.",
                    "label": 0
                },
                {
                    "sent": "Then you might have very different impression, right?",
                    "label": 0
                },
                {
                    "sent": "So suppose you take the coverage of the recent nuclear plant accident in Japan.",
                    "label": 0
                },
                {
                    "sent": "In China, Friends came back.",
                    "label": 0
                },
                {
                    "sent": "It's quite different from what we hear here.",
                    "label": 0
                },
                {
                    "sent": "And we want the topic image to be valued at least in two ways.",
                    "label": 1
                },
                {
                    "sent": "So we wanted to be predictive.",
                    "label": 0
                },
                {
                    "sent": "So if prediction is very very low, that's now transferred to either.",
                    "label": 0
                },
                {
                    "sent": "But we also understand that to make the prediction as as possible doesn't mean it's the most meaningful.",
                    "label": 0
                },
                {
                    "sent": "And later we discovered that.",
                    "label": 0
                },
                {
                    "sent": "So a certain point up to a certain point, like 70% predictability is very differentiating with meanings and then the meaning and predictability kind of decouples.",
                    "label": 0
                },
                {
                    "sent": "So they don't go always together to the end, and we wanted the words to be meaningful and also robust to pre processing.",
                    "label": 0
                },
                {
                    "sent": "So here just a run of.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I see us like there's a lot of work to do.",
                    "label": 0
                },
                {
                    "sent": "Data processing for most of the non modern data problems.",
                    "label": 1
                },
                {
                    "sent": "And subject knowledge also very important.",
                    "label": 0
                },
                {
                    "sent": "The earlier you bring that in, the better you off.",
                    "label": 0
                },
                {
                    "sent": "Even we have a lot of good automatic methods.",
                    "label": 0
                },
                {
                    "sent": "And here I'm not actually developing any new methodology in this talk.",
                    "label": 0
                },
                {
                    "sent": "I do believe strongly that theory comes in.",
                    "label": 0
                },
                {
                    "sent": "Necessarily for high dimensional problems, because it's very hard for us to understand things without.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the mental abstraction.",
                    "label": 0
                },
                {
                    "sent": "So I see theory as a useful.",
                    "label": 0
                },
                {
                    "sent": "Two to help us to do good work in practice.",
                    "label": 0
                },
                {
                    "sent": "And validation, so we cannot just run away from models.",
                    "label": 0
                },
                {
                    "sent": "We really have to know bring the problem back to the context and have some other outside statistics framework to validate what we see.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's a road map for today's talk.",
                    "label": 1
                },
                {
                    "sent": "I'll give very brief introduction, probably unnecessary for sparse modeling.",
                    "label": 0
                },
                {
                    "sent": "I couldn't help keep bring back the 70s model selection saying that model selections process is not really just knew.",
                    "label": 1
                },
                {
                    "sent": "Now it's around for a long time.",
                    "label": 0
                },
                {
                    "sent": "And then I will cover some.",
                    "label": 0
                },
                {
                    "sent": "I'll try to give you a geometric view of a recent work with.",
                    "label": 0
                },
                {
                    "sent": "Anne Megabowl Ravikumar actually is here today and also Wainwright were right on Wednesday.",
                    "label": 0
                },
                {
                    "sent": "And then I'll come back to the topic image Ng problem show you what we did.",
                    "label": 0
                },
                {
                    "sent": "Mainly it's actually not approved pre processing and also human validation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is courtesy of Wikipedia is I don't know whether it's really described anything about all come but so nice photo to show who is.",
                    "label": 0
                },
                {
                    "sent": "14th century English logician and Franciscan Friar.",
                    "label": 1
                },
                {
                    "sent": "He is named well.",
                    "label": 0
                },
                {
                    "sent": "We attribute this principle parsimony to him called outcomes.",
                    "label": 0
                },
                {
                    "sent": "Razor entities must not be multiplied beyond necessity.",
                    "label": 0
                },
                {
                    "sent": "And I gave this this slide a couple years ago in a workshop in Europe.",
                    "label": 0
                },
                {
                    "sent": "Friend on Paul.",
                    "label": 0
                },
                {
                    "sent": "Mentioned that actually you can trace this back to Greek writing, so I'm sure if you go further anytime when you start it's very simple.",
                    "label": 0
                },
                {
                    "sent": "Reasonable criteria, so if I want to be a little mathematical and then give you some notation.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have a script XN tell me the end typos of my data XYZ appear dimensional vector which is a predictor.",
                    "label": 0
                },
                {
                    "sent": "So in my.",
                    "label": 0
                },
                {
                    "sent": "Topic image Ng problem.",
                    "label": 0
                },
                {
                    "sent": "If I take China out or everything relate to China and then.",
                    "label": 0
                },
                {
                    "sent": "The XI will contain either.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so now the other words exist or not, or some word counts.",
                    "label": 0
                },
                {
                    "sent": "You can take different versions and why would be this document is an has contained word relate to China or not.",
                    "label": 0
                },
                {
                    "sent": "Annina matrix notation.",
                    "label": 0
                },
                {
                    "sent": "I can write a linear model as white to Exeter, Platycerium, and people realize that if you do maximum likelihood, you do these squares and the Gaussian assumption you end up with the largest model.",
                    "label": 0
                },
                {
                    "sent": "If you want to select, you have two to the power P sub models for me depends on how much light do trigram.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Diagram my pee is like in the millions.",
                    "label": 0
                },
                {
                    "sent": "And you don't want to go for the largest model first.",
                    "label": 0
                },
                {
                    "sent": "You cannot fit it.",
                    "label": 0
                },
                {
                    "sent": "In the 70s, people already realized the problem.",
                    "label": 1
                },
                {
                    "sent": "Actually, if you read features paper in 1921, he was already doing some model selection or he was already aware of the model selection problem by the formal Dragons and terminology didn't really came about until the 70s, and you have a key key.",
                    "label": 0
                },
                {
                    "sent": "It's a Japanese statistician who started the Institute of Statistical Mathematics and Metal CP at Bell Labs.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that you actually try to estimate the prediction error using model.",
                    "label": 0
                },
                {
                    "sent": "We nowadays use to use.",
                    "label": 0
                },
                {
                    "sent": "To use a cross validation empirically estimated estimation error.",
                    "label": 0
                },
                {
                    "sent": "By that time the data was not as abandoned, so people are using estimation of prediction error as the criteria to select the model.",
                    "label": 0
                },
                {
                    "sent": "Naturally, this leads to a penalized maximum, likelihood or penalize these squares and AIC, which is a key key information criteria, although he will insist, is our information criteria that you penalized by two times the dimension of the model.",
                    "label": 0
                },
                {
                    "sent": "Usually we were feeding.",
                    "label": 0
                },
                {
                    "sent": "Audio only squares.",
                    "label": 0
                },
                {
                    "sent": "I'm not dealing with the Sigma Scarping estimated, just make the presenter easier and a few years later Schwartz did abbasian version and he basically took the vision framework and did a Laplacian approximation to the posterior probabilities and discovered that to the 1st order the priorities and manner.",
                    "label": 0
                },
                {
                    "sent": "This is very much related to the syntactic normality of Asian estimate under good conditions, so it's kind of the same analytical tools and the prize disappear because syntactically you data override your prior.",
                    "label": 0
                },
                {
                    "sent": "And you end up penalized the dimension by log in, which depends on the sample size than two, which AIC, AIC.",
                    "label": 0
                },
                {
                    "sent": "We know more or less is very closely related to cross validation and gives you.",
                    "label": 0
                },
                {
                    "sent": "Good predictions MBC.",
                    "label": 0
                },
                {
                    "sent": "When you have major predictors and less important ones, you have nice two groups, be icy, cuts it better and then there's always coding criteria which happened to work on many years ago which will coincide with the IC41 four man has many other connection with coding and also Bayesian statistics.",
                    "label": 0
                },
                {
                    "sent": "So both if you use more.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms we say that that criteria penalized with L0, which is because you're counting.",
                    "label": 0
                },
                {
                    "sent": "If you go back to my account, my calculation of P is like in the millions.",
                    "label": 1
                },
                {
                    "sent": "You will have two to the power P or 10 to the power 30,000.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Models.",
                    "label": 0
                },
                {
                    "sent": "To choose from OK and for me, my end is probably thousands or 10s of thousands.",
                    "label": 0
                },
                {
                    "sent": "Most of time will say that this convent or research is too expensive, which are definitely true to possibly run over this possibilities, but I want to make an argument that is also not necessary.",
                    "label": 0
                },
                {
                    "sent": "Because for statistics, we're really not doing infinite precision optimization we have.",
                    "label": 0
                },
                {
                    "sent": "Only Say 10s of thousands of observations.",
                    "label": 0
                },
                {
                    "sent": "And the number of submodels is astronomical compared with the sample size.",
                    "label": 1
                },
                {
                    "sent": "I don't think anybody in this room would believe that we actually have the power to tell these models apart.",
                    "label": 0
                },
                {
                    "sent": "Basically this is something which we can take advantage of when the noise or the data is random.",
                    "label": 0
                },
                {
                    "sent": "We really kind of using a surrogate right?",
                    "label": 0
                },
                {
                    "sent": "If you go to the classical framework, we really want the Oracle.",
                    "label": 0
                },
                {
                    "sent": "Likelihood, which will give you the true distribution because we true parameter because we don't, we use random surrogate or sampling surrogate and that surrogate wobbles right?",
                    "label": 0
                },
                {
                    "sent": "Because depends on what sample you get.",
                    "label": 0
                },
                {
                    "sent": "It's a random approximation and therefore we don't have to be very precise.",
                    "label": 0
                },
                {
                    "sent": "We just have to be as precise as the data allows us and this kind of roughly says that there's no need to go through this sub models because we don't have the power to tell them apart.",
                    "label": 0
                },
                {
                    "sent": "And lot of models basically.",
                    "label": 0
                },
                {
                    "sent": "Equivalent from the point of the model, the data, because we cannot tell them apart.",
                    "label": 0
                },
                {
                    "sent": "So if you do the convex relaxation of this L zero problem to go to lesu, you basically just cover the path in all this possible models.",
                    "label": 0
                },
                {
                    "sent": "As long as your pass goes through this equivalent classes for predicted propers, you basically engage.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And This is why lifestyle.",
                    "label": 0
                },
                {
                    "sent": "I'm sure I already heard about Lesu is.",
                    "label": 0
                },
                {
                    "sent": "Basically it's all one penalized least squares.",
                    "label": 0
                },
                {
                    "sent": "You are have this counters if P is 2, the circle represents a counter line of the first term, which is the least squares.",
                    "label": 0
                },
                {
                    "sent": "And then you have L1 panelization, which is equivalent to L1 constraint.",
                    "label": 0
                },
                {
                    "sent": "And if you think about you have this diamond and you through like a nice smooth ellipsoid type of thing at this most like you end up hitting.",
                    "label": 0
                },
                {
                    "sent": "The diamond at the corner and the color you have sparsity.",
                    "label": 0
                },
                {
                    "sent": "It doesn't always happen that way, but most sometimes it does and nobody has really articulate exactly what you mean by, you know, most of the time.",
                    "label": 0
                },
                {
                    "sent": "You will have to introduce some probability distribution.",
                    "label": 0
                },
                {
                    "sent": "And again, this message now in most time.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Attribute to the name came from TV show newspaper 1996.",
                    "label": 0
                },
                {
                    "sent": "He calls the SU means at least absolute selection and shrinkage operator.",
                    "label": 0
                },
                {
                    "sent": "There's two as might be permitted.",
                    "label": 0
                },
                {
                    "sent": "They were already works.",
                    "label": 0
                },
                {
                    "sent": "There was basis pursuit in signal processing by turn down the Hall and then there is this non negative garage by bryman.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It's kind of very nice mathematic.",
                    "label": 0
                },
                {
                    "sent": "Lee Ann.",
                    "label": 0
                },
                {
                    "sent": "In practice, there's also in the beginning quadratic programming tool, so people really have been loving it.",
                    "label": 0
                },
                {
                    "sent": "In the beginning, people using QP to solve it for each Lambda, because the quadratic programming problem.",
                    "label": 0
                },
                {
                    "sent": "But soon people realize that well in mathematics people don't usually select Lambda, but when you select Lambda you have to calculate it for a bunch of lambdas you should really tie order solution together.",
                    "label": 0
                },
                {
                    "sent": "So there was a second generation which is past following algorithms which work pretty well too, like 2 million predictors.",
                    "label": 0
                },
                {
                    "sent": "And there's lots there.",
                    "label": 0
                },
                {
                    "sent": "Couldn't buy anything at all, and there was an earlier work actually, not quite interstitial literature by offspring at all in 2000 and then recently.",
                    "label": 0
                },
                {
                    "sent": "Somebody told me that there was even earlier paper 1957 in kind of mathematical finance by Mark, which he solved whole salute of quadratic programming problem use pass following.",
                    "label": 0
                },
                {
                    "sent": "So I'm sure there would be earlier reference if we dig further.",
                    "label": 0
                },
                {
                    "sent": "Right now, the four really, really large P. The gradient type of methods to the leading the 1st order method is still very much active risk.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there's a lot of active research on extending to various sparsity to being bringing prior information.",
                    "label": 0
                },
                {
                    "sent": "I'll just add more regularity, even lawsuits, regularised alot of times on peace.",
                    "label": 0
                },
                {
                    "sent": "Really large is not enough regularization.",
                    "label": 0
                },
                {
                    "sent": "So I'm now going to the agreement and the sparse modeling so and also, of course there's a parallel field called sparse sensing, a compressed sensing, and there they emphasize more the design of X.",
                    "label": 0
                },
                {
                    "sent": "So sparse modeling we tend to use it when we kind of passively observe X&Y, without worrying about how to collect taxes.",
                    "label": 0
                },
                {
                    "sent": "But compressed sensing is signal processing engineering.",
                    "label": 0
                },
                {
                    "sent": "There's more emphasis on, but how to choose X or randomize X, but the mathematics are very much the same.",
                    "label": 0
                },
                {
                    "sent": "So for the last 10 or 15 years, statistical theoretical research have really reviewed that.",
                    "label": 1
                },
                {
                    "sent": "Indiana One piece very large.",
                    "label": 0
                },
                {
                    "sent": "If you don't have low dimensional structures, hopeless right?",
                    "label": 0
                },
                {
                    "sent": "That's kind of lost generation.",
                    "label": 0
                },
                {
                    "sent": "You go back to 2030 years.",
                    "label": 0
                },
                {
                    "sent": "We have this recent convergence and people look at it.",
                    "label": 0
                },
                {
                    "sent": "If your functions actually ambient so it's really impede dimension, you quickly run out.",
                    "label": 0
                },
                {
                    "sent": "The samples are even for an equal to like 100.",
                    "label": 0
                },
                {
                    "sent": "But Luckily I think lot of times I wouldn't say all the time nature does give us some structure to work with and we do have approximate low dimensional structure and therefore we pay like price for search in this high dimensional dimension, but with something not too bad.",
                    "label": 0
                },
                {
                    "sent": "If the tales of the noise in the linear model discussion, but when the tails is not Gaussian you can see that things are a lot harder.",
                    "label": 0
                },
                {
                    "sent": "So we have this log P kind of term.",
                    "label": 0
                },
                {
                    "sent": "Which is very tight to the thin tail of Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "An sparse sparsity.",
                    "label": 0
                },
                {
                    "sent": "It's really want the simplest low dimensional structure and linear model.",
                    "label": 1
                },
                {
                    "sent": "Again is the simplest model, so it's very nice to analyze that because mathematically you can go far further and there's a lot of work analyzing the soup.",
                    "label": 1
                },
                {
                    "sent": "You can use prediction error, you can use L2 error.",
                    "label": 1
                },
                {
                    "sent": "You can also use model selection, consistency and most of the time we assume the.",
                    "label": 0
                },
                {
                    "sent": "The true model is sparse, right?",
                    "label": 0
                },
                {
                    "sent": "So it's kind of there's a nice match between the model and the Mansard.",
                    "label": 0
                },
                {
                    "sent": "Recently we have done some work.",
                    "label": 0
                },
                {
                    "sent": "Assume the noise is not really Gaussian, but pursue amazingly less.",
                    "label": 0
                },
                {
                    "sent": "You actually also came up pretty nicely.",
                    "label": 0
                },
                {
                    "sent": "We kind of motivated at proper medical image in where you cannot assume Gaussian ality, but amazingly well imagine signal to noise ratio.",
                    "label": 0
                },
                {
                    "sent": "The SU actually turned out to be pretty good.",
                    "label": 0
                },
                {
                    "sent": "So there's been lots of lots of lots of work taking while two or three criteria to analyze the.",
                    "label": 0
                },
                {
                    "sent": "So would I want to present.",
                    "label": 0
                },
                {
                    "sent": "Next is some recent will try to unify a lot of the works and hopefully bring more clarity into what's going on in high dimension.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First was the NIPS paper 209 and we submitted Journal paper.",
                    "label": 0
                },
                {
                    "sent": "I think now we we invited to do one part of the paper to Institute science.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "Let me go to a bit abstraction so.",
                    "label": 0
                },
                {
                    "sent": "This is a general problem.",
                    "label": 0
                },
                {
                    "sent": "If you leave the squares, so my first blue expression L is a loss function could be the minus log of the log likelihood function from general idea squares like logistic regression.",
                    "label": 0
                },
                {
                    "sent": "And R doesn't have to be.",
                    "label": 0
                },
                {
                    "sent": "L1 could be some grouped lawsuit.",
                    "label": 0
                },
                {
                    "sent": "OK, but nevertheless you have a goodness of fit term.",
                    "label": 0
                },
                {
                    "sent": "The first term and you have a regularization term second term.",
                    "label": 1
                },
                {
                    "sent": "And the goal is to see whether if there's a true model which generate a theater star.",
                    "label": 0
                },
                {
                    "sent": "If City stars structure matches the regularizer, and then there's hope that we hope the estimator the difference will be small and the proper conditions so usually sit.",
                    "label": 0
                },
                {
                    "sent": "A star is kind of sparse.",
                    "label": 0
                },
                {
                    "sent": "Just think about a suit for this purpose, it's good enough, but I'll give you the general geometric interpretation of our result.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, any question.",
                    "label": 0
                },
                {
                    "sent": "So from now I will get a little heavy before was kind of just talking.",
                    "label": 0
                },
                {
                    "sent": "OK, so notation wise it's OK.",
                    "label": 0
                },
                {
                    "sent": "Here's a better view of the linear under sparse modeling, so there was the similarity between my slide and Martin slide, because we kind of.",
                    "label": 0
                },
                {
                    "sent": "Then you have this why which is in dimension.",
                    "label": 0
                },
                {
                    "sent": "And you have a rectangle X matrix that's kind of the feature of modern statistical problems piece larger than.",
                    "label": 0
                },
                {
                    "sent": "And then you can permute all the theaters so that non 0 ones come up in the pink block and kind of blueish.",
                    "label": 0
                },
                {
                    "sent": "Block is all the zeros.",
                    "label": 0
                },
                {
                    "sent": "OK, so big S is the non 0 status.",
                    "label": 0
                },
                {
                    "sent": "And the magenta is.",
                    "label": 0
                },
                {
                    "sent": "The noise I want to emphasize again later, you're going to see something like log P appearing.",
                    "label": 0
                },
                {
                    "sent": "There has a lot to do with.",
                    "label": 0
                },
                {
                    "sent": "Like Omega being short tails of sub Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's now like if I give you.",
                    "label": 0
                },
                {
                    "sent": "Along like a heavy tailed distribution, things just automatic go through some.",
                    "label": 0
                },
                {
                    "sent": "My son might not.",
                    "label": 0
                },
                {
                    "sent": "So so far, so don't think so.",
                    "label": 0
                },
                {
                    "sent": "High dimensional problems will be this nice search term log P. That's kind of very nice.",
                    "label": 0
                },
                {
                    "sent": "It's because you think about if Omega is closely.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have a lot of outliers going to be mixed up with the nonzeros, either because you can have huge observation why and it's not coming from the Cedar stars coming from the noise and you can see the problem gets harder OK and I just listed you know lots of papers.",
                    "label": 0
                },
                {
                    "sent": "If I'm not including work on my politics because there's no.",
                    "label": 0
                },
                {
                    "sent": "So lots and lots of work.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. And then there's another problem people have started, which is sparse graphical models for Gaussian likelihood, right?",
                    "label": 0
                },
                {
                    "sent": "So if you have multivariate Gaussian, you have a nice nice correspondence between conditional independence, say between 1, four.",
                    "label": 0
                },
                {
                    "sent": "And the inverse covariance matrix one full right?",
                    "label": 0
                },
                {
                    "sent": "So my black actually means zero, my white means higher.",
                    "label": 0
                },
                {
                    "sent": "OK, you see one and four.",
                    "label": 0
                },
                {
                    "sent": "Those like smoothing is because presentation.",
                    "label": 0
                },
                {
                    "sent": "This shouldn't be this like blurring.",
                    "label": 0
                },
                {
                    "sent": "It should be just zero.",
                    "label": 0
                },
                {
                    "sent": "Oh no 0.",
                    "label": 0
                },
                {
                    "sent": "So then you can formulate estimation of the sparse graphical models through.",
                    "label": 0
                },
                {
                    "sent": "Also at one panelization.",
                    "label": 0
                },
                {
                    "sent": "Of course, this is only one particular one.",
                    "label": 0
                },
                {
                    "sent": "Doesn't mean that we couldn't find better ones later, but that's what we're doing now and also work in there.",
                    "label": 0
                },
                {
                    "sent": "So again, you have a blue form, which is the loss function fit, and then you have a red which is a penalty and one personality doesn't hold.",
                    "label": 0
                },
                {
                    "sent": "The blue part can be interpreted as a Bregman divergences, so there's still some interpretation.",
                    "label": 0
                },
                {
                    "sent": "You're matching the coherence even you lose the conditional independence.",
                    "label": 0
                },
                {
                    "sent": "And the next one I'll just do a ad for Martin on Wednesday morning.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's not tomorrow.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "It's Wednesday morning at 8:15 so he will talk more about that so I won't go into that.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So about almost like a year and half ago, we're wondering that we did a couple examples ourselves and we see other people's works and we start wondering.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's something.",
                    "label": 0
                },
                {
                    "sent": "Unifying then doing case by case.",
                    "label": 0
                },
                {
                    "sent": "Right.",
                    "label": 0
                },
                {
                    "sent": "Right now you take statistics.",
                    "label": 0
                },
                {
                    "sent": "I mean if you go to the 50s and 40s and 30s people approving maximum likelihood, assembly morality, case by case and after a while there's a unifying framework which cover lots of cases.",
                    "label": 0
                },
                {
                    "sent": "So that's what we wanted to do.",
                    "label": 0
                },
                {
                    "sent": "We saw some similarity in doing linear squares, low rank and also.",
                    "label": 0
                },
                {
                    "sent": "Graphic, sparse graphical model estimation and the question is, can we fish out something essential to the good performance of this L1 penalized estimators and make it?",
                    "label": 0
                },
                {
                    "sent": "Actually we wanted to teach in graduate courses.",
                    "label": 0
                },
                {
                    "sent": "So it turns out is such indeed the case.",
                    "label": 0
                },
                {
                    "sent": "We cannot handle any regularization, was a little disappointment.",
                    "label": 0
                },
                {
                    "sent": "Actually, we're hoping that will cover a broader class of regularizers, but it didn't happen, at least with the techniques we used.",
                    "label": 0
                },
                {
                    "sent": "You need the regularizer 90 decompose relative to some subspaces, like the non 0100 ones.",
                    "label": 0
                },
                {
                    "sent": "They have to separate otherwise, or some triangle inequality argument just wouldn't go through.",
                    "label": 0
                },
                {
                    "sent": "And the second one is very classic in some sense.",
                    "label": 0
                },
                {
                    "sent": "We know that we need some convexity for the loss function, right?",
                    "label": 0
                },
                {
                    "sent": "If things are flat in loss function, there's no way you can cover the parameter, but to us, nice surprise, us already seen previous works.",
                    "label": 0
                },
                {
                    "sent": "You don't need the strong convexity over the whole space because we don't have it right.",
                    "label": 0
                },
                {
                    "sent": "P is larger than N, you won't have it, so we only need restricted strong convexity.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's let me just go back to maximum likelihood estimation under the bed.",
                    "label": 0
                },
                {
                    "sent": "Right, the reason so?",
                    "label": 0
                },
                {
                    "sent": "If you take my.",
                    "label": 0
                },
                {
                    "sent": "Solid curve as the Oracle.",
                    "label": 0
                },
                {
                    "sent": "I could function basically the likely function with expectation always infinite sample size.",
                    "label": 0
                },
                {
                    "sent": "Right and then you have truth.",
                    "label": 0
                },
                {
                    "sent": "Well maybe the other way around.",
                    "label": 0
                },
                {
                    "sent": "You have to imagine the article now because I have my minimize.",
                    "label": 0
                },
                {
                    "sent": "That's it ahead.",
                    "label": 0
                },
                {
                    "sent": "If it's either would be fine.",
                    "label": 0
                },
                {
                    "sent": "But as I said, right?",
                    "label": 0
                },
                {
                    "sent": "You have basically one work schedule, any kind of disturbance to this article loss function by Central Limit Theorem by concentration inequality, whatever you want to think, then you really don't have the the article you have random approximation and because the random browser is wildly occur of Delta L in terms of how close you are to the truth of the likehood function, when you do the minimization that introduce an error.",
                    "label": 0
                },
                {
                    "sent": "In terms of estimation, right?",
                    "label": 0
                },
                {
                    "sent": "You'll see the ancetre hat.",
                    "label": 0
                },
                {
                    "sent": "While you.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh, good curvature.",
                    "label": 0
                },
                {
                    "sent": "Then you can pin it down precisely.",
                    "label": 0
                },
                {
                    "sent": "The wobbly in the vertical direction translates pretty nicely, or even get reduced contracted too.",
                    "label": 0
                },
                {
                    "sent": "The difference between the estimator and the truth, but you have a flat or low curvature like could function, then a little disturbance in the vertical scale get translate to a huge disturbance.",
                    "label": 0
                },
                {
                    "sent": "So basically that's what the Fisher information tells us, is the curvature right?",
                    "label": 0
                },
                {
                    "sent": "So hashing you have good hashing.",
                    "label": 0
                },
                {
                    "sent": "Strong curvature you can estimate well and when you don't, you can't.",
                    "label": 1
                },
                {
                    "sent": "OK and then.",
                    "label": 0
                },
                {
                    "sent": "There's some target goal space.",
                    "label": 0
                },
                {
                    "sent": "They use central limit theorem since I've had to kill the sampling noise an you see fish information.",
                    "label": 0
                },
                {
                    "sent": "What happens in high dimension in high dimension?",
                    "label": 0
                },
                {
                    "sent": "Let's think about this crash lawsuit.",
                    "label": 0
                },
                {
                    "sent": "So how changes X prime X and if P is larger than N, your Hessian matrix is forever degenerate.",
                    "label": 0
                },
                {
                    "sent": "You always want to have.",
                    "label": 0
                },
                {
                    "sent": "Flight directions it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "What you do, doesn't measure how wide your axis is.",
                    "label": 0
                },
                {
                    "sent": "There's no escaping.",
                    "label": 0
                },
                {
                    "sent": "So if you do this square, that's kind of indirectly says.",
                    "label": 0
                },
                {
                    "sent": "Therefore, these squares you you have to do general inverse do have many many solutions because all the data points all the parameter values kind of lying on the flight direction will give you the same discourse and you don't know which one to choose.",
                    "label": 0
                },
                {
                    "sent": "However, it turns out if you look at if we import look at item 2.",
                    "label": 0
                },
                {
                    "sent": "If you look at the compass based the decomposable regularizers like L1.",
                    "label": 0
                },
                {
                    "sent": "While there's a match of the sparsity in the true model and L1.",
                    "label": 0
                },
                {
                    "sent": "If you do the SU this matching.",
                    "label": 0
                },
                {
                    "sent": "And Decomposability will force the difference of your true estimator, your estimator and truth to a cone which I use green see.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is not the most general case, but it's good enough for the talk.",
                    "label": 0
                },
                {
                    "sent": "But now what you need is that.",
                    "label": 0
                },
                {
                    "sent": "You actually only need.",
                    "label": 0
                },
                {
                    "sent": "The strong convexity on this cone.",
                    "label": 0
                },
                {
                    "sent": "Right, because assuming the sparse is small, this cone relative the whole ambient space is a small set.",
                    "label": 0
                },
                {
                    "sent": "And therefore.",
                    "label": 0
                },
                {
                    "sent": "You OK and you can prove if your design matrix now to change degenerate.",
                    "label": 0
                },
                {
                    "sent": "Then you can have this strong convexity on this small cone.",
                    "label": 0
                },
                {
                    "sent": "And therefore the old old, like a classical astronautics, will go through on this call.",
                    "label": 0
                },
                {
                    "sent": "Basically that's what's going on in high dimension.",
                    "label": 0
                },
                {
                    "sent": "In this simple case.",
                    "label": 0
                },
                {
                    "sent": "And, um.",
                    "label": 0
                },
                {
                    "sent": "Going through this exercise really made us ourselves very clear what needed.",
                    "label": 0
                },
                {
                    "sent": "Otherwise the deeper example use special properties.",
                    "label": 0
                },
                {
                    "sent": "You kind of get it, but you don't see the common structure.",
                    "label": 0
                },
                {
                    "sent": "So this is a mathematical result.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "I think I will probably don't want to go into becoming instrumentation.",
                    "label": 0
                },
                {
                    "sent": "Basically that's just writing out what I said in mathematical form.",
                    "label": 0
                },
                {
                    "sent": "So the work for different problems is to prove the strong convexity.",
                    "label": 0
                },
                {
                    "sent": "Over the cone for different problems and that can be challenging for general linear models.",
                    "label": 0
                },
                {
                    "sent": "Actually we needed a more refined result than what I'm presenting below in terms of graphics.",
                    "label": 0
                },
                {
                    "sent": "Actually we don't have a cone, we have something.",
                    "label": 0
                },
                {
                    "sent": "In cruise at the origin and then more work needed so the work basics being pushed to proving the strongest strong convexity, which is like you have to do for maximum likely to you have all this regularity conditions on the different partial derivatives of likelihood so similarly work there, but you basically need something so you can go back from the likelihood to the parameter.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what do I mean by decompose decomposable shoot?",
                    "label": 0
                },
                {
                    "sent": "Somehow couldn't make it stay.",
                    "label": 0
                },
                {
                    "sent": "I just said you take 2 subclasses subspaces and then the null nicely.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Become a part one part and the other.",
                    "label": 0
                },
                {
                    "sent": "So for the Sioux we have non zero part and zero part for low rank which you hear from marching on Wednesday.",
                    "label": 0
                },
                {
                    "sent": "You have a nice decomposition to buy more complex.",
                    "label": 0
                },
                {
                    "sent": "So we recover whatever we have gotten in the literature in the suitcase, so exact sparsity or hard sparsity you have S non zeros and you have zeros.",
                    "label": 0
                },
                {
                    "sent": "The rest 0.",
                    "label": 0
                },
                {
                    "sent": "So you see this quelaag P coming up.",
                    "label": 0
                },
                {
                    "sent": "So without the search, if you know the dimensions K you know which key parameters then you basically square root K overage, the right rate.",
                    "label": 0
                },
                {
                    "sent": "Because we don't know what it is you pay a penalty of log P. OK, so if you know your noise tail is Gaussian or sub Gaussian, then the effective sample size become over log P. There's still a nice little backup at envelope calculation.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because very conservative.",
                    "label": 0
                },
                {
                    "sent": "So at least you lost that much sample size.",
                    "label": 0
                },
                {
                    "sent": "OK, so should you do this search, you should just reduce your sample size by factor one over log and sometimes you do small.",
                    "label": 0
                },
                {
                    "sent": "But this is conservative.",
                    "label": 0
                },
                {
                    "sent": "So we recovered that and four week sparsity is that you do assume your parameters are exactly in the sparse.",
                    "label": 0
                },
                {
                    "sent": "Non zero, you have a little bit tail like say Q equal to half, so things kind of die out pretty fast.",
                    "label": 0
                },
                {
                    "sent": "And from practical point of view this is very very important is that you need your result today aggregate gracefully when the exact Sumption doesn't hold.",
                    "label": 0
                },
                {
                    "sent": "So you do have that policy.",
                    "label": 0
                },
                {
                    "sent": "OK, but I'm not addressing the other.",
                    "label": 0
                },
                {
                    "sent": "You know you have missing.",
                    "label": 0
                },
                {
                    "sent": "Predictors.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize that we can recover lots of.",
                    "label": 0
                },
                {
                    "sent": "Quite a few existing results and the at the time they kind of weak sparse linear model.",
                    "label": 1
                },
                {
                    "sent": "Kind of knew, but now it's been done with the form students, which kind of concurrent work and low rank estimation was also margins.",
                    "label": 0
                },
                {
                    "sent": "Work with the student, but the general idea models truly adding you.",
                    "label": 0
                },
                {
                    "sent": "But actually I didn't give you the hard part of her proof, the results so you won't have a cone there.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we learned is that you really need convexity over small part of sample space.",
                    "label": 0
                },
                {
                    "sent": "Now the samples with parameter space an depend alization these you there if there's a good match between the regularizer and your model assumption.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's kind of take hold message.",
                    "label": 0
                },
                {
                    "sent": "It's not nothing.",
                    "label": 0
                },
                {
                    "sent": "Two out of the ordinary and I've been working on some FMR problem.",
                    "label": 0
                },
                {
                    "sent": "Sparsity can make an argument, holds there it's a good approximation, but for the media analysis it's completely just.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A necessity we have to look at sparse presentations and this is joint work with Lorenda guy who is a copy of the grant we have together.",
                    "label": 0
                },
                {
                    "sent": "NSF CDI and we have a student Luke Meretrix, Anna, former Postal, now went back to China.",
                    "label": 0
                },
                {
                    "sent": "Ginger Jayanna student Laura and Brian Coward.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So still very much going on going so the the.",
                    "label": 0
                },
                {
                    "sent": "The project actually started by a question of of Lawrence, a social scientist from UC Davis.",
                    "label": 0
                },
                {
                    "sent": "The question will choose studying Muslim, how she was interested in seeing how mostly have been portrayed before after September 11 in US media.",
                    "label": 0
                },
                {
                    "sent": "And, um.",
                    "label": 0
                },
                {
                    "sent": "You can ask this question special small, interesting going overtime, which we're moving there, but I don't have a result to show yet.",
                    "label": 0
                },
                {
                    "sent": "So you want to see how particular topic is changed over a particular newspaper website.",
                    "label": 0
                },
                {
                    "sent": "And we want some automation of the corpus because.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We couldn't possibly read all the.",
                    "label": 0
                },
                {
                    "sent": "Documents out there.",
                    "label": 0
                },
                {
                    "sent": "As I said, since I'm kind of a little short of time, I'll just say that we want predictive and sparsity already, said Anne.",
                    "label": 0
                },
                {
                    "sent": "We want human validation, so here's a result from our current state of the method.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "Came up with Beijing contributed research.",
                    "label": 0
                },
                {
                    "sent": "So do you know anybody have a guess why contributed research come up their global Hu Jintao, the President Impulse in John Promise, Xinjiang People Liberation Army because they were there because India had protests in Shanghai.",
                    "label": 1
                },
                {
                    "sent": "So trying province earthquake anti bad trade Cougars who the people live in Sinjang.",
                    "label": 0
                },
                {
                    "sent": "And when Java the Prime Minister in Shanghai news?",
                    "label": 0
                },
                {
                    "sent": "But why contributed research is there?",
                    "label": 0
                },
                {
                    "sent": "Yes, because it doesn't happen with you know.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you got it.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of like us so we didn't tease out that part because we put everything together first.",
                    "label": 0
                },
                {
                    "sent": "We were positive, but you absolutely right.",
                    "label": 0
                },
                {
                    "sent": "That's why it's there.",
                    "label": 0
                },
                {
                    "sent": "So we should probably get rid of it.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Point, so here's the data.",
                    "label": 0
                },
                {
                    "sent": "We have the international section of New York Times.",
                    "label": 1
                },
                {
                    "sent": "And we have 130,000 paragraphs of about 10,000 articles.",
                    "label": 0
                },
                {
                    "sent": "So depends on what you need you take.",
                    "label": 0
                },
                {
                    "sent": "You can have big and or you can have relatively small and.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                },
                {
                    "sent": "Did we started with single word but now when we went to some phrases just to bring some more meaning so we end up with.",
                    "label": 0
                },
                {
                    "sent": "216 thousand after deleting some real words.",
                    "label": 0
                },
                {
                    "sent": "OK, we truncated and there are some replicates.",
                    "label": 0
                },
                {
                    "sent": "Is about data and we.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Move those.",
                    "label": 0
                },
                {
                    "sent": "Here's but interest time.",
                    "label": 0
                },
                {
                    "sent": "You have alot of things just before you can do the thing we do.",
                    "label": 0
                },
                {
                    "sent": "We didn't do stemming because we talked about it, decided not to because sometimes the two words actually have different meanings.",
                    "label": 0
                },
                {
                    "sent": "And we did bagger phrases vectorization.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We dropped the real words and you end up with a very sparse matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, so XIJK is the number of counts of the JS JS document.",
                    "label": 0
                },
                {
                    "sent": "The case term.",
                    "label": 0
                },
                {
                    "sent": "OK. And then we.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We we actually.",
                    "label": 0
                },
                {
                    "sent": "This is the first time we did it.",
                    "label": 0
                },
                {
                    "sent": "We didn't worry too much about preprocessing, but later realize that you have to normalize a little bit.",
                    "label": 0
                },
                {
                    "sent": "So we compared the remove spot word or rescaling by TFT.",
                    "label": 0
                },
                {
                    "sent": "FDIOU do what's mathematician do use our two normalization and then.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Labeling because the data just there.",
                    "label": 0
                },
                {
                    "sent": "Why we call one class this or that is up to us.",
                    "label": 0
                },
                {
                    "sent": "So we did different labeling whether we have when the China appeared.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Voice of China.",
                    "label": 0
                },
                {
                    "sent": "Chinese or China's.",
                    "label": 0
                },
                {
                    "sent": "We did like 5 different possibilities just to see whether that's sensitive.",
                    "label": 0
                },
                {
                    "sent": "We don't want our result to be very sensitive to our pre processing because that just means you're not capturing anything.",
                    "label": 0
                },
                {
                    "sent": "And we compared the measure we compare relative or fast.",
                    "label": 0
                },
                {
                    "sent": "So we will be able to run.",
                    "label": 0
                },
                {
                    "sent": "So this is the matter after you do all the pre processing Co occur, you don't use negative example.",
                    "label": 0
                },
                {
                    "sent": "Just look at the old articles which labeled China and.",
                    "label": 0
                },
                {
                    "sent": "You select phrases.",
                    "label": 0
                },
                {
                    "sent": "Correlation is a simple Stew measured.",
                    "label": 0
                },
                {
                    "sent": "An you do use the negative examples unless Sue and logistic regression with L1 penalty.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're running our method of modifications of Madigan groups BBR.",
                    "label": 0
                },
                {
                    "sent": "Take into account sparsity.",
                    "label": 0
                },
                {
                    "sent": "OK, here's a flow chart of what we do right.",
                    "label": 1
                },
                {
                    "sent": "You have corpus.",
                    "label": 0
                },
                {
                    "sent": "And then you make a data matrix and you do different way of to prepare the matrix and then for the labeling we also have different options and then for select races we have 4 right?",
                    "label": 0
                },
                {
                    "sent": "So in the end we get 120 possibilities we did.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Over factorial possibility.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a second round for our conference paper.",
                    "label": 0
                },
                {
                    "sent": "We didn't go so do such a sorrow.",
                    "label": 0
                },
                {
                    "sent": "Investigation.",
                    "label": 0
                },
                {
                    "sent": "I emphasize again that prediction is not a goal.",
                    "label": 1
                },
                {
                    "sent": "So we like prediction well, but we cannot really take it as seriously as used to.",
                    "label": 0
                },
                {
                    "sent": "If you predict predictions that go.",
                    "label": 0
                },
                {
                    "sent": "And we want to do summarization of human like raging.",
                    "label": 0
                },
                {
                    "sent": "Basically we want to replace human rating.",
                    "label": 1
                },
                {
                    "sent": "Response variable.",
                    "label": 0
                },
                {
                    "sent": "Open the document has China in it or not.",
                    "label": 0
                },
                {
                    "sent": "So two classes, so the last.",
                    "label": 0
                },
                {
                    "sent": "So you use the linear regression for the yes.",
                    "label": 0
                },
                {
                    "sent": "So what happened was that we began with logistics rationale penalty because that was more natural.",
                    "label": 0
                },
                {
                    "sent": "And later I suggested that why don't you do assume?",
                    "label": 0
                },
                {
                    "sent": "Because faster I know that from previous work for two classes that suit give pretty good result not more than two classes because the scaling will kill you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's why we find out.",
                    "label": 0
                },
                {
                    "sent": "Actually that's really good.",
                    "label": 0
                },
                {
                    "sent": "And it's faster.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We did actually.",
                    "label": 0
                },
                {
                    "sent": "We started with logistic regression.",
                    "label": 0
                },
                {
                    "sent": "We did it for a long time but then.",
                    "label": 0
                },
                {
                    "sent": "For computational reasons, so it was actually just running human experiment.",
                    "label": 1
                },
                {
                    "sent": "It's the first time we did it.",
                    "label": 0
                },
                {
                    "sent": "Took awhile to get approved.",
                    "label": 0
                },
                {
                    "sent": "And then nicely has business going.",
                    "label": 0
                },
                {
                    "sent": "Berkeley has a X lab.",
                    "label": 0
                },
                {
                    "sent": "They can run this experiment value and we design web survey which can be talking itself.",
                    "label": 0
                },
                {
                    "sent": "How we put it, but I won't have time.",
                    "label": 0
                },
                {
                    "sent": "But I can give you the paper and then the library could do 36 subjects right?",
                    "label": 0
                },
                {
                    "sent": "So this is not random sample.",
                    "label": 0
                },
                {
                    "sent": "This is either staff or students.",
                    "label": 0
                },
                {
                    "sent": "I don't think Professors wanted to do that.",
                    "label": 0
                },
                {
                    "sent": "It's too little pain.",
                    "label": 0
                },
                {
                    "sent": "So it's the Berkeley community, right?",
                    "label": 0
                },
                {
                    "sent": "So we basically validated some of methods to the Berkeley Community and we actually want to try a mechanic.",
                    "label": 0
                },
                {
                    "sent": "Turk fantasy, whether there's any difference.",
                    "label": 0
                },
                {
                    "sent": "But this human experiment takes time and we did a lot of randomization to make sure that we don't create systematic.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Errors and then we.",
                    "label": 0
                },
                {
                    "sent": "I'll just show you the result.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of.",
                    "label": 0
                },
                {
                    "sent": "I'll just show you one panel and then just give you the summary.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it's painful to go.",
                    "label": 0
                },
                {
                    "sent": "So the first panel, the top one you have on the vertical and horizontal axis you have for method cooccurrence correlation R1LR, which is logistics.",
                    "label": 0
                },
                {
                    "sent": "One panel of the regression analysis and you aggregated over all other one.",
                    "label": 0
                },
                {
                    "sent": "You have three curves.",
                    "label": 0
                },
                {
                    "sent": "Is we basically the first kind of look at the pre processing method as forced upward and R for rescaling with L2.",
                    "label": 0
                },
                {
                    "sent": "And then T is TF IDF.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can see that the higher the score the better.",
                    "label": 0
                },
                {
                    "sent": "So T. FIDF, which is which is a measured from the literature of the field.",
                    "label": 0
                },
                {
                    "sent": "Information retrieval works really well, which is not surprising because that's something impurity people ratifying that and across different methods.",
                    "label": 0
                },
                {
                    "sent": "So the first roll panel is we use documents as the unit.",
                    "label": 0
                },
                {
                    "sent": "If you move to the panel below it.",
                    "label": 0
                },
                {
                    "sent": "You have still have the same file measures called current correlation, LLR and Sue and you see.",
                    "label": 0
                },
                {
                    "sent": "The order.",
                    "label": 0
                },
                {
                    "sent": "Of how good.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things are in terms of re scaling changed, so R which is re skating use L2.",
                    "label": 0
                },
                {
                    "sent": "Becomes.",
                    "label": 0
                },
                {
                    "sent": "The best.",
                    "label": 0
                },
                {
                    "sent": "You can see that the SU.",
                    "label": 0
                },
                {
                    "sent": "And also one of our actually for this panels work better than the two other measures across the different.",
                    "label": 0
                },
                {
                    "sent": "It's not overall you would think that Sue is pretty robust, but.",
                    "label": 0
                },
                {
                    "sent": "L1 how are if you know which rescaling to use?",
                    "label": 0
                },
                {
                    "sent": "L1R is pretty good, but we worry that with the new units not newspaper or some other writing, you don't know whether how things work, so we kind of prefer the shoe and the other panels also shows the sushi for his lawsuit goes pretty well, but.",
                    "label": 0
                },
                {
                    "sent": "Summarize in the high level summaries.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That TFIDF, if you looking at articles, it's very good measure progress scaling, which is not surprising because that's what we took from the information retrieval and LY rescaling.",
                    "label": 0
                },
                {
                    "sent": "Oh, I'm sorry, was not L 281, we tried out too, but I think I won't spread out.",
                    "label": 0
                },
                {
                    "sent": "We didn't do our two or wireless killing is the best for paragraphs and assume if you consider all factors it seems to be the best overall and not just intern performance, but relative to performance measures and labeling options.",
                    "label": 1
                },
                {
                    "sent": "And OLR is not bad either but it's slower.",
                    "label": 0
                },
                {
                    "sent": "So that's why we prefer the SU.",
                    "label": 1
                },
                {
                    "sent": "And the algorithms are modified modifications of the BBR by magic in Group.",
                    "label": 0
                },
                {
                    "sent": "So we just they give us the source code with some modification.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the summary is that we really feel like sometimes not always abstraction, complaints, clarity.",
                    "label": 0
                },
                {
                    "sent": "But you have to start with.",
                    "label": 0
                },
                {
                    "sent": "The examples would have gone there.",
                    "label": 0
                },
                {
                    "sent": "Always have some example, you know what's going on and then you can do the general and for the social science problem media analysis.",
                    "label": 1
                },
                {
                    "sent": "I'm tossing the document XQ but I don't know whether my calls will go with it.",
                    "label": 1
                },
                {
                    "sent": "We did many processing schemes and predicting the guy we didn't develop any new algorithms, but I think our contribution is more formulating the problem into machine learning framework and also validation through human validation.",
                    "label": 0
                },
                {
                    "sent": "So why are we going in terms of the?",
                    "label": 0
                },
                {
                    "sent": "They are standings because Daniels the topic Image Ng Project we try to get more data from New York Time and the data actually is not the easiest and as you scoop it yourself.",
                    "label": 0
                },
                {
                    "sent": "And we haven't done crosstime analysis.",
                    "label": 0
                },
                {
                    "sent": "We also want to do analysis across different geographic regions and different time periods.",
                    "label": 0
                },
                {
                    "sent": "And because the message quite automatic, can we produce serious about finding something robusta?",
                    "label": 0
                },
                {
                    "sent": "Preprocessing we think might be possible to do different languages and then the Chinese visiting scholar is trying out with People's Daily and doing Chinese newspaper and so far we haven't read human experiment yet but looks reasonable.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we hope this is a step forward for some sensitive social science study.",
                    "label": 0
                },
                {
                    "sent": "And right now we're talking to a political scientist to maybe compared the two party platform documents, Republican and Democrat parties, and to see whether those documents actually annotated so they Republican Democrat, tend to use different terminologies.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And see whether we can automatically.",
                    "label": 0
                },
                {
                    "sent": "Car is kind of still more validation.",
                    "label": 0
                },
                {
                    "sent": "And also we're talking to a heist Business School professor who wants to study the British Parliament documents in terms of tax laws over since 1860, or something to see whether people allow the British laws have changed overtime.",
                    "label": 0
                },
                {
                    "sent": "So in the beginning, tax laws property laws were written by for particular Duke or Baron, and now it's more common.",
                    "label": 0
                },
                {
                    "sent": "So see whether we can capture that.",
                    "label": 0
                },
                {
                    "sent": "That's also something he had a conjecture to see whether we can.",
                    "label": 0
                },
                {
                    "sent": "Really recovered that he was actually also interested in analyzing Chinese textbooks over the years.",
                    "label": 0
                },
                {
                    "sent": "I said I'll stay out of that project.",
                    "label": 0
                },
                {
                    "sent": "And in terms of research side, we kind of want from the human data.",
                    "label": 0
                },
                {
                    "sent": "Where can we predict human performance?",
                    "label": 1
                },
                {
                    "sent": "We know prediction.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is not enough because of different methods.",
                    "label": 0
                },
                {
                    "sent": "Looking at prediction errors about 70%.",
                    "label": 0
                },
                {
                    "sent": "Is predictive and then after that you cannot help.",
                    "label": 0
                },
                {
                    "sent": "Can we find other features in data to predict human performance?",
                    "label": 0
                },
                {
                    "sent": "The other thing is can we go beyond just name entities, right?",
                    "label": 0
                },
                {
                    "sent": "You saw the latest on China is mainly names nouns.",
                    "label": 0
                },
                {
                    "sent": "Can we go to work?",
                    "label": 0
                },
                {
                    "sent": "So I really want to see we can actually give sentences and to help.",
                    "label": 0
                },
                {
                    "sent": "What did People's Republic?",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like People's Liberation Army do in Xinjiang, for example.",
                    "label": 1
                },
                {
                    "sent": "Right now there just say they were there so and then with visualization, I'm thinking that you can click on the name entity and see the next layer and the next layer and we're moving towards that with some initial ideas.",
                    "label": 0
                },
                {
                    "sent": "Let me thank my coauthors again and without them wouldn't have been possible an my funding agencies.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}