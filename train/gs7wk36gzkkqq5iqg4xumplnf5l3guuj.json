{
    "id": "gs7wk36gzkkqq5iqg4xumplnf5l3guuj",
    "title": "A Non-Parametric Approach to Dynamic Programming",
    "info": {
        "author": [
            "Oliver B Kroemer, Technische Universit\u00e4t Darmstadt"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/nips2011_kroemer_programming/",
    "segmentation": [
        [
            "Today I'm gonna be talking about a nonparametric approach to dynamic programming.",
            "As already said, this is joint work with my supervisor Jan Peters and we'll both associated with the technical investigation come down steps as well as the box Office 2 for intelligence."
        ],
        [
            "Steps of the work out loud.",
            "The classical gears has mainly been on learning and episodic scenarios specifically.",
            "Robot learn as seen here.",
            "So here we have a robot arm playing table tennis.",
            "And recently we've been also moving further into actually developing methods for achieving these learn skills together, and today I'm going to be talking about one of the theoretical results that is come out of these endeavors.",
            "Those of you who visit this quite often should remember this video from a couple of years ago by against Obama."
        ],
        [
            "So to begin with, I'm going to give a introduction background on reinforcement learning in general."
        ],
        [
            "Before going on to my method, so in reinforcement learning, we're really interested in looking at the interaction between agent and its environment at a certain time step T."
        ],
        [
            "The agent is waiting for himself and then in the state S. And given the state they just got to choose."
        ],
        [
            "Action which we give by A and the way that the agent chooses it is by its policy Pi of a given S. Now this policy can either be sarcastic or."
        ],
        [
            "Deterministic.",
            "Given this taking action, the environment is going to determine what the next state for the agent is, which we denote by as stash, and which thing is passed back to the agent.",
            "Along with this transition probability, P of S dash given IIS, the environment also has a."
        ],
        [
            "Reward.",
            "The reward is also function at the state in action."
        ],
        [
            "The idea is that the agent ultimately wants to determine what policy it may use it to maximize the accumulation of rewards as it moves around this cycle."
        ],
        [
            "Now important tool for reinforcement learning is value functions.",
            "Value function is given by the equation shown here and what it means is."
        ],
        [
            "Actually, an indication of the amount of rewards that an agent can expect to receive, given that it's in State S S = 0."
        ],
        [
            "We also put a discount factor as we have an infinite horizon.",
            "This discount factor is between zero and one.",
            "It simply means that we have given more weight to the rewards that come sooner time rather than farther."
        ],
        [
            "Future.",
            "And obviously our value is also dependent on what policy we choose.",
            "As this one to determine which States and actions we come across and select."
        ],
        [
            "The reason why value function is so important is because it indicates what a good state is.",
            "If you have a high value, that's a good state to be in.",
            "And if we want to have a new policy Pi which is going to have a higher, give us more rewards, we can actually find the second group policy by greedily selecting an action that will give us immediately high rewarded and leaders to state with a high value."
        ],
        [
            "Now the value function can also be written in the as the famous of Element equation shown here.",
            "So again, this is an expectation."
        ],
        [
            "However, now we've splits our future, all our rewards into two parts.",
            "We have the immediate reward given by the reward function, as well as all future rewards encapsulated in the value of S dash.",
            "So the value of the next state."
        ],
        [
            "The important thing about validation is that if the value is there for a system for circuit system, the true value is invariant underneath this recursion.",
            "So the VS Dash VS are going to have exactly the same form.",
            "It also be noted that every system has a unique value function and that one always exists."
        ],
        [
            "Now in this talk I'm going to be focusing on actually computing the value function for continuous systems.",
            "And I'm generally assuming, though, that the policy is constant crap."
        ],
        [
            "So let's first of all look at enforcement learning in general and the kind of approaches that we have out there."
        ],
        [
            "So the first type of approach three is a policy search.",
            "In this case, we take data with the agent, takes data from series of rollouts, and accumulates rewards, and using this data it directly determines new policies to choose."
        ],
        [
            "The second type of approach is known as value function methods.",
            "These methods take the data in order to approximate the value function of S and then use this value function to determine improved policies."
        ],
        [
            "The third approach, and the one that I'm gonna be focusing on today, is dynamic programming and dynamic programming.",
            "The agent collects data.",
            "Would you then uses to model the system using.",
            "This model is then determine what the value function is and use that value function to learn improve policy.",
            "Now as the goal is to today is to learn a value function, I'm going to 1st go look into the."
        ],
        [
            "Value function methods and give an example of how they approach the problem."
        ],
        [
            "So obviously they tried to approach the value directly using a function approximation, and these methods can again be slipping too."
        ],
        [
            "Three subcategories, the first one is Monte Carlo methods.",
            "These methods require the agent to performance long trajectories over and determine actually how much reward they accumulate over these trajectories, and then the function approximation affectively comes to supervised learning."
        ],
        [
            "However, there's quite a lot of variance of these methods."
        ],
        [
            "The second approach, which you just heard about, also is a temporal difference learning.",
            "When you think function approximation, however temporal difference learning."
        ],
        [
            "Andy Tobias Solutions and is fairly dependent on the basis features 1 uses the third."
        ],
        [
            "Approaches residual gradient methods for these methods.",
            "One performs gradient descent on the residual."
        ],
        [
            "And these will generally lead to a BI solution, unless one can actually double sample states which is not always possible.",
            "And also these methods can be quite slow.",
            "So these are the function value function methods."
        ],
        [
            "Now we're going to look at dynamic programming, so once we actually try to use a model and then determine the value for that model."
        ],
        [
            "Value dynamic programming has really only been solved for two types of system, so the first one is the discrete state system.",
            "So here we have a two state system which some of you might recognize.",
            "Have the models actually defined by a transition table so it's little you're telling table with one element for each state action and next state pairing.",
            "Sorry, drive through today.",
            "And we also have a reward table which has one element for each state in action."
        ],
        [
            "For this kind of system, we know that our value function also gives a bad tabular form as shown here.",
            "So which has one element for each state?",
            "So the problem with using this kind of approach."
        ],
        [
            "Your system is obviously that we would need to discretize our state space."
        ],
        [
            "Action Space, A continuous situation, continues.",
            "Model is actually the linear quadratic system, in which case the system is represented by a set of linear equations shown in blue.",
            "And also the blue line shown here gets the shift from the current status to the next state.",
            "For this case, we assume that there is a quadratic reward function in both."
        ],
        [
            "Stating the action and in fact our value function also has a quadratic reward as shown here in the equation and in the current state."
        ],
        [
            "This approach is that it will will require us to linearize our systems, which is annoying if we have none."
        ],
        [
            "So that should give you a general background.",
            "And now we're going to look at the proposed nonparametric dynamic programming approach.",
            "And this approach uses 3 steps.",
            "The first step is to actually model the system in a flexible nonparametric manner.",
            "The second step is to determine what the form of the value function is for this kind of system.",
            "And the third step is to actually determine, evaluate the policy, and determine the parameters for a value function."
        ],
        [
            "So to begin with, we're going to assume that our knowledge of the system comes from a set of N samples.",
            "For each sample we have.",
            "This takes an action at the next state as well as a reward."
        ],
        [
            "And what we're going to do is we're going to model the joint distribution POSNS Dash.",
            "So this is the probability of being in a state to performing an action and transition into the next state.",
            "This joint distribution is important because it contains both our transition probability of the system as well as our policy.",
            "A given S."
        ],
        [
            "Now, the way that we represent this is in a male parametric manner is using kernel density estimation, so this is shown here where we place a kernel function on each of our samples and we decompose the kernel into three parts, one for the for the next dates.",
            "When for the action and one for the current state.",
            "When this implies, is that a single sample cannot define covariances between, for instance, the states in the next state, but multiple samples can.",
            "And the good thing like using current density estimates is that there is a wide range of proofs or whatever kernel density estimates that do actually converge to the true distribution given sufficient number of samples."
        ],
        [
            "For the reward function, given that we have a kernel density estimate model the we can expect.",
            "A another Watson reward function as shown here and you should notice that we actually have the same kernel functions in our reward function as we do in our kernel density estimate."
        ],
        [
            "Now, given this model, we actually want to determine what is the form of our valued function and as member we we do this by determining what function form will be invariant underneath the Bellman equation."
        ],
        [
            "So if we take our common equation and we rewrite it, we can get it in this form shown here.",
            "And as you can see, the actions have actually been integrated out at this point as part of being in all policy."
        ],
        [
            "And also notice that this version up here is actually constant, is independent of S and.",
            "And this is actually thought of as the values for our transitions.",
            "So what this should?"
        ],
        [
            "US is that if we use again a number, I want some form for our value function.",
            "If we put that form into the VS dash we will still get a natural Watson form for the VMS, which means that this is actually invariant underneath the Bellman equation and therefore represents the true form of the value function for this."
        ],
        [
            "Model.",
            "So here we see it again, this time with the status and again once you notice about the kernel functions are the same ones as used for our kernel density estimate.",
            "And in fact, this value function is completely defined by."
        ],
        [
            "My model.",
            "So for policy evaluation we need to compute the bigger parameters.",
            "Are we do this using the computation show down here?",
            "So either is a vector of the theater parameters are as the vector sample rewards and Lambda is a stochastic or a transition matrix with elements IJ given by this function on the right.",
            "Now, this does require a matrix inversion.",
            "However, it is only a sparse matrix version, which can be done relatively efficiently."
        ],
        [
            "So here we see a summary of the.",
            "Algorithm again, so as inputs we take samples where we have the states.",
            "The next states in their wards.",
            "I've left out the actions here as we don't expect them explicitly and we need kernel functions.",
            "Biocide, although generally these will have the same form as there for the current state in the next state.",
            "So using this model we can actually represent fairly complicated systems.",
            "Given enough samples, the computations are has shown pretty.",
            "Slide we can compute Lambda beta and then this data tell us the value function as shown at the bottom.",
            "And these functions are will actually have a similar basis.",
            "Then as the transitions of our system that allowed the observed transition transition services."
        ],
        [
            "So now we're going to go look into a straightforward illustre Tori numerical value."
        ],
        [
            "Patient in particular, we're going to be looking at this system where we have a sinusoidal transition function, so this doesn't actually mean that the agent is going to go in a sinusoid, but rather that if it's going to move according to the plot on the left.",
            "So for current given state, it's going to transition to the corresponding next state and see it's a fairly nonlinear dynamics.",
            "And on the right we have effectively cosine, which tells us the current reward for the current stage on top of the transition of the sinusoidal part of the transition.",
            "We also have some noise as indicated by the dashed lines.",
            "To begin with, we want to actually find out what is the true form of the value function for this system, and we can do that using Monte Carlo methods.",
            "So we start off by taking 500,000 samples of this trajectory."
        ],
        [
            "And we get this value function and as you can see it's symmetrical, which is what we expect.",
            "Now we're going to take only 100 samples and see what we get using the nonparametric dynamic programming."
        ],
        [
            "Approach and we get this.",
            "So here you see, we've already gotten the rough shape of the value function.",
            "There is still some offset and we have a mean squared error over the state state space of 3.8.",
            "Three now."
        ],
        [
            "Increases to 200 samples.",
            "We already get a much better fit.",
            "I'm still a little bit rough around the edges, but our mean squared error has gone down to 0.02."
        ],
        [
            "And if you include 300, we get even closer and yeah."
        ],
        [
            "So as you can see.",
            "We can quite accurately predict the true value function using only a limited number of samples, and these are some quite promising initial results.",
            "So here we also see the model that was learned for the 300, so we don't explicitly need to compute this, but for illustration it's always nice to see it.",
            "So on the left we have the kernel density estimate and on the right we have the approximation of the reward function.",
            "Obviously, in the future we hope to implement this on a real robot, and to do that we will need some policy improvement."
        ],
        [
            "There as well.",
            "So that brings me to the conclusion of my talk, so I presented Ultrametric approach to dynamic programming.",
            "The main philosophy behind this approach was to try to approximate a value, a system and then determine the value function for that system, rather than to approximate the value function directly.",
            "By using a non parametric approach we can model the system in a very flexible manner.",
            "And afterwards we can precisely compute the value function for this model.",
            "Well, the benefits of NPP is that it ensures that the basis functions actually match the observed dynamics, and as you saw we already have some positive results and hopefully in the future we will have some real robot evaluations.",
            "I'd also like to point out that in the paper there's a second contribution which is a common framework for deriving not only nonparametric dynamic programming, but these squares, temporal difference learning and kernelized temporal difference.",
            "Learning, such as Gaussian process, temporal difference, learning.",
            "Thank you very much and I look for your questions first time.",
            "I was wondering about if you could tell us a few words about was the relationship between this work and the work by or Wellington Green kernel based reinforcement learning.",
            "So with kernel based reinforcement learning you also have this normalized kernel form right?",
            "Yes, so I think the best way of describing the relationship that is actually."
        ],
        [
            "If we look here so.",
            "We have these sizes, size and the size so the distribution over the next steps.",
            "If you change that into a Delta function then you should actually derive the same algorithm as this, so this is trying to sort of including more general approach to it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today I'm gonna be talking about a nonparametric approach to dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "As already said, this is joint work with my supervisor Jan Peters and we'll both associated with the technical investigation come down steps as well as the box Office 2 for intelligence.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Steps of the work out loud.",
                    "label": 0
                },
                {
                    "sent": "The classical gears has mainly been on learning and episodic scenarios specifically.",
                    "label": 1
                },
                {
                    "sent": "Robot learn as seen here.",
                    "label": 0
                },
                {
                    "sent": "So here we have a robot arm playing table tennis.",
                    "label": 0
                },
                {
                    "sent": "And recently we've been also moving further into actually developing methods for achieving these learn skills together, and today I'm going to be talking about one of the theoretical results that is come out of these endeavors.",
                    "label": 0
                },
                {
                    "sent": "Those of you who visit this quite often should remember this video from a couple of years ago by against Obama.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to begin with, I'm going to give a introduction background on reinforcement learning in general.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before going on to my method, so in reinforcement learning, we're really interested in looking at the interaction between agent and its environment at a certain time step T.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The agent is waiting for himself and then in the state S. And given the state they just got to choose.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Action which we give by A and the way that the agent chooses it is by its policy Pi of a given S. Now this policy can either be sarcastic or.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Deterministic.",
                    "label": 0
                },
                {
                    "sent": "Given this taking action, the environment is going to determine what the next state for the agent is, which we denote by as stash, and which thing is passed back to the agent.",
                    "label": 0
                },
                {
                    "sent": "Along with this transition probability, P of S dash given IIS, the environment also has a.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reward.",
                    "label": 0
                },
                {
                    "sent": "The reward is also function at the state in action.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea is that the agent ultimately wants to determine what policy it may use it to maximize the accumulation of rewards as it moves around this cycle.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now important tool for reinforcement learning is value functions.",
                    "label": 0
                },
                {
                    "sent": "Value function is given by the equation shown here and what it means is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, an indication of the amount of rewards that an agent can expect to receive, given that it's in State S S = 0.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also put a discount factor as we have an infinite horizon.",
                    "label": 0
                },
                {
                    "sent": "This discount factor is between zero and one.",
                    "label": 1
                },
                {
                    "sent": "It simply means that we have given more weight to the rewards that come sooner time rather than farther.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Future.",
                    "label": 0
                },
                {
                    "sent": "And obviously our value is also dependent on what policy we choose.",
                    "label": 0
                },
                {
                    "sent": "As this one to determine which States and actions we come across and select.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The reason why value function is so important is because it indicates what a good state is.",
                    "label": 0
                },
                {
                    "sent": "If you have a high value, that's a good state to be in.",
                    "label": 1
                },
                {
                    "sent": "And if we want to have a new policy Pi which is going to have a higher, give us more rewards, we can actually find the second group policy by greedily selecting an action that will give us immediately high rewarded and leaders to state with a high value.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the value function can also be written in the as the famous of Element equation shown here.",
                    "label": 0
                },
                {
                    "sent": "So again, this is an expectation.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, now we've splits our future, all our rewards into two parts.",
                    "label": 0
                },
                {
                    "sent": "We have the immediate reward given by the reward function, as well as all future rewards encapsulated in the value of S dash.",
                    "label": 0
                },
                {
                    "sent": "So the value of the next state.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The important thing about validation is that if the value is there for a system for circuit system, the true value is invariant underneath this recursion.",
                    "label": 0
                },
                {
                    "sent": "So the VS Dash VS are going to have exactly the same form.",
                    "label": 0
                },
                {
                    "sent": "It also be noted that every system has a unique value function and that one always exists.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now in this talk I'm going to be focusing on actually computing the value function for continuous systems.",
                    "label": 0
                },
                {
                    "sent": "And I'm generally assuming, though, that the policy is constant crap.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's first of all look at enforcement learning in general and the kind of approaches that we have out there.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first type of approach three is a policy search.",
                    "label": 0
                },
                {
                    "sent": "In this case, we take data with the agent, takes data from series of rollouts, and accumulates rewards, and using this data it directly determines new policies to choose.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second type of approach is known as value function methods.",
                    "label": 0
                },
                {
                    "sent": "These methods take the data in order to approximate the value function of S and then use this value function to determine improved policies.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The third approach, and the one that I'm gonna be focusing on today, is dynamic programming and dynamic programming.",
                    "label": 1
                },
                {
                    "sent": "The agent collects data.",
                    "label": 0
                },
                {
                    "sent": "Would you then uses to model the system using.",
                    "label": 1
                },
                {
                    "sent": "This model is then determine what the value function is and use that value function to learn improve policy.",
                    "label": 0
                },
                {
                    "sent": "Now as the goal is to today is to learn a value function, I'm going to 1st go look into the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Value function methods and give an example of how they approach the problem.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So obviously they tried to approach the value directly using a function approximation, and these methods can again be slipping too.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three subcategories, the first one is Monte Carlo methods.",
                    "label": 0
                },
                {
                    "sent": "These methods require the agent to performance long trajectories over and determine actually how much reward they accumulate over these trajectories, and then the function approximation affectively comes to supervised learning.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, there's quite a lot of variance of these methods.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second approach, which you just heard about, also is a temporal difference learning.",
                    "label": 0
                },
                {
                    "sent": "When you think function approximation, however temporal difference learning.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Andy Tobias Solutions and is fairly dependent on the basis features 1 uses the third.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approaches residual gradient methods for these methods.",
                    "label": 0
                },
                {
                    "sent": "One performs gradient descent on the residual.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these will generally lead to a BI solution, unless one can actually double sample states which is not always possible.",
                    "label": 0
                },
                {
                    "sent": "And also these methods can be quite slow.",
                    "label": 0
                },
                {
                    "sent": "So these are the function value function methods.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we're going to look at dynamic programming, so once we actually try to use a model and then determine the value for that model.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Value dynamic programming has really only been solved for two types of system, so the first one is the discrete state system.",
                    "label": 1
                },
                {
                    "sent": "So here we have a two state system which some of you might recognize.",
                    "label": 0
                },
                {
                    "sent": "Have the models actually defined by a transition table so it's little you're telling table with one element for each state action and next state pairing.",
                    "label": 0
                },
                {
                    "sent": "Sorry, drive through today.",
                    "label": 1
                },
                {
                    "sent": "And we also have a reward table which has one element for each state in action.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this kind of system, we know that our value function also gives a bad tabular form as shown here.",
                    "label": 0
                },
                {
                    "sent": "So which has one element for each state?",
                    "label": 0
                },
                {
                    "sent": "So the problem with using this kind of approach.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your system is obviously that we would need to discretize our state space.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Action Space, A continuous situation, continues.",
                    "label": 0
                },
                {
                    "sent": "Model is actually the linear quadratic system, in which case the system is represented by a set of linear equations shown in blue.",
                    "label": 0
                },
                {
                    "sent": "And also the blue line shown here gets the shift from the current status to the next state.",
                    "label": 0
                },
                {
                    "sent": "For this case, we assume that there is a quadratic reward function in both.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stating the action and in fact our value function also has a quadratic reward as shown here in the equation and in the current state.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This approach is that it will will require us to linearize our systems, which is annoying if we have none.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that should give you a general background.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to look at the proposed nonparametric dynamic programming approach.",
                    "label": 1
                },
                {
                    "sent": "And this approach uses 3 steps.",
                    "label": 0
                },
                {
                    "sent": "The first step is to actually model the system in a flexible nonparametric manner.",
                    "label": 0
                },
                {
                    "sent": "The second step is to determine what the form of the value function is for this kind of system.",
                    "label": 1
                },
                {
                    "sent": "And the third step is to actually determine, evaluate the policy, and determine the parameters for a value function.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to begin with, we're going to assume that our knowledge of the system comes from a set of N samples.",
                    "label": 1
                },
                {
                    "sent": "For each sample we have.",
                    "label": 0
                },
                {
                    "sent": "This takes an action at the next state as well as a reward.",
                    "label": 1
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we're going to do is we're going to model the joint distribution POSNS Dash.",
                    "label": 1
                },
                {
                    "sent": "So this is the probability of being in a state to performing an action and transition into the next state.",
                    "label": 0
                },
                {
                    "sent": "This joint distribution is important because it contains both our transition probability of the system as well as our policy.",
                    "label": 0
                },
                {
                    "sent": "A given S.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, the way that we represent this is in a male parametric manner is using kernel density estimation, so this is shown here where we place a kernel function on each of our samples and we decompose the kernel into three parts, one for the for the next dates.",
                    "label": 0
                },
                {
                    "sent": "When for the action and one for the current state.",
                    "label": 0
                },
                {
                    "sent": "When this implies, is that a single sample cannot define covariances between, for instance, the states in the next state, but multiple samples can.",
                    "label": 0
                },
                {
                    "sent": "And the good thing like using current density estimates is that there is a wide range of proofs or whatever kernel density estimates that do actually converge to the true distribution given sufficient number of samples.",
                    "label": 1
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the reward function, given that we have a kernel density estimate model the we can expect.",
                    "label": 0
                },
                {
                    "sent": "A another Watson reward function as shown here and you should notice that we actually have the same kernel functions in our reward function as we do in our kernel density estimate.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, given this model, we actually want to determine what is the form of our valued function and as member we we do this by determining what function form will be invariant underneath the Bellman equation.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we take our common equation and we rewrite it, we can get it in this form shown here.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, the actions have actually been integrated out at this point as part of being in all policy.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also notice that this version up here is actually constant, is independent of S and.",
                    "label": 0
                },
                {
                    "sent": "And this is actually thought of as the values for our transitions.",
                    "label": 0
                },
                {
                    "sent": "So what this should?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "US is that if we use again a number, I want some form for our value function.",
                    "label": 0
                },
                {
                    "sent": "If we put that form into the VS dash we will still get a natural Watson form for the VMS, which means that this is actually invariant underneath the Bellman equation and therefore represents the true form of the value function for this.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model.",
                    "label": 0
                },
                {
                    "sent": "So here we see it again, this time with the status and again once you notice about the kernel functions are the same ones as used for our kernel density estimate.",
                    "label": 0
                },
                {
                    "sent": "And in fact, this value function is completely defined by.",
                    "label": 1
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My model.",
                    "label": 0
                },
                {
                    "sent": "So for policy evaluation we need to compute the bigger parameters.",
                    "label": 1
                },
                {
                    "sent": "Are we do this using the computation show down here?",
                    "label": 0
                },
                {
                    "sent": "So either is a vector of the theater parameters are as the vector sample rewards and Lambda is a stochastic or a transition matrix with elements IJ given by this function on the right.",
                    "label": 0
                },
                {
                    "sent": "Now, this does require a matrix inversion.",
                    "label": 0
                },
                {
                    "sent": "However, it is only a sparse matrix version, which can be done relatively efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we see a summary of the.",
                    "label": 0
                },
                {
                    "sent": "Algorithm again, so as inputs we take samples where we have the states.",
                    "label": 0
                },
                {
                    "sent": "The next states in their wards.",
                    "label": 0
                },
                {
                    "sent": "I've left out the actions here as we don't expect them explicitly and we need kernel functions.",
                    "label": 0
                },
                {
                    "sent": "Biocide, although generally these will have the same form as there for the current state in the next state.",
                    "label": 0
                },
                {
                    "sent": "So using this model we can actually represent fairly complicated systems.",
                    "label": 0
                },
                {
                    "sent": "Given enough samples, the computations are has shown pretty.",
                    "label": 0
                },
                {
                    "sent": "Slide we can compute Lambda beta and then this data tell us the value function as shown at the bottom.",
                    "label": 0
                },
                {
                    "sent": "And these functions are will actually have a similar basis.",
                    "label": 0
                },
                {
                    "sent": "Then as the transitions of our system that allowed the observed transition transition services.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we're going to go look into a straightforward illustre Tori numerical value.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patient in particular, we're going to be looking at this system where we have a sinusoidal transition function, so this doesn't actually mean that the agent is going to go in a sinusoid, but rather that if it's going to move according to the plot on the left.",
                    "label": 0
                },
                {
                    "sent": "So for current given state, it's going to transition to the corresponding next state and see it's a fairly nonlinear dynamics.",
                    "label": 0
                },
                {
                    "sent": "And on the right we have effectively cosine, which tells us the current reward for the current stage on top of the transition of the sinusoidal part of the transition.",
                    "label": 0
                },
                {
                    "sent": "We also have some noise as indicated by the dashed lines.",
                    "label": 0
                },
                {
                    "sent": "To begin with, we want to actually find out what is the true form of the value function for this system, and we can do that using Monte Carlo methods.",
                    "label": 0
                },
                {
                    "sent": "So we start off by taking 500,000 samples of this trajectory.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we get this value function and as you can see it's symmetrical, which is what we expect.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to take only 100 samples and see what we get using the nonparametric dynamic programming.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approach and we get this.",
                    "label": 0
                },
                {
                    "sent": "So here you see, we've already gotten the rough shape of the value function.",
                    "label": 0
                },
                {
                    "sent": "There is still some offset and we have a mean squared error over the state state space of 3.8.",
                    "label": 0
                },
                {
                    "sent": "Three now.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Increases to 200 samples.",
                    "label": 0
                },
                {
                    "sent": "We already get a much better fit.",
                    "label": 0
                },
                {
                    "sent": "I'm still a little bit rough around the edges, but our mean squared error has gone down to 0.02.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you include 300, we get even closer and yeah.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as you can see.",
                    "label": 0
                },
                {
                    "sent": "We can quite accurately predict the true value function using only a limited number of samples, and these are some quite promising initial results.",
                    "label": 1
                },
                {
                    "sent": "So here we also see the model that was learned for the 300, so we don't explicitly need to compute this, but for illustration it's always nice to see it.",
                    "label": 0
                },
                {
                    "sent": "So on the left we have the kernel density estimate and on the right we have the approximation of the reward function.",
                    "label": 0
                },
                {
                    "sent": "Obviously, in the future we hope to implement this on a real robot, and to do that we will need some policy improvement.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There as well.",
                    "label": 0
                },
                {
                    "sent": "So that brings me to the conclusion of my talk, so I presented Ultrametric approach to dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "The main philosophy behind this approach was to try to approximate a value, a system and then determine the value function for that system, rather than to approximate the value function directly.",
                    "label": 1
                },
                {
                    "sent": "By using a non parametric approach we can model the system in a very flexible manner.",
                    "label": 1
                },
                {
                    "sent": "And afterwards we can precisely compute the value function for this model.",
                    "label": 1
                },
                {
                    "sent": "Well, the benefits of NPP is that it ensures that the basis functions actually match the observed dynamics, and as you saw we already have some positive results and hopefully in the future we will have some real robot evaluations.",
                    "label": 0
                },
                {
                    "sent": "I'd also like to point out that in the paper there's a second contribution which is a common framework for deriving not only nonparametric dynamic programming, but these squares, temporal difference learning and kernelized temporal difference.",
                    "label": 1
                },
                {
                    "sent": "Learning, such as Gaussian process, temporal difference, learning.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much and I look for your questions first time.",
                    "label": 0
                },
                {
                    "sent": "I was wondering about if you could tell us a few words about was the relationship between this work and the work by or Wellington Green kernel based reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So with kernel based reinforcement learning you also have this normalized kernel form right?",
                    "label": 0
                },
                {
                    "sent": "Yes, so I think the best way of describing the relationship that is actually.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look here so.",
                    "label": 0
                },
                {
                    "sent": "We have these sizes, size and the size so the distribution over the next steps.",
                    "label": 0
                },
                {
                    "sent": "If you change that into a Delta function then you should actually derive the same algorithm as this, so this is trying to sort of including more general approach to it.",
                    "label": 0
                }
            ]
        }
    }
}