{
    "id": "kl5tzugeddhj5rv6kmj7q4ck2e5vroli",
    "title": "Learning Graph Quantization",
    "info": {
        "author": [
            "Shankar Deepak Srinivasan, Neural Inforamtion Processing Group, TU Berlin"
        ],
        "published": "Sept. 13, 2010",
        "recorded": "August 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/ssspr2010_srinivasan_lgq/",
    "segmentation": [
        [
            "Good afternoon everybody.",
            "I would like to welcome you to."
        ],
        [
            "The first panel talk on learning graph quantization.",
            "First, look at the motivation of working with classifiers in the domain of attributed graphs.",
            "So attributed graphs is commonly used in structural pattern recognition to represent data there potentially very powerful means to represent data with a high descriptive power.",
            "Unlike for feature vectors.",
            "Because of the structural relationships are between the nodes that could be used to encode relationships or interdictor relationships quite efficiently.",
            "How about in many applications they are constrained by the fact that there are not many methods or algorithms for standard machine learning tasks like classification and clustering.",
            "So that means that classification or clustering of patterns represented by attributed graph is quite a fundamental problem in structural pattern recognition.",
            "So almost all the almost all the state of the art techniques somehow rely on embedding the graphs into a feature vector space and to carry out the classification or clustering tasks task that in because I mean the important question is how you embed them into a feature vector space.",
            "I hope when you do this embedding into feature vector of feature vector space because you cannot represent the relationship between the nodes and different structural components efficiently, this inevitably leads to loss of structural information and that means the projection or the reconstruction back in the structure or domain is not a trivial task.",
            "It's actually quite a phone."
        ],
        [
            "So we propose.",
            "A particular presentation of attributed graphs, not just of attributed graph, but almost all data that could be represented in a combinatorial structure which we just tentatively called it a structure spaces.",
            "So the objective is that we we achieve an isomorphic an isometric embedding of combinatorial structures such as attributed graphs of point patterns into into into a vector space without losing the structural information.",
            "So this so we do it in the formula that explains just now.",
            "So in attributed graph is nothing but a set of verticies.",
            "A set of edges and an attribute function, and it's completely described by its adjacency matrix X that lives in the space of some mattress is.",
            "And of course I mean the the edge of the attribute functions could map vertice is onto any attribute set and same with the edges.",
            "But of course I mean it would be much easier for just for theoretical info analysis analytical purposes if it were just a vector space, but in principle theory could could be applied to any attribute or any attribute space if you will and now consider the space of all N by N matrices, which actually is a vector space are present as EN by N. Let T denote a subgroup of all N by N permutation matrices, that's it's a group of all permutations that could act upon.",
            "The set of all NBA in Mattress is.",
            "Now to adjacency matrix is equivalent if there is a permutation or a re labeling of the notes that gives one graph with or one metrics with from the other an algebraically.",
            "That means that if there is a problem, if there is a permutation matrix P belonging to the set of all permutation matrix or belong to subgroup such that P transpose XP is equal to X dash, that means that these two adjacency matrix is equal and or in graph theoretic terms the two graphs are set to be isomorphic.",
            "So that means that you have a finite group.",
            "Basically group of automation actresses and you have a background space and any finite group that acts on a background space partitions the space into equivalence classes to disjoint equivalence classes.",
            "So in our case that means that the equivalence class of structures partition the underlying space into disjoint classes, which are called caution sets that they projecting every structure to its abstract representations.",
            "Now intuitively all the take home message from this slide is that for every structure.",
            "You have.",
            "You could abstract out the node, labelings the node ordering and then you could do.",
            "If you could obtain what's called a substrate structure and any two graphs that have just different node label is basically belong to the same abstract structure or belong."
        ],
        [
            "The same question set.",
            "So that means that with this representation of graphs that is mapped onto an abstract structure, which is actually a set, it's possible to define some sort of a graph distance measure.",
            "So graph distance measure is that would do I sense defined as a minimum of all possible representations of graph X that is generated by the structure and the same with respect to the other graph as well.",
            "And it's it's not a difficult task to show that this is indeed a metric.",
            "And equivalently, is also possible to defend some sort of a generalized inner product, maximum of all possible representations of the two structures.",
            "This behaves pretty much like the standard inner product for in real vector spaces, which measures the similarity between vectors.",
            "So this generalized inner product or structural dot product, if you can call it, measures the structural similarity between the graphs and computing the quantities.",
            "Is this conceptually easy?",
            "It's just you fix the representation or the ordering of 1 graph, and then you carry out.",
            "A possible all possible minimization or maximization of respective quantities over all possible representations of the other.",
            "Of course, that means that you have to run through all the possible permutation matrices.",
            "That means there is some sort of a graph matching problem, and that's the price to pay in terms of the computational complexity."
        ],
        [
            "So now to summarize we have we have the baby have mathematical representation of office space where all the graphs are all the commentary structures reside and in such a space we managed to define some sort of metric and this metric obviously gives rise to an alignment or permutation, and that permutation alignment is some sort of an optimal alignment.",
            "So that means we have notion of space and metric and alignment.",
            "I think this is all that we need to construct a learning graph quantization algorithm.",
            "So a learning graph quantization algorithm is an algorithm to construct classifier for set of attributed graphs, just like LB Q4 vectors.",
            "So a set of prototypes are chosen such that the source of the patterns are classified by means of your nearest prototype using the nearest prototype classification framework.",
            "Now, the advantages of this this kind of classification algorithm for attributed for activated glass is that it provides a very easy extent extension to multiclass problem.",
            "It's quite intuitive, it's it's easy to extend to multiclass problems and also in many applications prototypes act as typical class representatives that could be used for further applications, for example.",
            "You have if you have many images, that a person to this graphs, I mean so the prototype of all cars or prototypes of the set of all chairs would be some sort of protector which could be used for further human expert intervention."
        ],
        [
            "Yeah, so the objective is to construct the classifier C from the space of equal and space of groups from the from the equal and space of graphs such that it Maps to a class labels.",
            "Which are parameterized by K prototypes.",
            "Look up for parameters by K prototypes with class labels and our aim is to construct the prototypes such that they should test, predict the class labels of graphs from the space from the colon space of graphs.",
            "And forgot to put rip should be as far away from the decision surface as possible, so this is done by pulling the prototypes away from the incorrect label data and pushing them towards the entire towards the correct label data throughout the entire training set.",
            "So the algorithm is pretty much similar to what?"
        ],
        [
            "Dog Covenant has developed for learning for Vectors is a learning vector quantization algorithm.",
            "This is the elementary learning graph quantization which we just call it as learning graph quantization type one algorithm.",
            "You are given a training set as from from the space of graphs and the courtesy and product of the labels with the labeling information as well, Ellison label function and we choose the initial protests.",
            "It's of course I mean, in practice the choice of initial prototypes are the initialization of prototypes.",
            "Does seem to play a role in the final classification accuracy, so we suggest that we initialize the other prototypes, be initialized by some sort of hierarchical clustering procedure.",
            "Oh yeah, and do the following steps into convergence in that you just select a random training example from the training set you find out the nearest prototype.",
            "You align the data with the nearest or the other incoming pattern graph the nearest prototype an with some learning rate Nita you update according to the following rule you that is, you push the prototype towards the incoming data pattern.",
            "If the classification is correct and he pushed the prototypes away from the incorrect data from the incoming data point.",
            "If the classification is incorrect.",
            "And you repeat that over a few training cycles and the learning rate and the annealing procedure for the learning rate, or to be found in the paper."
        ],
        [
            "And we we validated the effectiveness of the learning graph quantization procedure over some data or some datasets from the AM graph data set repository, which was just three datasets.",
            "And I will spare the details because I think we're running out of time, but the results, the results, the discussion or to be found in the paper that's compatible.",
            "To the best of the state of the art techniques."
        ],
        [
            "But with a little bit of you with a few distinctiveness that I would like to point out now.",
            "OK, so the distinctiveness of the suggested approach?",
            "I mean, one is the 1st and the most important thing is that."
        ],
        [
            "I have a structure preserving representations of attributed graphs into a space.",
            "I mean, the ocean space is what this is the simplest example of what is called an orbifold.",
            "Again, I won't go into the details of that, but it just puts it in a mathematically sound framework and it could be referred to in the paper.",
            "So such an embedding of the graphs without losing their structural information into a space gives rise to an inherent definition of graph metric and an update rule.",
            "And that gives us a way to extend principle standard machine learning algorithms such as.",
            "I mean, we already have studied central clustering the domain of attributed graphs with this representations and some soft clustering algorithms as well.",
            "And now this forms learning graph quantization and we plan to extend it to probabilistic learning ref quantization and finally move towards generative models.",
            "So we have a principled extension of developing machine learning algorithms, at least for classification.",
            "And the domain of structural pattern recognition, more specifically for attributed class.",
            "And as I mentioned earlier, prototypes could act as typical class representatives for expert interpretation by for humans or by further software mobile computers in the case maybe, or to device better classification strategies.",
            "In fact, we demonstrate in a further in an upcoming work how these prototypes could be used to construct a better dissimilarity space for embedding graphs into a vector."
        ],
        [
            "And of course, the few open questions and some way ahead thoughts.",
            "The 1st is a computational costs.",
            "It's we're paying a huge price in terms of the graph matching operation, although we use an exact graph matching operations that has been quite well tuned.",
            "Running time is simply the algorithm just takes too much time, so any shortcuts to studying to reduce the number of graph matching operations is really worth the effort.",
            "And that would also be should be done in conjunction with exploring the geometry of the structure spaces.",
            "So we need to explore how the geometry of such a quotient spaces an we should.",
            "Could this geometry be exploited to do some computational shortcuts?",
            "Of course, I mean it should this relates to any faster, better graph matching algorithms out there.",
            "I mean, that's the whole question.",
            "And of course I mentioned earlier extend other NVQ type algorithms to attributed graphs.",
            "That is you could try to construct some learning quantization type algorithms with probabilistic models built in as well and.",
            "Dog that would be that would be the first step to developing some generative models.",
            "Of course I mean mathematically and theoretically it would be very interesting to look at the generalized theory for learning orbifolds.",
            "Just like general theory for learning over manifolds that we heard today.",
            "To establish some consistency reserves and so on.",
            "And of course I mean everything hinges upon the way the theory must be developed and tuned in conjunction with applications and applications depend on the theory, so it's some sort of chicken and egg problem, but I think I think that that's the way ahead.",
            "And have that girl."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good afternoon everybody.",
                    "label": 0
                },
                {
                    "sent": "I would like to welcome you to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first panel talk on learning graph quantization.",
                    "label": 1
                },
                {
                    "sent": "First, look at the motivation of working with classifiers in the domain of attributed graphs.",
                    "label": 0
                },
                {
                    "sent": "So attributed graphs is commonly used in structural pattern recognition to represent data there potentially very powerful means to represent data with a high descriptive power.",
                    "label": 1
                },
                {
                    "sent": "Unlike for feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Because of the structural relationships are between the nodes that could be used to encode relationships or interdictor relationships quite efficiently.",
                    "label": 0
                },
                {
                    "sent": "How about in many applications they are constrained by the fact that there are not many methods or algorithms for standard machine learning tasks like classification and clustering.",
                    "label": 1
                },
                {
                    "sent": "So that means that classification or clustering of patterns represented by attributed graph is quite a fundamental problem in structural pattern recognition.",
                    "label": 0
                },
                {
                    "sent": "So almost all the almost all the state of the art techniques somehow rely on embedding the graphs into a feature vector space and to carry out the classification or clustering tasks task that in because I mean the important question is how you embed them into a feature vector space.",
                    "label": 0
                },
                {
                    "sent": "I hope when you do this embedding into feature vector of feature vector space because you cannot represent the relationship between the nodes and different structural components efficiently, this inevitably leads to loss of structural information and that means the projection or the reconstruction back in the structure or domain is not a trivial task.",
                    "label": 0
                },
                {
                    "sent": "It's actually quite a phone.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we propose.",
                    "label": 0
                },
                {
                    "sent": "A particular presentation of attributed graphs, not just of attributed graph, but almost all data that could be represented in a combinatorial structure which we just tentatively called it a structure spaces.",
                    "label": 0
                },
                {
                    "sent": "So the objective is that we we achieve an isomorphic an isometric embedding of combinatorial structures such as attributed graphs of point patterns into into into a vector space without losing the structural information.",
                    "label": 1
                },
                {
                    "sent": "So this so we do it in the formula that explains just now.",
                    "label": 0
                },
                {
                    "sent": "So in attributed graph is nothing but a set of verticies.",
                    "label": 0
                },
                {
                    "sent": "A set of edges and an attribute function, and it's completely described by its adjacency matrix X that lives in the space of some mattress is.",
                    "label": 1
                },
                {
                    "sent": "And of course I mean the the edge of the attribute functions could map vertice is onto any attribute set and same with the edges.",
                    "label": 0
                },
                {
                    "sent": "But of course I mean it would be much easier for just for theoretical info analysis analytical purposes if it were just a vector space, but in principle theory could could be applied to any attribute or any attribute space if you will and now consider the space of all N by N matrices, which actually is a vector space are present as EN by N. Let T denote a subgroup of all N by N permutation matrices, that's it's a group of all permutations that could act upon.",
                    "label": 0
                },
                {
                    "sent": "The set of all NBA in Mattress is.",
                    "label": 0
                },
                {
                    "sent": "Now to adjacency matrix is equivalent if there is a permutation or a re labeling of the notes that gives one graph with or one metrics with from the other an algebraically.",
                    "label": 0
                },
                {
                    "sent": "That means that if there is a problem, if there is a permutation matrix P belonging to the set of all permutation matrix or belong to subgroup such that P transpose XP is equal to X dash, that means that these two adjacency matrix is equal and or in graph theoretic terms the two graphs are set to be isomorphic.",
                    "label": 0
                },
                {
                    "sent": "So that means that you have a finite group.",
                    "label": 0
                },
                {
                    "sent": "Basically group of automation actresses and you have a background space and any finite group that acts on a background space partitions the space into equivalence classes to disjoint equivalence classes.",
                    "label": 0
                },
                {
                    "sent": "So in our case that means that the equivalence class of structures partition the underlying space into disjoint classes, which are called caution sets that they projecting every structure to its abstract representations.",
                    "label": 1
                },
                {
                    "sent": "Now intuitively all the take home message from this slide is that for every structure.",
                    "label": 0
                },
                {
                    "sent": "You have.",
                    "label": 0
                },
                {
                    "sent": "You could abstract out the node, labelings the node ordering and then you could do.",
                    "label": 0
                },
                {
                    "sent": "If you could obtain what's called a substrate structure and any two graphs that have just different node label is basically belong to the same abstract structure or belong.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The same question set.",
                    "label": 0
                },
                {
                    "sent": "So that means that with this representation of graphs that is mapped onto an abstract structure, which is actually a set, it's possible to define some sort of a graph distance measure.",
                    "label": 0
                },
                {
                    "sent": "So graph distance measure is that would do I sense defined as a minimum of all possible representations of graph X that is generated by the structure and the same with respect to the other graph as well.",
                    "label": 0
                },
                {
                    "sent": "And it's it's not a difficult task to show that this is indeed a metric.",
                    "label": 1
                },
                {
                    "sent": "And equivalently, is also possible to defend some sort of a generalized inner product, maximum of all possible representations of the two structures.",
                    "label": 1
                },
                {
                    "sent": "This behaves pretty much like the standard inner product for in real vector spaces, which measures the similarity between vectors.",
                    "label": 0
                },
                {
                    "sent": "So this generalized inner product or structural dot product, if you can call it, measures the structural similarity between the graphs and computing the quantities.",
                    "label": 0
                },
                {
                    "sent": "Is this conceptually easy?",
                    "label": 0
                },
                {
                    "sent": "It's just you fix the representation or the ordering of 1 graph, and then you carry out.",
                    "label": 0
                },
                {
                    "sent": "A possible all possible minimization or maximization of respective quantities over all possible representations of the other.",
                    "label": 1
                },
                {
                    "sent": "Of course, that means that you have to run through all the possible permutation matrices.",
                    "label": 0
                },
                {
                    "sent": "That means there is some sort of a graph matching problem, and that's the price to pay in terms of the computational complexity.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now to summarize we have we have the baby have mathematical representation of office space where all the graphs are all the commentary structures reside and in such a space we managed to define some sort of metric and this metric obviously gives rise to an alignment or permutation, and that permutation alignment is some sort of an optimal alignment.",
                    "label": 0
                },
                {
                    "sent": "So that means we have notion of space and metric and alignment.",
                    "label": 0
                },
                {
                    "sent": "I think this is all that we need to construct a learning graph quantization algorithm.",
                    "label": 0
                },
                {
                    "sent": "So a learning graph quantization algorithm is an algorithm to construct classifier for set of attributed graphs, just like LB Q4 vectors.",
                    "label": 1
                },
                {
                    "sent": "So a set of prototypes are chosen such that the source of the patterns are classified by means of your nearest prototype using the nearest prototype classification framework.",
                    "label": 1
                },
                {
                    "sent": "Now, the advantages of this this kind of classification algorithm for attributed for activated glass is that it provides a very easy extent extension to multiclass problem.",
                    "label": 0
                },
                {
                    "sent": "It's quite intuitive, it's it's easy to extend to multiclass problems and also in many applications prototypes act as typical class representatives that could be used for further applications, for example.",
                    "label": 0
                },
                {
                    "sent": "You have if you have many images, that a person to this graphs, I mean so the prototype of all cars or prototypes of the set of all chairs would be some sort of protector which could be used for further human expert intervention.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so the objective is to construct the classifier C from the space of equal and space of groups from the from the equal and space of graphs such that it Maps to a class labels.",
                    "label": 0
                },
                {
                    "sent": "Which are parameterized by K prototypes.",
                    "label": 0
                },
                {
                    "sent": "Look up for parameters by K prototypes with class labels and our aim is to construct the prototypes such that they should test, predict the class labels of graphs from the space from the colon space of graphs.",
                    "label": 1
                },
                {
                    "sent": "And forgot to put rip should be as far away from the decision surface as possible, so this is done by pulling the prototypes away from the incorrect label data and pushing them towards the entire towards the correct label data throughout the entire training set.",
                    "label": 1
                },
                {
                    "sent": "So the algorithm is pretty much similar to what?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dog Covenant has developed for learning for Vectors is a learning vector quantization algorithm.",
                    "label": 1
                },
                {
                    "sent": "This is the elementary learning graph quantization which we just call it as learning graph quantization type one algorithm.",
                    "label": 1
                },
                {
                    "sent": "You are given a training set as from from the space of graphs and the courtesy and product of the labels with the labeling information as well, Ellison label function and we choose the initial protests.",
                    "label": 1
                },
                {
                    "sent": "It's of course I mean, in practice the choice of initial prototypes are the initialization of prototypes.",
                    "label": 0
                },
                {
                    "sent": "Does seem to play a role in the final classification accuracy, so we suggest that we initialize the other prototypes, be initialized by some sort of hierarchical clustering procedure.",
                    "label": 1
                },
                {
                    "sent": "Oh yeah, and do the following steps into convergence in that you just select a random training example from the training set you find out the nearest prototype.",
                    "label": 1
                },
                {
                    "sent": "You align the data with the nearest or the other incoming pattern graph the nearest prototype an with some learning rate Nita you update according to the following rule you that is, you push the prototype towards the incoming data pattern.",
                    "label": 0
                },
                {
                    "sent": "If the classification is correct and he pushed the prototypes away from the incorrect data from the incoming data point.",
                    "label": 0
                },
                {
                    "sent": "If the classification is incorrect.",
                    "label": 0
                },
                {
                    "sent": "And you repeat that over a few training cycles and the learning rate and the annealing procedure for the learning rate, or to be found in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we we validated the effectiveness of the learning graph quantization procedure over some data or some datasets from the AM graph data set repository, which was just three datasets.",
                    "label": 1
                },
                {
                    "sent": "And I will spare the details because I think we're running out of time, but the results, the results, the discussion or to be found in the paper that's compatible.",
                    "label": 0
                },
                {
                    "sent": "To the best of the state of the art techniques.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But with a little bit of you with a few distinctiveness that I would like to point out now.",
                    "label": 0
                },
                {
                    "sent": "OK, so the distinctiveness of the suggested approach?",
                    "label": 0
                },
                {
                    "sent": "I mean, one is the 1st and the most important thing is that.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have a structure preserving representations of attributed graphs into a space.",
                    "label": 1
                },
                {
                    "sent": "I mean, the ocean space is what this is the simplest example of what is called an orbifold.",
                    "label": 1
                },
                {
                    "sent": "Again, I won't go into the details of that, but it just puts it in a mathematically sound framework and it could be referred to in the paper.",
                    "label": 0
                },
                {
                    "sent": "So such an embedding of the graphs without losing their structural information into a space gives rise to an inherent definition of graph metric and an update rule.",
                    "label": 1
                },
                {
                    "sent": "And that gives us a way to extend principle standard machine learning algorithms such as.",
                    "label": 1
                },
                {
                    "sent": "I mean, we already have studied central clustering the domain of attributed graphs with this representations and some soft clustering algorithms as well.",
                    "label": 0
                },
                {
                    "sent": "And now this forms learning graph quantization and we plan to extend it to probabilistic learning ref quantization and finally move towards generative models.",
                    "label": 1
                },
                {
                    "sent": "So we have a principled extension of developing machine learning algorithms, at least for classification.",
                    "label": 0
                },
                {
                    "sent": "And the domain of structural pattern recognition, more specifically for attributed class.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned earlier, prototypes could act as typical class representatives for expert interpretation by for humans or by further software mobile computers in the case maybe, or to device better classification strategies.",
                    "label": 1
                },
                {
                    "sent": "In fact, we demonstrate in a further in an upcoming work how these prototypes could be used to construct a better dissimilarity space for embedding graphs into a vector.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of course, the few open questions and some way ahead thoughts.",
                    "label": 1
                },
                {
                    "sent": "The 1st is a computational costs.",
                    "label": 0
                },
                {
                    "sent": "It's we're paying a huge price in terms of the graph matching operation, although we use an exact graph matching operations that has been quite well tuned.",
                    "label": 0
                },
                {
                    "sent": "Running time is simply the algorithm just takes too much time, so any shortcuts to studying to reduce the number of graph matching operations is really worth the effort.",
                    "label": 0
                },
                {
                    "sent": "And that would also be should be done in conjunction with exploring the geometry of the structure spaces.",
                    "label": 1
                },
                {
                    "sent": "So we need to explore how the geometry of such a quotient spaces an we should.",
                    "label": 0
                },
                {
                    "sent": "Could this geometry be exploited to do some computational shortcuts?",
                    "label": 0
                },
                {
                    "sent": "Of course, I mean it should this relates to any faster, better graph matching algorithms out there.",
                    "label": 1
                },
                {
                    "sent": "I mean, that's the whole question.",
                    "label": 0
                },
                {
                    "sent": "And of course I mentioned earlier extend other NVQ type algorithms to attributed graphs.",
                    "label": 1
                },
                {
                    "sent": "That is you could try to construct some learning quantization type algorithms with probabilistic models built in as well and.",
                    "label": 0
                },
                {
                    "sent": "Dog that would be that would be the first step to developing some generative models.",
                    "label": 0
                },
                {
                    "sent": "Of course I mean mathematically and theoretically it would be very interesting to look at the generalized theory for learning orbifolds.",
                    "label": 0
                },
                {
                    "sent": "Just like general theory for learning over manifolds that we heard today.",
                    "label": 0
                },
                {
                    "sent": "To establish some consistency reserves and so on.",
                    "label": 0
                },
                {
                    "sent": "And of course I mean everything hinges upon the way the theory must be developed and tuned in conjunction with applications and applications depend on the theory, so it's some sort of chicken and egg problem, but I think I think that that's the way ahead.",
                    "label": 1
                },
                {
                    "sent": "And have that girl.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}