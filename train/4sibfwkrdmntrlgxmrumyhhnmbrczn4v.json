{
    "id": "4sibfwkrdmntrlgxmrumyhhnmbrczn4v",
    "title": "Active Learning for Imitation",
    "info": {
        "author": [
            "Manuel Lopes, School of Computing and Mathematics, University of Plymouth"
        ],
        "published": "Nov. 8, 2010",
        "recorded": "June 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning"
        ]
    },
    "url": "http://videolectures.net/rss2010_lopes_ali/",
    "segmentation": [
        [
            "I'm going to talk about active learning for imitation.",
            "I'm going to show now I'm universal Plymouth.",
            "I'm going to present results that I did with some of them with Ruben.",
            "Some of them would lose money assignments with Francisco Mel."
        ],
        [
            "And so the objectives of the talk is is to give some perspective on imitation at several several ways.",
            "So show some techniques for imitation, but in show imitation for several goals like motor learning, but also test learning and social learning.",
            "And of course, as the topic of this workshop, we'll see how some of the approaches can be generalized for an active learning context."
        ],
        [
            "So imitation in robotics has been used as an intuitive way to program them, so a person can just do a gesture and the robot should be able to learn something from this gesture.",
            "So I'm I'm interested not only on using the demonstration to see how to solve a task, but I'm I'm considering also learning what the test case, so if you just have a motion you don't know what you want to do with the.",
            "With the robot in that environment, you should be able to extract what is the goal of the task, not just how to solve it.",
            "So and then this would allow users to program the robot to perform very different tasks even in the same environment.",
            "Either you can program it to just put all the bottles in the same place, or just to spread them in a nice shape you want.",
            "So as a topic for this workshop.",
            "So typically in learning by imitation, you need very long demonstrations, and typically you give as much information as you can so that you can disambiguate the goal of the demonstration, or try to.",
            "To incorporate as much knowledge as you can, so the demonstration is not really sure what is the most informative sample that it can provide to the robot.",
            "So we want the robot to ask the sampled it needs to improve its learning."
        ],
        [
            "So you can.",
            "You can use imitation to like to practice motor skills to learn several motor skills."
        ],
        [
            "You can, you can use imitation to program a user to do some task and then you can see that the motion and the goal.",
            "It's it's different, so the same motion might mean Ello or might be scratch my back."
        ],
        [
            "And.",
            "So I can use it for very complex to continuous and discrete tasks."
        ],
        [
            "But there's also another very important point is that imitation is something that is important for your social acceptance and your social learning and how to learn how to behave with the others."
        ],
        [
            "So this deadline of my talk so maybe thoughts imitation.",
            "So talk about approaches to imitation and mainly about inverse reinforcement learning.",
            "Then we go to the Beijing settings so that we can have posteriors to be able to do active, and then I'll go for active inverse reinforcement learning, and so some ideas we have been developing to try to combine inverse reinforcement learning and supervised learning."
        ],
        [
            "An imitation.",
            "So one thing that we have to consider that in imitation we have to infer what is the goal of the other, and so we will be influenced by several."
        ],
        [
            "Knowledge, so first there's some experiments from biology that that show us that's what the motion it itself is.",
            "Just the motion doesn't give all the information.",
            "For instance, there's this famous experiment where someone touches the the the table with the head, and there's a light switch here.",
            "And when you touch it, it turns the light on and you can do this either with your hands free or read our ends restricted.",
            "And what happens with children is that if you do in this situation.",
            "Children will imitate through the head.",
            "If you do in this situation you imitate with the hand, so that's what children do.",
            "So it means that here you infer that the person can use the hand or can use the head.",
            "So we choose the head.",
            "It might there's some reason for that it was a choice between two options.",
            "Here there's no choice, so you infer that the goal might be just to touch the delight and and you just did this because you couldn't use your hand, so you imitate as the most efficient way.",
            "So the difference the number of available options change what is inferred as the goal of the test."
        ],
        [
            "So any you can see that if you try to understand someone approaches a light switch and turns the light switch with an E. And you can do this either with your ends free or your ends with some folders and you'll see that the way your brain interprets this action.",
            "And his actions different.",
            "So here the motor resonance mechanisms can understand this.",
            "And here you cannot understand these do test restrictions.",
            "So we have to do is to use arena zoning and areas of low brain to understand this."
        ],
        [
            "OK, and you can see that for instance, if you don't know our box works.",
            "So in this task you can put a.",
            "You demonstrate someone putting a stick on the top.",
            "Container any front container and if you ask and you can do this in the transparent condition that you understand that the top container is useless because the food is in the front container.",
            "Or you can use in the OPEC condition and you see that here the chips will just go directly to weather for this.",
            "And here they will do the useless action.",
            "Also because the knowledge about the world.",
            "You have to work change watching info."
        ],
        [
            "And it's interesting.",
            "Interesting that children will do the less efficient imitation than chimps, because maybe the social drive they have to act as the others is more important than getting the food."
        ],
        [
            "OK, so the knowledge about the world and considerations about contextual restrictions will change your behavior in our algorithms of imitation will will be left."
        ],
        [
            "Deal with this.",
            "OK, so in Roberts what is copied so many different people as as approach limitation in different contexts and they they might be copying different things.",
            "OK, either the joint level trajectory, stats level trajectories, but maybe only the final state is being copied.",
            "Maybe only state transitions and so."
        ],
        [
            "Several approaches can can be, so I would divide the approach for imitation.",
            "These three main main areas, so you could just copy the goal you see.",
            "What's the final states of the world, and then you just plan out to get there by your own means.",
            "You can use more or less supervised learning where you where we have a policy and you try to learn directly the policy with some generalization capabilities.",
            "Of course an you you might have some things like inverse.",
            "Women wearing for the criteria behind the demonstration.",
            "So this is the most, let's say, the most efficient one.",
            "This might be the one that's less better generation capabilities, but just be careful with this,"
        ],
        [
            "OK, so in in in for the first reinforcement learning we are in the in the standard Markov decision process where we have a model of the world.",
            "We have a reward function and you want to Max to choose a policy that maximizes the value function you want to choose our actions so that we get as much reviewer."
        ],
        [
            "This weekend so this is a typical reinforcement learning problem of dynamic programming.",
            "We have a model of the world.",
            "You have a reward function, any computer policy.",
            "Universal reinforcement learning.",
            "You have different settings, so you have.",
            "The model of the world you have samples from the policy and you want to infer the reward function.",
            "So we want to infer what is the goal of the task, not how to solve the desk, but what what you want to do."
        ],
        [
            "So this is a very well defined problem, so because one reward might give multiple a season, one policy might give multiple rewards and having complete demonstrations are impractical, so we will try to go from this standard in first reformed setting and generalize it to the active."
        ],
        [
            "I think.",
            "So the original formulation from him in 90.",
            "9 or 2000 it was was very, very clear.",
            "So you just start with your your Bellman equation.",
            "You just rewrite it as a matrix notation.",
            "You have your value function is Y minus gamma.",
            "The transition probability times the reward.",
            "And then of course, your reward is.",
            "This is this function?",
            "OK, so you just have to find a reward."
        ],
        [
            "It's.",
            "So if you have samples, so for instance, Action A is demonstrating in state X, you know the value of action A in state X is bigger than the values of for all.",
            "Then all the other actions.",
            "OK, if you put this in matrix notation, you get this condition OK, and so this is the problem that you have to solve to solve the IRL problem.",
            "You have to find the reward function that that achieves these restrictions."
        ],
        [
            "So one problem is that we have to see if it's generalizes, and then if you can sample it inefficient way.",
            "So if you have a problem with then state Sen M actions and you if you demonstrate all the states, you'll see that you'll have N * N conditions.",
            "And of course, this is much more conditions that all the end possible linear independent conditions that we have.",
            "So it's it means that we don't need to visit all the states.",
            "So you have to have the policy completely completely defined."
        ],
        [
            "So one basic method would be just to to.",
            "To check in on visited state, see from the transition probabilities.",
            "If this gives a linear independent condition and if it does, you ask a sample on this state.",
            "If it doesn't, you don't ask a sample on this state an in this with the construction, you'll see that you will not visit all the states as a more."
        ],
        [
            "A clear example.",
            "You can see that for instance, in this world model, if someone tells you that in this state this action is optimal, you'll see it means that a is better than B.",
            "You'll see of course, that's indeed.",
            "This action is also better than this because you know that this state is better than this state.",
            "So from that previous algorithm we see that we just need to visit one state in this problem."
        ],
        [
            "And then if you if you keep doing this, you see that for instance, for this problem at least four states you need to visit two states, so it doesn't depend on the number of states depends on the dynamics of the world."
        ],
        [
            "And if you have like agreed Lauralee, you need to visit N -- N * N -- 1 states."
        ],
        [
            "So this is just a professional salary, so it will show that we can generalize and you can provide an efficient sampling of the problem, but you cannot of course deal with general transition matrix and noisy demonstrations.",
            "That's what we have to solve now."
        ],
        [
            "So if you if you put this in a Bayesian setting, we have a demonstration again with States and actions we have to compute.",
            "If you have a price distribution of the rewards, we have to compute the likelihoods of a demonstration is like like a Boltzmann policy gives the likelihoods and to compute the reward you compute with using Monte Carlo methods the posterior over the rewards given that policy."
        ],
        [
            "So for instance.",
            "So if you have a problem someone is doing is doing object manipulation task.",
            "And so you don't know what is the goal of this task.",
            "So someone is displaying with objects around you don't know what the objects are, so we know for some states that the demonstration was this.",
            "You wanted to know what to do in the other states."
        ],
        [
            "OK, so you can see that using this by universal reinforcement learning, we can actually compute the optimal policy for states that were not demonstrated.",
            "For instance, this state was not demonstrated and you learn the correct policy here.",
            "There was some noise and the demonstration was wrong, and even then we could compute the correct action and you see that here the evolution of the noise we have in the demonstration and the policy error.",
            "So even if you have like.",
            "15% or 20% to recognition errors.",
            "We can even then compute the optimal policy for this for this task."
        ],
        [
            "OK, and of course the robot is then able to to execute the correct test given for states that were not demonstrated."
        ],
        [
            "We can see that going back to these experiments in psychology we can see that.",
            "We can see that some of the behaviors we see in biology this this algorithm also."
        ],
        [
            "The achieves this, for instance, for this experiment, if you try to model it will see that in the end three teacher, you'll see that you will imitate much more than when the restricted you have restricted teacher.",
            "So we can see that again the data written takes into account the task wrist."
        ],
        [
            "Options that we see and for the non box as we don't know the model of the world again we can match the empirical expn."
        ],
        [
            "Payments.",
            "OK, final, let's go for the active set."
        ],
        [
            "I think so.",
            "We we we we need a measure of the uncertainty, new policy estimation, and so we'll use these uncertainties to choose the best sites to ask for demonstration.",
            "So in IRL, then certainty is measured in terms of the rewards, and of course we have to propagate this reward."
        ],
        [
            "The policy.",
            "Came so we have to estimate the posterior of the rewards and then for each state we compute the entropy of the policy and then we ask.",
            "A demonstration on this."
        ],
        [
            "It.",
            "Hey so forgiven state, we compute the entropy of the of the policy.",
            "And then that's that's the information we used to ask a demonstration as we have.",
            "So we have samples for samples of the reward, and then we compute the policy for its reward and that's."
        ],
        [
            "Information we use.",
            "OK, as an example, so take into account that."
        ],
        [
            "So if you want to find the maximum of a function and you have an initial demonstration that says that here you should go right and you should go left, you compute the reward function so you see that the maximum isn't going to be here, because here you want to go to the right.",
            "You're going to the left, so the maximum should be here.",
            "So you're met with our method, which is going to ask a demonstration here.",
            "That's where we have higher and certainty in the policy, and then you see that the samples of the rewards get shifted to the right and then you ask more demonstrations and then you can.",
            "Correctly estimated the reward function."
        ],
        [
            "OK, I'm gonna run."
        ],
        [
            "Out of time, so and so if you consider problems with much higher dimensions.",
            "So for instance for two understates and you consider like some random problems and some problems with more."
        ],
        [
            "Less structure you can see.",
            "Can you see it?",
            "So you can see that the well so the random this is random strategy.",
            "So this is the policy loss.",
            "This is the size of the demonstration, and you see that the active learning is.",
            "Is able to learn much faster than using the random than the random approach for a problem with about 200 states."
        ],
        [
            "So the the size of demonstration continues reduced using active learning techniques and again depends will be will depend on the world dynamics on and on the reward stretch."
        ],
        [
            "How much end ref.",
            "10 minutes so excited.",
            "So another idea to use active learning for demonstration is going to try to use to combine.",
            "Supervised techniques and if."
        ],
        [
            "First inverse learning techniques so.",
            "So because of the sampling in the inverse reinforcement learning the destination is very is can become very slow.",
            "That's why we are limited in the number of states we can.",
            "We can deal and when using supervised learning things are very fast with all the techniques we have for regression.",
            "So the idea is try to combine both when I need to combine both is try to embed the MDP structure in supervised learning machine.",
            "So if user kernel method we can.",
            "Use the Take the MVP structure to a kernel.",
            "Then we can use.",
            "Supervised learning techniques."
        ],
        [
            "So there are several metrics for MDP's.",
            "For instance, if you have this this problem that you have to reach this goal and you have these two states, you see that due to the symmetries this state is equivalent to this state.",
            "OK, so you have the same distance to the goal an if you change names of the actions you can get to the goal in the same number of steps.",
            "So this state and these are equivalent to take into account.",
            "Symmetric, so if the next state is the same, two states are similar.",
            "OK, so for if from two different states we get to the same states, these two states are equivalent."
        ],
        [
            "OK, so if you define an MDP metric we get the distance between state action pairs from this week and we have a kernel.",
            "You can acquire demonstration and then if we fit.",
            "If you learn with a regression, we we we learn a policy that takes into account the dynamics of the world."
        ],
        [
            "In the sons way.",
            "Of course, if you have a problem that is like very like a navigation problem, maybe just the kernel, Gaussian kernel is enough and and you can learn directly the policy.",
            "But if you have discrete states combined with continuous, of course the the trivial kernel will not.",
            "Will not be the."
        ],
        [
            "The correct one.",
            "So to compute the kernel we have, we have to compute distance between distributions.",
            "You can use the earth movers distance to compute distance between distributions, so but this just gives the distance between 2."
        ],
        [
            "Dates.",
            "And of course, then we have to take into account long-term.",
            "Dynamics of the world.",
            "And for this there's there's other techniques so.",
            "We take into account so the counter of each distance between 2:00 to state action pairs and then to take into account the long term.",
            "The situation we have to find the fixed point of the outdoor distance metric and so you see that for that case I showed you will be able to see that these two states are equivalent."
        ],
        [
            "OK, so in terms of results you can see that so far this is a problem of around understates.",
            "You see that.",
            "Then if you do, this is the inverse reinforcement.",
            "Learning the generalization capability.",
            "So this is the size of demonstration.",
            "This will be a complete demonstration.",
            "You see that you have.",
            "You have a very high baseline, so you have a good estimation of policy since the beginning.",
            "Because of the structure of the MDP, and then you can grow.",
            "And then you can learn the policy so you don't get 200% due to the local minima you have in gradient methods.",
            "And using this new technique you can you can see that you you you can learn much faster and you can get a better final result.",
            "OK, so this would be the result you'd get using just a trivial kernel like takes into account like the distance, degree distance between states."
        ],
        [
            "OK, and then you see that you are also robust with the noise."
        ],
        [
            "So to take into account and active extension we have to learn the policy with we use a multinomial distribution.",
            "And then to find the posterior of the policy.",
            "And then we choose the states again.",
            "The states with higher variance.",
            "Any request these states?"
        ],
        [
            "Demonstration OK, and then you see that again the active learning.",
            "So this is the correct classification rate for the correct actions.",
            "This is the active learning approach.",
            "This would be just a random approach and this is.",
            "Just making sure that you visit all the states in a random order.",
            "OK, you see that you can get again of 10 to 20% in the classification, right?",
            "Using nine active learning approach, this is without noise and this is if you have errors in the demonstration.",
            "But even then the active learning approach can give good results."
        ],
        [
            "So we this is if we we use MDP induced metrics we can learn you can get a kernel that gives good generalization capabilities for learning by demonstration setting.",
            "So the kernel doesn't depend on demonstration, so it's.",
            "We could get better results if you would do this dependence, But then we would have to use sampling methods and you'd go back to the IRL problem.",
            "So the computing the kernel is a bit costly, but then for at learning time it's almost instantaneously those due to the regression techniques.",
            "And of course there's still some initialization of the ground distance that might impact the results.",
            "So now we have to maybe have approximated methods to be able to compute the kernels for very high dimensions and you have to be able to generalize this to continuous domains."
        ],
        [
            "So here we consider the case where we can synthesize a sample so the robot can ask.",
            "I want to sample in this state or this state.",
            "So and this might be a problem because getting to that state might be difficult and maybe some the best thing would be to compute the optimal path.",
            "So asking a path for the demonstration and not just one state.",
            "Maybe this would be better for control."
        ],
        [
            "Whoops terms of conclusions.",
            "We show that we can generalize that inverse reinforcement learning to an active learning setting that we can handle hundreds of states.",
            "We we can indeed reduce the number of samples required to get a good estimation.",
            "And some limitations.",
            "We know that the prior knowledge will impact the results.",
            "But typically the active it's never worse than than random.",
            "Maybe sometimes it just.",
            "Is equal to run depends on the on the dynamics of the world, and we're trying to to try to make a unified view of inverse reinforcement learning and regression based techniques for."
        ],
        [
            "For imitation.",
            "OK, some references.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to talk about active learning for imitation.",
                    "label": 1
                },
                {
                    "sent": "I'm going to show now I'm universal Plymouth.",
                    "label": 0
                },
                {
                    "sent": "I'm going to present results that I did with some of them with Ruben.",
                    "label": 0
                },
                {
                    "sent": "Some of them would lose money assignments with Francisco Mel.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so the objectives of the talk is is to give some perspective on imitation at several several ways.",
                    "label": 1
                },
                {
                    "sent": "So show some techniques for imitation, but in show imitation for several goals like motor learning, but also test learning and social learning.",
                    "label": 0
                },
                {
                    "sent": "And of course, as the topic of this workshop, we'll see how some of the approaches can be generalized for an active learning context.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So imitation in robotics has been used as an intuitive way to program them, so a person can just do a gesture and the robot should be able to learn something from this gesture.",
                    "label": 1
                },
                {
                    "sent": "So I'm I'm interested not only on using the demonstration to see how to solve a task, but I'm I'm considering also learning what the test case, so if you just have a motion you don't know what you want to do with the.",
                    "label": 1
                },
                {
                    "sent": "With the robot in that environment, you should be able to extract what is the goal of the task, not just how to solve it.",
                    "label": 0
                },
                {
                    "sent": "So and then this would allow users to program the robot to perform very different tasks even in the same environment.",
                    "label": 0
                },
                {
                    "sent": "Either you can program it to just put all the bottles in the same place, or just to spread them in a nice shape you want.",
                    "label": 0
                },
                {
                    "sent": "So as a topic for this workshop.",
                    "label": 0
                },
                {
                    "sent": "So typically in learning by imitation, you need very long demonstrations, and typically you give as much information as you can so that you can disambiguate the goal of the demonstration, or try to.",
                    "label": 0
                },
                {
                    "sent": "To incorporate as much knowledge as you can, so the demonstration is not really sure what is the most informative sample that it can provide to the robot.",
                    "label": 0
                },
                {
                    "sent": "So we want the robot to ask the sampled it needs to improve its learning.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "You can use imitation to like to practice motor skills to learn several motor skills.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can, you can use imitation to program a user to do some task and then you can see that the motion and the goal.",
                    "label": 0
                },
                {
                    "sent": "It's it's different, so the same motion might mean Ello or might be scratch my back.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So I can use it for very complex to continuous and discrete tasks.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there's also another very important point is that imitation is something that is important for your social acceptance and your social learning and how to learn how to behave with the others.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this deadline of my talk so maybe thoughts imitation.",
                    "label": 0
                },
                {
                    "sent": "So talk about approaches to imitation and mainly about inverse reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "Then we go to the Beijing settings so that we can have posteriors to be able to do active, and then I'll go for active inverse reinforcement learning, and so some ideas we have been developing to try to combine inverse reinforcement learning and supervised learning.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An imitation.",
                    "label": 0
                },
                {
                    "sent": "So one thing that we have to consider that in imitation we have to infer what is the goal of the other, and so we will be influenced by several.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Knowledge, so first there's some experiments from biology that that show us that's what the motion it itself is.",
                    "label": 0
                },
                {
                    "sent": "Just the motion doesn't give all the information.",
                    "label": 0
                },
                {
                    "sent": "For instance, there's this famous experiment where someone touches the the the table with the head, and there's a light switch here.",
                    "label": 0
                },
                {
                    "sent": "And when you touch it, it turns the light on and you can do this either with your hands free or read our ends restricted.",
                    "label": 0
                },
                {
                    "sent": "And what happens with children is that if you do in this situation.",
                    "label": 0
                },
                {
                    "sent": "Children will imitate through the head.",
                    "label": 0
                },
                {
                    "sent": "If you do in this situation you imitate with the hand, so that's what children do.",
                    "label": 0
                },
                {
                    "sent": "So it means that here you infer that the person can use the hand or can use the head.",
                    "label": 0
                },
                {
                    "sent": "So we choose the head.",
                    "label": 0
                },
                {
                    "sent": "It might there's some reason for that it was a choice between two options.",
                    "label": 0
                },
                {
                    "sent": "Here there's no choice, so you infer that the goal might be just to touch the delight and and you just did this because you couldn't use your hand, so you imitate as the most efficient way.",
                    "label": 0
                },
                {
                    "sent": "So the difference the number of available options change what is inferred as the goal of the test.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So any you can see that if you try to understand someone approaches a light switch and turns the light switch with an E. And you can do this either with your ends free or your ends with some folders and you'll see that the way your brain interprets this action.",
                    "label": 0
                },
                {
                    "sent": "And his actions different.",
                    "label": 0
                },
                {
                    "sent": "So here the motor resonance mechanisms can understand this.",
                    "label": 0
                },
                {
                    "sent": "And here you cannot understand these do test restrictions.",
                    "label": 0
                },
                {
                    "sent": "So we have to do is to use arena zoning and areas of low brain to understand this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and you can see that for instance, if you don't know our box works.",
                    "label": 0
                },
                {
                    "sent": "So in this task you can put a.",
                    "label": 0
                },
                {
                    "sent": "You demonstrate someone putting a stick on the top.",
                    "label": 0
                },
                {
                    "sent": "Container any front container and if you ask and you can do this in the transparent condition that you understand that the top container is useless because the food is in the front container.",
                    "label": 0
                },
                {
                    "sent": "Or you can use in the OPEC condition and you see that here the chips will just go directly to weather for this.",
                    "label": 0
                },
                {
                    "sent": "And here they will do the useless action.",
                    "label": 0
                },
                {
                    "sent": "Also because the knowledge about the world.",
                    "label": 1
                },
                {
                    "sent": "You have to work change watching info.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's interesting.",
                    "label": 0
                },
                {
                    "sent": "Interesting that children will do the less efficient imitation than chimps, because maybe the social drive they have to act as the others is more important than getting the food.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the knowledge about the world and considerations about contextual restrictions will change your behavior in our algorithms of imitation will will be left.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Deal with this.",
                    "label": 0
                },
                {
                    "sent": "OK, so in Roberts what is copied so many different people as as approach limitation in different contexts and they they might be copying different things.",
                    "label": 1
                },
                {
                    "sent": "OK, either the joint level trajectory, stats level trajectories, but maybe only the final state is being copied.",
                    "label": 1
                },
                {
                    "sent": "Maybe only state transitions and so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Several approaches can can be, so I would divide the approach for imitation.",
                    "label": 1
                },
                {
                    "sent": "These three main main areas, so you could just copy the goal you see.",
                    "label": 0
                },
                {
                    "sent": "What's the final states of the world, and then you just plan out to get there by your own means.",
                    "label": 0
                },
                {
                    "sent": "You can use more or less supervised learning where you where we have a policy and you try to learn directly the policy with some generalization capabilities.",
                    "label": 1
                },
                {
                    "sent": "Of course an you you might have some things like inverse.",
                    "label": 1
                },
                {
                    "sent": "Women wearing for the criteria behind the demonstration.",
                    "label": 0
                },
                {
                    "sent": "So this is the most, let's say, the most efficient one.",
                    "label": 0
                },
                {
                    "sent": "This might be the one that's less better generation capabilities, but just be careful with this,",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in in in for the first reinforcement learning we are in the in the standard Markov decision process where we have a model of the world.",
                    "label": 0
                },
                {
                    "sent": "We have a reward function and you want to Max to choose a policy that maximizes the value function you want to choose our actions so that we get as much reviewer.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This weekend so this is a typical reinforcement learning problem of dynamic programming.",
                    "label": 1
                },
                {
                    "sent": "We have a model of the world.",
                    "label": 0
                },
                {
                    "sent": "You have a reward function, any computer policy.",
                    "label": 0
                },
                {
                    "sent": "Universal reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "You have different settings, so you have.",
                    "label": 0
                },
                {
                    "sent": "The model of the world you have samples from the policy and you want to infer the reward function.",
                    "label": 1
                },
                {
                    "sent": "So we want to infer what is the goal of the task, not how to solve the desk, but what what you want to do.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a very well defined problem, so because one reward might give multiple a season, one policy might give multiple rewards and having complete demonstrations are impractical, so we will try to go from this standard in first reformed setting and generalize it to the active.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "So the original formulation from him in 90.",
                    "label": 0
                },
                {
                    "sent": "9 or 2000 it was was very, very clear.",
                    "label": 0
                },
                {
                    "sent": "So you just start with your your Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "You just rewrite it as a matrix notation.",
                    "label": 1
                },
                {
                    "sent": "You have your value function is Y minus gamma.",
                    "label": 0
                },
                {
                    "sent": "The transition probability times the reward.",
                    "label": 0
                },
                {
                    "sent": "And then of course, your reward is.",
                    "label": 0
                },
                {
                    "sent": "This is this function?",
                    "label": 0
                },
                {
                    "sent": "OK, so you just have to find a reward.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "So if you have samples, so for instance, Action A is demonstrating in state X, you know the value of action A in state X is bigger than the values of for all.",
                    "label": 1
                },
                {
                    "sent": "Then all the other actions.",
                    "label": 0
                },
                {
                    "sent": "OK, if you put this in matrix notation, you get this condition OK, and so this is the problem that you have to solve to solve the IRL problem.",
                    "label": 0
                },
                {
                    "sent": "You have to find the reward function that that achieves these restrictions.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one problem is that we have to see if it's generalizes, and then if you can sample it inefficient way.",
                    "label": 0
                },
                {
                    "sent": "So if you have a problem with then state Sen M actions and you if you demonstrate all the states, you'll see that you'll have N * N conditions.",
                    "label": 1
                },
                {
                    "sent": "And of course, this is much more conditions that all the end possible linear independent conditions that we have.",
                    "label": 1
                },
                {
                    "sent": "So it's it means that we don't need to visit all the states.",
                    "label": 0
                },
                {
                    "sent": "So you have to have the policy completely completely defined.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one basic method would be just to to.",
                    "label": 0
                },
                {
                    "sent": "To check in on visited state, see from the transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "If this gives a linear independent condition and if it does, you ask a sample on this state.",
                    "label": 0
                },
                {
                    "sent": "If it doesn't, you don't ask a sample on this state an in this with the construction, you'll see that you will not visit all the states as a more.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A clear example.",
                    "label": 0
                },
                {
                    "sent": "You can see that for instance, in this world model, if someone tells you that in this state this action is optimal, you'll see it means that a is better than B.",
                    "label": 1
                },
                {
                    "sent": "You'll see of course, that's indeed.",
                    "label": 1
                },
                {
                    "sent": "This action is also better than this because you know that this state is better than this state.",
                    "label": 0
                },
                {
                    "sent": "So from that previous algorithm we see that we just need to visit one state in this problem.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then if you if you keep doing this, you see that for instance, for this problem at least four states you need to visit two states, so it doesn't depend on the number of states depends on the dynamics of the world.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you have like agreed Lauralee, you need to visit N -- N * N -- 1 states.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just a professional salary, so it will show that we can generalize and you can provide an efficient sampling of the problem, but you cannot of course deal with general transition matrix and noisy demonstrations.",
                    "label": 0
                },
                {
                    "sent": "That's what we have to solve now.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you if you put this in a Bayesian setting, we have a demonstration again with States and actions we have to compute.",
                    "label": 0
                },
                {
                    "sent": "If you have a price distribution of the rewards, we have to compute the likelihoods of a demonstration is like like a Boltzmann policy gives the likelihoods and to compute the reward you compute with using Monte Carlo methods the posterior over the rewards given that policy.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for instance.",
                    "label": 0
                },
                {
                    "sent": "So if you have a problem someone is doing is doing object manipulation task.",
                    "label": 0
                },
                {
                    "sent": "And so you don't know what is the goal of this task.",
                    "label": 1
                },
                {
                    "sent": "So someone is displaying with objects around you don't know what the objects are, so we know for some states that the demonstration was this.",
                    "label": 0
                },
                {
                    "sent": "You wanted to know what to do in the other states.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so you can see that using this by universal reinforcement learning, we can actually compute the optimal policy for states that were not demonstrated.",
                    "label": 0
                },
                {
                    "sent": "For instance, this state was not demonstrated and you learn the correct policy here.",
                    "label": 0
                },
                {
                    "sent": "There was some noise and the demonstration was wrong, and even then we could compute the correct action and you see that here the evolution of the noise we have in the demonstration and the policy error.",
                    "label": 0
                },
                {
                    "sent": "So even if you have like.",
                    "label": 0
                },
                {
                    "sent": "15% or 20% to recognition errors.",
                    "label": 1
                },
                {
                    "sent": "We can even then compute the optimal policy for this for this task.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and of course the robot is then able to to execute the correct test given for states that were not demonstrated.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can see that going back to these experiments in psychology we can see that.",
                    "label": 0
                },
                {
                    "sent": "We can see that some of the behaviors we see in biology this this algorithm also.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The achieves this, for instance, for this experiment, if you try to model it will see that in the end three teacher, you'll see that you will imitate much more than when the restricted you have restricted teacher.",
                    "label": 0
                },
                {
                    "sent": "So we can see that again the data written takes into account the task wrist.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Options that we see and for the non box as we don't know the model of the world again we can match the empirical expn.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Payments.",
                    "label": 0
                },
                {
                    "sent": "OK, final, let's go for the active set.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think so.",
                    "label": 0
                },
                {
                    "sent": "We we we we need a measure of the uncertainty, new policy estimation, and so we'll use these uncertainties to choose the best sites to ask for demonstration.",
                    "label": 0
                },
                {
                    "sent": "So in IRL, then certainty is measured in terms of the rewards, and of course we have to propagate this reward.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The policy.",
                    "label": 0
                },
                {
                    "sent": "Came so we have to estimate the posterior of the rewards and then for each state we compute the entropy of the policy and then we ask.",
                    "label": 0
                },
                {
                    "sent": "A demonstration on this.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "Hey so forgiven state, we compute the entropy of the of the policy.",
                    "label": 0
                },
                {
                    "sent": "And then that's that's the information we used to ask a demonstration as we have.",
                    "label": 0
                },
                {
                    "sent": "So we have samples for samples of the reward, and then we compute the policy for its reward and that's.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Information we use.",
                    "label": 0
                },
                {
                    "sent": "OK, as an example, so take into account that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you want to find the maximum of a function and you have an initial demonstration that says that here you should go right and you should go left, you compute the reward function so you see that the maximum isn't going to be here, because here you want to go to the right.",
                    "label": 1
                },
                {
                    "sent": "You're going to the left, so the maximum should be here.",
                    "label": 0
                },
                {
                    "sent": "So you're met with our method, which is going to ask a demonstration here.",
                    "label": 0
                },
                {
                    "sent": "That's where we have higher and certainty in the policy, and then you see that the samples of the rewards get shifted to the right and then you ask more demonstrations and then you can.",
                    "label": 0
                },
                {
                    "sent": "Correctly estimated the reward function.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'm gonna run.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Out of time, so and so if you consider problems with much higher dimensions.",
                    "label": 0
                },
                {
                    "sent": "So for instance for two understates and you consider like some random problems and some problems with more.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Less structure you can see.",
                    "label": 0
                },
                {
                    "sent": "Can you see it?",
                    "label": 0
                },
                {
                    "sent": "So you can see that the well so the random this is random strategy.",
                    "label": 0
                },
                {
                    "sent": "So this is the policy loss.",
                    "label": 0
                },
                {
                    "sent": "This is the size of the demonstration, and you see that the active learning is.",
                    "label": 0
                },
                {
                    "sent": "Is able to learn much faster than using the random than the random approach for a problem with about 200 states.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the the size of demonstration continues reduced using active learning techniques and again depends will be will depend on the world dynamics on and on the reward stretch.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How much end ref.",
                    "label": 0
                },
                {
                    "sent": "10 minutes so excited.",
                    "label": 0
                },
                {
                    "sent": "So another idea to use active learning for demonstration is going to try to use to combine.",
                    "label": 0
                },
                {
                    "sent": "Supervised techniques and if.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First inverse learning techniques so.",
                    "label": 0
                },
                {
                    "sent": "So because of the sampling in the inverse reinforcement learning the destination is very is can become very slow.",
                    "label": 0
                },
                {
                    "sent": "That's why we are limited in the number of states we can.",
                    "label": 0
                },
                {
                    "sent": "We can deal and when using supervised learning things are very fast with all the techniques we have for regression.",
                    "label": 0
                },
                {
                    "sent": "So the idea is try to combine both when I need to combine both is try to embed the MDP structure in supervised learning machine.",
                    "label": 1
                },
                {
                    "sent": "So if user kernel method we can.",
                    "label": 0
                },
                {
                    "sent": "Use the Take the MVP structure to a kernel.",
                    "label": 0
                },
                {
                    "sent": "Then we can use.",
                    "label": 0
                },
                {
                    "sent": "Supervised learning techniques.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are several metrics for MDP's.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you have this this problem that you have to reach this goal and you have these two states, you see that due to the symmetries this state is equivalent to this state.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have the same distance to the goal an if you change names of the actions you can get to the goal in the same number of steps.",
                    "label": 0
                },
                {
                    "sent": "So this state and these are equivalent to take into account.",
                    "label": 0
                },
                {
                    "sent": "Symmetric, so if the next state is the same, two states are similar.",
                    "label": 1
                },
                {
                    "sent": "OK, so for if from two different states we get to the same states, these two states are equivalent.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so if you define an MDP metric we get the distance between state action pairs from this week and we have a kernel.",
                    "label": 0
                },
                {
                    "sent": "You can acquire demonstration and then if we fit.",
                    "label": 0
                },
                {
                    "sent": "If you learn with a regression, we we we learn a policy that takes into account the dynamics of the world.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the sons way.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you have a problem that is like very like a navigation problem, maybe just the kernel, Gaussian kernel is enough and and you can learn directly the policy.",
                    "label": 0
                },
                {
                    "sent": "But if you have discrete states combined with continuous, of course the the trivial kernel will not.",
                    "label": 0
                },
                {
                    "sent": "Will not be the.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The correct one.",
                    "label": 0
                },
                {
                    "sent": "So to compute the kernel we have, we have to compute distance between distributions.",
                    "label": 0
                },
                {
                    "sent": "You can use the earth movers distance to compute distance between distributions, so but this just gives the distance between 2.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dates.",
                    "label": 0
                },
                {
                    "sent": "And of course, then we have to take into account long-term.",
                    "label": 0
                },
                {
                    "sent": "Dynamics of the world.",
                    "label": 0
                },
                {
                    "sent": "And for this there's there's other techniques so.",
                    "label": 0
                },
                {
                    "sent": "We take into account so the counter of each distance between 2:00 to state action pairs and then to take into account the long term.",
                    "label": 0
                },
                {
                    "sent": "The situation we have to find the fixed point of the outdoor distance metric and so you see that for that case I showed you will be able to see that these two states are equivalent.",
                    "label": 1
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in terms of results you can see that so far this is a problem of around understates.",
                    "label": 0
                },
                {
                    "sent": "You see that.",
                    "label": 0
                },
                {
                    "sent": "Then if you do, this is the inverse reinforcement.",
                    "label": 0
                },
                {
                    "sent": "Learning the generalization capability.",
                    "label": 0
                },
                {
                    "sent": "So this is the size of demonstration.",
                    "label": 0
                },
                {
                    "sent": "This will be a complete demonstration.",
                    "label": 0
                },
                {
                    "sent": "You see that you have.",
                    "label": 0
                },
                {
                    "sent": "You have a very high baseline, so you have a good estimation of policy since the beginning.",
                    "label": 0
                },
                {
                    "sent": "Because of the structure of the MDP, and then you can grow.",
                    "label": 0
                },
                {
                    "sent": "And then you can learn the policy so you don't get 200% due to the local minima you have in gradient methods.",
                    "label": 0
                },
                {
                    "sent": "And using this new technique you can you can see that you you you can learn much faster and you can get a better final result.",
                    "label": 0
                },
                {
                    "sent": "OK, so this would be the result you'd get using just a trivial kernel like takes into account like the distance, degree distance between states.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and then you see that you are also robust with the noise.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to take into account and active extension we have to learn the policy with we use a multinomial distribution.",
                    "label": 0
                },
                {
                    "sent": "And then to find the posterior of the policy.",
                    "label": 1
                },
                {
                    "sent": "And then we choose the states again.",
                    "label": 0
                },
                {
                    "sent": "The states with higher variance.",
                    "label": 0
                },
                {
                    "sent": "Any request these states?",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Demonstration OK, and then you see that again the active learning.",
                    "label": 0
                },
                {
                    "sent": "So this is the correct classification rate for the correct actions.",
                    "label": 0
                },
                {
                    "sent": "This is the active learning approach.",
                    "label": 1
                },
                {
                    "sent": "This would be just a random approach and this is.",
                    "label": 0
                },
                {
                    "sent": "Just making sure that you visit all the states in a random order.",
                    "label": 0
                },
                {
                    "sent": "OK, you see that you can get again of 10 to 20% in the classification, right?",
                    "label": 0
                },
                {
                    "sent": "Using nine active learning approach, this is without noise and this is if you have errors in the demonstration.",
                    "label": 0
                },
                {
                    "sent": "But even then the active learning approach can give good results.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we this is if we we use MDP induced metrics we can learn you can get a kernel that gives good generalization capabilities for learning by demonstration setting.",
                    "label": 1
                },
                {
                    "sent": "So the kernel doesn't depend on demonstration, so it's.",
                    "label": 0
                },
                {
                    "sent": "We could get better results if you would do this dependence, But then we would have to use sampling methods and you'd go back to the IRL problem.",
                    "label": 0
                },
                {
                    "sent": "So the computing the kernel is a bit costly, but then for at learning time it's almost instantaneously those due to the regression techniques.",
                    "label": 1
                },
                {
                    "sent": "And of course there's still some initialization of the ground distance that might impact the results.",
                    "label": 1
                },
                {
                    "sent": "So now we have to maybe have approximated methods to be able to compute the kernels for very high dimensions and you have to be able to generalize this to continuous domains.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here we consider the case where we can synthesize a sample so the robot can ask.",
                    "label": 1
                },
                {
                    "sent": "I want to sample in this state or this state.",
                    "label": 0
                },
                {
                    "sent": "So and this might be a problem because getting to that state might be difficult and maybe some the best thing would be to compute the optimal path.",
                    "label": 1
                },
                {
                    "sent": "So asking a path for the demonstration and not just one state.",
                    "label": 0
                },
                {
                    "sent": "Maybe this would be better for control.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Whoops terms of conclusions.",
                    "label": 0
                },
                {
                    "sent": "We show that we can generalize that inverse reinforcement learning to an active learning setting that we can handle hundreds of states.",
                    "label": 1
                },
                {
                    "sent": "We we can indeed reduce the number of samples required to get a good estimation.",
                    "label": 0
                },
                {
                    "sent": "And some limitations.",
                    "label": 1
                },
                {
                    "sent": "We know that the prior knowledge will impact the results.",
                    "label": 1
                },
                {
                    "sent": "But typically the active it's never worse than than random.",
                    "label": 1
                },
                {
                    "sent": "Maybe sometimes it just.",
                    "label": 0
                },
                {
                    "sent": "Is equal to run depends on the on the dynamics of the world, and we're trying to to try to make a unified view of inverse reinforcement learning and regression based techniques for.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For imitation.",
                    "label": 0
                },
                {
                    "sent": "OK, some references.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}