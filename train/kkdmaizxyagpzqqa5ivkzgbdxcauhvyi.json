{
    "id": "kkdmaizxyagpzqqa5ivkzgbdxcauhvyi",
    "title": "Synonym Analysis for Predicate Expansion",
    "info": {
        "introducer": [
            "Laura Hollink, Centrum Wiskunde & Informatica (CWI)"
        ],
        "author": [
            "Ziawasch Abedjan, Hasso-Plattner-Institute, University of Potsdam"
        ],
        "published": "July 8, 2013",
        "recorded": "May 2013",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Big Data"
        ]
    },
    "url": "http://videolectures.net/eswc2013_abedjan_synonym/",
    "segmentation": [
        [
            "Our next speaker is a sea of us Abidjan.",
            "So hello everybody, I am Siavash and I'm also from as applied in Institute in Potsdam.",
            "So you see we are really interested in linked data.",
            "And you."
        ],
        [
            "Really, when we start to talk about linked open data, we show this picture and show the complexity and virginity of linked open data.",
            "That is quite motivating and interesting at the same time."
        ],
        [
            "Then we start to show the actual data that is content like data about persons or locations or architectural buildings or cities, actors or movies and how they are connected.",
            "'cause maybe the president is the president of the country and the cities are connected because they are in the also in the same country the actor is connected to.",
            "The other people because of some.",
            "Tax issues and of course the actor is playing is in one movie and all of them are again sorry.",
            "That was the first are connected with the country and then on top of that."
        ],
        [
            "Say OK, large companies are using linked open data because they provide knowledge that can be used for enriching the users or whatever.",
            "And then again."
        ],
        [
            "Say to other people.",
            "OK, linked open data is great.",
            "Let's try it.",
            "What can we do?",
            "OK, then user might just go to the DB Pedia data set.",
            "That's an excerpt from the DB Pedia data set for movies.",
            "And wants to discover movies where the band about did the music, so he somehow comes up with a query saying OK, I'm looking for all movies where the composer is other.",
            "So then he gets a result that's knowing me knowing you something.",
            "Maybe what he doesn't know, but then he remembers OK, but there was a great movie that is Mamma Mia.",
            "Why is that not retrieved?",
            "Then you looked into the data and you see OK.",
            "It's connected with music composer.",
            "And you should have again.",
            "And formulated your query in a different way and that's already not so great anymore because it's already difficult to use the data."
        ],
        [
            "So there are many reasons for that.",
            "For example, first of all you need domain specific knowledge.",
            "You need to know the data so user you cannot just go to the.",
            "Data set and try to use it.",
            "And even if you know the ontology of the data, you still have problems because sometimes the data publishers ignore the rules that are provided by the ontology.",
            "So you have a mismatch between data and ontology, for example.",
            "So ontology might define person, wait for an athlete's weight, but most users are most data publishers used white in action.",
            "In the actual scenario, because it's shorter and more convenient or.",
            "Whatever or because of because the data is automatically generated or extracted.",
            "So we have basically a synonym discrepancy that makes it difficult for users and services as well to use linked open data."
        ],
        [
            "Let us look into the query logs of use word.",
            "There we can see, for example, how.",
            "Some services, I guess because these are very complex queries that I have only where I have only put some excess of these queries into the slide deal with this problem.",
            "As you can see there are some conjunctions, union constructions of different patterns containing the same property, similar properties like the Pedia prop name and RDF's label.",
            "Another example is when.",
            "Was also in the data is like set when where you have also the union of Prop name, official name and 4th name pointing to the same object value.",
            "So what basically the services or the user we don't know because we only have the locks is doing.",
            "He is basically expanding his query with synonymously used predicate.",
            "So it is useful to discover them when you want to retrieve the complete set of results that you want to have."
        ],
        [
            "There have been already some research on query expansion in sparkle scenarios, who Tato at all presented an approach that is based on.",
            "Inference of sub property.",
            "An subclass your key.",
            "Relationships but basically do rise on well defined ontologies as it.",
            "Expects that if you want to enlarge their set of results that you want to have, you just need to go to the higher class in the ontology or in the higher class and the property hierarchy.",
            "But as I told before, we have already the problem that ontology and data do not always.",
            "Confirm to each other, then another approach that is more data driven was proposed by Albert Sony at all, where they look for Co occurrences of entities and predicates in so-called documents that are generated by considering the F sets of RDF facts and also external datasets.",
            "So that is.",
            "And, uh, the next, but because they are already using external datasets, what we actually don't want to, we want to see what we can do just using the knowledge graph that is ready because you don't want to add more complexity always to using different datasets for enriching your knowledge base."
        ],
        [
            "So for discovering synonymously used predicates we have basically the following following intuition.",
            "First of all, we expect the predicates to have a similar range to some extent that could be defined by some threshold that's basically very similar to instance based schema matching.",
            "Ideas presented by ARM, and further they should not Co occur for the same subjects.",
            "That is, basically, that's already not so true, because if you follow the Harris theorem on synonyms there, you expect that synonyms occur in the same context always.",
            "So, but for query expansion where you want to complete your results, you have to look for those predicates that are not occurring for your.",
            "Search entity, but maybe for others.",
            "So here we have already A twist and our methodology is really simple.",
            "We apply Association rules on on the statement of RDF data, and for that we have.",
            "Introduce mining configuration approach that I will show in the following."
        ],
        [
            "Yeah, first of all.",
            "If you want to do Association rule mining on RDF data, you have first of all to define your transaction database.",
            "As many of you might know, you need like a transaction database is a database where you have bags of items where you look for Co occurrences of items within these different bags that are in your transaction database.",
            "So all approach we have created.",
            "This abstract view on creating transaction databases with different contexts and transactions by basically using each part of the statement, either as the context or as a target value.",
            "For this mining approach.",
            "So a context identifies.",
            "So to say the transaction ID.",
            "So we group all predicates, so to say by the subject, so the subject is the context and the target are in that example as a predicate.",
            "And for each of these transaction databases that we can generate, we have some kind of semantics, like if we look at all bags of predicates we for Co occurrences, we might end up in schema analysis if we group all subjects per predicate, we do some ontological clustering analysis and so on."
        ],
        [
            "So let us look into one example where we want to discover all predicates that has the same range to some extent.",
            "That is, when you group all predicates by their object values.",
            "So given our previous example where we had these movies, we would group.",
            "All predicates by by the object values that would create the following transaction database where you have the transaction ID's.",
            "UK, Arbor and comedy and English which model objects on the right hand side of the table and the transactions that contains the accordingly predicates.",
            "So as you can see, composer music composer already in the same transaction for this example and of course for many other bands as well.",
            "So this way we could discover these examples.",
            "This is a pair of predicates.",
            "And we could so we can apply the same methodology for also for discovering the predicates that do not cocoa for this same subject, so.",
            "Ask since I just forgot it is of course we could do also the pairwise comparison of each predicate, but looking what is very strange overlap of both predicates, but that's quite expensive as you might as you have to compare each predicate with another.",
            "In this way we are creating creating basically an inverted index, which makes it really fast to discover those pairs of predicates."
        ],
        [
            "Yeah, if you then again create a data transaction database based on the subjects as the context value.",
            "The.",
            "Creates the following transaction database where we have the subjects as transaction IDs and again the predicates in the transactions.",
            "And now we're interested in exclusively Co occurring predicates.",
            "As you can see, music composer and composer cocoa for different subjects.",
            "So basically we're looking for all of those predicates that only occur this way.",
            "And in that sense, when you're talking about Association rules, a rule can be in both directions.",
            "You can't have composer to not music composer and music composer to not composer with different confidence values.",
            "So you have to come up with some kind of aggregation of these confidences.",
            "And for that we have applied multiple aggregation function like taking the maximum minimum or the F measure of that and further we applied the reverse correlation coefficient which takes both directs into account and another function was proposed by Catarella in their paper about discovering tables in the web where where they show all they look also at the odds of both predicates Co occurring with very different predicates at in general.",
            "So having these two components range content filtering and schema analysis."
        ],
        [
            "We can show you the algorithm work for, which is pretty straightforward.",
            "You get a RDF graph or knowledge base, then you look for the predicates that have similar ranges by applying the range content filtering, but by mining in the context of objects, then you.",
            "Store all of those that are frequent based on your defined support threshold.",
            "And for that, for those you do the schema analysis and look for the path with the highest negative correlation.",
            "And then you get your candidates for predicate expansion later."
        ],
        [
            "We have applied multiple evaluations on different datasets with regard to all different aggregation methods.",
            "Also, as you can see, our first evaluation on the magnitude data set is awesome, 'cause it's 100% precision.",
            "But of course magnitude is a very small data set restricted to one domain of data, that is music and there you don't get into troubles with like Discovering Foundation place and birthplace and so on.",
            "That won't occur.",
            "But then again we go to the next datasets we already have.",
            "Very bad results and that's why God why it is already a very clean data set so you only have this.",
            "Concrete predicates that used throughout the data set for the same type of entities, but you have of course entities from different domains like places, people and locations as well.",
            "And the same applies if you apply our approach on the complete DVD pedia data set.",
            "But it looks somehow men because we have more colors in the in the figure but still not.",
            "Promising, so that's why we need to apply the approach on more specific domains of the data set is can see for person.",
            "It's already much better than for the complete data set on work.",
            "It is again a bit better because work is again more concise and you have like these interesting synonym problems like what is an artist, what is in, when do you use, starring and so on.",
            "When to use writer and also and these are occurring in this place.",
            "And for organization as well, where you have foundation place, origonal place and so on.",
            "You have done it."
        ],
        [
            "Other experiment for looking for the recall of the results.",
            "For that we had two.",
            "For that experiment, we chose Magnatune as well as do Pedial work.",
            "And we manually classified all 9456 pairs of properties that are there and the computer scientist scientists agreed on 80 two candidates.",
            "So as I see the relation is not really.",
            "Big 82 out of 9000.",
            "It's already very small.",
            "And.",
            "Here's the top result for our approach on that data set that you can see the first ones artist starring is.",
            "One could agree on that artist music composer as well.",
            "Also write as well creating writer and of course our example composer and music composer on the 5th place.",
            "But looking at the recall we can see if we have a high support threshold on our range filtering approach.",
            "We cannot retrieve all of those.",
            "We already have a cutter cutting line at 0.3 recall and if we but of course we have high precision, so that's always a tradeoff.",
            "If you again reduce the support threshold by a factor of 10.",
            "We can end up retrieving all the results that we have classified, but of course in that case we have a really low precision.",
            "By retreiving a lot of.",
            "False positives.",
            "And as you can see, all these different measures for negative correlation are basically behaving very similar.",
            "The same function.",
            "One would say performance best, but it's not significant in that case."
        ],
        [
            "As I already mentioned, the approach to my predicates in the context of objects is much faster than the naive approach where you would take each pair of predicates and look for the value overlap, and that's all datasets.",
            "It wasn't by an order of magnitude faster than this approach."
        ],
        [
            "To conclude, we have showed how to deal with an inconsistent state inconsistent ELODIE data through query expand.",
            "For that we showed that we apply a synonym, an analysis that is based on Association rule mining, and further that it of course works best on domain specific data where you have entities of a certain type.",
            "And you have also showed that the approach is a much faster than current approaches on instant based schema matching.",
            "Future work is of course for now.",
            "We have showed how to deal with problems in linked open data when we want to use it, but the next obvious question is maybe we should change the data set itself or the ontology.",
            "And that's I think more challenging, because changing the data is always accompanied with further problems.",
            "So thank you for your attention.",
            "I was wondering, you said you used this.",
            "I mean I'm interested in how you use the useful to data set exactly.",
            "Is there anything in the data set that you would like to?",
            "Change or that you would need in order to do this kind of research better.",
            "I have to say that basically the analysis of the user data set is pretty much done by Johannes because he worked on that data set during his work and there he already told me that there are these examples in the data set and that's why I came to it.",
            "Alright then, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our next speaker is a sea of us Abidjan.",
                    "label": 0
                },
                {
                    "sent": "So hello everybody, I am Siavash and I'm also from as applied in Institute in Potsdam.",
                    "label": 0
                },
                {
                    "sent": "So you see we are really interested in linked data.",
                    "label": 0
                },
                {
                    "sent": "And you.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really, when we start to talk about linked open data, we show this picture and show the complexity and virginity of linked open data.",
                    "label": 0
                },
                {
                    "sent": "That is quite motivating and interesting at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we start to show the actual data that is content like data about persons or locations or architectural buildings or cities, actors or movies and how they are connected.",
                    "label": 0
                },
                {
                    "sent": "'cause maybe the president is the president of the country and the cities are connected because they are in the also in the same country the actor is connected to.",
                    "label": 0
                },
                {
                    "sent": "The other people because of some.",
                    "label": 0
                },
                {
                    "sent": "Tax issues and of course the actor is playing is in one movie and all of them are again sorry.",
                    "label": 0
                },
                {
                    "sent": "That was the first are connected with the country and then on top of that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say OK, large companies are using linked open data because they provide knowledge that can be used for enriching the users or whatever.",
                    "label": 0
                },
                {
                    "sent": "And then again.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Say to other people.",
                    "label": 0
                },
                {
                    "sent": "OK, linked open data is great.",
                    "label": 0
                },
                {
                    "sent": "Let's try it.",
                    "label": 0
                },
                {
                    "sent": "What can we do?",
                    "label": 0
                },
                {
                    "sent": "OK, then user might just go to the DB Pedia data set.",
                    "label": 0
                },
                {
                    "sent": "That's an excerpt from the DB Pedia data set for movies.",
                    "label": 0
                },
                {
                    "sent": "And wants to discover movies where the band about did the music, so he somehow comes up with a query saying OK, I'm looking for all movies where the composer is other.",
                    "label": 0
                },
                {
                    "sent": "So then he gets a result that's knowing me knowing you something.",
                    "label": 1
                },
                {
                    "sent": "Maybe what he doesn't know, but then he remembers OK, but there was a great movie that is Mamma Mia.",
                    "label": 0
                },
                {
                    "sent": "Why is that not retrieved?",
                    "label": 0
                },
                {
                    "sent": "Then you looked into the data and you see OK.",
                    "label": 0
                },
                {
                    "sent": "It's connected with music composer.",
                    "label": 0
                },
                {
                    "sent": "And you should have again.",
                    "label": 0
                },
                {
                    "sent": "And formulated your query in a different way and that's already not so great anymore because it's already difficult to use the data.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are many reasons for that.",
                    "label": 0
                },
                {
                    "sent": "For example, first of all you need domain specific knowledge.",
                    "label": 1
                },
                {
                    "sent": "You need to know the data so user you cannot just go to the.",
                    "label": 0
                },
                {
                    "sent": "Data set and try to use it.",
                    "label": 0
                },
                {
                    "sent": "And even if you know the ontology of the data, you still have problems because sometimes the data publishers ignore the rules that are provided by the ontology.",
                    "label": 0
                },
                {
                    "sent": "So you have a mismatch between data and ontology, for example.",
                    "label": 0
                },
                {
                    "sent": "So ontology might define person, wait for an athlete's weight, but most users are most data publishers used white in action.",
                    "label": 0
                },
                {
                    "sent": "In the actual scenario, because it's shorter and more convenient or.",
                    "label": 0
                },
                {
                    "sent": "Whatever or because of because the data is automatically generated or extracted.",
                    "label": 1
                },
                {
                    "sent": "So we have basically a synonym discrepancy that makes it difficult for users and services as well to use linked open data.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let us look into the query logs of use word.",
                    "label": 0
                },
                {
                    "sent": "There we can see, for example, how.",
                    "label": 0
                },
                {
                    "sent": "Some services, I guess because these are very complex queries that I have only where I have only put some excess of these queries into the slide deal with this problem.",
                    "label": 0
                },
                {
                    "sent": "As you can see there are some conjunctions, union constructions of different patterns containing the same property, similar properties like the Pedia prop name and RDF's label.",
                    "label": 0
                },
                {
                    "sent": "Another example is when.",
                    "label": 0
                },
                {
                    "sent": "Was also in the data is like set when where you have also the union of Prop name, official name and 4th name pointing to the same object value.",
                    "label": 0
                },
                {
                    "sent": "So what basically the services or the user we don't know because we only have the locks is doing.",
                    "label": 0
                },
                {
                    "sent": "He is basically expanding his query with synonymously used predicate.",
                    "label": 1
                },
                {
                    "sent": "So it is useful to discover them when you want to retrieve the complete set of results that you want to have.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There have been already some research on query expansion in sparkle scenarios, who Tato at all presented an approach that is based on.",
                    "label": 0
                },
                {
                    "sent": "Inference of sub property.",
                    "label": 0
                },
                {
                    "sent": "An subclass your key.",
                    "label": 0
                },
                {
                    "sent": "Relationships but basically do rise on well defined ontologies as it.",
                    "label": 0
                },
                {
                    "sent": "Expects that if you want to enlarge their set of results that you want to have, you just need to go to the higher class in the ontology or in the higher class and the property hierarchy.",
                    "label": 0
                },
                {
                    "sent": "But as I told before, we have already the problem that ontology and data do not always.",
                    "label": 0
                },
                {
                    "sent": "Confirm to each other, then another approach that is more data driven was proposed by Albert Sony at all, where they look for Co occurrences of entities and predicates in so-called documents that are generated by considering the F sets of RDF facts and also external datasets.",
                    "label": 0
                },
                {
                    "sent": "So that is.",
                    "label": 0
                },
                {
                    "sent": "And, uh, the next, but because they are already using external datasets, what we actually don't want to, we want to see what we can do just using the knowledge graph that is ready because you don't want to add more complexity always to using different datasets for enriching your knowledge base.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for discovering synonymously used predicates we have basically the following following intuition.",
                    "label": 1
                },
                {
                    "sent": "First of all, we expect the predicates to have a similar range to some extent that could be defined by some threshold that's basically very similar to instance based schema matching.",
                    "label": 0
                },
                {
                    "sent": "Ideas presented by ARM, and further they should not Co occur for the same subjects.",
                    "label": 1
                },
                {
                    "sent": "That is, basically, that's already not so true, because if you follow the Harris theorem on synonyms there, you expect that synonyms occur in the same context always.",
                    "label": 0
                },
                {
                    "sent": "So, but for query expansion where you want to complete your results, you have to look for those predicates that are not occurring for your.",
                    "label": 0
                },
                {
                    "sent": "Search entity, but maybe for others.",
                    "label": 1
                },
                {
                    "sent": "So here we have already A twist and our methodology is really simple.",
                    "label": 0
                },
                {
                    "sent": "We apply Association rules on on the statement of RDF data, and for that we have.",
                    "label": 0
                },
                {
                    "sent": "Introduce mining configuration approach that I will show in the following.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, first of all.",
                    "label": 0
                },
                {
                    "sent": "If you want to do Association rule mining on RDF data, you have first of all to define your transaction database.",
                    "label": 1
                },
                {
                    "sent": "As many of you might know, you need like a transaction database is a database where you have bags of items where you look for Co occurrences of items within these different bags that are in your transaction database.",
                    "label": 0
                },
                {
                    "sent": "So all approach we have created.",
                    "label": 0
                },
                {
                    "sent": "This abstract view on creating transaction databases with different contexts and transactions by basically using each part of the statement, either as the context or as a target value.",
                    "label": 1
                },
                {
                    "sent": "For this mining approach.",
                    "label": 0
                },
                {
                    "sent": "So a context identifies.",
                    "label": 0
                },
                {
                    "sent": "So to say the transaction ID.",
                    "label": 0
                },
                {
                    "sent": "So we group all predicates, so to say by the subject, so the subject is the context and the target are in that example as a predicate.",
                    "label": 0
                },
                {
                    "sent": "And for each of these transaction databases that we can generate, we have some kind of semantics, like if we look at all bags of predicates we for Co occurrences, we might end up in schema analysis if we group all subjects per predicate, we do some ontological clustering analysis and so on.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let us look into one example where we want to discover all predicates that has the same range to some extent.",
                    "label": 0
                },
                {
                    "sent": "That is, when you group all predicates by their object values.",
                    "label": 0
                },
                {
                    "sent": "So given our previous example where we had these movies, we would group.",
                    "label": 0
                },
                {
                    "sent": "All predicates by by the object values that would create the following transaction database where you have the transaction ID's.",
                    "label": 0
                },
                {
                    "sent": "UK, Arbor and comedy and English which model objects on the right hand side of the table and the transactions that contains the accordingly predicates.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, composer music composer already in the same transaction for this example and of course for many other bands as well.",
                    "label": 0
                },
                {
                    "sent": "So this way we could discover these examples.",
                    "label": 0
                },
                {
                    "sent": "This is a pair of predicates.",
                    "label": 0
                },
                {
                    "sent": "And we could so we can apply the same methodology for also for discovering the predicates that do not cocoa for this same subject, so.",
                    "label": 0
                },
                {
                    "sent": "Ask since I just forgot it is of course we could do also the pairwise comparison of each predicate, but looking what is very strange overlap of both predicates, but that's quite expensive as you might as you have to compare each predicate with another.",
                    "label": 0
                },
                {
                    "sent": "In this way we are creating creating basically an inverted index, which makes it really fast to discover those pairs of predicates.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, if you then again create a data transaction database based on the subjects as the context value.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Creates the following transaction database where we have the subjects as transaction IDs and again the predicates in the transactions.",
                    "label": 0
                },
                {
                    "sent": "And now we're interested in exclusively Co occurring predicates.",
                    "label": 0
                },
                {
                    "sent": "As you can see, music composer and composer cocoa for different subjects.",
                    "label": 0
                },
                {
                    "sent": "So basically we're looking for all of those predicates that only occur this way.",
                    "label": 0
                },
                {
                    "sent": "And in that sense, when you're talking about Association rules, a rule can be in both directions.",
                    "label": 0
                },
                {
                    "sent": "You can't have composer to not music composer and music composer to not composer with different confidence values.",
                    "label": 1
                },
                {
                    "sent": "So you have to come up with some kind of aggregation of these confidences.",
                    "label": 0
                },
                {
                    "sent": "And for that we have applied multiple aggregation function like taking the maximum minimum or the F measure of that and further we applied the reverse correlation coefficient which takes both directs into account and another function was proposed by Catarella in their paper about discovering tables in the web where where they show all they look also at the odds of both predicates Co occurring with very different predicates at in general.",
                    "label": 1
                },
                {
                    "sent": "So having these two components range content filtering and schema analysis.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can show you the algorithm work for, which is pretty straightforward.",
                    "label": 0
                },
                {
                    "sent": "You get a RDF graph or knowledge base, then you look for the predicates that have similar ranges by applying the range content filtering, but by mining in the context of objects, then you.",
                    "label": 1
                },
                {
                    "sent": "Store all of those that are frequent based on your defined support threshold.",
                    "label": 1
                },
                {
                    "sent": "And for that, for those you do the schema analysis and look for the path with the highest negative correlation.",
                    "label": 1
                },
                {
                    "sent": "And then you get your candidates for predicate expansion later.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have applied multiple evaluations on different datasets with regard to all different aggregation methods.",
                    "label": 0
                },
                {
                    "sent": "Also, as you can see, our first evaluation on the magnitude data set is awesome, 'cause it's 100% precision.",
                    "label": 0
                },
                {
                    "sent": "But of course magnitude is a very small data set restricted to one domain of data, that is music and there you don't get into troubles with like Discovering Foundation place and birthplace and so on.",
                    "label": 0
                },
                {
                    "sent": "That won't occur.",
                    "label": 0
                },
                {
                    "sent": "But then again we go to the next datasets we already have.",
                    "label": 0
                },
                {
                    "sent": "Very bad results and that's why God why it is already a very clean data set so you only have this.",
                    "label": 0
                },
                {
                    "sent": "Concrete predicates that used throughout the data set for the same type of entities, but you have of course entities from different domains like places, people and locations as well.",
                    "label": 0
                },
                {
                    "sent": "And the same applies if you apply our approach on the complete DVD pedia data set.",
                    "label": 0
                },
                {
                    "sent": "But it looks somehow men because we have more colors in the in the figure but still not.",
                    "label": 0
                },
                {
                    "sent": "Promising, so that's why we need to apply the approach on more specific domains of the data set is can see for person.",
                    "label": 0
                },
                {
                    "sent": "It's already much better than for the complete data set on work.",
                    "label": 0
                },
                {
                    "sent": "It is again a bit better because work is again more concise and you have like these interesting synonym problems like what is an artist, what is in, when do you use, starring and so on.",
                    "label": 0
                },
                {
                    "sent": "When to use writer and also and these are occurring in this place.",
                    "label": 0
                },
                {
                    "sent": "And for organization as well, where you have foundation place, origonal place and so on.",
                    "label": 0
                },
                {
                    "sent": "You have done it.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other experiment for looking for the recall of the results.",
                    "label": 0
                },
                {
                    "sent": "For that we had two.",
                    "label": 0
                },
                {
                    "sent": "For that experiment, we chose Magnatune as well as do Pedial work.",
                    "label": 0
                },
                {
                    "sent": "And we manually classified all 9456 pairs of properties that are there and the computer scientist scientists agreed on 80 two candidates.",
                    "label": 1
                },
                {
                    "sent": "So as I see the relation is not really.",
                    "label": 0
                },
                {
                    "sent": "Big 82 out of 9000.",
                    "label": 0
                },
                {
                    "sent": "It's already very small.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "Here's the top result for our approach on that data set that you can see the first ones artist starring is.",
                    "label": 0
                },
                {
                    "sent": "One could agree on that artist music composer as well.",
                    "label": 0
                },
                {
                    "sent": "Also write as well creating writer and of course our example composer and music composer on the 5th place.",
                    "label": 0
                },
                {
                    "sent": "But looking at the recall we can see if we have a high support threshold on our range filtering approach.",
                    "label": 0
                },
                {
                    "sent": "We cannot retrieve all of those.",
                    "label": 0
                },
                {
                    "sent": "We already have a cutter cutting line at 0.3 recall and if we but of course we have high precision, so that's always a tradeoff.",
                    "label": 0
                },
                {
                    "sent": "If you again reduce the support threshold by a factor of 10.",
                    "label": 0
                },
                {
                    "sent": "We can end up retrieving all the results that we have classified, but of course in that case we have a really low precision.",
                    "label": 0
                },
                {
                    "sent": "By retreiving a lot of.",
                    "label": 0
                },
                {
                    "sent": "False positives.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, all these different measures for negative correlation are basically behaving very similar.",
                    "label": 0
                },
                {
                    "sent": "The same function.",
                    "label": 0
                },
                {
                    "sent": "One would say performance best, but it's not significant in that case.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I already mentioned, the approach to my predicates in the context of objects is much faster than the naive approach where you would take each pair of predicates and look for the value overlap, and that's all datasets.",
                    "label": 0
                },
                {
                    "sent": "It wasn't by an order of magnitude faster than this approach.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To conclude, we have showed how to deal with an inconsistent state inconsistent ELODIE data through query expand.",
                    "label": 1
                },
                {
                    "sent": "For that we showed that we apply a synonym, an analysis that is based on Association rule mining, and further that it of course works best on domain specific data where you have entities of a certain type.",
                    "label": 0
                },
                {
                    "sent": "And you have also showed that the approach is a much faster than current approaches on instant based schema matching.",
                    "label": 1
                },
                {
                    "sent": "Future work is of course for now.",
                    "label": 0
                },
                {
                    "sent": "We have showed how to deal with problems in linked open data when we want to use it, but the next obvious question is maybe we should change the data set itself or the ontology.",
                    "label": 0
                },
                {
                    "sent": "And that's I think more challenging, because changing the data is always accompanied with further problems.",
                    "label": 0
                },
                {
                    "sent": "So thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "I was wondering, you said you used this.",
                    "label": 0
                },
                {
                    "sent": "I mean I'm interested in how you use the useful to data set exactly.",
                    "label": 0
                },
                {
                    "sent": "Is there anything in the data set that you would like to?",
                    "label": 0
                },
                {
                    "sent": "Change or that you would need in order to do this kind of research better.",
                    "label": 0
                },
                {
                    "sent": "I have to say that basically the analysis of the user data set is pretty much done by Johannes because he worked on that data set during his work and there he already told me that there are these examples in the data set and that's why I came to it.",
                    "label": 0
                },
                {
                    "sent": "Alright then, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}