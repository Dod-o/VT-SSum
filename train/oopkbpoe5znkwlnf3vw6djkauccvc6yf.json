{
    "id": "oopkbpoe5znkwlnf3vw6djkauccvc6yf",
    "title": "Learning without Concentration",
    "info": {
        "author": [
            "Shahar Mendelson, Australian National University"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2014_mendelson_learning/",
    "segmentation": [
        [
            "So I'm going to start with."
        ],
        [
            "My main message.",
            "Um and my main message is, well, there are a few of them.",
            "First of all is that you can still do learning and statistics in situations where you concentration is not true.",
            "So I want to distinguish between we are not clever enough to prove concentration inequality's, but they are true in situations where concentration is simply false.",
            "I'm talking about the latter.",
            "The second thing is that well, because of technical reasons, we have the tendency of making assumptions that allow us to prove results right.",
            "We are, we want to prove results and sometimes we don't have the right machinery, so we make assumptions.",
            "And then the assumption sort of stick and it takes a long time to remove these assumptions.",
            "So one has to be very careful and the last thing a very smart guy told me once that people start by making you know working on a result, proving your result, using new tools and then a few years afterward they discovered that people actually started studying the tools rather than the results.",
            "So this is something that you should be careful of, right?",
            "Make sure that you know the question that you're trying to answer.",
            "And this is something that I've learned because I worked for a long time proving trying to prove or improve results that ended up to be totally useless for what I wanted.",
            "So just reminder, this is criticism about me, not about anyone else."
        ],
        [
            "OK, now the unnecessary assumptions in the context of learning that have been used are first uniformly bounded.",
            "Function classes, which is just a particular case of assuming an envelope function, Lipschitz losses that you need concentration inequality's like telegrams, concentration, inequality's for empirical processes, contraction, combinatorial VC, dimension.",
            "All of these are necessary.",
            "They are helpful.",
            "They are the basis of a wonderful set of problems, but you don't necessarily need them, so just bear that in mind."
        ],
        [
            "Now the problem that I'm going to present their or talk about today is a restricted set up.",
            "I I the methods can be applied to a much broader set up, but because of time constraint I'm I'm not going to talk about those and it's a situation where I have a function class, an unknown target, a loss function which will always always be the squared loss, and if that will be the empirical minimizer, F star is the best one in the class relative to the squared loss and I want to find.",
            "Function F had to show that the empirical minimizer is close to the best in the class.",
            "These results can be extended to other things like regularization and whatever you want, but just I want to focus on this."
        ],
        [
            "For the time being.",
            "Now.",
            "In general, the way you do this, the way you show that an empirical minimizer is well behaved so it's close to the true minimizer is by noticing that if you write the excess loss functional empirically, the minimizer will be non positive because F star the best in the class is a competitor, and it gives you 0.",
            "On the other hand, the true risk is always non negative and the game is to show that you have a gap between the.",
            "Empirical behavior in the actual behavior of of the empirical minimizer.",
            "In this way, you can you can find it.",
            "OK, that's roughly the idea."
        ],
        [
            "Now the standard method of analysis, the way that people have been studying this, is the following.",
            "You take your class, you look at the set of functions that are slightly far away from the minimizer, and what you want to show is that for those functions, the true risk is proportional to the empirical risk accessories.",
            "Now, since the true one is non negative and the empirical one is not positive for the minimizer, the minimizer can't live in that set.",
            "So that means that the minimizer.",
            "Is outside the set that you have dealt with, which means it's close to the best in the class.",
            "That's roughly the best philosophy up till now.",
            "No problems.",
            "Everyone's happy.",
            "The point is that the way this usually is proved this.",
            "Never mind the way this inequality usually works, there the top one is by proving the two sided inequality saying showing that the ratio estimate of the empirical divided by the actual deviates by one deviates from one by no more than 1/2.",
            "Now philosophically say OK, I wanted the 1 sided inequality.",
            "This is a 2 sided inequality.",
            "What's the big deal?",
            "Have I paid a big price and the point is, yes, you've paid a huge price.",
            "It takes time to see it, but it's true."
        ],
        [
            "Now just just to give you sort of a point of reference, the way of proving this this result in the classical setup is the following.",
            "You write the right the squared excess loss.",
            "You decompose it into two components, a quadratic component and a multiplier component.",
            "So everything is centered around the best in the class, and the linear term in F -- F star is multiplied by something.",
            "That's the noise term, right?",
            "It's the distance, it's the difference between.",
            "Ugh, it and the best function in the class from their use empirical processes.",
            "Methods you look at, the excess excess empirical excess loss functional and use symmetrization, then use.",
            "Contraction contraction kills the dependence on the loss.",
            "You're left with something that depends only on F minus star, and then you use concentration inequality's.",
            "Now for all this set of tools to work you need to make many many many assumptions."
        ],
        [
            "Right arm.",
            "This results that was mentioned in previous lecture is by Peter Bartlett available scan myself from 2005 and essentially it says that if you start with a class of functions that are convex class of functions that are bounded by one in a target that's bounded by one, you look at the function KN which is the localized Rademacher complexity.",
            "You look at a fixed point of that and that determines the estimate on the distance between the empirical minimizer and the best in the class.",
            "Now when you read it, yeah, it's you know when we prove that we were very happy, of course, but there are obvious problems with the statement.",
            "OK, first of all, the fact that you you restrict yourself to functions that are bounded by one and a target that's bounded by 1 means that there are whole set of problems you can't do with.",
            "For example Gaussian regression.",
            "It doesn't fall within this second thing you would expect your rates to improve once your noise level is low.",
            "The bouncer doesn't depend on the noise OK, and the last thing which you can see from the formulation of the result is even if you are in a bounded situation, right, you have a good L Infinity bound.",
            "The complexity parameter scales in the wrong way with the L Infinity bound you can't see it from the way I've written it, but."
        ],
        [
            "And well, this is a summary of what I said, but in the example of, you know, doing ERM in a multiple of the L1 and ball, which is, you know, used in lasuen compressed sensing in the whole wide thing wide variety of problems.",
            "If you apply the previous result, you get something that scales like our square and doesn't depend on the variance of the noise where the right weight scales like our times, the variance of the noise.",
            "Well depending on.",
            "On other parameters, but this is like the main trip, so it should decay with the noise and scale in the right way with the Infinity bound.",
            "Now."
        ],
        [
            "So.",
            "At this point you said, OK, you've used the wrong machinery to address the previous problem.",
            "That's why you were stuck.",
            "Well, I've spent a lot of time trying to develop the right machinery and it's really unpleasant in, you know, very technical, but it works only up to a point, right?",
            "If you are not willing to assume something very strong in the class, like sub Gaussian or slightly stronger, slightly weaker than sub Gaussian but still exponential decaying tails.",
            "I don't know how.",
            "I don't know how to.",
            "You know, expand the previous proof.",
            "Um, and Moreover, if you slightly decrease these assumptions, there's already strong assumptions.",
            "Concentration doesn't work.",
            "Contraction doesn't work, ratio estimate is false, right?",
            "It's not that I don't know how to prove it is that it's simply false.",
            "And the question is, can you deal with can you do learning in an unbounded scenario in which?",
            "Your random variables only exhibit heavy tails, which means polynomial decay rather than exponential one."
        ],
        [
            "Now the point is that when you take one nonnegative function in error, I've taken the squares because I'm going to talk about the squares saying that empirical means deviate from the actual mean by something that's proportional to the actual mean is actually two inequalities in upper tail in the lower T, and while the upper tail only holds with.",
            "Constant probability the way I've written here, you know, in a sense that Chebyshev's inequality is sharp.",
            "The lower tail holds with exponential probability in the sample size.",
            "So somehow for free, without any assumptions, you get very very high probability estimates, as long as you're only talking about the lower tails.",
            "And thankfully what we want is just the lower tip, not the opportune.",
            "That's that's sort of the key for everything."
        ],
        [
            "Now, as I written a few slides earlier, I've decomposed the empirical process.",
            "The excess risk to two components.",
            "One is a multiplier process, work season ID sample of the noise.",
            "Of course it need not be independent, right?",
            "It's just ID sampling noise and a quadratic.",
            "Now if you look if you just look at it, you see that the interaction with the noise only appears in the linear term.",
            "Write the quadratic is somehow noise free, which leads you to believe that as long as you can control the quadratic in the low noise situation, that will be the determining factor, right?",
            "And also under certain circumstances that's when exact recovery is possible.",
            "And for higher noise, high noise levels, the multiplier starts to become dominant, and now the question is how you know you you define parameters one which measures the dominance of the multiplier, the other measures the dominance of the of the quadratic and then what you would like to say is that the quadratic which is always positive dominates the multiplier, which could be negative and therefore the excess risk.",
            "Empirical risk is nonnegative.",
            "This is the way to get the exclusion principle."
        ],
        [
            "So.",
            "The parameter that is used to dominate the multiplier is essentially a localized version of the multiplier process, and the point is I don't know if you can see the colors, but the point is that when you compare it with the parameter used in the in the.",
            "Previous fear might mentioned essentially replaces the noise by Trivial L Infinity bound on the noise.",
            "The scaling is the same like you want to define the fixed point where this.",
            "Let's call it Rademacher complexity.",
            "Scales like the radius square square root of N. But in one you have the multiplier inside and then the other hand in the other one essentially erased it and remove replace it with a trivial estimate which is the L Infinity thing.",
            "So one scales correctly with with the noise.",
            "And the other one does.",
            "And the point is to show that above that level, this multiplier process is upper bounded by the distance between F&F Star Square.",
            "Yes."
        ],
        [
            "The quadratic term on the other hand has a similar term.",
            "Only the scaling is likely different.",
            "It's not scared like the radius square, but rather than the radius.",
            "And when you compare it with the again with the same.",
            "Complexity as before the Rademacher run you see that in the old one we had on one hand are square instead of arm, but on the left hand side we multiply things by the L Infinity bound, so again it's a much weaker estimate.",
            "OK, and the idea is to show that.",
            "If you're above a certain threshold, the quadratic is lower bounded by, again, a constant times the distance.",
            "Now, of course, you need to show that this holds and the question is what do you need to prove this result?",
            "And you need very little."
        ],
        [
            "All you need is a small blister.",
            "You need.",
            "You need to show that.",
            "Take all your functions in your class and you take you want to ensure that with constant probability the value is greater than some fixed parameter divided by the L2 norm.",
            "That's all you need, which is very minimal, right?",
            "Any type of moment equivalence gives you that, but you don't even need it.",
            "You can have it with just a second moment and without having two plus epsilon.",
            "And the main result is the."
        ],
        [
            "Once you have that.",
            "OK, once you have constants for which.",
            "This small bowl holds essentially.",
            "The distance between effect and F star is smaller than the maximum between these two parameters, which are always better than the ones in the theory might I'd mentioned.",
            "And also you don't need to assume anything on moments of class members, so the function classes just subsidy.",
            "Nell too, closed, convex.",
            "The target is just in L2.",
            "And all you need is a small property and if you check it, it gives you essentially with minimal assumptions it gives you the minimax rates in general.",
            "So that's the story.",
            "Thank you for your time."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to start with.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My main message.",
                    "label": 0
                },
                {
                    "sent": "Um and my main message is, well, there are a few of them.",
                    "label": 0
                },
                {
                    "sent": "First of all is that you can still do learning and statistics in situations where you concentration is not true.",
                    "label": 0
                },
                {
                    "sent": "So I want to distinguish between we are not clever enough to prove concentration inequality's, but they are true in situations where concentration is simply false.",
                    "label": 1
                },
                {
                    "sent": "I'm talking about the latter.",
                    "label": 0
                },
                {
                    "sent": "The second thing is that well, because of technical reasons, we have the tendency of making assumptions that allow us to prove results right.",
                    "label": 0
                },
                {
                    "sent": "We are, we want to prove results and sometimes we don't have the right machinery, so we make assumptions.",
                    "label": 0
                },
                {
                    "sent": "And then the assumption sort of stick and it takes a long time to remove these assumptions.",
                    "label": 0
                },
                {
                    "sent": "So one has to be very careful and the last thing a very smart guy told me once that people start by making you know working on a result, proving your result, using new tools and then a few years afterward they discovered that people actually started studying the tools rather than the results.",
                    "label": 0
                },
                {
                    "sent": "So this is something that you should be careful of, right?",
                    "label": 1
                },
                {
                    "sent": "Make sure that you know the question that you're trying to answer.",
                    "label": 0
                },
                {
                    "sent": "And this is something that I've learned because I worked for a long time proving trying to prove or improve results that ended up to be totally useless for what I wanted.",
                    "label": 0
                },
                {
                    "sent": "So just reminder, this is criticism about me, not about anyone else.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now the unnecessary assumptions in the context of learning that have been used are first uniformly bounded.",
                    "label": 1
                },
                {
                    "sent": "Function classes, which is just a particular case of assuming an envelope function, Lipschitz losses that you need concentration inequality's like telegrams, concentration, inequality's for empirical processes, contraction, combinatorial VC, dimension.",
                    "label": 0
                },
                {
                    "sent": "All of these are necessary.",
                    "label": 0
                },
                {
                    "sent": "They are helpful.",
                    "label": 0
                },
                {
                    "sent": "They are the basis of a wonderful set of problems, but you don't necessarily need them, so just bear that in mind.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the problem that I'm going to present their or talk about today is a restricted set up.",
                    "label": 0
                },
                {
                    "sent": "I I the methods can be applied to a much broader set up, but because of time constraint I'm I'm not going to talk about those and it's a situation where I have a function class, an unknown target, a loss function which will always always be the squared loss, and if that will be the empirical minimizer, F star is the best one in the class relative to the squared loss and I want to find.",
                    "label": 0
                },
                {
                    "sent": "Function F had to show that the empirical minimizer is close to the best in the class.",
                    "label": 1
                },
                {
                    "sent": "These results can be extended to other things like regularization and whatever you want, but just I want to focus on this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the time being.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "In general, the way you do this, the way you show that an empirical minimizer is well behaved so it's close to the true minimizer is by noticing that if you write the excess loss functional empirically, the minimizer will be non positive because F star the best in the class is a competitor, and it gives you 0.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, the true risk is always non negative and the game is to show that you have a gap between the.",
                    "label": 1
                },
                {
                    "sent": "Empirical behavior in the actual behavior of of the empirical minimizer.",
                    "label": 0
                },
                {
                    "sent": "In this way, you can you can find it.",
                    "label": 0
                },
                {
                    "sent": "OK, that's roughly the idea.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the standard method of analysis, the way that people have been studying this, is the following.",
                    "label": 1
                },
                {
                    "sent": "You take your class, you look at the set of functions that are slightly far away from the minimizer, and what you want to show is that for those functions, the true risk is proportional to the empirical risk accessories.",
                    "label": 0
                },
                {
                    "sent": "Now, since the true one is non negative and the empirical one is not positive for the minimizer, the minimizer can't live in that set.",
                    "label": 0
                },
                {
                    "sent": "So that means that the minimizer.",
                    "label": 0
                },
                {
                    "sent": "Is outside the set that you have dealt with, which means it's close to the best in the class.",
                    "label": 0
                },
                {
                    "sent": "That's roughly the best philosophy up till now.",
                    "label": 0
                },
                {
                    "sent": "No problems.",
                    "label": 0
                },
                {
                    "sent": "Everyone's happy.",
                    "label": 0
                },
                {
                    "sent": "The point is that the way this usually is proved this.",
                    "label": 1
                },
                {
                    "sent": "Never mind the way this inequality usually works, there the top one is by proving the two sided inequality saying showing that the ratio estimate of the empirical divided by the actual deviates by one deviates from one by no more than 1/2.",
                    "label": 0
                },
                {
                    "sent": "Now philosophically say OK, I wanted the 1 sided inequality.",
                    "label": 0
                },
                {
                    "sent": "This is a 2 sided inequality.",
                    "label": 0
                },
                {
                    "sent": "What's the big deal?",
                    "label": 0
                },
                {
                    "sent": "Have I paid a big price and the point is, yes, you've paid a huge price.",
                    "label": 0
                },
                {
                    "sent": "It takes time to see it, but it's true.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now just just to give you sort of a point of reference, the way of proving this this result in the classical setup is the following.",
                    "label": 0
                },
                {
                    "sent": "You write the right the squared excess loss.",
                    "label": 0
                },
                {
                    "sent": "You decompose it into two components, a quadratic component and a multiplier component.",
                    "label": 0
                },
                {
                    "sent": "So everything is centered around the best in the class, and the linear term in F -- F star is multiplied by something.",
                    "label": 1
                },
                {
                    "sent": "That's the noise term, right?",
                    "label": 0
                },
                {
                    "sent": "It's the distance, it's the difference between.",
                    "label": 0
                },
                {
                    "sent": "Ugh, it and the best function in the class from their use empirical processes.",
                    "label": 0
                },
                {
                    "sent": "Methods you look at, the excess excess empirical excess loss functional and use symmetrization, then use.",
                    "label": 1
                },
                {
                    "sent": "Contraction contraction kills the dependence on the loss.",
                    "label": 1
                },
                {
                    "sent": "You're left with something that depends only on F minus star, and then you use concentration inequality's.",
                    "label": 0
                },
                {
                    "sent": "Now for all this set of tools to work you need to make many many many assumptions.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right arm.",
                    "label": 0
                },
                {
                    "sent": "This results that was mentioned in previous lecture is by Peter Bartlett available scan myself from 2005 and essentially it says that if you start with a class of functions that are convex class of functions that are bounded by one in a target that's bounded by one, you look at the function KN which is the localized Rademacher complexity.",
                    "label": 1
                },
                {
                    "sent": "You look at a fixed point of that and that determines the estimate on the distance between the empirical minimizer and the best in the class.",
                    "label": 0
                },
                {
                    "sent": "Now when you read it, yeah, it's you know when we prove that we were very happy, of course, but there are obvious problems with the statement.",
                    "label": 0
                },
                {
                    "sent": "OK, first of all, the fact that you you restrict yourself to functions that are bounded by one and a target that's bounded by 1 means that there are whole set of problems you can't do with.",
                    "label": 0
                },
                {
                    "sent": "For example Gaussian regression.",
                    "label": 0
                },
                {
                    "sent": "It doesn't fall within this second thing you would expect your rates to improve once your noise level is low.",
                    "label": 0
                },
                {
                    "sent": "The bouncer doesn't depend on the noise OK, and the last thing which you can see from the formulation of the result is even if you are in a bounded situation, right, you have a good L Infinity bound.",
                    "label": 0
                },
                {
                    "sent": "The complexity parameter scales in the wrong way with the L Infinity bound you can't see it from the way I've written it, but.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And well, this is a summary of what I said, but in the example of, you know, doing ERM in a multiple of the L1 and ball, which is, you know, used in lasuen compressed sensing in the whole wide thing wide variety of problems.",
                    "label": 1
                },
                {
                    "sent": "If you apply the previous result, you get something that scales like our square and doesn't depend on the variance of the noise where the right weight scales like our times, the variance of the noise.",
                    "label": 0
                },
                {
                    "sent": "Well depending on.",
                    "label": 0
                },
                {
                    "sent": "On other parameters, but this is like the main trip, so it should decay with the noise and scale in the right way with the Infinity bound.",
                    "label": 1
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "At this point you said, OK, you've used the wrong machinery to address the previous problem.",
                    "label": 0
                },
                {
                    "sent": "That's why you were stuck.",
                    "label": 0
                },
                {
                    "sent": "Well, I've spent a lot of time trying to develop the right machinery and it's really unpleasant in, you know, very technical, but it works only up to a point, right?",
                    "label": 0
                },
                {
                    "sent": "If you are not willing to assume something very strong in the class, like sub Gaussian or slightly stronger, slightly weaker than sub Gaussian but still exponential decaying tails.",
                    "label": 0
                },
                {
                    "sent": "I don't know how.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to.",
                    "label": 0
                },
                {
                    "sent": "You know, expand the previous proof.",
                    "label": 0
                },
                {
                    "sent": "Um, and Moreover, if you slightly decrease these assumptions, there's already strong assumptions.",
                    "label": 0
                },
                {
                    "sent": "Concentration doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Contraction doesn't work, ratio estimate is false, right?",
                    "label": 1
                },
                {
                    "sent": "It's not that I don't know how to prove it is that it's simply false.",
                    "label": 1
                },
                {
                    "sent": "And the question is, can you deal with can you do learning in an unbounded scenario in which?",
                    "label": 0
                },
                {
                    "sent": "Your random variables only exhibit heavy tails, which means polynomial decay rather than exponential one.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the point is that when you take one nonnegative function in error, I've taken the squares because I'm going to talk about the squares saying that empirical means deviate from the actual mean by something that's proportional to the actual mean is actually two inequalities in upper tail in the lower T, and while the upper tail only holds with.",
                    "label": 1
                },
                {
                    "sent": "Constant probability the way I've written here, you know, in a sense that Chebyshev's inequality is sharp.",
                    "label": 1
                },
                {
                    "sent": "The lower tail holds with exponential probability in the sample size.",
                    "label": 1
                },
                {
                    "sent": "So somehow for free, without any assumptions, you get very very high probability estimates, as long as you're only talking about the lower tails.",
                    "label": 0
                },
                {
                    "sent": "And thankfully what we want is just the lower tip, not the opportune.",
                    "label": 0
                },
                {
                    "sent": "That's that's sort of the key for everything.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, as I written a few slides earlier, I've decomposed the empirical process.",
                    "label": 0
                },
                {
                    "sent": "The excess risk to two components.",
                    "label": 0
                },
                {
                    "sent": "One is a multiplier process, work season ID sample of the noise.",
                    "label": 1
                },
                {
                    "sent": "Of course it need not be independent, right?",
                    "label": 1
                },
                {
                    "sent": "It's just ID sampling noise and a quadratic.",
                    "label": 1
                },
                {
                    "sent": "Now if you look if you just look at it, you see that the interaction with the noise only appears in the linear term.",
                    "label": 0
                },
                {
                    "sent": "Write the quadratic is somehow noise free, which leads you to believe that as long as you can control the quadratic in the low noise situation, that will be the determining factor, right?",
                    "label": 0
                },
                {
                    "sent": "And also under certain circumstances that's when exact recovery is possible.",
                    "label": 0
                },
                {
                    "sent": "And for higher noise, high noise levels, the multiplier starts to become dominant, and now the question is how you know you you define parameters one which measures the dominance of the multiplier, the other measures the dominance of the of the quadratic and then what you would like to say is that the quadratic which is always positive dominates the multiplier, which could be negative and therefore the excess risk.",
                    "label": 1
                },
                {
                    "sent": "Empirical risk is nonnegative.",
                    "label": 0
                },
                {
                    "sent": "This is the way to get the exclusion principle.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The parameter that is used to dominate the multiplier is essentially a localized version of the multiplier process, and the point is I don't know if you can see the colors, but the point is that when you compare it with the parameter used in the in the.",
                    "label": 0
                },
                {
                    "sent": "Previous fear might mentioned essentially replaces the noise by Trivial L Infinity bound on the noise.",
                    "label": 0
                },
                {
                    "sent": "The scaling is the same like you want to define the fixed point where this.",
                    "label": 0
                },
                {
                    "sent": "Let's call it Rademacher complexity.",
                    "label": 0
                },
                {
                    "sent": "Scales like the radius square square root of N. But in one you have the multiplier inside and then the other hand in the other one essentially erased it and remove replace it with a trivial estimate which is the L Infinity thing.",
                    "label": 0
                },
                {
                    "sent": "So one scales correctly with with the noise.",
                    "label": 0
                },
                {
                    "sent": "And the other one does.",
                    "label": 0
                },
                {
                    "sent": "And the point is to show that above that level, this multiplier process is upper bounded by the distance between F&F Star Square.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The quadratic term on the other hand has a similar term.",
                    "label": 1
                },
                {
                    "sent": "Only the scaling is likely different.",
                    "label": 0
                },
                {
                    "sent": "It's not scared like the radius square, but rather than the radius.",
                    "label": 0
                },
                {
                    "sent": "And when you compare it with the again with the same.",
                    "label": 0
                },
                {
                    "sent": "Complexity as before the Rademacher run you see that in the old one we had on one hand are square instead of arm, but on the left hand side we multiply things by the L Infinity bound, so again it's a much weaker estimate.",
                    "label": 0
                },
                {
                    "sent": "OK, and the idea is to show that.",
                    "label": 1
                },
                {
                    "sent": "If you're above a certain threshold, the quadratic is lower bounded by, again, a constant times the distance.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, you need to show that this holds and the question is what do you need to prove this result?",
                    "label": 0
                },
                {
                    "sent": "And you need very little.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All you need is a small blister.",
                    "label": 0
                },
                {
                    "sent": "You need.",
                    "label": 0
                },
                {
                    "sent": "You need to show that.",
                    "label": 0
                },
                {
                    "sent": "Take all your functions in your class and you take you want to ensure that with constant probability the value is greater than some fixed parameter divided by the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "That's all you need, which is very minimal, right?",
                    "label": 0
                },
                {
                    "sent": "Any type of moment equivalence gives you that, but you don't even need it.",
                    "label": 0
                },
                {
                    "sent": "You can have it with just a second moment and without having two plus epsilon.",
                    "label": 0
                },
                {
                    "sent": "And the main result is the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Once you have that.",
                    "label": 0
                },
                {
                    "sent": "OK, once you have constants for which.",
                    "label": 1
                },
                {
                    "sent": "This small bowl holds essentially.",
                    "label": 0
                },
                {
                    "sent": "The distance between effect and F star is smaller than the maximum between these two parameters, which are always better than the ones in the theory might I'd mentioned.",
                    "label": 0
                },
                {
                    "sent": "And also you don't need to assume anything on moments of class members, so the function classes just subsidy.",
                    "label": 0
                },
                {
                    "sent": "Nell too, closed, convex.",
                    "label": 0
                },
                {
                    "sent": "The target is just in L2.",
                    "label": 0
                },
                {
                    "sent": "And all you need is a small property and if you check it, it gives you essentially with minimal assumptions it gives you the minimax rates in general.",
                    "label": 1
                },
                {
                    "sent": "So that's the story.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your time.",
                    "label": 0
                }
            ]
        }
    }
}