{
    "id": "armkzbv2xioi4opqmeubjxcjwhiewnnb",
    "title": "Efficient Temporal Reasoning on Streams of Events with DOTR",
    "info": {
        "author": [
            "Alessandro Margara, Politecnico di Milano"
        ],
        "published": "July 10, 2018",
        "recorded": "June 2018",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2018_margara_temporal_reasoning/",
    "segmentation": [
        [
            "Sort of introduction.",
            "Good afternoon everybody.",
            "Melisandre Margara from Politecnico di Milano and is a joint work with other colleagues from Politecnico, John, Paolo and Dario and with Daniela Dallaglio from University of Zurich.",
            "So the topic of our talk is stream reasoning.",
            "Stream reasoning involves making sense of large volumes or streaming data in near real time, possibly integrated with some rich background knowledge about the application domain with the."
        ],
        [
            "All or supporting.",
            "Decision-making so manual or automated decision making reaction to what we see to the fact that we see from the streaming data."
        ],
        [
            "Here at least some applications that are typical applications that can take advantage of stream reasoning as found in some recent publications on the topic.",
            "They include trend detection in social media, smart cities, credit card fraud detection software, system monitoring, and many others.",
            "Starting around these application domain."
        ],
        [
            "Same here, I tried to summarize some of the requirements that I think are fundamental for string reasoning.",
            "First, flexible data model that enables to integrate to combine data coming from multiple and heterogeneous sources.",
            "Second, support, as I mentioned for rich background knowledge that describes your application domain.",
            "3rd performance in terms of high throughput, so I must be able to process large volumes of input streaming data.",
            "And in terms of low latency, so I must be able to provide answers as soon as possible because the answers are needed to take decisions, and in some cases it might be critical.",
            "For example, can be danger and I want to react as fast as possible.",
            "Finally, many applications require expressive reasoning and by expressive reasoning I include temporal reasoning.",
            "So the capability to reason about the order and the temporal relations between the facts that I observe from the stream."
        ],
        [
            "OK, so if we look at the state of the art we respect to our four.",
            "Our four requirements here.",
            "I list the traditional stream processing technologies as well as.",
            "Research tools from RDF stream processing.",
            "An stream reasoning.",
            "So in terms of flexible data model, as the name suggests, RDF stream processing or stream reasoning already provides a good support for flexibility by using RDF as a data model.",
            "It also supports rich background knowledge, including the possibility to do to perform on to logical reasoning, inference on the streaming and the background knowledge which is not the case for traditional string processing technologies in terms of performance, there are some very mature stream processing engines.",
            "In the in the research on our, the upstream processing and stream reasoning, I would put variable performance because here researchers have investigated mainly the tradeoff between expressivity, an performance, so I think it really depends on the specific system you are analyzing in terms of expressive reasoning.",
            "I would argue that the current.",
            "Mainstream approach for all the upstream processing has a limited temp support for temporal reasoning, and I will show you the model to support these this claim.",
            "So the main model forced."
        ],
        [
            "Same reasoning works in this way you have a background knowledge and the background knowledge is encoded as a possibly large."
        ],
        [
            "RDF graph.",
            "Then"
        ],
        [
            "The data in your stream is also represented as RDF triples or graphs, depending on the specific system you are considering, and they are time and notated so at time T equals to one, you have some data at times T equals to two.",
            "You have some other data.",
            "So how you perform the processing?",
            "Well, you have some standing queries, so some queries that always remain there and you continuously you repeatedly.",
            "Update the results of these queries based on the current data.",
            "What I mean by the current data.",
            "Well you user."
        ],
        [
            "Formalism, which is known as window and the window mechanism captures the recent portion of the streaming data.",
            "For example, here I have a window of size 2 that captures the last two points in time, so at time T2 here is where my window is.",
            "It captures both the data that is in T = 2 and the data at the previous point in time.",
            "Then I consider as through everything that is in the background knowledge plus everything that is in the window.",
            "I can perform inference so I can do ontological reasoning and then I use these results to answer my extending queries and for example this can be the result that integrates both the streaming and the background knowledge.",
            "OK, then, at time T = 2 three the window is low."
        ],
        [
            "Lights."
        ],
        [
            "OK, so windows size and this slide, which means when they move in this case they slide every one second for example.",
            "So at time T = 2 three the Windows light and I can compute again there."
        ],
        [
            "Lots to my query, which can be different from the previous ones."
        ],
        [
            "OK, this model is good to update the results to standing queries.",
            "It's conceived for that reason.",
            "But presents some problems when it is combined with temporal reasoning and I show you some examples.",
            "So first everything that is in the current window is considered as true.",
            "So imagine, for example, that a time T equals to two, there is some information that says there is a talk talk one in room 42.",
            "OK, then at time T = 2 three."
        ],
        [
            "There is another information."
        ],
        [
            "These are talk 2 in room 42.",
            "So if I consider both in formation as true, I can arrive to the wrong."
        ],
        [
            "Conclusion that there are two talks going on in parallel in the same in the same room."
        ],
        [
            "Another problem is when I try to reason on the temporal relations between data items.",
            "For example, if this is required in many applications I mentioned for example trend analysis in Twitter or social media in general.",
            "For example, I want to detect some event, some fact X follow by affect Y within three time units.",
            "What happens so first they have to identify a window.",
            "Let's say that I identify a window Side 3 because that's required to identify a pattern which is long three time units.",
            "OK, so."
        ],
        [
            "So here I have my window icon."
        ],
        [
            "Cute my results and I find X followed by Y, then the Windows lights.",
            "And then."
        ],
        [
            "And again, I find X followed by Y.",
            "So the point here is that if I want to notify the user whenever something happens in time, here I have duplicate detection.",
            "So I find the pattern X followed by Y twice, even if it is driven by the same data and that might be undesired.",
            "OK, one can claim."
        ],
        [
            "You can change the slide of your window so for exam."
        ],
        [
            "Or you can make your window.",
            "We say tumbling so it never overlaps in this way.",
            "If two subsequent Windows never overlap, then you have the guarantee that you never detect the same pattern twice the same sequence twice.",
            "OK, fine, let's try with a tumbling window of size and slide three.",
            "Unfortunately here it depends on the synchronization of the window with respect to the data that you have.",
            "So for example, if the window is synchronizing that way.",
            "So here you don't have X followed by Y because you only have X.",
            "Then you move the window."
        ],
        [
            "And then you only have Y, so you don't have X followed by Y.",
            "So here the occurrence is not detected.",
            "And in general, in general it is impossible to determine up front window size and the Windows Live that always avoid duplication and loss, so it's a more general problem."
        ],
        [
            "OK, so to solve these problems we introduce our model, the Daughter model which stands for the Touch.",
            "The coupled sorry ontological and temporal reasoning.",
            "And the goal is to introduce a paradigm that provides the temporal reasoning capabilities of stream processing technologies, and in particular of event processing complexity and processing technologies with the flexibility of stream reasoning systems in terms of data model and background knowledge, while maintaining a high level of performance.",
            "So how does it work?",
            "So the model decouples."
        ],
        [
            "The ontological and temporal reasoning and ontological reasoning takes place at each point in time separately.",
            "So we consider all the data that is."
        ],
        [
            "That arrives at time T equals to one, and we derive."
        ],
        [
            "All the facts that."
        ],
        [
            "Can be derived from what happens at D equals to 1 plus the background knowledge and the same for the other."
        ],
        [
            "Time.",
            "OK."
        ],
        [
            "And these are our."
        ],
        [
            "Facts of interest that take place in different points in time and then."
        ],
        [
            "Apparently we perform a temporal reasoning to correlate in time the different facts.",
            "So to be."
        ],
        [
            "More precise, we use Datalog rules to combine the background knowledge plus the information that is valid at time T, and we use parkour queries to extract relevant facts.",
            "So we do ontological reasoning, including everything that is valid at time T plus everything that is in the background knowledge.",
            "We have our set of information and from this information we extract the relevant facts that we call events using Sparkle queries.",
            "I will show you an example of query in.",
            "In a couple of slides.",
            "And then we use temporal reasoning to correlate."
        ],
        [
            "8.",
            "Events together into patterns of events, temporal patterns of events.",
            "So to correlate as a formalism to correlate in time, we use our previous work which is event processing language called test."
        ],
        [
            "So let me first introduce Tesla.",
            "So here I want to highlight only the temporal aspects.",
            "So this is not the model I'm presenting here, but only the temporal aspects of the model.",
            "And then I show you how I integrate these into RDF.",
            "So in Tesla you can define, so here we don't have RDF again, so we have more simple and structured data.",
            "In Tesla you can define new concepts.",
            "For example the concept or complex event of fire starting from primitive events.",
            "So you select the primitive events that you can receive, for example from sensors.",
            "So you select individual events, then you combine them in temporal sequences.",
            "So for example here I say I want smoke and smoke must be preceded by a temperature with a given value within a given time span.",
            "So we have metric, temporal temporal logic.",
            "So we have the possibility to express time constraints.",
            "Then we can have parameters to bind the content of different events overtime.",
            "Then we have the possibility to express negations, so something that must not occur in a given time span.",
            "And then they are not shown here, but it is more expressive than these.",
            "You can do some computations, for example, you can require the average temperature to be higher than a given threshold in a given period of time.",
            "We have customizable selection and consumption policy.",
            "So for example here if I have a sequence with a smoke and I have several events of type temperature, which one do I pick in this case I select the last one because there is the keyword.",
            "Last, but they can customize the behavior.",
            "I can take the first one in that time span.",
            "I can take each possible.",
            "Temperature event and create a different fire for each of them.",
            "And customizable consumption policies consumption means the which events that participate in a rule are not available anymore for further triggering of the same rule.",
            "So it's expressive in this sense, and then you have hierarchies to build complex events starting from other complex events, so you can create hierarchies of events.",
            "And the temporal reasoning of Tesla you can find in the original Tesla paper is formally defined through metric temporal logic formulas.",
            "So in practice in our model we are using.",
            "Sparkle to extract events and then we a fragment of a metric temporal logic formulas to combine different events today."
        ],
        [
            "And this is how the daughter language looks like.",
            "It's identical to Tesla, but the selection is now done by means of sparkle queries.",
            "So the event definition, starting from the streaming and the background knowledge plus all implicit knowledge that you can derive using reasoning, are extracted using the sparkle queries."
        ],
        [
            "OK, that was the model and the language.",
            "What about implementation?",
            "So the fact that we be couple ontological and temporal reasoning enables us to reuse existing systems to perform the two tasks.",
            "Basically, and that's the current implementation.",
            "The current prototype that we have, and as I will show, we already have good results.",
            "But we will investigate in the future how we can exploit.",
            "The more the integration of the two parts.",
            "So basically what we do is we have we start from let's take on top.",
            "You have a daughter rule.",
            "You have a rule parcel that splits the different parts.",
            "Install so this particle queries into RDF store.",
            "Then it uses the native.",
            "Engine for Tesla rules, which is called T. Rex, and then it uses a graph definition to transform back the results of T. Rex into RDF graphs.",
            "So when you receive time annotated RBF graphs, you put your new information into the background knowledge as a background knowledge we use.",
            "Are there folks which is which enables us to have a high level of performance because it is in memory and it also enables incremental reasoning so we can incrementally compute all the."
        ],
        [
            "The revisions that derive from the GNU RBF graphs that are inserted into the.",
            "Background knowledge then we run the sparkle queries and the sparkle queries define our events, then the events are translated into the format of T Rex.",
            "T Rex processes the.",
            "Temporal part produces some results, and the results are translated back to RDF and to present to be presented as output.",
            "OK."
        ],
        [
            "As for the evaluation, we had two types of experiments, comparison with state of the art solutions and sensitivity to parameters.",
            "Comparing with the state of the art, we consider two representatives of the stream reasoning domain.",
            "This parlance equals an one representative of the stream processing community which should be which is designed to be fast but less expressive esper.",
            "And we use the CD bench benchmark which was presented at IWC.",
            "Here I have to say that the city bench queries are based on Windows on the mechanism of Windows, so basically.",
            "Our tool and the other tools that are based on Windows provide different semantics, but we try to make the things compatible so too.",
            "Try to have the same answer set even if the semantics are slightly different, because there are no established benchmarks for stream reasoning that provide different formalisms.",
            "So here is."
        ],
        [
            "Comparison, as you can see.",
            "So first in in Esper we.",
            "Manually encoded the ontological reasoning because it's not expressive enough, so it doesn't include ontological reasoning, so we had to include it manually and that's why, as you can see here.",
            "So here in presenting the processing time for each input RDF graph based on the frequency of arrival while changing the frequency of arrival on the X axis.",
            "As you can see, Esper is the one that performs better while daughter.",
            "Presents much better performance with respect to the sparkle and sequels, not is the logarithmic scale."
        ],
        [
            "In the case of a different query query Coutu query two, we also achieve even better performance.",
            "Much better also than Esper, because here we can really exploit our additional.",
            "Expressivity to better tune the task to answer the rule.",
            "So here we need to get the current weather condition in this particular query.",
            "Since we want the current so the last one that we have seen from the streams.",
            "If we use Windows we have to periodically re evaluate everything when we receive new information, which is why all the other systems perform.",
            "Worse than ours while daughter avoids duplicated valuations and distractions."
        ],
        [
            "Finally, sensitivity to parameters.",
            "We also set up a default scenario.",
            "With with synthetic data and we study the sensitivity while changing one parameter at a time.",
            "You can find all the details of this scenario on the paper here.",
            "What I can show you is that we notice that the most of the part in our current implementation, most of the cost for answering the queries is due."
        ],
        [
            "To query evaluation to sparkle query evaluation, meaning that in this scenario ontological reasoning an temporary zoning event processing are less expensive.",
            "They account for almost 1/3 of the time, while the query valuation accounts for 2/3 of the time.",
            "OK."
        ],
        [
            "And then we change some parameters.",
            "The number of rules, the length of the event patterns, the size of the background knowledge, the number of Datalog rules, and here are the results that we see.",
            "So basically from the first 2 graphs you can see that the number of rules with the number of rules we have almost stable performance up to a certain point, and then at that point the number of rules.",
            "So the number of sparkle queries that you have to run.",
            "Or the number of data log rules that you have to compute start to become the bottleneck and the performance decreases.",
            "Notice however, that on the X axis you have a logarithmic scale and still even in the most challenging case you are below 500 milliseconds.",
            "Then we study temporal reasoning.",
            "Temporal reasoning is not really expensive because it's done in an efficient complex event processing engine.",
            "And finally, we notice that the size if we remain in the main memory, the size of the knowledge base is not really relevant to does not really influence the processing time, which is dominated by other factors."
        ],
        [
            "And that leads me to the conclusions.",
            "So I presented the daughter model with the idea of the coupling, ontological and temporal reasoning for stream reasoning applications.",
            "I show the comparison with state of the art solutions, showing that we are more expressive in terms of temporal reasoning, so we provide more operators and more possibility to tune your temporal reasoning, and that sometimes can lead to better performance because you can better specify how you want to answer your queries.",
            "Again, our current implementation is based on a prototype that reuses as a black box existing tools, so we are currently investigating optimized implementations and also in the paper there is some initial investigation in this area that shows some promising results and then we want to compare with other existing formalisms.",
            "For example, recently in SWC there was a presentation on stream reasoning engine based on ASP.",
            "Thanks a lot for."
        ],
        [
            "Your attention and if you have answering here.",
            "Yeah hi, thanks for a very interesting talk.",
            "Maybe I missed it, but so you mentioned for Esther you had to manually implement ontology reasoning.",
            "How did you do that?",
            "Did you use some sort of rule based standard custom entailment rules set?",
            "Or no?",
            "I don't know.",
            "So basically, since we had the rules so we knew what we wanted, so we manually added all the implicit in formation within the streaming data.",
            "Streaming data was augmented with these was enriched with all the attributes that you could derive from reasoning, but that was really tailored to answer that specific query.",
            "To be fair in the comparison.",
            "OK, OK. Yeah I have a second question.",
            "Well not more.",
            "More of a more of a comment really.",
            "I work for a research group that specializes in health informatics and a big topic hot topic in that domain is so-called alert fatigue.",
            "Physicians having every other minute or so having an alert pop up because of a patient has some blood values or some some temperature changes and some recent work and there is.",
            "Kind of counting trying to read out the irrelevant alerts by focusing on temporal patterns, right?",
            "It doesn't matter if you have a temperature, but if that temperature increased X number of percent during a 5 minute time window, then you have a problem, right?",
            "So do you see your work somehow being applicable to that?",
            "Those kinds of problems as well?",
            "Yes, yes, absolutely.",
            "So I would say that the traditional model based on Windows is continuously updates results.",
            "Two queries instead, our model detects temporal patterns, which it seems to be what you are looking for.",
            "An I didn't go much into the detail of Tesla, but you can check out the original paper.",
            "There is also the formalism.",
            "Yeah, using hierarchies of events you can also detect trends like the one that you mentioned.",
            "So for example, I want an always increasing temperature followed by something else.",
            "Yeah, so you can basically derive the your main building blocks.",
            "Like increasing temperature and then you iterate by having a recursive rule, you can iterate over these and find patterns of always increasing temperature.",
            "OK, but probably I lost followers some but I wasn't.",
            "I just want to confirm.",
            "That's what you are doing is.",
            "Her doing temporal reasoning, execution so, so you probably you have a one result for one temporal execution San and how you combine this with the next temporal execution.",
            "So it's like because for me it's like how to find a pattern between these temporal and how to how you re late this this temporal execution.",
            "So thank you.",
            "Yeah, thank you, so it's finding patterns.",
            "Yeah, I agree.",
            "My question was on this like the bottleneck of Sparkle queries.",
            "You said that this was the main bottleneck, so do you figure out some way to improve this?",
            "Like for instance, I know taking some caching mechanisms between different times or so we are.",
            "We are investigating that one.",
            "One possible mechanism that we also investigating the paper so it's also present in the paper is to merge all the queries that are similar.",
            "So basically if the queries only differ.",
            "For the selection path, so there is a filter as particle filter, you remove the spark or filter so you reduce the number of queries and then you delegate the filter into the complex event processing engine which is faster.",
            "But of course other possibilities like caching are also possible.",
            "Thank you, thanks a lot, thanks again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sort of introduction.",
                    "label": 0
                },
                {
                    "sent": "Good afternoon everybody.",
                    "label": 0
                },
                {
                    "sent": "Melisandre Margara from Politecnico di Milano and is a joint work with other colleagues from Politecnico, John, Paolo and Dario and with Daniela Dallaglio from University of Zurich.",
                    "label": 0
                },
                {
                    "sent": "So the topic of our talk is stream reasoning.",
                    "label": 0
                },
                {
                    "sent": "Stream reasoning involves making sense of large volumes or streaming data in near real time, possibly integrated with some rich background knowledge about the application domain with the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All or supporting.",
                    "label": 0
                },
                {
                    "sent": "Decision-making so manual or automated decision making reaction to what we see to the fact that we see from the streaming data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here at least some applications that are typical applications that can take advantage of stream reasoning as found in some recent publications on the topic.",
                    "label": 0
                },
                {
                    "sent": "They include trend detection in social media, smart cities, credit card fraud detection software, system monitoring, and many others.",
                    "label": 1
                },
                {
                    "sent": "Starting around these application domain.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Same here, I tried to summarize some of the requirements that I think are fundamental for string reasoning.",
                    "label": 0
                },
                {
                    "sent": "First, flexible data model that enables to integrate to combine data coming from multiple and heterogeneous sources.",
                    "label": 1
                },
                {
                    "sent": "Second, support, as I mentioned for rich background knowledge that describes your application domain.",
                    "label": 1
                },
                {
                    "sent": "3rd performance in terms of high throughput, so I must be able to process large volumes of input streaming data.",
                    "label": 0
                },
                {
                    "sent": "And in terms of low latency, so I must be able to provide answers as soon as possible because the answers are needed to take decisions, and in some cases it might be critical.",
                    "label": 0
                },
                {
                    "sent": "For example, can be danger and I want to react as fast as possible.",
                    "label": 0
                },
                {
                    "sent": "Finally, many applications require expressive reasoning and by expressive reasoning I include temporal reasoning.",
                    "label": 0
                },
                {
                    "sent": "So the capability to reason about the order and the temporal relations between the facts that I observe from the stream.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so if we look at the state of the art we respect to our four.",
                    "label": 1
                },
                {
                    "sent": "Our four requirements here.",
                    "label": 0
                },
                {
                    "sent": "I list the traditional stream processing technologies as well as.",
                    "label": 1
                },
                {
                    "sent": "Research tools from RDF stream processing.",
                    "label": 0
                },
                {
                    "sent": "An stream reasoning.",
                    "label": 0
                },
                {
                    "sent": "So in terms of flexible data model, as the name suggests, RDF stream processing or stream reasoning already provides a good support for flexibility by using RDF as a data model.",
                    "label": 1
                },
                {
                    "sent": "It also supports rich background knowledge, including the possibility to do to perform on to logical reasoning, inference on the streaming and the background knowledge which is not the case for traditional string processing technologies in terms of performance, there are some very mature stream processing engines.",
                    "label": 0
                },
                {
                    "sent": "In the in the research on our, the upstream processing and stream reasoning, I would put variable performance because here researchers have investigated mainly the tradeoff between expressivity, an performance, so I think it really depends on the specific system you are analyzing in terms of expressive reasoning.",
                    "label": 0
                },
                {
                    "sent": "I would argue that the current.",
                    "label": 0
                },
                {
                    "sent": "Mainstream approach for all the upstream processing has a limited temp support for temporal reasoning, and I will show you the model to support these this claim.",
                    "label": 0
                },
                {
                    "sent": "So the main model forced.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same reasoning works in this way you have a background knowledge and the background knowledge is encoded as a possibly large.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "RDF graph.",
                    "label": 0
                },
                {
                    "sent": "Then",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The data in your stream is also represented as RDF triples or graphs, depending on the specific system you are considering, and they are time and notated so at time T equals to one, you have some data at times T equals to two.",
                    "label": 0
                },
                {
                    "sent": "You have some other data.",
                    "label": 0
                },
                {
                    "sent": "So how you perform the processing?",
                    "label": 0
                },
                {
                    "sent": "Well, you have some standing queries, so some queries that always remain there and you continuously you repeatedly.",
                    "label": 0
                },
                {
                    "sent": "Update the results of these queries based on the current data.",
                    "label": 0
                },
                {
                    "sent": "What I mean by the current data.",
                    "label": 0
                },
                {
                    "sent": "Well you user.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formalism, which is known as window and the window mechanism captures the recent portion of the streaming data.",
                    "label": 0
                },
                {
                    "sent": "For example, here I have a window of size 2 that captures the last two points in time, so at time T2 here is where my window is.",
                    "label": 0
                },
                {
                    "sent": "It captures both the data that is in T = 2 and the data at the previous point in time.",
                    "label": 0
                },
                {
                    "sent": "Then I consider as through everything that is in the background knowledge plus everything that is in the window.",
                    "label": 0
                },
                {
                    "sent": "I can perform inference so I can do ontological reasoning and then I use these results to answer my extending queries and for example this can be the result that integrates both the streaming and the background knowledge.",
                    "label": 0
                },
                {
                    "sent": "OK, then, at time T = 2 three the window is low.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lights.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so windows size and this slide, which means when they move in this case they slide every one second for example.",
                    "label": 0
                },
                {
                    "sent": "So at time T = 2 three the Windows light and I can compute again there.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lots to my query, which can be different from the previous ones.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this model is good to update the results to standing queries.",
                    "label": 0
                },
                {
                    "sent": "It's conceived for that reason.",
                    "label": 0
                },
                {
                    "sent": "But presents some problems when it is combined with temporal reasoning and I show you some examples.",
                    "label": 1
                },
                {
                    "sent": "So first everything that is in the current window is considered as true.",
                    "label": 0
                },
                {
                    "sent": "So imagine, for example, that a time T equals to two, there is some information that says there is a talk talk one in room 42.",
                    "label": 0
                },
                {
                    "sent": "OK, then at time T = 2 three.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is another information.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are talk 2 in room 42.",
                    "label": 0
                },
                {
                    "sent": "So if I consider both in formation as true, I can arrive to the wrong.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conclusion that there are two talks going on in parallel in the same in the same room.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another problem is when I try to reason on the temporal relations between data items.",
                    "label": 0
                },
                {
                    "sent": "For example, if this is required in many applications I mentioned for example trend analysis in Twitter or social media in general.",
                    "label": 0
                },
                {
                    "sent": "For example, I want to detect some event, some fact X follow by affect Y within three time units.",
                    "label": 1
                },
                {
                    "sent": "What happens so first they have to identify a window.",
                    "label": 0
                },
                {
                    "sent": "Let's say that I identify a window Side 3 because that's required to identify a pattern which is long three time units.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here I have my window icon.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cute my results and I find X followed by Y, then the Windows lights.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And again, I find X followed by Y.",
                    "label": 1
                },
                {
                    "sent": "So the point here is that if I want to notify the user whenever something happens in time, here I have duplicate detection.",
                    "label": 0
                },
                {
                    "sent": "So I find the pattern X followed by Y twice, even if it is driven by the same data and that might be undesired.",
                    "label": 0
                },
                {
                    "sent": "OK, one can claim.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can change the slide of your window so for exam.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or you can make your window.",
                    "label": 0
                },
                {
                    "sent": "We say tumbling so it never overlaps in this way.",
                    "label": 0
                },
                {
                    "sent": "If two subsequent Windows never overlap, then you have the guarantee that you never detect the same pattern twice the same sequence twice.",
                    "label": 0
                },
                {
                    "sent": "OK, fine, let's try with a tumbling window of size and slide three.",
                    "label": 1
                },
                {
                    "sent": "Unfortunately here it depends on the synchronization of the window with respect to the data that you have.",
                    "label": 0
                },
                {
                    "sent": "So for example, if the window is synchronizing that way.",
                    "label": 1
                },
                {
                    "sent": "So here you don't have X followed by Y because you only have X.",
                    "label": 0
                },
                {
                    "sent": "Then you move the window.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then you only have Y, so you don't have X followed by Y.",
                    "label": 1
                },
                {
                    "sent": "So here the occurrence is not detected.",
                    "label": 0
                },
                {
                    "sent": "And in general, in general it is impossible to determine up front window size and the Windows Live that always avoid duplication and loss, so it's a more general problem.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so to solve these problems we introduce our model, the Daughter model which stands for the Touch.",
                    "label": 1
                },
                {
                    "sent": "The coupled sorry ontological and temporal reasoning.",
                    "label": 1
                },
                {
                    "sent": "And the goal is to introduce a paradigm that provides the temporal reasoning capabilities of stream processing technologies, and in particular of event processing complexity and processing technologies with the flexibility of stream reasoning systems in terms of data model and background knowledge, while maintaining a high level of performance.",
                    "label": 0
                },
                {
                    "sent": "So how does it work?",
                    "label": 0
                },
                {
                    "sent": "So the model decouples.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The ontological and temporal reasoning and ontological reasoning takes place at each point in time separately.",
                    "label": 0
                },
                {
                    "sent": "So we consider all the data that is.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That arrives at time T equals to one, and we derive.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All the facts that.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can be derived from what happens at D equals to 1 plus the background knowledge and the same for the other.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are our.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Facts of interest that take place in different points in time and then.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apparently we perform a temporal reasoning to correlate in time the different facts.",
                    "label": 0
                },
                {
                    "sent": "So to be.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More precise, we use Datalog rules to combine the background knowledge plus the information that is valid at time T, and we use parkour queries to extract relevant facts.",
                    "label": 0
                },
                {
                    "sent": "So we do ontological reasoning, including everything that is valid at time T plus everything that is in the background knowledge.",
                    "label": 1
                },
                {
                    "sent": "We have our set of information and from this information we extract the relevant facts that we call events using Sparkle queries.",
                    "label": 0
                },
                {
                    "sent": "I will show you an example of query in.",
                    "label": 0
                },
                {
                    "sent": "In a couple of slides.",
                    "label": 0
                },
                {
                    "sent": "And then we use temporal reasoning to correlate.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "8.",
                    "label": 0
                },
                {
                    "sent": "Events together into patterns of events, temporal patterns of events.",
                    "label": 0
                },
                {
                    "sent": "So to correlate as a formalism to correlate in time, we use our previous work which is event processing language called test.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me first introduce Tesla.",
                    "label": 0
                },
                {
                    "sent": "So here I want to highlight only the temporal aspects.",
                    "label": 0
                },
                {
                    "sent": "So this is not the model I'm presenting here, but only the temporal aspects of the model.",
                    "label": 0
                },
                {
                    "sent": "And then I show you how I integrate these into RDF.",
                    "label": 0
                },
                {
                    "sent": "So in Tesla you can define, so here we don't have RDF again, so we have more simple and structured data.",
                    "label": 0
                },
                {
                    "sent": "In Tesla you can define new concepts.",
                    "label": 0
                },
                {
                    "sent": "For example the concept or complex event of fire starting from primitive events.",
                    "label": 0
                },
                {
                    "sent": "So you select the primitive events that you can receive, for example from sensors.",
                    "label": 0
                },
                {
                    "sent": "So you select individual events, then you combine them in temporal sequences.",
                    "label": 0
                },
                {
                    "sent": "So for example here I say I want smoke and smoke must be preceded by a temperature with a given value within a given time span.",
                    "label": 0
                },
                {
                    "sent": "So we have metric, temporal temporal logic.",
                    "label": 0
                },
                {
                    "sent": "So we have the possibility to express time constraints.",
                    "label": 0
                },
                {
                    "sent": "Then we can have parameters to bind the content of different events overtime.",
                    "label": 0
                },
                {
                    "sent": "Then we have the possibility to express negations, so something that must not occur in a given time span.",
                    "label": 0
                },
                {
                    "sent": "And then they are not shown here, but it is more expressive than these.",
                    "label": 0
                },
                {
                    "sent": "You can do some computations, for example, you can require the average temperature to be higher than a given threshold in a given period of time.",
                    "label": 0
                },
                {
                    "sent": "We have customizable selection and consumption policy.",
                    "label": 0
                },
                {
                    "sent": "So for example here if I have a sequence with a smoke and I have several events of type temperature, which one do I pick in this case I select the last one because there is the keyword.",
                    "label": 0
                },
                {
                    "sent": "Last, but they can customize the behavior.",
                    "label": 0
                },
                {
                    "sent": "I can take the first one in that time span.",
                    "label": 0
                },
                {
                    "sent": "I can take each possible.",
                    "label": 0
                },
                {
                    "sent": "Temperature event and create a different fire for each of them.",
                    "label": 0
                },
                {
                    "sent": "And customizable consumption policies consumption means the which events that participate in a rule are not available anymore for further triggering of the same rule.",
                    "label": 0
                },
                {
                    "sent": "So it's expressive in this sense, and then you have hierarchies to build complex events starting from other complex events, so you can create hierarchies of events.",
                    "label": 0
                },
                {
                    "sent": "And the temporal reasoning of Tesla you can find in the original Tesla paper is formally defined through metric temporal logic formulas.",
                    "label": 1
                },
                {
                    "sent": "So in practice in our model we are using.",
                    "label": 0
                },
                {
                    "sent": "Sparkle to extract events and then we a fragment of a metric temporal logic formulas to combine different events today.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is how the daughter language looks like.",
                    "label": 0
                },
                {
                    "sent": "It's identical to Tesla, but the selection is now done by means of sparkle queries.",
                    "label": 0
                },
                {
                    "sent": "So the event definition, starting from the streaming and the background knowledge plus all implicit knowledge that you can derive using reasoning, are extracted using the sparkle queries.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, that was the model and the language.",
                    "label": 0
                },
                {
                    "sent": "What about implementation?",
                    "label": 0
                },
                {
                    "sent": "So the fact that we be couple ontological and temporal reasoning enables us to reuse existing systems to perform the two tasks.",
                    "label": 1
                },
                {
                    "sent": "Basically, and that's the current implementation.",
                    "label": 0
                },
                {
                    "sent": "The current prototype that we have, and as I will show, we already have good results.",
                    "label": 0
                },
                {
                    "sent": "But we will investigate in the future how we can exploit.",
                    "label": 0
                },
                {
                    "sent": "The more the integration of the two parts.",
                    "label": 0
                },
                {
                    "sent": "So basically what we do is we have we start from let's take on top.",
                    "label": 0
                },
                {
                    "sent": "You have a daughter rule.",
                    "label": 0
                },
                {
                    "sent": "You have a rule parcel that splits the different parts.",
                    "label": 0
                },
                {
                    "sent": "Install so this particle queries into RDF store.",
                    "label": 0
                },
                {
                    "sent": "Then it uses the native.",
                    "label": 0
                },
                {
                    "sent": "Engine for Tesla rules, which is called T. Rex, and then it uses a graph definition to transform back the results of T. Rex into RDF graphs.",
                    "label": 0
                },
                {
                    "sent": "So when you receive time annotated RBF graphs, you put your new information into the background knowledge as a background knowledge we use.",
                    "label": 0
                },
                {
                    "sent": "Are there folks which is which enables us to have a high level of performance because it is in memory and it also enables incremental reasoning so we can incrementally compute all the.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The revisions that derive from the GNU RBF graphs that are inserted into the.",
                    "label": 0
                },
                {
                    "sent": "Background knowledge then we run the sparkle queries and the sparkle queries define our events, then the events are translated into the format of T Rex.",
                    "label": 0
                },
                {
                    "sent": "T Rex processes the.",
                    "label": 0
                },
                {
                    "sent": "Temporal part produces some results, and the results are translated back to RDF and to present to be presented as output.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As for the evaluation, we had two types of experiments, comparison with state of the art solutions and sensitivity to parameters.",
                    "label": 0
                },
                {
                    "sent": "Comparing with the state of the art, we consider two representatives of the stream reasoning domain.",
                    "label": 0
                },
                {
                    "sent": "This parlance equals an one representative of the stream processing community which should be which is designed to be fast but less expressive esper.",
                    "label": 0
                },
                {
                    "sent": "And we use the CD bench benchmark which was presented at IWC.",
                    "label": 0
                },
                {
                    "sent": "Here I have to say that the city bench queries are based on Windows on the mechanism of Windows, so basically.",
                    "label": 0
                },
                {
                    "sent": "Our tool and the other tools that are based on Windows provide different semantics, but we try to make the things compatible so too.",
                    "label": 0
                },
                {
                    "sent": "Try to have the same answer set even if the semantics are slightly different, because there are no established benchmarks for stream reasoning that provide different formalisms.",
                    "label": 0
                },
                {
                    "sent": "So here is.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Comparison, as you can see.",
                    "label": 0
                },
                {
                    "sent": "So first in in Esper we.",
                    "label": 0
                },
                {
                    "sent": "Manually encoded the ontological reasoning because it's not expressive enough, so it doesn't include ontological reasoning, so we had to include it manually and that's why, as you can see here.",
                    "label": 0
                },
                {
                    "sent": "So here in presenting the processing time for each input RDF graph based on the frequency of arrival while changing the frequency of arrival on the X axis.",
                    "label": 0
                },
                {
                    "sent": "As you can see, Esper is the one that performs better while daughter.",
                    "label": 0
                },
                {
                    "sent": "Presents much better performance with respect to the sparkle and sequels, not is the logarithmic scale.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the case of a different query query Coutu query two, we also achieve even better performance.",
                    "label": 0
                },
                {
                    "sent": "Much better also than Esper, because here we can really exploit our additional.",
                    "label": 0
                },
                {
                    "sent": "Expressivity to better tune the task to answer the rule.",
                    "label": 0
                },
                {
                    "sent": "So here we need to get the current weather condition in this particular query.",
                    "label": 0
                },
                {
                    "sent": "Since we want the current so the last one that we have seen from the streams.",
                    "label": 0
                },
                {
                    "sent": "If we use Windows we have to periodically re evaluate everything when we receive new information, which is why all the other systems perform.",
                    "label": 0
                },
                {
                    "sent": "Worse than ours while daughter avoids duplicated valuations and distractions.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, sensitivity to parameters.",
                    "label": 0
                },
                {
                    "sent": "We also set up a default scenario.",
                    "label": 0
                },
                {
                    "sent": "With with synthetic data and we study the sensitivity while changing one parameter at a time.",
                    "label": 0
                },
                {
                    "sent": "You can find all the details of this scenario on the paper here.",
                    "label": 0
                },
                {
                    "sent": "What I can show you is that we notice that the most of the part in our current implementation, most of the cost for answering the queries is due.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To query evaluation to sparkle query evaluation, meaning that in this scenario ontological reasoning an temporary zoning event processing are less expensive.",
                    "label": 0
                },
                {
                    "sent": "They account for almost 1/3 of the time, while the query valuation accounts for 2/3 of the time.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we change some parameters.",
                    "label": 0
                },
                {
                    "sent": "The number of rules, the length of the event patterns, the size of the background knowledge, the number of Datalog rules, and here are the results that we see.",
                    "label": 0
                },
                {
                    "sent": "So basically from the first 2 graphs you can see that the number of rules with the number of rules we have almost stable performance up to a certain point, and then at that point the number of rules.",
                    "label": 0
                },
                {
                    "sent": "So the number of sparkle queries that you have to run.",
                    "label": 0
                },
                {
                    "sent": "Or the number of data log rules that you have to compute start to become the bottleneck and the performance decreases.",
                    "label": 0
                },
                {
                    "sent": "Notice however, that on the X axis you have a logarithmic scale and still even in the most challenging case you are below 500 milliseconds.",
                    "label": 0
                },
                {
                    "sent": "Then we study temporal reasoning.",
                    "label": 0
                },
                {
                    "sent": "Temporal reasoning is not really expensive because it's done in an efficient complex event processing engine.",
                    "label": 0
                },
                {
                    "sent": "And finally, we notice that the size if we remain in the main memory, the size of the knowledge base is not really relevant to does not really influence the processing time, which is dominated by other factors.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that leads me to the conclusions.",
                    "label": 0
                },
                {
                    "sent": "So I presented the daughter model with the idea of the coupling, ontological and temporal reasoning for stream reasoning applications.",
                    "label": 0
                },
                {
                    "sent": "I show the comparison with state of the art solutions, showing that we are more expressive in terms of temporal reasoning, so we provide more operators and more possibility to tune your temporal reasoning, and that sometimes can lead to better performance because you can better specify how you want to answer your queries.",
                    "label": 0
                },
                {
                    "sent": "Again, our current implementation is based on a prototype that reuses as a black box existing tools, so we are currently investigating optimized implementations and also in the paper there is some initial investigation in this area that shows some promising results and then we want to compare with other existing formalisms.",
                    "label": 0
                },
                {
                    "sent": "For example, recently in SWC there was a presentation on stream reasoning engine based on ASP.",
                    "label": 0
                },
                {
                    "sent": "Thanks a lot for.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your attention and if you have answering here.",
                    "label": 0
                },
                {
                    "sent": "Yeah hi, thanks for a very interesting talk.",
                    "label": 0
                },
                {
                    "sent": "Maybe I missed it, but so you mentioned for Esther you had to manually implement ontology reasoning.",
                    "label": 0
                },
                {
                    "sent": "How did you do that?",
                    "label": 0
                },
                {
                    "sent": "Did you use some sort of rule based standard custom entailment rules set?",
                    "label": 0
                },
                {
                    "sent": "Or no?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "So basically, since we had the rules so we knew what we wanted, so we manually added all the implicit in formation within the streaming data.",
                    "label": 0
                },
                {
                    "sent": "Streaming data was augmented with these was enriched with all the attributes that you could derive from reasoning, but that was really tailored to answer that specific query.",
                    "label": 0
                },
                {
                    "sent": "To be fair in the comparison.",
                    "label": 0
                },
                {
                    "sent": "OK, OK. Yeah I have a second question.",
                    "label": 0
                },
                {
                    "sent": "Well not more.",
                    "label": 0
                },
                {
                    "sent": "More of a more of a comment really.",
                    "label": 0
                },
                {
                    "sent": "I work for a research group that specializes in health informatics and a big topic hot topic in that domain is so-called alert fatigue.",
                    "label": 0
                },
                {
                    "sent": "Physicians having every other minute or so having an alert pop up because of a patient has some blood values or some some temperature changes and some recent work and there is.",
                    "label": 0
                },
                {
                    "sent": "Kind of counting trying to read out the irrelevant alerts by focusing on temporal patterns, right?",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter if you have a temperature, but if that temperature increased X number of percent during a 5 minute time window, then you have a problem, right?",
                    "label": 0
                },
                {
                    "sent": "So do you see your work somehow being applicable to that?",
                    "label": 0
                },
                {
                    "sent": "Those kinds of problems as well?",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, absolutely.",
                    "label": 0
                },
                {
                    "sent": "So I would say that the traditional model based on Windows is continuously updates results.",
                    "label": 0
                },
                {
                    "sent": "Two queries instead, our model detects temporal patterns, which it seems to be what you are looking for.",
                    "label": 0
                },
                {
                    "sent": "An I didn't go much into the detail of Tesla, but you can check out the original paper.",
                    "label": 0
                },
                {
                    "sent": "There is also the formalism.",
                    "label": 0
                },
                {
                    "sent": "Yeah, using hierarchies of events you can also detect trends like the one that you mentioned.",
                    "label": 0
                },
                {
                    "sent": "So for example, I want an always increasing temperature followed by something else.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you can basically derive the your main building blocks.",
                    "label": 0
                },
                {
                    "sent": "Like increasing temperature and then you iterate by having a recursive rule, you can iterate over these and find patterns of always increasing temperature.",
                    "label": 0
                },
                {
                    "sent": "OK, but probably I lost followers some but I wasn't.",
                    "label": 0
                },
                {
                    "sent": "I just want to confirm.",
                    "label": 0
                },
                {
                    "sent": "That's what you are doing is.",
                    "label": 0
                },
                {
                    "sent": "Her doing temporal reasoning, execution so, so you probably you have a one result for one temporal execution San and how you combine this with the next temporal execution.",
                    "label": 0
                },
                {
                    "sent": "So it's like because for me it's like how to find a pattern between these temporal and how to how you re late this this temporal execution.",
                    "label": 0
                },
                {
                    "sent": "So thank you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thank you, so it's finding patterns.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I agree.",
                    "label": 0
                },
                {
                    "sent": "My question was on this like the bottleneck of Sparkle queries.",
                    "label": 0
                },
                {
                    "sent": "You said that this was the main bottleneck, so do you figure out some way to improve this?",
                    "label": 0
                },
                {
                    "sent": "Like for instance, I know taking some caching mechanisms between different times or so we are.",
                    "label": 0
                },
                {
                    "sent": "We are investigating that one.",
                    "label": 0
                },
                {
                    "sent": "One possible mechanism that we also investigating the paper so it's also present in the paper is to merge all the queries that are similar.",
                    "label": 0
                },
                {
                    "sent": "So basically if the queries only differ.",
                    "label": 0
                },
                {
                    "sent": "For the selection path, so there is a filter as particle filter, you remove the spark or filter so you reduce the number of queries and then you delegate the filter into the complex event processing engine which is faster.",
                    "label": 0
                },
                {
                    "sent": "But of course other possibilities like caching are also possible.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thanks a lot, thanks again.",
                    "label": 0
                }
            ]
        }
    }
}