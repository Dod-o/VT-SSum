{
    "id": "eqsryzpndhh3u4fb5v2jeen6wuu6j4zu",
    "title": "Topic Models in ALVIS",
    "info": {
        "author": [
            "Wray Buntine, Faculty of Information Technology, Monash University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/iiia06_buntine_tma/",
    "segmentation": [
        [
            "OK, so this is.",
            "Joint work with Kim of Alton, but also a lot of other people have been involved over the years.",
            "Um?",
            "So Monte was doing some debugging of all sorts of things yesterday and."
        ],
        [
            "So that's the topic models.",
            "We're using in this project.",
            "What what I'm calling discrete PCA, so I'll give a background a quick background of PCA and ICA.",
            "There's this whole.",
            "Set of acronyms.",
            "You know?",
            "So I've got another 3 letters.",
            "Then we'll look at the bit of a background you know I could go into some math, but instead it's quite a broad area.",
            "Instead, I'll do the the sort of background religion, and in this case is a statistical religion.",
            "If you're in the machine learning community, you know it's quite a.",
            "People can be quite dogmatic.",
            "Then I'll look at the topics built with Wikipedia and then we'll just have a quick look at what's done in search for this so.",
            "There's an older demo up.",
            "Running at the moment and the software that the time that we have is is available on this site here so."
        ],
        [
            "So just rough.",
            "Idea of the processing.",
            "Here's some text, but when we're trading, is it about as a bag of words?",
            "So what we see is this.",
            "We just see the individual words with word counts.",
            "And we've completely scrambled the order, so that's the representation.",
            "That's used, and if you've got a large number of documents with a large number of words.",
            "Typical vector is incredibly sparse because most words aren't used.",
            "Um so.",
            "Uh, sort of a typical analysis you might have, say, approaching a million documents, half a million different words, and there's probably.",
            "Um?",
            "Sparsity is 99.9% or more in that matrix."
        ],
        [
            "Principle components analysis, which, in information retrieval has been called latent as a variation called latent semantic analysis, has a long history.",
            "All sorts of people have been doing it under all sorts of names.",
            "It's known as a dimensionality reduction technique.",
            "One of the most well used techniques in statistics, very amazingly, very recently, people developed a Gaussian interpretation.",
            "For it, which is sort of a fully Bayesian probabilistic model for it.",
            "Just it's bizarre that this happened at such a light time.",
            "And people usually use sparse lapack methods to do the computation.",
            "In fact, I first encountered it when shots of Manning and Schutz, the.",
            "Natural language processing guy.",
            "He was using our supercomputer at NASA.",
            "I got him the time on there to play with this, but he was sort of unhappy with the results, so we dropped it."
        ],
        [
            "Now the big thing about discrete data is if you remember your statistics 101.",
            "Is that just when you got counts, if the counts are sort of moderate size, a discrete distribution.",
            "Here this is a binomial distribution with a count of 20.",
            "And in this case you can see that it's actually quite Gaussian, but if you've got sparse data, so you get mostly zeros, it's incredibly non Gaussian.",
            "So when you when you have discrete data, so matrices with mostly zeros, the Gaussian interpretation, which is what PCA is, fails miserably.",
            "Um?",
            "The image people know that, and they often use a pass on modeling.",
            "And this is this is a plus on.",
            "Approximated with the process on as well."
        ],
        [
            "So.",
            "Another method which people developed originally, I think these guys are French.",
            "But the finish group is very well known for TKK.",
            "Um?",
            "And it was sort of building on PCI.",
            "I won't go into the details, but it's been very successful, used in a lot of cases, but again, it's it's not really.",
            "Good when is a small dynamic range.",
            "So when you've got discrete data with just a few values.",
            "Um?",
            "People often get over this by sort of putting their data through a TF IDF filter, and that is probably a good way of doing doing with it.",
            "So."
        ],
        [
            "I don't think I claimed too much about ICA being suboptimal in in discrete cases, but.",
            "Um?",
            "So you can discretize both of these models.",
            "You can for the Gaussian model that people developed.",
            "You can just replace Gaussians by multinomial's in the general formula.",
            "The math you pull out a Gaussian and put in a multinomial and you end up with something that's you can work with as a statistician.",
            "And this is, you know, papers.",
            "Other people have done this as well.",
            "And if you instead of a multinomial individual user, pass on a gamma, you get something that's sort of somewhat rather like ICA, so people have done this.",
            "It's very interesting, sort of people coming up from many different angles as many different inventors.",
            "I was going to show you this, but it took me a little while to bring it up, so there's a very nice demonstration in the one of the early papers in this area where they show words and they color the words depending on the component there on.",
            "So the idea is that these components are operating at a word level rather than a document level.",
            "And that's a good aspect."
        ],
        [
            "Of them, so they're breaking up documents at the word level.",
            "So instead of clustering documents in complete your clustering.",
            "The words in the document.",
            "Very new."
        ],
        [
            "This illustration by David Bly and his colleagues, so a quick review of the history and stuff and then."
        ],
        [
            "To get into this application, I want to do this because this, well, just so many people have been doing this.",
            "The sort of early early genius is really, really, really insightful and very good work.",
            "Soon Lee in non negative matrix factorization, P LSI which several people have mentioned.",
            "Thomas Hoffman.",
            "Ingenious bits of work.",
            "The statisticians, the.",
            "Jeanette geneticists got into this early to completely separate.",
            "And some applied statisticians were doing it as well.",
            "Very early on they call it soft clustering or greater membership.",
            "So whole bunch of people have been doing this this Pritchard.",
            "Group.",
            "They are actually the most cited in terms of if you look up references, they've got hundreds of citations in things like science and nature based on their software.",
            "So an ear model is equivalent to the IT actually a little bit of a generalization of the part of the blind model, LDA.",
            "Blizz group with Michael Jordan.",
            "Of course introduced a mean field algorithm for it.",
            "So anyway, there's a whole bunch of different groups and you may have heard some of these names or others.",
            "The image people are usually more familiar with non negative matrix factorization.",
            "The information retrieval people familiar with the latent semantic analysis and PLSI.",
            "And the machine learning people usually know LDA.",
            "And often nobody knows the others, so."
        ],
        [
            "All manner of algorithms and well theories that we use in statistics and machine learning can be applied.",
            "So in fact it's a very nice exercise to work through.",
            "That's sort of a first year postgraduate.",
            "First year graduate would set someone with a bachelors, but is doing like a Masters or something very good exercise to work through because of the different techniques you can apply.",
            "And if you go through the papers you'll see every different manner of methods.",
            "Um?"
        ],
        [
            "Applaud so and it also has an enormous number of interpretations as sort of a robust way of doing ICA for discrete data and discrete version of the PCA and.",
            "Modeling the individual words in a collection so it's a very.",
            "Um?"
        ],
        [
            "Sensible way, and in fact it does turn out that all of the methods and instance of the latest model coming from a sort of a maverick.",
            "John Kenny at Berkeley, who developed a gamma pass on model.",
            "That's."
        ],
        [
            "Sort of theory background so.",
            "Um?",
            "The thing about the algorithms is the thing I will just.",
            "I guess I focus on here is that it's it's really just a few times slower.",
            "We've tested this then PCA, so if you're going to run a PC and a big matrix well, and you got discrete data, you might as well be using these algorithms because there only a few times slower than sort of optimal PCA.",
            "And there are different algorithms and the one there's a griffithsin stivers or some people in US psychologists.",
            "They developed a rather ingenious algorithm that works very well for the small document case.",
            "So if you're doing this pairs of words in different languages, you could apply their algorithm to this.",
            "And it's probably the only one the other algorithms could not be applied to the sort of.",
            "Pad words or paid sentences because they just don't operate."
        ],
        [
            "Um?",
            "So.",
            "Uh.",
            "I just did want to say while people talk about Gibbs sampling and all of this, it's not Gibbs sampling in the sense of a statistician who's doing sound statistical inference.",
            "It's Gibbs sampling in the sense of a computer scientist who just got some estimation method.",
            "And you're not trying to do sound statistical inference.",
            "Talk to me afterwards if you want to.",
            "See Phil, what the differences, but it's quite quite critical if you're a statistician.",
            "You look at the numbers I'm about to show and you should laugh.",
            "If you're."
        ],
        [
            "Thinking I'm presenting statistical inference, I'm not.",
            "I'm presenting estimation.",
            "So.",
            "Um?",
            "The software I'm using for this we've put on the web and a whole bunch of people have been involved in it so."
        ],
        [
            "Now we we took the.",
            "Wikipedia English language Wikipedia from December 2005 there's about 980 pages.",
            "That's not including the redirects, but it is including the category pages.",
            "Awful lot of background processing on here, so this is keeping Kimmel entertained for quite some time.",
            "Dealing with all of this.",
            "We lemmatized the words too.",
            "Put some sense into them.",
            "You'll see when I show display that.",
            "It does.",
            "You can understand things a lot better by splitting the words up into their parts of speech.",
            "The smaller reccuring words are dropped, so you're left with a vocabulary of about a million.",
            "It's typical in these sorts of things that vocabulary size for is typically of the order of the document size, depending on how many you toss out, so there's about a million documents about a million words.",
            "Once we've dropped the smaller ones.",
            "Um?",
            "I haven't told you about these particular models, but I'm using a variation.",
            "That others don't use that, as it makes the.",
            "It encourages sparsity, and it always wins.",
            "He always improves the likelihoods and everything, so it's rather neat trick to it.",
            "Makes my matrices a lot sparser, but it's always an improvement in terms of fitting quality.",
            "And remember, we're dealing with a.",
            "You know documents with maybe 300 words, 1000 words.",
            "So incredibly sparse things here.",
            "This is the for this particular data set on building a 400 component model, and I'm taking about 6 days on a pretty big system.",
            "If it was your desktop, maybe 2 weeks.",
            "So it's a lot of computation.",
            "For this it's about two Giga text to gigabytes of text in it.",
            "Um?"
        ],
        [
            "Obviously, to do this I've got to have stop start and.",
            "You know checkpointing and all of this to make it run OK. Um?",
            "So let's see if I can find the.",
            "Internet Explorer.",
            "Dear Oh dear, how do I open a document?",
            "Sorry, I'm used to Mozilla I.",
            "Is Facebook?",
            "File.",
            "Open.",
            "Browse alright, so this is results.",
            "Now the thing to realize is that labeling is done by hand so you know.",
            "Three guys in a room for a couple of hours basically have done this purpose about six man hours plus about an hour of my time in neatening it up.",
            "So there's that.",
            "Perhaps the day's work to produce what you the names you see here.",
            "And to put the little category on there, there's the thing that this differs from a regular clustering where you're breaking a whole bag of documents into pieces is that you get these things like code, so you get SSI units, which is a physical units of measurement, names of people, nobility, mathematical symbols, you get components for these kinds of things, and that happens.",
            "That wouldn't happen if your disk clustering documents into pieces into into groups.",
            "Your clustering parts of documents, your clustering, the words in documents, so you know you could look at the.",
            "G. Genealogy, yeah, that's an interesting one, let's say.",
            "Oh bloody hell.",
            "Bloody miss windows.",
            "I don't know.",
            "Is there a Mozilla here somewhere?",
            "Surely they've got Mozilla on this.",
            "Might want to click on this.",
            "'cause what?",
            "It's a HTML document but.",
            "Yes, what what this?",
            "Alarm Clock.",
            "For dinner protecting alright just protecting it from HTML yeah right?",
            "Yeah, I don't put HTML extensions and all of my files, but I guess so.",
            "The thing is you can see here the this genealogy.",
            "You can see that the words that have come out, it's all the different sort of relationships their house is included as a baby, but by grouping in terms of nouns and verbs verbs you see that the verbs, adjectives and URLs there a lot.",
            "Different frequencies of them, so if you don't split them up into these groups, you sort of lose them in each other, but you can see the adjectives and verbs are very applicable.",
            "There was one on Greek mythology that's funny because all the verbs are like killed and murdered and stuff.",
            "Greek mythology.",
            "I don't know why.",
            "Always found it into tiny so and the URLs here as well so you know one of the early work in this field was people.",
            "Using this in sort of a page rank style and you can do that.",
            "There's some random words.",
            "I've got a bug in the algorithm.",
            "He said the last third and not actually there genuinely random, but these are the 1st third of random from the.",
            "From the component.",
            "And then the interesting thing is the ranking.",
            "So you can do topic specific ranking.",
            "That's another whole talk, but you can rank the documents using a page rank that's targeting this particular topic.",
            "And it's not the one from Stanford which isn't much good.",
            "It's a one from Washington, which is a lot better.",
            "But you can see the top category here.",
            "Category children, marriage, genealogy, all of these.",
            "The interesting ones come out.",
            "The the.",
            "The typical documents are the same.",
            "OK, now back on with just finished the talk here.",
            "And then we'll have a quick look at the."
        ],
        [
            "Who is that it?"
        ],
        [
            "So in search, what do we do with these?",
            "Well, the the thing people like to do is sort of a mixed topic.",
            "Browse and search mixing the two things and there's some of the very early nice demos and applications.",
            "Successful applications in say that some of the semantic web area are doing this, their mixing browse and search so.",
            "And there's different ways of selecting your topics, and I'll show you that here, so that's how.",
            "Our idea is to.",
            "If you've got if your medical domain or something you use mess, you don't want to discover topics.",
            "It's a waste of time if people designed a good set of topics, you use them, but if you don't have a good set of topics then then you want to use something like this."
        ],
        [
            "Um?",
            "And just to show you this is an example of a set of topics from search engine's.",
            "I don't know of any of any topic hierarchy that has this kind of Fidelity in it.",
            "For search engines you can search engine optimization, acceptable search engine optimization forums and discussions, Google Founders, a whole area in the news is on Google founders Anil Aereas, one last night on Slashdot.",
            "If you looked at it.",
            "You know domain name registration, so this level of topic you just wouldn't get from a typical hierarchy or something you get somewhere else.",
            "It's it's really where you want to use a system like this to give you a very nice to discover a good hire."
        ],
        [
            "Before you.",
            "I don't like to do math on a talk, but basically some very this is just sort of using the ideas that Martin was showing us yesterday.",
            "I think it was.",
            "It's the same stuff.",
            "If you want to estimate the probability of the topics.",
            "That you have for a particular query.",
            "Then you the probability of documents for that query and then you.",
            "Average that with the probability of documents, probability of topics on that document.",
            "So for all the documents in our in our system, we've got the topic sits in an its proportion of the topic.",
            "So we've got these probabilities in a vector.",
            "It's a sparse vector, and this probability of the document given the query.",
            "That's what the information retrieval system gives us very quickly.",
            "So this is a sparse vector.",
            "This is generated very quickly, and so this kind of calculation can be done at about the same magnitude as.",
            "As as the retrieval task itself, what I'm not telling you though, is well.",
            "I at the moment I'm not treating this sparse, so it's done quite just kind of slowly."
        ],
        [
            "OK, so last thing I'm going to do is show you the.",
            "Some examples.",
            "Senaga type everything in.",
            "We sort of cooked up a bit of this interface recently, so it's not really neat, but.",
            "Alright, take calls FBI.",
            "And that's a space.",
            "I just put a space in.",
            "I don't know.",
            "Say I say they were FBI and CIA.",
            "Is something happening?",
            "Now this.",
            "This is a bit slow because the now I've done this in sort of a.",
            "What do you call these A tag cloud style?",
            "So the strength of here.",
            "These are named entities that have been selected to match the topic.",
            "This is done moderately poorly, actually, but it's not too bad for some, so you know you can see the.",
            "Obviously it's got the actual topics that we put in, but there's all the related things you know.",
            "The newspapers and the Soviet Union and September 11.",
            "All of that stuff.",
            "Watergate scandal.",
            "All of that stuff in there.",
            "It's fun.",
            "This is actually quite entertaining and then this is the topics as selected from the algorithm.",
            "So this is the big one.",
            "You can see crime famous people, murders and riots, national security law and justice.",
            "So this is the topic that you've got for this spread of topics.",
            "So what am I doing?",
            "Do another one.",
            "You can actually be quite specific here so.",
            "Or dinner.",
            "Yeah.",
            "Banana and Apple.",
            "So would a banana and Apple have in common?",
            "Yes, this is something Australia is famous for is the big banana.",
            "There's also the big pineapple on the big shape.",
            "All sorts of strange bizarre tourist attractions, but so the entities that have in common.",
            "Whether these are largely fruit and some other things, but the topic is overwhelming.",
            "Lee Food and Drink and agriculture.",
            "So anyway, you can play with this.",
            "It's working reasonably well.",
            "We'll have to.",
            "We'll be putting it into a better interface at some stage and improving the entities algorithm, but so the idea is we can suggest some topics to you based on a query you put in and then you maybe use them to filter later queries so that the rest of this use of this in a way we haven't got it well, we have in another system, But this this interface at the moment is just.",
            "Telling you the entities and topics related to a particular thing.",
            "So we've sort of setting this up in a pipeline so you can do it with a with a search engine content we saw earlier.",
            "You can do it with a sort of a biology content and come up with these things.",
            "By the way, this this these things.",
            "These results use a link text.",
            "The anchor text they ranked.",
            "Because the Wikipedia actually has very good ranking, Pagerank works quite well on it.",
            "And other things, so there's quite a bit of activity behind the scenes on this.",
            "This retrieval here.",
            "And of course, is all used in the analysis of these things.",
            "So those simple formula I showed you and it's just the idea of doing extra things using the retrieval engine is sort of a fast probabilistic processor.",
            "So anyway, there we go.",
            "That's that's the talk.",
            "This is actually around, so if you remember this, you could play with it for a little bit before we take it down.",
            "Put out."
        ],
        [
            "Thank you.",
            "So.",
            "Mia questions.",
            "Any queries who wants to do a query?",
            "So this is something that on top of the existing.",
            "Index.",
            "Oh, we got it on an index because we couldn't get access to the.",
            "You need to get access to the scores.",
            "As well as the.",
            "Using the retrieval system both to give you the set of the short list of documents to look at, but also their scores.",
            "So you need a short list of say 200 or 100 to be able to do this processing well, and the systems already produced that short list and the scores as a side effect of their work.",
            "So you just feed that into the name recognition, the name collection, and a topic collection.",
            "And it's all moderately fast.",
            "So.",
            "Based on that.",
            "Yeah.",
            "Goodnight we haven't seen this this demo though.",
            "No, Sir.",
            "Thank you.",
            "Yes, I."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "Joint work with Kim of Alton, but also a lot of other people have been involved over the years.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So Monte was doing some debugging of all sorts of things yesterday and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the topic models.",
                    "label": 0
                },
                {
                    "sent": "We're using in this project.",
                    "label": 0
                },
                {
                    "sent": "What what I'm calling discrete PCA, so I'll give a background a quick background of PCA and ICA.",
                    "label": 1
                },
                {
                    "sent": "There's this whole.",
                    "label": 0
                },
                {
                    "sent": "Set of acronyms.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "So I've got another 3 letters.",
                    "label": 0
                },
                {
                    "sent": "Then we'll look at the bit of a background you know I could go into some math, but instead it's quite a broad area.",
                    "label": 0
                },
                {
                    "sent": "Instead, I'll do the the sort of background religion, and in this case is a statistical religion.",
                    "label": 0
                },
                {
                    "sent": "If you're in the machine learning community, you know it's quite a.",
                    "label": 0
                },
                {
                    "sent": "People can be quite dogmatic.",
                    "label": 0
                },
                {
                    "sent": "Then I'll look at the topics built with Wikipedia and then we'll just have a quick look at what's done in search for this so.",
                    "label": 0
                },
                {
                    "sent": "There's an older demo up.",
                    "label": 0
                },
                {
                    "sent": "Running at the moment and the software that the time that we have is is available on this site here so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just rough.",
                    "label": 0
                },
                {
                    "sent": "Idea of the processing.",
                    "label": 0
                },
                {
                    "sent": "Here's some text, but when we're trading, is it about as a bag of words?",
                    "label": 1
                },
                {
                    "sent": "So what we see is this.",
                    "label": 0
                },
                {
                    "sent": "We just see the individual words with word counts.",
                    "label": 0
                },
                {
                    "sent": "And we've completely scrambled the order, so that's the representation.",
                    "label": 1
                },
                {
                    "sent": "That's used, and if you've got a large number of documents with a large number of words.",
                    "label": 0
                },
                {
                    "sent": "Typical vector is incredibly sparse because most words aren't used.",
                    "label": 1
                },
                {
                    "sent": "Um so.",
                    "label": 0
                },
                {
                    "sent": "Uh, sort of a typical analysis you might have, say, approaching a million documents, half a million different words, and there's probably.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Sparsity is 99.9% or more in that matrix.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Principle components analysis, which, in information retrieval has been called latent as a variation called latent semantic analysis, has a long history.",
                    "label": 1
                },
                {
                    "sent": "All sorts of people have been doing it under all sorts of names.",
                    "label": 1
                },
                {
                    "sent": "It's known as a dimensionality reduction technique.",
                    "label": 0
                },
                {
                    "sent": "One of the most well used techniques in statistics, very amazingly, very recently, people developed a Gaussian interpretation.",
                    "label": 0
                },
                {
                    "sent": "For it, which is sort of a fully Bayesian probabilistic model for it.",
                    "label": 1
                },
                {
                    "sent": "Just it's bizarre that this happened at such a light time.",
                    "label": 0
                },
                {
                    "sent": "And people usually use sparse lapack methods to do the computation.",
                    "label": 0
                },
                {
                    "sent": "In fact, I first encountered it when shots of Manning and Schutz, the.",
                    "label": 0
                },
                {
                    "sent": "Natural language processing guy.",
                    "label": 0
                },
                {
                    "sent": "He was using our supercomputer at NASA.",
                    "label": 0
                },
                {
                    "sent": "I got him the time on there to play with this, but he was sort of unhappy with the results, so we dropped it.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the big thing about discrete data is if you remember your statistics 101.",
                    "label": 1
                },
                {
                    "sent": "Is that just when you got counts, if the counts are sort of moderate size, a discrete distribution.",
                    "label": 0
                },
                {
                    "sent": "Here this is a binomial distribution with a count of 20.",
                    "label": 0
                },
                {
                    "sent": "And in this case you can see that it's actually quite Gaussian, but if you've got sparse data, so you get mostly zeros, it's incredibly non Gaussian.",
                    "label": 1
                },
                {
                    "sent": "So when you when you have discrete data, so matrices with mostly zeros, the Gaussian interpretation, which is what PCA is, fails miserably.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The image people know that, and they often use a pass on modeling.",
                    "label": 0
                },
                {
                    "sent": "And this is this is a plus on.",
                    "label": 0
                },
                {
                    "sent": "Approximated with the process on as well.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Another method which people developed originally, I think these guys are French.",
                    "label": 0
                },
                {
                    "sent": "But the finish group is very well known for TKK.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And it was sort of building on PCI.",
                    "label": 0
                },
                {
                    "sent": "I won't go into the details, but it's been very successful, used in a lot of cases, but again, it's it's not really.",
                    "label": 0
                },
                {
                    "sent": "Good when is a small dynamic range.",
                    "label": 0
                },
                {
                    "sent": "So when you've got discrete data with just a few values.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "People often get over this by sort of putting their data through a TF IDF filter, and that is probably a good way of doing doing with it.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't think I claimed too much about ICA being suboptimal in in discrete cases, but.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So you can discretize both of these models.",
                    "label": 0
                },
                {
                    "sent": "You can for the Gaussian model that people developed.",
                    "label": 0
                },
                {
                    "sent": "You can just replace Gaussians by multinomial's in the general formula.",
                    "label": 0
                },
                {
                    "sent": "The math you pull out a Gaussian and put in a multinomial and you end up with something that's you can work with as a statistician.",
                    "label": 0
                },
                {
                    "sent": "And this is, you know, papers.",
                    "label": 0
                },
                {
                    "sent": "Other people have done this as well.",
                    "label": 0
                },
                {
                    "sent": "And if you instead of a multinomial individual user, pass on a gamma, you get something that's sort of somewhat rather like ICA, so people have done this.",
                    "label": 0
                },
                {
                    "sent": "It's very interesting, sort of people coming up from many different angles as many different inventors.",
                    "label": 0
                },
                {
                    "sent": "I was going to show you this, but it took me a little while to bring it up, so there's a very nice demonstration in the one of the early papers in this area where they show words and they color the words depending on the component there on.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that these components are operating at a word level rather than a document level.",
                    "label": 0
                },
                {
                    "sent": "And that's a good aspect.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of them, so they're breaking up documents at the word level.",
                    "label": 1
                },
                {
                    "sent": "So instead of clustering documents in complete your clustering.",
                    "label": 0
                },
                {
                    "sent": "The words in the document.",
                    "label": 0
                },
                {
                    "sent": "Very new.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This illustration by David Bly and his colleagues, so a quick review of the history and stuff and then.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To get into this application, I want to do this because this, well, just so many people have been doing this.",
                    "label": 0
                },
                {
                    "sent": "The sort of early early genius is really, really, really insightful and very good work.",
                    "label": 0
                },
                {
                    "sent": "Soon Lee in non negative matrix factorization, P LSI which several people have mentioned.",
                    "label": 0
                },
                {
                    "sent": "Thomas Hoffman.",
                    "label": 0
                },
                {
                    "sent": "Ingenious bits of work.",
                    "label": 0
                },
                {
                    "sent": "The statisticians, the.",
                    "label": 0
                },
                {
                    "sent": "Jeanette geneticists got into this early to completely separate.",
                    "label": 0
                },
                {
                    "sent": "And some applied statisticians were doing it as well.",
                    "label": 0
                },
                {
                    "sent": "Very early on they call it soft clustering or greater membership.",
                    "label": 1
                },
                {
                    "sent": "So whole bunch of people have been doing this this Pritchard.",
                    "label": 0
                },
                {
                    "sent": "Group.",
                    "label": 0
                },
                {
                    "sent": "They are actually the most cited in terms of if you look up references, they've got hundreds of citations in things like science and nature based on their software.",
                    "label": 0
                },
                {
                    "sent": "So an ear model is equivalent to the IT actually a little bit of a generalization of the part of the blind model, LDA.",
                    "label": 0
                },
                {
                    "sent": "Blizz group with Michael Jordan.",
                    "label": 0
                },
                {
                    "sent": "Of course introduced a mean field algorithm for it.",
                    "label": 0
                },
                {
                    "sent": "So anyway, there's a whole bunch of different groups and you may have heard some of these names or others.",
                    "label": 1
                },
                {
                    "sent": "The image people are usually more familiar with non negative matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "The information retrieval people familiar with the latent semantic analysis and PLSI.",
                    "label": 1
                },
                {
                    "sent": "And the machine learning people usually know LDA.",
                    "label": 0
                },
                {
                    "sent": "And often nobody knows the others, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All manner of algorithms and well theories that we use in statistics and machine learning can be applied.",
                    "label": 0
                },
                {
                    "sent": "So in fact it's a very nice exercise to work through.",
                    "label": 0
                },
                {
                    "sent": "That's sort of a first year postgraduate.",
                    "label": 0
                },
                {
                    "sent": "First year graduate would set someone with a bachelors, but is doing like a Masters or something very good exercise to work through because of the different techniques you can apply.",
                    "label": 0
                },
                {
                    "sent": "And if you go through the papers you'll see every different manner of methods.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Applaud so and it also has an enormous number of interpretations as sort of a robust way of doing ICA for discrete data and discrete version of the PCA and.",
                    "label": 1
                },
                {
                    "sent": "Modeling the individual words in a collection so it's a very.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sensible way, and in fact it does turn out that all of the methods and instance of the latest model coming from a sort of a maverick.",
                    "label": 0
                },
                {
                    "sent": "John Kenny at Berkeley, who developed a gamma pass on model.",
                    "label": 0
                },
                {
                    "sent": "That's.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sort of theory background so.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The thing about the algorithms is the thing I will just.",
                    "label": 0
                },
                {
                    "sent": "I guess I focus on here is that it's it's really just a few times slower.",
                    "label": 0
                },
                {
                    "sent": "We've tested this then PCA, so if you're going to run a PC and a big matrix well, and you got discrete data, you might as well be using these algorithms because there only a few times slower than sort of optimal PCA.",
                    "label": 1
                },
                {
                    "sent": "And there are different algorithms and the one there's a griffithsin stivers or some people in US psychologists.",
                    "label": 0
                },
                {
                    "sent": "They developed a rather ingenious algorithm that works very well for the small document case.",
                    "label": 1
                },
                {
                    "sent": "So if you're doing this pairs of words in different languages, you could apply their algorithm to this.",
                    "label": 0
                },
                {
                    "sent": "And it's probably the only one the other algorithms could not be applied to the sort of.",
                    "label": 0
                },
                {
                    "sent": "Pad words or paid sentences because they just don't operate.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "I just did want to say while people talk about Gibbs sampling and all of this, it's not Gibbs sampling in the sense of a statistician who's doing sound statistical inference.",
                    "label": 1
                },
                {
                    "sent": "It's Gibbs sampling in the sense of a computer scientist who just got some estimation method.",
                    "label": 0
                },
                {
                    "sent": "And you're not trying to do sound statistical inference.",
                    "label": 1
                },
                {
                    "sent": "Talk to me afterwards if you want to.",
                    "label": 0
                },
                {
                    "sent": "See Phil, what the differences, but it's quite quite critical if you're a statistician.",
                    "label": 0
                },
                {
                    "sent": "You look at the numbers I'm about to show and you should laugh.",
                    "label": 0
                },
                {
                    "sent": "If you're.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thinking I'm presenting statistical inference, I'm not.",
                    "label": 0
                },
                {
                    "sent": "I'm presenting estimation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The software I'm using for this we've put on the web and a whole bunch of people have been involved in it so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we we took the.",
                    "label": 0
                },
                {
                    "sent": "Wikipedia English language Wikipedia from December 2005 there's about 980 pages.",
                    "label": 1
                },
                {
                    "sent": "That's not including the redirects, but it is including the category pages.",
                    "label": 0
                },
                {
                    "sent": "Awful lot of background processing on here, so this is keeping Kimmel entertained for quite some time.",
                    "label": 0
                },
                {
                    "sent": "Dealing with all of this.",
                    "label": 0
                },
                {
                    "sent": "We lemmatized the words too.",
                    "label": 0
                },
                {
                    "sent": "Put some sense into them.",
                    "label": 0
                },
                {
                    "sent": "You'll see when I show display that.",
                    "label": 0
                },
                {
                    "sent": "It does.",
                    "label": 0
                },
                {
                    "sent": "You can understand things a lot better by splitting the words up into their parts of speech.",
                    "label": 0
                },
                {
                    "sent": "The smaller reccuring words are dropped, so you're left with a vocabulary of about a million.",
                    "label": 0
                },
                {
                    "sent": "It's typical in these sorts of things that vocabulary size for is typically of the order of the document size, depending on how many you toss out, so there's about a million documents about a million words.",
                    "label": 0
                },
                {
                    "sent": "Once we've dropped the smaller ones.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I haven't told you about these particular models, but I'm using a variation.",
                    "label": 0
                },
                {
                    "sent": "That others don't use that, as it makes the.",
                    "label": 0
                },
                {
                    "sent": "It encourages sparsity, and it always wins.",
                    "label": 0
                },
                {
                    "sent": "He always improves the likelihoods and everything, so it's rather neat trick to it.",
                    "label": 0
                },
                {
                    "sent": "Makes my matrices a lot sparser, but it's always an improvement in terms of fitting quality.",
                    "label": 0
                },
                {
                    "sent": "And remember, we're dealing with a.",
                    "label": 0
                },
                {
                    "sent": "You know documents with maybe 300 words, 1000 words.",
                    "label": 0
                },
                {
                    "sent": "So incredibly sparse things here.",
                    "label": 0
                },
                {
                    "sent": "This is the for this particular data set on building a 400 component model, and I'm taking about 6 days on a pretty big system.",
                    "label": 1
                },
                {
                    "sent": "If it was your desktop, maybe 2 weeks.",
                    "label": 0
                },
                {
                    "sent": "So it's a lot of computation.",
                    "label": 0
                },
                {
                    "sent": "For this it's about two Giga text to gigabytes of text in it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Obviously, to do this I've got to have stop start and.",
                    "label": 0
                },
                {
                    "sent": "You know checkpointing and all of this to make it run OK. Um?",
                    "label": 0
                },
                {
                    "sent": "So let's see if I can find the.",
                    "label": 0
                },
                {
                    "sent": "Internet Explorer.",
                    "label": 0
                },
                {
                    "sent": "Dear Oh dear, how do I open a document?",
                    "label": 0
                },
                {
                    "sent": "Sorry, I'm used to Mozilla I.",
                    "label": 0
                },
                {
                    "sent": "Is Facebook?",
                    "label": 0
                },
                {
                    "sent": "File.",
                    "label": 0
                },
                {
                    "sent": "Open.",
                    "label": 0
                },
                {
                    "sent": "Browse alright, so this is results.",
                    "label": 0
                },
                {
                    "sent": "Now the thing to realize is that labeling is done by hand so you know.",
                    "label": 0
                },
                {
                    "sent": "Three guys in a room for a couple of hours basically have done this purpose about six man hours plus about an hour of my time in neatening it up.",
                    "label": 0
                },
                {
                    "sent": "So there's that.",
                    "label": 0
                },
                {
                    "sent": "Perhaps the day's work to produce what you the names you see here.",
                    "label": 0
                },
                {
                    "sent": "And to put the little category on there, there's the thing that this differs from a regular clustering where you're breaking a whole bag of documents into pieces is that you get these things like code, so you get SSI units, which is a physical units of measurement, names of people, nobility, mathematical symbols, you get components for these kinds of things, and that happens.",
                    "label": 0
                },
                {
                    "sent": "That wouldn't happen if your disk clustering documents into pieces into into groups.",
                    "label": 0
                },
                {
                    "sent": "Your clustering parts of documents, your clustering, the words in documents, so you know you could look at the.",
                    "label": 0
                },
                {
                    "sent": "G. Genealogy, yeah, that's an interesting one, let's say.",
                    "label": 0
                },
                {
                    "sent": "Oh bloody hell.",
                    "label": 0
                },
                {
                    "sent": "Bloody miss windows.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Is there a Mozilla here somewhere?",
                    "label": 0
                },
                {
                    "sent": "Surely they've got Mozilla on this.",
                    "label": 0
                },
                {
                    "sent": "Might want to click on this.",
                    "label": 0
                },
                {
                    "sent": "'cause what?",
                    "label": 0
                },
                {
                    "sent": "It's a HTML document but.",
                    "label": 0
                },
                {
                    "sent": "Yes, what what this?",
                    "label": 0
                },
                {
                    "sent": "Alarm Clock.",
                    "label": 0
                },
                {
                    "sent": "For dinner protecting alright just protecting it from HTML yeah right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't put HTML extensions and all of my files, but I guess so.",
                    "label": 0
                },
                {
                    "sent": "The thing is you can see here the this genealogy.",
                    "label": 0
                },
                {
                    "sent": "You can see that the words that have come out, it's all the different sort of relationships their house is included as a baby, but by grouping in terms of nouns and verbs verbs you see that the verbs, adjectives and URLs there a lot.",
                    "label": 0
                },
                {
                    "sent": "Different frequencies of them, so if you don't split them up into these groups, you sort of lose them in each other, but you can see the adjectives and verbs are very applicable.",
                    "label": 0
                },
                {
                    "sent": "There was one on Greek mythology that's funny because all the verbs are like killed and murdered and stuff.",
                    "label": 0
                },
                {
                    "sent": "Greek mythology.",
                    "label": 0
                },
                {
                    "sent": "I don't know why.",
                    "label": 0
                },
                {
                    "sent": "Always found it into tiny so and the URLs here as well so you know one of the early work in this field was people.",
                    "label": 0
                },
                {
                    "sent": "Using this in sort of a page rank style and you can do that.",
                    "label": 0
                },
                {
                    "sent": "There's some random words.",
                    "label": 0
                },
                {
                    "sent": "I've got a bug in the algorithm.",
                    "label": 0
                },
                {
                    "sent": "He said the last third and not actually there genuinely random, but these are the 1st third of random from the.",
                    "label": 0
                },
                {
                    "sent": "From the component.",
                    "label": 0
                },
                {
                    "sent": "And then the interesting thing is the ranking.",
                    "label": 0
                },
                {
                    "sent": "So you can do topic specific ranking.",
                    "label": 0
                },
                {
                    "sent": "That's another whole talk, but you can rank the documents using a page rank that's targeting this particular topic.",
                    "label": 0
                },
                {
                    "sent": "And it's not the one from Stanford which isn't much good.",
                    "label": 0
                },
                {
                    "sent": "It's a one from Washington, which is a lot better.",
                    "label": 0
                },
                {
                    "sent": "But you can see the top category here.",
                    "label": 0
                },
                {
                    "sent": "Category children, marriage, genealogy, all of these.",
                    "label": 0
                },
                {
                    "sent": "The interesting ones come out.",
                    "label": 0
                },
                {
                    "sent": "The the.",
                    "label": 0
                },
                {
                    "sent": "The typical documents are the same.",
                    "label": 0
                },
                {
                    "sent": "OK, now back on with just finished the talk here.",
                    "label": 0
                },
                {
                    "sent": "And then we'll have a quick look at the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who is that it?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in search, what do we do with these?",
                    "label": 0
                },
                {
                    "sent": "Well, the the thing people like to do is sort of a mixed topic.",
                    "label": 0
                },
                {
                    "sent": "Browse and search mixing the two things and there's some of the very early nice demos and applications.",
                    "label": 0
                },
                {
                    "sent": "Successful applications in say that some of the semantic web area are doing this, their mixing browse and search so.",
                    "label": 0
                },
                {
                    "sent": "And there's different ways of selecting your topics, and I'll show you that here, so that's how.",
                    "label": 0
                },
                {
                    "sent": "Our idea is to.",
                    "label": 0
                },
                {
                    "sent": "If you've got if your medical domain or something you use mess, you don't want to discover topics.",
                    "label": 0
                },
                {
                    "sent": "It's a waste of time if people designed a good set of topics, you use them, but if you don't have a good set of topics then then you want to use something like this.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And just to show you this is an example of a set of topics from search engine's.",
                    "label": 0
                },
                {
                    "sent": "I don't know of any of any topic hierarchy that has this kind of Fidelity in it.",
                    "label": 0
                },
                {
                    "sent": "For search engines you can search engine optimization, acceptable search engine optimization forums and discussions, Google Founders, a whole area in the news is on Google founders Anil Aereas, one last night on Slashdot.",
                    "label": 1
                },
                {
                    "sent": "If you looked at it.",
                    "label": 1
                },
                {
                    "sent": "You know domain name registration, so this level of topic you just wouldn't get from a typical hierarchy or something you get somewhere else.",
                    "label": 0
                },
                {
                    "sent": "It's it's really where you want to use a system like this to give you a very nice to discover a good hire.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before you.",
                    "label": 0
                },
                {
                    "sent": "I don't like to do math on a talk, but basically some very this is just sort of using the ideas that Martin was showing us yesterday.",
                    "label": 0
                },
                {
                    "sent": "I think it was.",
                    "label": 0
                },
                {
                    "sent": "It's the same stuff.",
                    "label": 0
                },
                {
                    "sent": "If you want to estimate the probability of the topics.",
                    "label": 0
                },
                {
                    "sent": "That you have for a particular query.",
                    "label": 0
                },
                {
                    "sent": "Then you the probability of documents for that query and then you.",
                    "label": 0
                },
                {
                    "sent": "Average that with the probability of documents, probability of topics on that document.",
                    "label": 0
                },
                {
                    "sent": "So for all the documents in our in our system, we've got the topic sits in an its proportion of the topic.",
                    "label": 0
                },
                {
                    "sent": "So we've got these probabilities in a vector.",
                    "label": 0
                },
                {
                    "sent": "It's a sparse vector, and this probability of the document given the query.",
                    "label": 0
                },
                {
                    "sent": "That's what the information retrieval system gives us very quickly.",
                    "label": 0
                },
                {
                    "sent": "So this is a sparse vector.",
                    "label": 0
                },
                {
                    "sent": "This is generated very quickly, and so this kind of calculation can be done at about the same magnitude as.",
                    "label": 0
                },
                {
                    "sent": "As as the retrieval task itself, what I'm not telling you though, is well.",
                    "label": 0
                },
                {
                    "sent": "I at the moment I'm not treating this sparse, so it's done quite just kind of slowly.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so last thing I'm going to do is show you the.",
                    "label": 0
                },
                {
                    "sent": "Some examples.",
                    "label": 0
                },
                {
                    "sent": "Senaga type everything in.",
                    "label": 0
                },
                {
                    "sent": "We sort of cooked up a bit of this interface recently, so it's not really neat, but.",
                    "label": 0
                },
                {
                    "sent": "Alright, take calls FBI.",
                    "label": 0
                },
                {
                    "sent": "And that's a space.",
                    "label": 0
                },
                {
                    "sent": "I just put a space in.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Say I say they were FBI and CIA.",
                    "label": 0
                },
                {
                    "sent": "Is something happening?",
                    "label": 0
                },
                {
                    "sent": "Now this.",
                    "label": 0
                },
                {
                    "sent": "This is a bit slow because the now I've done this in sort of a.",
                    "label": 0
                },
                {
                    "sent": "What do you call these A tag cloud style?",
                    "label": 0
                },
                {
                    "sent": "So the strength of here.",
                    "label": 0
                },
                {
                    "sent": "These are named entities that have been selected to match the topic.",
                    "label": 0
                },
                {
                    "sent": "This is done moderately poorly, actually, but it's not too bad for some, so you know you can see the.",
                    "label": 0
                },
                {
                    "sent": "Obviously it's got the actual topics that we put in, but there's all the related things you know.",
                    "label": 0
                },
                {
                    "sent": "The newspapers and the Soviet Union and September 11.",
                    "label": 0
                },
                {
                    "sent": "All of that stuff.",
                    "label": 0
                },
                {
                    "sent": "Watergate scandal.",
                    "label": 0
                },
                {
                    "sent": "All of that stuff in there.",
                    "label": 0
                },
                {
                    "sent": "It's fun.",
                    "label": 0
                },
                {
                    "sent": "This is actually quite entertaining and then this is the topics as selected from the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is the big one.",
                    "label": 0
                },
                {
                    "sent": "You can see crime famous people, murders and riots, national security law and justice.",
                    "label": 0
                },
                {
                    "sent": "So this is the topic that you've got for this spread of topics.",
                    "label": 0
                },
                {
                    "sent": "So what am I doing?",
                    "label": 0
                },
                {
                    "sent": "Do another one.",
                    "label": 0
                },
                {
                    "sent": "You can actually be quite specific here so.",
                    "label": 0
                },
                {
                    "sent": "Or dinner.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Banana and Apple.",
                    "label": 0
                },
                {
                    "sent": "So would a banana and Apple have in common?",
                    "label": 0
                },
                {
                    "sent": "Yes, this is something Australia is famous for is the big banana.",
                    "label": 0
                },
                {
                    "sent": "There's also the big pineapple on the big shape.",
                    "label": 0
                },
                {
                    "sent": "All sorts of strange bizarre tourist attractions, but so the entities that have in common.",
                    "label": 0
                },
                {
                    "sent": "Whether these are largely fruit and some other things, but the topic is overwhelming.",
                    "label": 0
                },
                {
                    "sent": "Lee Food and Drink and agriculture.",
                    "label": 0
                },
                {
                    "sent": "So anyway, you can play with this.",
                    "label": 0
                },
                {
                    "sent": "It's working reasonably well.",
                    "label": 0
                },
                {
                    "sent": "We'll have to.",
                    "label": 0
                },
                {
                    "sent": "We'll be putting it into a better interface at some stage and improving the entities algorithm, but so the idea is we can suggest some topics to you based on a query you put in and then you maybe use them to filter later queries so that the rest of this use of this in a way we haven't got it well, we have in another system, But this this interface at the moment is just.",
                    "label": 0
                },
                {
                    "sent": "Telling you the entities and topics related to a particular thing.",
                    "label": 0
                },
                {
                    "sent": "So we've sort of setting this up in a pipeline so you can do it with a with a search engine content we saw earlier.",
                    "label": 0
                },
                {
                    "sent": "You can do it with a sort of a biology content and come up with these things.",
                    "label": 0
                },
                {
                    "sent": "By the way, this this these things.",
                    "label": 0
                },
                {
                    "sent": "These results use a link text.",
                    "label": 0
                },
                {
                    "sent": "The anchor text they ranked.",
                    "label": 0
                },
                {
                    "sent": "Because the Wikipedia actually has very good ranking, Pagerank works quite well on it.",
                    "label": 0
                },
                {
                    "sent": "And other things, so there's quite a bit of activity behind the scenes on this.",
                    "label": 0
                },
                {
                    "sent": "This retrieval here.",
                    "label": 0
                },
                {
                    "sent": "And of course, is all used in the analysis of these things.",
                    "label": 0
                },
                {
                    "sent": "So those simple formula I showed you and it's just the idea of doing extra things using the retrieval engine is sort of a fast probabilistic processor.",
                    "label": 0
                },
                {
                    "sent": "So anyway, there we go.",
                    "label": 0
                },
                {
                    "sent": "That's that's the talk.",
                    "label": 0
                },
                {
                    "sent": "This is actually around, so if you remember this, you could play with it for a little bit before we take it down.",
                    "label": 0
                },
                {
                    "sent": "Put out.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Mia questions.",
                    "label": 0
                },
                {
                    "sent": "Any queries who wants to do a query?",
                    "label": 0
                },
                {
                    "sent": "So this is something that on top of the existing.",
                    "label": 0
                },
                {
                    "sent": "Index.",
                    "label": 0
                },
                {
                    "sent": "Oh, we got it on an index because we couldn't get access to the.",
                    "label": 0
                },
                {
                    "sent": "You need to get access to the scores.",
                    "label": 0
                },
                {
                    "sent": "As well as the.",
                    "label": 0
                },
                {
                    "sent": "Using the retrieval system both to give you the set of the short list of documents to look at, but also their scores.",
                    "label": 0
                },
                {
                    "sent": "So you need a short list of say 200 or 100 to be able to do this processing well, and the systems already produced that short list and the scores as a side effect of their work.",
                    "label": 0
                },
                {
                    "sent": "So you just feed that into the name recognition, the name collection, and a topic collection.",
                    "label": 0
                },
                {
                    "sent": "And it's all moderately fast.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Based on that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Goodnight we haven't seen this this demo though.",
                    "label": 0
                },
                {
                    "sent": "No, Sir.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes, I.",
                    "label": 0
                }
            ]
        }
    }
}