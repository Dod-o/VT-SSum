{
    "id": "dpoewelhu2ubxetnkb33ub32ctcbqpka",
    "title": "Transductive Inference for Class Membership Propagation in Web Ontologies",
    "info": {
        "introducer": [
            "Marko Grobelnik, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "author": [
            "Pasquale Minervini, University of Bari"
        ],
        "published": "July 8, 2013",
        "recorded": "May 2013",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Big Data"
        ]
    },
    "url": "http://videolectures.net/eswc2013_minervini_class/",
    "segmentation": [
        [
            "Good afternoon, I'm just calling Aveni and Communion still Barry and I'm going to present this work about transductive inference membership application in web ontologies."
        ],
        [
            "The main motivation is that given Dextrose state of the semantic Web only relying on purely deductive inference may be limiting.",
            "For example, in terms of scalability of reasoning in terms of handling and certainty also pleaded active inference doesn't does not exploit, say, statistical regularity's.",
            "You may have in the data and also be.",
            "Effective in the real world, the active inference needs some real step of knowledge engineering to build.",
            "An effective useful knowledge base and machine learning has been proposed to deal with such issues.",
            "An in particular to exploit.",
            "Like this you may find in in the data you."
        ],
        [
            "You find many applications of machine learning techniques in the context of semantic web concept learning.",
            "You have schema induction and assertion prediction.",
            "In particular, here we are focusing on certain prediction that is predicting the total value of entries in the in the box.",
            "And.",
            "Say for example, the concept membership relation of an individual to given concept and this problem is often tackled with the statistical models in semantic web, relatedly, charger."
        ],
        [
            "Such statistical models radar generative discriminative.",
            "In genetic models you have fully fledged probability distribution modeling all the variables involved in the prediction task and then the genetic prediction model basically calculates the posterior distribution of the target variable and use that distribution to perform predictions and discriminative models are in some sense simpler.",
            "They tend only to the aim only at determine mining, the most likely value.",
            "For some variable, so they aim at basically placing the chicken boundaries in the instance space.",
            "Um?",
            "OK."
        ],
        [
            "OK, basically in real life ontology so many often learning task in learning learning task you have bought labeled and unlabeled examples.",
            "For example, if you want to predict the membership to some concept, you may have a few individuals that belong to that concept and the positive examples of few individuals that belong to the complement to the negation of that concept at the negative examples and you may have a huge.",
            "Couples of individuals.",
            "How was the concept membership relation to the target concept is not known?",
            "And this is machine.",
            "Machine learning is cast as learning from bottle, labored and labored examples, and generative models tend to deal naturally with the.",
            "Slips available data during learning, for example by using the expectation maximization algorithm and the situation is it's a bit more tricky.",
            "In the case of.",
            "Learning discriminative models from both labeled and unlabeled data."
        ],
        [
            "We basically proposed using semi supervised learning.",
            "In the context of semantic Web knowledge, base is basically the same as provides learning.",
            "He's learning from bottle labeled and unlabeled examples and grounds on the assumption that our instances are distributed, informative with respect to the distribution of the labels.",
            "And.",
            "This is often implemented in real life learning algorithm.",
            "By means of this general assumption, the semi supervised the smoothness assumptions.",
            "That says that if you instances are in the same identity area in identity region of the instance space, they will tend to have the same labels.",
            "For example here we have the two months data set in the left plot.",
            "We are only given one positive one negative example.",
            "And in that case, we may be tempted to build a classifier by simply putting that shown boundary right between the two examples in the middle.",
            "We also given.",
            "The summer labeled examples, and we recognize this structure, underlining.",
            "The distribution of the instances there are not only formally distributed in this instance space and on the right you have the actual labels that you may reconstruct by making use of the semi supervised smoothness assumption."
        ],
        [
            "In Semi supervised learning you also have two kinds of settings.",
            "You have either inductive supervised learning and transductive semi supervised learning, inductive supervised learning.",
            "You want to learn an.",
            "Additional function defined of a hole in space.",
            "While in transductive supervised learning you only are interested in having an extra extra prediction of the labels for given set of unlabeled instances, which is.",
            "Really simple task here.",
            "In the picture we have have more sketch explaining what's the main difference.",
            "Basically to perform inductive inference you pick up to first have to pick any prothesis from the hypothesis space, and then use that hypothesis deductively to perform predictions while in transductive inference you can avoid the intermediate.",
            "Inductive and deductive steps which allows you to.",
            "To scale to large knowledge bases."
        ],
        [
            "OK, so we in particular purpose performing.",
            "Transductive semi supervised.",
            "The inference in the context of semantic Web knowledge basis.",
            "So the aim will be to find, given a set of labeled, unlabeled instances, that is, individuals either member of the concept member of the negation of that concept and with an unknown concept membership relation.",
            "To find according to the assumption I said before labeling, that is a set of concept concept memberships that various smoothly between similar individuals.",
            "And and also is consistent with the few training examples we were given.",
            "And basically to scale to try to scale to large knowledge bases.",
            "We proposed making use of graph graph based regularization that also other than predicting.",
            "Concept membership also allows to have a confidence value, some kind of confidence value associated to the infrared concept memberships, and this is the algorithm in short, basically.",
            "As in the previous presentation, we create a similarity graph between the individuals in the training set, both labeled and unlabeled, and then we propagate information in this similarity graph by minimizing a cost function which can be minimized in this case.",
            "In very efficiently.",
            "And this cost function basically enforces consistency with the training examples and smoothness with respect to the structure of the similarity graph.",
            "And in this approach we use graph and description logic kernels to build this similarity graph.",
            "So it's like if we are acting in the in the kernels in a product space."
        ],
        [
            "Those are a few examples of the similarity graph graphs arising from real life ontologies.",
            "Here we have the AFB affiliations ontology on the left we have persons on the right we have particles calculated calculated by means of a kernel and the colors reflect the.",
            "I search group affiliations of ADA persons and articles and we see that the research group affiliations tend to vary smoothly, that is.",
            "Individuals in the same cluster tend to have a similar behavior with respect to individuals addressing the individuals in the in the same cluster."
        ],
        [
            "And this is the same phenomenon happening into on a much smaller scale happening onto ontologists montalla.",
            "Geez, from the tones repository."
        ],
        [
            "And this is the propagation going on.",
            "Basically, we there was labeled example on the graph on the left and on the right on the right we have that by minimizing the cost function we are able to propagate the information to the graph.",
            "An red means we have a stronger confidence in that label and as you walk away from the library example, you see that.",
            "The notes tend to become less red, and that means we have less confidence in that in that labeling.",
            "OK."
        ],
        [
            "Um, OK.",
            "This is actually we propagate information basically.",
            "We encode the similarities in two and a symmetric adjacency matrix W net one and relax labels, binary labels representing membership to the concept of membership to the complement of the concept we represented, relax those labels into real numbers and we get real numbers from ranging from minus one to one.",
            "And so we need that cost function that bot that penalizes divergents from the training labels.",
            "So that enforces the consistency with the labeled examples in the new labels.",
            "And also enforces a smoothness that is similar individuals individuals in the same identity region of the graph of the similarity graph must tend to have the same label and for example in this cost function we have this component.",
            "Sorry this component that enforces consistency with training examples.",
            "Those are training labels and also the labels.",
            "We are input into the cost function and here we enforce.",
            "Smoothness between similar instances.",
            "This is basically this.",
            "This is the graph Laplacian, but if you read this in this way, it's much more understandable.",
            "But basically if this is non zero, that is if two instances are linked to each other in the similarity graph.",
            "Um, this value.",
            "OK is a non zero and so those two values that there two labels must tend to be similar.",
            "Otherwise this component growth had cost growth and you're not minimizing the cost function anymore."
        ],
        [
            "It.",
            "OK, basically I think I will skip may detail say here basically finding global global minimum for this cost function boils down to solving a large sparse linear system.",
            "There is a problem that as our nearly linear complexity with respect to the number of nonzero entries in the coefficient matrix, so also there are MapReduce based implementations for this, so this can be implemented efficiently, it can.",
            "Possibly scale on larger knowledge basis."
        ],
        [
            "OK, we also tried a different approach, basically our.",
            "We tried instead of having a single similarity graph resulting from single kernel.",
            "Basically we tried to combine more similarity graphs those and waited.",
            "Each simulated graph is associated to a weight and weights are learned with optimization methods and.",
            "And each similarity graph is referred to.",
            "Some kind of relation between individuals, say in the case of persons in the ifb ontology.",
            "We have Seiko auto ship over document, friendship relation.",
            "Staying in the same office and so on.",
            "We've focused said on the task of predicting.",
            "Essential group affiliations in the IFB affiliations ontology and we tried.",
            "Up several methods and use the SVM as our baseline because.",
            "Assertion prediction by using SVM is a recently proposed set of the after this commutative method, so we use that as a baseline.",
            "Anne.",
            "And we basically we tried."
        ],
        [
            "Two different cost functions and that this was indeed too conservative, because this revealed to 10 to push labels to 02 strongly, while this.",
            "This is much more conservative than this, so this results obtained with this cost function where useful, so we're only reporting the results of this and the operation of discounts function called Macondo work, where this move is pushed to 0."
        ],
        [
            "As well as this, that means he breathed McComb, mark of random walk."
        ],
        [
            "And he had some results, and we have the results.",
            "Using those results are used are obtained to tenfold cross validation and.",
            "We are reporting but F1 and the area under the precision recall curve.",
            "And here we have the results obtained using nine faults in the training step, and here only five faults in the training step and.",
            "Show mea it emerges that there are nice results.",
            "With the method combining different regularizers becausw basically here if we only really 2 on a kernel for predicting building the similarity graph.",
            "There are many relations that provide an informant no information to the predictive task, but instead they include noise in the prediction so.",
            "The area under the Bush nickel cover decreases a lot."
        ],
        [
            "OK. And all the source code is available online.",
            "And.",
            "I indeed in the beginning was thinking that GAFCON elsewhere.",
            "OK, for detecting for the neighborhood.",
            "For.",
            "For each individual in the knowledge base.",
            "But yeah, it really did not to be always the case.",
            "And also by looking about labels propagate.",
            "You also get hints about how well is a kernel for sustain prediction task and.",
            "The current effort is.",
            "About learning.",
            "Efficiently learning combination of kernels, learning the best combination of kernels with respect to domain with respect to ascertain couple of domain and prediction tasks by taking account but.",
            "The terminological part of the.",
            "Ontology that this by using description logic kernels and the relational structure between the individuals in the knowledge base.",
            "Um and some preliminary work is already available in the sewers here, but it's maybe too early to claim it's finished and there will be some work about this.",
            "OK, I think I've finished.",
            "Just have a question.",
            "I mean you are constructing green graph kernels and for this I mean you have to construct freezer crafts for resources.",
            "So which tree depth or craft diameter to use for this and how do you construct those efficiently basically?",
            "You mean what are you doing?",
            "Focus stacking the graph?",
            "And basically sparkle gel.",
            "I asked it's an extension of Sparkle for description, logic, knowledge basis and like I ask him like.",
            "Rolls.",
            "And say.",
            "With the type of what and it gives me all the all the triples that I need to construct the graph anyway.",
            "It's also that component is available online.",
            "Just.",
            "OK, do you think picking larger crafts would make a difference?",
            "Do you think I'm picking larger crafts and make a difference?",
            "I'm now would describe that then you street at one.",
            "Basically, if I understand you correctly, so would make a difference to select larger crafts for each resource.",
            "You mean how much the Gulf sides impacts on the results?",
            "Yes, basically, graph kernels tend to consider the only safe for two nodes basically tend to consider only their neighborhood, so I think they this neighborhood can be.",
            "Those neighborhood can be easily retrieved by, say, sparkle queries.",
            "I even, even if it's DPS, a DBA is huge of course, but I think you can retrieve what you need to create a kernel.",
            "Say for example the subtree kernel percentage last year in this conference.",
            "Bye bye.",
            "A sparkle query asking for the neighborhood associated to two resources.",
            "My question would be so sorry I missed the half of your talk because it just arrived, so I don't know.",
            "But how so?",
            "Right now you're just finding out what the best kernel is by testing it, basically like using it in your learning algorithm.",
            "Could you think about coming up with some rules of thumb that you say I have some measures on my graph, like I don't know if this centrality is like this.",
            "I would recommend you to use this graph kernel or something like that so.",
            "Something that you don't have to train all those kernels on the graph.",
            "I believe.",
            "I believe it's very task related like I've been giving a look to the AFB affiliations ontology for some time and like.",
            "I was able to come up with the prediction task.",
            "We were OK.",
            "So.",
            "Like given a kernel, the prediction task could.",
            "There was a prediction task that could have been OK with the kernel and another that would have been totally not OK with that kernel's I think.",
            "I personally believe that minimizing some kind of.",
            "Loss function, say I getting saved the best linear combination of a set of ethical genius set of kernels is maybe the best way to go.",
            "And also I think I forgot to say this, but like here I used to learn the weights associated to which similarity matrix similarity graph.",
            "I used the basically I enforced the sparseness.",
            "So some moves were pushed to zero and this I believe is OK because this also gives hints about which relations are relevant to that task and that in my opinion personal opinion allows you to go beyond the simple assertion prediction because once you get that one relation is important for task.",
            "We can also encode that knowledge in form saying form of description logic axioms.",
            "So you go beyond.",
            "Such a prediction to some kind of schema induction?",
            "Yeah, but my question was a little bit different so so right now you need to test.",
            "You need to evaluate all of those words.",
            "So do you think there's a way where you don't need to do this?",
            "I mean it's very elegant.",
            "Machine learning algorithm does it for you, but do you think there would be some some way where you don't have to try all of the kernels?",
            "But say I look at the graph, I analyze the graph with some measures and then say this is probably the most suitable kernel for this problem.",
            "It's non trivial.",
            "In this exact moment, but it would be of course it would be nice because you would have you would avoid a lot of optimization work.",
            "Yeah, thank you.",
            "You could do a little bit of kernel learning I guess.",
            "I mean this is not uncommon, technique is just you didn't use it.",
            "So scalability.",
            "I think it's a little bit of an issue here in general.",
            "Now.",
            "Basically, Attic metrics basically.",
            "If you would say yes or no because you don't have time.",
            "No OK no no no.",
            "Because basically OK no.",
            "There's also an implementation in mouth for solving this problem, so I think even with the.",
            "And there is more left.",
            "You can build something that scales very well and you have amazing bad performance on SVM.",
            "How did you set it up so that it works with the simple add margin SVM?",
            "Basically I'm using groupby convex optimizer.",
            "The yeah, my implementation is also available here.",
            "Yeah, I won't go to check right now.",
            "It just why?",
            "Because it was proposed on a Journal article a few months ago as at state of the art method for.",
            "And for the induction of robust classifiers, sure the question is how did you set it up so that it works so so bad.",
            "Should be like twice as good.",
            "Still wouldn't be the best, but yeah.",
            "I think the.",
            "In the using the position on the area under the curve is unfair.",
            "With the SVM, I think we have to give a look to the F1 is even worse.",
            "Because it it?",
            "It's a Boolean value.",
            "It's not really a regression value.",
            "And basically it's a simple graph kernel fed to the support vector machine that did the classification.",
            "OK, I guess we need to finish unless there's some urgent stuff.",
            "OK thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good afternoon, I'm just calling Aveni and Communion still Barry and I'm going to present this work about transductive inference membership application in web ontologies.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The main motivation is that given Dextrose state of the semantic Web only relying on purely deductive inference may be limiting.",
                    "label": 1
                },
                {
                    "sent": "For example, in terms of scalability of reasoning in terms of handling and certainty also pleaded active inference doesn't does not exploit, say, statistical regularity's.",
                    "label": 0
                },
                {
                    "sent": "You may have in the data and also be.",
                    "label": 0
                },
                {
                    "sent": "Effective in the real world, the active inference needs some real step of knowledge engineering to build.",
                    "label": 0
                },
                {
                    "sent": "An effective useful knowledge base and machine learning has been proposed to deal with such issues.",
                    "label": 0
                },
                {
                    "sent": "An in particular to exploit.",
                    "label": 0
                },
                {
                    "sent": "Like this you may find in in the data you.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You find many applications of machine learning techniques in the context of semantic web concept learning.",
                    "label": 1
                },
                {
                    "sent": "You have schema induction and assertion prediction.",
                    "label": 1
                },
                {
                    "sent": "In particular, here we are focusing on certain prediction that is predicting the total value of entries in the in the box.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Say for example, the concept membership relation of an individual to given concept and this problem is often tackled with the statistical models in semantic web, relatedly, charger.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Such statistical models radar generative discriminative.",
                    "label": 0
                },
                {
                    "sent": "In genetic models you have fully fledged probability distribution modeling all the variables involved in the prediction task and then the genetic prediction model basically calculates the posterior distribution of the target variable and use that distribution to perform predictions and discriminative models are in some sense simpler.",
                    "label": 1
                },
                {
                    "sent": "They tend only to the aim only at determine mining, the most likely value.",
                    "label": 0
                },
                {
                    "sent": "For some variable, so they aim at basically placing the chicken boundaries in the instance space.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, basically in real life ontology so many often learning task in learning learning task you have bought labeled and unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "For example, if you want to predict the membership to some concept, you may have a few individuals that belong to that concept and the positive examples of few individuals that belong to the complement to the negation of that concept at the negative examples and you may have a huge.",
                    "label": 0
                },
                {
                    "sent": "Couples of individuals.",
                    "label": 0
                },
                {
                    "sent": "How was the concept membership relation to the target concept is not known?",
                    "label": 0
                },
                {
                    "sent": "And this is machine.",
                    "label": 0
                },
                {
                    "sent": "Machine learning is cast as learning from bottle, labored and labored examples, and generative models tend to deal naturally with the.",
                    "label": 0
                },
                {
                    "sent": "Slips available data during learning, for example by using the expectation maximization algorithm and the situation is it's a bit more tricky.",
                    "label": 0
                },
                {
                    "sent": "In the case of.",
                    "label": 0
                },
                {
                    "sent": "Learning discriminative models from both labeled and unlabeled data.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We basically proposed using semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "In the context of semantic Web knowledge, base is basically the same as provides learning.",
                    "label": 0
                },
                {
                    "sent": "He's learning from bottle labeled and unlabeled examples and grounds on the assumption that our instances are distributed, informative with respect to the distribution of the labels.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This is often implemented in real life learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "By means of this general assumption, the semi supervised the smoothness assumptions.",
                    "label": 0
                },
                {
                    "sent": "That says that if you instances are in the same identity area in identity region of the instance space, they will tend to have the same labels.",
                    "label": 0
                },
                {
                    "sent": "For example here we have the two months data set in the left plot.",
                    "label": 0
                },
                {
                    "sent": "We are only given one positive one negative example.",
                    "label": 0
                },
                {
                    "sent": "And in that case, we may be tempted to build a classifier by simply putting that shown boundary right between the two examples in the middle.",
                    "label": 0
                },
                {
                    "sent": "We also given.",
                    "label": 0
                },
                {
                    "sent": "The summer labeled examples, and we recognize this structure, underlining.",
                    "label": 0
                },
                {
                    "sent": "The distribution of the instances there are not only formally distributed in this instance space and on the right you have the actual labels that you may reconstruct by making use of the semi supervised smoothness assumption.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In Semi supervised learning you also have two kinds of settings.",
                    "label": 0
                },
                {
                    "sent": "You have either inductive supervised learning and transductive semi supervised learning, inductive supervised learning.",
                    "label": 0
                },
                {
                    "sent": "You want to learn an.",
                    "label": 0
                },
                {
                    "sent": "Additional function defined of a hole in space.",
                    "label": 0
                },
                {
                    "sent": "While in transductive supervised learning you only are interested in having an extra extra prediction of the labels for given set of unlabeled instances, which is.",
                    "label": 0
                },
                {
                    "sent": "Really simple task here.",
                    "label": 0
                },
                {
                    "sent": "In the picture we have have more sketch explaining what's the main difference.",
                    "label": 0
                },
                {
                    "sent": "Basically to perform inductive inference you pick up to first have to pick any prothesis from the hypothesis space, and then use that hypothesis deductively to perform predictions while in transductive inference you can avoid the intermediate.",
                    "label": 0
                },
                {
                    "sent": "Inductive and deductive steps which allows you to.",
                    "label": 0
                },
                {
                    "sent": "To scale to large knowledge bases.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we in particular purpose performing.",
                    "label": 0
                },
                {
                    "sent": "Transductive semi supervised.",
                    "label": 0
                },
                {
                    "sent": "The inference in the context of semantic Web knowledge basis.",
                    "label": 0
                },
                {
                    "sent": "So the aim will be to find, given a set of labeled, unlabeled instances, that is, individuals either member of the concept member of the negation of that concept and with an unknown concept membership relation.",
                    "label": 1
                },
                {
                    "sent": "To find according to the assumption I said before labeling, that is a set of concept concept memberships that various smoothly between similar individuals.",
                    "label": 1
                },
                {
                    "sent": "And and also is consistent with the few training examples we were given.",
                    "label": 1
                },
                {
                    "sent": "And basically to scale to try to scale to large knowledge bases.",
                    "label": 0
                },
                {
                    "sent": "We proposed making use of graph graph based regularization that also other than predicting.",
                    "label": 0
                },
                {
                    "sent": "Concept membership also allows to have a confidence value, some kind of confidence value associated to the infrared concept memberships, and this is the algorithm in short, basically.",
                    "label": 1
                },
                {
                    "sent": "As in the previous presentation, we create a similarity graph between the individuals in the training set, both labeled and unlabeled, and then we propagate information in this similarity graph by minimizing a cost function which can be minimized in this case.",
                    "label": 1
                },
                {
                    "sent": "In very efficiently.",
                    "label": 0
                },
                {
                    "sent": "And this cost function basically enforces consistency with the training examples and smoothness with respect to the structure of the similarity graph.",
                    "label": 0
                },
                {
                    "sent": "And in this approach we use graph and description logic kernels to build this similarity graph.",
                    "label": 0
                },
                {
                    "sent": "So it's like if we are acting in the in the kernels in a product space.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those are a few examples of the similarity graph graphs arising from real life ontologies.",
                    "label": 0
                },
                {
                    "sent": "Here we have the AFB affiliations ontology on the left we have persons on the right we have particles calculated calculated by means of a kernel and the colors reflect the.",
                    "label": 0
                },
                {
                    "sent": "I search group affiliations of ADA persons and articles and we see that the research group affiliations tend to vary smoothly, that is.",
                    "label": 0
                },
                {
                    "sent": "Individuals in the same cluster tend to have a similar behavior with respect to individuals addressing the individuals in the in the same cluster.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the same phenomenon happening into on a much smaller scale happening onto ontologists montalla.",
                    "label": 0
                },
                {
                    "sent": "Geez, from the tones repository.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the propagation going on.",
                    "label": 0
                },
                {
                    "sent": "Basically, we there was labeled example on the graph on the left and on the right on the right we have that by minimizing the cost function we are able to propagate the information to the graph.",
                    "label": 0
                },
                {
                    "sent": "An red means we have a stronger confidence in that label and as you walk away from the library example, you see that.",
                    "label": 0
                },
                {
                    "sent": "The notes tend to become less red, and that means we have less confidence in that in that labeling.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um, OK.",
                    "label": 0
                },
                {
                    "sent": "This is actually we propagate information basically.",
                    "label": 0
                },
                {
                    "sent": "We encode the similarities in two and a symmetric adjacency matrix W net one and relax labels, binary labels representing membership to the concept of membership to the complement of the concept we represented, relax those labels into real numbers and we get real numbers from ranging from minus one to one.",
                    "label": 1
                },
                {
                    "sent": "And so we need that cost function that bot that penalizes divergents from the training labels.",
                    "label": 1
                },
                {
                    "sent": "So that enforces the consistency with the labeled examples in the new labels.",
                    "label": 0
                },
                {
                    "sent": "And also enforces a smoothness that is similar individuals individuals in the same identity region of the graph of the similarity graph must tend to have the same label and for example in this cost function we have this component.",
                    "label": 0
                },
                {
                    "sent": "Sorry this component that enforces consistency with training examples.",
                    "label": 1
                },
                {
                    "sent": "Those are training labels and also the labels.",
                    "label": 0
                },
                {
                    "sent": "We are input into the cost function and here we enforce.",
                    "label": 0
                },
                {
                    "sent": "Smoothness between similar instances.",
                    "label": 0
                },
                {
                    "sent": "This is basically this.",
                    "label": 0
                },
                {
                    "sent": "This is the graph Laplacian, but if you read this in this way, it's much more understandable.",
                    "label": 0
                },
                {
                    "sent": "But basically if this is non zero, that is if two instances are linked to each other in the similarity graph.",
                    "label": 0
                },
                {
                    "sent": "Um, this value.",
                    "label": 0
                },
                {
                    "sent": "OK is a non zero and so those two values that there two labels must tend to be similar.",
                    "label": 0
                },
                {
                    "sent": "Otherwise this component growth had cost growth and you're not minimizing the cost function anymore.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "OK, basically I think I will skip may detail say here basically finding global global minimum for this cost function boils down to solving a large sparse linear system.",
                    "label": 1
                },
                {
                    "sent": "There is a problem that as our nearly linear complexity with respect to the number of nonzero entries in the coefficient matrix, so also there are MapReduce based implementations for this, so this can be implemented efficiently, it can.",
                    "label": 1
                },
                {
                    "sent": "Possibly scale on larger knowledge basis.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we also tried a different approach, basically our.",
                    "label": 0
                },
                {
                    "sent": "We tried instead of having a single similarity graph resulting from single kernel.",
                    "label": 0
                },
                {
                    "sent": "Basically we tried to combine more similarity graphs those and waited.",
                    "label": 0
                },
                {
                    "sent": "Each simulated graph is associated to a weight and weights are learned with optimization methods and.",
                    "label": 1
                },
                {
                    "sent": "And each similarity graph is referred to.",
                    "label": 0
                },
                {
                    "sent": "Some kind of relation between individuals, say in the case of persons in the ifb ontology.",
                    "label": 0
                },
                {
                    "sent": "We have Seiko auto ship over document, friendship relation.",
                    "label": 0
                },
                {
                    "sent": "Staying in the same office and so on.",
                    "label": 0
                },
                {
                    "sent": "We've focused said on the task of predicting.",
                    "label": 0
                },
                {
                    "sent": "Essential group affiliations in the IFB affiliations ontology and we tried.",
                    "label": 1
                },
                {
                    "sent": "Up several methods and use the SVM as our baseline because.",
                    "label": 0
                },
                {
                    "sent": "Assertion prediction by using SVM is a recently proposed set of the after this commutative method, so we use that as a baseline.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And we basically we tried.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two different cost functions and that this was indeed too conservative, because this revealed to 10 to push labels to 02 strongly, while this.",
                    "label": 0
                },
                {
                    "sent": "This is much more conservative than this, so this results obtained with this cost function where useful, so we're only reporting the results of this and the operation of discounts function called Macondo work, where this move is pushed to 0.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As well as this, that means he breathed McComb, mark of random walk.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And he had some results, and we have the results.",
                    "label": 0
                },
                {
                    "sent": "Using those results are used are obtained to tenfold cross validation and.",
                    "label": 0
                },
                {
                    "sent": "We are reporting but F1 and the area under the precision recall curve.",
                    "label": 0
                },
                {
                    "sent": "And here we have the results obtained using nine faults in the training step, and here only five faults in the training step and.",
                    "label": 0
                },
                {
                    "sent": "Show mea it emerges that there are nice results.",
                    "label": 0
                },
                {
                    "sent": "With the method combining different regularizers becausw basically here if we only really 2 on a kernel for predicting building the similarity graph.",
                    "label": 0
                },
                {
                    "sent": "There are many relations that provide an informant no information to the predictive task, but instead they include noise in the prediction so.",
                    "label": 0
                },
                {
                    "sent": "The area under the Bush nickel cover decreases a lot.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. And all the source code is available online.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I indeed in the beginning was thinking that GAFCON elsewhere.",
                    "label": 0
                },
                {
                    "sent": "OK, for detecting for the neighborhood.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "For each individual in the knowledge base.",
                    "label": 0
                },
                {
                    "sent": "But yeah, it really did not to be always the case.",
                    "label": 1
                },
                {
                    "sent": "And also by looking about labels propagate.",
                    "label": 0
                },
                {
                    "sent": "You also get hints about how well is a kernel for sustain prediction task and.",
                    "label": 1
                },
                {
                    "sent": "The current effort is.",
                    "label": 0
                },
                {
                    "sent": "About learning.",
                    "label": 1
                },
                {
                    "sent": "Efficiently learning combination of kernels, learning the best combination of kernels with respect to domain with respect to ascertain couple of domain and prediction tasks by taking account but.",
                    "label": 1
                },
                {
                    "sent": "The terminological part of the.",
                    "label": 0
                },
                {
                    "sent": "Ontology that this by using description logic kernels and the relational structure between the individuals in the knowledge base.",
                    "label": 0
                },
                {
                    "sent": "Um and some preliminary work is already available in the sewers here, but it's maybe too early to claim it's finished and there will be some work about this.",
                    "label": 0
                },
                {
                    "sent": "OK, I think I've finished.",
                    "label": 0
                },
                {
                    "sent": "Just have a question.",
                    "label": 0
                },
                {
                    "sent": "I mean you are constructing green graph kernels and for this I mean you have to construct freezer crafts for resources.",
                    "label": 0
                },
                {
                    "sent": "So which tree depth or craft diameter to use for this and how do you construct those efficiently basically?",
                    "label": 0
                },
                {
                    "sent": "You mean what are you doing?",
                    "label": 0
                },
                {
                    "sent": "Focus stacking the graph?",
                    "label": 0
                },
                {
                    "sent": "And basically sparkle gel.",
                    "label": 0
                },
                {
                    "sent": "I asked it's an extension of Sparkle for description, logic, knowledge basis and like I ask him like.",
                    "label": 0
                },
                {
                    "sent": "Rolls.",
                    "label": 0
                },
                {
                    "sent": "And say.",
                    "label": 0
                },
                {
                    "sent": "With the type of what and it gives me all the all the triples that I need to construct the graph anyway.",
                    "label": 0
                },
                {
                    "sent": "It's also that component is available online.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "OK, do you think picking larger crafts would make a difference?",
                    "label": 0
                },
                {
                    "sent": "Do you think I'm picking larger crafts and make a difference?",
                    "label": 0
                },
                {
                    "sent": "I'm now would describe that then you street at one.",
                    "label": 0
                },
                {
                    "sent": "Basically, if I understand you correctly, so would make a difference to select larger crafts for each resource.",
                    "label": 0
                },
                {
                    "sent": "You mean how much the Gulf sides impacts on the results?",
                    "label": 0
                },
                {
                    "sent": "Yes, basically, graph kernels tend to consider the only safe for two nodes basically tend to consider only their neighborhood, so I think they this neighborhood can be.",
                    "label": 0
                },
                {
                    "sent": "Those neighborhood can be easily retrieved by, say, sparkle queries.",
                    "label": 0
                },
                {
                    "sent": "I even, even if it's DPS, a DBA is huge of course, but I think you can retrieve what you need to create a kernel.",
                    "label": 0
                },
                {
                    "sent": "Say for example the subtree kernel percentage last year in this conference.",
                    "label": 0
                },
                {
                    "sent": "Bye bye.",
                    "label": 0
                },
                {
                    "sent": "A sparkle query asking for the neighborhood associated to two resources.",
                    "label": 0
                },
                {
                    "sent": "My question would be so sorry I missed the half of your talk because it just arrived, so I don't know.",
                    "label": 0
                },
                {
                    "sent": "But how so?",
                    "label": 0
                },
                {
                    "sent": "Right now you're just finding out what the best kernel is by testing it, basically like using it in your learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Could you think about coming up with some rules of thumb that you say I have some measures on my graph, like I don't know if this centrality is like this.",
                    "label": 0
                },
                {
                    "sent": "I would recommend you to use this graph kernel or something like that so.",
                    "label": 0
                },
                {
                    "sent": "Something that you don't have to train all those kernels on the graph.",
                    "label": 0
                },
                {
                    "sent": "I believe.",
                    "label": 0
                },
                {
                    "sent": "I believe it's very task related like I've been giving a look to the AFB affiliations ontology for some time and like.",
                    "label": 0
                },
                {
                    "sent": "I was able to come up with the prediction task.",
                    "label": 0
                },
                {
                    "sent": "We were OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Like given a kernel, the prediction task could.",
                    "label": 0
                },
                {
                    "sent": "There was a prediction task that could have been OK with the kernel and another that would have been totally not OK with that kernel's I think.",
                    "label": 0
                },
                {
                    "sent": "I personally believe that minimizing some kind of.",
                    "label": 1
                },
                {
                    "sent": "Loss function, say I getting saved the best linear combination of a set of ethical genius set of kernels is maybe the best way to go.",
                    "label": 0
                },
                {
                    "sent": "And also I think I forgot to say this, but like here I used to learn the weights associated to which similarity matrix similarity graph.",
                    "label": 0
                },
                {
                    "sent": "I used the basically I enforced the sparseness.",
                    "label": 0
                },
                {
                    "sent": "So some moves were pushed to zero and this I believe is OK because this also gives hints about which relations are relevant to that task and that in my opinion personal opinion allows you to go beyond the simple assertion prediction because once you get that one relation is important for task.",
                    "label": 0
                },
                {
                    "sent": "We can also encode that knowledge in form saying form of description logic axioms.",
                    "label": 0
                },
                {
                    "sent": "So you go beyond.",
                    "label": 0
                },
                {
                    "sent": "Such a prediction to some kind of schema induction?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but my question was a little bit different so so right now you need to test.",
                    "label": 0
                },
                {
                    "sent": "You need to evaluate all of those words.",
                    "label": 0
                },
                {
                    "sent": "So do you think there's a way where you don't need to do this?",
                    "label": 0
                },
                {
                    "sent": "I mean it's very elegant.",
                    "label": 0
                },
                {
                    "sent": "Machine learning algorithm does it for you, but do you think there would be some some way where you don't have to try all of the kernels?",
                    "label": 0
                },
                {
                    "sent": "But say I look at the graph, I analyze the graph with some measures and then say this is probably the most suitable kernel for this problem.",
                    "label": 0
                },
                {
                    "sent": "It's non trivial.",
                    "label": 0
                },
                {
                    "sent": "In this exact moment, but it would be of course it would be nice because you would have you would avoid a lot of optimization work.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thank you.",
                    "label": 0
                },
                {
                    "sent": "You could do a little bit of kernel learning I guess.",
                    "label": 0
                },
                {
                    "sent": "I mean this is not uncommon, technique is just you didn't use it.",
                    "label": 0
                },
                {
                    "sent": "So scalability.",
                    "label": 0
                },
                {
                    "sent": "I think it's a little bit of an issue here in general.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Basically, Attic metrics basically.",
                    "label": 0
                },
                {
                    "sent": "If you would say yes or no because you don't have time.",
                    "label": 0
                },
                {
                    "sent": "No OK no no no.",
                    "label": 0
                },
                {
                    "sent": "Because basically OK no.",
                    "label": 0
                },
                {
                    "sent": "There's also an implementation in mouth for solving this problem, so I think even with the.",
                    "label": 0
                },
                {
                    "sent": "And there is more left.",
                    "label": 0
                },
                {
                    "sent": "You can build something that scales very well and you have amazing bad performance on SVM.",
                    "label": 0
                },
                {
                    "sent": "How did you set it up so that it works with the simple add margin SVM?",
                    "label": 0
                },
                {
                    "sent": "Basically I'm using groupby convex optimizer.",
                    "label": 0
                },
                {
                    "sent": "The yeah, my implementation is also available here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I won't go to check right now.",
                    "label": 0
                },
                {
                    "sent": "It just why?",
                    "label": 0
                },
                {
                    "sent": "Because it was proposed on a Journal article a few months ago as at state of the art method for.",
                    "label": 0
                },
                {
                    "sent": "And for the induction of robust classifiers, sure the question is how did you set it up so that it works so so bad.",
                    "label": 0
                },
                {
                    "sent": "Should be like twice as good.",
                    "label": 0
                },
                {
                    "sent": "Still wouldn't be the best, but yeah.",
                    "label": 0
                },
                {
                    "sent": "I think the.",
                    "label": 0
                },
                {
                    "sent": "In the using the position on the area under the curve is unfair.",
                    "label": 0
                },
                {
                    "sent": "With the SVM, I think we have to give a look to the F1 is even worse.",
                    "label": 0
                },
                {
                    "sent": "Because it it?",
                    "label": 0
                },
                {
                    "sent": "It's a Boolean value.",
                    "label": 0
                },
                {
                    "sent": "It's not really a regression value.",
                    "label": 0
                },
                {
                    "sent": "And basically it's a simple graph kernel fed to the support vector machine that did the classification.",
                    "label": 0
                },
                {
                    "sent": "OK, I guess we need to finish unless there's some urgent stuff.",
                    "label": 0
                },
                {
                    "sent": "OK thanks.",
                    "label": 0
                }
            ]
        }
    }
}