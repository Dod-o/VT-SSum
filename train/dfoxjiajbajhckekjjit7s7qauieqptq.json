{
    "id": "dfoxjiajbajhckekjjit7s7qauieqptq",
    "title": "Selective Sampling for Information Extraction with a Committee of Classifiers",
    "info": {
        "author": [
            "Ben Hachey, University of Edinburgh"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "April 2005",
        "category": [
            "Top->Computer Science->Information Extraction"
        ]
    },
    "url": "http://videolectures.net/pcw05_hachey_ssiec/",
    "segmentation": [
        [
            "So I'll present the approach that we had.",
            "We only participated, but Neil said in the active learning task actually.",
            "So this would be just sort of an active learning talk.",
            "This is joint work with Marcus Becker who is here, and Claire, Grover, and Ewan Klein in Edinburgh."
        ],
        [
            "So first sort of introduce our approach and talk a bit about or present the results in a slightly different way than what Neil did.",
            "And then actually most of the talk will be a bit about discussion of.",
            "Of look at a couple of different selection metrics in the one that I present 1st and then talk about.",
            "Sort of how approaches to active learning interact with the cost metric that you use.",
            "And then just a little bit of air analysis."
        ],
        [
            "So there's sort of.",
            "In the literature, sort of two main approaches to active learning, uncertainty sampling and queried by committee.",
            "In the first uncertainty sampling, the usefulness is estimated by the uncertainty of the single learner.",
            "So use just the confidence from the learner to choose which examples the classifier is least confident about.",
            "Or you can look at the entropy and label examples for which the output distribution from the classifier has the highest entropy.",
            "This will be the flatter stitch distribution.",
            "We follow the second approach, queried by committee, where the usefulness of an example is estimated by the disagreement among a committee of learners.",
            "And again, there are a number of ways to go about this.",
            "You look at vote entropy.",
            "The disagreement between the winners for different classifiers it KL divergent switch looks at the difference between the class output distribution with different classifiers or F score, which I'll talk about later, which is sort of more structural comparison of the output to the different classifiers."
        ],
        [
            "So there's a number of ways to create a committee.",
            "People have used begging, randomly perturbing event counts, and Nitesh Chawla talk.",
            "Done on Monday about random feature subspaces.",
            "And.",
            "The problem with these I mean these are these are good because they are automatic, but we're not insured diversity and in our setting where we want to look at the disagreement between the classifiers.",
            "We opted to go for a hand handcrafted feature split, actually because we can ensure.",
            "Diversity.",
            "Between the feature sets that we use and can ensure some level of independence between the fields.",
            "It would be interesting though, to look at perhaps like random feature subspaces and see how it compares to the handcrafted feature splits, but we haven't done that just now.",
            "So, and we use a maximum entropy Markov model classifier, which is one of the classifiers.",
            "I think that Chris from the Stanford Group will talk about later."
        ],
        [
            "So this is the feature split that we used.",
            "So in the first feature set, basically what we have are features of the word, the shape of the word and the position in the document of the token.",
            "And then these are combined with other sort of sequencing features, so previously predicted.",
            "Entity tags.",
            "And then the second feature set looks at parts of speech.",
            "In occurrence patterns of proper nouns feature, that kind of looks at capitalized sequences that have a curd.",
            "Other places in the document.",
            "And then some more sequencing features and sequencing features here with shapes."
        ],
        [
            "So that Cal Divergents isn't mentioned, qualifies the degree of disagreement between distributions.",
            "Anne.",
            "I suppose most people are familiar with this already, but just visually you can look at the the first 2 plots on the on the top on the right.",
            "Here are going to be given.",
            "Are going to.",
            "Be much more similar given.",
            "Higher or lower KL divergences than the 1st and the 3rd plots.",
            "Now the thing about this is so, so this is a token level score, so we get the tag distribution at the token level and we calculate the KL divergent for each token but were actually selecting for this task is documents.",
            "So we need a way of converting this into a document level measure.",
            "And we took a fairly simple approach of just taking the average token level KL divergent to get a document level score."
        ],
        [
            "Anne.",
            "So these are the results, but the bottom, the red line is the random learning curve based on the subsets that were created in the data by the organizers.",
            "Anne.",
            "At this plot, kind of reflects the fact that what we're doing is taking different seed sets for each of the active learning experiments.",
            "So you see, for the most part.",
            "We're improving.",
            "I mean this is.",
            "This is a fairly happy flat.",
            "We're doing pretty well.",
            "In terms of selecting informative examples and improving over baseline."
        ],
        [
            "Best average improvement over baseline is 1.3 points.",
            "F score an average percent.",
            "So I think this is the best.",
            "Absolute improvement in terms of score.",
            "I think we would have been.",
            "We would have been second in terms of average percent improvement in F score.",
            "After the Mercury system.",
            "And our absolute scores are sort of."
        ],
        [
            "So."
        ],
        [
            "We also ran experiments with a few other selection metrics.",
            "So you can imagine using maximum KL divergent.",
            "Which is just taking the maximum token level Cal Divergent together document level measure.",
            "You could also imagine using F compliment which is just based on the F square.",
            "This is the structural measure that I suggested before.",
            "So here you get a sort of structural comparison between analysis so.",
            "Instead of having a token level score, it's a sort of phrase level score, so you would get the F score for the top.",
            "Imagine that the two lines on the right, the two sort of pictures, are sentences.",
            "So in the top you have one on the left.",
            "There you have one aligned sentence with that also has the same tag, the A&A.",
            "So so for this you would get a sort of low F score because the next one, the A and the B you wouldn't get points for F score and see again doesn't align with anything.",
            "And then the bottom sentence you would get 0 for F score.",
            "So so I guess the point is just that this is looking at sort of phrase level structural kind of comparison.",
            "Instead of this token level kale divergent.",
            "And this is this is just based on the standard F score.",
            "It's imbalanced precision reco."
        ],
        [
            "So that's sort of what I would like to discuss is.",
            "Costing active learning so, So what we did for this task was we just had one cost measure to estimate the cost of active learning.",
            "We just looked.",
            "We were just plotting things against documents basically.",
            "But what we really want to do with active learning is compare the reduction in cost and reduction in error rate in terms of annotator effort and pay a real cost measure.",
            "So what we've done is, we've we've also plotted the results with several other sort of estimated cost measures sentence, which is actually equivalent to document.",
            "And number of tokens and number of entities to kind of look at the effect of the different cost metric."
        ],
        [
            "So this is actually not on the results.",
            "I'm about to show aren't aren't on the conference on the CFP corpus.",
            "This is actually on some related work we did with.",
            "Biomedical named entity recognition using the junior corpus.",
            "So this uses 12,500 sentence is we run like full simulated active learning experiments instead of this sort of setting with different seed sets.",
            "And see to 500 sentence is a pool of 10,000 sentences and a test set of 2017."
        ],
        [
            "OK, so this is the first plot.",
            "This is plotting against sentences or documents.",
            "And the cost and error aren't particularly important here.",
            "What I just want to show is that the so for the sentence cost metric, maximum KL divergences, the top curve, all of the selection metrics perform better than random, but maximum is the best here."
        ],
        [
            "But if we look at the token cost metric, average KL divergences performing better than.",
            "Maximum or F score.",
            "And then.",
            "Or sorry.",
            "Great if we look at the tokens, average scale divergences performing better."
        ],
        [
            "And then if we look at entities F scores performing better, so with these different cosmetics we get a different result.",
            "Basically, for each cast magic."
        ],
        [
            "So in the previous work, we sort of looked at at the effect that the cost metrics have on the types of documents that are selected, so so.",
            "The column on the left is the selection metric, then the next two columns are the number of tokens in the sentence or document and the number of entities.",
            "So random is sin in parentheses is standard deviation.",
            "So for random we get sentences that are about to 26, about 27 tokens in length, with about almost 2.8 entities, almost three entities.",
            "Then if we look at the F Councilman selection measure, we get roughly the same length sentences, but with.",
            "Fewer entities maximum.",
            "Cal Divergents selects considerably longer sentence is.",
            "And then average KL divergent selects.",
            "Sentences that are about the same as random but have more entities.",
            "So in the previous work we considered average KL divergent to be better because we get sentences that are about the same length but have more positive examples for entities.",
            "But again, I mean.",
            "None of these sort of cost metrics are sort of sufficient for determining the real annotation costs."
        ],
        [
            "Alright, so this just shows that there's the same kind of effect in the in the conference, the CFP data.",
            "It's not quite as pronounced because we're only doing one round of selective sampling, but.",
            "So this is this is with the document task metrics or sentence."
        ],
        [
            "And this is with the token cost metric and you can see that.",
            "To kill that Cal averages is the green lines there.",
            "And when you look at the token cost metric, it's somewhat clear that Cal average is performing better than the other cosmetics."
        ],
        [
            "Again with document.",
            "It's really much more difficult to see.",
            "So."
        ],
        [
            "Discussion I guess, just kind of the point here is that we believe it's difficult to do comparison between metrics.",
            "Because this document unit cost metric, another sort of text based costs, cost metrics are not necessarily realistic.",
            "Estimates of real cost, so this is just a kind of suggestion for future evaluation.",
            "Be really cool to kind of corpus with a measure of real annotation cost at some level in the corpus, so so we can do plots that are.",
            "Against real costs.",
            "Of course this is more work for annotation."
        ],
        [
            "OK, so just a bit of sort of error analysis.",
            "We made confusion matrices at the token level.",
            "We removed the beginning an inside tag so we could just look at.",
            "At.",
            "At it's just comparing if it's labeled as an entity, one of the entity types or being not an entity type.",
            "In the actual annotation that we were using, we convert that to the first token of a certain entity will have be tag and the and the ones following it will have tags saying that it's beginning or inside.",
            "But we stripped this to do the confusion matrix, so it's just looking at this sort of token level."
        ],
        [
            "Um?",
            "So the main mass is on the diagonals, which is good.",
            "And for the most part, so on the top we have.",
            "Random sorry, go back."
        ],
        [
            "So we sort of comparing confusion matrices for random baseline.",
            "This is with 320 documents and the compared comperable selective sampling.",
            "Round, which is with a seed of 280 documents plus 40 selectively sample documents."
        ],
        [
            "So.",
            "In terms of correct classifications, we do tend to get most mostly improvements going from the selectively sampled.",
            "Going from random to selective.",
            "Especially so for the outside tag improves and these are all quite small improvements because this is only 40 documents.",
            "It's not.",
            "It's not much of a difference, so it's kind of hard to make these comparisons without doing, say, more rounds of active learning and doing, doing and getting more data.",
            "But this is just to get a sort of rough idea.",
            "I mean, I mean, a lot of these may not be significant, but just to get a rough idea.",
            "Anyway, for the most part along the diagonal, things improve a little bit.",
            "There's a few places.",
            "Conference home page.",
            "The last one in the bottom right corner.",
            "For instance, actually goes down, but as we saw from Neil slides, this seems to be the hardest single category."
        ],
        [
            "An OK, this is just to show that most of the errors are actually on.",
            "Boundary problems.",
            "They're not.",
            "We're not confusing entity types.",
            "Most of the errors are identifying whether there's an entity or not in a certain position."
        ],
        [
            "And then this is just so for the most part.",
            "But again, there's there's.",
            "There's not sort of errors between entity types, But there's this one place where conference home page is an workshop.",
            "Home pages seem to be.",
            "Highly confusable so.",
            "In this case, classifying conference home pages with Shop homepage has actually gone up for randoms to selective, but the opposite has the error rate has gone down.",
            "So this is kind of.",
            "So there's a risk with this type of active learning that we're doing that.",
            "If you're choosing things that there's uncertainty about or disagreement about, you might be choosing things for your annotators to annotate and wasting time on things that your features or your classifier cannot actually learn, and we think that this might actually be part of the case here with.",
            "The difference between conference and workshop and there are going to be some contextual features that will help us, but that's not true in all cases.",
            "We could be selecting examples that have no contextual information.",
            "That's going to help.",
            "Just a small.",
            "Cardio?"
        ],
        [
            "So I presented our approach which is using KL divergent to measure disagreement among committee of maximum entropy Markov model classifiers.",
            "Average improvement was 1.3.",
            "Points F score in about 2.1% at school and then just a couple of suggestions that maybe we can talk about later.",
            "It would be nice to put some sort of.",
            "Real cost information in the data so we can plot against a real cost measure.",
            "Better than these estimated cosmetics.",
            "And it would also be.",
            "We already have like sort of a whole bunch of you mentioned that some people were submitting as many as 50 different runs for.",
            "So I mean, there's already this task is really complex, but it would also be nice to sort of look at full simulated experiments with full learning curves to sort of be able to tease out some effects a bit more.",
            "That's it, thank you."
        ],
        [
            "Question.",
            "Sorry.",
            "Oh wait, sorry, so these are features like.",
            "So you take this.",
            "A word and say like if it's alphanumeric and in the first position and capitalized then you should put maybe a capital X and then if the rest of it is sorry alphabetically the first position and capitalize your account without the rest of it is.",
            "Lower case alphabetic then you just put a single lowercase X so it's something that kind of tries to generalize both the shapes of words.",
            "We didn't actually do experiments with or without it, but it generally does tend to.",
            "It helps you to generalize in cases where maybe you have sparse data about word features.",
            "So in general it doesn't prove I don't.",
            "I don't have any specific answers.",
            "Results Fisher right.",
            "Yes, again.",
            "It's sort of an intuition that we have from and from the coach Richard.",
            "That is preferable to have independent features.",
            "Well, right?",
            "I mean, it obviously we don't have or what we have is not completely independent because I mean, for instance we have.",
            "Words in one feature set in Word, shapes and other features.",
            "But we're we're making a compromise between having roughly equal performance between features as well.",
            "So I mean well in in the coaching literature.",
            "Dependencies been found to be fairly important.",
            "But then there's other work by Miles Osborne.",
            "Jason offers that is at 12 inches.",
            "It has found that the it's not as important, actively.",
            "Location for example independence.",
            "The main difference.",
            "System in two different modes, so there was one mode where we were selecting the examples from the pool and this was when we went when it features, but but then when we run the when we run the system on the test set, we actually are combined with higher feature set.",
            "So we sort of like Ruby.",
            "So it's not necessarily a maybe not necessary that you have the best performance ever while you select examples.",
            "Basically just want you want to identify the difficult ones.",
            "We should then absolute training set.",
            "Pictures such that.",
            "If.",
            "Based on this for features is less than bundle.",
            "I mean.",
            "They're not using all the features, but they're not really poor either.",
            "Anything I mean OK. A few.",
            "Using only the subset.",
            "Officer 68.",
            "Question.",
            "Shakespeare.",
            "I can't say for sure.",
            "No it was.",
            "It was the yeah, it was the earth dog.",
            "What you described was a little different than the actual shape feature, yeah, but it was the shape feature that came with the gate annotation.",
            "Yeah.",
            "Location.",
            "Right, yeah, yeah.",
            "There are lots of operations that you can do on the shape of the world.",
            "We said not to do anything else except use the information that we gave.",
            "It is capitalized or lowercase because otherwise you can do a number of other things of that, yeah.",
            "OK so thanks for this."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll present the approach that we had.",
                    "label": 0
                },
                {
                    "sent": "We only participated, but Neil said in the active learning task actually.",
                    "label": 0
                },
                {
                    "sent": "So this would be just sort of an active learning talk.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Marcus Becker who is here, and Claire, Grover, and Ewan Klein in Edinburgh.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first sort of introduce our approach and talk a bit about or present the results in a slightly different way than what Neil did.",
                    "label": 0
                },
                {
                    "sent": "And then actually most of the talk will be a bit about discussion of.",
                    "label": 0
                },
                {
                    "sent": "Of look at a couple of different selection metrics in the one that I present 1st and then talk about.",
                    "label": 1
                },
                {
                    "sent": "Sort of how approaches to active learning interact with the cost metric that you use.",
                    "label": 1
                },
                {
                    "sent": "And then just a little bit of air analysis.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's sort of.",
                    "label": 0
                },
                {
                    "sent": "In the literature, sort of two main approaches to active learning, uncertainty sampling and queried by committee.",
                    "label": 1
                },
                {
                    "sent": "In the first uncertainty sampling, the usefulness is estimated by the uncertainty of the single learner.",
                    "label": 0
                },
                {
                    "sent": "So use just the confidence from the learner to choose which examples the classifier is least confident about.",
                    "label": 0
                },
                {
                    "sent": "Or you can look at the entropy and label examples for which the output distribution from the classifier has the highest entropy.",
                    "label": 1
                },
                {
                    "sent": "This will be the flatter stitch distribution.",
                    "label": 0
                },
                {
                    "sent": "We follow the second approach, queried by committee, where the usefulness of an example is estimated by the disagreement among a committee of learners.",
                    "label": 1
                },
                {
                    "sent": "And again, there are a number of ways to go about this.",
                    "label": 0
                },
                {
                    "sent": "You look at vote entropy.",
                    "label": 0
                },
                {
                    "sent": "The disagreement between the winners for different classifiers it KL divergent switch looks at the difference between the class output distribution with different classifiers or F score, which I'll talk about later, which is sort of more structural comparison of the output to the different classifiers.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's a number of ways to create a committee.",
                    "label": 0
                },
                {
                    "sent": "People have used begging, randomly perturbing event counts, and Nitesh Chawla talk.",
                    "label": 1
                },
                {
                    "sent": "Done on Monday about random feature subspaces.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The problem with these I mean these are these are good because they are automatic, but we're not insured diversity and in our setting where we want to look at the disagreement between the classifiers.",
                    "label": 0
                },
                {
                    "sent": "We opted to go for a hand handcrafted feature split, actually because we can ensure.",
                    "label": 1
                },
                {
                    "sent": "Diversity.",
                    "label": 0
                },
                {
                    "sent": "Between the feature sets that we use and can ensure some level of independence between the fields.",
                    "label": 1
                },
                {
                    "sent": "It would be interesting though, to look at perhaps like random feature subspaces and see how it compares to the handcrafted feature splits, but we haven't done that just now.",
                    "label": 0
                },
                {
                    "sent": "So, and we use a maximum entropy Markov model classifier, which is one of the classifiers.",
                    "label": 0
                },
                {
                    "sent": "I think that Chris from the Stanford Group will talk about later.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the feature split that we used.",
                    "label": 1
                },
                {
                    "sent": "So in the first feature set, basically what we have are features of the word, the shape of the word and the position in the document of the token.",
                    "label": 0
                },
                {
                    "sent": "And then these are combined with other sort of sequencing features, so previously predicted.",
                    "label": 0
                },
                {
                    "sent": "Entity tags.",
                    "label": 1
                },
                {
                    "sent": "And then the second feature set looks at parts of speech.",
                    "label": 0
                },
                {
                    "sent": "In occurrence patterns of proper nouns feature, that kind of looks at capitalized sequences that have a curd.",
                    "label": 1
                },
                {
                    "sent": "Other places in the document.",
                    "label": 0
                },
                {
                    "sent": "And then some more sequencing features and sequencing features here with shapes.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that Cal Divergents isn't mentioned, qualifies the degree of disagreement between distributions.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I suppose most people are familiar with this already, but just visually you can look at the the first 2 plots on the on the top on the right.",
                    "label": 0
                },
                {
                    "sent": "Here are going to be given.",
                    "label": 0
                },
                {
                    "sent": "Are going to.",
                    "label": 0
                },
                {
                    "sent": "Be much more similar given.",
                    "label": 0
                },
                {
                    "sent": "Higher or lower KL divergences than the 1st and the 3rd plots.",
                    "label": 0
                },
                {
                    "sent": "Now the thing about this is so, so this is a token level score, so we get the tag distribution at the token level and we calculate the KL divergent for each token but were actually selecting for this task is documents.",
                    "label": 0
                },
                {
                    "sent": "So we need a way of converting this into a document level measure.",
                    "label": 0
                },
                {
                    "sent": "And we took a fairly simple approach of just taking the average token level KL divergent to get a document level score.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So these are the results, but the bottom, the red line is the random learning curve based on the subsets that were created in the data by the organizers.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "At this plot, kind of reflects the fact that what we're doing is taking different seed sets for each of the active learning experiments.",
                    "label": 0
                },
                {
                    "sent": "So you see, for the most part.",
                    "label": 0
                },
                {
                    "sent": "We're improving.",
                    "label": 0
                },
                {
                    "sent": "I mean this is.",
                    "label": 0
                },
                {
                    "sent": "This is a fairly happy flat.",
                    "label": 0
                },
                {
                    "sent": "We're doing pretty well.",
                    "label": 0
                },
                {
                    "sent": "In terms of selecting informative examples and improving over baseline.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Best average improvement over baseline is 1.3 points.",
                    "label": 0
                },
                {
                    "sent": "F score an average percent.",
                    "label": 0
                },
                {
                    "sent": "So I think this is the best.",
                    "label": 0
                },
                {
                    "sent": "Absolute improvement in terms of score.",
                    "label": 0
                },
                {
                    "sent": "I think we would have been.",
                    "label": 0
                },
                {
                    "sent": "We would have been second in terms of average percent improvement in F score.",
                    "label": 0
                },
                {
                    "sent": "After the Mercury system.",
                    "label": 0
                },
                {
                    "sent": "And our absolute scores are sort of.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also ran experiments with a few other selection metrics.",
                    "label": 1
                },
                {
                    "sent": "So you can imagine using maximum KL divergent.",
                    "label": 0
                },
                {
                    "sent": "Which is just taking the maximum token level Cal Divergent together document level measure.",
                    "label": 0
                },
                {
                    "sent": "You could also imagine using F compliment which is just based on the F square.",
                    "label": 0
                },
                {
                    "sent": "This is the structural measure that I suggested before.",
                    "label": 1
                },
                {
                    "sent": "So here you get a sort of structural comparison between analysis so.",
                    "label": 0
                },
                {
                    "sent": "Instead of having a token level score, it's a sort of phrase level score, so you would get the F score for the top.",
                    "label": 0
                },
                {
                    "sent": "Imagine that the two lines on the right, the two sort of pictures, are sentences.",
                    "label": 0
                },
                {
                    "sent": "So in the top you have one on the left.",
                    "label": 0
                },
                {
                    "sent": "There you have one aligned sentence with that also has the same tag, the A&A.",
                    "label": 0
                },
                {
                    "sent": "So so for this you would get a sort of low F score because the next one, the A and the B you wouldn't get points for F score and see again doesn't align with anything.",
                    "label": 0
                },
                {
                    "sent": "And then the bottom sentence you would get 0 for F score.",
                    "label": 0
                },
                {
                    "sent": "So so I guess the point is just that this is looking at sort of phrase level structural kind of comparison.",
                    "label": 0
                },
                {
                    "sent": "Instead of this token level kale divergent.",
                    "label": 0
                },
                {
                    "sent": "And this is this is just based on the standard F score.",
                    "label": 0
                },
                {
                    "sent": "It's imbalanced precision reco.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's sort of what I would like to discuss is.",
                    "label": 0
                },
                {
                    "sent": "Costing active learning so, So what we did for this task was we just had one cost measure to estimate the cost of active learning.",
                    "label": 0
                },
                {
                    "sent": "We just looked.",
                    "label": 0
                },
                {
                    "sent": "We were just plotting things against documents basically.",
                    "label": 0
                },
                {
                    "sent": "But what we really want to do with active learning is compare the reduction in cost and reduction in error rate in terms of annotator effort and pay a real cost measure.",
                    "label": 1
                },
                {
                    "sent": "So what we've done is, we've we've also plotted the results with several other sort of estimated cost measures sentence, which is actually equivalent to document.",
                    "label": 0
                },
                {
                    "sent": "And number of tokens and number of entities to kind of look at the effect of the different cost metric.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is actually not on the results.",
                    "label": 0
                },
                {
                    "sent": "I'm about to show aren't aren't on the conference on the CFP corpus.",
                    "label": 0
                },
                {
                    "sent": "This is actually on some related work we did with.",
                    "label": 0
                },
                {
                    "sent": "Biomedical named entity recognition using the junior corpus.",
                    "label": 0
                },
                {
                    "sent": "So this uses 12,500 sentence is we run like full simulated active learning experiments instead of this sort of setting with different seed sets.",
                    "label": 0
                },
                {
                    "sent": "And see to 500 sentence is a pool of 10,000 sentences and a test set of 2017.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is the first plot.",
                    "label": 0
                },
                {
                    "sent": "This is plotting against sentences or documents.",
                    "label": 0
                },
                {
                    "sent": "And the cost and error aren't particularly important here.",
                    "label": 0
                },
                {
                    "sent": "What I just want to show is that the so for the sentence cost metric, maximum KL divergences, the top curve, all of the selection metrics perform better than random, but maximum is the best here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if we look at the token cost metric, average KL divergences performing better than.",
                    "label": 0
                },
                {
                    "sent": "Maximum or F score.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Or sorry.",
                    "label": 0
                },
                {
                    "sent": "Great if we look at the tokens, average scale divergences performing better.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then if we look at entities F scores performing better, so with these different cosmetics we get a different result.",
                    "label": 0
                },
                {
                    "sent": "Basically, for each cast magic.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the previous work, we sort of looked at at the effect that the cost metrics have on the types of documents that are selected, so so.",
                    "label": 0
                },
                {
                    "sent": "The column on the left is the selection metric, then the next two columns are the number of tokens in the sentence or document and the number of entities.",
                    "label": 1
                },
                {
                    "sent": "So random is sin in parentheses is standard deviation.",
                    "label": 0
                },
                {
                    "sent": "So for random we get sentences that are about to 26, about 27 tokens in length, with about almost 2.8 entities, almost three entities.",
                    "label": 0
                },
                {
                    "sent": "Then if we look at the F Councilman selection measure, we get roughly the same length sentences, but with.",
                    "label": 0
                },
                {
                    "sent": "Fewer entities maximum.",
                    "label": 0
                },
                {
                    "sent": "Cal Divergents selects considerably longer sentence is.",
                    "label": 0
                },
                {
                    "sent": "And then average KL divergent selects.",
                    "label": 0
                },
                {
                    "sent": "Sentences that are about the same as random but have more entities.",
                    "label": 0
                },
                {
                    "sent": "So in the previous work we considered average KL divergent to be better because we get sentences that are about the same length but have more positive examples for entities.",
                    "label": 1
                },
                {
                    "sent": "But again, I mean.",
                    "label": 0
                },
                {
                    "sent": "None of these sort of cost metrics are sort of sufficient for determining the real annotation costs.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so this just shows that there's the same kind of effect in the in the conference, the CFP data.",
                    "label": 0
                },
                {
                    "sent": "It's not quite as pronounced because we're only doing one round of selective sampling, but.",
                    "label": 1
                },
                {
                    "sent": "So this is this is with the document task metrics or sentence.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is with the token cost metric and you can see that.",
                    "label": 1
                },
                {
                    "sent": "To kill that Cal averages is the green lines there.",
                    "label": 0
                },
                {
                    "sent": "And when you look at the token cost metric, it's somewhat clear that Cal average is performing better than the other cosmetics.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again with document.",
                    "label": 0
                },
                {
                    "sent": "It's really much more difficult to see.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Discussion I guess, just kind of the point here is that we believe it's difficult to do comparison between metrics.",
                    "label": 1
                },
                {
                    "sent": "Because this document unit cost metric, another sort of text based costs, cost metrics are not necessarily realistic.",
                    "label": 1
                },
                {
                    "sent": "Estimates of real cost, so this is just a kind of suggestion for future evaluation.",
                    "label": 0
                },
                {
                    "sent": "Be really cool to kind of corpus with a measure of real annotation cost at some level in the corpus, so so we can do plots that are.",
                    "label": 1
                },
                {
                    "sent": "Against real costs.",
                    "label": 0
                },
                {
                    "sent": "Of course this is more work for annotation.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just a bit of sort of error analysis.",
                    "label": 0
                },
                {
                    "sent": "We made confusion matrices at the token level.",
                    "label": 0
                },
                {
                    "sent": "We removed the beginning an inside tag so we could just look at.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "At it's just comparing if it's labeled as an entity, one of the entity types or being not an entity type.",
                    "label": 0
                },
                {
                    "sent": "In the actual annotation that we were using, we convert that to the first token of a certain entity will have be tag and the and the ones following it will have tags saying that it's beginning or inside.",
                    "label": 0
                },
                {
                    "sent": "But we stripped this to do the confusion matrix, so it's just looking at this sort of token level.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the main mass is on the diagonals, which is good.",
                    "label": 0
                },
                {
                    "sent": "And for the most part, so on the top we have.",
                    "label": 0
                },
                {
                    "sent": "Random sorry, go back.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we sort of comparing confusion matrices for random baseline.",
                    "label": 1
                },
                {
                    "sent": "This is with 320 documents and the compared comperable selective sampling.",
                    "label": 1
                },
                {
                    "sent": "Round, which is with a seed of 280 documents plus 40 selectively sample documents.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In terms of correct classifications, we do tend to get most mostly improvements going from the selectively sampled.",
                    "label": 0
                },
                {
                    "sent": "Going from random to selective.",
                    "label": 0
                },
                {
                    "sent": "Especially so for the outside tag improves and these are all quite small improvements because this is only 40 documents.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's not much of a difference, so it's kind of hard to make these comparisons without doing, say, more rounds of active learning and doing, doing and getting more data.",
                    "label": 0
                },
                {
                    "sent": "But this is just to get a sort of rough idea.",
                    "label": 0
                },
                {
                    "sent": "I mean, I mean, a lot of these may not be significant, but just to get a rough idea.",
                    "label": 0
                },
                {
                    "sent": "Anyway, for the most part along the diagonal, things improve a little bit.",
                    "label": 0
                },
                {
                    "sent": "There's a few places.",
                    "label": 0
                },
                {
                    "sent": "Conference home page.",
                    "label": 0
                },
                {
                    "sent": "The last one in the bottom right corner.",
                    "label": 0
                },
                {
                    "sent": "For instance, actually goes down, but as we saw from Neil slides, this seems to be the hardest single category.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An OK, this is just to show that most of the errors are actually on.",
                    "label": 0
                },
                {
                    "sent": "Boundary problems.",
                    "label": 0
                },
                {
                    "sent": "They're not.",
                    "label": 0
                },
                {
                    "sent": "We're not confusing entity types.",
                    "label": 0
                },
                {
                    "sent": "Most of the errors are identifying whether there's an entity or not in a certain position.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then this is just so for the most part.",
                    "label": 0
                },
                {
                    "sent": "But again, there's there's.",
                    "label": 0
                },
                {
                    "sent": "There's not sort of errors between entity types, But there's this one place where conference home page is an workshop.",
                    "label": 0
                },
                {
                    "sent": "Home pages seem to be.",
                    "label": 0
                },
                {
                    "sent": "Highly confusable so.",
                    "label": 0
                },
                {
                    "sent": "In this case, classifying conference home pages with Shop homepage has actually gone up for randoms to selective, but the opposite has the error rate has gone down.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of.",
                    "label": 0
                },
                {
                    "sent": "So there's a risk with this type of active learning that we're doing that.",
                    "label": 0
                },
                {
                    "sent": "If you're choosing things that there's uncertainty about or disagreement about, you might be choosing things for your annotators to annotate and wasting time on things that your features or your classifier cannot actually learn, and we think that this might actually be part of the case here with.",
                    "label": 0
                },
                {
                    "sent": "The difference between conference and workshop and there are going to be some contextual features that will help us, but that's not true in all cases.",
                    "label": 0
                },
                {
                    "sent": "We could be selecting examples that have no contextual information.",
                    "label": 0
                },
                {
                    "sent": "That's going to help.",
                    "label": 0
                },
                {
                    "sent": "Just a small.",
                    "label": 0
                },
                {
                    "sent": "Cardio?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I presented our approach which is using KL divergent to measure disagreement among committee of maximum entropy Markov model classifiers.",
                    "label": 1
                },
                {
                    "sent": "Average improvement was 1.3.",
                    "label": 0
                },
                {
                    "sent": "Points F score in about 2.1% at school and then just a couple of suggestions that maybe we can talk about later.",
                    "label": 0
                },
                {
                    "sent": "It would be nice to put some sort of.",
                    "label": 1
                },
                {
                    "sent": "Real cost information in the data so we can plot against a real cost measure.",
                    "label": 0
                },
                {
                    "sent": "Better than these estimated cosmetics.",
                    "label": 0
                },
                {
                    "sent": "And it would also be.",
                    "label": 0
                },
                {
                    "sent": "We already have like sort of a whole bunch of you mentioned that some people were submitting as many as 50 different runs for.",
                    "label": 0
                },
                {
                    "sent": "So I mean, there's already this task is really complex, but it would also be nice to sort of look at full simulated experiments with full learning curves to sort of be able to tease out some effects a bit more.",
                    "label": 0
                },
                {
                    "sent": "That's it, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Oh wait, sorry, so these are features like.",
                    "label": 0
                },
                {
                    "sent": "So you take this.",
                    "label": 0
                },
                {
                    "sent": "A word and say like if it's alphanumeric and in the first position and capitalized then you should put maybe a capital X and then if the rest of it is sorry alphabetically the first position and capitalize your account without the rest of it is.",
                    "label": 0
                },
                {
                    "sent": "Lower case alphabetic then you just put a single lowercase X so it's something that kind of tries to generalize both the shapes of words.",
                    "label": 0
                },
                {
                    "sent": "We didn't actually do experiments with or without it, but it generally does tend to.",
                    "label": 0
                },
                {
                    "sent": "It helps you to generalize in cases where maybe you have sparse data about word features.",
                    "label": 0
                },
                {
                    "sent": "So in general it doesn't prove I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't have any specific answers.",
                    "label": 0
                },
                {
                    "sent": "Results Fisher right.",
                    "label": 0
                },
                {
                    "sent": "Yes, again.",
                    "label": 0
                },
                {
                    "sent": "It's sort of an intuition that we have from and from the coach Richard.",
                    "label": 0
                },
                {
                    "sent": "That is preferable to have independent features.",
                    "label": 0
                },
                {
                    "sent": "Well, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, it obviously we don't have or what we have is not completely independent because I mean, for instance we have.",
                    "label": 0
                },
                {
                    "sent": "Words in one feature set in Word, shapes and other features.",
                    "label": 0
                },
                {
                    "sent": "But we're we're making a compromise between having roughly equal performance between features as well.",
                    "label": 0
                },
                {
                    "sent": "So I mean well in in the coaching literature.",
                    "label": 0
                },
                {
                    "sent": "Dependencies been found to be fairly important.",
                    "label": 0
                },
                {
                    "sent": "But then there's other work by Miles Osborne.",
                    "label": 0
                },
                {
                    "sent": "Jason offers that is at 12 inches.",
                    "label": 0
                },
                {
                    "sent": "It has found that the it's not as important, actively.",
                    "label": 0
                },
                {
                    "sent": "Location for example independence.",
                    "label": 0
                },
                {
                    "sent": "The main difference.",
                    "label": 0
                },
                {
                    "sent": "System in two different modes, so there was one mode where we were selecting the examples from the pool and this was when we went when it features, but but then when we run the when we run the system on the test set, we actually are combined with higher feature set.",
                    "label": 0
                },
                {
                    "sent": "So we sort of like Ruby.",
                    "label": 0
                },
                {
                    "sent": "So it's not necessarily a maybe not necessary that you have the best performance ever while you select examples.",
                    "label": 0
                },
                {
                    "sent": "Basically just want you want to identify the difficult ones.",
                    "label": 0
                },
                {
                    "sent": "We should then absolute training set.",
                    "label": 0
                },
                {
                    "sent": "Pictures such that.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "Based on this for features is less than bundle.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "They're not using all the features, but they're not really poor either.",
                    "label": 0
                },
                {
                    "sent": "Anything I mean OK. A few.",
                    "label": 0
                },
                {
                    "sent": "Using only the subset.",
                    "label": 0
                },
                {
                    "sent": "Officer 68.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Shakespeare.",
                    "label": 0
                },
                {
                    "sent": "I can't say for sure.",
                    "label": 0
                },
                {
                    "sent": "No it was.",
                    "label": 0
                },
                {
                    "sent": "It was the yeah, it was the earth dog.",
                    "label": 0
                },
                {
                    "sent": "What you described was a little different than the actual shape feature, yeah, but it was the shape feature that came with the gate annotation.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Location.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "There are lots of operations that you can do on the shape of the world.",
                    "label": 0
                },
                {
                    "sent": "We said not to do anything else except use the information that we gave.",
                    "label": 0
                },
                {
                    "sent": "It is capitalized or lowercase because otherwise you can do a number of other things of that, yeah.",
                    "label": 0
                },
                {
                    "sent": "OK so thanks for this.",
                    "label": 0
                }
            ]
        }
    }
}