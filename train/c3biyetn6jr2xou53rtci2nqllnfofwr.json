{
    "id": "c3biyetn6jr2xou53rtci2nqllnfofwr",
    "title": "Trading off Mistakes and Don't-Know Predictions",
    "info": {
        "author": [
            "Avrim Blum, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/nips2010_blum_tom/",
    "segmentation": [
        [
            "Thank you.",
            "So first, like to say that actually most of this work is due to my coauthors, I mean, and Morteza, but they're unfortunately not able to make it here due to visa issues.",
            "So this works about."
        ],
        [
            "Following theoretical question non statistical question so and that's what happens if you allow I don't know is in a standard online learning model.",
            "So what do I mean by online learning model?",
            "I mean the world presents some example, for instance an email message asking you to do or not do certain important things.",
            "Or maybe it's something telling you just want a bunch of money and you have to answer.",
            "Is this spam or not?",
            "And then you're given the correct answer but will allow the algorithm to also say I don't know.",
            "And the question is, if we view saying I don't know is being a lot more lot better than making a mistake, how can you trade these off?",
            "And it turns out there's 2 interesting ends of the spectrum, so one is if your domain likes a medical diagnosis where mistakes are very costly.",
            "What happens then?",
            "If you think of yourself as being allowed, just maybe just a couple of mistakes in the whole process or on the other hand, what if there's maybe a constant factor relationship between them and would like to convert as many mistakes as possible into only a constant factor?",
            "More don't knows, so we'll look at this both information theoretically and algorithmically.",
            "So just to get a feel for this information, theoretically there's a close connection of this to the classic eggdrop."
        ],
        [
            "Puzzle, so the egg dropping piles.",
            "You're trying to figure out the highest floor you can drop some pretty strong egg without it breaking, so then florist total.",
            "So if you have a lot of eggs, you can just do binary search.",
            "It takes you log base two event drops.",
            "If you only have 1 egg then you have to do linear search up from the bottom.",
            "You can see that 'cause if you ever skip a floor and then the thing breaks, you never know if it would have broken the one you skipped.",
            "So the classic puzzles with about 2 eggs will give you 1 second to think about it.",
            "The answer is it Six 2 sqrt N eggs.",
            "2 squared event drops with K exits.",
            "About enter.",
            "The 1 / K and it turns out you can use this to show that if you allow K -- 1 mistakes, you can learn any function.",
            "Some class with roughly the size of the class to the 1 / K."
        ],
        [
            "Nose and so this roughly actually corresponds to the problem of learning an initial subinterval.",
            "The algorithmic results we have are there for learning the class of disjunctions.",
            "It's a classic result that end mistakes are necessary and sufficient.",
            "In the worst case you can reduce this to an over 2 mistakes in a relatively simple way, adding at most N don't knows more complicated getting over three mistakes, adding 1.5 times then don't knows.",
            "For linear separators you can use a random walk procedure.",
            "To estimate the volume of the version space and get an ice bound that simulates in some ways this sort of information theoretic bound the bounds here little more complex if you want to see them, you'll have to see the poster."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So first, like to say that actually most of this work is due to my coauthors, I mean, and Morteza, but they're unfortunately not able to make it here due to visa issues.",
                    "label": 0
                },
                {
                    "sent": "So this works about.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Following theoretical question non statistical question so and that's what happens if you allow I don't know is in a standard online learning model.",
                    "label": 1
                },
                {
                    "sent": "So what do I mean by online learning model?",
                    "label": 0
                },
                {
                    "sent": "I mean the world presents some example, for instance an email message asking you to do or not do certain important things.",
                    "label": 0
                },
                {
                    "sent": "Or maybe it's something telling you just want a bunch of money and you have to answer.",
                    "label": 1
                },
                {
                    "sent": "Is this spam or not?",
                    "label": 0
                },
                {
                    "sent": "And then you're given the correct answer but will allow the algorithm to also say I don't know.",
                    "label": 0
                },
                {
                    "sent": "And the question is, if we view saying I don't know is being a lot more lot better than making a mistake, how can you trade these off?",
                    "label": 1
                },
                {
                    "sent": "And it turns out there's 2 interesting ends of the spectrum, so one is if your domain likes a medical diagnosis where mistakes are very costly.",
                    "label": 1
                },
                {
                    "sent": "What happens then?",
                    "label": 0
                },
                {
                    "sent": "If you think of yourself as being allowed, just maybe just a couple of mistakes in the whole process or on the other hand, what if there's maybe a constant factor relationship between them and would like to convert as many mistakes as possible into only a constant factor?",
                    "label": 0
                },
                {
                    "sent": "More don't knows, so we'll look at this both information theoretically and algorithmically.",
                    "label": 0
                },
                {
                    "sent": "So just to get a feel for this information, theoretically there's a close connection of this to the classic eggdrop.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Puzzle, so the egg dropping piles.",
                    "label": 0
                },
                {
                    "sent": "You're trying to figure out the highest floor you can drop some pretty strong egg without it breaking, so then florist total.",
                    "label": 1
                },
                {
                    "sent": "So if you have a lot of eggs, you can just do binary search.",
                    "label": 1
                },
                {
                    "sent": "It takes you log base two event drops.",
                    "label": 0
                },
                {
                    "sent": "If you only have 1 egg then you have to do linear search up from the bottom.",
                    "label": 0
                },
                {
                    "sent": "You can see that 'cause if you ever skip a floor and then the thing breaks, you never know if it would have broken the one you skipped.",
                    "label": 0
                },
                {
                    "sent": "So the classic puzzles with about 2 eggs will give you 1 second to think about it.",
                    "label": 0
                },
                {
                    "sent": "The answer is it Six 2 sqrt N eggs.",
                    "label": 0
                },
                {
                    "sent": "2 squared event drops with K exits.",
                    "label": 0
                },
                {
                    "sent": "About enter.",
                    "label": 1
                },
                {
                    "sent": "The 1 / K and it turns out you can use this to show that if you allow K -- 1 mistakes, you can learn any function.",
                    "label": 0
                },
                {
                    "sent": "Some class with roughly the size of the class to the 1 / K.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nose and so this roughly actually corresponds to the problem of learning an initial subinterval.",
                    "label": 0
                },
                {
                    "sent": "The algorithmic results we have are there for learning the class of disjunctions.",
                    "label": 1
                },
                {
                    "sent": "It's a classic result that end mistakes are necessary and sufficient.",
                    "label": 1
                },
                {
                    "sent": "In the worst case you can reduce this to an over 2 mistakes in a relatively simple way, adding at most N don't knows more complicated getting over three mistakes, adding 1.5 times then don't knows.",
                    "label": 1
                },
                {
                    "sent": "For linear separators you can use a random walk procedure.",
                    "label": 0
                },
                {
                    "sent": "To estimate the volume of the version space and get an ice bound that simulates in some ways this sort of information theoretic bound the bounds here little more complex if you want to see them, you'll have to see the poster.",
                    "label": 1
                }
            ]
        }
    }
}