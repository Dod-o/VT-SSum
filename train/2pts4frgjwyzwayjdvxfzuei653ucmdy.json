{
    "id": "2pts4frgjwyzwayjdvxfzuei653ucmdy",
    "title": "Adaptive p-Posterior Mixture-Model Kernels for Multiple Instance Learning",
    "info": {
        "author": [
            "Hua-Yan Wang, National Laboratory On Machine Perception, Peking University"
        ],
        "published": "Aug. 7, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/icml08_wang_apm/",
    "segmentation": [
        [
            "On now I will give a brief introduction to our work on adaptive PMM kernels for multiple instance learning could also solve this work.",
            "Include professor Young Chung from Hong Kong, UST.",
            "Yeah case.",
            "And Professor John Bean from Peking University."
        ],
        [
            "OK, here is our story line.",
            "First I will give a brief introduction to the concept of multiple instance learning.",
            "Then we'll go through some different applications of multiple instance learning and we'll see how they differ from each other.",
            "Based on these analysis, we will introduce the generalized framework of multiple instance learning, and we'll see how how it can accommodate these differences.",
            "Finally, I will give a simple but effective solution to the generalized multiple instance learning problem we proposed and it is named DPM kernels."
        ],
        [
            "OK, now we'll start from."
        ],
        [
            "First part.",
            "Are the original motivation of multiple instance learning, as we all know, is the application of drug activity prediction.",
            "In this application we have bags which are a certain kind of drug molecules which can adopt A number of shapes.",
            "So the bag is a molecule and instances are different shapes that it can adopt.",
            "So a shape that binds well to the target protein which is active is labeled as positive.",
            "And the shape that not is labeled as negative.",
            "OK, so in wet lab experiments this molecule will be operated active 'cause it has one shape that is active.",
            "OK, but wet lab experiments as we know cost a lot of time and money.",
            "So so with multiple instance learning, our goal is to predict with confidence the activity of molecules.",
            "Enjoy lab which means just by computers without experiments.",
            "Which is good because it saves money for biologists and makes money for for us."
        ],
        [
            "OK, so so setting off multiple instance learning goes like this.",
            "We have bags of instances.",
            "And these instances are labeled as either positive or negative.",
            "And we have a positive bag if and only at least one of its instances positive.",
            "OK, so this one is positive.",
            "This is negative and this one is."
        ],
        [
            "Positive.",
            "So in multiple instance learning, the learner just cannot see the labels of the instances, it can only see the labeled bags with unlabeled instances in them.",
            "So and learner is required to predict labels for previously unseen bags.",
            "OK, so so this is pretty much setting off multiple instances."
        ],
        [
            "And we will see.",
            "For instance, learning has been adapted to different application domains and we will go through these demands and we'll see how they do."
        ],
        [
            "Far from each other.",
            "OK, the first one is of course drug activity prediction, which is all original motivation of of this problem.",
            "In this application of positive instances, definite evidence for positive bag, which means we just one positive instance for labeling the back as positive."
        ],
        [
            "OK, so on the other hand we multiple instance learning has been used in document classification, in which the bags are documents regarded as bags of words, phrases, sentences or paragraphs that depends.",
            "Our representation of documents.",
            "OK, so so suppose we have a problem of distinguish economix documents from from other documents.",
            "So we have the documents on Economix labeled as positive and other documents labeled as negative for labeling document as positively.",
            "So in this in this bag we have we have two positive instance in here they are market and economy.",
            "OK so in this.",
            "In this application, the positive instances are we can we can say they are strong.",
            "Evidence is for for positive back 'cause we need several such evidence to labeling positive back, but it is noticeable that it's not.",
            "It's not the same as the drug activity prediction.",
            "'cause if we just have one positive instance in the bag, we may not label this document as positive becausw Also, maybe he's making a metaphor or something like that.",
            "So we may have such noises."
        ],
        [
            "OK, so the third application of multiple instance learning which is popular these days with image classification.",
            "In this application, the image is regarded as a bag of local operations and we just ignore the these relationships, spatial relationship and semantic relationships among these local observations we just represent.",
            "Represent them as the feature vectors.",
            "OK, so we can have a quick impression of what these image features.",
            "Goes like on the right on the right side of this figure.",
            "And we can see that they are largely some response of some filters.",
            "Basically, they're very low level representations, which may have had little connection to the high level image labels such as a human face or building or car or something like that.",
            "OK, so these these instances serve as weak evidences for labeling the bag as positive or negative.",
            "So we may need many positive instance for labeling the positive back, which means if in image we have apart look like human nodes and part look like looks like a human eye and another part looks like human mouse and we have many such positive instances, we can label it as human phase.",
            "But if we have only.",
            "One of them are two of them is hard to determine what what it is.",
            "OK, so."
        ],
        [
            "So to sum up, we we can put the different application domains of multiple instance learning on the axis like this drug activity prediction is on the left on the left side.",
            "An image classification is on the right side.",
            "So document is in somewhere between them.",
            "OK, so so if we go to the left, the positive instances are weak evidences for positive bags and then we have to.",
            "And we need many positive instance to label back as positive.",
            "And if we go to the left hand of the axis, things are just opposite."
        ],
        [
            "OK, so in order to accommodate these differences among different applications of multiple instance learning we we just generalized framework or the definition of multiple instance learning."
        ],
        [
            "In this way?",
            "OK, so so as we have stayed at, we have mentioned different applications can be put on this axis and the traditional multiple instance learning with this definition is can be put on the left end of this axis and we want to have a generalized framework of multiple instance learning which can accommodate this whole degree of freedom."
        ],
        [
            "OK, so so in multiple instance learning we we have the learner presented with bags of instances.",
            "With these bags with labels on bags but no labels on instances and we want to predict labels on new bags and this is just the same as generalized multiple instance learning, except that in generalized setting bag is positive if and only if more than X percent of these instances."
        ],
        [
            "Is positive.",
            "OK, so the most important thing here is that this parameter as percent.",
            "Is underlying mechanic some in some applications in some data set, which is which may different in different datasets and is unknown to the learner.",
            "OK, so.",
            "So this parameter as percent can be viewed as approximate.",
            "This degree of freedom on this axis, but it is only some props approximation becausw the actual mechanism that instances determined back labels can be very complicated in real world applications.",
            "So we just approximate this complexity with one degree of freedom."
        ],
        [
            "Chris, always such setting.",
            "We we have these major challenges arising from generalized multiple instance learning.",
            "The underlying parameter, as percentages, of course, varies across different datasets arising from different application domains and it is unknown to the learner which is the most difficult part.",
            "So our problem is how can we learn who is presented only with labeled bags?",
            "Discover the underlying difference in in this underlying mechanism.",
            "OK, so so more importantly or Furthermore, how can the learner automatically adapt itself to these different mechanisms?"
        ],
        [
            "OK, so so this leads to our framework of PPM."
        ],
        [
            "Nose.",
            "OK, and our solution to this problem is very simple.",
            "First we have the instances represented in some feature space.",
            "OK, so we just fixed mixture model to all instances and we can represent each instance.",
            "As the posterior posteriors over the mixture components.",
            "OK, so so then we just represent each bag as aggregate posteriors.",
            "Or which which is calculated by some.",
            "Some overall is instances.",
            "This is actually something like histogram.",
            "So we have the formal definition that follows.",
            "We just sum over all the instances to get the aggregate post."
        ],
        [
            "Serious representation.",
            "OK, so so the aggregate."
        ],
        [
            "Posterior representation is denoted as apacci X, where X is back."
        ],
        [
            "OK, so the the most important part of this work is we impose a map on the representation of bags.",
            "We just add a small P is shoulder.",
            "OK, so this is very interesting 'cause if with a large P. We just attenuated minor patterns in this representation and with a small P which is enhanced minor patterns in this represent."
        ],
        [
            "And so, so how does it affect?",
            "So learning process.",
            "Queso.",
            "We have we have different applications of multiple instance learning pool here and.",
            "And with yeah, so this Colonel idea of this of this work is that we have intuition that for for the applications on the left side left hand side of this axis, the minor patterns should be enhanced when comparing bags to determine the similarity.",
            "And on the right hand side of this axis, the minor patterns should be attenuated when comparing bags.",
            "OK, so basically our intuition is that we should have a large P4 four if the application domain has a large parameter S percent if the S is large, then P should be large.",
            "Effect is smaller than pee should be small.",
            "OK, so so this is just intuition and I think how to put this in a rigorous and formal way that we can write proof is very interesting future work, so in this work we just validate this intuition with some.",
            "OK.",
            "So here's the definition of our kernel function, which is then the PPM kernels.",
            "Stand for P posterior mixture model Colonel.",
            "We just put a P on the on the shoulder of the SCIEX and we just make the."
        ],
        [
            "The inner product.",
            "OK, so in order to validate validate our our approach to this problem.",
            "We generate three synthetic datasets as follows.",
            "OK, so we first we randomly initialize, initialize some the parameters of some Gaussian mixture models and we just sample these models to get the instances.",
            "Then we just randomly allocate these instances into a number of bags.",
            "And we just label these bags to get our synthetics datasets.",
            "OK, so for labeling these bags, we first we first label the instances according to rule.",
            "That instance instance forms certain mixture components are labeled as positive.",
            "OK, so when the other instances are labeled, we just label the bags with the S percent rule.",
            "OK. Yeah, so so we just set set the S percent be at least 120 percent, 50% to get multiple instance learning data set 1, two and three.",
            "OK, so so the most important, most interesting thing here is that the learner only sees bag labels with instances.",
            "They're unlabeled instances so and we keep 50 percent, 50% positive versus negative bags for all datasets.",
            "So basically the datasets can hardly be distinguished from from each other, except that they have a different underlying rule of generating the back labels."
        ],
        [
            "So a very interesting phenomenon we observe that is, is that the we just compute the kernel alignment of our PM kernel function with the ideal kernel function idea.",
            "Colonel is very easy to compute because we have at ground truth.",
            "We have these back labels.",
            "So the so we have the kernel alignment plotted as a function of the parameter P in the kernel in the PPM kernel function.",
            "Basically, we can observe that these three on these three different datasets, the optimal value of P, just varies as the underlying parameter as percent increases.",
            "So for the first data set with this person be at least one we have smallest.",
            "Of optimal value of P. So the learner just revealed the underlying difference among these datasets, which I think is very into."
        ],
        [
            "Sting OK, so we also compare our method to the state of the art.",
            "Multiple instance learning techniques.",
            "So this as we all know these two are drug activity prediction datasets.",
            "And elephant Tiger folks images and track one track.",
            "Two are text text documents.",
            "And I think the most interesting part of this result is that the optimal value of P is relatively small for these two, this application and relatively large for the image datasets, so of course it's not very stable, but I think it can reveal some trends that our approach is valid is valid on the real world data set."
        ],
        [
            "OK thanks, I thought.",
            "Time for questions.",
            "I guess the question I had was.",
            "So this is very similar of course to what the computer vision people are doing with his dreams.",
            "Done quite a bit of work recently in computer vision and trying to build those dictionaries more discriminatively.",
            "Yeah yeah.",
            "Instead of just using Gaussian mixture model, yeah do you have any thoughts about how to do that in in this setting?",
            "Yeah I think I think our inferences here is to use this.",
            "I think our emphasis is used this P value to tune this kernel function in the previous and in the first step we.",
            "Chill the mixture model so I think this is just two assault, no directions.",
            "So you can.",
            "You can use other methods to build this mixture model beauty of more accurately vocabulary and you can also use this P use P parameter tuning kernel function.",
            "Yes, the question is.",
            "Currently you determine the search speed by exhaustive search, yeah?",
            "This sometime can be realized in some task, but for many applications, if you do not know the appropriate range of P, yeah.",
            "Somewhat difficult, yeah.",
            "Yeah, yeah, I think I think that's that would be a very interesting future work which refer to this one.",
            "Do you need to correct?",
            "When?",
            "I mean I guess in your synthetic data experiments all the bags were the same size.",
            "Yeah, I was back with same side.",
            "Yeah right yeah.",
            "OK, confirmations and others had more than 100.",
            "Yeah yeah I think so.",
            "How is that correct for that?",
            "Oh OK so so."
        ],
        [
            "Over representation of data called Aggregate post zeros or we have this normalized normalization operator put in front 2.",
            "I think this this is make the representation unbiased towards K. Yeah yeah.",
            "Last chance for questions.",
            "Let's thank the speaker again, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On now I will give a brief introduction to our work on adaptive PMM kernels for multiple instance learning could also solve this work.",
                    "label": 1
                },
                {
                    "sent": "Include professor Young Chung from Hong Kong, UST.",
                    "label": 0
                },
                {
                    "sent": "Yeah case.",
                    "label": 1
                },
                {
                    "sent": "And Professor John Bean from Peking University.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here is our story line.",
                    "label": 0
                },
                {
                    "sent": "First I will give a brief introduction to the concept of multiple instance learning.",
                    "label": 0
                },
                {
                    "sent": "Then we'll go through some different applications of multiple instance learning and we'll see how they differ from each other.",
                    "label": 1
                },
                {
                    "sent": "Based on these analysis, we will introduce the generalized framework of multiple instance learning, and we'll see how how it can accommodate these differences.",
                    "label": 0
                },
                {
                    "sent": "Finally, I will give a simple but effective solution to the generalized multiple instance learning problem we proposed and it is named DPM kernels.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now we'll start from.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First part.",
                    "label": 0
                },
                {
                    "sent": "Are the original motivation of multiple instance learning, as we all know, is the application of drug activity prediction.",
                    "label": 1
                },
                {
                    "sent": "In this application we have bags which are a certain kind of drug molecules which can adopt A number of shapes.",
                    "label": 1
                },
                {
                    "sent": "So the bag is a molecule and instances are different shapes that it can adopt.",
                    "label": 1
                },
                {
                    "sent": "So a shape that binds well to the target protein which is active is labeled as positive.",
                    "label": 1
                },
                {
                    "sent": "And the shape that not is labeled as negative.",
                    "label": 0
                },
                {
                    "sent": "OK, so in wet lab experiments this molecule will be operated active 'cause it has one shape that is active.",
                    "label": 0
                },
                {
                    "sent": "OK, but wet lab experiments as we know cost a lot of time and money.",
                    "label": 0
                },
                {
                    "sent": "So so with multiple instance learning, our goal is to predict with confidence the activity of molecules.",
                    "label": 0
                },
                {
                    "sent": "Enjoy lab which means just by computers without experiments.",
                    "label": 0
                },
                {
                    "sent": "Which is good because it saves money for biologists and makes money for for us.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so setting off multiple instance learning goes like this.",
                    "label": 1
                },
                {
                    "sent": "We have bags of instances.",
                    "label": 1
                },
                {
                    "sent": "And these instances are labeled as either positive or negative.",
                    "label": 1
                },
                {
                    "sent": "And we have a positive bag if and only at least one of its instances positive.",
                    "label": 0
                },
                {
                    "sent": "OK, so this one is positive.",
                    "label": 0
                },
                {
                    "sent": "This is negative and this one is.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Positive.",
                    "label": 0
                },
                {
                    "sent": "So in multiple instance learning, the learner just cannot see the labels of the instances, it can only see the labeled bags with unlabeled instances in them.",
                    "label": 1
                },
                {
                    "sent": "So and learner is required to predict labels for previously unseen bags.",
                    "label": 1
                },
                {
                    "sent": "OK, so so this is pretty much setting off multiple instances.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we will see.",
                    "label": 0
                },
                {
                    "sent": "For instance, learning has been adapted to different application domains and we will go through these demands and we'll see how they do.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Far from each other.",
                    "label": 0
                },
                {
                    "sent": "OK, the first one is of course drug activity prediction, which is all original motivation of of this problem.",
                    "label": 0
                },
                {
                    "sent": "In this application of positive instances, definite evidence for positive bag, which means we just one positive instance for labeling the back as positive.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so on the other hand we multiple instance learning has been used in document classification, in which the bags are documents regarded as bags of words, phrases, sentences or paragraphs that depends.",
                    "label": 1
                },
                {
                    "sent": "Our representation of documents.",
                    "label": 0
                },
                {
                    "sent": "OK, so so suppose we have a problem of distinguish economix documents from from other documents.",
                    "label": 0
                },
                {
                    "sent": "So we have the documents on Economix labeled as positive and other documents labeled as negative for labeling document as positively.",
                    "label": 0
                },
                {
                    "sent": "So in this in this bag we have we have two positive instance in here they are market and economy.",
                    "label": 1
                },
                {
                    "sent": "OK so in this.",
                    "label": 0
                },
                {
                    "sent": "In this application, the positive instances are we can we can say they are strong.",
                    "label": 0
                },
                {
                    "sent": "Evidence is for for positive back 'cause we need several such evidence to labeling positive back, but it is noticeable that it's not.",
                    "label": 1
                },
                {
                    "sent": "It's not the same as the drug activity prediction.",
                    "label": 0
                },
                {
                    "sent": "'cause if we just have one positive instance in the bag, we may not label this document as positive becausw Also, maybe he's making a metaphor or something like that.",
                    "label": 0
                },
                {
                    "sent": "So we may have such noises.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the third application of multiple instance learning which is popular these days with image classification.",
                    "label": 0
                },
                {
                    "sent": "In this application, the image is regarded as a bag of local operations and we just ignore the these relationships, spatial relationship and semantic relationships among these local observations we just represent.",
                    "label": 1
                },
                {
                    "sent": "Represent them as the feature vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can have a quick impression of what these image features.",
                    "label": 0
                },
                {
                    "sent": "Goes like on the right on the right side of this figure.",
                    "label": 0
                },
                {
                    "sent": "And we can see that they are largely some response of some filters.",
                    "label": 0
                },
                {
                    "sent": "Basically, they're very low level representations, which may have had little connection to the high level image labels such as a human face or building or car or something like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so these these instances serve as weak evidences for labeling the bag as positive or negative.",
                    "label": 1
                },
                {
                    "sent": "So we may need many positive instance for labeling the positive back, which means if in image we have apart look like human nodes and part look like looks like a human eye and another part looks like human mouse and we have many such positive instances, we can label it as human phase.",
                    "label": 0
                },
                {
                    "sent": "But if we have only.",
                    "label": 0
                },
                {
                    "sent": "One of them are two of them is hard to determine what what it is.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to sum up, we we can put the different application domains of multiple instance learning on the axis like this drug activity prediction is on the left on the left side.",
                    "label": 1
                },
                {
                    "sent": "An image classification is on the right side.",
                    "label": 0
                },
                {
                    "sent": "So document is in somewhere between them.",
                    "label": 1
                },
                {
                    "sent": "OK, so so if we go to the left, the positive instances are weak evidences for positive bags and then we have to.",
                    "label": 0
                },
                {
                    "sent": "And we need many positive instance to label back as positive.",
                    "label": 0
                },
                {
                    "sent": "And if we go to the left hand of the axis, things are just opposite.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in order to accommodate these differences among different applications of multiple instance learning we we just generalized framework or the definition of multiple instance learning.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this way?",
                    "label": 0
                },
                {
                    "sent": "OK, so so as we have stayed at, we have mentioned different applications can be put on this axis and the traditional multiple instance learning with this definition is can be put on the left end of this axis and we want to have a generalized framework of multiple instance learning which can accommodate this whole degree of freedom.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so in multiple instance learning we we have the learner presented with bags of instances.",
                    "label": 0
                },
                {
                    "sent": "With these bags with labels on bags but no labels on instances and we want to predict labels on new bags and this is just the same as generalized multiple instance learning, except that in generalized setting bag is positive if and only if more than X percent of these instances.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is positive.",
                    "label": 0
                },
                {
                    "sent": "OK, so the most important thing here is that this parameter as percent.",
                    "label": 0
                },
                {
                    "sent": "Is underlying mechanic some in some applications in some data set, which is which may different in different datasets and is unknown to the learner.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So this parameter as percent can be viewed as approximate.",
                    "label": 0
                },
                {
                    "sent": "This degree of freedom on this axis, but it is only some props approximation becausw the actual mechanism that instances determined back labels can be very complicated in real world applications.",
                    "label": 1
                },
                {
                    "sent": "So we just approximate this complexity with one degree of freedom.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Chris, always such setting.",
                    "label": 0
                },
                {
                    "sent": "We we have these major challenges arising from generalized multiple instance learning.",
                    "label": 0
                },
                {
                    "sent": "The underlying parameter, as percentages, of course, varies across different datasets arising from different application domains and it is unknown to the learner which is the most difficult part.",
                    "label": 1
                },
                {
                    "sent": "So our problem is how can we learn who is presented only with labeled bags?",
                    "label": 0
                },
                {
                    "sent": "Discover the underlying difference in in this underlying mechanism.",
                    "label": 0
                },
                {
                    "sent": "OK, so so more importantly or Furthermore, how can the learner automatically adapt itself to these different mechanisms?",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so this leads to our framework of PPM.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nose.",
                    "label": 0
                },
                {
                    "sent": "OK, and our solution to this problem is very simple.",
                    "label": 0
                },
                {
                    "sent": "First we have the instances represented in some feature space.",
                    "label": 0
                },
                {
                    "sent": "OK, so we just fixed mixture model to all instances and we can represent each instance.",
                    "label": 1
                },
                {
                    "sent": "As the posterior posteriors over the mixture components.",
                    "label": 1
                },
                {
                    "sent": "OK, so so then we just represent each bag as aggregate posteriors.",
                    "label": 0
                },
                {
                    "sent": "Or which which is calculated by some.",
                    "label": 0
                },
                {
                    "sent": "Some overall is instances.",
                    "label": 0
                },
                {
                    "sent": "This is actually something like histogram.",
                    "label": 0
                },
                {
                    "sent": "So we have the formal definition that follows.",
                    "label": 0
                },
                {
                    "sent": "We just sum over all the instances to get the aggregate post.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Serious representation.",
                    "label": 0
                },
                {
                    "sent": "OK, so so the aggregate.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Posterior representation is denoted as apacci X, where X is back.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the the most important part of this work is we impose a map on the representation of bags.",
                    "label": 0
                },
                {
                    "sent": "We just add a small P is shoulder.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is very interesting 'cause if with a large P. We just attenuated minor patterns in this representation and with a small P which is enhanced minor patterns in this represent.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so, so how does it affect?",
                    "label": 0
                },
                {
                    "sent": "So learning process.",
                    "label": 0
                },
                {
                    "sent": "Queso.",
                    "label": 0
                },
                {
                    "sent": "We have we have different applications of multiple instance learning pool here and.",
                    "label": 0
                },
                {
                    "sent": "And with yeah, so this Colonel idea of this of this work is that we have intuition that for for the applications on the left side left hand side of this axis, the minor patterns should be enhanced when comparing bags to determine the similarity.",
                    "label": 1
                },
                {
                    "sent": "And on the right hand side of this axis, the minor patterns should be attenuated when comparing bags.",
                    "label": 1
                },
                {
                    "sent": "OK, so basically our intuition is that we should have a large P4 four if the application domain has a large parameter S percent if the S is large, then P should be large.",
                    "label": 0
                },
                {
                    "sent": "Effect is smaller than pee should be small.",
                    "label": 0
                },
                {
                    "sent": "OK, so so this is just intuition and I think how to put this in a rigorous and formal way that we can write proof is very interesting future work, so in this work we just validate this intuition with some.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here's the definition of our kernel function, which is then the PPM kernels.",
                    "label": 0
                },
                {
                    "sent": "Stand for P posterior mixture model Colonel.",
                    "label": 0
                },
                {
                    "sent": "We just put a P on the on the shoulder of the SCIEX and we just make the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The inner product.",
                    "label": 0
                },
                {
                    "sent": "OK, so in order to validate validate our our approach to this problem.",
                    "label": 1
                },
                {
                    "sent": "We generate three synthetic datasets as follows.",
                    "label": 1
                },
                {
                    "sent": "OK, so we first we randomly initialize, initialize some the parameters of some Gaussian mixture models and we just sample these models to get the instances.",
                    "label": 0
                },
                {
                    "sent": "Then we just randomly allocate these instances into a number of bags.",
                    "label": 0
                },
                {
                    "sent": "And we just label these bags to get our synthetics datasets.",
                    "label": 1
                },
                {
                    "sent": "OK, so for labeling these bags, we first we first label the instances according to rule.",
                    "label": 0
                },
                {
                    "sent": "That instance instance forms certain mixture components are labeled as positive.",
                    "label": 1
                },
                {
                    "sent": "OK, so when the other instances are labeled, we just label the bags with the S percent rule.",
                    "label": 1
                },
                {
                    "sent": "OK. Yeah, so so we just set set the S percent be at least 120 percent, 50% to get multiple instance learning data set 1, two and three.",
                    "label": 0
                },
                {
                    "sent": "OK, so so the most important, most interesting thing here is that the learner only sees bag labels with instances.",
                    "label": 0
                },
                {
                    "sent": "They're unlabeled instances so and we keep 50 percent, 50% positive versus negative bags for all datasets.",
                    "label": 0
                },
                {
                    "sent": "So basically the datasets can hardly be distinguished from from each other, except that they have a different underlying rule of generating the back labels.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a very interesting phenomenon we observe that is, is that the we just compute the kernel alignment of our PM kernel function with the ideal kernel function idea.",
                    "label": 0
                },
                {
                    "sent": "Colonel is very easy to compute because we have at ground truth.",
                    "label": 0
                },
                {
                    "sent": "We have these back labels.",
                    "label": 0
                },
                {
                    "sent": "So the so we have the kernel alignment plotted as a function of the parameter P in the kernel in the PPM kernel function.",
                    "label": 0
                },
                {
                    "sent": "Basically, we can observe that these three on these three different datasets, the optimal value of P, just varies as the underlying parameter as percent increases.",
                    "label": 0
                },
                {
                    "sent": "So for the first data set with this person be at least one we have smallest.",
                    "label": 1
                },
                {
                    "sent": "Of optimal value of P. So the learner just revealed the underlying difference among these datasets, which I think is very into.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sting OK, so we also compare our method to the state of the art.",
                    "label": 0
                },
                {
                    "sent": "Multiple instance learning techniques.",
                    "label": 0
                },
                {
                    "sent": "So this as we all know these two are drug activity prediction datasets.",
                    "label": 0
                },
                {
                    "sent": "And elephant Tiger folks images and track one track.",
                    "label": 0
                },
                {
                    "sent": "Two are text text documents.",
                    "label": 0
                },
                {
                    "sent": "And I think the most interesting part of this result is that the optimal value of P is relatively small for these two, this application and relatively large for the image datasets, so of course it's not very stable, but I think it can reveal some trends that our approach is valid is valid on the real world data set.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK thanks, I thought.",
                    "label": 0
                },
                {
                    "sent": "Time for questions.",
                    "label": 0
                },
                {
                    "sent": "I guess the question I had was.",
                    "label": 0
                },
                {
                    "sent": "So this is very similar of course to what the computer vision people are doing with his dreams.",
                    "label": 0
                },
                {
                    "sent": "Done quite a bit of work recently in computer vision and trying to build those dictionaries more discriminatively.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Instead of just using Gaussian mixture model, yeah do you have any thoughts about how to do that in in this setting?",
                    "label": 0
                },
                {
                    "sent": "Yeah I think I think our inferences here is to use this.",
                    "label": 0
                },
                {
                    "sent": "I think our emphasis is used this P value to tune this kernel function in the previous and in the first step we.",
                    "label": 0
                },
                {
                    "sent": "Chill the mixture model so I think this is just two assault, no directions.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "You can use other methods to build this mixture model beauty of more accurately vocabulary and you can also use this P use P parameter tuning kernel function.",
                    "label": 0
                },
                {
                    "sent": "Yes, the question is.",
                    "label": 0
                },
                {
                    "sent": "Currently you determine the search speed by exhaustive search, yeah?",
                    "label": 0
                },
                {
                    "sent": "This sometime can be realized in some task, but for many applications, if you do not know the appropriate range of P, yeah.",
                    "label": 0
                },
                {
                    "sent": "Somewhat difficult, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, I think I think that's that would be a very interesting future work which refer to this one.",
                    "label": 0
                },
                {
                    "sent": "Do you need to correct?",
                    "label": 0
                },
                {
                    "sent": "When?",
                    "label": 0
                },
                {
                    "sent": "I mean I guess in your synthetic data experiments all the bags were the same size.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I was back with same side.",
                    "label": 0
                },
                {
                    "sent": "Yeah right yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, confirmations and others had more than 100.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah I think so.",
                    "label": 0
                },
                {
                    "sent": "How is that correct for that?",
                    "label": 0
                },
                {
                    "sent": "Oh OK so so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over representation of data called Aggregate post zeros or we have this normalized normalization operator put in front 2.",
                    "label": 0
                },
                {
                    "sent": "I think this this is make the representation unbiased towards K. Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Last chance for questions.",
                    "label": 0
                },
                {
                    "sent": "Let's thank the speaker again, thank you.",
                    "label": 0
                }
            ]
        }
    }
}