{
    "id": "tnylhfvllhyzqbqgfruy63sy7cux7uop",
    "title": "Semantic Sentiment Analysis of Twitter",
    "info": {
        "author": [
            "Hassan Saif, Knowledge Media Institute (KMI), Open University (OU)"
        ],
        "published": "Dec. 3, 2012",
        "recorded": "November 2012",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2012_saif_sentiment_analysis/",
    "segmentation": [
        [
            "So my name is Hassan versus student second year at the Open University KMI in UK."
        ],
        [
            "I'm going to talk today about.",
            "My work on semantic sentiment answers of Twitter gonna start with general background and definition of the problem and talk about related work and my approach.",
            "Move to the evaluation and talk about the demo and future work.",
            "Of course with conclusion so."
        ],
        [
            "So.",
            "Sometimes the answers is the task of fund ifying positive and negative opinions, and choose and attitudes in text.",
            "In fact, this problem is very old problem where much work has been done so far in this area from confidential text such as movie reviews or product reviews."
        ],
        [
            "This line of work actually followed by two main approaches.",
            "The first approach is the lexical based approach, where the semantic Orient or the sentiment orientation of the sentence.",
            "Is the sum of all its operated words inside the sentence.",
            "So for example, I if we have a sentence, I really love iPhone 4S.",
            "We compare the word love to sentiment lexicon and we found that.",
            "Or we find that love is positive word.",
            "So the whole sentence would be positive."
        ],
        [
            "Second line of work actually follow the machine learning approach and this approach.",
            "The problem considered as a normal text classification problem where we train our model using one of the famous or the standard machine learning methods such as SVM or naive Bayes, and we learn this model from manually updated or whatever updated data set, and then we use the model to infer the sentiment from documents from the test set."
        ],
        [
            "However, when it comes to Twitter, the problem becomes much more difficult.",
            "This is because of very special characteristics that Twitter has, such as the short length of status updates and language variations where people tend to use a firm words and slangs rock.",
            "You'll stuff inside their tweets and the third problem or the third characteristics actually is the open domain nature of Twitter where there is no restriction of what people tweet about in which about watch.",
            "So they can in which domain?",
            "These challenges actually bring us to question do we really need?"
        ],
        [
            "To perform sentiment analysis over Twitter.",
            "In fact, with your study, major studies showed that there is a strong connection between the public opinion of people on Twitter to word certain topic or event, and the actual results of that event on the ground or the real life for example.",
            "For.",
            "For example, we our previous study on Twitter sentiment analysis.",
            "We collect data in the week of the occasional election 2010, and we figure out that the winning party or the ruling political party, the Conservative Party, has received the largest number of tweets, Day or the positive sentiment of the tweets data.",
            "So which like prove that our conclusion that centered answers over Twitch data is really important problem?"
        ],
        [
            "So some work has been done so far for Twitter."
        ],
        [
            "Dennis is this can be divided in three main lines.",
            "The first line of work.",
            "Consider the problem again.",
            "Ask test classification problem where they train their classifiers from dated tweets, data like using the distance supervision approach.",
            "According to this approach, a tweet is annotated with a positive or negative sentiment based on the smile is inside all the emoticons inside the tweet.",
            "So for example, if I have a treat containing happy Smiley or happy or monticone, then it's positive and if it contains an Smiley sad Smiley, so it's negative.",
            "And then they just train their classifier from uni grams or N grams and part of speech tags."
        ],
        [
            "The second line of work argued that relying on these noisy, noisy label training data may highlight the classifier performance, so they they propose using label propagation approach where they build a graph containing users, tweets and grams and hashtags and the connection between them.",
            "So for example, user has a tweet and tweet has unigrams and engrams could be hashtag for example.",
            "Then they applied label propagation method throughout the graph and actually they start with a small number of labeled tweets.",
            "And then they apply label propagation throughout the graph."
        ],
        [
            "Third line of work here.",
            "Consider the problem as feature engineering problem.",
            "When they investigate various set of features such as engrams and part of speech tags and they also propose using the microblogging features that Twitter has, such as the hashtag on emoticons, abbreviation, anti fires and then found.",
            "Actually that these features outperform using the N grams and other features."
        ],
        [
            "Mantex OK, so we actually here propose a new set of features derived from the semantic orientation or the semantic representation of tweets data."
        ],
        [
            "The main idea is to extract semantic concepts from Tuesday to incorporate this concept as councils.",
            "Additional features for supervised classifier training.",
            "Now to understand the ID."
        ],
        [
            "And the motivation behind the idea I'm going to give an example.",
            "So let's assume that we have.",
            "Short of our vocabulary from the training data and this book is, this snapshot contains entities such as iPad, MacBook Pro and iPod and we mapped these entities to a concept product output.",
            "Now from the training data we we can note that.",
            "These entities received more positive tweet, more positive sentiment than negative sentiment.",
            "Hence we can say that product Apple received more positive sentiment annotator sentiment.",
            "Now if we have a suite from the testing set or the test set.",
            "Finally I got my iPhone with a product for the normal machine learning approaches.",
            "Actually we don't have any indicator here.",
            "What is this sentence about?",
            "Is it?",
            "Does it have a positive sentiment or negative sentiment?",
            "However, another approach or idea is this tweet has.",
            "IPhone as an entity and iPhone is mapped to a concept product Apple and we know that product output is mapped to these entities that are positive and product.",
            "Apple is then a positive.",
            "Then the iPhone itself is positive and then we can say that the whole sentence is more likely or the whole suite is more likely to be positive and negative.",
            "So, um."
        ],
        [
            "We explored as well three different methods to incorporate our semantic features.",
            "The first 2 methods are the replacement method and argumentation method.",
            "We call this method the shallow semantic methods for the replacement.",
            "For example, we replace all entities in tweets, data with their corresponding concepts such as rugby will replace Ruby with the sports.",
            "Now, all tweets that containing entities that fair or that associate with the sport.",
            "As a concept.",
            "We also replace these entities with the sport as a concept for recommendation.",
            "We just simply add these.",
            "Concepts as additional features, the feature space."
        ],
        [
            "Now more advanced way to incorporate our features is through interpolation.",
            "Here we interpolate the unigram language model in Naive Bayes with the generative model of word given concept and the concept given a class.",
            "And another example again, that we have a snapshot of vocabulary and it contains warms logic, bombs and rootkits.",
            "As a as entities and they receive negative sentiment than positive sentiment.",
            "Hence we can say that computer virus as a concept is negative concept.",
            "Now we have again at a tweet from test set a Trojan horse virus mess up my laptop.",
            "Tuition horses are entity mapped to both negative concept then too.",
            "Mars itself is a negative and the whole tweet is more likely to be negative than positive."
        ],
        [
            "For the entity concept extraction, we investigated three different or we explored the three different third party tools for the entity extraction and the concept mappings they met up in Cali and Alchemy API.",
            "We evaluated these three tools based on the number of extracted entities and the quality or the accuracy of the entity concept mappings.",
            "Our three evaluators found that Alchemy API extract the highest number of entities and it received the high or it reduced the highest.",
            "Curacy of the entity concept mappings.",
            "Hence we depend on acne API to extract our entities and for the T concept mappings."
        ],
        [
            "Now for the evaluation.",
            "We evaluate our features on three different using three different data sets.",
            "The first one is the standard stand for Twitter sentiment from Stanford University.",
            "This data set concept consists of training and test set, and it's actually a large data set contains like 1 1/2 million tweets.",
            "However, we just selected 60,000 tweets here to prove our concept and the test set.",
            "Is actually small again between larger using our online rotation tool and then finally we have 1000 tweets as a test set the the other another.",
            "Another data set is the tweets is the data set that contained tweets about the health care reform in that happen US and the third one is Obama keen debates in 2010.",
            "And because of this, the third one doesn't have the test set.",
            "We perform 10 fold cross validation, 10 fold cross validation.",
            "Actually on this data.",
            "Um?"
        ],
        [
            "We compare our features to three different baselines.",
            "The first one is the standard unigram features and the second one is again the parts of speech features.",
            "The third one is central features for these features.",
            "Actually, we have already proposed these features for sentiment analysis and obvious work, and we extract these features using the GST join sentimental back model that extract the topics from this data and share the 7th sentiment topic.",
            "So, for example, for the sentence I like the new iPad.",
            "One is here is the number of the index of the topic and zero is the label of the sentiment of the pronoun I.",
            "And one here, for example, this one is neutral, one is positive and two is negative.",
            "So we add these features as additional features, the feature space."
        ],
        [
            "So we reported here the performance for three different methods on incorporating the semantic features in terms of accuracy.",
            "So as we can not hear that semantic relation outperforms the other two methods.",
            "Replacement argumentation.",
            "This could be explained that the mere use of the replacement that will replace all concepts with all entities we lose a lot of data.",
            "So for example, if we have two tweets, the first tweet, I like Obama, I support Obama and other treats.",
            "I don't support Romney, for example.",
            "Then, if we replace both entities with their concept, President US President, then both tweets will become a support US President and I don't support you as president.",
            "So because of that we call this as shallow semantic methods and because of that we performance like dropped significantly, actually.",
            "Um?"
        ],
        [
            "We also compare our our semantic features against the three baselines and for the old data sets and for the positive sentiment, Anika sentiment detection tasks in terms of precision, recall, and F measure.",
            "Here we have to find these.",
            "Actually, the first one is.",
            "For semantic features, they outperform both baselines that Ingram and partial speech for all datasets and for for both tasks.",
            "However, for for HCR and AMD we cannot that joint sentiment topic model outperformed the semantic features for ACR D, But if you're still out, perform the center features for SDS.",
            "Go back to the data sets.",
            "CS is a Stanford Twitter sentiment data set.",
            "This is very general data set that contain various topics or large number of topics.",
            "This is different from the RMD that contained tweets about specific topics and they are much smaller than Stanford sentimentally, so this could be an explanation of why joint sentiment topic outperform Stanford data set algorithm.",
            "Sorry, semantic features on Stanford data set."
        ],
        [
            "And also we report the average precision, recall and F measure for all three data sets, and we can also note here that.",
            "Semantic features were found better for the negative sentiment detection, while sentiment of features before better for the positive sentiment detection task.",
            "Now on average we can find that semantic features perform outperform the sentiment topic features in terms of precision.",
            "While sentiment of features outperforms semantic features for the in terms of recall and F measure.",
            "Now for the.",
            "Social data analysis that we have where we have large number of data that flow in real time.",
            "Actually we are more interested in the precision than the recall.",
            "Or let's say that the precision is much more important indicator than the recall in this problem."
        ],
        [
            "Now, as I said that we suffer from the lack of annotated data for testing purposes.",
            "So we build an online interface called the Terminator that able users today tweets, tweets, data as most negative and neutral.",
            "Sentiment.",
            "And then we are able to extend our test set to 1000 tweets using this tool.",
            "Later we implement our work in this paper as additional features here.",
            "Additional services such as the semantic sentiment and Sentiment Edition.",
            "Anyway, you can go online and check this tool Twitter.com."
        ],
        [
            "OK, to conclude, sentences over trees data is really painful problem because of the special characteristics that I mentioned on Twitter.",
            "We proposed using sentiment semantic features for Twitter using three different incorporation methods, replacement documentation, interpolation.",
            "We found out interpolation outperform replacement and argumentation.",
            "We also compare our features against three different baselines, and we found that in average that semantic features more precise than other features.",
            "At the end I can say that there is no winning approach here or knowing features in this study.",
            "It depends on the on the nature of the data sets.",
            "So while semantic features perform better on large data set that contain general tweets and various topics, we can see also that or we can note that joint sentiment topic features outperform other features on smaller data sets that are centered around specific topics.",
            "I'm sorry."
        ],
        [
            "As a future work.",
            "Sorry.",
            "As a future work, so one bottleneck for approaches, this semantic concept extraction we use here Alchemy API that give that provides us with up to 30 constant different concepts.",
            "These concepts are actually general.",
            "We believe that if we can.",
            "Choose the concept based on certain level of granularity.",
            "This will improve their sentiment, classification accuracy or performance.",
            "So we would like to explore more refined ways or tools to forensics, traction and entity concept mappings.",
            "We also would like to propose selective interpolation method.",
            "Actually here we interpolate all concepts again.",
            "So we would like to interpolate the concepts that does matter or that do matter to the classic classification.",
            "This could be measured.",
            "Based on the contribution of each concept to the classification performance.",
            "We also proposed here classifiers that attack sentiment from.",
            "Sorry that detect sentiment positive and negative sentiment would like also to study providing more classy like hybrid classifiers that is able to also provide neutral sentiment detection."
        ],
        [
            "His references."
        ],
        [
            "Thank you, you can contact me on my email or Twitter and you can ask me whatever you would like to ask.",
            "Yeah."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So my name is Hassan versus student second year at the Open University KMI in UK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to talk today about.",
                    "label": 0
                },
                {
                    "sent": "My work on semantic sentiment answers of Twitter gonna start with general background and definition of the problem and talk about related work and my approach.",
                    "label": 1
                },
                {
                    "sent": "Move to the evaluation and talk about the demo and future work.",
                    "label": 0
                },
                {
                    "sent": "Of course with conclusion so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the answers is the task of fund ifying positive and negative opinions, and choose and attitudes in text.",
                    "label": 1
                },
                {
                    "sent": "In fact, this problem is very old problem where much work has been done so far in this area from confidential text such as movie reviews or product reviews.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This line of work actually followed by two main approaches.",
                    "label": 0
                },
                {
                    "sent": "The first approach is the lexical based approach, where the semantic Orient or the sentiment orientation of the sentence.",
                    "label": 0
                },
                {
                    "sent": "Is the sum of all its operated words inside the sentence.",
                    "label": 0
                },
                {
                    "sent": "So for example, I if we have a sentence, I really love iPhone 4S.",
                    "label": 1
                },
                {
                    "sent": "We compare the word love to sentiment lexicon and we found that.",
                    "label": 0
                },
                {
                    "sent": "Or we find that love is positive word.",
                    "label": 0
                },
                {
                    "sent": "So the whole sentence would be positive.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second line of work actually follow the machine learning approach and this approach.",
                    "label": 0
                },
                {
                    "sent": "The problem considered as a normal text classification problem where we train our model using one of the famous or the standard machine learning methods such as SVM or naive Bayes, and we learn this model from manually updated or whatever updated data set, and then we use the model to infer the sentiment from documents from the test set.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, when it comes to Twitter, the problem becomes much more difficult.",
                    "label": 0
                },
                {
                    "sent": "This is because of very special characteristics that Twitter has, such as the short length of status updates and language variations where people tend to use a firm words and slangs rock.",
                    "label": 1
                },
                {
                    "sent": "You'll stuff inside their tweets and the third problem or the third characteristics actually is the open domain nature of Twitter where there is no restriction of what people tweet about in which about watch.",
                    "label": 0
                },
                {
                    "sent": "So they can in which domain?",
                    "label": 0
                },
                {
                    "sent": "These challenges actually bring us to question do we really need?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To perform sentiment analysis over Twitter.",
                    "label": 0
                },
                {
                    "sent": "In fact, with your study, major studies showed that there is a strong connection between the public opinion of people on Twitter to word certain topic or event, and the actual results of that event on the ground or the real life for example.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "For example, we our previous study on Twitter sentiment analysis.",
                    "label": 1
                },
                {
                    "sent": "We collect data in the week of the occasional election 2010, and we figure out that the winning party or the ruling political party, the Conservative Party, has received the largest number of tweets, Day or the positive sentiment of the tweets data.",
                    "label": 0
                },
                {
                    "sent": "So which like prove that our conclusion that centered answers over Twitch data is really important problem?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So some work has been done so far for Twitter.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dennis is this can be divided in three main lines.",
                    "label": 0
                },
                {
                    "sent": "The first line of work.",
                    "label": 0
                },
                {
                    "sent": "Consider the problem again.",
                    "label": 0
                },
                {
                    "sent": "Ask test classification problem where they train their classifiers from dated tweets, data like using the distance supervision approach.",
                    "label": 0
                },
                {
                    "sent": "According to this approach, a tweet is annotated with a positive or negative sentiment based on the smile is inside all the emoticons inside the tweet.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I have a treat containing happy Smiley or happy or monticone, then it's positive and if it contains an Smiley sad Smiley, so it's negative.",
                    "label": 0
                },
                {
                    "sent": "And then they just train their classifier from uni grams or N grams and part of speech tags.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second line of work argued that relying on these noisy, noisy label training data may highlight the classifier performance, so they they propose using label propagation approach where they build a graph containing users, tweets and grams and hashtags and the connection between them.",
                    "label": 0
                },
                {
                    "sent": "So for example, user has a tweet and tweet has unigrams and engrams could be hashtag for example.",
                    "label": 0
                },
                {
                    "sent": "Then they applied label propagation method throughout the graph and actually they start with a small number of labeled tweets.",
                    "label": 1
                },
                {
                    "sent": "And then they apply label propagation throughout the graph.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Third line of work here.",
                    "label": 0
                },
                {
                    "sent": "Consider the problem as feature engineering problem.",
                    "label": 1
                },
                {
                    "sent": "When they investigate various set of features such as engrams and part of speech tags and they also propose using the microblogging features that Twitter has, such as the hashtag on emoticons, abbreviation, anti fires and then found.",
                    "label": 0
                },
                {
                    "sent": "Actually that these features outperform using the N grams and other features.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mantex OK, so we actually here propose a new set of features derived from the semantic orientation or the semantic representation of tweets data.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The main idea is to extract semantic concepts from Tuesday to incorporate this concept as councils.",
                    "label": 1
                },
                {
                    "sent": "Additional features for supervised classifier training.",
                    "label": 0
                },
                {
                    "sent": "Now to understand the ID.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the motivation behind the idea I'm going to give an example.",
                    "label": 1
                },
                {
                    "sent": "So let's assume that we have.",
                    "label": 0
                },
                {
                    "sent": "Short of our vocabulary from the training data and this book is, this snapshot contains entities such as iPad, MacBook Pro and iPod and we mapped these entities to a concept product output.",
                    "label": 0
                },
                {
                    "sent": "Now from the training data we we can note that.",
                    "label": 0
                },
                {
                    "sent": "These entities received more positive tweet, more positive sentiment than negative sentiment.",
                    "label": 0
                },
                {
                    "sent": "Hence we can say that product Apple received more positive sentiment annotator sentiment.",
                    "label": 0
                },
                {
                    "sent": "Now if we have a suite from the testing set or the test set.",
                    "label": 0
                },
                {
                    "sent": "Finally I got my iPhone with a product for the normal machine learning approaches.",
                    "label": 0
                },
                {
                    "sent": "Actually we don't have any indicator here.",
                    "label": 0
                },
                {
                    "sent": "What is this sentence about?",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "Does it have a positive sentiment or negative sentiment?",
                    "label": 0
                },
                {
                    "sent": "However, another approach or idea is this tweet has.",
                    "label": 0
                },
                {
                    "sent": "IPhone as an entity and iPhone is mapped to a concept product Apple and we know that product output is mapped to these entities that are positive and product.",
                    "label": 0
                },
                {
                    "sent": "Apple is then a positive.",
                    "label": 0
                },
                {
                    "sent": "Then the iPhone itself is positive and then we can say that the whole sentence is more likely or the whole suite is more likely to be positive and negative.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We explored as well three different methods to incorporate our semantic features.",
                    "label": 1
                },
                {
                    "sent": "The first 2 methods are the replacement method and argumentation method.",
                    "label": 0
                },
                {
                    "sent": "We call this method the shallow semantic methods for the replacement.",
                    "label": 1
                },
                {
                    "sent": "For example, we replace all entities in tweets, data with their corresponding concepts such as rugby will replace Ruby with the sports.",
                    "label": 0
                },
                {
                    "sent": "Now, all tweets that containing entities that fair or that associate with the sport.",
                    "label": 0
                },
                {
                    "sent": "As a concept.",
                    "label": 0
                },
                {
                    "sent": "We also replace these entities with the sport as a concept for recommendation.",
                    "label": 0
                },
                {
                    "sent": "We just simply add these.",
                    "label": 0
                },
                {
                    "sent": "Concepts as additional features, the feature space.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now more advanced way to incorporate our features is through interpolation.",
                    "label": 0
                },
                {
                    "sent": "Here we interpolate the unigram language model in Naive Bayes with the generative model of word given concept and the concept given a class.",
                    "label": 0
                },
                {
                    "sent": "And another example again, that we have a snapshot of vocabulary and it contains warms logic, bombs and rootkits.",
                    "label": 0
                },
                {
                    "sent": "As a as entities and they receive negative sentiment than positive sentiment.",
                    "label": 0
                },
                {
                    "sent": "Hence we can say that computer virus as a concept is negative concept.",
                    "label": 0
                },
                {
                    "sent": "Now we have again at a tweet from test set a Trojan horse virus mess up my laptop.",
                    "label": 1
                },
                {
                    "sent": "Tuition horses are entity mapped to both negative concept then too.",
                    "label": 0
                },
                {
                    "sent": "Mars itself is a negative and the whole tweet is more likely to be negative than positive.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the entity concept extraction, we investigated three different or we explored the three different third party tools for the entity extraction and the concept mappings they met up in Cali and Alchemy API.",
                    "label": 1
                },
                {
                    "sent": "We evaluated these three tools based on the number of extracted entities and the quality or the accuracy of the entity concept mappings.",
                    "label": 1
                },
                {
                    "sent": "Our three evaluators found that Alchemy API extract the highest number of entities and it received the high or it reduced the highest.",
                    "label": 0
                },
                {
                    "sent": "Curacy of the entity concept mappings.",
                    "label": 0
                },
                {
                    "sent": "Hence we depend on acne API to extract our entities and for the T concept mappings.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now for the evaluation.",
                    "label": 0
                },
                {
                    "sent": "We evaluate our features on three different using three different data sets.",
                    "label": 0
                },
                {
                    "sent": "The first one is the standard stand for Twitter sentiment from Stanford University.",
                    "label": 0
                },
                {
                    "sent": "This data set concept consists of training and test set, and it's actually a large data set contains like 1 1/2 million tweets.",
                    "label": 0
                },
                {
                    "sent": "However, we just selected 60,000 tweets here to prove our concept and the test set.",
                    "label": 0
                },
                {
                    "sent": "Is actually small again between larger using our online rotation tool and then finally we have 1000 tweets as a test set the the other another.",
                    "label": 0
                },
                {
                    "sent": "Another data set is the tweets is the data set that contained tweets about the health care reform in that happen US and the third one is Obama keen debates in 2010.",
                    "label": 0
                },
                {
                    "sent": "And because of this, the third one doesn't have the test set.",
                    "label": 0
                },
                {
                    "sent": "We perform 10 fold cross validation, 10 fold cross validation.",
                    "label": 0
                },
                {
                    "sent": "Actually on this data.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We compare our features to three different baselines.",
                    "label": 0
                },
                {
                    "sent": "The first one is the standard unigram features and the second one is again the parts of speech features.",
                    "label": 0
                },
                {
                    "sent": "The third one is central features for these features.",
                    "label": 0
                },
                {
                    "sent": "Actually, we have already proposed these features for sentiment analysis and obvious work, and we extract these features using the GST join sentimental back model that extract the topics from this data and share the 7th sentiment topic.",
                    "label": 0
                },
                {
                    "sent": "So, for example, for the sentence I like the new iPad.",
                    "label": 1
                },
                {
                    "sent": "One is here is the number of the index of the topic and zero is the label of the sentiment of the pronoun I.",
                    "label": 0
                },
                {
                    "sent": "And one here, for example, this one is neutral, one is positive and two is negative.",
                    "label": 0
                },
                {
                    "sent": "So we add these features as additional features, the feature space.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we reported here the performance for three different methods on incorporating the semantic features in terms of accuracy.",
                    "label": 1
                },
                {
                    "sent": "So as we can not hear that semantic relation outperforms the other two methods.",
                    "label": 0
                },
                {
                    "sent": "Replacement argumentation.",
                    "label": 0
                },
                {
                    "sent": "This could be explained that the mere use of the replacement that will replace all concepts with all entities we lose a lot of data.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we have two tweets, the first tweet, I like Obama, I support Obama and other treats.",
                    "label": 0
                },
                {
                    "sent": "I don't support Romney, for example.",
                    "label": 0
                },
                {
                    "sent": "Then, if we replace both entities with their concept, President US President, then both tweets will become a support US President and I don't support you as president.",
                    "label": 0
                },
                {
                    "sent": "So because of that we call this as shallow semantic methods and because of that we performance like dropped significantly, actually.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also compare our our semantic features against the three baselines and for the old data sets and for the positive sentiment, Anika sentiment detection tasks in terms of precision, recall, and F measure.",
                    "label": 0
                },
                {
                    "sent": "Here we have to find these.",
                    "label": 0
                },
                {
                    "sent": "Actually, the first one is.",
                    "label": 0
                },
                {
                    "sent": "For semantic features, they outperform both baselines that Ingram and partial speech for all datasets and for for both tasks.",
                    "label": 0
                },
                {
                    "sent": "However, for for HCR and AMD we cannot that joint sentiment topic model outperformed the semantic features for ACR D, But if you're still out, perform the center features for SDS.",
                    "label": 0
                },
                {
                    "sent": "Go back to the data sets.",
                    "label": 0
                },
                {
                    "sent": "CS is a Stanford Twitter sentiment data set.",
                    "label": 0
                },
                {
                    "sent": "This is very general data set that contain various topics or large number of topics.",
                    "label": 0
                },
                {
                    "sent": "This is different from the RMD that contained tweets about specific topics and they are much smaller than Stanford sentimentally, so this could be an explanation of why joint sentiment topic outperform Stanford data set algorithm.",
                    "label": 0
                },
                {
                    "sent": "Sorry, semantic features on Stanford data set.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also we report the average precision, recall and F measure for all three data sets, and we can also note here that.",
                    "label": 0
                },
                {
                    "sent": "Semantic features were found better for the negative sentiment detection, while sentiment of features before better for the positive sentiment detection task.",
                    "label": 0
                },
                {
                    "sent": "Now on average we can find that semantic features perform outperform the sentiment topic features in terms of precision.",
                    "label": 0
                },
                {
                    "sent": "While sentiment of features outperforms semantic features for the in terms of recall and F measure.",
                    "label": 0
                },
                {
                    "sent": "Now for the.",
                    "label": 0
                },
                {
                    "sent": "Social data analysis that we have where we have large number of data that flow in real time.",
                    "label": 0
                },
                {
                    "sent": "Actually we are more interested in the precision than the recall.",
                    "label": 0
                },
                {
                    "sent": "Or let's say that the precision is much more important indicator than the recall in this problem.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, as I said that we suffer from the lack of annotated data for testing purposes.",
                    "label": 0
                },
                {
                    "sent": "So we build an online interface called the Terminator that able users today tweets, tweets, data as most negative and neutral.",
                    "label": 0
                },
                {
                    "sent": "Sentiment.",
                    "label": 0
                },
                {
                    "sent": "And then we are able to extend our test set to 1000 tweets using this tool.",
                    "label": 0
                },
                {
                    "sent": "Later we implement our work in this paper as additional features here.",
                    "label": 0
                },
                {
                    "sent": "Additional services such as the semantic sentiment and Sentiment Edition.",
                    "label": 0
                },
                {
                    "sent": "Anyway, you can go online and check this tool Twitter.com.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, to conclude, sentences over trees data is really painful problem because of the special characteristics that I mentioned on Twitter.",
                    "label": 0
                },
                {
                    "sent": "We proposed using sentiment semantic features for Twitter using three different incorporation methods, replacement documentation, interpolation.",
                    "label": 1
                },
                {
                    "sent": "We found out interpolation outperform replacement and argumentation.",
                    "label": 0
                },
                {
                    "sent": "We also compare our features against three different baselines, and we found that in average that semantic features more precise than other features.",
                    "label": 1
                },
                {
                    "sent": "At the end I can say that there is no winning approach here or knowing features in this study.",
                    "label": 0
                },
                {
                    "sent": "It depends on the on the nature of the data sets.",
                    "label": 0
                },
                {
                    "sent": "So while semantic features perform better on large data set that contain general tweets and various topics, we can see also that or we can note that joint sentiment topic features outperform other features on smaller data sets that are centered around specific topics.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a future work.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "As a future work, so one bottleneck for approaches, this semantic concept extraction we use here Alchemy API that give that provides us with up to 30 constant different concepts.",
                    "label": 0
                },
                {
                    "sent": "These concepts are actually general.",
                    "label": 0
                },
                {
                    "sent": "We believe that if we can.",
                    "label": 0
                },
                {
                    "sent": "Choose the concept based on certain level of granularity.",
                    "label": 1
                },
                {
                    "sent": "This will improve their sentiment, classification accuracy or performance.",
                    "label": 0
                },
                {
                    "sent": "So we would like to explore more refined ways or tools to forensics, traction and entity concept mappings.",
                    "label": 0
                },
                {
                    "sent": "We also would like to propose selective interpolation method.",
                    "label": 1
                },
                {
                    "sent": "Actually here we interpolate all concepts again.",
                    "label": 0
                },
                {
                    "sent": "So we would like to interpolate the concepts that does matter or that do matter to the classic classification.",
                    "label": 0
                },
                {
                    "sent": "This could be measured.",
                    "label": 0
                },
                {
                    "sent": "Based on the contribution of each concept to the classification performance.",
                    "label": 1
                },
                {
                    "sent": "We also proposed here classifiers that attack sentiment from.",
                    "label": 0
                },
                {
                    "sent": "Sorry that detect sentiment positive and negative sentiment would like also to study providing more classy like hybrid classifiers that is able to also provide neutral sentiment detection.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "His references.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you, you can contact me on my email or Twitter and you can ask me whatever you would like to ask.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        }
    }
}