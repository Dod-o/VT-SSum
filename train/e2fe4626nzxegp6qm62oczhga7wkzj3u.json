{
    "id": "e2fe4626nzxegp6qm62oczhga7wkzj3u",
    "title": "Estimation of gradients and coordinate covariation in classification",
    "info": {
        "author": [
            "Sayan Mukherjee, Department of Statistical Science, Duke University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Output"
        ]
    },
    "url": "http://videolectures.net/oh06_mukherjee_egccc/",
    "segmentation": [
        [
            "Videos.",
            "Can only wait here from University.",
            "Long time were together at MIT with friends so.",
            "So yeah, I'm at Duke University.",
            "I mean the Statistics Department and also the Computer science Department and also the Genome Institute.",
            "This is mainly joint work with Chengwu and towards the end some of the work I'm going to talk about is also joint work with things Lanzhou, who's at the City University of Hong Kong.",
            "The problem itself and I'll try to define and explain what's going on and the title is not very informative yet, so I guess just open houses about complex outputs and what I'm going to do is really look at the problem of classification, where the outputs not so complex it's one or minus one, but I'm going to make it really complex.",
            "I'm going to start putting matrices and large vectors and doing all kinds of stuff like that.",
            "Not just 'cause I feel like it.",
            "There's actually a reason.",
            "The reason why is there's something fundamental in statistics called model identify ability and that you have a Model classic classical linear statistics right have a model.",
            "I have many variables.",
            "These variables have explanations like height, weight, heart rate, so on so forth.",
            "And when you build your linear model, maybe even number, you want some understanding or explanation.",
            "Of these variables OK.",
            "The paradigm that I tend to work in.",
            "Is what's called in statistics to large P small end paradigm where very few observations and I have thousands and."
        ],
        [
            "Dozens of variables.",
            "So save 30,000 variables and I have 100 observations OK. Well, I want to classify this.",
            "We have good classifiers SVM's Ridge regression, their variety of methods, right but but the problem with most of these methods is they don't tell me the importance of each of these variables and how they relate to each other.",
            "Because this problem it's underdetermined.",
            "It's it's not identifiable.",
            "So the issue is in this paradigm, how do I get back the sense of identify ability and that's going to be the problem that I'm going to go after.",
            "OK and.",
            "In this context are two classical questions that you're asking about.",
            "One is what's called variable salience, which is straight problem of variable selection.",
            "Which variables are important and the second one is.",
            "How do variables covary?",
            "OK, and again, I'm doing this in the context of 30,000 variables, 100 samples.",
            "OK, now there's a very practical reason why I want to do this, because I"
        ],
        [
            "Work in genomics problems.",
            "I work in problems where we have measurements of 30,000 genes from people who have cancer.",
            "Or you know, type a cancer type, beat cancer.",
            "Or often I measure 30,000 variables from particular cell line that's been perturbed in particular way.",
            "OK, so let's say I have one cell line that's been perturbed, one that hasn't been perturbed.",
            "This is my model system.",
            "What I'd like to get out of this model system are sub pathways.",
            "I'd like to see which genes are coordinate LeEco varying and I'd like to dissect the model with respect to these right and that's this issue of coordinate covariation and then later on what I'm going to do is take these.",
            "Different sets of genes, or these different models and I'm going to ask, say them in real human tumors to see how particular perturbations are varying in tumors late and the third aspect of that, you try to hit that with drugs.",
            "So this is a practical reason why I care about the South issue about coordinate covariation.",
            "Um, by the way, if anyone has questions or complaints or anything at any point, please."
        ],
        [
            "Feel free to say whatever you want, so I'm going to formulate this problem of learning covert coordinate covariation and relevance and the framework of shrinkage estimation or Tikhonov regularization and you'll see this will make my outputs complex OK."
        ],
        [
            "Let's set the framework up OK.",
            "The classic framework or classic framework for classification is the following.",
            "My input data X is a compact metric space in a P dimensional space.",
            "OK, my wiser to labels minus one one and my Z is simply a cross product on X cross Y sample Z is a draw.",
            "Typically ID of."
        ],
        [
            "And samples from this cross product space.",
            "Total institution is what do you mean by ID.",
            "So there is an underlying distribution.",
            "You'll see it.",
            "Yeah, it's not very little appear very, very soon, I promise.",
            "And then hypothesis space is a set."
        ],
        [
            "Functions F that go from the input to a real value, let's say Y OK.",
            "I've lost functional pushes from our cross are to positive values and smoothness functional from Maps function space to a positive number.",
            "Classic example is what's called the arcade."
        ],
        [
            "Just normal OK. And then I can write down a loss function so smooth."
        ],
        [
            "Is the regularization as a regular so I can write down the loss function?",
            "This is some penalty VFX I these are my training data.",
            "I see how close these guys are and this is some smoothness and this lambdas a trade off and this is convinced called maximum pastori estimator OK."
        ],
        [
            "So one of the classic way of penalizing smoothness or adding a regularization term is using what's called an arc.",
            "AHS Norm RK Trip stands for reproducing kernel Hilbert space.",
            "This particular structure, particular type of Hilbert space, and the second word in it is kernel, and this is an example of a kernel.",
            "It's a continuous symmetric positive definite function and an example is a Gaussian.",
            "OK, now the arc HS is."
        ],
        [
            "Basically, the linear span of, for example, these Gaussian.",
            "So you can put a calcium down over any data point you look at the span of that you look at the closure.",
            "This is the arc ahs and have very nice property in that I can take an inner product in the arc HS and it just gives me back a kernel.",
            "And again, this is a reproducing property.",
            "If I take a function F of XI, take its inner product with the kernel, I get back the function and some sets.",
            "It's acting like a Delta function, but the function itself is in the arcade."
        ],
        [
            "Yes, norm.",
            "And Lastly, this minimization problem that I had before."
        ],
        [
            "If I solve this optimization problem, I get something that's linear in the number of kernels and the data.",
            "OK, so this takes an infinite dimensional optimization problem down to an end dimensional.",
            "Optimization problem OK."
        ],
        [
            "So in the case of classification, a standard way of doing this is I have.",
            "I have Y is minus 1 + 1 and then my function sign of F. Is going to be my mapping from X to Y and a classic loss function that you can use is something called the logistic loss right?",
            "And that's just log of either 1 E to the minus YF of X.",
            "And this is really.",
            "You'll see it very soon.",
            "This is really assuming a Bernoulli model of your data, right?",
            "So each point X there's a probability that it's going to be one or minus one basically.",
            "So now I can rewrite this type of penalized loss function, right?",
            "And what's my?"
        ],
        [
            "The model is equivalent to saying that the labels of different points are independent of each other.",
            "Well, OK, so I haven't said anything about what's going on between X&X, right?",
            "I've just said that right now at each X it's a Bernoulli.",
            "OK, now what you will find is that.",
            "You will have a marginal on X, right?",
            "And then I'll have a conditional of why an ex right when I would have so man what I hope and what I will stress throughout this talk as I'm going to assume that this marginal is smooth, I'm going to assume that the conditional given the marginal is also smooth.",
            "Otherwise my life is really bad.",
            "And there's not much structure in the data, OK?",
            "OK, so my classification error.",
            "Simply looking at the sign of this function F right?",
            "Anna Bayes optimal classifier, let's call it F row of F role is the measure role of X, Y that's giving rise to the data.",
            "So it's going to have one if the probability that this conditional probability of y = 1 is greater than minus one, otherwise it's minus one very simple statement.",
            "And so then I can define this function F RO as a log likelihood.",
            "OK, so this is all well defined.",
            "OK, great."
        ],
        [
            "And in terms of convergence, what you expect is as this regularization term goes to zero as our number of sample goes to Infinity, the sign of our estimated function F should go to the sign of the classification function.",
            "One would hope that this happens, and this is a classical setting of classification, and now I'm going to make stuff comp."
        ],
        [
            "OK, I'm not going to try to just learn a classification function.",
            "What I'm going to do is I'm going to try to learn the gradient of the classification function and I will hopefully motivate that soon.",
            "Um?",
            "The gradient of the classification function.",
            "We assume that it exists, is basically the partial partial derivatives.",
            "Thinking of it as a column vector.",
            "That's basically it.",
            "What am I going to do with this thing?",
            "Well, I can do variable selection by looking at the norms of each of the partial derivatives, right?",
            "Because that's telling me how much the classification function is changing with respect to one of the coordinates.",
            "And then I can look at coordinate covariation by looking at the inner product between two of them between let's say the 1st and the 10th series would be telling me with respect to variation in the function."
        ],
        [
            "Is a variation with the coordinates OK?",
            "This is reasonably clear to people.",
            "OK, great, This is why I'm learning the gradient.",
            "How can I motivate learning the gradient?"
        ],
        [
            "Well, you know like you do any other grade and you do Taylor expansion.",
            "Let's say if I knew what the.",
            "The classification function were right at a point X. I'm sorry to point if I wanted to know datapoint acts.",
            "What I can do is Taylor expanded point unirex right?",
            "And it's just going to be the dot product between this difference and the gradient.",
            "OK now."
        ],
        [
            "What I'm going to do is estimate.",
            "I need to estimate F Ro because this is here I'm going to estimate by function G and I'm going to estimate its gradient by this efec.",
            "OK, so this basically.",
            "Same thing here.",
            "We written here with G as my classification function estimate an effect as my gradient estimate.",
            "OK, now this has to hold for points near each other, right?",
            "You don't want to be doing Taylor expansion on points very very far away because, well, this is bad.",
            "So the way we enforce."
        ],
        [
            "It is a process that is a classification function which are not given.",
            "Right, but it's a class."
        ],
        [
            "Application function.",
            "That's your log of the the log, the exactly the log likelihood ratio.",
            "If I knew the true probabilities, if I knew the true conditional probabilities right, I could take the log likelihood ratios and that would be.",
            "That would be a base optimal classifier exactly, but I don't know that.",
            "Yeah, I'm going to assume it's worth it.",
            "The sign of it would be the sign of it would be the Bayes optimal exactly.",
            "Smooth to get this special.",
            "OK. OK, so this is just imposing some locality.",
            "This is a Gaussian.",
            "It's got this funny little P + 2 if anyone wants to know why that extra P + 2 comes in, I can tell him at the end, but it's for technical reasons, but it's effectively a Gaussian OK. And let's say I use logistic again loss function, I can write down this empirical minimizer OK, which is just saying that while I write times my G, which is my estimate of the classification function times my gradient estimate minus difference right?",
            "These have to be going in the right direction.",
            "OK, so classically what you see here is why I would see why I I'm sorry F of.",
            "Just F of XI, right?",
            "But I'm just here approximating F of XI by this Taylor expansion.",
            "OK?",
            "And then I waited by how close things are.",
            "OK, that's basically all I'm doing here.",
            "Yeah, so one thing I'd like to note is if are actually doing the regression problem, it would be a little bit simpler because I wouldn't have to estimate the underlying classification because I'd actually have the two regressors.",
            "But because I only have ones and minus ones, I also have to estimate this underlying."
        ],
        [
            "Classification function OK Now the last thing I need to do is define in Arc HS norm and the way I'm just going to do that is by simply.",
            "Writing down an arcade, just Norman.",
            "Each of the coordinates, right?",
            "And then just summing that up?",
            "Overall coordinates OK, you can do something more complicated.",
            "Marcy and Charlie had been doing some very nice work on vector valued functions, and you can do things like that, but we're just going to do a very simple thing, OK?",
            "It's just a logistic.",
            "I should.",
            "Should it be XJ in this F?",
            "Yeah, you're right.",
            "Yeah, I always get these things yeah, yeah yeah, you're right, thank you yeah yeah, yeah, sorry I. GOG is my estimate of the classification function.",
            "Yeah, the likelihood ratio, the log likelihood ratio OK?",
            "OK, so now finally have an algorithm.",
            "OK, I'm going to simultaneously estimate the classification function and its gradient by basically minimizing."
        ],
        [
            "This is empirical error.",
            "This is a regularization on the classification function.",
            "This is a regularization on the gradient OK and I have three regularization parameters.",
            "OK.",
            "So one very natural."
        ],
        [
            "And as well, why are you doing all of this right?",
            "Why don't you just estimate the function in the 1st place and take partial derivatives would be anything that any sensible person."
        ],
        [
            "Do instead of doing all this bullshit, however, there's an issue there.",
            "First of all, empirically, it's much better to put the constraints on the derivatives.",
            "It turns out if you go to higher, higher dimensions because you have better control of each of the individual dimensions.",
            "Um?",
            "So the deeper reason though is that the function FO is going to be in RKHS.",
            "In general its partial derivatives will not be OK. That can still be answer if someone can tell me well you can use a sobolov space which has particular structure.",
            "Then I know the derivatives will be in RKHS or something like that.",
            "That is true, but then the inner products are not well defined because you don't know what coordinate system you're working in and I'll get to this like 4 slides or five slides down the road and so doing this very particular construction we have everything well defined.",
            "So our mathematically or coordinate systems for these inner product are very clear, so that's that's really the main reason why we do this and empirically it works a lot better OK?",
            "So.",
            "If I look at this optimization, I'm going to get two functions out of it, right G&F back."
        ],
        [
            "And what you'll see is both of these have a representative theorem and that they can be written as a linear combination of kernel functions.",
            "The classification function G in this case aiz.",
            "These are scalars, as you would expect them to be in the classical theory.",
            "In the case of the gradient well, these should be vectors, right?",
            "Because this thing is a vector, and so it's a vector, and all is clear.",
            "You can compute."
        ],
        [
            "If you look at this kind of semi naively, well, you can say OK, I can use Newton's method to compute this, but it looks like a really ugly optimization problem 'cause it's an RMP by RNP, and being let's say, two what's 100 or 20 between 20 and 100 P being 30,000.",
            "You feed this into Matlab.",
            "It's not pretty at all, however.",
            "How can you reduce the matrix size?",
            "Well, we can rewrite the objective function as an objective function fee of C and Alpha."
        ],
        [
            "Right, and this is just everything in matrix form.",
            "Basically, take the gradient, set it equal to 0 now."
        ],
        [
            "Here is your see here is this matrix of cizia exactly an Alpha?",
            "Is the vector?",
            "Now if you look a little bit more carefully what you'll see there's a key quantity in this, and these are these points XI minus X JS and what you'll see is really the solution of this problem is going to be.",
            "In the span of differences between points, because if you see up here I have the XI minus XJ, right?",
            "And then I'm waiting it by thing.",
            "So really I'm in some sense in a subspace of all of the differences of my data, right?",
            "And so this says that, OK, well this is going to have rank of at least D N -- 1 right?",
            "So optimization problem is really RND by RND so it's like almost like an SVD.",
            "You can do an SVD and then look at this much lower dimensional space that's got runtime order MD squared in memory order NP and we can solve this if you.",
            "I have thousands and thousands of genes which I have in not so many samples, so it's nice.",
            "OK. And just as a side note, my colleague Chung implemented this he before he was doing pure theory.",
            "Before this I gave him some code and it was amazing.",
            "Two days Matlab code worked really well.",
            "It's quite impressed anyways.",
            "So then one asks, well, OK, this is good.",
            "Can you prove something about the convergence of the gradient?",
            "Can you say that this is consistent or something like that, OK?",
            "And we can we get horrible rates.",
            "But let me tell you about the horrible rates 1st and then let me tell you about some of the things you need to."
        ],
        [
            "Able to prove this.",
            "OK, so the classic you know, if you look at classical generalization bounds, I'd say something like classification function minus the true classification function F RO the L2 norm of this with respect to the measure X is going to be something of the order of enter the minus 1 / 2.",
            "If you're really lucky, you can get minus one something in between, right?",
            "We get something to the minus 1 / P and this is with probability and then we get something similar for the great convergence of the gradient.",
            "To the true gradient OK.",
            "But we need certain requirements.",
            "First requirement that we require is basically that the density corresponding to the distribution that generates the data is basically Holder continuous and you need this because if you think about it, if you have a density that's jumping up like this and going through right, you can't estimate gradients there.",
            "It's nonsensical, you have no hope.",
            "The other thing that you have to do is worry about the basically the edge.",
            "If all of your measures concentrated right around the edge.",
            "Again estimating a gradient.",
            "Is completely it's very problematic and nonsensical, so if it is the case that the density is let's say something like Holder continuous or has some smoothness right, then both of these will be met and will be OK and we can get these type of results.",
            "OK again these are really horrible.",
            "Before I try to improve this and tell you how you can think about improving this, I'd like to show you some toy in real data that we ran this on OK, actually, before I go on the classic way of improving these type of rates, if you look at generalization bounds you assume something about the noise that integrates ebook off noise conditions.",
            "There are various ways about doing this, there are more classical ways where you can say you have 0 error, so on so forth.",
            "That's not how we're going to improve rates.",
            "We're going to assume that there is strong input structure right?",
            "In that really you have a low dimensional manifold.",
            "And I'll get to that in a second.",
            "OK, so."
        ],
        [
            "One quantity that could be of interest is basically if I look at the norm of any one of the directional derivatives, we normalize it by the norm of all of the directional derivatives.",
            "That tells me in some sense how important one of the directional derivatives is.",
            "Another quantity that's of interest."
        ],
        [
            "And one of the main quantities will come back to is what we call an empirical covariance matrix, which is inner product between two of the directional derivative.",
            "So here I have the case directional derivative, the elf directional derivatives take its inner product and I can construct the matrix of this for all of these OK.",
            "So let's look at the linear example.",
            "This first linear example is a linear example, which could have been addressed with PC's."
        ],
        [
            "Well.",
            "But will show how it works in our method.",
            "OK, I have the first 10 coordinates for class one are normal 1.51 next ten R -- 3 one the rest is noise simile for class minus one.",
            "I get something similar.",
            "I think these signs should be flipped, but if you look at this this is what the."
        ],
        [
            "Data looks like OK, so these are first class one class two OK. And then if I look at the RK chest norm, that's the output of my algorithm.",
            "Hello represent oh I'm sorry, blue is very negative.",
            "Red is very positive.",
            "OK, so this is sample 1, sample 2, up to sample 40 and if you notice the 1st 20 samples are pretty similar.",
            "Last 20 samples are pretty similar to this one.",
            "Classifying this one and if you look at the arcade just over the different dimensions, UCI pick out the first 10, then 10 through 20, then 40, two 5050 through 60.",
            "So this looks good, but PCA would have worked here.",
            "OK, this is a case where I added I went the wrong way."
        ],
        [
            "Few more things.",
            "This is what the covariation matrix looks like and you see it blocks beautifully, so you see which variables are covarying together and this is the conditional probability that my classification function is spitting out and see it does well.",
            "It's an easy example, why shouldn't it?",
            "If I make it more noisy?"
        ],
        [
            "This case is so noisy that PCA would not work because the variance in all of the dimensions is about equal.",
            "So you really need to know the labels right and what you're going to see is, well, we do better, but we should do better.",
            "We have the label information right?",
            "So we pick up the first 10 in the next 20 and the 50 in the 60.",
            "But everything is more noisy because, well, that's more noisy.",
            "And you get similar results on the Covariation matrix and also the."
        ],
        [
            "OK, now let me go to a non than your example."
        ],
        [
            "Plus 112, I'll just show you what it looks like.",
            "There are 200 dimensions.",
            "The 1st two are useful.",
            "The rest of them are not."
        ],
        [
            "This is what it looks like in the first two class, one is red.",
            "+2 is blue, so you have a sign in the cosine right, and then if you look at the empirical covariance matrix you see that you know the first one in the 1 one and one and 2 two are important and if you see one one and 1 two, they don't covary together because sine and cosine orthogonal so you get back what you expect and then you get peace."
        ],
        [
            "If you rank the the partial derivatives, the 1st two are big, the rest are small, so it starts giving you back what you want."
        ],
        [
            "And then this is what my classification function did without any variable selection.",
            "So then if I go back and let's say, select the first 34 variables, you get a better result.",
            "OK, so this is kind of the framework that we're going to work in and this is gene expression data."
        ],
        [
            "That everyone's beaten to death but because everyones beaten to death, I have to log in it one more time.",
            "You have 38 training examples in 35 test examples.",
            "It's in a 7129 dimensional space.",
            "Two types of leukemia.",
            "We used 35 for the test, done 38 for the training, 35 for the test and all this."
        ],
        [
            "Picture is basically really classic method in statistics and machine learning is something called backward selection or forward selection.",
            "That's all.",
            "This is right here.",
            "We've said OK if we look at our classification function with all of the genes.",
            "This is what it is.",
            "If I go down you get rid of the bottom 4129 using the.",
            "The norm of the partial derivatives that I get one error and so on so forth.",
            "And this is what any reasonable algorithm would be like.",
            "We're not doing any better than anyone else.",
            "We're not doing any worse than anyone else.",
            "Unless you're an idiot, this is what you're going to do.",
            "But the main point is what you are getting in this method is you're getting this covariation which you wouldn't otherwise OK.",
            "So this is compared to another method.",
            "This is something that the computational biology community used a lot space."
        ],
        [
            "Cyclea T statistic.",
            "For each gene, I compute the means of Class 1 to subtract off the mean of the Class 2, divide by standard deviations, scale that in the same way.",
            "OK, and then what you see."
        ],
        [
            "It's kind of faded red is, I think ours blue.",
            "Is that other ones that eat."
        ],
        [
            "Type statistic and what you see is in general the decay of our methods a lot quicker be this could be this bad, it is what it is.",
            "If you actually run an L1 norm SVM, it'll do a lot like this in terms of its decay.",
            "A classic L2 norm will be a bit more like this, but it will still decay faster.",
            "Decaying for all the decay of the norms of the partial derivatives.",
            "Now, so far I've been working in an ambient space.",
            "I've gotten these, you know, these convergence results at 10 to the minus 1 / P. And if I don't assume anything, I can't do better, OK, but assuming all of the coordinates are independent, is a stupid B not true and see defeats the whole point of what I'm trying to do?",
            "I'm trying to understand how these coordinates.",
            "You know how they vary with each other, how they occur.",
            "OK, so I'm going to assume that the data is concentrated on a manifold.",
            "Which is a subspace of RPM.",
            "Going to assume somehow this manifold is really concentrated on some D dimensional space, OK?",
            "Now, if I'm given a smooth orthonormal vector field."
        ],
        [
            "This D dimensional space E1 through EDI can define a gradient on the manifold OK, and that's basically this.",
            "OK, now what I'm really going to try to do now is basically."
        ],
        [
            "Doing the Taylor expansion on the manifold.",
            "OK, what I need to do that is for each point on the manifold is I need a chart which takes the local area around the point on the manifold into the ambient space.",
            "OK, I'm sorry into Rd OK and that's just what I have here and it has to satisfy basically these normal coordinate conditions.",
            "OK so I just have to have some normal coordinate for every point on the manifold and this is natural because you can't really define the gradient unless you have normal coordinates.",
            "Anywhere I mean it didn't get space or being in a manifold and so then I can write a Taylor expansion around.",
            "On any point P in terms of this chart, in these normal coordinates OK, However, there's a problem here in the."
        ],
        [
            "None of these.",
            "I don't know what the manifold is.",
            "I'm not given this normal coordinate system or this local expression because I don't get the manifold I get data in the ambient space, right?",
            "And So what can I do well?"
        ],
        [
            "First thing is I have to assume that there's an embedding that Maps from the manifold back to the ambient space OK, and then what I have in some sense in reality the data is drawn from the manifold, but I'm not given the local expression of the data, but I'm given its image right.",
            "I'm given the image map of this onto the ambient space.",
            "OK, which is an RP.",
            "What I can show?"
        ],
        [
            "Is if I assume?",
            "That this embedding has an inverse.",
            "OK, I need to assume that then I can rewrite the Taylor expansion here, which is really in terms of Adi dementional space.",
            "But in the coordinates, the normal coordinate systems of the manifold I can rewrite this as a Taylor expansion of X in a P dimensional space, which is a composition of my function F and.",
            "The inverse map, and that's what I've written here.",
            "OK, so this is 2 purposes.",
            "This serves two purposes."
        ],
        [
            "Um?",
            "The first purpose that it serves, which is more important, is that it tells us that our algorithm does not need to be changed for the manifold setting.",
            "You can be in the ambient space.",
            "You can be in the manifold setting.",
            "The same algorithm is working in both cases, and the reason why is because of the locality of the Taylor expansion, right?",
            "You're looking at everything locally and that's why you get this thing back OK, and then you can prove that you get rates of convergence on the terms of the dimension of the manifold.",
            "OK, so this is good.",
            "However I still have a problem.",
            "What have I given you?",
            "I've given you a P dimensional vector, right?",
            "But we know that the manifold somehow relies on a D dimensional space and I really somehow want to understand this D dimensional structure and the geometry of this D dimensional structure.",
            "So I want to somehow be able to get this back.",
            "So how can I get this back?"
        ],
        [
            "So one way of getting this back is to look at the what we call the empirical covariance matrix, which is just the inner product of each of the partial derivatives OK?",
            "So.",
            "You can use this covariance matrix similarly as you.",
            "You would use what's called the data or the design matrix in PCA.",
            "So what do you do in PCA?",
            "You take your."
        ],
        [
            "Right, it's called X.",
            "You multiply X by X transpose, right?",
            "And you get a covariance matrix and then you look at eigenvectors and eigenvalues of this right?",
            "And this in some sense says that these are the dimensions of the greatest variance, so we're going to do the same thing here.",
            "OK, Ann."
        ],
        [
            "And this this method or this idea is very very similar to some of the supervised non linearity dimension nonlinear dimensionality reduction methods that have been thought of recently like local linear amending or ISOMAP or Laplacian eigen Maps, and very very closely related to something called Hessian.",
            "Eigen Maps.",
            "OK so.",
            "This is a proposition you don't really need to read it in detail.",
            "What it's saying is if I have a function on RP and I assume it's gradient exists, then I have vector V&R."
        ],
        [
            "It's a Kate important feature.",
            "If it's Norm is 1 and there exists no K -- 1 previous to it, all of them with norm one.",
            "Basically such that this one if I take the inner product with the gradient in the L Infinity norm, it's bigger than the K + 1 one, but smaller than the previous ones.",
            "OK, and this is looking at it in the Infinity Norm.",
            "However the Infinity Norm is hard to work with and it may not be the good norm in the 1st place.",
            "So if we replace the Infinity norm with the RK chest norm.",
            "Then the Keith important feature is simply the eigenvector corresponding to the Kate eigenvalue of the covariance matrix.",
            "So this is just in some sense a theorem version of why something like PCA makes sense in this setting.",
            "OK, and."
        ],
        [
            "This proposition suggests that we project our data on the top K eigenvectors.",
            "Now what K is a good question, but I won't answer that now 'cause I don't know how to.",
            "But again, the space projection down to this K dimensional space should reflect the geometry of the classification.",
            "You can do something similar for regression.",
            "OK, one good thing is you actually don't have to construct the P by P matrix, because if you remember, again everything is of rank D, so."
        ],
        [
            "It turns out that you can compute this thing in order N squared P and cube time and order PN memory.",
            "So this is a good thing and I don't have pictures yet because we just work this out.",
            "But on some data set it actually looks quite nice OK?",
            "You were saying you wanted to get back to the full space.",
            "We were actually getting.",
            "Down to lower dimension I get down.",
            "Yeah so so with my gradients, right?",
            "I'm in P dimensions right now.",
            "What I've done is I've somehow projected down to D or K dimensional space where I hope all of the geometric structure of the data lies.",
            "OK, OK. Yeah, in some sense.",
            "Ideally you'd always like to really get back this D dimensional manifold, but I don't see how to do it.",
            "I just don't.",
            "I don't see.",
            "How to do it?",
            "I mean, I can see that getting projections that reflect the geometry, but I don't see how to get the full dimensional dimensional manifold.",
            "Let's see, so we still have more to do, I'm."
        ],
        [
            "I'm working on a basean version of this.",
            "For certain reasons, in that I do a lot of Asian stuff, but also.",
            "It gives me a different way of looking at it.",
            "Maybe in the similar way that PC has a probabilistic interpretation.",
            "I could get a probabilistic interpretation of it, and actually more importantly is it maybe give me a way to get confidence intervals for some of those estimates so I can kind of get an error bar and see you know which of these coordinates are really varying more or less OK. Other things we have formulated, the semi supervised version.",
            "We'd like to implement it using the semi supervised version.",
            "For example.",
            "Maybe we can actually get an idea for what the real dimensionality of the manifold is, but we're not sure that's one of the things we're looking at.",
            "So we think that this has a relation to information geometry and this covariance matrix is a particular case of."
        ],
        [
            "Non parametric analogs of Fisher information matrix.",
            "And I think that's it, so I guess it was short, but.",
            "There's actually a strong relation with this work, and some of the work that people."
        ],
        [
            "I've been doing on manifold regularization.",
            "This is specifically some of the work by Parthan Yogi and Misha Belkin, or they have been doing.",
            "Do we have a?",
            "Great thanks, so they've been looking at the following.",
            "Maybe we shouldn't be using this?",
            "Do you want some soda water?",
            "Try some mineral water.",
            "Chill.",
            "Yeah, that works.",
            "Not well, yeah.",
            "So let's see.",
            "I haven't some other slide.",
            "Let's see if I still have.",
            "I can write on the sheet.",
            "In the end, I could not reconstruct the manifold.",
            "So they have a slightly different problem.",
            "They they can get to.",
            "So yeah, let me write down what they have and then So what they have is they have a loss function.",
            "Sum over, you know I equals one through N the votes just even do square loss F of X I -- Y ^2.",
            "Then they have an extra term which is some over IJWIJ can think of it the same exact way to what I said before and this is over all of the unlabeled data.",
            "So this is one through N + M. Ambien, being the number of labeled data and being the more unlabeled data and then you can write this as F of X I -- F of XJ.",
            "Squared OK, and this is basically a graph.",
            "Laplacian OK, right?",
            "It's a wait, again.",
            "Preserve locality.",
            "The distance between exciting.",
            "I do.",
            "It's embedded in WHA that's in WSJ.",
            "So this is a way which is some locality or OK.",
            "So in some sense OK, this this this operator the graph Laplacian.",
            "You can think of as a approximation right of Dell.",
            "Right?",
            "Which is the Laplace continuous version which is a loss.",
            "Beltrami operator OK now.",
            "So this is what they're looking at.",
            "This is what they're constructing and.",
            "In their paper, for example, where they did Laplacian Eigen Maps, they didn't get back the manifold they got back in projection which will preserve distances on the manifold locally locally, which we will do as well.",
            "But it's slightly different in that we have the label information as well the key quantity that we look at is basically something like Dell.",
            "F Delta X you can think about it.",
            "So we have the gradient right plus differences of the functions, right?",
            "So this is in some sense the key quantity in their work.",
            "This is a key quantity in our work right?",
            "And then once we take an inner product of this Dell F with this Dell F, we get the full Hessian, whereas they have the Laplace operator, right?",
            "So that's a question, there's there's a very good reason why in their work they use a Laplace operator because this is invariant with respect to coordinate change, right?",
            "So they want to study this thing.",
            "In our case we do not want something invariant with respect to coordinate changes.",
            "We're trying to learn the coordinate system or natural coordinate system for the framework, which is why we look at.",
            "You know, slightly different operators from that perspective.",
            "Enter.",
            "Can you say something a bit more about why, what, how it was applied?",
            "I mean, what's the why this was useful?",
            "OK yeah, yeah.",
            "How much time do I have?",
            "Just out of curiosity?",
            "OK, so let me show you actually something about why.",
            "I'll be quick, I promise.",
            "Where do I have this?",
            "Country videos Oh yeah.",
            "Radios who is this what?",
            "I know this is the wrong one.",
            "Annoying.",
            "Oh, it can't open it.",
            "OK, well, I can't get it to open, but.",
            "I know why I'm wrong.",
            "OK, that was much better.",
            "OK.",
            "So the actual application of this that we're really looking at, and this is actually something that we have right now in the preclinical trials.",
            "We're looking at lung cancer.",
            "OK, and we're actually trying to find a way of assigning patients particular risk number one using classical classification methods, and then also if we think someone's going to recur, how to give them particular type of treatments?",
            "OK, and that comes in something called the pathway paradigm and the pathway paradigm is where we're actually using trying to.",
            "Break these things apart how they covary so we can understand what the different pathways are.",
            "This is just some bullshit you should doctors.",
            "Estimates please.",
            "How do you get so OK?",
            "Yes, choose.",
            "How do you get ground truth?",
            "This is a very good question.",
            "So typically what OK.",
            "So the classic leasee is that you make a prediction or they have some stratification of people who get sick or won't they make these two type of curve say they're different but that doesn't predict how an individual is going to be right, which was your question.",
            "OK so basically what we actually really have is we come in with patients they see.",
            "We see patients.",
            "They might be stage 1A or higher.",
            "So The Who World Health Organization has a way of classifying patients as to whether they're really going to be badly off or less badly off.",
            "Depending on how big the tumors are there 14 variables that they do use do this OK. And then if your stage 180 actually observe you to cut the reset the tumor, then they observe you.",
            "If you're higher, they give you chemotherapy.",
            "What's nice about this is that if we think that someone is going to relapse, it's a lot easier to give them chemotherapy than in the other way around, right, doctors are more willing to give treatment, and so basically we have.",
            "Patient data from this we have people that are, let's say alive.",
            "After five years, people that are dead after 2 1/2 years we have.",
            "I think 90 odd patients and what you find is if you try to use just the 14 variables at The Who uses, you are not so accurate, OK?",
            "Predicting whether a person we.",
            "Re occurrence.",
            "Yes, there is intervention therapy.",
            "There is reception in between.",
            "OK, so they cut the tumor out.",
            "If you're one or they don't give you chemo if you're not one.",
            "They do give you chemo.",
            "OK, So what we look at is given everyone, OK.",
            "They recur or not OK, this is our.",
            "How accurately can we predict this?",
            "And if you use just a 14 variables by The Who you're about 61% correct.",
            "OK, in terms of recurrence now some of those people were stage one way.",
            "Some of them were stage higher.",
            "We don't distinguish yet.",
            "OK, now if you use all of the variables to 20,000 or whatever, use basically with SVM, some kernel method or regularize logistic, this is overrated.",
            "It's really not 94%.",
            "It's like 85.",
            "Really this is a bit of overfitting 'cause it's a leave one out and it's cheating, but it's really around 85, so you do a lot better now.",
            "So then the idea is you know patients are you predict to not do well.",
            "You re classify as high risk and you give them chemo.",
            "OK so that's kind of the basic part, but the part where we start to use these?",
            "False negative there is.",
            "Yeah, I don't exactly know.",
            "But actually in this case it was about equal.",
            "It was actually equal and for this model, but it's usually different, but in this one I think it was actually about equal.",
            "But the doctors don't like it that way.",
            "They wanted more confidence in one and the other, but I forget right now.",
            "So where where this covariation comes in is a question of, well, if you give people chemo, some of them will relapse and what do you do about them?",
            "You just can't predict all you're going to relapse, you know.",
            "So I mean, it's probably want to do something more helpful, right?",
            "And so there this is this issue.",
            "The biology idea behind this is that.",
            "Jeans are kind of occurring together.",
            "Their particular oncogenes if you hit an oncogene there going to be other jeans that are affected by it, right?",
            "Some of the classical ones are asmik there.",
            "There are variety of these things.",
            "So the idea is if I can go into cell lines and decompose and understand what the models are for these particular ones and then say these things in tumors, right?",
            "Then I can start decomposing this.",
            "So an example of this is they took.",
            "This is work done by Andrea Bildenden.",
            "Joan Evans is lab at Duke.",
            "They basically took human cell lines.",
            "I think these are breast cell lines.",
            "And half of them, let's say with with an oncogene wrasse by basically by some biochemistry and the rest of them were hit by null thing that doesn't do anything.",
            "This is done on the chip, but these are in cell lines.",
            "If these are not in human tumors right and so then you have these things and you build models and these are what in some sense we're trying to get after what this covariation method is you want to break apart into pathways and sub pathways.",
            "Now that I have this model I can go back into the human data and look at OK. Well, do some of these models.",
            "Are they more or less expressed in the human cancer so I can break up the human cancer as these models you can think of them as factors if you did principle components.",
            "These are mathematical factors.",
            "Statistical factors.",
            "These are in some sense, biological factors that you're going to be decomposing your tumors in terms of and this is what they look like and.",
            "Anyway, so that's kind, and then the last thing is for some of these pathways or these oncogenes we have drugs that can hit them, and so that's the last part of it.",
            "You use pathway specific drugs, but that's the application that we are going after.",
            "And that's the idea.",
            "Using those correlations is to really try and say OK, these two genes are highly correlated.",
            "Therefore there in the same path.",
            "That's what I'm arguing 1st order on saying jeans by statistical definition of a pathway are genes that are significantly differentiated between the two conditions and coordinate LeEco very or Co Express, and I'm hoping 1st order.",
            "That's an approximation of what a biological pathway is, because no one can define that.",
            "So in fact, when you say you do.",
            "Principle components analysis and you get K dimensions that tells you there are sort of K independent parts.",
            "That would be my would be my statement.",
            "I mean Lucy.",
            "I don't know how to get K yet.",
            "I mean there are lots of things I don't know, but you know we're working on it and then you can actually look at which genes are involved in each of those pathways and maybe see if we can hit one of them or yeah.",
            "Or if we know that those belong to particular oncogenic pathways.",
            "If they overlap heavily with particular oncogenic pathway.",
            "So on and so forth or I mean or in some of them.",
            "I even know that it is because.",
            "That's how they constructed it and they went into a cell line to hit it with a very particular perturbation, which they know is an oncogene or particularly relevant gene, and so that's why.",
            "Don't you ever.",
            "Every issue is trying to validate that I have a very strong big issue with trying to validate my conclusion.",
            "I mean we're doing this pre clinical trial.",
            "This is the closest thing you get to validation, right?",
            "We're going to have.",
            "I think 200 patients and try to do this and see if we actually improve their health.",
            "I mean not screw them up.",
            "I mean this is, you know your predictions.",
            "Actually, you know.",
            "Sure, yes.",
            "Yeah, so who see in any way coming out?",
            "But more subtle definition part?",
            "I mean, you know independent pathways or somehow?",
            "I mean, uh, presumably in in the real cell there are lots of interleaving, yeah?",
            "Tree model, right?",
            "That's PR.",
            "You can do a hierarchical model.",
            "There are people who do Bayesian factor models.",
            "There are many things you can try to do out of this.",
            "I don't know.",
            "I mean, yeah, I just don't know how to do partitioning yet or things like that.",
            "I mean, I'm still thinking about that, but yeah, I mean this is just one model.",
            "I mean I could put different structures on how things are related and do different things.",
            "I haven't gotten there yet, so yeah.",
            "Anyways, that's that."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Videos.",
                    "label": 0
                },
                {
                    "sent": "Can only wait here from University.",
                    "label": 0
                },
                {
                    "sent": "Long time were together at MIT with friends so.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I'm at Duke University.",
                    "label": 0
                },
                {
                    "sent": "I mean the Statistics Department and also the Computer science Department and also the Genome Institute.",
                    "label": 0
                },
                {
                    "sent": "This is mainly joint work with Chengwu and towards the end some of the work I'm going to talk about is also joint work with things Lanzhou, who's at the City University of Hong Kong.",
                    "label": 0
                },
                {
                    "sent": "The problem itself and I'll try to define and explain what's going on and the title is not very informative yet, so I guess just open houses about complex outputs and what I'm going to do is really look at the problem of classification, where the outputs not so complex it's one or minus one, but I'm going to make it really complex.",
                    "label": 0
                },
                {
                    "sent": "I'm going to start putting matrices and large vectors and doing all kinds of stuff like that.",
                    "label": 0
                },
                {
                    "sent": "Not just 'cause I feel like it.",
                    "label": 0
                },
                {
                    "sent": "There's actually a reason.",
                    "label": 0
                },
                {
                    "sent": "The reason why is there's something fundamental in statistics called model identify ability and that you have a Model classic classical linear statistics right have a model.",
                    "label": 0
                },
                {
                    "sent": "I have many variables.",
                    "label": 0
                },
                {
                    "sent": "These variables have explanations like height, weight, heart rate, so on so forth.",
                    "label": 0
                },
                {
                    "sent": "And when you build your linear model, maybe even number, you want some understanding or explanation.",
                    "label": 0
                },
                {
                    "sent": "Of these variables OK.",
                    "label": 0
                },
                {
                    "sent": "The paradigm that I tend to work in.",
                    "label": 0
                },
                {
                    "sent": "Is what's called in statistics to large P small end paradigm where very few observations and I have thousands and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dozens of variables.",
                    "label": 0
                },
                {
                    "sent": "So save 30,000 variables and I have 100 observations OK. Well, I want to classify this.",
                    "label": 0
                },
                {
                    "sent": "We have good classifiers SVM's Ridge regression, their variety of methods, right but but the problem with most of these methods is they don't tell me the importance of each of these variables and how they relate to each other.",
                    "label": 0
                },
                {
                    "sent": "Because this problem it's underdetermined.",
                    "label": 0
                },
                {
                    "sent": "It's it's not identifiable.",
                    "label": 0
                },
                {
                    "sent": "So the issue is in this paradigm, how do I get back the sense of identify ability and that's going to be the problem that I'm going to go after.",
                    "label": 0
                },
                {
                    "sent": "OK and.",
                    "label": 0
                },
                {
                    "sent": "In this context are two classical questions that you're asking about.",
                    "label": 0
                },
                {
                    "sent": "One is what's called variable salience, which is straight problem of variable selection.",
                    "label": 0
                },
                {
                    "sent": "Which variables are important and the second one is.",
                    "label": 0
                },
                {
                    "sent": "How do variables covary?",
                    "label": 0
                },
                {
                    "sent": "OK, and again, I'm doing this in the context of 30,000 variables, 100 samples.",
                    "label": 0
                },
                {
                    "sent": "OK, now there's a very practical reason why I want to do this, because I",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work in genomics problems.",
                    "label": 0
                },
                {
                    "sent": "I work in problems where we have measurements of 30,000 genes from people who have cancer.",
                    "label": 0
                },
                {
                    "sent": "Or you know, type a cancer type, beat cancer.",
                    "label": 0
                },
                {
                    "sent": "Or often I measure 30,000 variables from particular cell line that's been perturbed in particular way.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say I have one cell line that's been perturbed, one that hasn't been perturbed.",
                    "label": 0
                },
                {
                    "sent": "This is my model system.",
                    "label": 0
                },
                {
                    "sent": "What I'd like to get out of this model system are sub pathways.",
                    "label": 0
                },
                {
                    "sent": "I'd like to see which genes are coordinate LeEco varying and I'd like to dissect the model with respect to these right and that's this issue of coordinate covariation and then later on what I'm going to do is take these.",
                    "label": 0
                },
                {
                    "sent": "Different sets of genes, or these different models and I'm going to ask, say them in real human tumors to see how particular perturbations are varying in tumors late and the third aspect of that, you try to hit that with drugs.",
                    "label": 0
                },
                {
                    "sent": "So this is a practical reason why I care about the South issue about coordinate covariation.",
                    "label": 0
                },
                {
                    "sent": "Um, by the way, if anyone has questions or complaints or anything at any point, please.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Feel free to say whatever you want, so I'm going to formulate this problem of learning covert coordinate covariation and relevance and the framework of shrinkage estimation or Tikhonov regularization and you'll see this will make my outputs complex OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's set the framework up OK.",
                    "label": 0
                },
                {
                    "sent": "The classic framework or classic framework for classification is the following.",
                    "label": 0
                },
                {
                    "sent": "My input data X is a compact metric space in a P dimensional space.",
                    "label": 0
                },
                {
                    "sent": "OK, my wiser to labels minus one one and my Z is simply a cross product on X cross Y sample Z is a draw.",
                    "label": 0
                },
                {
                    "sent": "Typically ID of.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And samples from this cross product space.",
                    "label": 0
                },
                {
                    "sent": "Total institution is what do you mean by ID.",
                    "label": 0
                },
                {
                    "sent": "So there is an underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "You'll see it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's not very little appear very, very soon, I promise.",
                    "label": 0
                },
                {
                    "sent": "And then hypothesis space is a set.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functions F that go from the input to a real value, let's say Y OK.",
                    "label": 0
                },
                {
                    "sent": "I've lost functional pushes from our cross are to positive values and smoothness functional from Maps function space to a positive number.",
                    "label": 0
                },
                {
                    "sent": "Classic example is what's called the arcade.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just normal OK. And then I can write down a loss function so smooth.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the regularization as a regular so I can write down the loss function?",
                    "label": 0
                },
                {
                    "sent": "This is some penalty VFX I these are my training data.",
                    "label": 0
                },
                {
                    "sent": "I see how close these guys are and this is some smoothness and this lambdas a trade off and this is convinced called maximum pastori estimator OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one of the classic way of penalizing smoothness or adding a regularization term is using what's called an arc.",
                    "label": 0
                },
                {
                    "sent": "AHS Norm RK Trip stands for reproducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "This particular structure, particular type of Hilbert space, and the second word in it is kernel, and this is an example of a kernel.",
                    "label": 0
                },
                {
                    "sent": "It's a continuous symmetric positive definite function and an example is a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK, now the arc HS is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically, the linear span of, for example, these Gaussian.",
                    "label": 1
                },
                {
                    "sent": "So you can put a calcium down over any data point you look at the span of that you look at the closure.",
                    "label": 0
                },
                {
                    "sent": "This is the arc ahs and have very nice property in that I can take an inner product in the arc HS and it just gives me back a kernel.",
                    "label": 1
                },
                {
                    "sent": "And again, this is a reproducing property.",
                    "label": 0
                },
                {
                    "sent": "If I take a function F of XI, take its inner product with the kernel, I get back the function and some sets.",
                    "label": 0
                },
                {
                    "sent": "It's acting like a Delta function, but the function itself is in the arcade.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, norm.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, this minimization problem that I had before.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I solve this optimization problem, I get something that's linear in the number of kernels and the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so this takes an infinite dimensional optimization problem down to an end dimensional.",
                    "label": 0
                },
                {
                    "sent": "Optimization problem OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the case of classification, a standard way of doing this is I have.",
                    "label": 0
                },
                {
                    "sent": "I have Y is minus 1 + 1 and then my function sign of F. Is going to be my mapping from X to Y and a classic loss function that you can use is something called the logistic loss right?",
                    "label": 1
                },
                {
                    "sent": "And that's just log of either 1 E to the minus YF of X.",
                    "label": 0
                },
                {
                    "sent": "And this is really.",
                    "label": 0
                },
                {
                    "sent": "You'll see it very soon.",
                    "label": 0
                },
                {
                    "sent": "This is really assuming a Bernoulli model of your data, right?",
                    "label": 0
                },
                {
                    "sent": "So each point X there's a probability that it's going to be one or minus one basically.",
                    "label": 0
                },
                {
                    "sent": "So now I can rewrite this type of penalized loss function, right?",
                    "label": 0
                },
                {
                    "sent": "And what's my?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The model is equivalent to saying that the labels of different points are independent of each other.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, so I haven't said anything about what's going on between X&X, right?",
                    "label": 0
                },
                {
                    "sent": "I've just said that right now at each X it's a Bernoulli.",
                    "label": 0
                },
                {
                    "sent": "OK, now what you will find is that.",
                    "label": 0
                },
                {
                    "sent": "You will have a marginal on X, right?",
                    "label": 0
                },
                {
                    "sent": "And then I'll have a conditional of why an ex right when I would have so man what I hope and what I will stress throughout this talk as I'm going to assume that this marginal is smooth, I'm going to assume that the conditional given the marginal is also smooth.",
                    "label": 0
                },
                {
                    "sent": "Otherwise my life is really bad.",
                    "label": 0
                },
                {
                    "sent": "And there's not much structure in the data, OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so my classification error.",
                    "label": 0
                },
                {
                    "sent": "Simply looking at the sign of this function F right?",
                    "label": 0
                },
                {
                    "sent": "Anna Bayes optimal classifier, let's call it F row of F role is the measure role of X, Y that's giving rise to the data.",
                    "label": 0
                },
                {
                    "sent": "So it's going to have one if the probability that this conditional probability of y = 1 is greater than minus one, otherwise it's minus one very simple statement.",
                    "label": 0
                },
                {
                    "sent": "And so then I can define this function F RO as a log likelihood.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is all well defined.",
                    "label": 0
                },
                {
                    "sent": "OK, great.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in terms of convergence, what you expect is as this regularization term goes to zero as our number of sample goes to Infinity, the sign of our estimated function F should go to the sign of the classification function.",
                    "label": 0
                },
                {
                    "sent": "One would hope that this happens, and this is a classical setting of classification, and now I'm going to make stuff comp.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I'm not going to try to just learn a classification function.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to do is I'm going to try to learn the gradient of the classification function and I will hopefully motivate that soon.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The gradient of the classification function.",
                    "label": 1
                },
                {
                    "sent": "We assume that it exists, is basically the partial partial derivatives.",
                    "label": 0
                },
                {
                    "sent": "Thinking of it as a column vector.",
                    "label": 0
                },
                {
                    "sent": "That's basically it.",
                    "label": 0
                },
                {
                    "sent": "What am I going to do with this thing?",
                    "label": 0
                },
                {
                    "sent": "Well, I can do variable selection by looking at the norms of each of the partial derivatives, right?",
                    "label": 0
                },
                {
                    "sent": "Because that's telling me how much the classification function is changing with respect to one of the coordinates.",
                    "label": 0
                },
                {
                    "sent": "And then I can look at coordinate covariation by looking at the inner product between two of them between let's say the 1st and the 10th series would be telling me with respect to variation in the function.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is a variation with the coordinates OK?",
                    "label": 0
                },
                {
                    "sent": "This is reasonably clear to people.",
                    "label": 0
                },
                {
                    "sent": "OK, great, This is why I'm learning the gradient.",
                    "label": 1
                },
                {
                    "sent": "How can I motivate learning the gradient?",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, you know like you do any other grade and you do Taylor expansion.",
                    "label": 0
                },
                {
                    "sent": "Let's say if I knew what the.",
                    "label": 0
                },
                {
                    "sent": "The classification function were right at a point X. I'm sorry to point if I wanted to know datapoint acts.",
                    "label": 0
                },
                {
                    "sent": "What I can do is Taylor expanded point unirex right?",
                    "label": 0
                },
                {
                    "sent": "And it's just going to be the dot product between this difference and the gradient.",
                    "label": 0
                },
                {
                    "sent": "OK now.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I'm going to do is estimate.",
                    "label": 0
                },
                {
                    "sent": "I need to estimate F Ro because this is here I'm going to estimate by function G and I'm going to estimate its gradient by this efec.",
                    "label": 0
                },
                {
                    "sent": "OK, so this basically.",
                    "label": 0
                },
                {
                    "sent": "Same thing here.",
                    "label": 0
                },
                {
                    "sent": "We written here with G as my classification function estimate an effect as my gradient estimate.",
                    "label": 0
                },
                {
                    "sent": "OK, now this has to hold for points near each other, right?",
                    "label": 0
                },
                {
                    "sent": "You don't want to be doing Taylor expansion on points very very far away because, well, this is bad.",
                    "label": 0
                },
                {
                    "sent": "So the way we enforce.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is a process that is a classification function which are not given.",
                    "label": 0
                },
                {
                    "sent": "Right, but it's a class.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Application function.",
                    "label": 0
                },
                {
                    "sent": "That's your log of the the log, the exactly the log likelihood ratio.",
                    "label": 0
                },
                {
                    "sent": "If I knew the true probabilities, if I knew the true conditional probabilities right, I could take the log likelihood ratios and that would be.",
                    "label": 0
                },
                {
                    "sent": "That would be a base optimal classifier exactly, but I don't know that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm going to assume it's worth it.",
                    "label": 0
                },
                {
                    "sent": "The sign of it would be the sign of it would be the Bayes optimal exactly.",
                    "label": 0
                },
                {
                    "sent": "Smooth to get this special.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, so this is just imposing some locality.",
                    "label": 0
                },
                {
                    "sent": "This is a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It's got this funny little P + 2 if anyone wants to know why that extra P + 2 comes in, I can tell him at the end, but it's for technical reasons, but it's effectively a Gaussian OK. And let's say I use logistic again loss function, I can write down this empirical minimizer OK, which is just saying that while I write times my G, which is my estimate of the classification function times my gradient estimate minus difference right?",
                    "label": 0
                },
                {
                    "sent": "These have to be going in the right direction.",
                    "label": 0
                },
                {
                    "sent": "OK, so classically what you see here is why I would see why I I'm sorry F of.",
                    "label": 0
                },
                {
                    "sent": "Just F of XI, right?",
                    "label": 0
                },
                {
                    "sent": "But I'm just here approximating F of XI by this Taylor expansion.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "And then I waited by how close things are.",
                    "label": 0
                },
                {
                    "sent": "OK, that's basically all I'm doing here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so one thing I'd like to note is if are actually doing the regression problem, it would be a little bit simpler because I wouldn't have to estimate the underlying classification because I'd actually have the two regressors.",
                    "label": 0
                },
                {
                    "sent": "But because I only have ones and minus ones, I also have to estimate this underlying.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classification function OK Now the last thing I need to do is define in Arc HS norm and the way I'm just going to do that is by simply.",
                    "label": 0
                },
                {
                    "sent": "Writing down an arcade, just Norman.",
                    "label": 0
                },
                {
                    "sent": "Each of the coordinates, right?",
                    "label": 0
                },
                {
                    "sent": "And then just summing that up?",
                    "label": 0
                },
                {
                    "sent": "Overall coordinates OK, you can do something more complicated.",
                    "label": 0
                },
                {
                    "sent": "Marcy and Charlie had been doing some very nice work on vector valued functions, and you can do things like that, but we're just going to do a very simple thing, OK?",
                    "label": 0
                },
                {
                    "sent": "It's just a logistic.",
                    "label": 0
                },
                {
                    "sent": "I should.",
                    "label": 0
                },
                {
                    "sent": "Should it be XJ in this F?",
                    "label": 0
                },
                {
                    "sent": "Yeah, you're right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I always get these things yeah, yeah yeah, you're right, thank you yeah yeah, yeah, sorry I. GOG is my estimate of the classification function.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the likelihood ratio, the log likelihood ratio OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so now finally have an algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to simultaneously estimate the classification function and its gradient by basically minimizing.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is empirical error.",
                    "label": 0
                },
                {
                    "sent": "This is a regularization on the classification function.",
                    "label": 1
                },
                {
                    "sent": "This is a regularization on the gradient OK and I have three regularization parameters.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So one very natural.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as well, why are you doing all of this right?",
                    "label": 0
                },
                {
                    "sent": "Why don't you just estimate the function in the 1st place and take partial derivatives would be anything that any sensible person.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do instead of doing all this bullshit, however, there's an issue there.",
                    "label": 0
                },
                {
                    "sent": "First of all, empirically, it's much better to put the constraints on the derivatives.",
                    "label": 0
                },
                {
                    "sent": "It turns out if you go to higher, higher dimensions because you have better control of each of the individual dimensions.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the deeper reason though is that the function FO is going to be in RKHS.",
                    "label": 0
                },
                {
                    "sent": "In general its partial derivatives will not be OK. That can still be answer if someone can tell me well you can use a sobolov space which has particular structure.",
                    "label": 1
                },
                {
                    "sent": "Then I know the derivatives will be in RKHS or something like that.",
                    "label": 1
                },
                {
                    "sent": "That is true, but then the inner products are not well defined because you don't know what coordinate system you're working in and I'll get to this like 4 slides or five slides down the road and so doing this very particular construction we have everything well defined.",
                    "label": 0
                },
                {
                    "sent": "So our mathematically or coordinate systems for these inner product are very clear, so that's that's really the main reason why we do this and empirically it works a lot better OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If I look at this optimization, I'm going to get two functions out of it, right G&F back.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what you'll see is both of these have a representative theorem and that they can be written as a linear combination of kernel functions.",
                    "label": 0
                },
                {
                    "sent": "The classification function G in this case aiz.",
                    "label": 0
                },
                {
                    "sent": "These are scalars, as you would expect them to be in the classical theory.",
                    "label": 0
                },
                {
                    "sent": "In the case of the gradient well, these should be vectors, right?",
                    "label": 0
                },
                {
                    "sent": "Because this thing is a vector, and so it's a vector, and all is clear.",
                    "label": 0
                },
                {
                    "sent": "You can compute.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you look at this kind of semi naively, well, you can say OK, I can use Newton's method to compute this, but it looks like a really ugly optimization problem 'cause it's an RMP by RNP, and being let's say, two what's 100 or 20 between 20 and 100 P being 30,000.",
                    "label": 1
                },
                {
                    "sent": "You feed this into Matlab.",
                    "label": 0
                },
                {
                    "sent": "It's not pretty at all, however.",
                    "label": 0
                },
                {
                    "sent": "How can you reduce the matrix size?",
                    "label": 0
                },
                {
                    "sent": "Well, we can rewrite the objective function as an objective function fee of C and Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, and this is just everything in matrix form.",
                    "label": 0
                },
                {
                    "sent": "Basically, take the gradient, set it equal to 0 now.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is your see here is this matrix of cizia exactly an Alpha?",
                    "label": 0
                },
                {
                    "sent": "Is the vector?",
                    "label": 0
                },
                {
                    "sent": "Now if you look a little bit more carefully what you'll see there's a key quantity in this, and these are these points XI minus X JS and what you'll see is really the solution of this problem is going to be.",
                    "label": 0
                },
                {
                    "sent": "In the span of differences between points, because if you see up here I have the XI minus XJ, right?",
                    "label": 0
                },
                {
                    "sent": "And then I'm waiting it by thing.",
                    "label": 0
                },
                {
                    "sent": "So really I'm in some sense in a subspace of all of the differences of my data, right?",
                    "label": 0
                },
                {
                    "sent": "And so this says that, OK, well this is going to have rank of at least D N -- 1 right?",
                    "label": 0
                },
                {
                    "sent": "So optimization problem is really RND by RND so it's like almost like an SVD.",
                    "label": 0
                },
                {
                    "sent": "You can do an SVD and then look at this much lower dimensional space that's got runtime order MD squared in memory order NP and we can solve this if you.",
                    "label": 0
                },
                {
                    "sent": "I have thousands and thousands of genes which I have in not so many samples, so it's nice.",
                    "label": 0
                },
                {
                    "sent": "OK. And just as a side note, my colleague Chung implemented this he before he was doing pure theory.",
                    "label": 0
                },
                {
                    "sent": "Before this I gave him some code and it was amazing.",
                    "label": 0
                },
                {
                    "sent": "Two days Matlab code worked really well.",
                    "label": 0
                },
                {
                    "sent": "It's quite impressed anyways.",
                    "label": 0
                },
                {
                    "sent": "So then one asks, well, OK, this is good.",
                    "label": 0
                },
                {
                    "sent": "Can you prove something about the convergence of the gradient?",
                    "label": 0
                },
                {
                    "sent": "Can you say that this is consistent or something like that, OK?",
                    "label": 0
                },
                {
                    "sent": "And we can we get horrible rates.",
                    "label": 0
                },
                {
                    "sent": "But let me tell you about the horrible rates 1st and then let me tell you about some of the things you need to.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Able to prove this.",
                    "label": 0
                },
                {
                    "sent": "OK, so the classic you know, if you look at classical generalization bounds, I'd say something like classification function minus the true classification function F RO the L2 norm of this with respect to the measure X is going to be something of the order of enter the minus 1 / 2.",
                    "label": 0
                },
                {
                    "sent": "If you're really lucky, you can get minus one something in between, right?",
                    "label": 0
                },
                {
                    "sent": "We get something to the minus 1 / P and this is with probability and then we get something similar for the great convergence of the gradient.",
                    "label": 0
                },
                {
                    "sent": "To the true gradient OK.",
                    "label": 0
                },
                {
                    "sent": "But we need certain requirements.",
                    "label": 0
                },
                {
                    "sent": "First requirement that we require is basically that the density corresponding to the distribution that generates the data is basically Holder continuous and you need this because if you think about it, if you have a density that's jumping up like this and going through right, you can't estimate gradients there.",
                    "label": 0
                },
                {
                    "sent": "It's nonsensical, you have no hope.",
                    "label": 0
                },
                {
                    "sent": "The other thing that you have to do is worry about the basically the edge.",
                    "label": 0
                },
                {
                    "sent": "If all of your measures concentrated right around the edge.",
                    "label": 0
                },
                {
                    "sent": "Again estimating a gradient.",
                    "label": 0
                },
                {
                    "sent": "Is completely it's very problematic and nonsensical, so if it is the case that the density is let's say something like Holder continuous or has some smoothness right, then both of these will be met and will be OK and we can get these type of results.",
                    "label": 0
                },
                {
                    "sent": "OK again these are really horrible.",
                    "label": 0
                },
                {
                    "sent": "Before I try to improve this and tell you how you can think about improving this, I'd like to show you some toy in real data that we ran this on OK, actually, before I go on the classic way of improving these type of rates, if you look at generalization bounds you assume something about the noise that integrates ebook off noise conditions.",
                    "label": 0
                },
                {
                    "sent": "There are various ways about doing this, there are more classical ways where you can say you have 0 error, so on so forth.",
                    "label": 0
                },
                {
                    "sent": "That's not how we're going to improve rates.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume that there is strong input structure right?",
                    "label": 0
                },
                {
                    "sent": "In that really you have a low dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "And I'll get to that in a second.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One quantity that could be of interest is basically if I look at the norm of any one of the directional derivatives, we normalize it by the norm of all of the directional derivatives.",
                    "label": 1
                },
                {
                    "sent": "That tells me in some sense how important one of the directional derivatives is.",
                    "label": 0
                },
                {
                    "sent": "Another quantity that's of interest.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one of the main quantities will come back to is what we call an empirical covariance matrix, which is inner product between two of the directional derivative.",
                    "label": 1
                },
                {
                    "sent": "So here I have the case directional derivative, the elf directional derivatives take its inner product and I can construct the matrix of this for all of these OK.",
                    "label": 0
                },
                {
                    "sent": "So let's look at the linear example.",
                    "label": 0
                },
                {
                    "sent": "This first linear example is a linear example, which could have been addressed with PC's.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "But will show how it works in our method.",
                    "label": 0
                },
                {
                    "sent": "OK, I have the first 10 coordinates for class one are normal 1.51 next ten R -- 3 one the rest is noise simile for class minus one.",
                    "label": 0
                },
                {
                    "sent": "I get something similar.",
                    "label": 0
                },
                {
                    "sent": "I think these signs should be flipped, but if you look at this this is what the.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data looks like OK, so these are first class one class two OK. And then if I look at the RK chest norm, that's the output of my algorithm.",
                    "label": 0
                },
                {
                    "sent": "Hello represent oh I'm sorry, blue is very negative.",
                    "label": 0
                },
                {
                    "sent": "Red is very positive.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is sample 1, sample 2, up to sample 40 and if you notice the 1st 20 samples are pretty similar.",
                    "label": 0
                },
                {
                    "sent": "Last 20 samples are pretty similar to this one.",
                    "label": 0
                },
                {
                    "sent": "Classifying this one and if you look at the arcade just over the different dimensions, UCI pick out the first 10, then 10 through 20, then 40, two 5050 through 60.",
                    "label": 0
                },
                {
                    "sent": "So this looks good, but PCA would have worked here.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a case where I added I went the wrong way.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Few more things.",
                    "label": 0
                },
                {
                    "sent": "This is what the covariation matrix looks like and you see it blocks beautifully, so you see which variables are covarying together and this is the conditional probability that my classification function is spitting out and see it does well.",
                    "label": 0
                },
                {
                    "sent": "It's an easy example, why shouldn't it?",
                    "label": 0
                },
                {
                    "sent": "If I make it more noisy?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This case is so noisy that PCA would not work because the variance in all of the dimensions is about equal.",
                    "label": 0
                },
                {
                    "sent": "So you really need to know the labels right and what you're going to see is, well, we do better, but we should do better.",
                    "label": 0
                },
                {
                    "sent": "We have the label information right?",
                    "label": 0
                },
                {
                    "sent": "So we pick up the first 10 in the next 20 and the 50 in the 60.",
                    "label": 0
                },
                {
                    "sent": "But everything is more noisy because, well, that's more noisy.",
                    "label": 0
                },
                {
                    "sent": "And you get similar results on the Covariation matrix and also the.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now let me go to a non than your example.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Plus 112, I'll just show you what it looks like.",
                    "label": 0
                },
                {
                    "sent": "There are 200 dimensions.",
                    "label": 0
                },
                {
                    "sent": "The 1st two are useful.",
                    "label": 0
                },
                {
                    "sent": "The rest of them are not.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is what it looks like in the first two class, one is red.",
                    "label": 0
                },
                {
                    "sent": "+2 is blue, so you have a sign in the cosine right, and then if you look at the empirical covariance matrix you see that you know the first one in the 1 one and one and 2 two are important and if you see one one and 1 two, they don't covary together because sine and cosine orthogonal so you get back what you expect and then you get peace.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you rank the the partial derivatives, the 1st two are big, the rest are small, so it starts giving you back what you want.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then this is what my classification function did without any variable selection.",
                    "label": 0
                },
                {
                    "sent": "So then if I go back and let's say, select the first 34 variables, you get a better result.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is kind of the framework that we're going to work in and this is gene expression data.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That everyone's beaten to death but because everyones beaten to death, I have to log in it one more time.",
                    "label": 0
                },
                {
                    "sent": "You have 38 training examples in 35 test examples.",
                    "label": 0
                },
                {
                    "sent": "It's in a 7129 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Two types of leukemia.",
                    "label": 0
                },
                {
                    "sent": "We used 35 for the test, done 38 for the training, 35 for the test and all this.",
                    "label": 1
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Picture is basically really classic method in statistics and machine learning is something called backward selection or forward selection.",
                    "label": 0
                },
                {
                    "sent": "That's all.",
                    "label": 0
                },
                {
                    "sent": "This is right here.",
                    "label": 0
                },
                {
                    "sent": "We've said OK if we look at our classification function with all of the genes.",
                    "label": 0
                },
                {
                    "sent": "This is what it is.",
                    "label": 0
                },
                {
                    "sent": "If I go down you get rid of the bottom 4129 using the.",
                    "label": 0
                },
                {
                    "sent": "The norm of the partial derivatives that I get one error and so on so forth.",
                    "label": 0
                },
                {
                    "sent": "And this is what any reasonable algorithm would be like.",
                    "label": 0
                },
                {
                    "sent": "We're not doing any better than anyone else.",
                    "label": 0
                },
                {
                    "sent": "We're not doing any worse than anyone else.",
                    "label": 0
                },
                {
                    "sent": "Unless you're an idiot, this is what you're going to do.",
                    "label": 0
                },
                {
                    "sent": "But the main point is what you are getting in this method is you're getting this covariation which you wouldn't otherwise OK.",
                    "label": 0
                },
                {
                    "sent": "So this is compared to another method.",
                    "label": 0
                },
                {
                    "sent": "This is something that the computational biology community used a lot space.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cyclea T statistic.",
                    "label": 0
                },
                {
                    "sent": "For each gene, I compute the means of Class 1 to subtract off the mean of the Class 2, divide by standard deviations, scale that in the same way.",
                    "label": 0
                },
                {
                    "sent": "OK, and then what you see.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's kind of faded red is, I think ours blue.",
                    "label": 0
                },
                {
                    "sent": "Is that other ones that eat.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Type statistic and what you see is in general the decay of our methods a lot quicker be this could be this bad, it is what it is.",
                    "label": 0
                },
                {
                    "sent": "If you actually run an L1 norm SVM, it'll do a lot like this in terms of its decay.",
                    "label": 0
                },
                {
                    "sent": "A classic L2 norm will be a bit more like this, but it will still decay faster.",
                    "label": 0
                },
                {
                    "sent": "Decaying for all the decay of the norms of the partial derivatives.",
                    "label": 0
                },
                {
                    "sent": "Now, so far I've been working in an ambient space.",
                    "label": 0
                },
                {
                    "sent": "I've gotten these, you know, these convergence results at 10 to the minus 1 / P. And if I don't assume anything, I can't do better, OK, but assuming all of the coordinates are independent, is a stupid B not true and see defeats the whole point of what I'm trying to do?",
                    "label": 0
                },
                {
                    "sent": "I'm trying to understand how these coordinates.",
                    "label": 0
                },
                {
                    "sent": "You know how they vary with each other, how they occur.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to assume that the data is concentrated on a manifold.",
                    "label": 0
                },
                {
                    "sent": "Which is a subspace of RPM.",
                    "label": 0
                },
                {
                    "sent": "Going to assume somehow this manifold is really concentrated on some D dimensional space, OK?",
                    "label": 0
                },
                {
                    "sent": "Now, if I'm given a smooth orthonormal vector field.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This D dimensional space E1 through EDI can define a gradient on the manifold OK, and that's basically this.",
                    "label": 0
                },
                {
                    "sent": "OK, now what I'm really going to try to do now is basically.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doing the Taylor expansion on the manifold.",
                    "label": 0
                },
                {
                    "sent": "OK, what I need to do that is for each point on the manifold is I need a chart which takes the local area around the point on the manifold into the ambient space.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm sorry into Rd OK and that's just what I have here and it has to satisfy basically these normal coordinate conditions.",
                    "label": 0
                },
                {
                    "sent": "OK so I just have to have some normal coordinate for every point on the manifold and this is natural because you can't really define the gradient unless you have normal coordinates.",
                    "label": 1
                },
                {
                    "sent": "Anywhere I mean it didn't get space or being in a manifold and so then I can write a Taylor expansion around.",
                    "label": 0
                },
                {
                    "sent": "On any point P in terms of this chart, in these normal coordinates OK, However, there's a problem here in the.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "None of these.",
                    "label": 0
                },
                {
                    "sent": "I don't know what the manifold is.",
                    "label": 0
                },
                {
                    "sent": "I'm not given this normal coordinate system or this local expression because I don't get the manifold I get data in the ambient space, right?",
                    "label": 0
                },
                {
                    "sent": "And So what can I do well?",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First thing is I have to assume that there's an embedding that Maps from the manifold back to the ambient space OK, and then what I have in some sense in reality the data is drawn from the manifold, but I'm not given the local expression of the data, but I'm given its image right.",
                    "label": 1
                },
                {
                    "sent": "I'm given the image map of this onto the ambient space.",
                    "label": 0
                },
                {
                    "sent": "OK, which is an RP.",
                    "label": 0
                },
                {
                    "sent": "What I can show?",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is if I assume?",
                    "label": 0
                },
                {
                    "sent": "That this embedding has an inverse.",
                    "label": 0
                },
                {
                    "sent": "OK, I need to assume that then I can rewrite the Taylor expansion here, which is really in terms of Adi dementional space.",
                    "label": 0
                },
                {
                    "sent": "But in the coordinates, the normal coordinate systems of the manifold I can rewrite this as a Taylor expansion of X in a P dimensional space, which is a composition of my function F and.",
                    "label": 0
                },
                {
                    "sent": "The inverse map, and that's what I've written here.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is 2 purposes.",
                    "label": 0
                },
                {
                    "sent": "This serves two purposes.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The first purpose that it serves, which is more important, is that it tells us that our algorithm does not need to be changed for the manifold setting.",
                    "label": 0
                },
                {
                    "sent": "You can be in the ambient space.",
                    "label": 0
                },
                {
                    "sent": "You can be in the manifold setting.",
                    "label": 0
                },
                {
                    "sent": "The same algorithm is working in both cases, and the reason why is because of the locality of the Taylor expansion, right?",
                    "label": 0
                },
                {
                    "sent": "You're looking at everything locally and that's why you get this thing back OK, and then you can prove that you get rates of convergence on the terms of the dimension of the manifold.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is good.",
                    "label": 0
                },
                {
                    "sent": "However I still have a problem.",
                    "label": 0
                },
                {
                    "sent": "What have I given you?",
                    "label": 0
                },
                {
                    "sent": "I've given you a P dimensional vector, right?",
                    "label": 0
                },
                {
                    "sent": "But we know that the manifold somehow relies on a D dimensional space and I really somehow want to understand this D dimensional structure and the geometry of this D dimensional structure.",
                    "label": 1
                },
                {
                    "sent": "So I want to somehow be able to get this back.",
                    "label": 0
                },
                {
                    "sent": "So how can I get this back?",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one way of getting this back is to look at the what we call the empirical covariance matrix, which is just the inner product of each of the partial derivatives OK?",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "You can use this covariance matrix similarly as you.",
                    "label": 0
                },
                {
                    "sent": "You would use what's called the data or the design matrix in PCA.",
                    "label": 0
                },
                {
                    "sent": "So what do you do in PCA?",
                    "label": 0
                },
                {
                    "sent": "You take your.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, it's called X.",
                    "label": 0
                },
                {
                    "sent": "You multiply X by X transpose, right?",
                    "label": 0
                },
                {
                    "sent": "And you get a covariance matrix and then you look at eigenvectors and eigenvalues of this right?",
                    "label": 0
                },
                {
                    "sent": "And this in some sense says that these are the dimensions of the greatest variance, so we're going to do the same thing here.",
                    "label": 0
                },
                {
                    "sent": "OK, Ann.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this this method or this idea is very very similar to some of the supervised non linearity dimension nonlinear dimensionality reduction methods that have been thought of recently like local linear amending or ISOMAP or Laplacian eigen Maps, and very very closely related to something called Hessian.",
                    "label": 0
                },
                {
                    "sent": "Eigen Maps.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "This is a proposition you don't really need to read it in detail.",
                    "label": 0
                },
                {
                    "sent": "What it's saying is if I have a function on RP and I assume it's gradient exists, then I have vector V&R.",
                    "label": 1
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a Kate important feature.",
                    "label": 1
                },
                {
                    "sent": "If it's Norm is 1 and there exists no K -- 1 previous to it, all of them with norm one.",
                    "label": 0
                },
                {
                    "sent": "Basically such that this one if I take the inner product with the gradient in the L Infinity norm, it's bigger than the K + 1 one, but smaller than the previous ones.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is looking at it in the Infinity Norm.",
                    "label": 0
                },
                {
                    "sent": "However the Infinity Norm is hard to work with and it may not be the good norm in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "So if we replace the Infinity norm with the RK chest norm.",
                    "label": 1
                },
                {
                    "sent": "Then the Keith important feature is simply the eigenvector corresponding to the Kate eigenvalue of the covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "So this is just in some sense a theorem version of why something like PCA makes sense in this setting.",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This proposition suggests that we project our data on the top K eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Now what K is a good question, but I won't answer that now 'cause I don't know how to.",
                    "label": 0
                },
                {
                    "sent": "But again, the space projection down to this K dimensional space should reflect the geometry of the classification.",
                    "label": 0
                },
                {
                    "sent": "You can do something similar for regression.",
                    "label": 0
                },
                {
                    "sent": "OK, one good thing is you actually don't have to construct the P by P matrix, because if you remember, again everything is of rank D, so.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It turns out that you can compute this thing in order N squared P and cube time and order PN memory.",
                    "label": 0
                },
                {
                    "sent": "So this is a good thing and I don't have pictures yet because we just work this out.",
                    "label": 0
                },
                {
                    "sent": "But on some data set it actually looks quite nice OK?",
                    "label": 0
                },
                {
                    "sent": "You were saying you wanted to get back to the full space.",
                    "label": 0
                },
                {
                    "sent": "We were actually getting.",
                    "label": 0
                },
                {
                    "sent": "Down to lower dimension I get down.",
                    "label": 0
                },
                {
                    "sent": "Yeah so so with my gradients, right?",
                    "label": 0
                },
                {
                    "sent": "I'm in P dimensions right now.",
                    "label": 0
                },
                {
                    "sent": "What I've done is I've somehow projected down to D or K dimensional space where I hope all of the geometric structure of the data lies.",
                    "label": 0
                },
                {
                    "sent": "OK, OK. Yeah, in some sense.",
                    "label": 0
                },
                {
                    "sent": "Ideally you'd always like to really get back this D dimensional manifold, but I don't see how to do it.",
                    "label": 0
                },
                {
                    "sent": "I just don't.",
                    "label": 0
                },
                {
                    "sent": "I don't see.",
                    "label": 0
                },
                {
                    "sent": "How to do it?",
                    "label": 0
                },
                {
                    "sent": "I mean, I can see that getting projections that reflect the geometry, but I don't see how to get the full dimensional dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "Let's see, so we still have more to do, I'm.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm working on a basean version of this.",
                    "label": 0
                },
                {
                    "sent": "For certain reasons, in that I do a lot of Asian stuff, but also.",
                    "label": 0
                },
                {
                    "sent": "It gives me a different way of looking at it.",
                    "label": 0
                },
                {
                    "sent": "Maybe in the similar way that PC has a probabilistic interpretation.",
                    "label": 0
                },
                {
                    "sent": "I could get a probabilistic interpretation of it, and actually more importantly is it maybe give me a way to get confidence intervals for some of those estimates so I can kind of get an error bar and see you know which of these coordinates are really varying more or less OK. Other things we have formulated, the semi supervised version.",
                    "label": 0
                },
                {
                    "sent": "We'd like to implement it using the semi supervised version.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can actually get an idea for what the real dimensionality of the manifold is, but we're not sure that's one of the things we're looking at.",
                    "label": 0
                },
                {
                    "sent": "So we think that this has a relation to information geometry and this covariance matrix is a particular case of.",
                    "label": 1
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Non parametric analogs of Fisher information matrix.",
                    "label": 1
                },
                {
                    "sent": "And I think that's it, so I guess it was short, but.",
                    "label": 1
                },
                {
                    "sent": "There's actually a strong relation with this work, and some of the work that people.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I've been doing on manifold regularization.",
                    "label": 0
                },
                {
                    "sent": "This is specifically some of the work by Parthan Yogi and Misha Belkin, or they have been doing.",
                    "label": 0
                },
                {
                    "sent": "Do we have a?",
                    "label": 0
                },
                {
                    "sent": "Great thanks, so they've been looking at the following.",
                    "label": 0
                },
                {
                    "sent": "Maybe we shouldn't be using this?",
                    "label": 0
                },
                {
                    "sent": "Do you want some soda water?",
                    "label": 0
                },
                {
                    "sent": "Try some mineral water.",
                    "label": 0
                },
                {
                    "sent": "Chill.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that works.",
                    "label": 0
                },
                {
                    "sent": "Not well, yeah.",
                    "label": 0
                },
                {
                    "sent": "So let's see.",
                    "label": 0
                },
                {
                    "sent": "I haven't some other slide.",
                    "label": 0
                },
                {
                    "sent": "Let's see if I still have.",
                    "label": 0
                },
                {
                    "sent": "I can write on the sheet.",
                    "label": 0
                },
                {
                    "sent": "In the end, I could not reconstruct the manifold.",
                    "label": 0
                },
                {
                    "sent": "So they have a slightly different problem.",
                    "label": 0
                },
                {
                    "sent": "They they can get to.",
                    "label": 0
                },
                {
                    "sent": "So yeah, let me write down what they have and then So what they have is they have a loss function.",
                    "label": 0
                },
                {
                    "sent": "Sum over, you know I equals one through N the votes just even do square loss F of X I -- Y ^2.",
                    "label": 0
                },
                {
                    "sent": "Then they have an extra term which is some over IJWIJ can think of it the same exact way to what I said before and this is over all of the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So this is one through N + M. Ambien, being the number of labeled data and being the more unlabeled data and then you can write this as F of X I -- F of XJ.",
                    "label": 0
                },
                {
                    "sent": "Squared OK, and this is basically a graph.",
                    "label": 0
                },
                {
                    "sent": "Laplacian OK, right?",
                    "label": 0
                },
                {
                    "sent": "It's a wait, again.",
                    "label": 0
                },
                {
                    "sent": "Preserve locality.",
                    "label": 0
                },
                {
                    "sent": "The distance between exciting.",
                    "label": 0
                },
                {
                    "sent": "I do.",
                    "label": 0
                },
                {
                    "sent": "It's embedded in WHA that's in WSJ.",
                    "label": 0
                },
                {
                    "sent": "So this is a way which is some locality or OK.",
                    "label": 0
                },
                {
                    "sent": "So in some sense OK, this this this operator the graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "You can think of as a approximation right of Dell.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Which is the Laplace continuous version which is a loss.",
                    "label": 0
                },
                {
                    "sent": "Beltrami operator OK now.",
                    "label": 0
                },
                {
                    "sent": "So this is what they're looking at.",
                    "label": 0
                },
                {
                    "sent": "This is what they're constructing and.",
                    "label": 0
                },
                {
                    "sent": "In their paper, for example, where they did Laplacian Eigen Maps, they didn't get back the manifold they got back in projection which will preserve distances on the manifold locally locally, which we will do as well.",
                    "label": 0
                },
                {
                    "sent": "But it's slightly different in that we have the label information as well the key quantity that we look at is basically something like Dell.",
                    "label": 0
                },
                {
                    "sent": "F Delta X you can think about it.",
                    "label": 0
                },
                {
                    "sent": "So we have the gradient right plus differences of the functions, right?",
                    "label": 0
                },
                {
                    "sent": "So this is in some sense the key quantity in their work.",
                    "label": 0
                },
                {
                    "sent": "This is a key quantity in our work right?",
                    "label": 0
                },
                {
                    "sent": "And then once we take an inner product of this Dell F with this Dell F, we get the full Hessian, whereas they have the Laplace operator, right?",
                    "label": 0
                },
                {
                    "sent": "So that's a question, there's there's a very good reason why in their work they use a Laplace operator because this is invariant with respect to coordinate change, right?",
                    "label": 0
                },
                {
                    "sent": "So they want to study this thing.",
                    "label": 0
                },
                {
                    "sent": "In our case we do not want something invariant with respect to coordinate changes.",
                    "label": 0
                },
                {
                    "sent": "We're trying to learn the coordinate system or natural coordinate system for the framework, which is why we look at.",
                    "label": 0
                },
                {
                    "sent": "You know, slightly different operators from that perspective.",
                    "label": 0
                },
                {
                    "sent": "Enter.",
                    "label": 0
                },
                {
                    "sent": "Can you say something a bit more about why, what, how it was applied?",
                    "label": 0
                },
                {
                    "sent": "I mean, what's the why this was useful?",
                    "label": 0
                },
                {
                    "sent": "OK yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "How much time do I have?",
                    "label": 0
                },
                {
                    "sent": "Just out of curiosity?",
                    "label": 0
                },
                {
                    "sent": "OK, so let me show you actually something about why.",
                    "label": 0
                },
                {
                    "sent": "I'll be quick, I promise.",
                    "label": 0
                },
                {
                    "sent": "Where do I have this?",
                    "label": 0
                },
                {
                    "sent": "Country videos Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "Radios who is this what?",
                    "label": 0
                },
                {
                    "sent": "I know this is the wrong one.",
                    "label": 0
                },
                {
                    "sent": "Annoying.",
                    "label": 0
                },
                {
                    "sent": "Oh, it can't open it.",
                    "label": 0
                },
                {
                    "sent": "OK, well, I can't get it to open, but.",
                    "label": 0
                },
                {
                    "sent": "I know why I'm wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, that was much better.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the actual application of this that we're really looking at, and this is actually something that we have right now in the preclinical trials.",
                    "label": 0
                },
                {
                    "sent": "We're looking at lung cancer.",
                    "label": 0
                },
                {
                    "sent": "OK, and we're actually trying to find a way of assigning patients particular risk number one using classical classification methods, and then also if we think someone's going to recur, how to give them particular type of treatments?",
                    "label": 0
                },
                {
                    "sent": "OK, and that comes in something called the pathway paradigm and the pathway paradigm is where we're actually using trying to.",
                    "label": 0
                },
                {
                    "sent": "Break these things apart how they covary so we can understand what the different pathways are.",
                    "label": 0
                },
                {
                    "sent": "This is just some bullshit you should doctors.",
                    "label": 0
                },
                {
                    "sent": "Estimates please.",
                    "label": 0
                },
                {
                    "sent": "How do you get so OK?",
                    "label": 0
                },
                {
                    "sent": "Yes, choose.",
                    "label": 0
                },
                {
                    "sent": "How do you get ground truth?",
                    "label": 0
                },
                {
                    "sent": "This is a very good question.",
                    "label": 1
                },
                {
                    "sent": "So typically what OK.",
                    "label": 0
                },
                {
                    "sent": "So the classic leasee is that you make a prediction or they have some stratification of people who get sick or won't they make these two type of curve say they're different but that doesn't predict how an individual is going to be right, which was your question.",
                    "label": 0
                },
                {
                    "sent": "OK so basically what we actually really have is we come in with patients they see.",
                    "label": 0
                },
                {
                    "sent": "We see patients.",
                    "label": 0
                },
                {
                    "sent": "They might be stage 1A or higher.",
                    "label": 0
                },
                {
                    "sent": "So The Who World Health Organization has a way of classifying patients as to whether they're really going to be badly off or less badly off.",
                    "label": 0
                },
                {
                    "sent": "Depending on how big the tumors are there 14 variables that they do use do this OK. And then if your stage 180 actually observe you to cut the reset the tumor, then they observe you.",
                    "label": 0
                },
                {
                    "sent": "If you're higher, they give you chemotherapy.",
                    "label": 0
                },
                {
                    "sent": "What's nice about this is that if we think that someone is going to relapse, it's a lot easier to give them chemotherapy than in the other way around, right, doctors are more willing to give treatment, and so basically we have.",
                    "label": 0
                },
                {
                    "sent": "Patient data from this we have people that are, let's say alive.",
                    "label": 0
                },
                {
                    "sent": "After five years, people that are dead after 2 1/2 years we have.",
                    "label": 0
                },
                {
                    "sent": "I think 90 odd patients and what you find is if you try to use just the 14 variables at The Who uses, you are not so accurate, OK?",
                    "label": 0
                },
                {
                    "sent": "Predicting whether a person we.",
                    "label": 0
                },
                {
                    "sent": "Re occurrence.",
                    "label": 0
                },
                {
                    "sent": "Yes, there is intervention therapy.",
                    "label": 0
                },
                {
                    "sent": "There is reception in between.",
                    "label": 0
                },
                {
                    "sent": "OK, so they cut the tumor out.",
                    "label": 0
                },
                {
                    "sent": "If you're one or they don't give you chemo if you're not one.",
                    "label": 0
                },
                {
                    "sent": "They do give you chemo.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we look at is given everyone, OK.",
                    "label": 0
                },
                {
                    "sent": "They recur or not OK, this is our.",
                    "label": 0
                },
                {
                    "sent": "How accurately can we predict this?",
                    "label": 0
                },
                {
                    "sent": "And if you use just a 14 variables by The Who you're about 61% correct.",
                    "label": 0
                },
                {
                    "sent": "OK, in terms of recurrence now some of those people were stage one way.",
                    "label": 0
                },
                {
                    "sent": "Some of them were stage higher.",
                    "label": 0
                },
                {
                    "sent": "We don't distinguish yet.",
                    "label": 0
                },
                {
                    "sent": "OK, now if you use all of the variables to 20,000 or whatever, use basically with SVM, some kernel method or regularize logistic, this is overrated.",
                    "label": 0
                },
                {
                    "sent": "It's really not 94%.",
                    "label": 0
                },
                {
                    "sent": "It's like 85.",
                    "label": 0
                },
                {
                    "sent": "Really this is a bit of overfitting 'cause it's a leave one out and it's cheating, but it's really around 85, so you do a lot better now.",
                    "label": 0
                },
                {
                    "sent": "So then the idea is you know patients are you predict to not do well.",
                    "label": 0
                },
                {
                    "sent": "You re classify as high risk and you give them chemo.",
                    "label": 0
                },
                {
                    "sent": "OK so that's kind of the basic part, but the part where we start to use these?",
                    "label": 0
                },
                {
                    "sent": "False negative there is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't exactly know.",
                    "label": 0
                },
                {
                    "sent": "But actually in this case it was about equal.",
                    "label": 0
                },
                {
                    "sent": "It was actually equal and for this model, but it's usually different, but in this one I think it was actually about equal.",
                    "label": 0
                },
                {
                    "sent": "But the doctors don't like it that way.",
                    "label": 0
                },
                {
                    "sent": "They wanted more confidence in one and the other, but I forget right now.",
                    "label": 0
                },
                {
                    "sent": "So where where this covariation comes in is a question of, well, if you give people chemo, some of them will relapse and what do you do about them?",
                    "label": 0
                },
                {
                    "sent": "You just can't predict all you're going to relapse, you know.",
                    "label": 0
                },
                {
                    "sent": "So I mean, it's probably want to do something more helpful, right?",
                    "label": 0
                },
                {
                    "sent": "And so there this is this issue.",
                    "label": 0
                },
                {
                    "sent": "The biology idea behind this is that.",
                    "label": 0
                },
                {
                    "sent": "Jeans are kind of occurring together.",
                    "label": 0
                },
                {
                    "sent": "Their particular oncogenes if you hit an oncogene there going to be other jeans that are affected by it, right?",
                    "label": 1
                },
                {
                    "sent": "Some of the classical ones are asmik there.",
                    "label": 0
                },
                {
                    "sent": "There are variety of these things.",
                    "label": 0
                },
                {
                    "sent": "So the idea is if I can go into cell lines and decompose and understand what the models are for these particular ones and then say these things in tumors, right?",
                    "label": 0
                },
                {
                    "sent": "Then I can start decomposing this.",
                    "label": 0
                },
                {
                    "sent": "So an example of this is they took.",
                    "label": 0
                },
                {
                    "sent": "This is work done by Andrea Bildenden.",
                    "label": 0
                },
                {
                    "sent": "Joan Evans is lab at Duke.",
                    "label": 0
                },
                {
                    "sent": "They basically took human cell lines.",
                    "label": 0
                },
                {
                    "sent": "I think these are breast cell lines.",
                    "label": 0
                },
                {
                    "sent": "And half of them, let's say with with an oncogene wrasse by basically by some biochemistry and the rest of them were hit by null thing that doesn't do anything.",
                    "label": 0
                },
                {
                    "sent": "This is done on the chip, but these are in cell lines.",
                    "label": 0
                },
                {
                    "sent": "If these are not in human tumors right and so then you have these things and you build models and these are what in some sense we're trying to get after what this covariation method is you want to break apart into pathways and sub pathways.",
                    "label": 0
                },
                {
                    "sent": "Now that I have this model I can go back into the human data and look at OK. Well, do some of these models.",
                    "label": 0
                },
                {
                    "sent": "Are they more or less expressed in the human cancer so I can break up the human cancer as these models you can think of them as factors if you did principle components.",
                    "label": 0
                },
                {
                    "sent": "These are mathematical factors.",
                    "label": 0
                },
                {
                    "sent": "Statistical factors.",
                    "label": 0
                },
                {
                    "sent": "These are in some sense, biological factors that you're going to be decomposing your tumors in terms of and this is what they look like and.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so that's kind, and then the last thing is for some of these pathways or these oncogenes we have drugs that can hit them, and so that's the last part of it.",
                    "label": 0
                },
                {
                    "sent": "You use pathway specific drugs, but that's the application that we are going after.",
                    "label": 0
                },
                {
                    "sent": "And that's the idea.",
                    "label": 0
                },
                {
                    "sent": "Using those correlations is to really try and say OK, these two genes are highly correlated.",
                    "label": 0
                },
                {
                    "sent": "Therefore there in the same path.",
                    "label": 0
                },
                {
                    "sent": "That's what I'm arguing 1st order on saying jeans by statistical definition of a pathway are genes that are significantly differentiated between the two conditions and coordinate LeEco very or Co Express, and I'm hoping 1st order.",
                    "label": 0
                },
                {
                    "sent": "That's an approximation of what a biological pathway is, because no one can define that.",
                    "label": 0
                },
                {
                    "sent": "So in fact, when you say you do.",
                    "label": 0
                },
                {
                    "sent": "Principle components analysis and you get K dimensions that tells you there are sort of K independent parts.",
                    "label": 0
                },
                {
                    "sent": "That would be my would be my statement.",
                    "label": 0
                },
                {
                    "sent": "I mean Lucy.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to get K yet.",
                    "label": 0
                },
                {
                    "sent": "I mean there are lots of things I don't know, but you know we're working on it and then you can actually look at which genes are involved in each of those pathways and maybe see if we can hit one of them or yeah.",
                    "label": 0
                },
                {
                    "sent": "Or if we know that those belong to particular oncogenic pathways.",
                    "label": 0
                },
                {
                    "sent": "If they overlap heavily with particular oncogenic pathway.",
                    "label": 0
                },
                {
                    "sent": "So on and so forth or I mean or in some of them.",
                    "label": 0
                },
                {
                    "sent": "I even know that it is because.",
                    "label": 0
                },
                {
                    "sent": "That's how they constructed it and they went into a cell line to hit it with a very particular perturbation, which they know is an oncogene or particularly relevant gene, and so that's why.",
                    "label": 0
                },
                {
                    "sent": "Don't you ever.",
                    "label": 0
                },
                {
                    "sent": "Every issue is trying to validate that I have a very strong big issue with trying to validate my conclusion.",
                    "label": 0
                },
                {
                    "sent": "I mean we're doing this pre clinical trial.",
                    "label": 0
                },
                {
                    "sent": "This is the closest thing you get to validation, right?",
                    "label": 0
                },
                {
                    "sent": "We're going to have.",
                    "label": 0
                },
                {
                    "sent": "I think 200 patients and try to do this and see if we actually improve their health.",
                    "label": 0
                },
                {
                    "sent": "I mean not screw them up.",
                    "label": 0
                },
                {
                    "sent": "I mean this is, you know your predictions.",
                    "label": 0
                },
                {
                    "sent": "Actually, you know.",
                    "label": 0
                },
                {
                    "sent": "Sure, yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so who see in any way coming out?",
                    "label": 0
                },
                {
                    "sent": "But more subtle definition part?",
                    "label": 1
                },
                {
                    "sent": "I mean, you know independent pathways or somehow?",
                    "label": 0
                },
                {
                    "sent": "I mean, uh, presumably in in the real cell there are lots of interleaving, yeah?",
                    "label": 0
                },
                {
                    "sent": "Tree model, right?",
                    "label": 0
                },
                {
                    "sent": "That's PR.",
                    "label": 0
                },
                {
                    "sent": "You can do a hierarchical model.",
                    "label": 0
                },
                {
                    "sent": "There are people who do Bayesian factor models.",
                    "label": 0
                },
                {
                    "sent": "There are many things you can try to do out of this.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean, yeah, I just don't know how to do partitioning yet or things like that.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm still thinking about that, but yeah, I mean this is just one model.",
                    "label": 0
                },
                {
                    "sent": "I mean I could put different structures on how things are related and do different things.",
                    "label": 0
                },
                {
                    "sent": "I haven't gotten there yet, so yeah.",
                    "label": 0
                },
                {
                    "sent": "Anyways, that's that.",
                    "label": 0
                }
            ]
        }
    }
}