{
    "id": "wt2ejccmxrbuz64mrfdkkh26yrhdxvmv",
    "title": "Agnostic KWIK learning and efficient approximate reinforcement learning",
    "info": {
        "author": [
            "Csaba Szepesv\u00e1ri, Department of Computing Science, University of Alberta"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/colt2011_szepesvari_agnostic/",
    "segmentation": [
        [
            "I am lucky enough to actually present two talks for you guys today.",
            "The first talk is together with SQL.",
            "Each transita who is very sorry not to be able to present, but he had to go to a English literature conference to London and it's like you couldn't skip that.",
            "So you can understand it like just learning theory versus English literature.",
            "This is recorded right?"
        ],
        [
            "Right, so this talk is about agnostic quick learning.",
            "And we're trying to extend the quick learning frameworks so that we can handle larger programming classes and we are not restricted by what we assume about the environment.",
            "This is, I think, natural desire, that we want to do, and so here's the outline for the talk.",
            "So first, I'm going to introduce the basic concepts, talk about efficient reinforcement learning.",
            "Introduce the quick framework for those who are not familiar with it, and then I'm going to talk about agnostic learning some definitions and like how it applies to the reinforcement learning on control problem and then results for several problem classes.",
            "So it's pretty straight."
        ],
        [
            "Right, So what is reinforcement learning so in reinforcement learning, illustrated by just getting lost in a maze, you want to learn about an environment by pursuing some long-term goals, like collecting reward your interest in, typically maximizing the sum up the rewards, overtime, and."
        ],
        [
            "What makes this difficult is that you don't know about the environment, so you're learning simultaneously both environment and try to collect rewards and why you're doing that."
        ],
        [
            "Since you don't know the environment, you have to go to places that you haven't seen before.",
            "It's obvious because otherwise you can just miss the big rewards, so you really have to get to those places that you haven't seen.",
            "No, the question is how much time to spend on visiting places that you haven't seen versus how much time to spend.",
            "Parts of the space that you have already seen.",
            "Given that wherever you go, you'll see some.",
            "You will have some uncertainty left about how much reward you can collect there, because you know the word is stochastic, so you will never actually know for sure.",
            "That OK, if you go to this place, then this is the best place ever.",
            "'cause you have this noisy feedback.",
            "Alright, so that's the problem that we study in reinforcement learning.",
            "Or that's one major problem that studying."
        ],
        [
            "First, find learning and.",
            "The various mathematical frameworks exist to study this problem.",
            "In one framework you're interested in making a bounded number of harshly suboptimal stamps, so this is like a minimum requirement that you wish to achieve, and for this you need to balance exploration and exploitation.",
            "And there are lots of results out there in the niche chair that cover various cases like 4.",
            "You know, sing simple.",
            "MDP's finite MDP is factored.",
            "MDP's MVP sees linear dynamics in certain sense, and so on so forth, under which you can prove that there exists target items which are fairly efficiently which are able to fairly efficiently explore the environment and collect the rewards so they're not making sort of many mistake."
        ],
        [
            "OK, and so how these algorithms work?",
            "Many of these algorithms work on the basis of these are Max construction, which is like a general scheme for constructing efficient reinforcement learning algorithms.",
            "And the way these algorithms work is that they keep track of the unknown areas in the environment, like in the state space.",
            "So part of the state space is classified as being on and.",
            "The algorithm explicitly assumes that you know unknown territories are good to go to, so they're like heaven, so they really want to go there to explore.",
            "And this is how they are going to balance exploration and exploitation and.",
            "They, with this assumption, they come up with an optimal way to act.",
            "So with this you know Twig model idea about the environment and then they follow what they believe is the best for them and you know two things can happen.",
            "Either you stay in the area that you already know, in which case what happened is that like you're kind of disregarding those unknown areas, because there are two far away it would take too much time to get there.",
            "And then you collect a lot of rewards.",
            "Or the other thing that can happen is that you're leaving that area and you go to these unseen places and then you discover those and and so if that happens, then you actually learn.",
            "So you may you didn't collect a reward because you plan to like you.",
            "You set up your mind that you just want to go to these unseen places and so you give up your desire to collect the rewards in the the known area.",
            "So you set up so high rewards that you really want to go to those places, and so if it happens that you actually go there, then you're going to gain information.",
            "So in both cases you're gaining.",
            "Something you want case?",
            "You're gaining information.",
            "The other case you're gaining reward and therefore you can eventually proved."
        ],
        [
            "That things workout.",
            "So there's the basic idea, and since there are many frameworks where these ideas been worked out, there was a desire to generalize the whole thing and factor out what is important about you.",
            "Know the different aspects of this learning problem, and so the framework where this happens is called a quick learning framework.",
            "That's an abbreviation for, knows what it knows and.",
            "Max slides are going to be about that framework, so the important thing here is that there are the known areas and so what's important that the quick learner has to know.",
            "If it knows something or doesn't know something, and the known area classification is based on the uncertainty left in what you learned about the environment, right?",
            "A quick learner is going."
        ],
        [
            "To be such alright?",
            "So here is an illustration of graphically the station what a quick learner is and what it needs to do.",
            "So this is like a sequential online learning framework with A twist.",
            "An agent is playing with an adversary and then adverse.",
            "It's a prediction problem Interestingly, so it's like, you know, learning a model of the environment is like being able to predict what's going to happen next, so it's kind of be a prediction problem."
        ],
        [
            "So the adversary picks a concept that the learner needs to predict.",
            "And then a sequential interaction begins.",
            "So the concept is Chi.",
            "And then the following."
        ],
        [
            "Repeated and the adversary picks a query X, and this is communicated to the agent and then they."
        ],
        [
            "Agent can to do things either it is size that it doesn't know enough about that particle point in space to predict.",
            "What Jay would respond at that point, and then it says, OK, I'm going to pass, I don't know."
        ],
        [
            "And then in that case, the adversary is going to give the learner an noisy feedback and the learner cannot do."
        ],
        [
            "Itself or the learner predicts an answer.",
            "In which case?"
        ],
        [
            "The answer has to be accurate, so the protocol just ends THEOS.",
            "If the answer is not sufficiently accurate.",
            "And in this case, the learner doesn't get any feedback because, well, if it didn't fit, then it's everything is good, and if it failed then the game ended anyways, alright?",
            "So there's the quick framework that was introduced by leave Ocean with money."
        ],
        [
            "And eight.",
            "And so the way this can be used in an MDP framework is as follows.",
            "So this is like the umbrella algorithm that applies to many classes of environment.",
            "So you just have you know, an MDP learner and a planner, and then so it is the loop and the agent is exploring the environment so it uses the planner too optimistically.",
            "Plan in the with the help of the MDP learner, so the MDP learner is the quick agent that's learning about the environment.",
            "So it picks the current state and then it uses its planning algorithm to come up with a good action.",
            "So this planning algorithm can just use the model learn, which is a little bit modified in optimistic fashion, will come to that in a second and then execute.",
            "That action observes the next state and the associated the board.",
            "And then what happens is that if the MDP learner actually predicted for the given state action pair.",
            "That it doesn't know the answer, so it passed.",
            "And then it's given the opportunity to learn from the next state and the reward.",
            "And so it tries to refine its understanding of the environment and then the root and the loop repeats this clear.",
            "OK."
        ],
        [
            "So the optimistic rapper.",
            "That's used here in the planning algorithm, so the planning algorithm can consider this model.",
            "It can ask for the next state distribution and the rewards at any state action pair.",
            "And so the way it works is that, well, it can call this prediction function, and this checks with the MDP learner if the MVP learner actually knows about that particular state action pair, right?",
            "Then it returns whatever the MDP learner is predicting for the state action pair, the next day distribution, and the reward in the other case it determines.",
            "Blue back to the same state like the next day distribution is such that you just go back to the same state and you get kind of maximum reward like you are in have a OK.",
            "So if the planet is is bad with this modified model, what happens?",
            "Is that exactly what I said before at the parts of the state space for which the learner says I don't know, you will like instant ate, a very huge reward in the other parts.",
            "You just.",
            "Predict whatever the learner would predict.",
            "So that's like the umbrella."
        ],
        [
            "OK, and so this is a classic design that says that if there is an efficient quick learner then there is an efficient reinforcement learner.",
            "Kind of is the same bond.",
            "So what is an efficient quick learner by the way?",
            "So the quick requirement is that with you know a part from a failure event that has a control probability.",
            "A quick learner should never fail, and the number of passes should be bonded, so we're talking about like if the number of passes of the quick learner is bonded here, then the same number of mistakes are going to be inherited basically by the reinforcement learning agent, and this works very general."
        ],
        [
            "Alright.",
            "So, but the big question is, what if the environment is not contained in the class that a quick learner uses?",
            "That's the question."
        ],
        [
            "OK.",
            "So for example, you assume that the environment is a factor MD.",
            "Peabody environments actually not a factor MVP.",
            "Then we don't really have any guarantees, or you assume that there is some State app section going on, but the setup section is a little bit off and so forth.",
            "So you could continue this so we can't really assume make make those strong assumptions about the environment.",
            "It doesn't seem to be fair, OK."
        ],
        [
            "So then, what?",
            "What can we do?",
            "How can we modify the quick learning concept where we want to lose some kind of error?",
            "So this is illustrated on this picture.",
            "So the way you could do this is that you know the agent has a problem class in mind, and then there is another problem class that the environment has.",
            "And.",
            "Is it 5?",
            "OK, so maybe I'm merchant talks.",
            "So there is another environment class and there is a bounded distance between these two, and so this would be the concept that's picked by the environment and the agents.",
            "Just thinking about the linear fun."
        ],
        [
            "Alright."
        ],
        [
            "And so."
        ],
        [
            "You modify the basic framework in such a way that you allow bigger errors.",
            "So obviously if you cannot represent all the concepts after it reacts accuracy, then you have to lower slack.",
            "The little bigger slack, and we introduce discount.",
            "But it evenus framework but."
        ],
        [
            "Otherwise, it's kind of the same."
        ],
        [
            "And so I'm going to just skip this."
        ],
        [
            "Is the test because, well, it's kind of.",
            "The same idea applies."
        ],
        [
            "Adding the definition of what constitutes an efficient bounded quick learner, and so it is 1 main theorem that we proved in the paper which just generalizes the previous result to this agnostic quick framework, basically justifying the framework.",
            "So it says that if you have an agnostic quick learner with a certain complexity bond B, then that bond is inherited by this general reinforcement learner that."
        ],
        [
            "As shown before.",
            "OK, so this justifies using this agnostic quickforce."
        ],
        [
            "It works really nicely, but it leaves wide open.",
            "The question of what can we agnostically quick learn right?",
            "So in order not to make pretty vacuous claims we would need."
        ],
        [
            "To understand what can we learn at all and so small exercises?",
            "Basically, if you have a finite hypothesis class and you don't have any label noise."
        ],
        [
            "You know, then it's really easy, because for each query."
        ],
        [
            "You just see if like the remaining you are ruling out hypothesis as you go, and if you check if the remaining hypothesis agree up to an accuracy of D, then you just predict what ever any of these hypothesis is say."
        ],
        [
            "And this prediction is gotta be 2D accurate and."
        ],
        [
            "Otherwise, you have to pass because you don't know and."
        ],
        [
            "If you pass your risk."
        ],
        [
            "The information that you can exclude at least one high."
        ],
        [
            "Update this."
        ],
        [
            "And in this way, of course, this general learner is able to learn with at most each minus one passes, and it's going to make 2D accurate predictions.",
            "So the competitiveness factor is 2.",
            "Actually, you cannot improve that, unfortunately.",
            "So we give a counterexample."
        ],
        [
            "And."
        ],
        [
            "So that would be a."
        ],
        [
            "Run very quickly."
        ],
        [
            "So at that point, we didn't."
        ],
        [
            "We learn anything, but we could predict at that point us by the adversary like the we have too many hypothesis is we can't predict."
        ],
        [
            "But the."
        ],
        [
            "And then we learn."
        ],
        [
            "And so and so forth."
        ],
        [
            "You excluded and so I'm running out of time so."
        ],
        [
            "Going to skip this is bad.",
            "This is like motivating another algorithm for the noisy case.",
            "In the noisy case you what you have to do is pairwise competition between all.",
            "The hypothesis is nothing else seems to work.",
            "So what you do is that you have like 2 hypothesis."
        ],
        [
            "Let's say two linear functions and.",
            "Then there are those regions where they differ by lot."
        ],
        [
            "Right, and then if you have lots of examples and then you just average the examples over these examples, then you will clearly see that the averages are going to make a difference between the two functions and you can eventually detect one of them."
        ],
        [
            "And so that's how you."
        ],
        [
            "Learn."
        ],
        [
            "And so you can."
        ],
        [
            "Normalize this and."
        ],
        [
            "Yeah, and show some results so we come to this table of learning complexities which summarizes the results that we have so far.",
            "So very simple case.",
            "Finite deterministic case.",
            "Opted to the approximation agnostic.",
            "We can quick are actual and the finite noisy case.",
            "You can see that if you have finitely many Hyper V Xen hyper disease, you can order this.",
            "See some difference between quick learning and agnostic learning.",
            "But if you come to more complicated things, still very simple, like linear predictors, dimensional in our predictors you see huge differences suddenly so quick learning is just solving linear program.",
            "That's really easy.",
            "However, in the case of agnostic learning we have an upper bound of this slide which is like huge huge huge, very scary and we have a lower bound that shows that at least one of the exponential scaring has to be there.",
            "So it's not very nice at the moment and for the D dimensional linear noisy case.",
            "You also have an exponential blowup, so it's not looking very rosy at the moment."
        ],
        [
            "So in summary, this is a framework.",
            "Where we allowed the adversary to choose hypothesis outside of the concept class.",
            "Used by the learner.",
            "That's a very natural extension of the quick framework and we justify this framework.",
            "The choices of the other measures and such by proving the previous results in this new framework regarding the efficiency of frame first.",
            "Math learning, so you can design efficient reinforcement learning agents using this framework that can go beyond.",
            "So what can be represented explicitly with the hypothesis class?",
            "So that's good news, but generally we find that agnostically quick learning even simple things is getting pretty hard.",
            "Very soon."
        ],
        [
            "And so within handle agnostic quick learner learning for transition probabilities.",
            "But we think it can be done and so one very here is that maybe you know the framework is not truly idea because very soon we are getting this negative results which show that agnostic quick learning is actually very very difficult and there is nothing that says that this is actually necessary.",
            "So maybe this decomposition.",
            "No, the reinforcement learning paperin factoring out this quick learning aspect to it is not ideal, and it's not going to go very far.",
            "We don't know it's interesting to know that there are not many designs or not.",
            "Any results in the literature there where you would agnostically efficiently in any sense, reinforcement, learning, and environment.",
            "So these are, you know, groundbreaking first results, but at the same time this seems like a difficult problem maybe.",
            "Thank you please.",
            "Question.",
            "Hi, so I've been interested in quick learning recently and something I've noticed is for just ordinary non ignostic quick learning in the noisy setting right?",
            "It seems to be a very strong.",
            "The quick guarantee seems to be very strong in that I don't know of too many.",
            "Domains besides noisy linear regression, where you can even do non ignostic quick learning.",
            "So do you have an example?",
            "So did you only near aggression is that is that the result?",
            "OK, that's right.",
            "So you don't know if any example that cannot be agnostically learn but can be ordinary quick learned that isn't finite or linear vent fan.",
            "That but I wouldn't be surprised if that was the case, actually.",
            "Well, I I'm just, I'm just.",
            "I'm not confident that there are too many things that can be non agnostically quick learned.",
            "Yeah, I don't know.",
            "I don't know the answer to that right.",
            "Yeah, yeah, OK, let's leave it like that.",
            "Anymore questions.",
            "OK I have a question.",
            "So in in the quick model what happens is that the learner is never allowed to fail, right?",
            "So if the But if you grant the power of the learner, the you know the if you allow it to fail a few times.",
            "Is there something you can say that's much better?",
            "Yeah, this is so you can analyze reinforcement learning in other other frameworks like NET Framework and that's the next thing you should say.",
            "And there the learner can fail at any number of times, as long as the failures you know are not causing lots travel in this kind of an analysis, it's kind of.",
            "It's like the construction of this basic armex idea.",
            "That kind of requires that, so it's like if you change the algorithm, then then you probably don't need this.",
            "But if you stick to this algorithm, it's like necessary that you work with this quick learning framework.",
            "Anymore questions.",
            "OK, so let's start with the the next talk."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I am lucky enough to actually present two talks for you guys today.",
                    "label": 0
                },
                {
                    "sent": "The first talk is together with SQL.",
                    "label": 0
                },
                {
                    "sent": "Each transita who is very sorry not to be able to present, but he had to go to a English literature conference to London and it's like you couldn't skip that.",
                    "label": 0
                },
                {
                    "sent": "So you can understand it like just learning theory versus English literature.",
                    "label": 0
                },
                {
                    "sent": "This is recorded right?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so this talk is about agnostic quick learning.",
                    "label": 0
                },
                {
                    "sent": "And we're trying to extend the quick learning frameworks so that we can handle larger programming classes and we are not restricted by what we assume about the environment.",
                    "label": 0
                },
                {
                    "sent": "This is, I think, natural desire, that we want to do, and so here's the outline for the talk.",
                    "label": 0
                },
                {
                    "sent": "So first, I'm going to introduce the basic concepts, talk about efficient reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "Introduce the quick framework for those who are not familiar with it, and then I'm going to talk about agnostic learning some definitions and like how it applies to the reinforcement learning on control problem and then results for several problem classes.",
                    "label": 1
                },
                {
                    "sent": "So it's pretty straight.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, So what is reinforcement learning so in reinforcement learning, illustrated by just getting lost in a maze, you want to learn about an environment by pursuing some long-term goals, like collecting reward your interest in, typically maximizing the sum up the rewards, overtime, and.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What makes this difficult is that you don't know about the environment, so you're learning simultaneously both environment and try to collect rewards and why you're doing that.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Since you don't know the environment, you have to go to places that you haven't seen before.",
                    "label": 0
                },
                {
                    "sent": "It's obvious because otherwise you can just miss the big rewards, so you really have to get to those places that you haven't seen.",
                    "label": 0
                },
                {
                    "sent": "No, the question is how much time to spend on visiting places that you haven't seen versus how much time to spend.",
                    "label": 0
                },
                {
                    "sent": "Parts of the space that you have already seen.",
                    "label": 0
                },
                {
                    "sent": "Given that wherever you go, you'll see some.",
                    "label": 0
                },
                {
                    "sent": "You will have some uncertainty left about how much reward you can collect there, because you know the word is stochastic, so you will never actually know for sure.",
                    "label": 0
                },
                {
                    "sent": "That OK, if you go to this place, then this is the best place ever.",
                    "label": 0
                },
                {
                    "sent": "'cause you have this noisy feedback.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's the problem that we study in reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "Or that's one major problem that studying.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, find learning and.",
                    "label": 0
                },
                {
                    "sent": "The various mathematical frameworks exist to study this problem.",
                    "label": 0
                },
                {
                    "sent": "In one framework you're interested in making a bounded number of harshly suboptimal stamps, so this is like a minimum requirement that you wish to achieve, and for this you need to balance exploration and exploitation.",
                    "label": 1
                },
                {
                    "sent": "And there are lots of results out there in the niche chair that cover various cases like 4.",
                    "label": 0
                },
                {
                    "sent": "You know, sing simple.",
                    "label": 0
                },
                {
                    "sent": "MDP's finite MDP is factored.",
                    "label": 0
                },
                {
                    "sent": "MDP's MVP sees linear dynamics in certain sense, and so on so forth, under which you can prove that there exists target items which are fairly efficiently which are able to fairly efficiently explore the environment and collect the rewards so they're not making sort of many mistake.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and so how these algorithms work?",
                    "label": 0
                },
                {
                    "sent": "Many of these algorithms work on the basis of these are Max construction, which is like a general scheme for constructing efficient reinforcement learning algorithms.",
                    "label": 1
                },
                {
                    "sent": "And the way these algorithms work is that they keep track of the unknown areas in the environment, like in the state space.",
                    "label": 1
                },
                {
                    "sent": "So part of the state space is classified as being on and.",
                    "label": 0
                },
                {
                    "sent": "The algorithm explicitly assumes that you know unknown territories are good to go to, so they're like heaven, so they really want to go there to explore.",
                    "label": 0
                },
                {
                    "sent": "And this is how they are going to balance exploration and exploitation and.",
                    "label": 0
                },
                {
                    "sent": "They, with this assumption, they come up with an optimal way to act.",
                    "label": 0
                },
                {
                    "sent": "So with this you know Twig model idea about the environment and then they follow what they believe is the best for them and you know two things can happen.",
                    "label": 0
                },
                {
                    "sent": "Either you stay in the area that you already know, in which case what happened is that like you're kind of disregarding those unknown areas, because there are two far away it would take too much time to get there.",
                    "label": 0
                },
                {
                    "sent": "And then you collect a lot of rewards.",
                    "label": 0
                },
                {
                    "sent": "Or the other thing that can happen is that you're leaving that area and you go to these unseen places and then you discover those and and so if that happens, then you actually learn.",
                    "label": 0
                },
                {
                    "sent": "So you may you didn't collect a reward because you plan to like you.",
                    "label": 0
                },
                {
                    "sent": "You set up your mind that you just want to go to these unseen places and so you give up your desire to collect the rewards in the the known area.",
                    "label": 0
                },
                {
                    "sent": "So you set up so high rewards that you really want to go to those places, and so if it happens that you actually go there, then you're going to gain information.",
                    "label": 0
                },
                {
                    "sent": "So in both cases you're gaining.",
                    "label": 0
                },
                {
                    "sent": "Something you want case?",
                    "label": 0
                },
                {
                    "sent": "You're gaining information.",
                    "label": 0
                },
                {
                    "sent": "The other case you're gaining reward and therefore you can eventually proved.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That things workout.",
                    "label": 0
                },
                {
                    "sent": "So there's the basic idea, and since there are many frameworks where these ideas been worked out, there was a desire to generalize the whole thing and factor out what is important about you.",
                    "label": 0
                },
                {
                    "sent": "Know the different aspects of this learning problem, and so the framework where this happens is called a quick learning framework.",
                    "label": 0
                },
                {
                    "sent": "That's an abbreviation for, knows what it knows and.",
                    "label": 0
                },
                {
                    "sent": "Max slides are going to be about that framework, so the important thing here is that there are the known areas and so what's important that the quick learner has to know.",
                    "label": 1
                },
                {
                    "sent": "If it knows something or doesn't know something, and the known area classification is based on the uncertainty left in what you learned about the environment, right?",
                    "label": 1
                },
                {
                    "sent": "A quick learner is going.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be such alright?",
                    "label": 0
                },
                {
                    "sent": "So here is an illustration of graphically the station what a quick learner is and what it needs to do.",
                    "label": 0
                },
                {
                    "sent": "So this is like a sequential online learning framework with A twist.",
                    "label": 0
                },
                {
                    "sent": "An agent is playing with an adversary and then adverse.",
                    "label": 0
                },
                {
                    "sent": "It's a prediction problem Interestingly, so it's like, you know, learning a model of the environment is like being able to predict what's going to happen next, so it's kind of be a prediction problem.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the adversary picks a concept that the learner needs to predict.",
                    "label": 1
                },
                {
                    "sent": "And then a sequential interaction begins.",
                    "label": 0
                },
                {
                    "sent": "So the concept is Chi.",
                    "label": 0
                },
                {
                    "sent": "And then the following.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Repeated and the adversary picks a query X, and this is communicated to the agent and then they.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Agent can to do things either it is size that it doesn't know enough about that particle point in space to predict.",
                    "label": 0
                },
                {
                    "sent": "What Jay would respond at that point, and then it says, OK, I'm going to pass, I don't know.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then in that case, the adversary is going to give the learner an noisy feedback and the learner cannot do.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Itself or the learner predicts an answer.",
                    "label": 0
                },
                {
                    "sent": "In which case?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The answer has to be accurate, so the protocol just ends THEOS.",
                    "label": 1
                },
                {
                    "sent": "If the answer is not sufficiently accurate.",
                    "label": 0
                },
                {
                    "sent": "And in this case, the learner doesn't get any feedback because, well, if it didn't fit, then it's everything is good, and if it failed then the game ended anyways, alright?",
                    "label": 0
                },
                {
                    "sent": "So there's the quick framework that was introduced by leave Ocean with money.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And eight.",
                    "label": 0
                },
                {
                    "sent": "And so the way this can be used in an MDP framework is as follows.",
                    "label": 0
                },
                {
                    "sent": "So this is like the umbrella algorithm that applies to many classes of environment.",
                    "label": 0
                },
                {
                    "sent": "So you just have you know, an MDP learner and a planner, and then so it is the loop and the agent is exploring the environment so it uses the planner too optimistically.",
                    "label": 0
                },
                {
                    "sent": "Plan in the with the help of the MDP learner, so the MDP learner is the quick agent that's learning about the environment.",
                    "label": 0
                },
                {
                    "sent": "So it picks the current state and then it uses its planning algorithm to come up with a good action.",
                    "label": 0
                },
                {
                    "sent": "So this planning algorithm can just use the model learn, which is a little bit modified in optimistic fashion, will come to that in a second and then execute.",
                    "label": 0
                },
                {
                    "sent": "That action observes the next state and the associated the board.",
                    "label": 0
                },
                {
                    "sent": "And then what happens is that if the MDP learner actually predicted for the given state action pair.",
                    "label": 0
                },
                {
                    "sent": "That it doesn't know the answer, so it passed.",
                    "label": 0
                },
                {
                    "sent": "And then it's given the opportunity to learn from the next state and the reward.",
                    "label": 0
                },
                {
                    "sent": "And so it tries to refine its understanding of the environment and then the root and the loop repeats this clear.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the optimistic rapper.",
                    "label": 0
                },
                {
                    "sent": "That's used here in the planning algorithm, so the planning algorithm can consider this model.",
                    "label": 0
                },
                {
                    "sent": "It can ask for the next state distribution and the rewards at any state action pair.",
                    "label": 0
                },
                {
                    "sent": "And so the way it works is that, well, it can call this prediction function, and this checks with the MDP learner if the MVP learner actually knows about that particular state action pair, right?",
                    "label": 0
                },
                {
                    "sent": "Then it returns whatever the MDP learner is predicting for the state action pair, the next day distribution, and the reward in the other case it determines.",
                    "label": 0
                },
                {
                    "sent": "Blue back to the same state like the next day distribution is such that you just go back to the same state and you get kind of maximum reward like you are in have a OK.",
                    "label": 0
                },
                {
                    "sent": "So if the planet is is bad with this modified model, what happens?",
                    "label": 0
                },
                {
                    "sent": "Is that exactly what I said before at the parts of the state space for which the learner says I don't know, you will like instant ate, a very huge reward in the other parts.",
                    "label": 0
                },
                {
                    "sent": "You just.",
                    "label": 0
                },
                {
                    "sent": "Predict whatever the learner would predict.",
                    "label": 0
                },
                {
                    "sent": "So that's like the umbrella.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and so this is a classic design that says that if there is an efficient quick learner then there is an efficient reinforcement learner.",
                    "label": 1
                },
                {
                    "sent": "Kind of is the same bond.",
                    "label": 0
                },
                {
                    "sent": "So what is an efficient quick learner by the way?",
                    "label": 0
                },
                {
                    "sent": "So the quick requirement is that with you know a part from a failure event that has a control probability.",
                    "label": 0
                },
                {
                    "sent": "A quick learner should never fail, and the number of passes should be bonded, so we're talking about like if the number of passes of the quick learner is bonded here, then the same number of mistakes are going to be inherited basically by the reinforcement learning agent, and this works very general.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So, but the big question is, what if the environment is not contained in the class that a quick learner uses?",
                    "label": 1
                },
                {
                    "sent": "That's the question.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So for example, you assume that the environment is a factor MD.",
                    "label": 1
                },
                {
                    "sent": "Peabody environments actually not a factor MVP.",
                    "label": 0
                },
                {
                    "sent": "Then we don't really have any guarantees, or you assume that there is some State app section going on, but the setup section is a little bit off and so forth.",
                    "label": 0
                },
                {
                    "sent": "So you could continue this so we can't really assume make make those strong assumptions about the environment.",
                    "label": 0
                },
                {
                    "sent": "It doesn't seem to be fair, OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then, what?",
                    "label": 0
                },
                {
                    "sent": "What can we do?",
                    "label": 0
                },
                {
                    "sent": "How can we modify the quick learning concept where we want to lose some kind of error?",
                    "label": 0
                },
                {
                    "sent": "So this is illustrated on this picture.",
                    "label": 0
                },
                {
                    "sent": "So the way you could do this is that you know the agent has a problem class in mind, and then there is another problem class that the environment has.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Is it 5?",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe I'm merchant talks.",
                    "label": 0
                },
                {
                    "sent": "So there is another environment class and there is a bounded distance between these two, and so this would be the concept that's picked by the environment and the agents.",
                    "label": 0
                },
                {
                    "sent": "Just thinking about the linear fun.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You modify the basic framework in such a way that you allow bigger errors.",
                    "label": 0
                },
                {
                    "sent": "So obviously if you cannot represent all the concepts after it reacts accuracy, then you have to lower slack.",
                    "label": 0
                },
                {
                    "sent": "The little bigger slack, and we introduce discount.",
                    "label": 0
                },
                {
                    "sent": "But it evenus framework but.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Otherwise, it's kind of the same.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so I'm going to just skip this.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the test because, well, it's kind of.",
                    "label": 0
                },
                {
                    "sent": "The same idea applies.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adding the definition of what constitutes an efficient bounded quick learner, and so it is 1 main theorem that we proved in the paper which just generalizes the previous result to this agnostic quick framework, basically justifying the framework.",
                    "label": 0
                },
                {
                    "sent": "So it says that if you have an agnostic quick learner with a certain complexity bond B, then that bond is inherited by this general reinforcement learner that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As shown before.",
                    "label": 0
                },
                {
                    "sent": "OK, so this justifies using this agnostic quickforce.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It works really nicely, but it leaves wide open.",
                    "label": 0
                },
                {
                    "sent": "The question of what can we agnostically quick learn right?",
                    "label": 1
                },
                {
                    "sent": "So in order not to make pretty vacuous claims we would need.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To understand what can we learn at all and so small exercises?",
                    "label": 0
                },
                {
                    "sent": "Basically, if you have a finite hypothesis class and you don't have any label noise.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know, then it's really easy, because for each query.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You just see if like the remaining you are ruling out hypothesis as you go, and if you check if the remaining hypothesis agree up to an accuracy of D, then you just predict what ever any of these hypothesis is say.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this prediction is gotta be 2D accurate and.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Otherwise, you have to pass because you don't know and.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you pass your risk.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The information that you can exclude at least one high.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Update this.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in this way, of course, this general learner is able to learn with at most each minus one passes, and it's going to make 2D accurate predictions.",
                    "label": 0
                },
                {
                    "sent": "So the competitiveness factor is 2.",
                    "label": 0
                },
                {
                    "sent": "Actually, you cannot improve that, unfortunately.",
                    "label": 0
                },
                {
                    "sent": "So we give a counterexample.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that would be a.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Run very quickly.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at that point, we didn't.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We learn anything, but we could predict at that point us by the adversary like the we have too many hypothesis is we can't predict.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we learn.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You excluded and so I'm running out of time so.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to skip this is bad.",
                    "label": 0
                },
                {
                    "sent": "This is like motivating another algorithm for the noisy case.",
                    "label": 0
                },
                {
                    "sent": "In the noisy case you what you have to do is pairwise competition between all.",
                    "label": 0
                },
                {
                    "sent": "The hypothesis is nothing else seems to work.",
                    "label": 0
                },
                {
                    "sent": "So what you do is that you have like 2 hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's say two linear functions and.",
                    "label": 0
                },
                {
                    "sent": "Then there are those regions where they differ by lot.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, and then if you have lots of examples and then you just average the examples over these examples, then you will clearly see that the averages are going to make a difference between the two functions and you can eventually detect one of them.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so that's how you.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learn.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so you can.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Normalize this and.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, and show some results so we come to this table of learning complexities which summarizes the results that we have so far.",
                    "label": 0
                },
                {
                    "sent": "So very simple case.",
                    "label": 0
                },
                {
                    "sent": "Finite deterministic case.",
                    "label": 0
                },
                {
                    "sent": "Opted to the approximation agnostic.",
                    "label": 0
                },
                {
                    "sent": "We can quick are actual and the finite noisy case.",
                    "label": 0
                },
                {
                    "sent": "You can see that if you have finitely many Hyper V Xen hyper disease, you can order this.",
                    "label": 0
                },
                {
                    "sent": "See some difference between quick learning and agnostic learning.",
                    "label": 0
                },
                {
                    "sent": "But if you come to more complicated things, still very simple, like linear predictors, dimensional in our predictors you see huge differences suddenly so quick learning is just solving linear program.",
                    "label": 0
                },
                {
                    "sent": "That's really easy.",
                    "label": 0
                },
                {
                    "sent": "However, in the case of agnostic learning we have an upper bound of this slide which is like huge huge huge, very scary and we have a lower bound that shows that at least one of the exponential scaring has to be there.",
                    "label": 0
                },
                {
                    "sent": "So it's not very nice at the moment and for the D dimensional linear noisy case.",
                    "label": 0
                },
                {
                    "sent": "You also have an exponential blowup, so it's not looking very rosy at the moment.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary, this is a framework.",
                    "label": 1
                },
                {
                    "sent": "Where we allowed the adversary to choose hypothesis outside of the concept class.",
                    "label": 0
                },
                {
                    "sent": "Used by the learner.",
                    "label": 0
                },
                {
                    "sent": "That's a very natural extension of the quick framework and we justify this framework.",
                    "label": 0
                },
                {
                    "sent": "The choices of the other measures and such by proving the previous results in this new framework regarding the efficiency of frame first.",
                    "label": 0
                },
                {
                    "sent": "Math learning, so you can design efficient reinforcement learning agents using this framework that can go beyond.",
                    "label": 0
                },
                {
                    "sent": "So what can be represented explicitly with the hypothesis class?",
                    "label": 1
                },
                {
                    "sent": "So that's good news, but generally we find that agnostically quick learning even simple things is getting pretty hard.",
                    "label": 0
                },
                {
                    "sent": "Very soon.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so within handle agnostic quick learner learning for transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "But we think it can be done and so one very here is that maybe you know the framework is not truly idea because very soon we are getting this negative results which show that agnostic quick learning is actually very very difficult and there is nothing that says that this is actually necessary.",
                    "label": 0
                },
                {
                    "sent": "So maybe this decomposition.",
                    "label": 0
                },
                {
                    "sent": "No, the reinforcement learning paperin factoring out this quick learning aspect to it is not ideal, and it's not going to go very far.",
                    "label": 0
                },
                {
                    "sent": "We don't know it's interesting to know that there are not many designs or not.",
                    "label": 0
                },
                {
                    "sent": "Any results in the literature there where you would agnostically efficiently in any sense, reinforcement, learning, and environment.",
                    "label": 0
                },
                {
                    "sent": "So these are, you know, groundbreaking first results, but at the same time this seems like a difficult problem maybe.",
                    "label": 0
                },
                {
                    "sent": "Thank you please.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Hi, so I've been interested in quick learning recently and something I've noticed is for just ordinary non ignostic quick learning in the noisy setting right?",
                    "label": 0
                },
                {
                    "sent": "It seems to be a very strong.",
                    "label": 0
                },
                {
                    "sent": "The quick guarantee seems to be very strong in that I don't know of too many.",
                    "label": 0
                },
                {
                    "sent": "Domains besides noisy linear regression, where you can even do non ignostic quick learning.",
                    "label": 0
                },
                {
                    "sent": "So do you have an example?",
                    "label": 0
                },
                {
                    "sent": "So did you only near aggression is that is that the result?",
                    "label": 0
                },
                {
                    "sent": "OK, that's right.",
                    "label": 0
                },
                {
                    "sent": "So you don't know if any example that cannot be agnostically learn but can be ordinary quick learned that isn't finite or linear vent fan.",
                    "label": 0
                },
                {
                    "sent": "That but I wouldn't be surprised if that was the case, actually.",
                    "label": 0
                },
                {
                    "sent": "Well, I I'm just, I'm just.",
                    "label": 0
                },
                {
                    "sent": "I'm not confident that there are too many things that can be non agnostically quick learned.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't know the answer to that right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, OK, let's leave it like that.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "OK I have a question.",
                    "label": 0
                },
                {
                    "sent": "So in in the quick model what happens is that the learner is never allowed to fail, right?",
                    "label": 0
                },
                {
                    "sent": "So if the But if you grant the power of the learner, the you know the if you allow it to fail a few times.",
                    "label": 0
                },
                {
                    "sent": "Is there something you can say that's much better?",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is so you can analyze reinforcement learning in other other frameworks like NET Framework and that's the next thing you should say.",
                    "label": 0
                },
                {
                    "sent": "And there the learner can fail at any number of times, as long as the failures you know are not causing lots travel in this kind of an analysis, it's kind of.",
                    "label": 0
                },
                {
                    "sent": "It's like the construction of this basic armex idea.",
                    "label": 0
                },
                {
                    "sent": "That kind of requires that, so it's like if you change the algorithm, then then you probably don't need this.",
                    "label": 0
                },
                {
                    "sent": "But if you stick to this algorithm, it's like necessary that you work with this quick learning framework.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's start with the the next talk.",
                    "label": 0
                }
            ]
        }
    }
}