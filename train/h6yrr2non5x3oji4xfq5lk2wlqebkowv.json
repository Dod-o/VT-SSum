{
    "id": "h6yrr2non5x3oji4xfq5lk2wlqebkowv",
    "title": "Trade-Offs in Sampling-Based Adversarial Planning",
    "info": {
        "author": [
            "Raghuram Ramanujan, Department of Computer Science, Cornell University"
        ],
        "published": "July 21, 2011",
        "recorded": "June 2011",
        "category": [
            "Top->Computer Science->Artificial Intelligence->Planning and Scheduling"
        ]
    },
    "url": "http://videolectures.net/icaps2011_ramanujan_sampling/",
    "segmentation": [
        [
            "Thanks Martha on Raghuram Anagen from Cornell.",
            "This is presentation on our work in our paper title, tradeoffs in sampling based adversarial planning, joint work with Bart."
        ],
        [
            "So many of you might be aware of the upper confidence bounds for trees algorithm.",
            "It's been all the rage in the game playing community for the past few years, most notably because of its high profile success in the game of Go where it helped elevate the standard of computer.",
            "Go playing from that of a very weak level to master level.",
            "Play in mind by 9 Go, and since then it's also been successfully adapted for other challenging applications like general game playing and real time tactical assault planning."
        ],
        [
            "But given this impressive success, our understanding of the algorithm is still at a very preliminary stage.",
            "There are still a lot of gaps in what we know about it, and in particular our empirical understanding at this point is still fairly anecdotal, so we know that it works in some domains and doesn't work so well in others, but we really don't have a good understanding of why this is.",
            "So this work really focuses on trying to address that issue, so we were trying to gain deeper insights into why UCT works well in some domains by comparing it to the better understood mini Max algorithm in domains where this is the sort of comparison is feasible.",
            "So UCT actually has its roots in the so called multi Arm Bandit."
        ],
        [
            "Problem and well, what is that?",
            "Well, that's certainly an answer."
        ],
        [
            "But really what I mean is.",
            "Imagine you're in Vegas and you're feeling lucky and you've decided to play the slot machines.",
            "And you're faced with this dilemma now, so you need to come up with some sort of policy or strategy that tells you how you should allocate your money between these different machines.",
            "So each one pays you money based on some underlying distribution that you don't really know, and your task is you're trying to maximize how much money you can make.",
            "So in 2002, Peter Auer and his colleagues proposed this really elegant solution to this problem.",
            "Which is asymptotically optimal, an essentially all it says is you want to play every machine once, and thereafter you're going to play the machine that maximizes this quantity.",
            "So I'll explain what that means in a second.",
            "An essentially this term Q of K is the observed mean payoff of the Kate machine.",
            "Anna K is NFK is the number of times that you've chosen to play the Kate Machine.",
            "And finally T is the total number of times that you played any machine, so the total number of trials and what this quantity computes is essentially an upper confidence bound on the true mean of the true mean payoff of any of these machines.",
            "So that's where the initials UCB come from, but perhaps more into."
        ],
        [
            "The way to think of this expression is to split it apart into two components.",
            "So you have this Q of K. So essentially this term is going to be big for any machine that pays you a lot of money on a regular basis, so.",
            "Essentially, that rewards machines that have a high payoff.",
            "An in the second term, if you observe if she grows large and FK grows small, the second term grows in size.",
            "So in other words, if I've played these slots a number of times, but there's a particular machine that I've been ignoring.",
            "I'm going to get a bonus for exploring that machine a little more.",
            "And finally, you have this parameter C, which you can set to sort of balance these two competing demands.",
            "And what does this have to do with game playing?"
        ],
        [
            "So the insight behind UCT is that.",
            "OK, so the inside behind UCT is the realization that if you're incrementally building up a game tree then you sort of facing a similar dilemma in that if you're at some state and you're trying to decide what how you should explore this further, you're sort of torn between exploring what you know is a good move whereas or whether you should go and sort of investigate moves that you haven't really explored as much further.",
            "So."
        ],
        [
            "Taking this idea further, so I'll just quickly step through the algorithm.",
            "So what it essentially does is it builds up a search tree incrementally, so we're going to pick up at a point where you already have some tree and that you city is built up, and we're essentially going to do what I just described, so will treat every internal node as though it were multi unbanded.",
            "And what we do is we use this UCB one selection policy to descend this tree.",
            "So you're going to do that, and maybe you pick.",
            "This is the best.",
            "No Dan.",
            "Now note that we're at a position where it's our opponents turn to go so it doesn't make sense to maximize the payoff.",
            "So you're going to do the opposite, which is to minimize a symmetric lower confidence bound.",
            "And then you repeat this until you descend until so you repeat this to descend the tree, and then eventually you're going to hit some leaf node in the current search tree that you built up.",
            "Well, what do you do next so UCT adds a new node.",
            "So your tree is going to grow by one more node and then you need some means of estimating the value of this new node that you created.",
            "So at this point the algorithm performs so traditionally what's performed is something called a random play out, which is essentially a random completion of the game.",
            "So you're just going to play random moves until the game terminates, and then you get some feedback value.",
            "So something drawn from minus 1 + 1 to indicate a loss or win.",
            "Anne, once this."
        ],
        [
            "This done, you're going to do a backward pass and propagate this new information that you just discovered back up the tree.",
            "So you're going to update every Nook to come down the street, and the update is really very straightforward.",
            "So you're going to update the visit count for every node, so you just add one and the 2nd update has to do with the utility of these states.",
            "So all you're going to do is you're going to take the new feedback value R and you're going to average it into your initial estimate.",
            "So that was a rather static view of the algorithm, but."
        ],
        [
            "So here's a more dynamic view together give you hopefully a better feel for what it does, so you can't really see the other lines, but.",
            "Essentially, you're going to start at the root node every time, which is the Blue Square, and you're going to descend down the street, so that's that's the.",
            "That's the blue line, and as you do that, you're essentially getting feedback from layouts and propagating that back up.",
            "As I just described, there's other lines in there which are not really showing up, which would be the remainder of the tree that you've built so far.",
            "So you do this for as long as time allows you, and at the end, So what you have a decision to make so you have a decision to make.",
            "So you're simply going to look at the children at the first level and then pick the move that has either the highest mean or has been sampled the most number of times.",
            "And that's the movie making the game."
        ],
        [
            "Compare this to the traditional way of building trees, which is the minimax algorithm which is essentially just a depth first traversal of your tree up to some depth cut off.",
            "So what it ends up growing is a very regular sort of tree that it explores in a systematic fashion."
        ],
        [
            "So.",
            "Maybe this hopefully gives a slightly better picture, but maybe not.",
            "So essentially what you have these two algorithms that build very different kinds of trees, right?",
            "So UCT build something that looks like this, so it's kind of.",
            "It goes very deep into some parts of the search space, but it stays shallow in other parts, whereas minimax build something that's far more regular and it's a complete algorithm in the sense that modulo Alpha beta pruning it's going to essentially examine every node within this search frontier an.",
            "Currently what we know is that in go this is what works best, an in chess and checkers, and many other games that's traditionally been the method of choice.",
            "Anne."
        ],
        [
            "And as I mentioned earlier, our objective here is to sort of understand why UCT works better in some domains than others, and we're hoping to find a domain where both will sort of produce a reasonable level of play.",
            "So we can.",
            "So such a comparison is feasible, except here's the rub.",
            "We currently don't have such a domain, so either one algorithm is really good and the other one sucks, or vice versa.",
            "So we're really on the lookout for an algorithm where straight out of the box.",
            "Both algorithms can do pretty well."
        ],
        [
            "And in our quest, what we discovered is that this game is actually one such domain.",
            "The game of Mancala, which is fairly popular in parts of Africa and Asia, and it's a two player game played on a board that looks like this.",
            "So what you have are a number of pits and you have some number of stones in each pit, and the objective is you're trying to capture as many pets as you can.",
            "And a move consists of essentially just picking up the stones in a pit and then you saw them in a counterclockwise fashion.",
            "And if you have more stones when you reach the end, you simply wrap around and then you start putting them in on your opponents side.",
            "So you keep going this way and then any stone that ends up in these longer pits in the ends.",
            "So those are the stores, so those are the captured stones."
        ],
        [
            "And like I said, we discovered that in this domain, UCT actually does pretty well out of the box, and so we went, OK, great.",
            "So this is a domain where minimax is traditionally been the approach.",
            "And now we've been able to get you City to do pretty well here.",
            "So then we said about dissecting the algorithm.",
            "So we basically took it apart and started looking into which is a components of UCT actually contributed to its success here.",
            "And we examine 3K3 key tradeoffs.",
            "So first one has to do with.",
            "The topology of the tree that you see builds.",
            "So whether you build a more complete tree or whether you build a sparsity.",
            "The second one has to do with whether you want to perform more sampling or you want to create bigger trees.",
            "So recall that I said you perform a single player data leaf node.",
            "When you reach it, but there's really nothing stopping you from doing more playouts except you take a computational hit, so processing each leaf node will not take longer.",
            "Trade off there and finally I mentioned that when you propagate the reward signal up, it's usually averaged into your previous utility estimates.",
            "But this there's alternatives to that as well and we look into how using many maxing backups helps.",
            "And once we examined all this, we actually created a you CD player which was capable of beating.",
            "Minimax agents in this domain, but again, the quest is not to create a winning mancala player using UCT.",
            "The questions to understand Whyyou City works and what we discovered was that essentially in.",
            "In games, it appears that there can be phases of a game that are more conducive to one sort of search algorithm over another.",
            "So the traditional approach has been to just take an algorithm and then use it for all phases of the game.",
            "But our work suggests that it might be more advantages to use different search strategies based on the local structure of the search space, so that's a quick preview of the results so."
        ],
        [
            "I've been to the details, so this first tradeoff I mentioned has to do with the topology of the search space or what kind of trees do we build and what works better.",
            "And recall that I mentioned the UCB one formula has this for man in particular.",
            "There's this constant C which you can tune to sort of set whether you want to do more exploration or exploitation."
        ],
        [
            "And in this experiment, what we essentially do, is we ready that parameter?",
            "So that's what the X axis is, and on the Y axis, what we're plotting is how does a UCT agent with that setting of that parameter perform against a standardized mini Max agent?",
            "So when this value is really small.",
            "Well, that's almost an invisible tree.",
            "But I guess what you really get is this very sparse tree.",
            "So it goes very deep into some parts of the search space and stays very shallow otherwise.",
            "And when it's sort of too large, then you City does a lot of exploration and what you get is almost something that's more mini Max like, so it's a much more complete tree.",
            "And if you look at this sort of the shape of the curve, what's striking is that there's this clear peak, so there's a point where you sort of hit the sweet spot.",
            "An UCT builds a tree that somewhere in between these two extremes.",
            "And why this is kind of interesting is because I guess when chess research was in full swing in the 70s and 80s, conventional wisdom had it.",
            "So they discovered that.",
            "If you could do complete search, then it was really hard to be so people experimented with trying to use heuristics to sort of cleverly prune out really bad moves, but that didn't do so well.",
            "As you know, just using the time to just do a complete do a complete search and what this suggests is that we should perhaps be re examining that experience, and in particular the fact that we able to use trees like this to beat Minimax suggests that there may be domains out there where.",
            "Doing a selective search can actually be better than doing sort of a complete search like minimax."
        ],
        [
            "A couple of the other tradeoffs I mentioned, which I won't talk about too much in detail.",
            "I'd encourage you to read the paper, but I'll mention the results quickly.",
            "And the first one had to do with this sampling versus tree size issue, and the tradeoff here is, well, I can either build a big tree but perform just few players at the leaf nodes, or I can build a small tree, but do a lot of players at the leaf nodes and what we discovered that.",
            "Well, with a large tree and fewer play out, you actually get better performance.",
            "So in work we presented at UI, so this dovetails nicely with results we presented at UI, which basically said that random playouts surprisingly have information in them.",
            "It's just not very good quality, so it's kind of understandable that doing more random players doesn't buy you much more.",
            "And the other tradeoff I mentioned was this tradeoff between doing averaging and doing min maxing.",
            "So how should you back the information up and combine it with your previous estimates?",
            "So the thing about this domain is so the problem with goal was that we didn't really have good heuristics, so we had no idea how to do how to construct one for go.",
            "Soap layouts worked out really well there, but in this domain we actually have a good heuristic, so we decided to plug that in instead.",
            "So we totally do away with the play outs and then just use heuristics in place of 'em and what we discovered is that when you have such high quality heuristics, doing a mini maxing backup actually works better."
        ],
        [
            "So based on these insights, we constructed a new variant of UCT UCT that we call that we term UCT Max H. An in games against a minimax agent, we're able to defeat Minimax players, searching up to depth 12 after normalizing for things like the amount of search effort.",
            "Did the two algorithms perform?",
            "And now the question is, well, great, so we're beating Minimax, but what does that tell us about UCT?",
            "So how is it?",
            "From where does it driving this advantage?"
        ],
        [
            "And for that I need to give you a little bit of background about this notion of trap states.",
            "So this is based on a talk I gave it.",
            "I caps last year, but informally a trap state is basically a position where you can make a stupid move and then lose very quickly, right?",
            "So in chess and go differ in this aspect.",
            "So in chess you have these trap states at every level of the game.",
            "You can potentially lose a game after four moves if you're really bad at chess like I am, Whereas in go these trap states don't really appear.",
            "Until the latter stages of the game, so there's this vast beginning phase where there are really no trap sticks."
        ],
        [
            "And we showed that UCT has real problems.",
            "Insert spaces where there are strapped states.",
            "In particular, it can sort of get confused between what constitutes a good move and what constitutes a bad move.",
            "So if we are interested in showing so."
        ],
        [
            "Where does mancala fit in here?",
            "Well, it actually is somewhere in between, so similar to go in the sense that there's this big area at the beginning of the game where there are no trap States and then they show up later.",
            "But it's inherently a less complex game then go, so it's a smaller state space.",
            "So this blue region is much smaller than the comparable region in go.",
            "And since we're interested in seeing if you see, Twitter does better in the absence of traps, we can set up the following experiment.",
            "So."
        ],
        [
            "Essentially, in this Blue Zone.",
            "Will play RUCT variant against Minimax, and then once we detect that we've entered this zone with all the traps in it, we're going to switch over and we're just going to complete the game with two identical players.",
            "So essentially any difference in the standard of play will be due to the different decisions that were made up there."
        ],
        [
            "And that's exactly what we do.",
            "So we vary the search depth.",
            "Of the minimax opponent, that UCD Max plays against, and we measure the win rate of UCT Max agent against the Minimax player.",
            "And there are two curves here.",
            "Believe it or not.",
            "So this one, this ghostly curve that's marked by the points and the darker ones.",
            "So the darker one is.",
            "The curve for the partial game setting that I just described.",
            "So where you play UCT just in the higher reaches of the tree and then switch over.",
            "And the second curve is for the setting where you just play UCT against minimax all the way through.",
            "So there's no switchover phase.",
            "And what we will observe is that this clear gap in performance.",
            "So the curve actually shifts up when you force UCT to just make decisions in the top reaches of the game.",
            "So in parts of the game where there are no traps.",
            "So if you restrict you cities decision-making to just that part of the game, it seems to do better.",
            "So what this suggests is that even within the same game, that could be phases of the game, that sort of favor one sort of search strategy over another."
        ],
        [
            "So in conclusion, just a quick summary of our results.",
            "So we studied these trail tradeoffs in UCT and we discovered that this exploration, exploitation, balancing, is actually valuable in U city and it gives us something.",
            "And that many maxing backups actually work better with heuristics than averaging backups, but I guess the most interesting result was the fact that you can create hybrid search algorithms, which can basically try to probe the sort of the local search space that they're in.",
            "And then decide to choose a search strategy based on that.",
            "And it seems that doing that sort of using those hybrid search strategies can often be more effective than using one or the other.",
            "The whole way through.",
            "And as a follow up to this, our current work focuses on sort of extending these sort of investigations into synthetic domains.",
            "I gave a talk about this at the MTS Workshop here at Icaps, and I'd encourage you to read the paper that paper as well as sort of a follow up to this work.",
            "But yeah, thank you for your time.",
            "So how do you detect when you've gone from the blue states into the red states to switch over?",
            "So essentially we run a very quick minimax search, so before its usage is turned to win, we run a deeper minimax search to sort of count of the number of terminal nodes that were encountered within that.",
            "Search horizon and if we detect that some threshold has been exceeded then we just say where we're out of the Blue Zone.",
            "Back in Apple head.",
            "Yes, we ignore were mixed in the look ahead.",
            "That's simply just to see what's what region of the search space within.",
            "Melon.",
            "Yeah, so your last graph.",
            "I think I might have not got the full point, so the so."
        ],
        [
            "So you are varying the mini Max strength.",
            "Yep, and as you see T also expanding the same number of nodes.",
            "Yes, so that's always normalized for, so you see T always only gets to expand at most a tree as big as what many Max is sort of indicating that.",
            "Mini Max is really going to overtake it.",
            "Yeah, we do reach a point where it's hard.",
            "It gets harder to beat Minimax, and that's because I mean the deeper you look ahead.",
            "The issue is that wraps start appearing sooner, so especially in these partial game settings.",
            "So if you're playing against, say, a mini Max depth six player, the Blue Zone may last as many as 16 or 18 plies, but if you're playing against, say, many maxdepth 14 or death 16 player, that phase may only last five or 6 plus.",
            "So really you City has it's not really enough time for one player or the other to build up enough of a positional advantage.",
            "So that's one issue.",
            "And of course, if you are looking deeper and deeper ahead.",
            "Many Macs handles these terminal positions optimally, so it's really hard to beat it in that sense, so that's that's the other issue.",
            "It does it follow from what you are saying that you should eat does not make much sense when the evolution function is good, that is that in principle cannot beat minimax.",
            "OK, you mentioned that one type of minimax backups are better than average, but what about in general?",
            "So when you have good evaluation functions, probably use it, you will make less sense that more.",
            "Systematic methods.",
            "I don't think there's much work out there on using very good heuristics.",
            "I mean, I've almost always seen UCT combined with Playouts.",
            "There's not much work out there on combining it with touristics.",
            "If there if it is combined with heuristics.",
            "This usually also always a player component, so.",
            "Does that answer the question or I?",
            "OK. OK.",
            "So it seems that you cities starting to perform poorly when the cost of making mistake becomes very, very high.",
            "So if you make a mistake, you know you essentially lose right?",
            "So in this case, minimax essentially natural doing better because it's exploring all the options, right?",
            "So I was wondering, have you tried?",
            "Once you detect that you're in this?",
            "Stage when mistakes become costly, but if you turn up the exploration parameter of you citymax really really high so that it's forced to explore all the options.",
            "Have you tried this?",
            "We haven't tried it, but my intuition is that that one works so well because many Max agent also performs what's known as Alpha beta pruning, and that essentially is a huge speedup over just vanilla minimax.",
            "An UCT doesn't do that, so it still won't be building as deep a tree's minimax would.",
            "Actually I will use this opportunity and I will follow up on this question so when you look at UCT on the original paper that was written for MVP's.",
            "One thing that drives me crazy is that UCB is UCB component is the wrong thing to do.",
            "Why so?",
            "Because UCB was designed for regret minimization for clinical test kind of settings, while in MDP is definitely doing games we don't pay while we examine our simulation, right?",
            "Yeah, so going back to two.",
            "Do you kind of conclusions at the end?",
            "It will be interesting to see what will happen in the blue area if you replace UCT with just with the UCB component.",
            "Who is just uniform sampling?",
            "Because in theory it gives so the trick there is that it gives much better worst case guarantees for simple regret, not for regret minimization.",
            "But for simple regret minimization, which is like try whatever you want then at the.",
            "At the end the connection, right?",
            "But it comes with the, but it pays off starting with the kind of large number of samples right?",
            "Exactly, so it's kind of interesting thing to yeah, and to sort of add to that I guess the other thing we've looked at is sort of budgeted banded algorithms, so which are somewhat along the same line.",
            "So I guess what his point was, is that.",
            "I guess you see doesn't take into the fact that what you do during the search phase doesn't really play into the decision you make at the end, and also in particular, it doesn't take into the fact that you have a limited budget.",
            "So if I had an infinite budget, I might choose, which is what.",
            "Essentially, UCT assumes you might choose to build a different tree as compared to what you would build if you knew that you only get 10,000 iterations or whatever, and we did look into that at some point we tried budgeted bandit algorithms, and we couldn't really see any noticeable difference in the performance.",
            "Not in that particular setup though, but that would be something to try.",
            "OK, so thanks again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks Martha on Raghuram Anagen from Cornell.",
                    "label": 0
                },
                {
                    "sent": "This is presentation on our work in our paper title, tradeoffs in sampling based adversarial planning, joint work with Bart.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So many of you might be aware of the upper confidence bounds for trees algorithm.",
                    "label": 1
                },
                {
                    "sent": "It's been all the rage in the game playing community for the past few years, most notably because of its high profile success in the game of Go where it helped elevate the standard of computer.",
                    "label": 0
                },
                {
                    "sent": "Go playing from that of a very weak level to master level.",
                    "label": 0
                },
                {
                    "sent": "Play in mind by 9 Go, and since then it's also been successfully adapted for other challenging applications like general game playing and real time tactical assault planning.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But given this impressive success, our understanding of the algorithm is still at a very preliminary stage.",
                    "label": 1
                },
                {
                    "sent": "There are still a lot of gaps in what we know about it, and in particular our empirical understanding at this point is still fairly anecdotal, so we know that it works in some domains and doesn't work so well in others, but we really don't have a good understanding of why this is.",
                    "label": 1
                },
                {
                    "sent": "So this work really focuses on trying to address that issue, so we were trying to gain deeper insights into why UCT works well in some domains by comparing it to the better understood mini Max algorithm in domains where this is the sort of comparison is feasible.",
                    "label": 0
                },
                {
                    "sent": "So UCT actually has its roots in the so called multi Arm Bandit.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem and well, what is that?",
                    "label": 0
                },
                {
                    "sent": "Well, that's certainly an answer.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But really what I mean is.",
                    "label": 0
                },
                {
                    "sent": "Imagine you're in Vegas and you're feeling lucky and you've decided to play the slot machines.",
                    "label": 0
                },
                {
                    "sent": "And you're faced with this dilemma now, so you need to come up with some sort of policy or strategy that tells you how you should allocate your money between these different machines.",
                    "label": 0
                },
                {
                    "sent": "So each one pays you money based on some underlying distribution that you don't really know, and your task is you're trying to maximize how much money you can make.",
                    "label": 0
                },
                {
                    "sent": "So in 2002, Peter Auer and his colleagues proposed this really elegant solution to this problem.",
                    "label": 0
                },
                {
                    "sent": "Which is asymptotically optimal, an essentially all it says is you want to play every machine once, and thereafter you're going to play the machine that maximizes this quantity.",
                    "label": 1
                },
                {
                    "sent": "So I'll explain what that means in a second.",
                    "label": 0
                },
                {
                    "sent": "An essentially this term Q of K is the observed mean payoff of the Kate machine.",
                    "label": 1
                },
                {
                    "sent": "Anna K is NFK is the number of times that you've chosen to play the Kate Machine.",
                    "label": 0
                },
                {
                    "sent": "And finally T is the total number of times that you played any machine, so the total number of trials and what this quantity computes is essentially an upper confidence bound on the true mean of the true mean payoff of any of these machines.",
                    "label": 0
                },
                {
                    "sent": "So that's where the initials UCB come from, but perhaps more into.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The way to think of this expression is to split it apart into two components.",
                    "label": 0
                },
                {
                    "sent": "So you have this Q of K. So essentially this term is going to be big for any machine that pays you a lot of money on a regular basis, so.",
                    "label": 0
                },
                {
                    "sent": "Essentially, that rewards machines that have a high payoff.",
                    "label": 0
                },
                {
                    "sent": "An in the second term, if you observe if she grows large and FK grows small, the second term grows in size.",
                    "label": 0
                },
                {
                    "sent": "So in other words, if I've played these slots a number of times, but there's a particular machine that I've been ignoring.",
                    "label": 1
                },
                {
                    "sent": "I'm going to get a bonus for exploring that machine a little more.",
                    "label": 0
                },
                {
                    "sent": "And finally, you have this parameter C, which you can set to sort of balance these two competing demands.",
                    "label": 0
                },
                {
                    "sent": "And what does this have to do with game playing?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the insight behind UCT is that.",
                    "label": 0
                },
                {
                    "sent": "OK, so the inside behind UCT is the realization that if you're incrementally building up a game tree then you sort of facing a similar dilemma in that if you're at some state and you're trying to decide what how you should explore this further, you're sort of torn between exploring what you know is a good move whereas or whether you should go and sort of investigate moves that you haven't really explored as much further.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Taking this idea further, so I'll just quickly step through the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So what it essentially does is it builds up a search tree incrementally, so we're going to pick up at a point where you already have some tree and that you city is built up, and we're essentially going to do what I just described, so will treat every internal node as though it were multi unbanded.",
                    "label": 0
                },
                {
                    "sent": "And what we do is we use this UCB one selection policy to descend this tree.",
                    "label": 0
                },
                {
                    "sent": "So you're going to do that, and maybe you pick.",
                    "label": 0
                },
                {
                    "sent": "This is the best.",
                    "label": 0
                },
                {
                    "sent": "No Dan.",
                    "label": 0
                },
                {
                    "sent": "Now note that we're at a position where it's our opponents turn to go so it doesn't make sense to maximize the payoff.",
                    "label": 0
                },
                {
                    "sent": "So you're going to do the opposite, which is to minimize a symmetric lower confidence bound.",
                    "label": 1
                },
                {
                    "sent": "And then you repeat this until you descend until so you repeat this to descend the tree, and then eventually you're going to hit some leaf node in the current search tree that you built up.",
                    "label": 0
                },
                {
                    "sent": "Well, what do you do next so UCT adds a new node.",
                    "label": 0
                },
                {
                    "sent": "So your tree is going to grow by one more node and then you need some means of estimating the value of this new node that you created.",
                    "label": 0
                },
                {
                    "sent": "So at this point the algorithm performs so traditionally what's performed is something called a random play out, which is essentially a random completion of the game.",
                    "label": 0
                },
                {
                    "sent": "So you're just going to play random moves until the game terminates, and then you get some feedback value.",
                    "label": 0
                },
                {
                    "sent": "So something drawn from minus 1 + 1 to indicate a loss or win.",
                    "label": 0
                },
                {
                    "sent": "Anne, once this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This done, you're going to do a backward pass and propagate this new information that you just discovered back up the tree.",
                    "label": 0
                },
                {
                    "sent": "So you're going to update every Nook to come down the street, and the update is really very straightforward.",
                    "label": 0
                },
                {
                    "sent": "So you're going to update the visit count for every node, so you just add one and the 2nd update has to do with the utility of these states.",
                    "label": 1
                },
                {
                    "sent": "So all you're going to do is you're going to take the new feedback value R and you're going to average it into your initial estimate.",
                    "label": 0
                },
                {
                    "sent": "So that was a rather static view of the algorithm, but.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a more dynamic view together give you hopefully a better feel for what it does, so you can't really see the other lines, but.",
                    "label": 0
                },
                {
                    "sent": "Essentially, you're going to start at the root node every time, which is the Blue Square, and you're going to descend down the street, so that's that's the.",
                    "label": 0
                },
                {
                    "sent": "That's the blue line, and as you do that, you're essentially getting feedback from layouts and propagating that back up.",
                    "label": 0
                },
                {
                    "sent": "As I just described, there's other lines in there which are not really showing up, which would be the remainder of the tree that you've built so far.",
                    "label": 0
                },
                {
                    "sent": "So you do this for as long as time allows you, and at the end, So what you have a decision to make so you have a decision to make.",
                    "label": 0
                },
                {
                    "sent": "So you're simply going to look at the children at the first level and then pick the move that has either the highest mean or has been sampled the most number of times.",
                    "label": 0
                },
                {
                    "sent": "And that's the movie making the game.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compare this to the traditional way of building trees, which is the minimax algorithm which is essentially just a depth first traversal of your tree up to some depth cut off.",
                    "label": 0
                },
                {
                    "sent": "So what it ends up growing is a very regular sort of tree that it explores in a systematic fashion.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Maybe this hopefully gives a slightly better picture, but maybe not.",
                    "label": 0
                },
                {
                    "sent": "So essentially what you have these two algorithms that build very different kinds of trees, right?",
                    "label": 0
                },
                {
                    "sent": "So UCT build something that looks like this, so it's kind of.",
                    "label": 0
                },
                {
                    "sent": "It goes very deep into some parts of the search space, but it stays shallow in other parts, whereas minimax build something that's far more regular and it's a complete algorithm in the sense that modulo Alpha beta pruning it's going to essentially examine every node within this search frontier an.",
                    "label": 0
                },
                {
                    "sent": "Currently what we know is that in go this is what works best, an in chess and checkers, and many other games that's traditionally been the method of choice.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And as I mentioned earlier, our objective here is to sort of understand why UCT works better in some domains than others, and we're hoping to find a domain where both will sort of produce a reasonable level of play.",
                    "label": 1
                },
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "So such a comparison is feasible, except here's the rub.",
                    "label": 0
                },
                {
                    "sent": "We currently don't have such a domain, so either one algorithm is really good and the other one sucks, or vice versa.",
                    "label": 0
                },
                {
                    "sent": "So we're really on the lookout for an algorithm where straight out of the box.",
                    "label": 0
                },
                {
                    "sent": "Both algorithms can do pretty well.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in our quest, what we discovered is that this game is actually one such domain.",
                    "label": 0
                },
                {
                    "sent": "The game of Mancala, which is fairly popular in parts of Africa and Asia, and it's a two player game played on a board that looks like this.",
                    "label": 0
                },
                {
                    "sent": "So what you have are a number of pits and you have some number of stones in each pit, and the objective is you're trying to capture as many pets as you can.",
                    "label": 0
                },
                {
                    "sent": "And a move consists of essentially just picking up the stones in a pit and then you saw them in a counterclockwise fashion.",
                    "label": 1
                },
                {
                    "sent": "And if you have more stones when you reach the end, you simply wrap around and then you start putting them in on your opponents side.",
                    "label": 0
                },
                {
                    "sent": "So you keep going this way and then any stone that ends up in these longer pits in the ends.",
                    "label": 0
                },
                {
                    "sent": "So those are the stores, so those are the captured stones.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And like I said, we discovered that in this domain, UCT actually does pretty well out of the box, and so we went, OK, great.",
                    "label": 0
                },
                {
                    "sent": "So this is a domain where minimax is traditionally been the approach.",
                    "label": 0
                },
                {
                    "sent": "And now we've been able to get you City to do pretty well here.",
                    "label": 0
                },
                {
                    "sent": "So then we said about dissecting the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we basically took it apart and started looking into which is a components of UCT actually contributed to its success here.",
                    "label": 0
                },
                {
                    "sent": "And we examine 3K3 key tradeoffs.",
                    "label": 1
                },
                {
                    "sent": "So first one has to do with.",
                    "label": 0
                },
                {
                    "sent": "The topology of the tree that you see builds.",
                    "label": 0
                },
                {
                    "sent": "So whether you build a more complete tree or whether you build a sparsity.",
                    "label": 0
                },
                {
                    "sent": "The second one has to do with whether you want to perform more sampling or you want to create bigger trees.",
                    "label": 0
                },
                {
                    "sent": "So recall that I said you perform a single player data leaf node.",
                    "label": 0
                },
                {
                    "sent": "When you reach it, but there's really nothing stopping you from doing more playouts except you take a computational hit, so processing each leaf node will not take longer.",
                    "label": 0
                },
                {
                    "sent": "Trade off there and finally I mentioned that when you propagate the reward signal up, it's usually averaged into your previous utility estimates.",
                    "label": 0
                },
                {
                    "sent": "But this there's alternatives to that as well and we look into how using many maxing backups helps.",
                    "label": 0
                },
                {
                    "sent": "And once we examined all this, we actually created a you CD player which was capable of beating.",
                    "label": 0
                },
                {
                    "sent": "Minimax agents in this domain, but again, the quest is not to create a winning mancala player using UCT.",
                    "label": 1
                },
                {
                    "sent": "The questions to understand Whyyou City works and what we discovered was that essentially in.",
                    "label": 0
                },
                {
                    "sent": "In games, it appears that there can be phases of a game that are more conducive to one sort of search algorithm over another.",
                    "label": 0
                },
                {
                    "sent": "So the traditional approach has been to just take an algorithm and then use it for all phases of the game.",
                    "label": 0
                },
                {
                    "sent": "But our work suggests that it might be more advantages to use different search strategies based on the local structure of the search space, so that's a quick preview of the results so.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've been to the details, so this first tradeoff I mentioned has to do with the topology of the search space or what kind of trees do we build and what works better.",
                    "label": 0
                },
                {
                    "sent": "And recall that I mentioned the UCB one formula has this for man in particular.",
                    "label": 0
                },
                {
                    "sent": "There's this constant C which you can tune to sort of set whether you want to do more exploration or exploitation.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in this experiment, what we essentially do, is we ready that parameter?",
                    "label": 0
                },
                {
                    "sent": "So that's what the X axis is, and on the Y axis, what we're plotting is how does a UCT agent with that setting of that parameter perform against a standardized mini Max agent?",
                    "label": 0
                },
                {
                    "sent": "So when this value is really small.",
                    "label": 0
                },
                {
                    "sent": "Well, that's almost an invisible tree.",
                    "label": 0
                },
                {
                    "sent": "But I guess what you really get is this very sparse tree.",
                    "label": 0
                },
                {
                    "sent": "So it goes very deep into some parts of the search space and stays very shallow otherwise.",
                    "label": 0
                },
                {
                    "sent": "And when it's sort of too large, then you City does a lot of exploration and what you get is almost something that's more mini Max like, so it's a much more complete tree.",
                    "label": 0
                },
                {
                    "sent": "And if you look at this sort of the shape of the curve, what's striking is that there's this clear peak, so there's a point where you sort of hit the sweet spot.",
                    "label": 0
                },
                {
                    "sent": "An UCT builds a tree that somewhere in between these two extremes.",
                    "label": 0
                },
                {
                    "sent": "And why this is kind of interesting is because I guess when chess research was in full swing in the 70s and 80s, conventional wisdom had it.",
                    "label": 0
                },
                {
                    "sent": "So they discovered that.",
                    "label": 0
                },
                {
                    "sent": "If you could do complete search, then it was really hard to be so people experimented with trying to use heuristics to sort of cleverly prune out really bad moves, but that didn't do so well.",
                    "label": 0
                },
                {
                    "sent": "As you know, just using the time to just do a complete do a complete search and what this suggests is that we should perhaps be re examining that experience, and in particular the fact that we able to use trees like this to beat Minimax suggests that there may be domains out there where.",
                    "label": 0
                },
                {
                    "sent": "Doing a selective search can actually be better than doing sort of a complete search like minimax.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A couple of the other tradeoffs I mentioned, which I won't talk about too much in detail.",
                    "label": 1
                },
                {
                    "sent": "I'd encourage you to read the paper, but I'll mention the results quickly.",
                    "label": 0
                },
                {
                    "sent": "And the first one had to do with this sampling versus tree size issue, and the tradeoff here is, well, I can either build a big tree but perform just few players at the leaf nodes, or I can build a small tree, but do a lot of players at the leaf nodes and what we discovered that.",
                    "label": 0
                },
                {
                    "sent": "Well, with a large tree and fewer play out, you actually get better performance.",
                    "label": 0
                },
                {
                    "sent": "So in work we presented at UI, so this dovetails nicely with results we presented at UI, which basically said that random playouts surprisingly have information in them.",
                    "label": 0
                },
                {
                    "sent": "It's just not very good quality, so it's kind of understandable that doing more random players doesn't buy you much more.",
                    "label": 0
                },
                {
                    "sent": "And the other tradeoff I mentioned was this tradeoff between doing averaging and doing min maxing.",
                    "label": 0
                },
                {
                    "sent": "So how should you back the information up and combine it with your previous estimates?",
                    "label": 0
                },
                {
                    "sent": "So the thing about this domain is so the problem with goal was that we didn't really have good heuristics, so we had no idea how to do how to construct one for go.",
                    "label": 0
                },
                {
                    "sent": "Soap layouts worked out really well there, but in this domain we actually have a good heuristic, so we decided to plug that in instead.",
                    "label": 1
                },
                {
                    "sent": "So we totally do away with the play outs and then just use heuristics in place of 'em and what we discovered is that when you have such high quality heuristics, doing a mini maxing backup actually works better.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So based on these insights, we constructed a new variant of UCT UCT that we call that we term UCT Max H. An in games against a minimax agent, we're able to defeat Minimax players, searching up to depth 12 after normalizing for things like the amount of search effort.",
                    "label": 1
                },
                {
                    "sent": "Did the two algorithms perform?",
                    "label": 0
                },
                {
                    "sent": "And now the question is, well, great, so we're beating Minimax, but what does that tell us about UCT?",
                    "label": 0
                },
                {
                    "sent": "So how is it?",
                    "label": 0
                },
                {
                    "sent": "From where does it driving this advantage?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for that I need to give you a little bit of background about this notion of trap states.",
                    "label": 0
                },
                {
                    "sent": "So this is based on a talk I gave it.",
                    "label": 0
                },
                {
                    "sent": "I caps last year, but informally a trap state is basically a position where you can make a stupid move and then lose very quickly, right?",
                    "label": 1
                },
                {
                    "sent": "So in chess and go differ in this aspect.",
                    "label": 0
                },
                {
                    "sent": "So in chess you have these trap states at every level of the game.",
                    "label": 1
                },
                {
                    "sent": "You can potentially lose a game after four moves if you're really bad at chess like I am, Whereas in go these trap states don't really appear.",
                    "label": 0
                },
                {
                    "sent": "Until the latter stages of the game, so there's this vast beginning phase where there are really no trap sticks.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we showed that UCT has real problems.",
                    "label": 0
                },
                {
                    "sent": "Insert spaces where there are strapped states.",
                    "label": 0
                },
                {
                    "sent": "In particular, it can sort of get confused between what constitutes a good move and what constitutes a bad move.",
                    "label": 0
                },
                {
                    "sent": "So if we are interested in showing so.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Where does mancala fit in here?",
                    "label": 0
                },
                {
                    "sent": "Well, it actually is somewhere in between, so similar to go in the sense that there's this big area at the beginning of the game where there are no trap States and then they show up later.",
                    "label": 0
                },
                {
                    "sent": "But it's inherently a less complex game then go, so it's a smaller state space.",
                    "label": 0
                },
                {
                    "sent": "So this blue region is much smaller than the comparable region in go.",
                    "label": 0
                },
                {
                    "sent": "And since we're interested in seeing if you see, Twitter does better in the absence of traps, we can set up the following experiment.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Essentially, in this Blue Zone.",
                    "label": 0
                },
                {
                    "sent": "Will play RUCT variant against Minimax, and then once we detect that we've entered this zone with all the traps in it, we're going to switch over and we're just going to complete the game with two identical players.",
                    "label": 1
                },
                {
                    "sent": "So essentially any difference in the standard of play will be due to the different decisions that were made up there.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's exactly what we do.",
                    "label": 0
                },
                {
                    "sent": "So we vary the search depth.",
                    "label": 1
                },
                {
                    "sent": "Of the minimax opponent, that UCD Max plays against, and we measure the win rate of UCT Max agent against the Minimax player.",
                    "label": 0
                },
                {
                    "sent": "And there are two curves here.",
                    "label": 0
                },
                {
                    "sent": "Believe it or not.",
                    "label": 0
                },
                {
                    "sent": "So this one, this ghostly curve that's marked by the points and the darker ones.",
                    "label": 0
                },
                {
                    "sent": "So the darker one is.",
                    "label": 0
                },
                {
                    "sent": "The curve for the partial game setting that I just described.",
                    "label": 0
                },
                {
                    "sent": "So where you play UCT just in the higher reaches of the tree and then switch over.",
                    "label": 0
                },
                {
                    "sent": "And the second curve is for the setting where you just play UCT against minimax all the way through.",
                    "label": 0
                },
                {
                    "sent": "So there's no switchover phase.",
                    "label": 0
                },
                {
                    "sent": "And what we will observe is that this clear gap in performance.",
                    "label": 0
                },
                {
                    "sent": "So the curve actually shifts up when you force UCT to just make decisions in the top reaches of the game.",
                    "label": 0
                },
                {
                    "sent": "So in parts of the game where there are no traps.",
                    "label": 1
                },
                {
                    "sent": "So if you restrict you cities decision-making to just that part of the game, it seems to do better.",
                    "label": 0
                },
                {
                    "sent": "So what this suggests is that even within the same game, that could be phases of the game, that sort of favor one sort of search strategy over another.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, just a quick summary of our results.",
                    "label": 0
                },
                {
                    "sent": "So we studied these trail tradeoffs in UCT and we discovered that this exploration, exploitation, balancing, is actually valuable in U city and it gives us something.",
                    "label": 1
                },
                {
                    "sent": "And that many maxing backups actually work better with heuristics than averaging backups, but I guess the most interesting result was the fact that you can create hybrid search algorithms, which can basically try to probe the sort of the local search space that they're in.",
                    "label": 0
                },
                {
                    "sent": "And then decide to choose a search strategy based on that.",
                    "label": 0
                },
                {
                    "sent": "And it seems that doing that sort of using those hybrid search strategies can often be more effective than using one or the other.",
                    "label": 1
                },
                {
                    "sent": "The whole way through.",
                    "label": 0
                },
                {
                    "sent": "And as a follow up to this, our current work focuses on sort of extending these sort of investigations into synthetic domains.",
                    "label": 0
                },
                {
                    "sent": "I gave a talk about this at the MTS Workshop here at Icaps, and I'd encourage you to read the paper that paper as well as sort of a follow up to this work.",
                    "label": 0
                },
                {
                    "sent": "But yeah, thank you for your time.",
                    "label": 0
                },
                {
                    "sent": "So how do you detect when you've gone from the blue states into the red states to switch over?",
                    "label": 0
                },
                {
                    "sent": "So essentially we run a very quick minimax search, so before its usage is turned to win, we run a deeper minimax search to sort of count of the number of terminal nodes that were encountered within that.",
                    "label": 0
                },
                {
                    "sent": "Search horizon and if we detect that some threshold has been exceeded then we just say where we're out of the Blue Zone.",
                    "label": 0
                },
                {
                    "sent": "Back in Apple head.",
                    "label": 0
                },
                {
                    "sent": "Yes, we ignore were mixed in the look ahead.",
                    "label": 0
                },
                {
                    "sent": "That's simply just to see what's what region of the search space within.",
                    "label": 0
                },
                {
                    "sent": "Melon.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so your last graph.",
                    "label": 0
                },
                {
                    "sent": "I think I might have not got the full point, so the so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you are varying the mini Max strength.",
                    "label": 0
                },
                {
                    "sent": "Yep, and as you see T also expanding the same number of nodes.",
                    "label": 0
                },
                {
                    "sent": "Yes, so that's always normalized for, so you see T always only gets to expand at most a tree as big as what many Max is sort of indicating that.",
                    "label": 0
                },
                {
                    "sent": "Mini Max is really going to overtake it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we do reach a point where it's hard.",
                    "label": 0
                },
                {
                    "sent": "It gets harder to beat Minimax, and that's because I mean the deeper you look ahead.",
                    "label": 0
                },
                {
                    "sent": "The issue is that wraps start appearing sooner, so especially in these partial game settings.",
                    "label": 0
                },
                {
                    "sent": "So if you're playing against, say, a mini Max depth six player, the Blue Zone may last as many as 16 or 18 plies, but if you're playing against, say, many maxdepth 14 or death 16 player, that phase may only last five or 6 plus.",
                    "label": 0
                },
                {
                    "sent": "So really you City has it's not really enough time for one player or the other to build up enough of a positional advantage.",
                    "label": 0
                },
                {
                    "sent": "So that's one issue.",
                    "label": 0
                },
                {
                    "sent": "And of course, if you are looking deeper and deeper ahead.",
                    "label": 0
                },
                {
                    "sent": "Many Macs handles these terminal positions optimally, so it's really hard to beat it in that sense, so that's that's the other issue.",
                    "label": 0
                },
                {
                    "sent": "It does it follow from what you are saying that you should eat does not make much sense when the evolution function is good, that is that in principle cannot beat minimax.",
                    "label": 0
                },
                {
                    "sent": "OK, you mentioned that one type of minimax backups are better than average, but what about in general?",
                    "label": 0
                },
                {
                    "sent": "So when you have good evaluation functions, probably use it, you will make less sense that more.",
                    "label": 0
                },
                {
                    "sent": "Systematic methods.",
                    "label": 0
                },
                {
                    "sent": "I don't think there's much work out there on using very good heuristics.",
                    "label": 0
                },
                {
                    "sent": "I mean, I've almost always seen UCT combined with Playouts.",
                    "label": 0
                },
                {
                    "sent": "There's not much work out there on combining it with touristics.",
                    "label": 0
                },
                {
                    "sent": "If there if it is combined with heuristics.",
                    "label": 1
                },
                {
                    "sent": "This usually also always a player component, so.",
                    "label": 0
                },
                {
                    "sent": "Does that answer the question or I?",
                    "label": 0
                },
                {
                    "sent": "OK. OK.",
                    "label": 0
                },
                {
                    "sent": "So it seems that you cities starting to perform poorly when the cost of making mistake becomes very, very high.",
                    "label": 0
                },
                {
                    "sent": "So if you make a mistake, you know you essentially lose right?",
                    "label": 0
                },
                {
                    "sent": "So in this case, minimax essentially natural doing better because it's exploring all the options, right?",
                    "label": 0
                },
                {
                    "sent": "So I was wondering, have you tried?",
                    "label": 0
                },
                {
                    "sent": "Once you detect that you're in this?",
                    "label": 0
                },
                {
                    "sent": "Stage when mistakes become costly, but if you turn up the exploration parameter of you citymax really really high so that it's forced to explore all the options.",
                    "label": 0
                },
                {
                    "sent": "Have you tried this?",
                    "label": 0
                },
                {
                    "sent": "We haven't tried it, but my intuition is that that one works so well because many Max agent also performs what's known as Alpha beta pruning, and that essentially is a huge speedup over just vanilla minimax.",
                    "label": 0
                },
                {
                    "sent": "An UCT doesn't do that, so it still won't be building as deep a tree's minimax would.",
                    "label": 0
                },
                {
                    "sent": "Actually I will use this opportunity and I will follow up on this question so when you look at UCT on the original paper that was written for MVP's.",
                    "label": 0
                },
                {
                    "sent": "One thing that drives me crazy is that UCB is UCB component is the wrong thing to do.",
                    "label": 0
                },
                {
                    "sent": "Why so?",
                    "label": 0
                },
                {
                    "sent": "Because UCB was designed for regret minimization for clinical test kind of settings, while in MDP is definitely doing games we don't pay while we examine our simulation, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so going back to two.",
                    "label": 0
                },
                {
                    "sent": "Do you kind of conclusions at the end?",
                    "label": 0
                },
                {
                    "sent": "It will be interesting to see what will happen in the blue area if you replace UCT with just with the UCB component.",
                    "label": 0
                },
                {
                    "sent": "Who is just uniform sampling?",
                    "label": 0
                },
                {
                    "sent": "Because in theory it gives so the trick there is that it gives much better worst case guarantees for simple regret, not for regret minimization.",
                    "label": 0
                },
                {
                    "sent": "But for simple regret minimization, which is like try whatever you want then at the.",
                    "label": 0
                },
                {
                    "sent": "At the end the connection, right?",
                    "label": 0
                },
                {
                    "sent": "But it comes with the, but it pays off starting with the kind of large number of samples right?",
                    "label": 0
                },
                {
                    "sent": "Exactly, so it's kind of interesting thing to yeah, and to sort of add to that I guess the other thing we've looked at is sort of budgeted banded algorithms, so which are somewhat along the same line.",
                    "label": 0
                },
                {
                    "sent": "So I guess what his point was, is that.",
                    "label": 0
                },
                {
                    "sent": "I guess you see doesn't take into the fact that what you do during the search phase doesn't really play into the decision you make at the end, and also in particular, it doesn't take into the fact that you have a limited budget.",
                    "label": 0
                },
                {
                    "sent": "So if I had an infinite budget, I might choose, which is what.",
                    "label": 0
                },
                {
                    "sent": "Essentially, UCT assumes you might choose to build a different tree as compared to what you would build if you knew that you only get 10,000 iterations or whatever, and we did look into that at some point we tried budgeted bandit algorithms, and we couldn't really see any noticeable difference in the performance.",
                    "label": 0
                },
                {
                    "sent": "Not in that particular setup though, but that would be something to try.",
                    "label": 0
                },
                {
                    "sent": "OK, so thanks again.",
                    "label": 0
                }
            ]
        }
    }
}