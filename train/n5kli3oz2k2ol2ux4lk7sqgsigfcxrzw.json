{
    "id": "n5kli3oz2k2ol2ux4lk7sqgsigfcxrzw",
    "title": "Fast and Accurate k-means For Large Datasets",
    "info": {
        "author": [
            "Michael Shindler, Oregon State University"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips2011_shindler_largedatasets/",
    "segmentation": [
        [
            "As mentioned, I'm Michael Chandler.",
            "I'm at Oregon State University.",
            "This is going to be a fast and accurate K means algorithm for large datasets.",
            "This is joint work with Alex Wong of UCLA and Adam Myerson now at Google."
        ],
        [
            "So just to make sure everyone is on the same page where we all say for K means clustering, you are given a set of points and some value K, and the goal is to partition them into K subsets.",
            "By designating some subset K of them as facilities which are sometimes known as centers or means, depending on which subset of the literature you're reading is microphone OK?",
            "This is something seems to be moving the slides without me.",
            "The way you evaluate two different partitions on K means to figure out which one is better is typically the sum squared of error, which is for each point find the nearest of the means.",
            "Take the square of the distance between them, add these up and that's how you can denote the accuracy of a K means cost."
        ],
        [
            "So hopefully everyone here is familiar, at least in general with the problem.",
            "In fact, there is a very standard solution to the problem that has become so popular it's sometimes simply known as the K means algorithm, and that is due to Stuart Lloyd.",
            "While there are some problems with the algorithm that have been fixed in various forms, the one I'm most concerned with is that of large datasets where because the algorithm has to pass through the data many times.",
            "If the data you have to process is significantly larger than the amount of RAM you have.",
            "Then no amount of clever tricks with swap space and so on can really give it a reasonable speed, this is.",
            "Is there a second control to this that I'm missing?"
        ],
        [
            "Any case?",
            "So the goal I want is to come up with a good kamien solution without having to do random access to the data without needing very much memory relative to the size of it.",
            "And of course without using very much time."
        ],
        [
            "And so I'm going to present an algorithm for what we call the streaming setting.",
            "First point comes in that has to be a center 'cause there are no other ones.",
            "Sorry, seemingly.",
            "Anyway, first point is going to be designated as a center and as future points come in, we're going to either designate them as a centre or assign them to a center.",
            "They become a new center with probability proportional to their distance from the nearest current center, so this one is pretty far away.",
            "That's going to become a center itself.",
            "Another point comes in that's pretty close to a center, so intuitively you would want to keep that as part of that cluster itself.",
            "What we can now do, because we're limited on memory, is that we can.",
            "I don't know what is happening with the presentation.",
            "With the great points, we don't need to remember them anymore.",
            "We can maintain very little memory relative to the size of the data set.",
            "The.",
            "By simply forgetting the ones that are assigned to existing centers and instead keeping with the center a count of how many have been assigned to it and perhaps a running total towards its center of mass.",
            "So one of the things we're going to allow to happen is the existence of more than K clusters as the algorithm is running.",
            "Of course, the output itself has to be K centers, but here you see have added one different one prior to earlier.",
            "And what's going to happen is eventually you're going to run out of points.",
            "You will have finished reading the file.",
            "And when you're done, we're going to have some value Kappa facilities we want to consolidate these down to K, But because we know that the points we have fit within main memory, we can now run a standard K means algorithm on what's left to consolidate it.",
            "So we're guaranteed that the output itself will in fact be exact."
        ],
        [
            "K points.",
            "So this began as an attempt to take an algorithm from this year soda conference and turn into a more practical result.",
            "And so while the algorithm asymptotically had a great memory requirement, there was a very large leading constant part of our early contribution was to add that as long as the maximum memory you allow is anything lower bounded by K log N, including K log in itself, the algorithm we have will converge and will produce a quality answer.",
            "We also improved the cost guarantee.",
            "The ratio of the K means quality solution found versus the potential optimum over all possible partitions and the real big punch line that I think is really the coolest part of it is if you optimize our algorithm for runtime, you can achieve little oh of N * K time and by point of contrast, if I had given you a K mean solution and you needed to compute the quality of it, that would take you Theta event times K time.",
            "And in fact, empirically it does take us less time to generate a solution than it does to determine the cost of this solution."
        ],
        [
            "So there are other relevant algorithms to compute K means on a data stream on a very large data set.",
            "One of them is was in Nips two years ago.",
            "In other words, in Allen X last year an I'm going to be showing a comparison of how these perform empirically."
        ],
        [
            "Our goal, since I am emphasizing low memory, is to compare to the other algorithms with an equal and small amount of memory.",
            "The metrics of course are going to be both the cost of the solution, Anna time necessary to compute the solution.",
            "All the examples from this are going to be from Ucis Census 1990 data set.",
            "It was chosen because it had been in use it as a previous comparison for many K means works an it's fairly large.",
            "It's almost 2.5 million points, which I guess depending on your scale may or may not be, but in 68 dimensions it."
        ],
        [
            "Shows many of the features, so the time necessary to compute the solution.",
            "Ours is about the fastest.",
            "It's not always the fastest, but it's going to compare to it and we will improve on that in just a moment."
        ],
        [
            "And for the cost of for the K means cost were with enough memory ours will end up as."
        ],
        [
            "About the best.",
            "But the big bottleneck in the runtime, one of the we had seen."
        ],
        [
            "In earlier that the hours was about the fastest."
        ],
        [
            "But not quite."
        ],
        [
            "We took 2A software engineering approach of using a code profiler and trying to determine what was the biggest part of the runtime and we found that over 60% in every case was this step you read the next point from a file and you have to determine what is the nearest of the centers to it.",
            "And if you have Kappa facilities, it takes order Kappa time, each of which has to be compared in all of the D dimensions.",
            "And we observe this may seem like a very necessary step in K means.",
            "You can't really imagine AK means algorithm that doesn't assign each point to the nearest cluster.",
            "But that's exactly what we did.",
            "Imagine and it's based on that idea as well as the fact that computing the nearest would be a lot easier if these were in one dimension.",
            "So.",
            "We produce at the beginning of the program a random vector, and we store all of our facilities by their inner product with said vector projection.",
            "If you prefer to think of it geometrically.",
            "When we read our next point from the file, we figure out where that belongs on this line.",
            "And log rhythmic time instead.",
            "Of course, we run the risk we don't actually find the nearest neighbor itself.",
            "But identifying the two approximate nearest neighbors on this scheme simply takes log rhythmic time in the number of centers."
        ],
        [
            "And then we can go and find out which of those is it actually closer to and treat this as a pseudo nearest neighbor.",
            "Pretend that is the nearest for purposes of determining.",
            "Should this be a new cluster or should this be assigned to an?"
        ],
        [
            "System cluster.",
            "And when we do this, we find it substantially faster and across dozens of dozens of experimental results in the paper, we find that we are consistently the fastest when we go by this approach, and in fact, to tie into earlier.",
            "Experimentally, we found that the time necessary to compute the solution was less than the time it took us to figure out what the cost was speak."
        ],
        [
            "Of the costs.",
            "The cost change was usually minor.",
            "You occasionally have spikes like the one you see here, but in most of the test cases we were very close to the K means cost that we would have achieved."
        ],
        [
            "Had we used exact nearest neighbor."
        ],
        [
            "The red bars is this algorithm with the approximate nearest neighbor.",
            "The blue is our is without an the others of previous work."
        ],
        [
            "So the point of this was to demonstrate a substantial speedup.",
            "AK means algorithm that works on very large datasets with a big speedup while still being able to make both guarantees theoretically and demonstrate empirically a good quality clustering.",
            "If you think that this code would be useful to you in your work, that code is available online.",
            "I encourage you to help yourself and let me know if it's useful."
        ],
        [
            "Do an.",
            "I have time for questions.",
            "Hi.",
            "One question regards the projection that you do to this random random vector.",
            "How you can be sure that you don't?",
            "Induce a false positive when you.",
            "When you find that a OK to nearest neighbor clusters when you map the point cause there is a risk that in the original space the original point will not be close to your two.",
            "Clusters on this line, so wouldn't it be better to evaluate also the clusters that you found compared to other algorithms that don't use this projection we did with the projection was only used in ours, in fact they I guess the.",
            "Projection has stopped.",
            "The other algorithms we use did not use approximate nearest neighbor projection, so we could induce false positives and that's we are a large bit of the spike, so we do and induce some MIS categorizations.",
            "We do induce the false positive sometimes to the best of my knowledge, none of the other K means algorithms for the large sets have any variance where they will use this.",
            "In fact, the random projection approach that you as you mentioned.",
            "Was initially meant to be almost a straw man approach so that we could compare a so that we could create a better one to compare it to, but at least we have a baseline and with the exception of the case you see here, we were surprised by how well it worked empirically that we decided not to empirically compare it to something we thought could be better.",
            "When you get surprisingly great results, I think he's got another question.",
            "So did you do random projection them several times to reduce the false error?",
            "This we did not.",
            "We agree with that.",
            "One should again the random projection that you see here was as I said, it designed to be a baseline of kind of a silly approximate nearest neighbor an when it seems to work very well, we decided to simply stick with that as it was as far as the approximation guarantee, we do show that you do need to do a lot more than that to guarantee such a result."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As mentioned, I'm Michael Chandler.",
                    "label": 0
                },
                {
                    "sent": "I'm at Oregon State University.",
                    "label": 0
                },
                {
                    "sent": "This is going to be a fast and accurate K means algorithm for large datasets.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with Alex Wong of UCLA and Adam Myerson now at Google.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to make sure everyone is on the same page where we all say for K means clustering, you are given a set of points and some value K, and the goal is to partition them into K subsets.",
                    "label": 0
                },
                {
                    "sent": "By designating some subset K of them as facilities which are sometimes known as centers or means, depending on which subset of the literature you're reading is microphone OK?",
                    "label": 1
                },
                {
                    "sent": "This is something seems to be moving the slides without me.",
                    "label": 0
                },
                {
                    "sent": "The way you evaluate two different partitions on K means to figure out which one is better is typically the sum squared of error, which is for each point find the nearest of the means.",
                    "label": 0
                },
                {
                    "sent": "Take the square of the distance between them, add these up and that's how you can denote the accuracy of a K means cost.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So hopefully everyone here is familiar, at least in general with the problem.",
                    "label": 0
                },
                {
                    "sent": "In fact, there is a very standard solution to the problem that has become so popular it's sometimes simply known as the K means algorithm, and that is due to Stuart Lloyd.",
                    "label": 0
                },
                {
                    "sent": "While there are some problems with the algorithm that have been fixed in various forms, the one I'm most concerned with is that of large datasets where because the algorithm has to pass through the data many times.",
                    "label": 0
                },
                {
                    "sent": "If the data you have to process is significantly larger than the amount of RAM you have.",
                    "label": 0
                },
                {
                    "sent": "Then no amount of clever tricks with swap space and so on can really give it a reasonable speed, this is.",
                    "label": 0
                },
                {
                    "sent": "Is there a second control to this that I'm missing?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Any case?",
                    "label": 0
                },
                {
                    "sent": "So the goal I want is to come up with a good kamien solution without having to do random access to the data without needing very much memory relative to the size of it.",
                    "label": 1
                },
                {
                    "sent": "And of course without using very much time.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so I'm going to present an algorithm for what we call the streaming setting.",
                    "label": 0
                },
                {
                    "sent": "First point comes in that has to be a center 'cause there are no other ones.",
                    "label": 0
                },
                {
                    "sent": "Sorry, seemingly.",
                    "label": 0
                },
                {
                    "sent": "Anyway, first point is going to be designated as a center and as future points come in, we're going to either designate them as a centre or assign them to a center.",
                    "label": 0
                },
                {
                    "sent": "They become a new center with probability proportional to their distance from the nearest current center, so this one is pretty far away.",
                    "label": 0
                },
                {
                    "sent": "That's going to become a center itself.",
                    "label": 0
                },
                {
                    "sent": "Another point comes in that's pretty close to a center, so intuitively you would want to keep that as part of that cluster itself.",
                    "label": 0
                },
                {
                    "sent": "What we can now do, because we're limited on memory, is that we can.",
                    "label": 0
                },
                {
                    "sent": "I don't know what is happening with the presentation.",
                    "label": 0
                },
                {
                    "sent": "With the great points, we don't need to remember them anymore.",
                    "label": 0
                },
                {
                    "sent": "We can maintain very little memory relative to the size of the data set.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "By simply forgetting the ones that are assigned to existing centers and instead keeping with the center a count of how many have been assigned to it and perhaps a running total towards its center of mass.",
                    "label": 0
                },
                {
                    "sent": "So one of the things we're going to allow to happen is the existence of more than K clusters as the algorithm is running.",
                    "label": 1
                },
                {
                    "sent": "Of course, the output itself has to be K centers, but here you see have added one different one prior to earlier.",
                    "label": 0
                },
                {
                    "sent": "And what's going to happen is eventually you're going to run out of points.",
                    "label": 0
                },
                {
                    "sent": "You will have finished reading the file.",
                    "label": 0
                },
                {
                    "sent": "And when you're done, we're going to have some value Kappa facilities we want to consolidate these down to K, But because we know that the points we have fit within main memory, we can now run a standard K means algorithm on what's left to consolidate it.",
                    "label": 0
                },
                {
                    "sent": "So we're guaranteed that the output itself will in fact be exact.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "K points.",
                    "label": 0
                },
                {
                    "sent": "So this began as an attempt to take an algorithm from this year soda conference and turn into a more practical result.",
                    "label": 0
                },
                {
                    "sent": "And so while the algorithm asymptotically had a great memory requirement, there was a very large leading constant part of our early contribution was to add that as long as the maximum memory you allow is anything lower bounded by K log N, including K log in itself, the algorithm we have will converge and will produce a quality answer.",
                    "label": 1
                },
                {
                    "sent": "We also improved the cost guarantee.",
                    "label": 0
                },
                {
                    "sent": "The ratio of the K means quality solution found versus the potential optimum over all possible partitions and the real big punch line that I think is really the coolest part of it is if you optimize our algorithm for runtime, you can achieve little oh of N * K time and by point of contrast, if I had given you a K mean solution and you needed to compute the quality of it, that would take you Theta event times K time.",
                    "label": 0
                },
                {
                    "sent": "And in fact, empirically it does take us less time to generate a solution than it does to determine the cost of this solution.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are other relevant algorithms to compute K means on a data stream on a very large data set.",
                    "label": 0
                },
                {
                    "sent": "One of them is was in Nips two years ago.",
                    "label": 0
                },
                {
                    "sent": "In other words, in Allen X last year an I'm going to be showing a comparison of how these perform empirically.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our goal, since I am emphasizing low memory, is to compare to the other algorithms with an equal and small amount of memory.",
                    "label": 0
                },
                {
                    "sent": "The metrics of course are going to be both the cost of the solution, Anna time necessary to compute the solution.",
                    "label": 0
                },
                {
                    "sent": "All the examples from this are going to be from Ucis Census 1990 data set.",
                    "label": 0
                },
                {
                    "sent": "It was chosen because it had been in use it as a previous comparison for many K means works an it's fairly large.",
                    "label": 0
                },
                {
                    "sent": "It's almost 2.5 million points, which I guess depending on your scale may or may not be, but in 68 dimensions it.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Shows many of the features, so the time necessary to compute the solution.",
                    "label": 1
                },
                {
                    "sent": "Ours is about the fastest.",
                    "label": 0
                },
                {
                    "sent": "It's not always the fastest, but it's going to compare to it and we will improve on that in just a moment.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the cost of for the K means cost were with enough memory ours will end up as.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About the best.",
                    "label": 0
                },
                {
                    "sent": "But the big bottleneck in the runtime, one of the we had seen.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In earlier that the hours was about the fastest.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But not quite.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We took 2A software engineering approach of using a code profiler and trying to determine what was the biggest part of the runtime and we found that over 60% in every case was this step you read the next point from a file and you have to determine what is the nearest of the centers to it.",
                    "label": 0
                },
                {
                    "sent": "And if you have Kappa facilities, it takes order Kappa time, each of which has to be compared in all of the D dimensions.",
                    "label": 0
                },
                {
                    "sent": "And we observe this may seem like a very necessary step in K means.",
                    "label": 0
                },
                {
                    "sent": "You can't really imagine AK means algorithm that doesn't assign each point to the nearest cluster.",
                    "label": 0
                },
                {
                    "sent": "But that's exactly what we did.",
                    "label": 0
                },
                {
                    "sent": "Imagine and it's based on that idea as well as the fact that computing the nearest would be a lot easier if these were in one dimension.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We produce at the beginning of the program a random vector, and we store all of our facilities by their inner product with said vector projection.",
                    "label": 0
                },
                {
                    "sent": "If you prefer to think of it geometrically.",
                    "label": 0
                },
                {
                    "sent": "When we read our next point from the file, we figure out where that belongs on this line.",
                    "label": 0
                },
                {
                    "sent": "And log rhythmic time instead.",
                    "label": 0
                },
                {
                    "sent": "Of course, we run the risk we don't actually find the nearest neighbor itself.",
                    "label": 0
                },
                {
                    "sent": "But identifying the two approximate nearest neighbors on this scheme simply takes log rhythmic time in the number of centers.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we can go and find out which of those is it actually closer to and treat this as a pseudo nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "Pretend that is the nearest for purposes of determining.",
                    "label": 0
                },
                {
                    "sent": "Should this be a new cluster or should this be assigned to an?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "System cluster.",
                    "label": 0
                },
                {
                    "sent": "And when we do this, we find it substantially faster and across dozens of dozens of experimental results in the paper, we find that we are consistently the fastest when we go by this approach, and in fact, to tie into earlier.",
                    "label": 0
                },
                {
                    "sent": "Experimentally, we found that the time necessary to compute the solution was less than the time it took us to figure out what the cost was speak.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of the costs.",
                    "label": 0
                },
                {
                    "sent": "The cost change was usually minor.",
                    "label": 1
                },
                {
                    "sent": "You occasionally have spikes like the one you see here, but in most of the test cases we were very close to the K means cost that we would have achieved.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Had we used exact nearest neighbor.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The red bars is this algorithm with the approximate nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "The blue is our is without an the others of previous work.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the point of this was to demonstrate a substantial speedup.",
                    "label": 1
                },
                {
                    "sent": "AK means algorithm that works on very large datasets with a big speedup while still being able to make both guarantees theoretically and demonstrate empirically a good quality clustering.",
                    "label": 1
                },
                {
                    "sent": "If you think that this code would be useful to you in your work, that code is available online.",
                    "label": 0
                },
                {
                    "sent": "I encourage you to help yourself and let me know if it's useful.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do an.",
                    "label": 0
                },
                {
                    "sent": "I have time for questions.",
                    "label": 0
                },
                {
                    "sent": "Hi.",
                    "label": 0
                },
                {
                    "sent": "One question regards the projection that you do to this random random vector.",
                    "label": 0
                },
                {
                    "sent": "How you can be sure that you don't?",
                    "label": 0
                },
                {
                    "sent": "Induce a false positive when you.",
                    "label": 0
                },
                {
                    "sent": "When you find that a OK to nearest neighbor clusters when you map the point cause there is a risk that in the original space the original point will not be close to your two.",
                    "label": 0
                },
                {
                    "sent": "Clusters on this line, so wouldn't it be better to evaluate also the clusters that you found compared to other algorithms that don't use this projection we did with the projection was only used in ours, in fact they I guess the.",
                    "label": 0
                },
                {
                    "sent": "Projection has stopped.",
                    "label": 0
                },
                {
                    "sent": "The other algorithms we use did not use approximate nearest neighbor projection, so we could induce false positives and that's we are a large bit of the spike, so we do and induce some MIS categorizations.",
                    "label": 0
                },
                {
                    "sent": "We do induce the false positive sometimes to the best of my knowledge, none of the other K means algorithms for the large sets have any variance where they will use this.",
                    "label": 0
                },
                {
                    "sent": "In fact, the random projection approach that you as you mentioned.",
                    "label": 0
                },
                {
                    "sent": "Was initially meant to be almost a straw man approach so that we could compare a so that we could create a better one to compare it to, but at least we have a baseline and with the exception of the case you see here, we were surprised by how well it worked empirically that we decided not to empirically compare it to something we thought could be better.",
                    "label": 0
                },
                {
                    "sent": "When you get surprisingly great results, I think he's got another question.",
                    "label": 0
                },
                {
                    "sent": "So did you do random projection them several times to reduce the false error?",
                    "label": 0
                },
                {
                    "sent": "This we did not.",
                    "label": 0
                },
                {
                    "sent": "We agree with that.",
                    "label": 0
                },
                {
                    "sent": "One should again the random projection that you see here was as I said, it designed to be a baseline of kind of a silly approximate nearest neighbor an when it seems to work very well, we decided to simply stick with that as it was as far as the approximation guarantee, we do show that you do need to do a lot more than that to guarantee such a result.",
                    "label": 0
                }
            ]
        }
    }
}