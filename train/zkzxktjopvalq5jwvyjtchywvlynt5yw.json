{
    "id": "zkzxktjopvalq5jwvyjtchywvlynt5yw",
    "title": "Machine Learning on Distributions",
    "info": {
        "author": [
            "Barnab\u00e1s P\u00f3czos, Machine Learning Department, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_poczos_learning/",
    "segmentation": [
        [
            "Thanks a lot for the invitation."
        ],
        [
            "And this is a joint work with Jeffrey Young, Google, Larry, Artie and Alessandro from CMU."
        ],
        [
            "And today I'm going to talk about how to do machine learning on distribution.",
            "This is our goal, and please interrupt me anytime when something is not clear, OK?",
            "So you know most machine learning algorithms operate on vector real objects feature vectors, right?",
            "But the word is very complicated and often handcrafted.",
            "Vectorial features are just not good enough for us, and it's more natural to work with the complex inputs directly.",
            "For example, sets or distributions.",
            "So here are some examples.",
            "So this picture shows.",
            "Cluster of galaxies and our goal is to find interesting animals, Galaxy clusters, and how would you do that?",
            "So usually so one approach is that each Galaxy can be represented by a feature vector.",
            "So this Galaxy cluster can be represented by a set of feature vectors, right?",
            "And now we need to do and build a machine learning algorithms that can work on the set of these feature vectors.",
            "And generally it's just not a good approach to concatenate the individual set individual feature vectors to huge.",
            "Feature vector that just won't work.",
            "OK, and you know generally it can be useful when you deal with complex objects that you just break those complex objects into smaller parts and then you represent this complex object as a set of these smaller parts, right?",
            "So each Galaxy Galaxy cluster can be represented as set of feature vectors.",
            "Or if you want to do some computer vision.",
            "You want to solve some computer vision problem.",
            "Then each image can be represented as a set of image patches.",
            "OK, so you break your complex objects into smaller parts and then 3.",
            "This set elements table samples from some unknown distribution and now we need to do machine learning on these unknown distributions.",
            "OK, so this is."
        ],
        [
            "Or motivation?",
            "OK, so this is the outline of the presentation, so I'm going to talk about how to do machine learning on distribution and for this we need some topology on distributions, so we will discuss first how to estimate distances between distributions, how to estimate divergences between distributions.",
            "Then I will show you how to do machine learning on distributions.",
            "I will show some applications in computer vision, astronomy, turbulence, data processing and then I will show you some theoretical results as well.",
            "OK, and we will start with.",
            "Agents estimation."
        ],
        [
            "OK, so you know there are many divergences or distances that you can define on distributions.",
            "For example, L2 distance album distance, kullback, Leibler divergent, every versions maximum in discrepancy and many more.",
            "Today I'm just going to focus on one and this is how to estimate the rainy Alpha diversions between two distributions P&Q.",
            "So this is the definition of the rainy Alpha diversions for each Alpha, where P&Q are density functions.",
            "This is their distance.",
            "OK. How would you estimate?",
            "So you have some samples from distribution P, some samples from distribution Q then.",
            "What would be a natural day?",
            "To estimate this quantity.",
            "At first you one of these approaches that you estimate the density functions first, right the density for P&Q.",
            "You plug that here and you have an estimator.",
            "The problem the general density estimation is very difficult and I would like to show you a direct approach that can estimate this quantity here consistently without estimating P&Q.",
            "OK. Yep.",
            "Will you stay nearest neighborhood graphs?",
            "So that's a good point so.",
            "That can be used for entropy estimation.",
            "I don't know if anyone used, for example, minimum spanning trees to estimate divergences, but the idea will be very close to that one that we will use K nearest neighborhood graphs for estimating divergences, and you are perfectly right that that's a way to estimate consistently anthropy's build estimating density functions.",
            "The difference here will be that.",
            "If we use that approach, then Alpha should be, I think between zero and one, and here with this approach or for can be in larger range as well.",
            "OK, because I think those Euclidean property.",
            "You can convert it.",
            "Yeah, so it will be similar to that, but still there will be some differences OK?"
        ],
        [
            "So this is the estimator, how it works that we have an sample point points from distribution P. M sample points from distribution Q.",
            "And this is the rainy Alpha diversions, right?",
            "And I'm going to estimate this quantity here.",
            "OK. And.",
            "I fix a K save 2.",
            "And I calculate for each sample points from X from PSOXII calculated second nearest neighbor.",
            "This is first second nearest neighbor.",
            "And I didn't know that distance byro OK, and I do the same thing for the Red Point Red.",
            "Points as well.",
            "So for this point I calculate its second nearest neighbor among the red points.",
            "So this is first.",
            "This is its second nearest neighbor that will be new.",
            "OK, and I do that for every blue points.",
            "So now I have lots of flow and new gay nearest neighbor distances.",
            "And this is the estimator.",
            "And.",
            "I'm going to show you that this estimator is consistent estimator of this quantity and this is just for the rainy Alpha versions.",
            "But actually with this approach you can estimate this kind of functionality as well.",
            "OK, so to answer your question, I don't know how to generalize this approach to other Euclidean graphs.",
            "Like minimum spanning trees or other graphs as well.",
            "I just know this for K nearest neighbor distances.",
            "OK, and now you can."
        ],
        [
            "As the question, does it make sense this whole thing and what we can prove is that under some condition conditions this approach is asymptotically unbiased and L2 consistent.",
            "I think that's another difference that usually with this approach they show almost fully consistency.",
            "We will show Alto consistency here.",
            "Other questions.",
            "Yeah, so this is how we prove that the estimator is aseptically consistent.",
            "So this is the estimator here.",
            "I hope at some point it will go up good.",
            "So this is the estimator we need to prove that this true quantity.",
            "Will be the limit of the expected value of the estimator, so it's asymptotically unbiased.",
            "So I just replace this through quantity by its definition.",
            "And replace the estimated by its definition as well.",
            "And now I move this gamma."
        ],
        [
            "Guys so gamma is the number function to the other side.",
            "So this is what we need to prove.",
            "And you can observe that ear again this through a new, just Skinner gay nearest neighbor distance statistics.",
            "For each idea the same so I can just replace this.",
            "Expected average we won.",
            "And now what we can observe if I fix X1 conditional X, then the numerator and denominator will become conditioned on this quantity conditionally independent.",
            "So the right hand side here.",
            "This can be written in this form again through a new RK nearest neighbor distances.",
            "These the dimension of the points we want to estimate.",
            "The rainy Alpha versions, let's Alpha and the key thing that we can use here is that no matter what the original distribution P&Q are.",
            "The normal SK nearest neighbor distances or normalized?",
            "I mean this that Roque one again is the.",
            "Case nearest neighbor of XI in the sample points.",
            "I put that to the power D multiplied by North minus one and no matter what the original distribution was, this distribution will converge in distribution to Erlang or Gamma distribution.",
            "And if we have this thing here, then you can observe that this is in.",
            "The limit is just the Vine minus Alpha moments of this Erlang distribution and we have closed form for that.",
            "I fixed K. Fixed.",
            "And I fixed the dimension.",
            "Not a high dimension.",
            "Yeah, so if you be consistent for any high dimension, the convergence rate is another question.",
            "Because I can't.",
            "Yes so.",
            "No.",
            "I can talk about that later that I pretty sure that you can get convergence rates for this one as well, and we can talk about under what conditions you can get fast rate and slow rate.",
            "OK. Um?",
            "So anyhow, so this is what we know that this.",
            "Term here in distribution converges to gamma distribution.",
            "We know the moments of that and.",
            "This is what we need to prove.",
            "Right and you can see that this is indeed for this one.",
            "In the limit we have this quantity.",
            "This quantity.",
            "If I multiply this quantity by this quantity, antique expectation with respect to X one.",
            "I will get this very simple.",
            "All we need here is that if I have.",
            "A series of random variables cyan and it is converging in distribution to another random variable XI.",
            "Then their moments are converging as well."
        ],
        [
            "Very simple.",
            "The only problem is that general is not true.",
            "OK, so you can construct counterexamples when it's not true that a distribution converge into another distribution.",
            "And still the moments are not converging.",
            "And so to fix this thing here, we need some stronger property, which is called a synthetic uniform integrability.",
            "Defined by this quantity and we have to show that for our K nearest neighbors statistics, this asymptotically uniformly integrable property holds as well, and then you can prove that the moments are converging as well, but for us it took like 30 more pages to prove that.",
            "So even though ideas are simple, you might.",
            "Get some troubles and the other thing I want to emphasize here is that in the literature you can find several mistakes in this this problem, so this is what we need.",
            "The cyan converges in distribution side, then the expected values are converging as well.",
            "And we are at NIPS confessor conference so I can mention that you can find NIPS paper where they just say that this is a strong low of large number.",
            "So it's true.",
            "But this is not right and other mistakes you can find is that.",
            "We need that the CDF's converging for all you so this is a CDF of distribution.",
            "It's converging to this point wise.",
            "What we need is that the means are converging as well.",
            "And here is the proof.",
            "We have the Holy Bray theorem which says that for each bounded continuous function, this is true.",
            "So we're done.",
            "Right?",
            "The problem, of course, is that the identity function is not bounded.",
            "So it doesn't prove that back in it.",
            "Another mistake that you can find that you can also prove."
        ],
        [
            "That too.",
            "It's enough to prove that the limsup of this science expected value of this cyan random variables putting to the gamma.",
            "Times 1 plus epsilon power so you can find an epsilon bid which this is finite.",
            "It's enough to prove this.",
            "And the photo lemma says.",
            "That we have this limsup of these quantities is less than the expected value of the properties quantities.",
            "We already know that the limsup of these nearest neighbor distances it's Erlang or gamma distribution.",
            "For this we have finite moments.",
            "It's finite, we are done.",
            "What's the problem here?",
            "That this is not the first lemma.",
            "The first lemma is with limit another direction so.",
            "Sometimes things seem simple, but it's easy to make mistakes here.",
            "OK, so after."
        ],
        [
            "We know how to estimate divergences.",
            "I want to show you how to use that to do machine learning on distributions."
        ],
        [
            "OK, so many machine learning algorithms only require pairwise distances between the inputs or inner products between the inputs.",
            "So if we can estimate pairwise distances or inner products between distributions, then we can develop machine learning algorithms.",
            "On distributions and then we can solve classification regression, low dimensional embedding, anomaly detection on distributions.",
            "So the distribution classification is like this."
        ],
        [
            "We have some training distributions and here you can have.",
            "You can see some sample sets from these distributions, so I have six distributions and you can see some sample sets.",
            "And these distributions have some labels as well, so this is plus plus.",
            "Plus this is minus minus minus and the task is that I'm going to give you a new sample set.",
            "A special set and you have to tell me what would be the label for this.",
            "So how would you classify this?",
            "This would get minus, right?",
            "In this case.",
            "So the differences of this approach compared to.",
            "Standard machine learning algorithms on feature vectors is that the inputs here are not finite dimensional objects, but distributions.",
            "And actually, we don't even notice distributions, so it's like an error in variables models because we don't have the true density functions.",
            "We just have some finite sample sets from that."
        ],
        [
            "OK. And one of the algorithm that we can do is that we called support distribution machine is like this that we have T sample sets.",
            "Let's our training data.",
            "Each sample set has empty sample points.",
            "They are sampled from some PT distribution and each distribution has a class label YT minus one or plus one.",
            "And our task is that.",
            "Given.",
            "A new test point that has M sample points sample from some unknown distribution P. What would be its class labeled by?",
            "OK, and one approach is that.",
            "We have the distribution.",
            "In the training set PIPJ and we introduce some feature map.",
            "From these distributions and then calculate gram matrix kernel matrix between these distributions and then we can use support vector machine.",
            "When we have this game matrix to solve the classification problem.",
            "The only problem is that we don't know pipps the densities and because of that we don't know the elements of the kernel matrix either, right?",
            "But Luckily you know many kernels like linear kernel, polynomial, kernel, Gaussian kernel."
        ],
        [
            "As this simple form.",
            "Where we only need to estimate this kind of quantities and I already showed you an estimator.",
            "How to estimate this kind of quantities quantities?",
            "So if you want to use Gaussian kernel polynomial kernel, you just estimate this kind of quantities, then substitute these quantities to this kernel definitions.",
            "You have the kernel matrix.",
            "You can run your support vector machine algorithm and you can classify distributions.",
            "One problem with this approach that you estimate the matrix, the kernel matrix.",
            "It might not be positive some indefinite.",
            "For several reasons, but what you can do is that you just project this estimated you make it symmetric and you project this estimated gram matrix to the corner of PSD matrices, and then your algorithm runs.",
            "Is that clear?",
            "Another thing you can do is that here we use the Euclidean distance, but you can just replace this to say the range diversions as well, and then you can run the algorithm, Yep.",
            "So you have your training points and points.",
            "You calculate the gram matrix M by M. And to calculate you make it symmetric you, let's see you its transpose and divide by two and the estimation.",
            "So the projection of these gram matrix to the cone of positive semidefinite matrices.",
            "It's very easy actually.",
            "So 11 approaches you can prove that you just make a eigen decomposition and you replace the negative eigenvalues by zero and that will be the closest matrix to this in Frobenius norm.",
            "Oh it will be fine this way.",
            "Cozy and corner.",
            "So if we have estimation error bouncing.",
            "The Villa yes, but.",
            "So this would be, but I make some estimation error and in the estimated kernel matrix.",
            "There's a valid being invalid.",
            "So I don't know P&Q right?",
            "I estimate the distance only.",
            "And the estimated distance can be who knows what.",
            "Training exam.",
            "Yeah, so I have samples from a density Pi.",
            "Never know P. Right?",
            "I am not taking him so I don't want to take histograms.",
            "Right, so I want.",
            "So here I just need to estimate the.",
            "L2 distance between P&Q and, but I'm saying that you don't need to estimate the densities P&Q to calculate Delta distance between them.",
            "Estimated from Cape yes.",
            "Exactly.",
            "OK. And there are many other approaches to do these problems as well."
        ],
        [
            "So let's see some.",
            "This is the algorithm and let's see how good is this algorithm."
        ],
        [
            "So first I would like to show you some computer vision experiments.",
            "So what we do is that we break.",
            "We have an image we want to classify these images.",
            "We have a set of images and we want to classify images and what we do is that we break the images to smaller parts image patches.",
            "And we represent images as a set of these image patches, and these batches can be overlapping.",
            "Nonoverlapping Dispatch location can be gridpoints.",
            "Interesting points can be totally random.",
            "Doesn't matter the past sizes can be same different.",
            "They conform.",
            "Hierarchy doesn't matter.",
            "And what we do that in each image Patch?",
            "This sub to subtract sift.",
            "Feature vector, which is a 120 dimensional vector that was calculated just by this.",
            "Batch and we can compress that with PCA to say dimension D. And now each image is represented by a set of these D dimensional feature vectors.",
            "And we will consider each set as a sample from some unknown distribution.",
            "And now if you want to classify images, you can think of that as classifying those distributions that generates these feature vectors.",
            "So you can play with.",
            "Have whatever you like.",
            "So in the concrete examples I will show you some, but you have your freedom whatever you want, you can.",
            "You can still run the algorithm.",
            "And we acquired this image representation as a D dimensional dimensional sample sets representation of the image.",
            "OK, and here are."
        ],
        [
            "Examples, so we had 50 highway images.",
            "And we injected 5 animals images so they can see 4 rooms and this one is a three and we calculated these two dimensional sample set representations.",
            "So we divided this to image patches, calculated the features compressed that 2D and then you can see the sample each image was representative after that with a sample set.",
            "So this image was represented by this set this image by this set this image by this set and so on and we say.",
            "That so this anonymous represented by this set, we say that an image is a numerous if this calculated distribution is far from the rest.",
            "OK, and.",
            "Based on this, we can have an animal score and then we."
        ],
        [
            "Then order the images according to their normal score.",
            "So this got the highest CinemaScore.",
            "Discuss the second higher.",
            "This got the smallest animal score and these were the five injected animals images, so you can see that this algorithm can find over this five injected anomalous out of these from the first, then most analysis."
        ],
        [
            "Um?",
            "You might ask why I doing this nonparametric approach, why I just don't model this mixture of Gaussian?",
            "For example, you might think that, hey, what happens if I model this is a mixture of Gaussian and then calculate the distances between these mixture of Gaussians.",
            "But then this is what we would get.",
            "So this parametric approach.",
            "Would sort the animals.",
            "Images like this.",
            "So you can see that the true 5.",
            "We cannot find these five animals injected pictures among the first.",
            "Then images.",
            "So parametric approach in this problem."
        ],
        [
            "Doesn't work well.",
            "Another problem I would like to talk about is classifying noisy USPS datasets.",
            "So here you can see some images from the USPS data set, and.",
            "This is quite easy in that sense that even standard support vector machine based algorithms can achieve 97% accuracy on the classification.",
            "So we make this data set more challenging.",
            "In this way that we.",
            "Scaled images to have 160 one 160 sizes.",
            "And we consider these images as density functions and with sample.",
            "From this density functions, you can see the sample points.",
            "Here I sample 500 to the points.",
            "And I replaced the original data set by this.",
            "Kind of points.",
            "So here you can see the new data set.",
            "And now.",
            "We had 1000 training 1000 instances.",
            "And if I just run support vector machine on this data set where they were images, then the classification accuracy would drop from 97% to 82%.",
            "But if we.",
            "Use this super distribution machine ideas, then the accuracy would be 96%.",
            "So it just means that.",
            "If your data set is like this, then instead of using Euclidean distance, it might be better too.",
            "Use distribution based distances.",
            "OK."
        ],
        [
            "So I can demonstrate it further on this data set that I had 10 instances from 1234.",
            "Numbers, and I calculated the pairwise Euclidean distances between these numbers and with minimum with multidimensional scaling I embedded them into 2D.",
            "And here you can see the embedded points.",
            "There's no structure at all, but if I calculated the.",
            "The Euclidean distance between the distributions, not between the images between between the distributions.",
            "Then you can clearly 3 the CD structures and because of the classification is much easier."
        ],
        [
            "OK, here is another.",
            "Yep.",
            "So you could do that, but then you should play with the smoothness parameters and so on, right?",
            "It would include I was not testing how much you can get after that.",
            "So here is another data set.",
            "This is the coil.",
            "Data set, so we had these original images rotated froggies and I used some edge detection on these images.",
            "And.",
            "Then I run local linear embedding on these images and here you can see the embedded points to 2D.",
            "So you can see that this embedded points at this point corresponds to this flow.",
            "Give this one to this, this one to this.",
            "It doesn't keep any structure if you just use distances between these edge detected images.",
            "But again, if I consider them as samples from some 2D distributions and I usually then.",
            "Ali on distributions then you can see that it keeps the structure."
        ],
        [
            "OK, so this will just toy problems, but we run this distribution based machine learning on larger image.",
            "Datasets so this is a was a problem with the object classification problem.",
            "So we have eight categories.",
            "You can see these categories and we had some images from each category altogether we had.",
            "400 images and each represented each image by.",
            "576 Eighteen dimensional feature points and we run several algorithms on this classification problem.",
            "Using two fold cross validation and ten runs, you can see the results here.",
            "And the best.",
            "Algorithms so far was a bag of words approach that achieved 87% accuracy, but when we used our nonparametric rainy divisions based estimator than we got.",
            "Higher accuracy and the difference is significant.",
            "You might ask."
        ],
        [
            "How to choose Alpha in the Rainier for Divergent estimator you can use cross validation and here I'm using some results if you.",
            "Change Alpha between minus one and two.",
            "Then these are the accuracies so, but you can offset this that we got the highest accuracy for Alpha and Alpha is close to one that would be the kullback Leiber.",
            "Divergent, right?",
            "But still the highest accuracy was not exactly then off of US one."
        ],
        [
            "So we repeated these experiments with other datasets as well.",
            "So here you can see.",
            "Images from outdoor scene classifications.",
            "So we had eight different categories.",
            "The best published results that we found on this data set was this 91.57% our algorithm achieved.",
            "Better results and again this.",
            "Difference was significant.",
            "We but on the same algorithm."
        ],
        [
            "On Sport event classification as well, again we had eight categories and you can see that actually this might be quite challenging.",
            "This data set because the images are so different, right?",
            "So.",
            "And the best published result was 86.7%.",
            "Our algorithm achieved.",
            "A bit better on this data set as well.",
            "So what I would like to tell you with these experiments is that you know in computer vision.",
            "The goal of the researcher is just to achieve the best results with whatever algorithm you want, and they are not afraid of using sophisticated feature construction, complex algorithms and kind of heretics.",
            "And with these algorithms we run the same algorithm on these three datasets.",
            "We just use standard safety features and on the three datasets we got the three best performances.",
            "OK, there are other."
        ],
        [
            "Problems where this distribution based learning can be useful.",
            "One of them, as I mentioned, is defined.",
            "Interesting objects in Sky.",
            "For example, interesting Galaxy clusters.",
            "You know, astronomers take millions of pictures of the Sky each day.",
            "But they don't have time to look at those pictures.",
            "They want some automated methods that are able to find the most interesting objects.",
            "And.",
            "We use the Sloan Digital Sky survey.",
            "Which contains 505 Galaxy clusters.",
            "Usually each cluster has 10 to 50 galaxies altogether.",
            "We had more than 7500 galaxies in this data set, and again our goal was to find the most interesting Galaxy clusters and.",
            "We were even considering those cases where it can happen that each Galaxy in the Galaxy cluster looks totally normal, but the cluster itself looks interesting.",
            "And usually.",
            "Galaxies can be classified as blue and red galaxies based on their Spectra features, so the galaxies has higher Spectra.",
            "In this part the red Galaxies has higher Spectra in this part and usually Galaxy clusters has.",
            "Some red galaxies in the center.",
            "They are dead.",
            "Galaxies don't do anything interesting, and our own them.",
            "There are some blue galaxies.",
            "And we run our.",
            "Anomaly detector algorithm on this data set and this is what we found that the most animals, Galaxy clusters content mostly start forming blue galaxies and irregular galaxies.",
            "Irregular galaxies are interesting itself.",
            "Start from the blue.",
            "Galaxies are not that interesting, but that was very interesting that there were no red galaxies there.",
            "So blue galaxies are pretty normal, but usually there should be some red galaxies there as well, and because there were no red galaxies that it might mean that there were galaxies that they're just colliding there, and Stan formation happens, or other interesting things happen."
        ],
        [
            "There are other problems where this distribution based machine learning can help.",
            "One problem is understanding turbulence datasets.",
            "So understanding turbulence is very important if we want to develop more economical cars or safer faster airplanes or studying turbulence in the atmosphere of the sun is important to understand magnetic storms.",
            "Solar winds.",
            "You know ocean currents can affect the temperature throughout the world, and currently one of the biggest of Stachel in.",
            "Building Fusion power plants is to keep the plasma together.",
            "Because of that, understanding turbulences in plasma, it's very important."
        ],
        [
            "So our collaborators at Johns Hopkins University, they built huge fluid flow flow simulator and their goal was to find interesting events in these simulations.",
            "One tiny problem is that we don't know what interesting means.",
            "So you know science apparently reached the point where it's not only that we don't understand the real world, we don't even understand our simulations.",
            "Right so.",
            "We should know about interesting means and.",
            "As the first approach, we saw that.",
            "Maybe what this is can be interesting, so we try to classify.",
            "We try to detect vertices.",
            "That was our approach and we built a data set that contained 11 vertices and twenty negative examples.",
            "So here you can see one positive example vertex and we had twenty negative examples that they were nothing and we had to classify the distribution of.",
            "These velocities.",
            "And using our.",
            "Algorithm we got 97% accuracy on this data set.",
            "So basically we missed one out of this 11."
        ],
        [
            "What what this is?",
            "And here you can see the classification probabilities.",
            "So you can see that in this area indeed we got higher classification probability, and it looks like a vertex and here as well and low probabilities otherwise.",
            "And now you can ask how to really detect interesting.",
            "Events"
        ],
        [
            "This data set and what we did is that we don't have one class.",
            "Support distribution machine where we.",
            "You use that to find anomalous objects and here you can see the classification probabilities.",
            "It still has high probabilities, invertis is, but it got even higher probability.",
            "Then these vertices interact with each other so you can see this diamond shape that got the highest score.",
            "So it might mean that these kind of events were very rare in this data set.",
            "OK."
        ],
        [
            "And finally, I would like to show some theoretical results on this kind of algae."
        ],
        [
            "Things as well.",
            "So far we've got good experimental results, but you might ask what about Thierry?",
            "So?",
            "In practice you might ask these questions how many distributions we need in the training data set and how many samples from each distribution we need to get good classification accuracy.",
            "So I'm going to focus on distribution regression."
        ],
        [
            "Blam now.",
            "Just checking OK.",
            "So for regression one standard patient algorithm.",
            "Is there another way about Sun Gardner regression?",
            "This is the model YFX plus new X is a dimensional vector by is response variable means noise, expected value is zero and X and knew they are independent.",
            "And we have M training points.",
            "And we have a new test point, dimensional this point and we need to estimate this function FX, which is just the conditional expected value of Y.",
            "And this is the kinetic destiny you are familiar with, so you choose a can of function which has shaped like this.",
            "You use a bandwidth parameter.",
            "HM, it depends on how many points you have in the sample set and.",
            "To show that so under some you can prove that under some conditions this estimator can be consistent, and for that the bandwidth should go to zero, but not too fast.",
            "So you also want this quantity too.",
            "Oops, this should go to Infinity, right?",
            "And then the estimator is."
        ],
        [
            "Consistent.",
            "So our distribution regression is different in that sense that.",
            "Now we have.",
            "M distributions we have response variables again M But we don't know that density functions.",
            "We just have some IID sample points from these distributions.",
            "And.",
            "We assume that we have a meta distribution P that generates this distribution.",
            "IID in an IID V and from each distribution we have sample points.",
            "Our goal is to estimate the conditional expectation of the response variable conditioned on the distribution.",
            "So the difficulties here is that if you think about.",
            "The distributions they look like an infinite dimensional objects, right?",
            "And we still want to prove consistency.",
            "But if this dimension is.",
            "Infinite that we cannot get that.",
            "And the other problem is that we don't even know the true.",
            "Distributions we have this error in variables because from each distribution we just have some finite."
        ],
        [
            "Sample sets.",
            "So this is the estimate, so I showed you several other estimators.",
            "But to prove theory to prove consistency, I'm going to study this kind of estimator, which is another area but soon kernel regression are where.",
            "D is a distance between sample sets or empirical distributions.",
            "So.",
            "Traditionally, here you have the Euclidean distance between finite dimensional points.",
            "We replace that to the distribution between sample sets or empirical distributions.",
            "OK, we will use the add one distance here.",
            "And.",
            "The L1 distance between two distributions is divided by this quantity of API and PJR densities, so we need to estimate the densities as well.",
            "And for that we will use another kernel.",
            "So because of that, we say that this estimator is a kernel kernel estimator, so we have two channels, one here.",
            "For doing the regression and another kernel to measure the distances between distributions.",
            "Is that clear?",
            "So we have two bandwidths of this, Colonel in the estimator is by the dimension of the sample points is K. This subject of this P distributions here that will give us the distance between the empirical distributions and the subset of this distance here, and we can do the regression.",
            "Ask questions if.",
            "Yep.",
            "Yeah so.",
            "We want.",
            "We have M sample sets OK. You give me a new set and you ask what would be the response value in that set.",
            "It's a regression problem.",
            "This is the regression.",
            "What we are going to do.",
            "We just measure how far these sample sets are from each other.",
            "That's the substitute that here.",
            "And then it looks like the standard under a Batson kernel regression, right?",
            "H is the bandwidth.",
            "And we need to measure how far these distributions are from each other.",
            "And for that we will use this distance.",
            "So I have two sample sets.",
            "On these two sample sets, I estimate the density functions that P had IP had J and I calculate their L1 distance.",
            "For different datasets they must have the same dimension.",
            "So X is the same.",
            "Right?",
            "Yes.",
            "Oh, it doesn't matter because to prove that it was simpler for us.",
            "You can do other things too.",
            "Yeah, we will have two different bandwidths and we have to schedule them right.",
            "I will tell you how to do that.",
            "OK, other questions, so it's important to understand the basic problem.",
            "OK, and our goal Busta bound the expected value of the estimated.",
            "Response.",
            "Minus the true response.",
            "OK, and we want to show that under some conditions this risk is going to 0."
        ],
        [
            "OK, so we need a few assumptions to prove that.",
            "One assumption is that.",
            "F so remember our models that Y = F P plus noise this F. Is heard that continuous with some parameter beta and constant L?",
            "D Again is the distance between \u03c0 and PJ distributions.",
            "We will use this kind of kernels that we call a synthetic box Lipschitz Colonel, so this is the kernel function.",
            "We just need those kind of cars so this is zero.",
            "We just need those kind of kernels that are defined on positive or non negative numbers because we will put distances we will use them on distances these kernels and we assume that you can put a box in the kernel and you can put box outside of the scanner as well.",
            "So for example, Gaussian kernel.",
            "We don't know what to do with that.",
            "But there are many other kernels that are good for for us.",
            "We also assume that P&PI the distribution, the densities of these distributions are Lipschitz continuous.",
            "It could be generalized to hell there as well, but I'm not doing that now.",
            "We also assume that.",
            "This is a bounded regression problem, so both F and the observation wise they are bounded.",
            "And we will also assume that there's a from each distribution.",
            "We have some sample sets and the minimum value will be denoted by M and we assume this rate.",
            "Where M is the number of distributions and N is the number of these.",
            "This minimum value from the sample sets.",
            "OK. And.",
            "So there should be a relationship between the number of samples we have and the bandwidth.",
            "And for that we need.",
            "To assume something that again, air here is.",
            "The size of this box that I put in the kernel H is the bandwidth."
        ],
        [
            "So we need to bound the risk.",
            "Hello.",
            "Oops.",
            "So we will know one more definition and this is this will define so-called smallball probabilities.",
            "So we fixed the distribution P and we calculate and we fixed bandwidth H and we calculate what is the probability that another distribution is in this ball with radius H. What's the probability of that?",
            "And we will be interested in this.",
            "H is small.",
            "That's like the quality quantity smallbore probabilities.",
            "And this is the risk what we have.",
            "This is our main theorem.",
            "That the bandwidth.",
            "Of that kernel that Arthur asked should have this rate.",
            "It should go this path to zero and then we can prove that the risk is less than this quantity, which looks very ugly, but it might make some sense.",
            "So H again is the.",
            "Bandwidth parameter of our first kernel we have from each distribution at least N sample points.",
            "Beta is a parameter of the.",
            "Herder Continuity M is the number of distributions and everywhere we have the expected value of these small bowl probabilities.",
            "So this is a function of P. Right, and we take the expected.",
            "Station of that it looks very ugly.",
            "I understand that, but we can simplify these things a bit.",
            "So we will assume that.",
            "That method distribution that generates our distributions it has a doubling measure.",
            "Which is in the following sense that.",
            "We have this.",
            "We assume that with some D dimension D This property holds.",
            "That is, the probability of a bowl.",
            "Centered in P with radius H. If I multiply this H by epsilon.",
            "Then these balls behave like balls in the dimensional space and.",
            "If I divide these two balls small rooms, then we will have this property, so we assume this for now and we will discuss later how realistic this assumption is.",
            "And if you do that then it's very easy to prove that these ugly, expected, expected values that we have in our first theorem.",
            "Dave, you have some nice ones, so they will just look like this and then."
        ],
        [
            "Our main theorem has this very simple form.",
            "That risk.",
            "OK, I made some typos here, but so this.",
            "So yeah, so I should fix that.",
            "But so anyhow, so we got the quantity where that is depends only on H&M and M is the number of distribution H is the bandwidth of our first kernel.",
            "We already showed how to have the bandwidth for the other kernel and is the number of the samples from the distribution and we can show that this goes to 0 if you increase and."
        ],
        [
            "MI will show you that in the other slide I just want to show one more thing.",
            "Before that you might ask how realistic is to have this doubling dimension and I'm showing you some examples.",
            "Then we have finite dimension.",
            "So for example imagine that.",
            "You have a parametric family and the parameter parameter comes from a finite dimensional and that generates you.",
            "The distributions.",
            "Then in many under many conditions this.",
            "Family will have finite dimension, so imagine this example that I have Gaussian distributions where I just rotate this Gaussian distribution.",
            "If I embed them to.",
            "So now you can think so I'm just showing here how they look if I embed them to a 2D space.",
            "But now you can imagine that indeed if I measure the distances, how far these rotated Gaussians are from each other, because I'm just changing one parameter then.",
            "It might have a.",
            "Low dimension, so it lies in a low dimensional manifold.",
            "And the same thing happens.",
            "Here is if I have a 1 dimensional Gaussian distribution and I change two parameters, say the mean and the variance of Gaussian distributions, then they lie on a manifold on a 2 dimensional manifold and then this doubling dimension will be 2.",
            "So even though each distribution is a infinite dimensional object, if there's a process that generates this distribution if that eventually these distributions are on the manifold, then the doubling dimension is basically it's the interesting dimension of this manifold, and then you get polynomial rate.",
            "So I'm just showing the rates in these cases.",
            "So."
        ],
        [
            "Sick leave.",
            "Two cases and.",
            "One case, in one case the rate looks like this.",
            "In the other case.",
            "Today disappeared.",
            "It's here so another case that."
        ],
        [
            "Looks like this and.",
            "For the proofs, it's a bit messy, but what you need to do is you just decompose so this is the risk definition and you can decompose these two terms.",
            "Affect is the estimator and one case you assume.",
            "Here you assume that you know all those distributions, so there are no hats.",
            "Here in the other case, you don't.",
            "In the other case, you assume that you still have heads here and.",
            "Then it will be still ugly after this, but in 10 more pages you can prove it and we got similar results when instead of key.",
            "Instead of a kernel density estimator, we use scanner regression and we have.",
            "Consistency proof when the dimension is larger than two, so it seems that there is a curse of low dimensionality here that for small dimension we cannot prove consistency."
        ],
        [
            "In this case.",
            "Just skip."
        ],
        [
            "This part so I wanted to show.",
            "Why the small probabilities appear so naturally here?",
            "But I think I just skip this slide.",
            "Just show you one.",
            "The results of 1."
        ],
        [
            "Experiment here.",
            "We wanted to learn the skewness of beta distribution.",
            "So we had many beta distributions with different skewness is and in a supervised Lee way if used this our estimator then you can hear see the true values.",
            "And here.",
            "The estimated value is and.",
            "They also use this approach to learn the entropy of Gaussian distributions in a supervised way.",
            "So we had Gaussian distribution with different parameters and different entropies.",
            "And here you can see the.",
            "Through entropy curve, if you change parameters and here you can see the estimated values of the entropies, so it shows that you can get good results with this."
        ],
        [
            "Kind of regression method.",
            "So the take home message, but I showed you today is that in parametric way you can estimate rainy divergences between distributions without estimating densities.",
            "I show you after that how to use these divergences to.",
            "Do machine learning on distributions.",
            "I show you some experiments in computer vision, instrument turbulence, data processing and then I show you that some theoretical results as well, and.",
            "Thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks a lot for the invitation.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is a joint work with Jeffrey Young, Google, Larry, Artie and Alessandro from CMU.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And today I'm going to talk about how to do machine learning on distribution.",
                    "label": 0
                },
                {
                    "sent": "This is our goal, and please interrupt me anytime when something is not clear, OK?",
                    "label": 0
                },
                {
                    "sent": "So you know most machine learning algorithms operate on vector real objects feature vectors, right?",
                    "label": 1
                },
                {
                    "sent": "But the word is very complicated and often handcrafted.",
                    "label": 1
                },
                {
                    "sent": "Vectorial features are just not good enough for us, and it's more natural to work with the complex inputs directly.",
                    "label": 1
                },
                {
                    "sent": "For example, sets or distributions.",
                    "label": 0
                },
                {
                    "sent": "So here are some examples.",
                    "label": 0
                },
                {
                    "sent": "So this picture shows.",
                    "label": 0
                },
                {
                    "sent": "Cluster of galaxies and our goal is to find interesting animals, Galaxy clusters, and how would you do that?",
                    "label": 1
                },
                {
                    "sent": "So usually so one approach is that each Galaxy can be represented by a feature vector.",
                    "label": 1
                },
                {
                    "sent": "So this Galaxy cluster can be represented by a set of feature vectors, right?",
                    "label": 1
                },
                {
                    "sent": "And now we need to do and build a machine learning algorithms that can work on the set of these feature vectors.",
                    "label": 0
                },
                {
                    "sent": "And generally it's just not a good approach to concatenate the individual set individual feature vectors to huge.",
                    "label": 0
                },
                {
                    "sent": "Feature vector that just won't work.",
                    "label": 0
                },
                {
                    "sent": "OK, and you know generally it can be useful when you deal with complex objects that you just break those complex objects into smaller parts and then you represent this complex object as a set of these smaller parts, right?",
                    "label": 0
                },
                {
                    "sent": "So each Galaxy Galaxy cluster can be represented as set of feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Or if you want to do some computer vision.",
                    "label": 0
                },
                {
                    "sent": "You want to solve some computer vision problem.",
                    "label": 0
                },
                {
                    "sent": "Then each image can be represented as a set of image patches.",
                    "label": 0
                },
                {
                    "sent": "OK, so you break your complex objects into smaller parts and then 3.",
                    "label": 1
                },
                {
                    "sent": "This set elements table samples from some unknown distribution and now we need to do machine learning on these unknown distributions.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or motivation?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the outline of the presentation, so I'm going to talk about how to do machine learning on distribution and for this we need some topology on distributions, so we will discuss first how to estimate distances between distributions, how to estimate divergences between distributions.",
                    "label": 0
                },
                {
                    "sent": "Then I will show you how to do machine learning on distributions.",
                    "label": 1
                },
                {
                    "sent": "I will show some applications in computer vision, astronomy, turbulence, data processing and then I will show you some theoretical results as well.",
                    "label": 1
                },
                {
                    "sent": "OK, and we will start with.",
                    "label": 0
                },
                {
                    "sent": "Agents estimation.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so you know there are many divergences or distances that you can define on distributions.",
                    "label": 0
                },
                {
                    "sent": "For example, L2 distance album distance, kullback, Leibler divergent, every versions maximum in discrepancy and many more.",
                    "label": 1
                },
                {
                    "sent": "Today I'm just going to focus on one and this is how to estimate the rainy Alpha diversions between two distributions P&Q.",
                    "label": 0
                },
                {
                    "sent": "So this is the definition of the rainy Alpha diversions for each Alpha, where P&Q are density functions.",
                    "label": 0
                },
                {
                    "sent": "This is their distance.",
                    "label": 0
                },
                {
                    "sent": "OK. How would you estimate?",
                    "label": 0
                },
                {
                    "sent": "So you have some samples from distribution P, some samples from distribution Q then.",
                    "label": 0
                },
                {
                    "sent": "What would be a natural day?",
                    "label": 0
                },
                {
                    "sent": "To estimate this quantity.",
                    "label": 0
                },
                {
                    "sent": "At first you one of these approaches that you estimate the density functions first, right the density for P&Q.",
                    "label": 0
                },
                {
                    "sent": "You plug that here and you have an estimator.",
                    "label": 0
                },
                {
                    "sent": "The problem the general density estimation is very difficult and I would like to show you a direct approach that can estimate this quantity here consistently without estimating P&Q.",
                    "label": 0
                },
                {
                    "sent": "OK. Yep.",
                    "label": 0
                },
                {
                    "sent": "Will you stay nearest neighborhood graphs?",
                    "label": 0
                },
                {
                    "sent": "So that's a good point so.",
                    "label": 0
                },
                {
                    "sent": "That can be used for entropy estimation.",
                    "label": 0
                },
                {
                    "sent": "I don't know if anyone used, for example, minimum spanning trees to estimate divergences, but the idea will be very close to that one that we will use K nearest neighborhood graphs for estimating divergences, and you are perfectly right that that's a way to estimate consistently anthropy's build estimating density functions.",
                    "label": 0
                },
                {
                    "sent": "The difference here will be that.",
                    "label": 0
                },
                {
                    "sent": "If we use that approach, then Alpha should be, I think between zero and one, and here with this approach or for can be in larger range as well.",
                    "label": 0
                },
                {
                    "sent": "OK, because I think those Euclidean property.",
                    "label": 0
                },
                {
                    "sent": "You can convert it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it will be similar to that, but still there will be some differences OK?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the estimator, how it works that we have an sample point points from distribution P. M sample points from distribution Q.",
                    "label": 0
                },
                {
                    "sent": "And this is the rainy Alpha diversions, right?",
                    "label": 0
                },
                {
                    "sent": "And I'm going to estimate this quantity here.",
                    "label": 0
                },
                {
                    "sent": "OK. And.",
                    "label": 0
                },
                {
                    "sent": "I fix a K save 2.",
                    "label": 0
                },
                {
                    "sent": "And I calculate for each sample points from X from PSOXII calculated second nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "This is first second nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "And I didn't know that distance byro OK, and I do the same thing for the Red Point Red.",
                    "label": 0
                },
                {
                    "sent": "Points as well.",
                    "label": 0
                },
                {
                    "sent": "So for this point I calculate its second nearest neighbor among the red points.",
                    "label": 0
                },
                {
                    "sent": "So this is first.",
                    "label": 0
                },
                {
                    "sent": "This is its second nearest neighbor that will be new.",
                    "label": 0
                },
                {
                    "sent": "OK, and I do that for every blue points.",
                    "label": 0
                },
                {
                    "sent": "So now I have lots of flow and new gay nearest neighbor distances.",
                    "label": 0
                },
                {
                    "sent": "And this is the estimator.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you that this estimator is consistent estimator of this quantity and this is just for the rainy Alpha versions.",
                    "label": 0
                },
                {
                    "sent": "But actually with this approach you can estimate this kind of functionality as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so to answer your question, I don't know how to generalize this approach to other Euclidean graphs.",
                    "label": 0
                },
                {
                    "sent": "Like minimum spanning trees or other graphs as well.",
                    "label": 0
                },
                {
                    "sent": "I just know this for K nearest neighbor distances.",
                    "label": 0
                },
                {
                    "sent": "OK, and now you can.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As the question, does it make sense this whole thing and what we can prove is that under some condition conditions this approach is asymptotically unbiased and L2 consistent.",
                    "label": 1
                },
                {
                    "sent": "I think that's another difference that usually with this approach they show almost fully consistency.",
                    "label": 0
                },
                {
                    "sent": "We will show Alto consistency here.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is how we prove that the estimator is aseptically consistent.",
                    "label": 0
                },
                {
                    "sent": "So this is the estimator here.",
                    "label": 0
                },
                {
                    "sent": "I hope at some point it will go up good.",
                    "label": 0
                },
                {
                    "sent": "So this is the estimator we need to prove that this true quantity.",
                    "label": 0
                },
                {
                    "sent": "Will be the limit of the expected value of the estimator, so it's asymptotically unbiased.",
                    "label": 0
                },
                {
                    "sent": "So I just replace this through quantity by its definition.",
                    "label": 0
                },
                {
                    "sent": "And replace the estimated by its definition as well.",
                    "label": 0
                },
                {
                    "sent": "And now I move this gamma.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Guys so gamma is the number function to the other side.",
                    "label": 0
                },
                {
                    "sent": "So this is what we need to prove.",
                    "label": 0
                },
                {
                    "sent": "And you can observe that ear again this through a new, just Skinner gay nearest neighbor distance statistics.",
                    "label": 0
                },
                {
                    "sent": "For each idea the same so I can just replace this.",
                    "label": 0
                },
                {
                    "sent": "Expected average we won.",
                    "label": 0
                },
                {
                    "sent": "And now what we can observe if I fix X1 conditional X, then the numerator and denominator will become conditioned on this quantity conditionally independent.",
                    "label": 0
                },
                {
                    "sent": "So the right hand side here.",
                    "label": 0
                },
                {
                    "sent": "This can be written in this form again through a new RK nearest neighbor distances.",
                    "label": 0
                },
                {
                    "sent": "These the dimension of the points we want to estimate.",
                    "label": 0
                },
                {
                    "sent": "The rainy Alpha versions, let's Alpha and the key thing that we can use here is that no matter what the original distribution P&Q are.",
                    "label": 0
                },
                {
                    "sent": "The normal SK nearest neighbor distances or normalized?",
                    "label": 0
                },
                {
                    "sent": "I mean this that Roque one again is the.",
                    "label": 0
                },
                {
                    "sent": "Case nearest neighbor of XI in the sample points.",
                    "label": 0
                },
                {
                    "sent": "I put that to the power D multiplied by North minus one and no matter what the original distribution was, this distribution will converge in distribution to Erlang or Gamma distribution.",
                    "label": 0
                },
                {
                    "sent": "And if we have this thing here, then you can observe that this is in.",
                    "label": 0
                },
                {
                    "sent": "The limit is just the Vine minus Alpha moments of this Erlang distribution and we have closed form for that.",
                    "label": 0
                },
                {
                    "sent": "I fixed K. Fixed.",
                    "label": 0
                },
                {
                    "sent": "And I fixed the dimension.",
                    "label": 0
                },
                {
                    "sent": "Not a high dimension.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so if you be consistent for any high dimension, the convergence rate is another question.",
                    "label": 0
                },
                {
                    "sent": "Because I can't.",
                    "label": 0
                },
                {
                    "sent": "Yes so.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "I can talk about that later that I pretty sure that you can get convergence rates for this one as well, and we can talk about under what conditions you can get fast rate and slow rate.",
                    "label": 0
                },
                {
                    "sent": "OK. Um?",
                    "label": 0
                },
                {
                    "sent": "So anyhow, so this is what we know that this.",
                    "label": 0
                },
                {
                    "sent": "Term here in distribution converges to gamma distribution.",
                    "label": 0
                },
                {
                    "sent": "We know the moments of that and.",
                    "label": 0
                },
                {
                    "sent": "This is what we need to prove.",
                    "label": 1
                },
                {
                    "sent": "Right and you can see that this is indeed for this one.",
                    "label": 0
                },
                {
                    "sent": "In the limit we have this quantity.",
                    "label": 0
                },
                {
                    "sent": "This quantity.",
                    "label": 0
                },
                {
                    "sent": "If I multiply this quantity by this quantity, antique expectation with respect to X one.",
                    "label": 0
                },
                {
                    "sent": "I will get this very simple.",
                    "label": 1
                },
                {
                    "sent": "All we need here is that if I have.",
                    "label": 0
                },
                {
                    "sent": "A series of random variables cyan and it is converging in distribution to another random variable XI.",
                    "label": 0
                },
                {
                    "sent": "Then their moments are converging as well.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very simple.",
                    "label": 0
                },
                {
                    "sent": "The only problem is that general is not true.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can construct counterexamples when it's not true that a distribution converge into another distribution.",
                    "label": 0
                },
                {
                    "sent": "And still the moments are not converging.",
                    "label": 0
                },
                {
                    "sent": "And so to fix this thing here, we need some stronger property, which is called a synthetic uniform integrability.",
                    "label": 0
                },
                {
                    "sent": "Defined by this quantity and we have to show that for our K nearest neighbors statistics, this asymptotically uniformly integrable property holds as well, and then you can prove that the moments are converging as well, but for us it took like 30 more pages to prove that.",
                    "label": 0
                },
                {
                    "sent": "So even though ideas are simple, you might.",
                    "label": 0
                },
                {
                    "sent": "Get some troubles and the other thing I want to emphasize here is that in the literature you can find several mistakes in this this problem, so this is what we need.",
                    "label": 0
                },
                {
                    "sent": "The cyan converges in distribution side, then the expected values are converging as well.",
                    "label": 0
                },
                {
                    "sent": "And we are at NIPS confessor conference so I can mention that you can find NIPS paper where they just say that this is a strong low of large number.",
                    "label": 0
                },
                {
                    "sent": "So it's true.",
                    "label": 0
                },
                {
                    "sent": "But this is not right and other mistakes you can find is that.",
                    "label": 0
                },
                {
                    "sent": "We need that the CDF's converging for all you so this is a CDF of distribution.",
                    "label": 0
                },
                {
                    "sent": "It's converging to this point wise.",
                    "label": 0
                },
                {
                    "sent": "What we need is that the means are converging as well.",
                    "label": 0
                },
                {
                    "sent": "And here is the proof.",
                    "label": 0
                },
                {
                    "sent": "We have the Holy Bray theorem which says that for each bounded continuous function, this is true.",
                    "label": 0
                },
                {
                    "sent": "So we're done.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "The problem, of course, is that the identity function is not bounded.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't prove that back in it.",
                    "label": 0
                },
                {
                    "sent": "Another mistake that you can find that you can also prove.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That too.",
                    "label": 0
                },
                {
                    "sent": "It's enough to prove that the limsup of this science expected value of this cyan random variables putting to the gamma.",
                    "label": 0
                },
                {
                    "sent": "Times 1 plus epsilon power so you can find an epsilon bid which this is finite.",
                    "label": 0
                },
                {
                    "sent": "It's enough to prove this.",
                    "label": 0
                },
                {
                    "sent": "And the photo lemma says.",
                    "label": 0
                },
                {
                    "sent": "That we have this limsup of these quantities is less than the expected value of the properties quantities.",
                    "label": 0
                },
                {
                    "sent": "We already know that the limsup of these nearest neighbor distances it's Erlang or gamma distribution.",
                    "label": 0
                },
                {
                    "sent": "For this we have finite moments.",
                    "label": 0
                },
                {
                    "sent": "It's finite, we are done.",
                    "label": 0
                },
                {
                    "sent": "What's the problem here?",
                    "label": 0
                },
                {
                    "sent": "That this is not the first lemma.",
                    "label": 0
                },
                {
                    "sent": "The first lemma is with limit another direction so.",
                    "label": 0
                },
                {
                    "sent": "Sometimes things seem simple, but it's easy to make mistakes here.",
                    "label": 1
                },
                {
                    "sent": "OK, so after.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We know how to estimate divergences.",
                    "label": 0
                },
                {
                    "sent": "I want to show you how to use that to do machine learning on distributions.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so many machine learning algorithms only require pairwise distances between the inputs or inner products between the inputs.",
                    "label": 1
                },
                {
                    "sent": "So if we can estimate pairwise distances or inner products between distributions, then we can develop machine learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "On distributions and then we can solve classification regression, low dimensional embedding, anomaly detection on distributions.",
                    "label": 0
                },
                {
                    "sent": "So the distribution classification is like this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have some training distributions and here you can have.",
                    "label": 0
                },
                {
                    "sent": "You can see some sample sets from these distributions, so I have six distributions and you can see some sample sets.",
                    "label": 1
                },
                {
                    "sent": "And these distributions have some labels as well, so this is plus plus.",
                    "label": 0
                },
                {
                    "sent": "Plus this is minus minus minus and the task is that I'm going to give you a new sample set.",
                    "label": 0
                },
                {
                    "sent": "A special set and you have to tell me what would be the label for this.",
                    "label": 0
                },
                {
                    "sent": "So how would you classify this?",
                    "label": 0
                },
                {
                    "sent": "This would get minus, right?",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 1
                },
                {
                    "sent": "So the differences of this approach compared to.",
                    "label": 0
                },
                {
                    "sent": "Standard machine learning algorithms on feature vectors is that the inputs here are not finite dimensional objects, but distributions.",
                    "label": 1
                },
                {
                    "sent": "And actually, we don't even notice distributions, so it's like an error in variables models because we don't have the true density functions.",
                    "label": 0
                },
                {
                    "sent": "We just have some finite sample sets from that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. And one of the algorithm that we can do is that we called support distribution machine is like this that we have T sample sets.",
                    "label": 1
                },
                {
                    "sent": "Let's our training data.",
                    "label": 0
                },
                {
                    "sent": "Each sample set has empty sample points.",
                    "label": 0
                },
                {
                    "sent": "They are sampled from some PT distribution and each distribution has a class label YT minus one or plus one.",
                    "label": 0
                },
                {
                    "sent": "And our task is that.",
                    "label": 0
                },
                {
                    "sent": "Given.",
                    "label": 0
                },
                {
                    "sent": "A new test point that has M sample points sample from some unknown distribution P. What would be its class labeled by?",
                    "label": 0
                },
                {
                    "sent": "OK, and one approach is that.",
                    "label": 0
                },
                {
                    "sent": "We have the distribution.",
                    "label": 0
                },
                {
                    "sent": "In the training set PIPJ and we introduce some feature map.",
                    "label": 1
                },
                {
                    "sent": "From these distributions and then calculate gram matrix kernel matrix between these distributions and then we can use support vector machine.",
                    "label": 0
                },
                {
                    "sent": "When we have this game matrix to solve the classification problem.",
                    "label": 0
                },
                {
                    "sent": "The only problem is that we don't know pipps the densities and because of that we don't know the elements of the kernel matrix either, right?",
                    "label": 0
                },
                {
                    "sent": "But Luckily you know many kernels like linear kernel, polynomial, kernel, Gaussian kernel.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As this simple form.",
                    "label": 0
                },
                {
                    "sent": "Where we only need to estimate this kind of quantities and I already showed you an estimator.",
                    "label": 0
                },
                {
                    "sent": "How to estimate this kind of quantities quantities?",
                    "label": 0
                },
                {
                    "sent": "So if you want to use Gaussian kernel polynomial kernel, you just estimate this kind of quantities, then substitute these quantities to this kernel definitions.",
                    "label": 0
                },
                {
                    "sent": "You have the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "You can run your support vector machine algorithm and you can classify distributions.",
                    "label": 0
                },
                {
                    "sent": "One problem with this approach that you estimate the matrix, the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "It might not be positive some indefinite.",
                    "label": 0
                },
                {
                    "sent": "For several reasons, but what you can do is that you just project this estimated you make it symmetric and you project this estimated gram matrix to the corner of PSD matrices, and then your algorithm runs.",
                    "label": 1
                },
                {
                    "sent": "Is that clear?",
                    "label": 0
                },
                {
                    "sent": "Another thing you can do is that here we use the Euclidean distance, but you can just replace this to say the range diversions as well, and then you can run the algorithm, Yep.",
                    "label": 0
                },
                {
                    "sent": "So you have your training points and points.",
                    "label": 0
                },
                {
                    "sent": "You calculate the gram matrix M by M. And to calculate you make it symmetric you, let's see you its transpose and divide by two and the estimation.",
                    "label": 0
                },
                {
                    "sent": "So the projection of these gram matrix to the cone of positive semidefinite matrices.",
                    "label": 0
                },
                {
                    "sent": "It's very easy actually.",
                    "label": 0
                },
                {
                    "sent": "So 11 approaches you can prove that you just make a eigen decomposition and you replace the negative eigenvalues by zero and that will be the closest matrix to this in Frobenius norm.",
                    "label": 0
                },
                {
                    "sent": "Oh it will be fine this way.",
                    "label": 0
                },
                {
                    "sent": "Cozy and corner.",
                    "label": 0
                },
                {
                    "sent": "So if we have estimation error bouncing.",
                    "label": 0
                },
                {
                    "sent": "The Villa yes, but.",
                    "label": 0
                },
                {
                    "sent": "So this would be, but I make some estimation error and in the estimated kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "There's a valid being invalid.",
                    "label": 0
                },
                {
                    "sent": "So I don't know P&Q right?",
                    "label": 0
                },
                {
                    "sent": "I estimate the distance only.",
                    "label": 0
                },
                {
                    "sent": "And the estimated distance can be who knows what.",
                    "label": 0
                },
                {
                    "sent": "Training exam.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I have samples from a density Pi.",
                    "label": 0
                },
                {
                    "sent": "Never know P. Right?",
                    "label": 0
                },
                {
                    "sent": "I am not taking him so I don't want to take histograms.",
                    "label": 0
                },
                {
                    "sent": "Right, so I want.",
                    "label": 0
                },
                {
                    "sent": "So here I just need to estimate the.",
                    "label": 0
                },
                {
                    "sent": "L2 distance between P&Q and, but I'm saying that you don't need to estimate the densities P&Q to calculate Delta distance between them.",
                    "label": 0
                },
                {
                    "sent": "Estimated from Cape yes.",
                    "label": 0
                },
                {
                    "sent": "Exactly.",
                    "label": 0
                },
                {
                    "sent": "OK. And there are many other approaches to do these problems as well.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see some.",
                    "label": 0
                },
                {
                    "sent": "This is the algorithm and let's see how good is this algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first I would like to show you some computer vision experiments.",
                    "label": 0
                },
                {
                    "sent": "So what we do is that we break.",
                    "label": 0
                },
                {
                    "sent": "We have an image we want to classify these images.",
                    "label": 0
                },
                {
                    "sent": "We have a set of images and we want to classify images and what we do is that we break the images to smaller parts image patches.",
                    "label": 0
                },
                {
                    "sent": "And we represent images as a set of these image patches, and these batches can be overlapping.",
                    "label": 1
                },
                {
                    "sent": "Nonoverlapping Dispatch location can be gridpoints.",
                    "label": 0
                },
                {
                    "sent": "Interesting points can be totally random.",
                    "label": 0
                },
                {
                    "sent": "Doesn't matter the past sizes can be same different.",
                    "label": 0
                },
                {
                    "sent": "They conform.",
                    "label": 0
                },
                {
                    "sent": "Hierarchy doesn't matter.",
                    "label": 1
                },
                {
                    "sent": "And what we do that in each image Patch?",
                    "label": 0
                },
                {
                    "sent": "This sub to subtract sift.",
                    "label": 0
                },
                {
                    "sent": "Feature vector, which is a 120 dimensional vector that was calculated just by this.",
                    "label": 0
                },
                {
                    "sent": "Batch and we can compress that with PCA to say dimension D. And now each image is represented by a set of these D dimensional feature vectors.",
                    "label": 1
                },
                {
                    "sent": "And we will consider each set as a sample from some unknown distribution.",
                    "label": 0
                },
                {
                    "sent": "And now if you want to classify images, you can think of that as classifying those distributions that generates these feature vectors.",
                    "label": 0
                },
                {
                    "sent": "So you can play with.",
                    "label": 0
                },
                {
                    "sent": "Have whatever you like.",
                    "label": 0
                },
                {
                    "sent": "So in the concrete examples I will show you some, but you have your freedom whatever you want, you can.",
                    "label": 1
                },
                {
                    "sent": "You can still run the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And we acquired this image representation as a D dimensional dimensional sample sets representation of the image.",
                    "label": 0
                },
                {
                    "sent": "OK, and here are.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Examples, so we had 50 highway images.",
                    "label": 1
                },
                {
                    "sent": "And we injected 5 animals images so they can see 4 rooms and this one is a three and we calculated these two dimensional sample set representations.",
                    "label": 0
                },
                {
                    "sent": "So we divided this to image patches, calculated the features compressed that 2D and then you can see the sample each image was representative after that with a sample set.",
                    "label": 0
                },
                {
                    "sent": "So this image was represented by this set this image by this set this image by this set and so on and we say.",
                    "label": 0
                },
                {
                    "sent": "That so this anonymous represented by this set, we say that an image is a numerous if this calculated distribution is far from the rest.",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                },
                {
                    "sent": "Based on this, we can have an animal score and then we.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then order the images according to their normal score.",
                    "label": 0
                },
                {
                    "sent": "So this got the highest CinemaScore.",
                    "label": 0
                },
                {
                    "sent": "Discuss the second higher.",
                    "label": 0
                },
                {
                    "sent": "This got the smallest animal score and these were the five injected animals images, so you can see that this algorithm can find over this five injected anomalous out of these from the first, then most analysis.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You might ask why I doing this nonparametric approach, why I just don't model this mixture of Gaussian?",
                    "label": 0
                },
                {
                    "sent": "For example, you might think that, hey, what happens if I model this is a mixture of Gaussian and then calculate the distances between these mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "But then this is what we would get.",
                    "label": 0
                },
                {
                    "sent": "So this parametric approach.",
                    "label": 0
                },
                {
                    "sent": "Would sort the animals.",
                    "label": 0
                },
                {
                    "sent": "Images like this.",
                    "label": 0
                },
                {
                    "sent": "So you can see that the true 5.",
                    "label": 0
                },
                {
                    "sent": "We cannot find these five animals injected pictures among the first.",
                    "label": 0
                },
                {
                    "sent": "Then images.",
                    "label": 0
                },
                {
                    "sent": "So parametric approach in this problem.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doesn't work well.",
                    "label": 0
                },
                {
                    "sent": "Another problem I would like to talk about is classifying noisy USPS datasets.",
                    "label": 1
                },
                {
                    "sent": "So here you can see some images from the USPS data set, and.",
                    "label": 0
                },
                {
                    "sent": "This is quite easy in that sense that even standard support vector machine based algorithms can achieve 97% accuracy on the classification.",
                    "label": 0
                },
                {
                    "sent": "So we make this data set more challenging.",
                    "label": 0
                },
                {
                    "sent": "In this way that we.",
                    "label": 0
                },
                {
                    "sent": "Scaled images to have 160 one 160 sizes.",
                    "label": 0
                },
                {
                    "sent": "And we consider these images as density functions and with sample.",
                    "label": 0
                },
                {
                    "sent": "From this density functions, you can see the sample points.",
                    "label": 0
                },
                {
                    "sent": "Here I sample 500 to the points.",
                    "label": 0
                },
                {
                    "sent": "And I replaced the original data set by this.",
                    "label": 0
                },
                {
                    "sent": "Kind of points.",
                    "label": 0
                },
                {
                    "sent": "So here you can see the new data set.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                },
                {
                    "sent": "We had 1000 training 1000 instances.",
                    "label": 1
                },
                {
                    "sent": "And if I just run support vector machine on this data set where they were images, then the classification accuracy would drop from 97% to 82%.",
                    "label": 0
                },
                {
                    "sent": "But if we.",
                    "label": 0
                },
                {
                    "sent": "Use this super distribution machine ideas, then the accuracy would be 96%.",
                    "label": 0
                },
                {
                    "sent": "So it just means that.",
                    "label": 0
                },
                {
                    "sent": "If your data set is like this, then instead of using Euclidean distance, it might be better too.",
                    "label": 0
                },
                {
                    "sent": "Use distribution based distances.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I can demonstrate it further on this data set that I had 10 instances from 1234.",
                    "label": 0
                },
                {
                    "sent": "Numbers, and I calculated the pairwise Euclidean distances between these numbers and with minimum with multidimensional scaling I embedded them into 2D.",
                    "label": 1
                },
                {
                    "sent": "And here you can see the embedded points.",
                    "label": 0
                },
                {
                    "sent": "There's no structure at all, but if I calculated the.",
                    "label": 0
                },
                {
                    "sent": "The Euclidean distance between the distributions, not between the images between between the distributions.",
                    "label": 1
                },
                {
                    "sent": "Then you can clearly 3 the CD structures and because of the classification is much easier.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here is another.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So you could do that, but then you should play with the smoothness parameters and so on, right?",
                    "label": 0
                },
                {
                    "sent": "It would include I was not testing how much you can get after that.",
                    "label": 0
                },
                {
                    "sent": "So here is another data set.",
                    "label": 0
                },
                {
                    "sent": "This is the coil.",
                    "label": 0
                },
                {
                    "sent": "Data set, so we had these original images rotated froggies and I used some edge detection on these images.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Then I run local linear embedding on these images and here you can see the embedded points to 2D.",
                    "label": 1
                },
                {
                    "sent": "So you can see that this embedded points at this point corresponds to this flow.",
                    "label": 0
                },
                {
                    "sent": "Give this one to this, this one to this.",
                    "label": 1
                },
                {
                    "sent": "It doesn't keep any structure if you just use distances between these edge detected images.",
                    "label": 0
                },
                {
                    "sent": "But again, if I consider them as samples from some 2D distributions and I usually then.",
                    "label": 0
                },
                {
                    "sent": "Ali on distributions then you can see that it keeps the structure.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this will just toy problems, but we run this distribution based machine learning on larger image.",
                    "label": 0
                },
                {
                    "sent": "Datasets so this is a was a problem with the object classification problem.",
                    "label": 1
                },
                {
                    "sent": "So we have eight categories.",
                    "label": 0
                },
                {
                    "sent": "You can see these categories and we had some images from each category altogether we had.",
                    "label": 0
                },
                {
                    "sent": "400 images and each represented each image by.",
                    "label": 1
                },
                {
                    "sent": "576 Eighteen dimensional feature points and we run several algorithms on this classification problem.",
                    "label": 0
                },
                {
                    "sent": "Using two fold cross validation and ten runs, you can see the results here.",
                    "label": 0
                },
                {
                    "sent": "And the best.",
                    "label": 0
                },
                {
                    "sent": "Algorithms so far was a bag of words approach that achieved 87% accuracy, but when we used our nonparametric rainy divisions based estimator than we got.",
                    "label": 0
                },
                {
                    "sent": "Higher accuracy and the difference is significant.",
                    "label": 0
                },
                {
                    "sent": "You might ask.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How to choose Alpha in the Rainier for Divergent estimator you can use cross validation and here I'm using some results if you.",
                    "label": 0
                },
                {
                    "sent": "Change Alpha between minus one and two.",
                    "label": 0
                },
                {
                    "sent": "Then these are the accuracies so, but you can offset this that we got the highest accuracy for Alpha and Alpha is close to one that would be the kullback Leiber.",
                    "label": 0
                },
                {
                    "sent": "Divergent, right?",
                    "label": 0
                },
                {
                    "sent": "But still the highest accuracy was not exactly then off of US one.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we repeated these experiments with other datasets as well.",
                    "label": 0
                },
                {
                    "sent": "So here you can see.",
                    "label": 0
                },
                {
                    "sent": "Images from outdoor scene classifications.",
                    "label": 0
                },
                {
                    "sent": "So we had eight different categories.",
                    "label": 0
                },
                {
                    "sent": "The best published results that we found on this data set was this 91.57% our algorithm achieved.",
                    "label": 1
                },
                {
                    "sent": "Better results and again this.",
                    "label": 0
                },
                {
                    "sent": "Difference was significant.",
                    "label": 0
                },
                {
                    "sent": "We but on the same algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On Sport event classification as well, again we had eight categories and you can see that actually this might be quite challenging.",
                    "label": 0
                },
                {
                    "sent": "This data set because the images are so different, right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And the best published result was 86.7%.",
                    "label": 1
                },
                {
                    "sent": "Our algorithm achieved.",
                    "label": 0
                },
                {
                    "sent": "A bit better on this data set as well.",
                    "label": 1
                },
                {
                    "sent": "So what I would like to tell you with these experiments is that you know in computer vision.",
                    "label": 0
                },
                {
                    "sent": "The goal of the researcher is just to achieve the best results with whatever algorithm you want, and they are not afraid of using sophisticated feature construction, complex algorithms and kind of heretics.",
                    "label": 1
                },
                {
                    "sent": "And with these algorithms we run the same algorithm on these three datasets.",
                    "label": 0
                },
                {
                    "sent": "We just use standard safety features and on the three datasets we got the three best performances.",
                    "label": 0
                },
                {
                    "sent": "OK, there are other.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problems where this distribution based learning can be useful.",
                    "label": 0
                },
                {
                    "sent": "One of them, as I mentioned, is defined.",
                    "label": 0
                },
                {
                    "sent": "Interesting objects in Sky.",
                    "label": 0
                },
                {
                    "sent": "For example, interesting Galaxy clusters.",
                    "label": 0
                },
                {
                    "sent": "You know, astronomers take millions of pictures of the Sky each day.",
                    "label": 0
                },
                {
                    "sent": "But they don't have time to look at those pictures.",
                    "label": 0
                },
                {
                    "sent": "They want some automated methods that are able to find the most interesting objects.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We use the Sloan Digital Sky survey.",
                    "label": 1
                },
                {
                    "sent": "Which contains 505 Galaxy clusters.",
                    "label": 0
                },
                {
                    "sent": "Usually each cluster has 10 to 50 galaxies altogether.",
                    "label": 0
                },
                {
                    "sent": "We had more than 7500 galaxies in this data set, and again our goal was to find the most interesting Galaxy clusters and.",
                    "label": 0
                },
                {
                    "sent": "We were even considering those cases where it can happen that each Galaxy in the Galaxy cluster looks totally normal, but the cluster itself looks interesting.",
                    "label": 0
                },
                {
                    "sent": "And usually.",
                    "label": 0
                },
                {
                    "sent": "Galaxies can be classified as blue and red galaxies based on their Spectra features, so the galaxies has higher Spectra.",
                    "label": 0
                },
                {
                    "sent": "In this part the red Galaxies has higher Spectra in this part and usually Galaxy clusters has.",
                    "label": 0
                },
                {
                    "sent": "Some red galaxies in the center.",
                    "label": 0
                },
                {
                    "sent": "They are dead.",
                    "label": 0
                },
                {
                    "sent": "Galaxies don't do anything interesting, and our own them.",
                    "label": 0
                },
                {
                    "sent": "There are some blue galaxies.",
                    "label": 0
                },
                {
                    "sent": "And we run our.",
                    "label": 0
                },
                {
                    "sent": "Anomaly detector algorithm on this data set and this is what we found that the most animals, Galaxy clusters content mostly start forming blue galaxies and irregular galaxies.",
                    "label": 1
                },
                {
                    "sent": "Irregular galaxies are interesting itself.",
                    "label": 0
                },
                {
                    "sent": "Start from the blue.",
                    "label": 0
                },
                {
                    "sent": "Galaxies are not that interesting, but that was very interesting that there were no red galaxies there.",
                    "label": 0
                },
                {
                    "sent": "So blue galaxies are pretty normal, but usually there should be some red galaxies there as well, and because there were no red galaxies that it might mean that there were galaxies that they're just colliding there, and Stan formation happens, or other interesting things happen.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are other problems where this distribution based machine learning can help.",
                    "label": 0
                },
                {
                    "sent": "One problem is understanding turbulence datasets.",
                    "label": 0
                },
                {
                    "sent": "So understanding turbulence is very important if we want to develop more economical cars or safer faster airplanes or studying turbulence in the atmosphere of the sun is important to understand magnetic storms.",
                    "label": 1
                },
                {
                    "sent": "Solar winds.",
                    "label": 0
                },
                {
                    "sent": "You know ocean currents can affect the temperature throughout the world, and currently one of the biggest of Stachel in.",
                    "label": 0
                },
                {
                    "sent": "Building Fusion power plants is to keep the plasma together.",
                    "label": 1
                },
                {
                    "sent": "Because of that, understanding turbulences in plasma, it's very important.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our collaborators at Johns Hopkins University, they built huge fluid flow flow simulator and their goal was to find interesting events in these simulations.",
                    "label": 1
                },
                {
                    "sent": "One tiny problem is that we don't know what interesting means.",
                    "label": 0
                },
                {
                    "sent": "So you know science apparently reached the point where it's not only that we don't understand the real world, we don't even understand our simulations.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "We should know about interesting means and.",
                    "label": 0
                },
                {
                    "sent": "As the first approach, we saw that.",
                    "label": 0
                },
                {
                    "sent": "Maybe what this is can be interesting, so we try to classify.",
                    "label": 0
                },
                {
                    "sent": "We try to detect vertices.",
                    "label": 1
                },
                {
                    "sent": "That was our approach and we built a data set that contained 11 vertices and twenty negative examples.",
                    "label": 0
                },
                {
                    "sent": "So here you can see one positive example vertex and we had twenty negative examples that they were nothing and we had to classify the distribution of.",
                    "label": 0
                },
                {
                    "sent": "These velocities.",
                    "label": 0
                },
                {
                    "sent": "And using our.",
                    "label": 0
                },
                {
                    "sent": "Algorithm we got 97% accuracy on this data set.",
                    "label": 1
                },
                {
                    "sent": "So basically we missed one out of this 11.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What what this is?",
                    "label": 0
                },
                {
                    "sent": "And here you can see the classification probabilities.",
                    "label": 1
                },
                {
                    "sent": "So you can see that in this area indeed we got higher classification probability, and it looks like a vertex and here as well and low probabilities otherwise.",
                    "label": 0
                },
                {
                    "sent": "And now you can ask how to really detect interesting.",
                    "label": 0
                },
                {
                    "sent": "Events",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This data set and what we did is that we don't have one class.",
                    "label": 0
                },
                {
                    "sent": "Support distribution machine where we.",
                    "label": 0
                },
                {
                    "sent": "You use that to find anomalous objects and here you can see the classification probabilities.",
                    "label": 0
                },
                {
                    "sent": "It still has high probabilities, invertis is, but it got even higher probability.",
                    "label": 0
                },
                {
                    "sent": "Then these vertices interact with each other so you can see this diamond shape that got the highest score.",
                    "label": 0
                },
                {
                    "sent": "So it might mean that these kind of events were very rare in this data set.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, I would like to show some theoretical results on this kind of algae.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things as well.",
                    "label": 0
                },
                {
                    "sent": "So far we've got good experimental results, but you might ask what about Thierry?",
                    "label": 1
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "In practice you might ask these questions how many distributions we need in the training data set and how many samples from each distribution we need to get good classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to focus on distribution regression.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Blam now.",
                    "label": 0
                },
                {
                    "sent": "Just checking OK.",
                    "label": 0
                },
                {
                    "sent": "So for regression one standard patient algorithm.",
                    "label": 0
                },
                {
                    "sent": "Is there another way about Sun Gardner regression?",
                    "label": 0
                },
                {
                    "sent": "This is the model YFX plus new X is a dimensional vector by is response variable means noise, expected value is zero and X and knew they are independent.",
                    "label": 0
                },
                {
                    "sent": "And we have M training points.",
                    "label": 1
                },
                {
                    "sent": "And we have a new test point, dimensional this point and we need to estimate this function FX, which is just the conditional expected value of Y.",
                    "label": 0
                },
                {
                    "sent": "And this is the kinetic destiny you are familiar with, so you choose a can of function which has shaped like this.",
                    "label": 0
                },
                {
                    "sent": "You use a bandwidth parameter.",
                    "label": 0
                },
                {
                    "sent": "HM, it depends on how many points you have in the sample set and.",
                    "label": 0
                },
                {
                    "sent": "To show that so under some you can prove that under some conditions this estimator can be consistent, and for that the bandwidth should go to zero, but not too fast.",
                    "label": 0
                },
                {
                    "sent": "So you also want this quantity too.",
                    "label": 0
                },
                {
                    "sent": "Oops, this should go to Infinity, right?",
                    "label": 0
                },
                {
                    "sent": "And then the estimator is.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Consistent.",
                    "label": 0
                },
                {
                    "sent": "So our distribution regression is different in that sense that.",
                    "label": 1
                },
                {
                    "sent": "Now we have.",
                    "label": 0
                },
                {
                    "sent": "M distributions we have response variables again M But we don't know that density functions.",
                    "label": 0
                },
                {
                    "sent": "We just have some IID sample points from these distributions.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We assume that we have a meta distribution P that generates this distribution.",
                    "label": 0
                },
                {
                    "sent": "IID in an IID V and from each distribution we have sample points.",
                    "label": 0
                },
                {
                    "sent": "Our goal is to estimate the conditional expectation of the response variable conditioned on the distribution.",
                    "label": 0
                },
                {
                    "sent": "So the difficulties here is that if you think about.",
                    "label": 0
                },
                {
                    "sent": "The distributions they look like an infinite dimensional objects, right?",
                    "label": 0
                },
                {
                    "sent": "And we still want to prove consistency.",
                    "label": 0
                },
                {
                    "sent": "But if this dimension is.",
                    "label": 0
                },
                {
                    "sent": "Infinite that we cannot get that.",
                    "label": 0
                },
                {
                    "sent": "And the other problem is that we don't even know the true.",
                    "label": 0
                },
                {
                    "sent": "Distributions we have this error in variables because from each distribution we just have some finite.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample sets.",
                    "label": 0
                },
                {
                    "sent": "So this is the estimate, so I showed you several other estimators.",
                    "label": 0
                },
                {
                    "sent": "But to prove theory to prove consistency, I'm going to study this kind of estimator, which is another area but soon kernel regression are where.",
                    "label": 0
                },
                {
                    "sent": "D is a distance between sample sets or empirical distributions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Traditionally, here you have the Euclidean distance between finite dimensional points.",
                    "label": 0
                },
                {
                    "sent": "We replace that to the distribution between sample sets or empirical distributions.",
                    "label": 0
                },
                {
                    "sent": "OK, we will use the add one distance here.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The L1 distance between two distributions is divided by this quantity of API and PJR densities, so we need to estimate the densities as well.",
                    "label": 0
                },
                {
                    "sent": "And for that we will use another kernel.",
                    "label": 0
                },
                {
                    "sent": "So because of that, we say that this estimator is a kernel kernel estimator, so we have two channels, one here.",
                    "label": 0
                },
                {
                    "sent": "For doing the regression and another kernel to measure the distances between distributions.",
                    "label": 0
                },
                {
                    "sent": "Is that clear?",
                    "label": 0
                },
                {
                    "sent": "So we have two bandwidths of this, Colonel in the estimator is by the dimension of the sample points is K. This subject of this P distributions here that will give us the distance between the empirical distributions and the subset of this distance here, and we can do the regression.",
                    "label": 0
                },
                {
                    "sent": "Ask questions if.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "We want.",
                    "label": 0
                },
                {
                    "sent": "We have M sample sets OK. You give me a new set and you ask what would be the response value in that set.",
                    "label": 0
                },
                {
                    "sent": "It's a regression problem.",
                    "label": 0
                },
                {
                    "sent": "This is the regression.",
                    "label": 0
                },
                {
                    "sent": "What we are going to do.",
                    "label": 0
                },
                {
                    "sent": "We just measure how far these sample sets are from each other.",
                    "label": 0
                },
                {
                    "sent": "That's the substitute that here.",
                    "label": 0
                },
                {
                    "sent": "And then it looks like the standard under a Batson kernel regression, right?",
                    "label": 0
                },
                {
                    "sent": "H is the bandwidth.",
                    "label": 0
                },
                {
                    "sent": "And we need to measure how far these distributions are from each other.",
                    "label": 0
                },
                {
                    "sent": "And for that we will use this distance.",
                    "label": 0
                },
                {
                    "sent": "So I have two sample sets.",
                    "label": 0
                },
                {
                    "sent": "On these two sample sets, I estimate the density functions that P had IP had J and I calculate their L1 distance.",
                    "label": 0
                },
                {
                    "sent": "For different datasets they must have the same dimension.",
                    "label": 0
                },
                {
                    "sent": "So X is the same.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Oh, it doesn't matter because to prove that it was simpler for us.",
                    "label": 0
                },
                {
                    "sent": "You can do other things too.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we will have two different bandwidths and we have to schedule them right.",
                    "label": 0
                },
                {
                    "sent": "I will tell you how to do that.",
                    "label": 0
                },
                {
                    "sent": "OK, other questions, so it's important to understand the basic problem.",
                    "label": 0
                },
                {
                    "sent": "OK, and our goal Busta bound the expected value of the estimated.",
                    "label": 0
                },
                {
                    "sent": "Response.",
                    "label": 0
                },
                {
                    "sent": "Minus the true response.",
                    "label": 0
                },
                {
                    "sent": "OK, and we want to show that under some conditions this risk is going to 0.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we need a few assumptions to prove that.",
                    "label": 0
                },
                {
                    "sent": "One assumption is that.",
                    "label": 0
                },
                {
                    "sent": "F so remember our models that Y = F P plus noise this F. Is heard that continuous with some parameter beta and constant L?",
                    "label": 0
                },
                {
                    "sent": "D Again is the distance between \u03c0 and PJ distributions.",
                    "label": 0
                },
                {
                    "sent": "We will use this kind of kernels that we call a synthetic box Lipschitz Colonel, so this is the kernel function.",
                    "label": 0
                },
                {
                    "sent": "We just need those kind of cars so this is zero.",
                    "label": 0
                },
                {
                    "sent": "We just need those kind of kernels that are defined on positive or non negative numbers because we will put distances we will use them on distances these kernels and we assume that you can put a box in the kernel and you can put box outside of the scanner as well.",
                    "label": 0
                },
                {
                    "sent": "So for example, Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "We don't know what to do with that.",
                    "label": 0
                },
                {
                    "sent": "But there are many other kernels that are good for for us.",
                    "label": 0
                },
                {
                    "sent": "We also assume that P&PI the distribution, the densities of these distributions are Lipschitz continuous.",
                    "label": 0
                },
                {
                    "sent": "It could be generalized to hell there as well, but I'm not doing that now.",
                    "label": 0
                },
                {
                    "sent": "We also assume that.",
                    "label": 0
                },
                {
                    "sent": "This is a bounded regression problem, so both F and the observation wise they are bounded.",
                    "label": 1
                },
                {
                    "sent": "And we will also assume that there's a from each distribution.",
                    "label": 0
                },
                {
                    "sent": "We have some sample sets and the minimum value will be denoted by M and we assume this rate.",
                    "label": 0
                },
                {
                    "sent": "Where M is the number of distributions and N is the number of these.",
                    "label": 1
                },
                {
                    "sent": "This minimum value from the sample sets.",
                    "label": 1
                },
                {
                    "sent": "OK. And.",
                    "label": 0
                },
                {
                    "sent": "So there should be a relationship between the number of samples we have and the bandwidth.",
                    "label": 0
                },
                {
                    "sent": "And for that we need.",
                    "label": 0
                },
                {
                    "sent": "To assume something that again, air here is.",
                    "label": 0
                },
                {
                    "sent": "The size of this box that I put in the kernel H is the bandwidth.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we need to bound the risk.",
                    "label": 1
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "So we will know one more definition and this is this will define so-called smallball probabilities.",
                    "label": 0
                },
                {
                    "sent": "So we fixed the distribution P and we calculate and we fixed bandwidth H and we calculate what is the probability that another distribution is in this ball with radius H. What's the probability of that?",
                    "label": 0
                },
                {
                    "sent": "And we will be interested in this.",
                    "label": 0
                },
                {
                    "sent": "H is small.",
                    "label": 0
                },
                {
                    "sent": "That's like the quality quantity smallbore probabilities.",
                    "label": 0
                },
                {
                    "sent": "And this is the risk what we have.",
                    "label": 0
                },
                {
                    "sent": "This is our main theorem.",
                    "label": 0
                },
                {
                    "sent": "That the bandwidth.",
                    "label": 0
                },
                {
                    "sent": "Of that kernel that Arthur asked should have this rate.",
                    "label": 0
                },
                {
                    "sent": "It should go this path to zero and then we can prove that the risk is less than this quantity, which looks very ugly, but it might make some sense.",
                    "label": 0
                },
                {
                    "sent": "So H again is the.",
                    "label": 0
                },
                {
                    "sent": "Bandwidth parameter of our first kernel we have from each distribution at least N sample points.",
                    "label": 0
                },
                {
                    "sent": "Beta is a parameter of the.",
                    "label": 0
                },
                {
                    "sent": "Herder Continuity M is the number of distributions and everywhere we have the expected value of these small bowl probabilities.",
                    "label": 0
                },
                {
                    "sent": "So this is a function of P. Right, and we take the expected.",
                    "label": 0
                },
                {
                    "sent": "Station of that it looks very ugly.",
                    "label": 0
                },
                {
                    "sent": "I understand that, but we can simplify these things a bit.",
                    "label": 0
                },
                {
                    "sent": "So we will assume that.",
                    "label": 0
                },
                {
                    "sent": "That method distribution that generates our distributions it has a doubling measure.",
                    "label": 0
                },
                {
                    "sent": "Which is in the following sense that.",
                    "label": 0
                },
                {
                    "sent": "We have this.",
                    "label": 0
                },
                {
                    "sent": "We assume that with some D dimension D This property holds.",
                    "label": 0
                },
                {
                    "sent": "That is, the probability of a bowl.",
                    "label": 0
                },
                {
                    "sent": "Centered in P with radius H. If I multiply this H by epsilon.",
                    "label": 0
                },
                {
                    "sent": "Then these balls behave like balls in the dimensional space and.",
                    "label": 0
                },
                {
                    "sent": "If I divide these two balls small rooms, then we will have this property, so we assume this for now and we will discuss later how realistic this assumption is.",
                    "label": 0
                },
                {
                    "sent": "And if you do that then it's very easy to prove that these ugly, expected, expected values that we have in our first theorem.",
                    "label": 0
                },
                {
                    "sent": "Dave, you have some nice ones, so they will just look like this and then.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our main theorem has this very simple form.",
                    "label": 0
                },
                {
                    "sent": "That risk.",
                    "label": 0
                },
                {
                    "sent": "OK, I made some typos here, but so this.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so I should fix that.",
                    "label": 0
                },
                {
                    "sent": "But so anyhow, so we got the quantity where that is depends only on H&M and M is the number of distribution H is the bandwidth of our first kernel.",
                    "label": 0
                },
                {
                    "sent": "We already showed how to have the bandwidth for the other kernel and is the number of the samples from the distribution and we can show that this goes to 0 if you increase and.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "MI will show you that in the other slide I just want to show one more thing.",
                    "label": 0
                },
                {
                    "sent": "Before that you might ask how realistic is to have this doubling dimension and I'm showing you some examples.",
                    "label": 0
                },
                {
                    "sent": "Then we have finite dimension.",
                    "label": 0
                },
                {
                    "sent": "So for example imagine that.",
                    "label": 0
                },
                {
                    "sent": "You have a parametric family and the parameter parameter comes from a finite dimensional and that generates you.",
                    "label": 0
                },
                {
                    "sent": "The distributions.",
                    "label": 0
                },
                {
                    "sent": "Then in many under many conditions this.",
                    "label": 0
                },
                {
                    "sent": "Family will have finite dimension, so imagine this example that I have Gaussian distributions where I just rotate this Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "If I embed them to.",
                    "label": 0
                },
                {
                    "sent": "So now you can think so I'm just showing here how they look if I embed them to a 2D space.",
                    "label": 0
                },
                {
                    "sent": "But now you can imagine that indeed if I measure the distances, how far these rotated Gaussians are from each other, because I'm just changing one parameter then.",
                    "label": 0
                },
                {
                    "sent": "It might have a.",
                    "label": 0
                },
                {
                    "sent": "Low dimension, so it lies in a low dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "And the same thing happens.",
                    "label": 0
                },
                {
                    "sent": "Here is if I have a 1 dimensional Gaussian distribution and I change two parameters, say the mean and the variance of Gaussian distributions, then they lie on a manifold on a 2 dimensional manifold and then this doubling dimension will be 2.",
                    "label": 1
                },
                {
                    "sent": "So even though each distribution is a infinite dimensional object, if there's a process that generates this distribution if that eventually these distributions are on the manifold, then the doubling dimension is basically it's the interesting dimension of this manifold, and then you get polynomial rate.",
                    "label": 0
                },
                {
                    "sent": "So I'm just showing the rates in these cases.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sick leave.",
                    "label": 0
                },
                {
                    "sent": "Two cases and.",
                    "label": 0
                },
                {
                    "sent": "One case, in one case the rate looks like this.",
                    "label": 0
                },
                {
                    "sent": "In the other case.",
                    "label": 0
                },
                {
                    "sent": "Today disappeared.",
                    "label": 0
                },
                {
                    "sent": "It's here so another case that.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looks like this and.",
                    "label": 0
                },
                {
                    "sent": "For the proofs, it's a bit messy, but what you need to do is you just decompose so this is the risk definition and you can decompose these two terms.",
                    "label": 0
                },
                {
                    "sent": "Affect is the estimator and one case you assume.",
                    "label": 0
                },
                {
                    "sent": "Here you assume that you know all those distributions, so there are no hats.",
                    "label": 0
                },
                {
                    "sent": "Here in the other case, you don't.",
                    "label": 0
                },
                {
                    "sent": "In the other case, you assume that you still have heads here and.",
                    "label": 0
                },
                {
                    "sent": "Then it will be still ugly after this, but in 10 more pages you can prove it and we got similar results when instead of key.",
                    "label": 0
                },
                {
                    "sent": "Instead of a kernel density estimator, we use scanner regression and we have.",
                    "label": 0
                },
                {
                    "sent": "Consistency proof when the dimension is larger than two, so it seems that there is a curse of low dimensionality here that for small dimension we cannot prove consistency.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "Just skip.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This part so I wanted to show.",
                    "label": 0
                },
                {
                    "sent": "Why the small probabilities appear so naturally here?",
                    "label": 0
                },
                {
                    "sent": "But I think I just skip this slide.",
                    "label": 0
                },
                {
                    "sent": "Just show you one.",
                    "label": 0
                },
                {
                    "sent": "The results of 1.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiment here.",
                    "label": 0
                },
                {
                    "sent": "We wanted to learn the skewness of beta distribution.",
                    "label": 1
                },
                {
                    "sent": "So we had many beta distributions with different skewness is and in a supervised Lee way if used this our estimator then you can hear see the true values.",
                    "label": 0
                },
                {
                    "sent": "And here.",
                    "label": 0
                },
                {
                    "sent": "The estimated value is and.",
                    "label": 1
                },
                {
                    "sent": "They also use this approach to learn the entropy of Gaussian distributions in a supervised way.",
                    "label": 0
                },
                {
                    "sent": "So we had Gaussian distribution with different parameters and different entropies.",
                    "label": 0
                },
                {
                    "sent": "And here you can see the.",
                    "label": 0
                },
                {
                    "sent": "Through entropy curve, if you change parameters and here you can see the estimated values of the entropies, so it shows that you can get good results with this.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kind of regression method.",
                    "label": 0
                },
                {
                    "sent": "So the take home message, but I showed you today is that in parametric way you can estimate rainy divergences between distributions without estimating densities.",
                    "label": 0
                },
                {
                    "sent": "I show you after that how to use these divergences to.",
                    "label": 0
                },
                {
                    "sent": "Do machine learning on distributions.",
                    "label": 1
                },
                {
                    "sent": "I show you some experiments in computer vision, instrument turbulence, data processing and then I show you that some theoretical results as well, and.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 1
                }
            ]
        }
    }
}