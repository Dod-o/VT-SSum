{
    "id": "q6hswzwigw765f3gpvcw6seln3mrbstl",
    "title": "Semi-supervised Learning by Higher Order Regularization",
    "info": {
        "author": [
            "Xueyuan Zhou, Department of Computer Science, University of Chicago"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/aistats2011_zhou_learning/",
    "segmentation": [
        [
            "I'll be quick so hope we can finish it in time so it's some supervised learning by how other regularization is John work with Michelle Belkin from the University of the Ohio State University and my name is Yuan Zhou from the University of Chicago.",
            "So here's a quick."
        ],
        [
            "Outline basically the first half of the talk will be.",
            "I'll show you that a very popular algorithm is problematic, and in the second half will show how to solve it.",
            "OK so first semi supervised learning.",
            "What is sensor revenue from the name?",
            "Actually you already see it is something between supervised and unsupervised learning.",
            "You're given a label set level set means you have the data you have.",
            "The observation on the data, like the class of real value functions and you are also given a very large unable to set meaning that you don't have the observation on those.",
            "This set is also called partially observed data which is very intuitive way to state.",
            "So in this setting you can at least do two things.",
            "One is inductive.",
            "That means you want to learn a function over the whole domain."
        ],
        [
            "And another work you can do is transductive, which means you only want to know the estimate of observation on the unlabeled point.",
            "OK.",
            "So here's some stand."
        ],
        [
            "Annotation the labeled data is XL and its label is YL and unlabeled data is Xu.",
            "Usually we have a lot more unlabeled data than the labeled ones, and so the SSL is trying to find the function FX either on the whole domain, like the inductive setting or on the table set.",
            "In this case, we try to learn a vector, which is why you."
        ],
        [
            "And here's some assumption.",
            "We assume our data from a joint ID from a domain, which can be a manifold embedded in high dimension and density is bounded.",
            "Some very general setting.",
            "So here's some example how to use this where it can be used.",
            "For example, the first line is a classification for my Internet search queries.",
            "In this case, your ex will be the keyword query type into the search box and your why will be some predefined class.",
            "For example you trying to search a movie or restaurant or something like that and another.",
            "The second example is one of my favorite."
        ],
        [
            "Furthermore, it can be seen as a estimation problem.",
            "Our graph, for example, let's say link in the X in this case can be the profile like a user resume and why can be something.",
            "Furthermore, whether this guy is interested in machine learning job or not, and the next example is it also can be used in information retrieval like image ranking.",
            "Then your X will be the image and why it will be a ranking score.",
            "Here is another classification instead like.",
            "Aggression because this car is a real value function."
        ],
        [
            "OK, so then there is a very popular algorithm to try to solve this problem and the basic idea is trying to smooth function.",
            "Smooth is means you have.",
            "If you have similar input X you have a similar output Y and you can either do interpolation or you can do this square.",
            "So we have a.",
            "Here we have a weight WXY which describes a similarity between 2 random variable and let's see the out take the interpolation problem.",
            "Example, if you minimize that object if if two random variables are very similar, then the wait will be large and when you minimize it will penalize the difference between the estimation of those two.",
            "So this.",
            "Optimization problem will implement this idea.",
            "Similar X will give you a similar Y.",
            "So this problem can also be seen as a estimation on graphs.",
            "So then the random variable will be the not in the graph and the similarity weight will be the weight on the edge.",
            "So you can see there is a image there.",
            "It's a image digit.",
            "For example the link in example fit there nicely I think.",
            "OK, so let's see what's the problem of this."
        ],
        [
            "Problem with this solution.",
            "The object function can be written in the matrix form, where L is the cloud graph lesson which could hear a moment ago.",
            "So it is.",
            "It has a new space which is the constant functions and discuss.",
            "This metric form is a seminorm."
        ],
        [
            "So the setting will study is the following topic setting.",
            "We fixed the labels set.",
            "We let the unlabeled point go to Infinity, which is pretty practical because we have a lot more unlabeled data than legal advice.",
            "So then we N is the total number point if we left and go to Infinity, decrease because it's a differential operator graph labeling.",
            "I mean you have to shrink the neighborhood in order to get the final final structure data.",
            "So we have this limit which means this thing converted to a gradient square with the P square density.",
            "It's a symptom is easily seen from here.",
            "So then the semi supervised learning, with the infinite unlabeled point and fixed label."
        ],
        [
            "Data will be like this.",
            "So the smoothness believe idea will be will mean we have a small penalty which would be small seminorm.",
            "Of course, it's intuitively will see later why is intuitive, so we can use assign function to get the class in binary classification and the analysis applied to the square case.",
            "Similarly, in fact, we can already see tip of the iceberg here.",
            "Here.",
            "So well."
        ],
        [
            "See what happens next."
        ],
        [
            "So in some supply learning all these algorithms are designed to take advantage of those alible data part.",
            "So natural idea is if you have more and more unlabeled point, your algorithm should have a better and better result, but."
        ],
        [
            "Practice.",
            "It's kind of contrary to that.",
            "Furthermore, on the left is this other data unit.",
            "Uniform distribution of a square.",
            "So on the left we have a less animal part, so the solution is something like this.",
            "If you use a site function, you can see from the counter plot it should give you the two label part.",
            "It give you a pretty good nearest neighbor.",
            "But whether this estimate are smooth or not, it's difficult to see.",
            "But when we have a more and more valuable part, you can see the estimator converted to indicated function on the labor part.",
            "So it's flat like 0 if you sign function you don't know it's a plus or minus."
        ],
        [
            "Function I use a code because they are assigned their up to channel sign.",
            "This means the more unlabeled data you have, the best of the worst classifier guide and the worst result you have.",
            "This is exactly the opposite of your initial intuition."
        ],
        [
            "So let's see why this happens.",
            "Straightforward way is to just check the solution, plug back in right so we can use the first life to approximate the gradient square because P is bounded through that way.",
            "So and I write Dxi as H small value so this guy is old order H2D2 D is the intrinsic dimension or your domain.",
            "And if D is greater than two we can see.",
            "If we although the indicated function have infinite gradient on this point, but the whole integral is 0, which is smallest value you can get.",
            "So, and you can also use other methods to see a decode two.",
            "Also this also happened, so this means the indicator function."
        ],
        [
            "The smallest penalty, but yet it's not smooth and the nearby data around the label point have a very different value.",
            "Rap.",
            "So then this leads to the."
        ],
        [
            "Next question, what happened to this function space?",
            "This is the solution space, right?",
            "So this needs to take a closer look at this function space.",
            "Then you see it's called us a blue space, so it's if I like it, includes functions that function themselves and all the partial derivative up to all the M, including M or belong to L2.",
            "Then it's a subspace of order M. Our solutions based actually only each one is only the 1st order."
        ],
        [
            "Retail.",
            "So then the solution can be pretty straightforward.",
            "We just shrink because the solution space is too large, so we just shrink it.",
            "Hopefully we can get smoother solutions.",
            "So then the next step will be."
        ],
        [
            "How to implement this algorithm?",
            "How to compute it?"
        ],
        [
            "So the original definition of the norms of space used the partial derivatives, and it's very difficult to implement this powder on random sample which are drawn from unknown manifold.",
            "Furthermore, the boundary condition you just don't know where the boundary is."
        ],
        [
            "But Luckily we have a good way to implement it.",
            "So because there's a alternative way to describe the smoothness of Sobolev space by the Alto basis, which is the eigenfunctions of Laplacian and we can define this iteration latching signal.",
            "And use this norm to describe smoothness.",
            "So ether relax is just you applied operator time and time again.",
            "And notice the Lambda is increasing.",
            "Here is a different from the kernel, some a lot of time is decreasing but here."
        ],
        [
            "Is increasing.",
            "So there is a relation.",
            "The interlock lasting Sam Norm space actually is included in the Subspace S. Here is actually can be a real number, but later will only use integer because it's for computational reason.",
            "So now we change to change describing the space from pointwise.",
            "Function described by partial derivative to L2 function that described by the eigen functions."
        ],
        [
            "So then we arrive at this either last.",
            "Regularization problem.",
            "It's a.",
            "The only change to the existing problem is I used to iterate blessing, which numerically will be the power function or the matrix L which is graph blessing and you can see for fixed function smooth function.",
            "Let's say let's say C2, you have the following convergence.",
            "And if you have a nonuniform density, you can use weighted blessing and.",
            "If you have a boundary, you need some proper boundary conditions there.",
            "So."
        ],
        [
            "So let's see some toy example.",
            "Here we have a mixture of two Gaussian which are only separated by the first dimension.",
            "And then there are in 20 dimension space and all those plot is the projection on the first dimension.",
            "The left half is plus one, the right half is minus one.",
            "And then you can see the first one is M = 1 and the problem is exactly what we saw at first.",
            "If you see the Y axis is a .02.",
            "If you compare it to the label value is one is essentially zero and depends on how you pick those two label points just to label.",
            "One for each.",
            "It can be either up or either down and if we increase M, let's see.",
            "We can see there are two things happen.",
            "One is your threshold for sine function shift to the natural threshold which is zero.",
            "Make your solution stable.",
            "The second thing happen is you see the Y axis, particularly the last one.",
            "It's already got 2, two and roughly one for your labor point.",
            "So this solved that problem pretty good I think.",
            "Then I see some more examples."
        ],
        [
            "These are.",
            "The first one is a digit images, then there's some text later than the sum gene.",
            "Later the the first column M = y is the regular problem and.",
            "The second column is M because four we can see the improvement is across the board and particularly if you consider the simple change we made.",
            "We also can."
        ],
        [
            "Pair to another method, which is a test on the benchmark of this book and in the 11th there is a method here with additional L2 norm.",
            "I think the author is sitting here and we can see that the first column is the base case and because one and the last column is flashing and the second column is another method which is.",
            "A little different but related, and the third column is the best wishes.",
            "The best result from the book among 13 different algorithm.",
            "We can see the improvement for Italy, Lawson if you compared to the base case, the first one, the improvement is a pretty amazing.",
            "I think if you compare to the best one, it's a pretty competitive.",
            "Padilla, if you consider how those 13 algorithms are implemented, little detail here is sometimes people ask whether the additional L2 norm will solve that problem and the answer is no, because L2 function active space is essentially 0.",
            "And that's and there is a nested structure for solar space.",
            "The higher the order, the smaller the spaces.",
            "That means additional output number is already implemented in the graph regularizer, the FLF.",
            "OK, so here's a quick summary.",
            "Very specific messages you should use.",
            "It really blasting in most of the practical practical application because probably you don't.",
            "You won't have too many 1 dimensional problems.",
            "And another is.",
            "A little high level message is a lot of time.",
            "Intuition is very good for beginning first start something, but here we need actually a lot more, which is careful mathematical analysis for the particular message is the gradient square is not a good thing to measure smoothness in what in higher dimension, higher actually is.",
            "It only works in one dimensional space, and I'd like to take this opportunity to thank.",
            "The following people thought DuPont is a professor in my Department and then essence Rebel is a professor."
        ],
        [
            "Across the street from Total Technology Institute at Chicago and both Nadler hits from Wiseman.",
            "Institute of Science is real.",
            "Who is across the ocean and at last?",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll be quick so hope we can finish it in time so it's some supervised learning by how other regularization is John work with Michelle Belkin from the University of the Ohio State University and my name is Yuan Zhou from the University of Chicago.",
                    "label": 0
                },
                {
                    "sent": "So here's a quick.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Outline basically the first half of the talk will be.",
                    "label": 0
                },
                {
                    "sent": "I'll show you that a very popular algorithm is problematic, and in the second half will show how to solve it.",
                    "label": 0
                },
                {
                    "sent": "OK so first semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "What is sensor revenue from the name?",
                    "label": 0
                },
                {
                    "sent": "Actually you already see it is something between supervised and unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "You're given a label set level set means you have the data you have.",
                    "label": 0
                },
                {
                    "sent": "The observation on the data, like the class of real value functions and you are also given a very large unable to set meaning that you don't have the observation on those.",
                    "label": 0
                },
                {
                    "sent": "This set is also called partially observed data which is very intuitive way to state.",
                    "label": 0
                },
                {
                    "sent": "So in this setting you can at least do two things.",
                    "label": 0
                },
                {
                    "sent": "One is inductive.",
                    "label": 0
                },
                {
                    "sent": "That means you want to learn a function over the whole domain.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And another work you can do is transductive, which means you only want to know the estimate of observation on the unlabeled point.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here's some stand.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Annotation the labeled data is XL and its label is YL and unlabeled data is Xu.",
                    "label": 0
                },
                {
                    "sent": "Usually we have a lot more unlabeled data than the labeled ones, and so the SSL is trying to find the function FX either on the whole domain, like the inductive setting or on the table set.",
                    "label": 1
                },
                {
                    "sent": "In this case, we try to learn a vector, which is why you.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's some assumption.",
                    "label": 0
                },
                {
                    "sent": "We assume our data from a joint ID from a domain, which can be a manifold embedded in high dimension and density is bounded.",
                    "label": 0
                },
                {
                    "sent": "Some very general setting.",
                    "label": 0
                },
                {
                    "sent": "So here's some example how to use this where it can be used.",
                    "label": 0
                },
                {
                    "sent": "For example, the first line is a classification for my Internet search queries.",
                    "label": 0
                },
                {
                    "sent": "In this case, your ex will be the keyword query type into the search box and your why will be some predefined class.",
                    "label": 0
                },
                {
                    "sent": "For example you trying to search a movie or restaurant or something like that and another.",
                    "label": 0
                },
                {
                    "sent": "The second example is one of my favorite.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Furthermore, it can be seen as a estimation problem.",
                    "label": 0
                },
                {
                    "sent": "Our graph, for example, let's say link in the X in this case can be the profile like a user resume and why can be something.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, whether this guy is interested in machine learning job or not, and the next example is it also can be used in information retrieval like image ranking.",
                    "label": 1
                },
                {
                    "sent": "Then your X will be the image and why it will be a ranking score.",
                    "label": 0
                },
                {
                    "sent": "Here is another classification instead like.",
                    "label": 0
                },
                {
                    "sent": "Aggression because this car is a real value function.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so then there is a very popular algorithm to try to solve this problem and the basic idea is trying to smooth function.",
                    "label": 0
                },
                {
                    "sent": "Smooth is means you have.",
                    "label": 0
                },
                {
                    "sent": "If you have similar input X you have a similar output Y and you can either do interpolation or you can do this square.",
                    "label": 0
                },
                {
                    "sent": "So we have a.",
                    "label": 0
                },
                {
                    "sent": "Here we have a weight WXY which describes a similarity between 2 random variable and let's see the out take the interpolation problem.",
                    "label": 0
                },
                {
                    "sent": "Example, if you minimize that object if if two random variables are very similar, then the wait will be large and when you minimize it will penalize the difference between the estimation of those two.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "Optimization problem will implement this idea.",
                    "label": 0
                },
                {
                    "sent": "Similar X will give you a similar Y.",
                    "label": 1
                },
                {
                    "sent": "So this problem can also be seen as a estimation on graphs.",
                    "label": 0
                },
                {
                    "sent": "So then the random variable will be the not in the graph and the similarity weight will be the weight on the edge.",
                    "label": 0
                },
                {
                    "sent": "So you can see there is a image there.",
                    "label": 0
                },
                {
                    "sent": "It's a image digit.",
                    "label": 0
                },
                {
                    "sent": "For example the link in example fit there nicely I think.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see what's the problem of this.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem with this solution.",
                    "label": 0
                },
                {
                    "sent": "The object function can be written in the matrix form, where L is the cloud graph lesson which could hear a moment ago.",
                    "label": 0
                },
                {
                    "sent": "So it is.",
                    "label": 0
                },
                {
                    "sent": "It has a new space which is the constant functions and discuss.",
                    "label": 0
                },
                {
                    "sent": "This metric form is a seminorm.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the setting will study is the following topic setting.",
                    "label": 0
                },
                {
                    "sent": "We fixed the labels set.",
                    "label": 0
                },
                {
                    "sent": "We let the unlabeled point go to Infinity, which is pretty practical because we have a lot more unlabeled data than legal advice.",
                    "label": 0
                },
                {
                    "sent": "So then we N is the total number point if we left and go to Infinity, decrease because it's a differential operator graph labeling.",
                    "label": 0
                },
                {
                    "sent": "I mean you have to shrink the neighborhood in order to get the final final structure data.",
                    "label": 0
                },
                {
                    "sent": "So we have this limit which means this thing converted to a gradient square with the P square density.",
                    "label": 0
                },
                {
                    "sent": "It's a symptom is easily seen from here.",
                    "label": 0
                },
                {
                    "sent": "So then the semi supervised learning, with the infinite unlabeled point and fixed label.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Data will be like this.",
                    "label": 0
                },
                {
                    "sent": "So the smoothness believe idea will be will mean we have a small penalty which would be small seminorm.",
                    "label": 0
                },
                {
                    "sent": "Of course, it's intuitively will see later why is intuitive, so we can use assign function to get the class in binary classification and the analysis applied to the square case.",
                    "label": 0
                },
                {
                    "sent": "Similarly, in fact, we can already see tip of the iceberg here.",
                    "label": 1
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So well.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See what happens next.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in some supply learning all these algorithms are designed to take advantage of those alible data part.",
                    "label": 0
                },
                {
                    "sent": "So natural idea is if you have more and more unlabeled point, your algorithm should have a better and better result, but.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Practice.",
                    "label": 0
                },
                {
                    "sent": "It's kind of contrary to that.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, on the left is this other data unit.",
                    "label": 0
                },
                {
                    "sent": "Uniform distribution of a square.",
                    "label": 0
                },
                {
                    "sent": "So on the left we have a less animal part, so the solution is something like this.",
                    "label": 0
                },
                {
                    "sent": "If you use a site function, you can see from the counter plot it should give you the two label part.",
                    "label": 0
                },
                {
                    "sent": "It give you a pretty good nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "But whether this estimate are smooth or not, it's difficult to see.",
                    "label": 0
                },
                {
                    "sent": "But when we have a more and more valuable part, you can see the estimator converted to indicated function on the labor part.",
                    "label": 0
                },
                {
                    "sent": "So it's flat like 0 if you sign function you don't know it's a plus or minus.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Function I use a code because they are assigned their up to channel sign.",
                    "label": 0
                },
                {
                    "sent": "This means the more unlabeled data you have, the best of the worst classifier guide and the worst result you have.",
                    "label": 1
                },
                {
                    "sent": "This is exactly the opposite of your initial intuition.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's see why this happens.",
                    "label": 0
                },
                {
                    "sent": "Straightforward way is to just check the solution, plug back in right so we can use the first life to approximate the gradient square because P is bounded through that way.",
                    "label": 1
                },
                {
                    "sent": "So and I write Dxi as H small value so this guy is old order H2D2 D is the intrinsic dimension or your domain.",
                    "label": 1
                },
                {
                    "sent": "And if D is greater than two we can see.",
                    "label": 0
                },
                {
                    "sent": "If we although the indicated function have infinite gradient on this point, but the whole integral is 0, which is smallest value you can get.",
                    "label": 0
                },
                {
                    "sent": "So, and you can also use other methods to see a decode two.",
                    "label": 0
                },
                {
                    "sent": "Also this also happened, so this means the indicator function.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The smallest penalty, but yet it's not smooth and the nearby data around the label point have a very different value.",
                    "label": 0
                },
                {
                    "sent": "Rap.",
                    "label": 0
                },
                {
                    "sent": "So then this leads to the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next question, what happened to this function space?",
                    "label": 0
                },
                {
                    "sent": "This is the solution space, right?",
                    "label": 1
                },
                {
                    "sent": "So this needs to take a closer look at this function space.",
                    "label": 0
                },
                {
                    "sent": "Then you see it's called us a blue space, so it's if I like it, includes functions that function themselves and all the partial derivative up to all the M, including M or belong to L2.",
                    "label": 0
                },
                {
                    "sent": "Then it's a subspace of order M. Our solutions based actually only each one is only the 1st order.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Retail.",
                    "label": 0
                },
                {
                    "sent": "So then the solution can be pretty straightforward.",
                    "label": 1
                },
                {
                    "sent": "We just shrink because the solution space is too large, so we just shrink it.",
                    "label": 1
                },
                {
                    "sent": "Hopefully we can get smoother solutions.",
                    "label": 0
                },
                {
                    "sent": "So then the next step will be.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How to implement this algorithm?",
                    "label": 0
                },
                {
                    "sent": "How to compute it?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the original definition of the norms of space used the partial derivatives, and it's very difficult to implement this powder on random sample which are drawn from unknown manifold.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, the boundary condition you just don't know where the boundary is.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But Luckily we have a good way to implement it.",
                    "label": 1
                },
                {
                    "sent": "So because there's a alternative way to describe the smoothness of Sobolev space by the Alto basis, which is the eigenfunctions of Laplacian and we can define this iteration latching signal.",
                    "label": 0
                },
                {
                    "sent": "And use this norm to describe smoothness.",
                    "label": 0
                },
                {
                    "sent": "So ether relax is just you applied operator time and time again.",
                    "label": 0
                },
                {
                    "sent": "And notice the Lambda is increasing.",
                    "label": 1
                },
                {
                    "sent": "Here is a different from the kernel, some a lot of time is decreasing but here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is increasing.",
                    "label": 0
                },
                {
                    "sent": "So there is a relation.",
                    "label": 0
                },
                {
                    "sent": "The interlock lasting Sam Norm space actually is included in the Subspace S. Here is actually can be a real number, but later will only use integer because it's for computational reason.",
                    "label": 0
                },
                {
                    "sent": "So now we change to change describing the space from pointwise.",
                    "label": 1
                },
                {
                    "sent": "Function described by partial derivative to L2 function that described by the eigen functions.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then we arrive at this either last.",
                    "label": 0
                },
                {
                    "sent": "Regularization problem.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "The only change to the existing problem is I used to iterate blessing, which numerically will be the power function or the matrix L which is graph blessing and you can see for fixed function smooth function.",
                    "label": 0
                },
                {
                    "sent": "Let's say let's say C2, you have the following convergence.",
                    "label": 0
                },
                {
                    "sent": "And if you have a nonuniform density, you can use weighted blessing and.",
                    "label": 1
                },
                {
                    "sent": "If you have a boundary, you need some proper boundary conditions there.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's see some toy example.",
                    "label": 1
                },
                {
                    "sent": "Here we have a mixture of two Gaussian which are only separated by the first dimension.",
                    "label": 1
                },
                {
                    "sent": "And then there are in 20 dimension space and all those plot is the projection on the first dimension.",
                    "label": 0
                },
                {
                    "sent": "The left half is plus one, the right half is minus one.",
                    "label": 0
                },
                {
                    "sent": "And then you can see the first one is M = 1 and the problem is exactly what we saw at first.",
                    "label": 0
                },
                {
                    "sent": "If you see the Y axis is a .02.",
                    "label": 0
                },
                {
                    "sent": "If you compare it to the label value is one is essentially zero and depends on how you pick those two label points just to label.",
                    "label": 0
                },
                {
                    "sent": "One for each.",
                    "label": 0
                },
                {
                    "sent": "It can be either up or either down and if we increase M, let's see.",
                    "label": 0
                },
                {
                    "sent": "We can see there are two things happen.",
                    "label": 0
                },
                {
                    "sent": "One is your threshold for sine function shift to the natural threshold which is zero.",
                    "label": 0
                },
                {
                    "sent": "Make your solution stable.",
                    "label": 0
                },
                {
                    "sent": "The second thing happen is you see the Y axis, particularly the last one.",
                    "label": 0
                },
                {
                    "sent": "It's already got 2, two and roughly one for your labor point.",
                    "label": 0
                },
                {
                    "sent": "So this solved that problem pretty good I think.",
                    "label": 0
                },
                {
                    "sent": "Then I see some more examples.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are.",
                    "label": 0
                },
                {
                    "sent": "The first one is a digit images, then there's some text later than the sum gene.",
                    "label": 0
                },
                {
                    "sent": "Later the the first column M = y is the regular problem and.",
                    "label": 0
                },
                {
                    "sent": "The second column is M because four we can see the improvement is across the board and particularly if you consider the simple change we made.",
                    "label": 0
                },
                {
                    "sent": "We also can.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pair to another method, which is a test on the benchmark of this book and in the 11th there is a method here with additional L2 norm.",
                    "label": 0
                },
                {
                    "sent": "I think the author is sitting here and we can see that the first column is the base case and because one and the last column is flashing and the second column is another method which is.",
                    "label": 0
                },
                {
                    "sent": "A little different but related, and the third column is the best wishes.",
                    "label": 0
                },
                {
                    "sent": "The best result from the book among 13 different algorithm.",
                    "label": 1
                },
                {
                    "sent": "We can see the improvement for Italy, Lawson if you compared to the base case, the first one, the improvement is a pretty amazing.",
                    "label": 1
                },
                {
                    "sent": "I think if you compare to the best one, it's a pretty competitive.",
                    "label": 0
                },
                {
                    "sent": "Padilla, if you consider how those 13 algorithms are implemented, little detail here is sometimes people ask whether the additional L2 norm will solve that problem and the answer is no, because L2 function active space is essentially 0.",
                    "label": 0
                },
                {
                    "sent": "And that's and there is a nested structure for solar space.",
                    "label": 0
                },
                {
                    "sent": "The higher the order, the smaller the spaces.",
                    "label": 0
                },
                {
                    "sent": "That means additional output number is already implemented in the graph regularizer, the FLF.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's a quick summary.",
                    "label": 0
                },
                {
                    "sent": "Very specific messages you should use.",
                    "label": 0
                },
                {
                    "sent": "It really blasting in most of the practical practical application because probably you don't.",
                    "label": 0
                },
                {
                    "sent": "You won't have too many 1 dimensional problems.",
                    "label": 0
                },
                {
                    "sent": "And another is.",
                    "label": 0
                },
                {
                    "sent": "A little high level message is a lot of time.",
                    "label": 0
                },
                {
                    "sent": "Intuition is very good for beginning first start something, but here we need actually a lot more, which is careful mathematical analysis for the particular message is the gradient square is not a good thing to measure smoothness in what in higher dimension, higher actually is.",
                    "label": 0
                },
                {
                    "sent": "It only works in one dimensional space, and I'd like to take this opportunity to thank.",
                    "label": 0
                },
                {
                    "sent": "The following people thought DuPont is a professor in my Department and then essence Rebel is a professor.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Across the street from Total Technology Institute at Chicago and both Nadler hits from Wiseman.",
                    "label": 0
                },
                {
                    "sent": "Institute of Science is real.",
                    "label": 0
                },
                {
                    "sent": "Who is across the ocean and at last?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}