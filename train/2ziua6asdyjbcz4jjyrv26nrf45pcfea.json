{
    "id": "2ziua6asdyjbcz4jjyrv26nrf45pcfea",
    "title": "Beyond Seq2Seq with Augmented RNNs",
    "info": {
        "author": [
            "Edward Grefenstette, Google, Inc."
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_grefenstette_augmented_rnn/",
    "segmentation": [
        [
            "Thank you for the invitation.",
            "I went for beers with coming in last night and I'm kind of worried about what I said was going to be my talk because he was like it's promising.",
            "A lot of stuff I talked about.",
            "I was like, oh, I don't remember saying that so.",
            "If you go and look at the proceedings of ACL, which is one of the main natural language processing conferences or MLP, which is another one.",
            "A few years back, you would have seen when word avec LA's hip and everyone was using it.",
            "A bunch of papers in the semantics track on new way of doing word embedding.",
            "So the topic that I personally find really dull.",
            "Now if you look at this year and some of last year LS Tiens or recurrent Nets or attention or deep learning, just generic, they are in many many titles and I think it's fair to say that we've reached a point where using RNS or variance on RN ends to model sequences to model sequences, sequence mappings.",
            "And to learn representations of units of texts of sequences, whole sequences in order to label them or classify or perform interesting and exotic tasks have become one of the dominant paradigms.",
            "In terms of like applications of deep learning to natural language processing, I was thinking what can I talk about?",
            "You could just go to the proceedings of these conferences and read a bunch of dozen great ideas from each conference and just draw inspiration for your own research from that.",
            "Obviously in the previous lectures this week, we've covered a lot of the maths behind RNS.",
            "Some sequences, sequence modeling and some memory, so I thought, OK, I'm not going to revisit that, so I thought to myself, what could I write for this summer school?",
            "That would be interesting for people here, and I thought what would I like to have at this stage in my research when I'm starting my PhD or about to start a PhD or in the early stages of PhD, or even something I'd like to hear now, which is how?",
            "What sort of intuitions guide the selection of a particular model for a particular problem?",
            "What's with intuitions drive us to improve or extend models, and decide how we're going to improve and extend them?",
            "And I'm not going to answer that question because I said that's the open question of research, but I'm going to try and give you some insight into.",
            "My opinions on the matter, my intuitions about the matter, a lot of stuff I'm going to say might be controversial or or intentionally, just a little sort of high level, so don't take it all at face value.",
            "Take it with a grain of salt, but hopefully it will be interesting so."
        ],
        [
            "I'm going to talk a bit about, oh, this is not good start, so today I'm going to talk a bit about the limitations of RNS for standard sequence modeling and sequences secret mapping.",
            "How some of these limitations have been addressed by augmenting them with various forms of memory.",
            "So there will be some topic overlap with whom it talked about.",
            "The other day, I wasn't actually at lecture, so I'm hoping the content overlap.",
            "Won't be too high.",
            "And I'll try and use this to illustrate what sort of at least what my thinking process in designing or analyzing models is, and hopefully that will be somewhat informative when you're working on adapting existing models like RNZ STM's attention to your own problems, and thinking about how to fix, extend improve these models so it will be 6 parts.",
            "I'm going to sort of take a break 1/3 of the way through in 2/3 of the way through.",
            "These are fairly bite size, so don't.",
            "I'm hoping I'll get.",
            "I'll get through all of this.",
            "In the time allotted, I'm going to start by talking about the transduction bottleneck in sequence to sequence mapping, so this will be in relation to the idea of having an encoder in a decoder.",
            "Nothing fancy, no attention why that works in spite of the model moving on.",
            "From that, I'm going to try and talk about the limitations of recurrent neural networks of the slightly higher level, and this will be not so much mathematical and more on the sort of intuitive side on the basis of that, I'm going to revisit the notion of recurrent network not as a specific kind of cell or specific set of functions.",
            "But more as an API for putting a subnetwork structure into.",
            "And I'll talk about how attention stack enhanced darn ends and register machines like neural Turing Machines at API and what's interesting about them with regard to the original limitations."
        ],
        [
            "So the trans."
        ],
        [
            "And bottleneck, I'm going to go quickly through this sort of like background section, since you this is the NTH time you've heard the definition of an RNN, Chris Dyer pointed out that the RNN diagram sort of looks like a little standing man.",
            "So I had a little head.",
            "This is the only joke in the lecture, I promise.",
            "I'm just so proud that I made it looks OK.",
            "So recurrent neural network cells are going to output the distribution over a variety of things like the next symbol in a sequence or label, or in some cases we're not going to actually care about the output and they connect back into themselves.",
            "And the idea here is that this hidden layer that connects back into itself by looping preserves state and then can therefore model the history of the sequence or long range dependencies within a sequence."
        ],
        [
            "Our names are very popular language for obvious reasons because you can unroll them across the sequence to fit basically form a sort of feedforward network.",
            "They dynamically adapt to variable length sequences.",
            "We have a lot of those in language.",
            "We have no way of controlling how long a sentence will be in our data set, how long, how many sentence is there going to be in a dialogue?",
            "How many sentence is going to be in a paragraph or document?",
            "So it's nice to have this nice adaptive capacity.",
            "And they're very popular, because recent research has shown that they can capture fairly long range dependencies.",
            "As Kenyon talked about earlier today."
        ],
        [
            "They have obvious applications in natural language processing and again Ken talked about quite a few of these earlier, so I'm not going to dwell on these too much, but we can model.",
            "We can do language modeling, so we can use the chain rule for probabilities to infer the probability of the sequence.",
            "According to Corpus statistics of the corpus, we can do sequence labeling for online.",
            "So we start reading in symbol by symbol and for each symbol we output a particular label.",
            "For example, if you're doing speech to text.",
            "Can do sentence graphics classification so we can roll unroll an RNN across the sequence and then either take the final state or take new mean pooling.",
            "So averaging across all the outputs in order to get a single representation of that sentence and then classify classifying to sentiment, classifying the topic if you want.",
            "But for a lot of these approaches there are simpler or better models that exist both in traditional statistical sequence modeling and neural networks.",
            "Maybe a little controversial 'cause we've had a lot of recent advances across the last two years, but I still think that Destiny language models are cool.",
            "I still think that log bilinear models are cool, so there are sometimes better, simpler solutions."
        ],
        [
            "But makes earnings really cool.",
            "Is that this idea of state being able to capture dependencies with the history within the history of the sequence extends to being able to capture dependencies outside of the history of the sequence, or put it another way we can condition.",
            "We can condition the output sequence on arbitrary vector representations.",
            "So for example, instead of just if we ignore the red vector and we on the right would be simply unrolling and across the sequence to do, say, language modeling.",
            "If we initialize the start state or."
        ],
        [
            "Feed in an arbitrary vector.",
            "At each input, we can skew the distribution of strings.",
            "That's going to be output by the model towards something conditioned on arbitrary data, such as another language string from another language such as a vector representation of the topic of a document or anything you want.",
            "So in this respect, I actually I agree fully with show that being able to model the likelihood of a sequence, the conditional likelihood of a sequence is kind of a fundamental part of a.",
            "Natural language understanding."
        ],
        [
            "This idea of being."
        ],
        [
            "Able to condition the generation based on arbitrary representations that you might have obtained from another part of your network."
        ],
        [
            "Has led to this idea of doing sequence sequence mapping with recurrent neural networks.",
            "Many natural language processing and other tasks are in fact castable is sequence sequence transduction problems, Cho famously in several lectures as boldly claimed, everything really boils down the translation and this is kind of my version of that outrageous claim.",
            "But translation obviously is an example of English to French transaction.",
            "For example, parsing can be seen this taking a string and then transforming it into a tree.",
            "And if you really want to drink the koolaid, computation itself could be seen as taking input data and mapping that sequence of data into the result of the computation.",
            "And we'll see some examples of that."
        ],
        [
            "More formally, the goal of sequence sequence mapping is to transform vectorial representation of some sort sequence, which might have a different length than your target sequence, and we want to make that into a vector into the target sequence."
        ],
        [
            "This is typically done with R and ends.",
            "We're going to represent the source sequence is a single vector and then model the conditional distribution over the next target token with recurrent neural networks.",
            "So we read in the source in the most naive form, rereading the source sequence produces the vector representation by taking, let's say the final hidden layer.",
            "Then we train a separate RNN or maybe an RN with similar parameters to maximize the likelihood of the target sequence.",
            "Given that conditioning vector and the test time.",
            "We can generate the target we we feed in the source vector and then we can generate the target sequence Gridley, or use beam search or a variety of methods to get better output."
        ],
        [
            "This another way of another way this has been presented as an encoder decoder model, so you can think of.",
            "But this is mostly we've talked about there, sorry, So what?"
        ],
        [
            "The an example of how this works for translation, which again overlaps with shows I won't talk about this too much is going to have an RNN that reads in the words of your French sentence, and maybe you can stack the RNN in order to capture dependencies better or have a more expressive model.",
            "Reason the French words token by token until you reach a delimiting symbol or an end of sequence symbol, and at that point it's going to start out putting the words of the English translation at.",
            "Running time, we're going to feed in the correct output as the next input.",
            "at Test time, we're going to take the argmax for sample from the distribution over words here, and feed that in.",
            "And so that's been quite successful as Alias keyword.",
            "In fact, I'm going to argue that it's successful despite the architecture of this model."
        ],
        [
            "Another cool application again, so again talking just about simple sequence to sequence mapping models with R and ends was this paper by Wojtek Saramba, analysis skier in 2014, where they read simple Python scripts, character by character, and then would output the numerical result of computing that Python script character by character into.",
            "And this when it came out, blew everyone's minds and it is a cool paper.",
            "I mean, it's a cool result, but I mean you obviously have to look into what the data is a bit more carefully.",
            "There short Python scripts.",
            "It's actually a very very restricted subset of Python.",
            "For loops are never nested, and in fact always have this kind of formulation of iterating over a particular range and then incrementing, multiplying in particular variable.",
            "So there, in functional programming parlance, closer to fold or reduce.",
            "But it was very cool that this worked at all.",
            "In fact, when in 2014 I was like this, we've solved AI."
        ],
        [
            "But that sounds really cool, so we thought we can we solve machine translation with this.",
            "Can we solve computation well?",
            "Impressive results notwithstanding, there is an architectural defect in this simple sequence to sequence mapping model with LVM, which I like to call transmission bottleneck.",
            "So if you look at what's happening in this simple case of translation.",
            "You're going to ignore the output of the encoder RNN until you reach some separator symbol, and then you're going to start decoding into English.",
            "Then when you're training the model, you're going to get gradient with regard to the log likelihood or negative log likelihood of the correct token.",
            "That's going to go back into the network and flow back.",
            "This bottleneck goes two ways, so you can think of when you're decoding all the information required to produce the English sentence.",
            "All the information from the French sentence needs to be in the hidden representation at this state.",
            "This is a fixed size and representation, so it means that guess what if you have longer and longer sequences on the encoding end and you have a fixed size, then you're going to compress more and more and you're going to expect increasingly lossy decompression when you're decoding, but it's really the effect on the backward pass, which is the bottleneck gradient that the encoder and in fact these representations are going to get has to flow via the single point on the code or end you're going to get happily get gradient flowing.",
            "Your two sources, one through the forward history and two through the actual predictions, whereas here it has to just go through, possibly crushing, crushing 10 inches.",
            "Or if you have something like a GR, your LTM through a bunch of gates and it's reasonable.",
            "To expect that still be an imperfect transmission of gradient back to the encoder.",
            "What this means in practice is when you're training machine translation model again without attention, without any sort of fancy stuff, the majority of the training is dominated by learning the target sequence language model.",
            "So if you take your model when it's 80% trained because you know how long it's going to rain for an you sample and you sample like what sort of strings are going to come out of it, you'll find that it's typically going to be ignoring the encoder altogether, and it's only towards the later stages of training when gradient has started to propagate to the encoder that the right sort of behavior of conditional.",
            "Generation of the English.",
            "Given the French is going to happen.",
            "So I think this is a fairly intuitive and again if you disagree, we can talk about this later, but I think it's fairly obvious that while it's a cool model, it's translating in spite of the architectural constraints, so we're kind of fighting against a bottleneck that shouldn't be there."
        ],
        [
            "So that's sort of the empirical sort of intuitive explanation of a problem with this very simple approach to sequences, sequence modeling with RNS, but I'd like to talk about the limitations of Narendra more computational perspective."
        ],
        [
            "So who here is taking a class on theoretical computer science?",
            "OK, if you you good good is this is this completely foreign to anyone.",
            "This kind of hierarchy.",
            "Give a very quick explanation.",
            "So in theoretical computer science we have a hierarchy of automatic of like theoretical computing mechanisms that can perform different sort of degrees of complexity of computation or in formal language theory, which has nothing to do or little to do with that actual natural language.",
            "There's an equivalence between automata and classes of languages.",
            "So for example finite state machines where you simply have sort of a set of States and transition matrix between states.",
            "Based on the symbols that you're reading in a string, that's equivalent to the class of regular languages.",
            "These are languages that can be recognized by regular expression.",
            "If you take a step up and you add a stack to your finite state machine, you have a pushdown tamada and that actually allows you to recognize or larger class of formal languages.",
            "Context free grammars, context free languages such as those generated by context free grammars, and we typically think that actual natural languages are formal languages of this or higher and then skipping up a bit at the top of the computational hierarchy are Turing machines, which are sort of one of the fundamental models.",
            "Of modern computation and their equivalence classes, recursively innumerable languages, or in mathematical parlance, all the computable functions or primitive recursive functions of arithmetic.",
            "So those are super powerful, and we want like these are computers.",
            "These are sort of like slightly more crippled computers.",
            "Quick show of hands.",
            "Where do you think RN's are on this kind of hierarchy in terms of what they can be trained to do?",
            "Who thinks finite state machines?",
            "OK, one who thinks pushdown automata.",
            "OK, and who thinks Turing machines?",
            "OK, great, there's been.",
            "There's a bit of no hands up here you guys or is this a uninformed question?",
            "Who is the Turing machine?",
            "OK, so."
        ],
        [
            "In terms of what our names can express, there's this paper by Siegelman, Sontag 1995, that shows that RNS can express Turing machines.",
            "The hopefully vaguely correct sketch of this proof, since I haven't read this paper in five years.",
            "In fact, this might not be exactly their proof, but any Turing machine that's basically a set of states finite set of states finite language for the tape, and does anyone know Turing machines?",
            "I need expanding machine.",
            "So it's controller and it has an infinite tape and you can move over the tape and write symbols and read symbols and conditioning on what symbol you are.",
            "You transition states.",
            "So yeah.",
            "So instead of states a set of symbols that can be written to the tape or present on the tape and the transition function which reads the symbol that's currently at the read head on the tape and decides what action to perform, which might be out putting another single symbol or shifting left and right along the tape, and it's fairly obvious that you can translate this determining deterministically intern RNN which will.",
            "Do the same thing so you take one hot encoding of the hidden layer, one hot encoding of states are going to be the contents of your hidden layer, which will have size Q.",
            "You have one hot encoding of your symbols as input so that the tape is external to the RN.",
            "You have one hot encoding of symbols and left and right as outputs and identity as a recurrence.",
            "And then finally your transition function as an update matrix.",
            "And you can therefore, for any Turing machine, map it into an organ, and by extension most RN's will be able to express at least a set of Turing machines.",
            "It might not be a very interesting sewing machines, but you can if everything goes to something one hot and you have appropriate output functions.",
            "You can say look, there's a turning machine that can be expressed here.",
            "You learn about neural networks.",
            "You hear the function approximation theorem and everyone thinks hurray, we're going to solve everything within your own network.",
            "And obviously it's cool that you can approximate any function with a deep multilayer perceptrons.",
            "That doesn't mean you're going to be able to learn that function, and likewise here expressivity is definitely different than learnability.",
            "So just because an RN can express a class of Turing machines doesn't mean you should expect it to learn those Turing machines, and in fact, I think it would be absurd."
        ],
        [
            "And there's a few reasons why.",
            "Simple RNN's, and this includes Gru, zanele, STM.",
            "By simple, I don't mean trivially simple because there are some of the authors of these models here, but I mean ones not extended with some form of external memory or attention.",
            "I think they cannot learn Turing machines through normal sequence based maximum likelihood training.",
            "The first reason is that our names do not control the tape, so typically the training when you're training these models, you unroll the RNN across the sequence at.",
            "The state is the thing.",
            "You could simulate that there would be.",
            "Machine.",
            "That the memory is part of the bit big current net.",
            "Yeah, I mean so formally a Turing machine was still need an infinite tape, but but OK, I take I take away, I'll get to whether or not near you entering machines are Turing machines and a bit later in this talk I'm not going to prove any of these things like take this.",
            "Take this with a grain of salt.",
            "But if you think of the the tape is being outside the RNN, then we don't control the tape sequence is the sequence has to be exposed in the force order during the during the training, and so the ability of the model to output left right doesn't actually affect what you're going to be exposed to.",
            "Now.",
            "Obviously This is why I say with that little asterisks during normal sequence based training you could look into using reinforcement learning to actually control how the RNN is exposed to the environment, which would be the text, but I won't go into that now even if you think about other ways in which.",
            "It could simulate the tape internally and it's hidden layer.",
            "The fact that we train them with a maximum likelihood objective so that we want to maximize the likelihood of the data or the labels given the input condition.",
            "On some parameters, this produces models that are close to the training data distribution, so they haven't.",
            "Yep.",
            "When you say they can't learn them, do you mean that the because of the maximum likelihood objective they can't?",
            "Or because you need additional assumption?",
            "For example on how the data is collected?",
            "No, I think I think it is.",
            "That would allow this, which I guess was coming up with.",
            "Suggest that there are a set of assumptions under which you can just maybe you're not willing to make these assumptions in practice, yeah?",
            "We have to think about that a bit more, but yeah, I think definitely the way the way we typically collect data and expose data to the RNN doesn't fit the assumptions that I think you'd want in the sketch of that of that earlier proof.",
            "I don't know that I would suggest I don't think there's like a typical way in which we collected.",
            "Yes, yes.",
            "Do you mean if the data is collected under a fixed policy that is not necessarily the one that you wish to evaluate?",
            "I'm sorry I misunderstood your question, so you're suggesting that.",
            "Sometimes the data that we presented on our end with regard to, let's say, classifying it, doesn't actually want to actually induce a Turing machine just because of the set of examples we present to it.",
            "I guess I'm suggesting more broadly, there are cases assumptions under which data is collected in which we may be able to learn it.",
            "There's other assumptions under which we may not OK, I think I have to think about this further, so this isn't this isn't particularly meant to be an authoritative proof.",
            "This is kind of my hand WAVY argument against the proposition that our names are going, so I think.",
            "Maximum like using a maximum likelihood objective is going to produce a model that's very close to the training data distribution, so there.",
            "They're bad at generalizing data simple data despite they do it better than some other models, but they're not going to generalize to the extent of learning a general computation mechanism.",
            "We want to encourage models to fight against the maximum likelihood objective, and generalized by introducing forms of regularization, such as dropout, such as putting an L2 norm on the weights or an L1 norm which encourages waits to be sparse, which would actually bring us closer to what we want for encoding.",
            "Turning machine in an RNN, but I still think it's insane to expect simple regularization mechanisms like that.",
            "To just dump battering machine in your in your model weights magically.",
            "Now if feel free to prove me wrong, but I think it's good to have, like reasonable expectations about what your model is going to learn and disregard magic is.",
            "So I think there's the SM Tadic case.",
            "If you have Internet data computation to train and optimize, and you should be able to learn any kind of Turing machines, supposing that was big enough.",
            "But then of course, in practice we run along with a finite amount of data and finite computation, and so all of these architectural.",
            "Books or constraints that are putting in could work well when the kinds of Turing machine things we use in computer science are appropriate, so this is what has happened in the last couple of years.",
            "And you know more will come, but it's it's more like an encouragement to find solutions that fit this computational model.",
            "Yeah, so the fact that we have limited data not not sort of we don't provide infinite data to our models means yeah, they're going to find an adequate enough solution."
        ],
        [
            "So I'm going to argue and this is again very.",
            "This is an opinion definitely in the realm of opinion that ends when we in practice are just going to learn to approximate, albiet fairly large but finite finite state machines.",
            "This is simply because in a lot of problems I've tried to train, RNN, LCM, Gru, baselines to do something as simple as copying large sequences.",
            "There's absolutely terrible out of sample performance in that you can learn to copy sequences that is never seen before as long as the length of those sequences is within the bounds of what it's seen during training and as soon as you make them a little longer it starts to forget.",
            "So the empirical empirically.",
            "Live from the strong belief that RNS, Ellis teams, and Gru's are just going to approximate large finite state machines with a finite end, so they're effectively modeling order and Markov chain, but unlike N gram models, say you don't need to specify in advance, so they're very flexible in that if you get if you get some larger data that has longer range dependencies is going to be able to extend that end and adapt.",
            "So if indeed you buy, you bite the bullet and buy the RNS or approximating large finite state machines.",
            "This means that in theory memory list, but in final state machines you can simulate memory through dependencies.",
            "So for example, if you take a regex like dot star, A .8 which means ignore a bunch of characters, and then if I at the end of the string I see a followed by three symbols, we don't care which ones and another A then recognize you can translate this into an actual sort of.",
            "Generative model over the last character.",
            "If you feed in the field in the string until you need to predict that last character, and then that will induce a distribution over whether or not the final character should be a given.",
            "That I've seen a for symbols ago, which should be one in this case, so you can capture this very limited bounded form of memory through these dependencies even in finite state machines, and that's what I think we've been seeing in these simple sequence to sequence models that allows them to translate.",
            "But there's no incentive under the maximum likelihood objectives that we train sequence models in sequence to sequence models with simple RNN's.",
            "To learn dependencies beyond the sort and range observed during training and so.",
            "A lot of times, especially foreign networks require sort of inductive bias that forms that that may be able to learn larger dependencies than watcher training in the sense that yeah, sure, it can potentially learn just those dependencies in in the in the range or examine it, but that would mean for the Ireland to ignore it.",
            "To ignore it, it's sort of periodic order and the developers Casa Aficionado Unix system in the sense that.",
            "Imagine, for example, you have a very simple model that's only able to learn rotations, right?",
            "In what you're saying is, yeah, of course you can.",
            "You can potentially learn something a up to one time story, time, story, time there, but if you're running a rotation you keep on posting devastation, you will learn some very recently, which is an arbitrary longer.",
            "Yeah, but typically you'll find even those models they want their ability to like.",
            "Keep the period going and output ABC, ABC, ABC.",
            "It might degrade with time, so you always getting a bit of a bit of noise in the process is never, I think, unless you.",
            "Fire it and it's never perfect.",
            "OK so I take I take your point that it couldn't for some constraint for this is obviously this is a not a proof and be this is what we're talking about in the general case.",
            "I think that in most cases you'll find difficult difficulty learning longer range dependencies than those observed during training.",
            "Yep.",
            "It's true that the system might be recorded, control it because it's very nonlinear and eventually an epsilon of change will degrade overtime but still doesn't meet at the time it needs to be constrained by by the range of your examples.",
            "We could still even, even if it's finite, he could still be way way beyond your registration.",
            "OK, I agree with that certain extent, but I also think that there still there still going to be a limit that's driven by the training examples after we're going to find a sharp drop in degradation of performance.",
            "But again, I'm sure that there are corner cases where we can happily train.",
            "Earnings on fairly synthetic data where they will generalize.",
            "Beyond that, I'm just saying in most cases you will find a pretty severe drop in performance when you extend the length of the sequences, but there's nice generalization within the bounds of that length, which is why I say they're approximating large finite state machines."
        ],
        [
            "So what's wrong with our names approximating large finite state machines?",
            "Well, there's a few issues, both architectural and architectural.",
            "The first is that the RNN is there an end state is acting both as the controller, so determining what state it's in, but also is the memory saying OK.",
            "This is like the history of this is my complex state, right?",
            "This is what I've seen so far.",
            "The longer range dependency there, longer the range of the dependencies you want to store in the RNN or recognize with yarn and the more memory you're going to require, so the larger your representation is going to be.",
            "This should be fairly trivial.",
            "Tracking more dependencies also requires more memory.",
            "So yes, we've shown in like machine reading, I'll give you some examples that you can track very long range dependencies, but one with an RNN happily.",
            "But if you're doing something like copying, copying even fairly short sequences where each symbol has a dependency from the input as a dependency to a point in the output.",
            "As you would expect in machine translation, then you need larger and larger representations.",
            "The more of these dependencies you're tracking.",
            "If you have more complex or more structured dependencies, so you want to output C because you've seen A&B within a particular local dependency with each other, you're going to need more memory and finally, final state machines are pretty basic from a computational perspective, so maybe we can do better."
        ],
        [
            "Bring us back to language for a second and natural language.",
            "As I said earlier, is arguably if you ask linguist.",
            "Depending on which linguist you ask at least context free, which means we need something at least as expressive as a pushdown automata to model the syntax and.",
            "Arguably the semantics of language successfully an across all human languages.",
            "I don't actually believe this proposition, but I'm not going to argue that point today.",
            "I'm going to say simply that even if it's not so, even if natural language to English is a regular language in formal language parlance, rule parsimony matters quite a lot for the intractability of your learning problem.",
            "For your ability, the ability of your model to learn a set of rules.",
            "So here's an example of what I mean by that.",
            "Let's take the smallest.",
            "Well, at least one of the simplest context free languages that will not be recognized by.",
            "Finite state machine, which is A&BN, so it's the set of all strings where you have a number of days and then the same number of these.",
            "If in practice N is bounded by some capital N, so there's only a finite set of strings, then this becomes a regular language.",
            "Again, the regex that recognizes that regular language is actually fairly complex is going to be empty string, AAB or ABB or AAB B, up until a nesan bees.",
            "So you're going to implement rules there.",
            "Context free grammar that will recognize that entire set of strings and then obviously up to the infinite case just has two rules.",
            "So if you're saying OK in practice is bounded, and I don't care if I can recognize longer ones.",
            "You still have a nice incentive for using a context free grammar, because you only need to write down to rolls versus a regular language where you have to write down a large number of them.",
            "If N is large.",
            "So if you think about what your model is expressing what it's actual, practically speaking, expressive capabilities are what would you pick for a problem that is describable as if you had a choice between one that can solve your problem but is going to need to basically internalize N + 1 rules in its weights.",
            "And one that internalizes 2 rules.",
            "Which one do you think is going to be easier to train?",
            "It's obviously the one on the right, so even if your problem is regular, it's worth looking into slightly more expressive models."
        ],
        [
            "So just sort of summarize this point.",
            "Where I'm arguing here and again, this is an opinion, so if you strongly disagree then you definitely you might be completely right, but I'm arguing that we're kind of here with simple RNN.",
            "L. Stimson, Gru's, at the level finite state machines, a lot of interesting problems in computation and natural language understanding probably require us to be somewhere up here if we want to simulate a computer with a neural network as the dream of the neural Turing machine authors, we arguably want to be up here.",
            "So how do we get there?"
        ],
        [
            "We've had quite a few questions, so if you just in the sake of time we can skip to the next session and less someone has extra questions at this point.",
            "OK."
        ],
        [
            "OK, perfect, so I'm going to talk about how we can at least try the step up the step.",
            "Take a step up in the computational hierarchy by augmenting RNS with various forms of memories.",
            "And again, this you'll have had concrete examples of this assumes lecture yesterday.",
            "I'm going to not talk too much about specific models and more about the sort of generic principles behind different forms of memory and leave it to you to read the papers or invent your own within this kind of framework."
        ],
        [
            "I revisit our names.",
            "We typically have seen our nenzel Steam VR uses this kind of beast where it's you have some sort of hidden representation it loose back onto itself.",
            "You feed in vectors and you pop out vectors which might model distributions, and that's definitely a recurrent neural network, but perhaps influenced by the development of libraries like torch, Theano and Tensorflow.",
            "Come to try and see."
        ],
        [
            "At least convince myself that our names are more like a generic kind of API rather than a specific kind of cell, so we can fit several things inside this generic box, but the API is that it's going to be expecting inputs and outputs, and it's going to have a previous data next state.",
            "So in the case of a simple RNN, you'll have your inputs and outputs, and you have your previous and next state, but we're going to talk about in the rest of the."
        ],
        [
            "Lecture about what else you can put sort of more structured representations in here and how that's going to help with having a more expressive."
        ],
        [
            "LCMS.",
            "You could also see as like fitting into, so there's a bunch of this is purposefully complex, but there's a bunch of different wirings that you put to get the gates to work, but at the end of the day they still fit this generic API that you get inputs and outputs previous state in the next day or now, pairs of vectors, so it's a slightly more complex API."
        ],
        [
            "Mathematically, we're going to be talking just generically, as this RNN cell is a function which Maps things from an input domain paired with a previous state domain and produces an output elements of an output domain and next state domain.",
            "The elements of these states can be vectors.",
            "They can be tensors, they can be scalars.",
            "They can be nested or grouped, sets of scalars, and we're going to see a lot of examples about when they're not just simple representations.",
            "Also, typically you're going to find that in most applications that the previous state domain in the next state domain, these are types.",
            "Obviously are going to be the same, so you want your recurrence to always produce the same class of vectors, but I'm going to show you at least one example of where that constraint doesn't necessarily met and everything is fine.",
            "Also, in a lot of cases like language modeling, your input domain in your output domain are going to be the same and you might have some non differentiable function that Maps your outputs back into your inputs, like taking the argmax of a distribution over symbols."
        ],
        [
            "This API needs to satisfy a few constraints at least, and there are a few exceptions for this, but basically we want the gradient with regard to the next state.",
            "The gradient of the next state with regard to the inputs, the gradient of the next state with regard to the previous state, the gradient of the output with regard to the inputs, and with regard to the previous state, all to be well defined to be well defined.",
            "So we want those partial derivatives to be existing."
        ],
        [
            "So this is summarized briefly, but where I'm going with this whole idea of like stepping up into the API, stepping away from individual cells into this generic API, we want this.",
            "We're introducing this kind of framework where you want to basically just keep everything differentiable, use your previous and next.",
            "P inputs on your end outputs to track state within the cell.",
            "You can do whatever you want, and we're going to see some very structured sort of functions for this, because these sort of like see these as modules so you can nest and you can stack them.",
            "That's quite nice and I'm going to show you in the rest of the in the next few slides how we can address some of the issues discussed before in this talk with this API.",
            "By separating within the cell the controller, so whatever is responsible for tracking state and making decisions about what to do with input and output from the memory.",
            "Which is going to get which is going to contain some information pertaining to the history of the sequence for the containing elements or the conditioning."
        ],
        [
            "Illustrating this within this kind of box like framework is we're not going to have a control which might be an RN or an LCM or Gru deal with the input and output.",
            "And we're going to have some interaction with the memory.",
            "And that the form of this interaction is going to be the main difference between attention and tians and stacks.",
            "And then the previous state is now going to be pairs or triples or N tuples that we unpack into the memory in the controller hidden States and then we re pack them into the next state."
        ],
        [
            "So let's start with an illustration of how attention or read only memory in computational parlance fits."
        ],
        [
            "This API and you've had an introduction to attention in one of the previous lectures, right?",
            "Yes, in two minutes.",
            "OK, so this will this will be revisiting some of the things you've seen, but generically attention you can think it attention you can think as read only memory so you have an array of vectors that represent some data.",
            "You have a controller, that's your memory.",
            "You have a controller that deals with the input output logic and you want your controller to be able to read your data array at each time step and consume that data in some form by either using it to update its own internal representation or.",
            "Influence the output of the controller and finally you want to be able to accumulate gradients in your memory, so you want the way in which you interact with this array of vectors to be differentiable in order to pass information back in the form of gradients to the source of those vectors.",
            "So an encoder, for example."
        ],
        [
            "How this fits into this API is there are various ways the sort of early Fusion that is to hear?",
            "Did you come up with the term early Fusion?",
            "Yeah cool.",
            "OK, I like this term so there are various ways in which you can sort of like hook up your memory.",
            "Your read only memory to your controller.",
            "Within this API.",
            "The first simple approach, which is kind of the idea that your controller is going to be conditioned the updates on your controller state are going to be conditioned, not just on the inputs and its previous state, but also.",
            "On some read from the memory.",
            "And then and then from that the controller will decide what the output is."
        ],
        [
            "This kind of this is going to have a lot of Maps in this lecture, but this is as far as I'm going to go as very underspecified mathematics.",
            "But the early Fusion approach is going to have a previous state, which is going to be the previous state of the controller, and some fixed memory which is independent of the time step you're at.",
            "The controller, which might be an LTM is going to take in some inputs and the previous previous state updated representation and produce an output.",
            "The next state is simply going to be the next state of the controller and the memory unchanged, and the inputs in the early Fusion case are going to be concatenating the actual overall inputs to the model to the recurrent cell.",
            "Sorry with some read from the memory, which will be a function of the previous state of the model and.",
            "The memory, so in the early Fusion case, the state of the model at the previous time step is sort of looking ahead saying I'm going to need this information from the memory at the next timestep, fed into my input.",
            "A typical way of modeling attention is to simply take the matrix vector product of.",
            "If you want you want to 1st compute a distribution over the rows of the matrix, so you're going to do that by taking your hidden layer and perhaps transforming it with a linear transform, multiplying that by the matrix.",
            "You can put an nature of sigmoid there if you want you softmax over that to get a distribution over the rows or columns of the matrix depending on if you're doing row major or column major and by multiplying this distribution by the matrix, you simply take the expectation of the rows of the values of the rows of the matrix according to that distribution.",
            "So you're going to get a single vector out of that.",
            "And."
        ],
        [
            "Other option is to do late Fusion, so whereas early Fusion was saying I'm using the memory to update my internal state, Late Fusion is saying OK.",
            "It's like my internal state is going to tell me what to look for in the memory and I'm going to use that to influence or the content of the output vector of the controller."
        ],
        [
            "So how this works mathematically is we're going to the same sort of notion for the previous state and the next state we're going to update those with the controller.",
            "The controller output isn't going to be directly, isn't going to directly serve as the overall output of the overall cell, but instead we're going to use that to the hidden state of the of the controller in order to look at the memory, get a vector out of it, and then compose those through.",
            "You know addition or take the mean or take whatever sort of differentiable function of two vectors that you that you want and use that as the overall output.",
            "Then yeah.",
            "Yes."
        ],
        [
            "So.",
            "How does this fit in with the encoder decoder sort of schema that we've been discussing earlier?",
            "Well, here the encoder is what's going to be responsible for producing the array representations that will serve as a memory.",
            "So for example, for for a variable with sequence, so the source language string of French, for example, you're going to have one vector for token that's going to be a representation of the sequence at that position, you're going to pack those into an attention matrix, and the decoder is going to be a perhaps separately parameterized RNN.",
            "And plus memory model in our framework with the attention matrix produced by the encoder as the memory so great.",
            "Now because we're accumulating during decoding, gradients up the error with regard to the memory.",
            "We can then simply just pass those back to the encoder and those serve as gradients of the error with regard to the encoder."
        ],
        [
            "So this is nice.",
            "Now where the encoder is no longer gradient starved is getting gradients not just through the connection to the decoder.",
            "If you want to pass the final state of the encoder is start state for the decoder, but it has all the gradients coming through the memory.",
            "This allows us to compute soft alignments between sequences, show discussed, and that's nice from a linguistic perspective, because alignments do exist in sequence to sequence data.",
            "In particular transition translation and be able to infer those from the data is powerful augmentation to your model.",
            "We can also use attention to search for information in larger sequences, and I'll give an example of that, and finally memory on the forward pass isn't touched, so once you've produced your memory matrix, you don't have to.",
            "You don't have to update it during the forward pass during decoding, so you can just have that.",
            "It might seem kind of expensive in."
        ],
        [
            "Kind of schema to pass a big block of vectors through all time steps, but this could just be a pointer to a particular matrix.",
            "I don't know how easy that is to implement in tensor flow.",
            "Is Jeff Dean here.",
            "But and Tortoise was super easy to do."
        ],
        [
            "So it's nice."
        ],
        [
            "Sufficient?",
            "And this allows us to directly address this problem that we discussed in the very first part of this talk with this bottleneck, so no longer do we need to just pass gradients through the single vector all the way back into the encoder.",
            "But if you're doing attention at each time step you can see."
        ],
        [
            "That information flows forward through the recurrent state, but also through these very thin arrows I've added here to sort of indicate attention.",
            "And that means that gradient and is no longer just flowing back through the encoder.",
            "So through the decoder states back into the encoder, but also at each point you're actually getting the same benefits in a sense as the decoder was getting, you're getting gradient with regard to the output of the encoder at each time step, so that is like one of the reasons I think attention has been so successful.",
            "And such a nice general versatile."
        ],
        [
            "Model I'm going so talked a bit about the application of attention to translation is just want to illustrate this with a few examples outside of machine translation that are very similar that fit.",
            "Sometimes a sequence sequence modeling, sometimes not textual entailment, is one of the applications where we found that our ends with attention have yielded great empirical improvements over other methods.",
            "If you're not familiar with this task, recognizing textual entailment is you get a pair of sentences and you need to say whether or not there's a contradiction between these sentence is whether there's no connection, or whether there's an entailment relation, and in the simplest case, you don't have to specify the nature of the contradiction or direction of the entailment.",
            "So for example, if your hypothesis is a man is crowd surfing at the concert, that contradicts the man is at a football game that is no relation to the man is drunk, although that's possible.",
            "Sorry there was another crap joke and finally that entails.",
            "The man is at a concert so you need to be able to reduce sentences in classifying to three classes."
        ],
        [
            "We rocked Asheville, who is an Internet deep mind, last year adapted word by word attention mechanism very similar to the one Yoshi's Lab used for machine translation, but here we're not using it for sequence to sequence mapping.",
            "We're reading in a pair of sequences and we're only just classifying based on the final state, so all you care about is this final representation, and then you have a multilayer perceptrons which classifieds them to entailment contradiction or no relation.",
            "But the attention here is serving a very similar role as in machine translation.",
            "It's allowing you to provide alignments between the hypothesis and the premise on a word by word basis, and we can visit."
        ],
        [
            "Place these assignments using similar sort of heat map that you've seen in the machine translation alignment cases would find interesting connections.",
            "So for example, the hypothesis had kids and the premise had young girl and a young boy and it's even though they are discontiguous, had found that look, there's a discontiguous alignment between these two things that matches the entities being mentioned in the premise, and I found this really cool.",
            "Now you always have to take these sort of visualizations with a grain of salt, and that is like you can cherry pick them and.",
            "You know we tried not to, but we have had a bias to produce nice ones.",
            "In the paper we did show examples of failure and this is far too easy to read too much into it, like the expression like seeing the face of your favorite didian burnt toast.",
            "So I always take this with a grain of salt, but I found it very convincing that at least very encouraging that we could capture these sort of structured dependencies with attention rather than straight through the Ardennes layer."
        ],
        [
            "I talked about using attention not just as a method for computing alignments, but for searching possibly very large bit of text condition on like a question.",
            "A small bit of text we produced last year, or about a year and a half ago.",
            "A large scale supervised reading comprehension data set where you'd read a news article and then you get a closed form question where you need to predict what the missing word here was, and we had an optimization strategy which prevented simple degenerate language modeling solution so.",
            "In fact, you wouldn't see Oisin Tymon or Friday or Clarkson, but you'd see random strings that were entity markers, but this was quite."
        ],
        [
            "And you could.",
            "We had a simple attention model which we trained over this to produce representations.",
            "The encoder would read the text and produce representations of each position in the text.",
            "The decoder would read a smaller bit of text, so it was an encoder, encoder, decoder, kind of model decoder.",
            "Decoder would produce another read another small bit of text, which was the question and then classify into the answer.",
            "And again we."
        ],
        [
            "And see heat Maps of a different sort.",
            "We can sort of visualize the strength of the attention over the text conditioned on a particular query, and we see that it checks it doesn't pay attention to a few likely hypothesis, but focuses on the correct one.",
            "In this example was nice because the answering this question.",
            "Required some degree of NFR resolution.",
            "See that things like he referred to the sailor in the text, so that was quite cool again.",
            "Maybe there's a little cherry picking here, who knows, but it's cool to see that there are examples like this where you find complex dependencies in the text so attend."
        ],
        [
            "And is quickly summarized successfully mitigates some of the limitations of the original sequences sequence modeling, so obviously it's being used in every every other paper these days.",
            "Were you wanting to do sequence to sequence modeling, but I hope this provides some intuition as to as to why, what limitations it's overcoming.",
            "It's versatile and adaptable to many problems that fit the sequence to sequence paradigm, and in fact just symptoms classification paradigm.",
            "As we've seen in textual entailment, it can be tailored to specific sorts of process is so you can.",
            "Have kind of esoteric, precisely structured forms of attention, like pointer networks, which instead of producing the expectation of the vectors according to the attention variable, actually produce a distribution over those which your network uses directly as the output.",
            "Helpful for learning a good source of representations, because by propagating gradients back to the encoder, you're forcing the encoder to learn good representations of these position in the text.",
            "But these are very strongly tide to the task at hand, so maybe they're not the sort of generic sort of language representations we'd like to be learning.",
            "On the other hand, this read only kind of model puts a strong onus on the controller during decoding to track what's been read.",
            "So if you're doing iterative attention because you're not changing anything in the attention matrix, the controller needs to learn the logic of saying OK.",
            "If I'm trying to capture the fact I've read this and then I've linked this factor, that fact and I've learned that fact that this fact and now I'm drawing a conclusion.",
            "It needs to remember that it's red fact, one fact 2IN factory, so that does put a lot of pressure on the controller.",
            "That would be nice to alleviate.",
            "Readonly also means that the encoder needs to do a lot of the legwork.",
            "Fortunately, it's getting a lot more gradient than in the standard sequence to sequence set up, but it still needs to learn good sort of index vectors for the decoder to search over, and that may also be a limitation of this type of model."
        ],
        [
            "So that's where 2/3 of the way.",
            "If you are there any questions about what I've talked there or was there so much overlap with some it talked about yesterday that there are no questions?",
            "Good OK cool.",
            "So this brings us doing well with time.",
            "So briefly in the last part of this talk."
        ],
        [
            "To talk about taking a we've talked about so far how attention overcomes this sort of transduction bottleneck.",
            "I talked in the first part of this talk and finish.",
            "I want to talk a bit about overcoming the limitations.",
            "I've.",
            "Hopefully vaguely successfully argued for, although I completely understand why they were just something views in the room that our names are approximating finite state machines and we'd like them to be more expressive.",
            "I know that soon it didn't actually get around to talking about stacks yesterday, so hopefully this will be completely new."
        ],
        [
            "So we want to take a step up in the hierarchy from finite state machine to push down automata, which means that OK, if our RNN is modeling a finite state machine and a finite state machine plus a stack is a pushdown automata, let's just add it."
        ],
        [
            "Check and bring us up to this level."
        ],
        [
            "This is going to bring bring us back to this general schema of the controller memory factorization.",
            "This is going to provide a slight difference with attention where interaction was one way in so far as the information was coming from here to here based on keys or search vectors being produced by the controller.",
            "Now the controller is going to not just read from the memory but also write to it in a structured way as you would with the classical stack."
        ],
        [
            "This interaction is going to basically take this sort of form and illustrated slightly differently.",
            "You're going to have an R&M cell that is read the input from your document and produces output over labels or the next talk over the next symbol, or during decoding is going to read in the previously generated symbol and then produce a distribution over the next symbol, and at each time step it's going to write to the stack by pushing vectors onto it, or popping vectors off.",
            "And it's going to obtain at the next time step of read from the head of the stack."
        ],
        [
            "The stack itself fits the RN API, so this is a case of like nesting RNS, except one of the RNS has no.",
            "This has no parameters, and it's just arithmetic updates forward and backwards, but at each time step is going to be expecting an input, which is going to be a push signal, a pop signal, which are scalars and value vector, which is what the vector being pushed onto the stack is going to be, and it's going to have a state which is going to be separated into a value matrix, which is all the vectors stored on the stack.",
            "And a strength matrix, which is how much each of those vectors is at that time step still on the stack.",
            "So there's a weird sort of disconnect between the mathematical implementation of the stack, which is going to have all the vectors every pushed onto the stack on it, and then that combined with the strength, is going to produce the logical stack, which is actually allowing you to say this is what's actually currently on the stack.",
            "But this is the actual state.",
            "The output is simply going to be a read of the head, so it's going to be a vector with the same dimensions as the values.",
            "Stored on it and there are no free parameters."
        ],
        [
            "How you hook this up into and this is quite well.",
            "I liked with starting to think about our net.",
            "These this RN API is being something you can put a lot of structure into it.",
            "You can start to think about this is wiring up these diagrams like you like.",
            "I did in school with electricity is very cool.",
            "So the way this fits into this overall API of input, previous state, next day output is that the input.",
            "From the previous state, is going to be the previous read from the STACK, which is going to be concatenated with.",
            "The input is going to be the previous hidden layer of your Arden controller.",
            "By using the list again and it's going to be the actual previous state of the stack, which will be patched into the stack.",
            "The RNN is going to based on the previous read and the input at a particular time step output the overall output of the network.",
            "So prediction over the symbols is also going to output the inputs to the neural stack, so a vector.",
            "Put onto it and push and pop signals and then based on that the arithmetic updates in the neural stacker going to police produce deterministically the next read and the next state which will be concatenated with the hidden layer of the RNN to produce your next date.",
            "So it's fairly complex sort of wiring, but this is just and this is not the only way you could wire these two things together, and in fact this is not the only way you could model a stack, but this is serves to illustrate how inside this fairly simple API which previously we used to model A single STM single RNN or Gru.",
            "You can put in very complex arbitrary structure and as long as everything is differentiable, you can unroll this across time and be happy, yes.",
            "Come to think of it, you should keep it.",
            "Or maybe you could get an even more powerful model, but instead of just pushing updates not on here that you would have a separate stack for the weights matrices of the higher net.",
            "Yeah, so you can push our, but what vectors you put on to that?",
            "This is as I said first, this is only one way to implement all illustrate about how the stack works in a second, but I'll list also other stack based approaches, so there definitely there's more than one way to model stack.",
            "There's more than one way to formalize the interaction between the stack and the controller, so if you wanted to model some sort of recursive computational state, so actually model this as a computational stack in the Von Neumann architecture style, you could push at each time step the entire RNN state onto this, and then instead of just concatenating it here, like have a gating mechanism and do super fancy kind of stuff.",
            "Or the weights?",
            "Oh yeah, you could do that too.",
            "I mean, yes, because you could do that differentially.",
            "Yes.",
            "Yes you should.",
            "You should be creating with these things.",
            "You can definitely, definitely a valid way to do this is just this is an overly complex diagram to show that you can put incredibly complex structure within this API and and get sort of slightly more complex computational behavior, and it's up to you how to design it.",
            "As long as you satisfy the constraints that you get differential pathways between the outputs in the states."
        ],
        [
            "Just to quickly illustrate, because I've been talking about a stack without explaining how we can actually make a stack differentiable, it's a classically discrete data structure.",
            "We're going to simulate a discrete stack, or at least approximated discrete stack by allowing the push and pop signals, which are typically discrete operations in the classical stack to be continuous values between zero and one.",
            "And I'll give you an illustration about this.",
            "I'll talk briefly about Facebook's about Julian Michaels approach in a few minutes as well.",
            "So at particular time step, the controller, let's say, wants to push a vector V with a push strength D for down of 0.8.",
            "So you can interpret this.",
            "It's not really probabilistic, but you can interpret this as how certain the controller thinks that vector needs to be pushed to the top of the stack.",
            "So we're simply going to add that vector to our matrix of zeros, so there will be a matrix and a vector of scalars.",
            "Here we're going to logically put this vector onto the stack and associate with.",
            "A strength scalar which is simply going to be the push signal when we read from the stack, we're going to read from the top of the stack, down up until a cumulative strength of 1, right?",
            "So here you have a vector V that has a strength 0.8.",
            "That's all that's on the stack, so may read Vector is simply going to be that vector scaled by its associated strength, so you think here about the about discrete push and pop, where that's either zero or one.",
            "If I pushed it with zero, there'd be a strength of 0.",
            "So then my read would be an empty stack.",
            "Even though this is mathematically on the stack, but not logically so, and if it was a one, my read would be the most recently pushed vector onto the stack.",
            "The scale and the ability to have continuous values here makes everything differentiable, but I talk about about discretization in a second at the next time step we want to pop a bit so the controller says OK.",
            "I want to Pop Zero point with certainty 0.1 an.",
            "After that I'm going to push.",
            "You want to pop before pushing if you want to do these things simultaneously, because otherwise you're destroying information you just pushed onto the stack.",
            "But I want to pop with string 0.1 and so popping just removes just by.",
            "Simple subtraction strength from the head of the stack.",
            "So we take what with the previous head of the stack was and we remove that popping energy to down to 0.7.",
            "Following which I'm going to push the second vector with a strength signal of 0.5.",
            "So now we have this stack of incompletely pushed vectors in the sense because I popped a bit off the previous one and I pushed in part of the vector and I'm going to read down to a cumulative strength of 1 again.",
            "So I'm going to read V2 scaled by Zero point 5.",
            "And to that I'm going to add V1 scaled by Zero point 5 rather than 0.7 because I'm reading only down to the particular unit.",
            "So one is like saying this is what I think is at the head of the stack.",
            "Again, if you want to think about what's going to happen happen if you discretize at inference time.",
            "For example, the push and pop operations, then this.",
            "If this was pushed on with string 0.1 with strength one.",
            "Sorry, you have a strength of 1 associated with like the second vector, and the read would once again.",
            "Just be that second vector and if it was pushed with zero you would ignore it during the read and read through to V1.",
            "And then finally just illustrated removal from the stack, so the third time step, the controller says I want to pop with strength 0.9, so I'm pretty certain I want to remove a vector from the stack.",
            "Again, this is not proper probabilistic partial parlance, so that I use certainty with scare quotes.",
            "So this means I'm going to remove a 0.9 of strength from the head of the stack, so 0.5 goes to zero, which means we have 0.4 left of popping energy.",
            "We remove zero point 4.703.",
            "And finally we push the final vector.",
            "So now this vector here is logically removed from the stack when we read the head of the stack, we're going to read 0.9 V 3, so that's going to dominate the return on my read.",
            "And then we're going to ignore V2 because of the strength of 0 N V1.",
            "You're going to read 0.1 of it left, so here this is almost already discrete operations, simply because the controller has, for example, learn to be very certain about whether it wants to push or pop.",
            "And we're getting the read is almost the same as just reading the top of the discrete on top of the stack.",
            "The push has actually completely removed.",
            "So again, this is a slightly perhaps overly complex way of modeling a stack at inference time.",
            "You may want to discretize the operations if your model hasn't properly learn to push and pop the spring Cedar Point one, which in experiments we've seen that it does for fairly trivial data transduction problems.",
            "But or you could, or you could bite the bullet and just leave it, leave it with the ability to have a sort of.",
            "No, so we basically bound.",
            "We have a relatively on that operation, so you can model.",
            "These are all differentiable operations there piecewise differentiable because you have to put reluz in, but everything is kosher with regard to producing gradients with regard to the push and pop operations given reads."
        ],
        [
            "So we applied this to synthetic transduction tasks.",
            "So these are perfectly trivial problems to solve with one line of Python, we provided a large sequences of symbols from some alphabet, and the task was to read those in and then copy them.",
            "And this is where you see LTM start to fail to generalize quite well, and you can make these sequences very, very long.",
            "It's artificial data, so we can generate as much as we want, which is nice similar problem which is reversing a long sequence.",
            "Again, from an actual scientific perspective, pretty boring problem, but it's a nice test case for exploring the limitations of.",
            "Sequence sequence setting and seeing how adding a stack and can help with that.",
            "We tried slightly more linguistically motivated."
        ],
        [
            "Problem, so this is completely artificial language.",
            "We're just trying to simulate aspects of real language, but I won't go so far as to claim that we're going to solve machine translation with a stack enhanced oranges.",
            "Yet we simulated a subject verb object to subject object, verb reordering.",
            "So that would be, for example, translating English into Japanese.",
            "So we use inversion transduction grammars, which if you don't know what those are, don't worry to generate a bunch of data where everything there was.",
            "Valid latent grammatical mapping between the left sequences in the right sequences and we wanted to see if the stack could model that sort of like treated 3 strand transduction.",
            "We also tried to see if it could model transaction with a bit of contextual shifting, so we simulated a generalist of gendered grammar task like translating.",
            "English into German, where in English we say that regardless of whether we're talking about a man or woman or or something else, and in German, I don't actually speak German, but I'm told that there's these best didar Does anyone here speak German?",
            "OK, so this is simulating English to German translation, but again is not fully translation."
        ],
        [
            "Ann, we got some interesting results out of this.",
            "So for copying in reverse."
        ],
        [
            "We alongside stacks we could model using similar dynamics cues and double ended queues.",
            "So for copying using a Q solves the problem and my solved.",
            "I mean you get 100% accuracy even if you test on sequences that are two times the length of the training sequences.",
            "So general generalize is not within not just within the bounds of the training sequence length, but outside it.",
            "Then something else.",
            "Teams for these tasks completely failed to do for reversal stack solves the problem and double ended queue solve both.",
            "This should be fairly obvious why you get these poor performances here, because if you're copying with a stack, you're pushing all the data onto the stack when you're encoding, and then it comes out in the wrong order and similarly for Q, so it was reassuring to see that you had bad results here because it just backs off to ignoring the stack and using the controller as an LTM.",
            "And for the SVO DESCOVY problems which were there was much less diversity in the boundaries.",
            "We could get all seems to converge after a long time.",
            "But for most of these, the stack, queue, and deque, or at least the queue in the double ended queue, which is one we can push and pop on both ends, could solve them and."
        ],
        [
            "Bringing us back to this idea about rule parsimony affecting how tractable your learning problem is.",
            "This is particularly the street when you look at the training, and in fact even the validation set curves for these models.",
            "So this is you.",
            "See this across all these experiments.",
            "So for example, in the gender conjugation thing, these two things that rocket to the top."
        ],
        [
            "Are the queue in the double ended queue?"
        ],
        [
            "And by exploiting the memory within a few epochs, they shoot up to 100% accuracy because they're learning something similar to just two or three rules that are required to solve the problem.",
            "It's not a fairly artificial problem.",
            "James and the STACK, which for some reason isn't particularly well matched to this particular problem, take a long time to start vaguely approximating with like 9899% accuracy, training, distribution, and then fail to generalize when we make it longer.",
            "So again, this is not a proof, but I think it's a strong sort of empirical nudge towards the fact that if you enhance an RNN with a data structure which allows us to take a step up in the computational hierarchy.",
            "Even if your problem is regular, which these problems weren't that we always had a boundary on the training data, and on the test data.",
            "Able, they're able to learn accurate solutions and general solutions much faster because they're learning just internalizing a small set of rules rather than having to approximate a very large set of rules."
        ],
        [
            "Just like I said, there are different ways to produce a stack, so there's the paper I the model I talked about right here is from our NIPS paper in 2015.",
            "This is another very interesting paper by our module and Thomas Mikolov in 2015, where they simulated stacks and lists, which I think was kind of like double ended queues.",
            "The stack semantics are similar, a bit more lightweight, and that they don't factorize the strength of the values and the actual value is like we did, but have those both contiguous in that when you're.",
            "Pushing you have an array of vectors and you shift those up by the value of your push, your interleaving your state.",
            "Likewise popping is shift down or the other way around, which at training time is fine because it allows everything to be continuous at Test time, is problematic in that if you push with certainty 0.5 and then pop with certainty 0.5, every vector becomes an average of the value before and after, so push and pop are destructive unless you do them with close to.",
            "Perfect certainty, which is why in at inference time they discretize those decisions and you only use the continuous nature of pushing pop to train the model.",
            "And that's actually perfectly valid.",
            "In my mind.",
            "It's like it's a perfectly valid thing to do."
        ],
        [
            "Experiments we didn't need to discretize, but they were very synthetic experiments.",
            "I suspect if we wanted to train a continuous stack of the form I presented on more complex problems, we might also want to discretize and only exploit the continuous nature of the stack during."
        ],
        [
            "But that's completely fine.",
            "There's also an worth mentioning.",
            "It doesn't quite fit this controller memory framework I talked about earlier, but there's a really nice idea of using a pointer over previous states of an STM to simulate a stack in this paper by Chris Dyer, an aisle in 2015.",
            "Really worth reading 'cause there's a very cool experiment applying this to learn shift reduce parsing.",
            "Likewise, experiments in Julian Michaelov, I thought were quite sophisticated compared to the ones I presented.",
            "Very artificial ones I presented.",
            "So read these papers."
        ],
        [
            "So, just to summarize quickly what we've talked about with pushdown atomic, these are decent approximations of classical pushdown automata, so I think that were I'm not.",
            "I think it would be brave to claim that these various stack based mechanisms I just referred to take us completely from a finite state land to push down automata, but I think convincing step towards this, yes.",
            "So before you were arguing that Ascended Alex team couldn't learn to generalize beyond the sequences, it's all in your stack model.",
            "Your controller is an LTM, right?",
            "So if it's only learn to see examples writing to push N times, how can it learn to generalize to 2 * N so you don't need to only use the rule?",
            "You need to learn as much simpler.",
            "You don't need the account if your sample if you're.",
            "Yeah, OK, so so you're arguing that is still going to be a problem for.",
            "So for copying and for reversal as trivial and that it just needs to learn that.",
            "OK, I'm either in this mode which is popping pushing mode where I'm in popping mode, so I'm in my encoder, my decoder and it basically pushes until its season end of sequence symbol and then pops until until generating and sequence symbol.",
            "You're correct that if it needed to track the stack depth within the controller rather than having that be modeled by a register or something like that.",
            "That the model would probably not generalize as well beyond links where it's already learned account, so that that's a valid counterargument to the thesis that this is directly taking us up a class in the computational hierarchy, but I think there's definitely more potential to generalize in the way that we want by being.",
            "Put your bottlenecks.",
            "The license back, yeah yeah, OK, yes, yes, but so.",
            "Yeah, I mean just to bring us back to my point about it doesn't matter.",
            "For still modeling regular languages, if there's just fewer rules to learn, even if it's not going to unboundedly generalize, I still think it's it's it's."
        ],
        [
            "There is evidence that, at least for the simple transaction problems, it's learning a solution in a much quicker, much different way than what the teams are doing, and it's a bit of a sleight of hand to say, look.",
            "It's learning this rather than this.",
            "It might not be learning the general case of this, but it's obviously bringing something to the table."
        ],
        [
            "My question is about the fact that we're forcing ourselves into differentiable approximations of these data structures.",
            "Are we losing something when you do that in terms of expressive power or something else?",
            "Where do you see?",
            "Is this a hurting or it's not a big deal?",
            "Well, I don't think it's a big deal, but the experiments we've done are trivial, and as I'm pointing out here."
        ],
        [
            "Here.",
            "Yeah, I think there's been a little work on applying these architectures, so aside from and that's partly because I've done other stuff personally and I'm sure our model is.",
            "My question was, do you have intuition go wrong because of these sort of smooth stock operation?",
            "OK, so I mean it's like.",
            "When you pick what?"
        ],
        [
            "Structure using.",
            "You're imposing an architectural bias that isn't there in the case of an LC am, so if you pick the wrong data structure.",
            "Then you're going to, you know, suffer.",
            "You can obviously hook up a controller to several data structures that makes a learning problem more confusing and you lose some of maybe the elegance of modeling and push down to directly.",
            "So I feel like we're introducing bias at the architectural level in a way that might be harmful, but I'll be honest, I haven't actually experimented with these beyond this initial paper, enough to form a strong intuition about whether this will come and bite us, and that's the exciting part of research is we need to get around to doing that, yes?"
        ],
        [
            "You just you don't get any.",
            "You don't get any signal.",
            "There's an interesting question and answer that answer.",
            "How do you penalize against popping when there's nothing in it?",
            "So in fact, there is a degenerate sort of training case.",
            "If your models initialized such that the controller aggressively pops early in training so that it's constantly removing stuff from the stack is never going to get a lot of gradient on that pop operation, shifting it away from popping aggressively, which is bad.",
            "So when we started training these models, I don't know if our modeling and Thomas found similar problems.",
            "We found that it's like sometimes if you initialize them randomly, got lucky and it took that beautiful behavior of going straight to one and sometimes it sucked and this is really a case for against automatic differentiation.",
            "So what happened is really just for the paper I had written.",
            "The backward dynamics of the stack in the appendix and I look back upon them.",
            "I thought, OK, why isn't it not learning to populous?",
            "Why is it not learning to use the stack?",
            "And it's fairly obvious from looking at the gradient equations that it's like, OK, it's just.",
            "It's going to reach this like degenerate state where it's never going to get signal, so that led us to formulate an initialization strategy for the stack, which is described in the appendix, which is you simply initialize the bias on popping to minus one, and then that's learned away so that early in the early stages of training the model, the controller is discouraged from not from ignoring the stack, or in another way of putting it is encouraged to pop stuff to push stuff onto the stack, even if it's a wrong decision, because if it pushes stuff on, if it learns to manipulate.",
            "This tool it's given wrong, then it wrongly it moves away from that behavior.",
            "If it was biased against using it in the 1st place, then it will never explore its capabilities.",
            "So there's a nice sort of story there.",
            "I don't know if that answers your question, but I've shoehorned something I wanted to say into it, so that works for me."
        ],
        [
            "So let me very quickly finish.",
            "I got 5 minutes left.",
            "To talk about register machines, so I don't want to end this is neural Turing machines and similar architectures, which I love.",
            "The work I'm not a huge fan of the name, so I like to just think of these registers and this is the random acts."
        ],
        [
            "Memory two attentions read only memory."
        ],
        [
            "An arguably, if you drink the Koolaid, we're talking about being here in the computational hierarchy, but I'm not going to endorse that claim.",
            "I'll leave it to you to think about whether or not, or anywhere near that."
        ],
        [
            "So the structure here is.",
            "Compare this to attention where at each time step the memory would based on a key on a query vector produced by the controller, either at the previous time step where the current time step, the memory would return a vector."
        ],
        [
            "In register based memories, you're going to still have that, so you're going to read the memory in a manner possibly similar to attention, or to a variant on attention.",
            "But at each time step control is also going to update the memory, so the memory is now like dependent on the time step."
        ],
        [
            "The generic sort of setup of this is that the controller, as with attention augmented referral networks, deals with the information the input output from the environment from the text.",
            "The controller also produces distributions over memory registers for the purpose of reading from it a lot attention.",
            "An writing to it, the controller decides what to write an, possibly how much to erase, and finally the controller and memory states are updated using the right, the right operation and the read operation an as well.",
            "As the input from the environment, which allows the recurrent cell produce the output in."
        ],
        [
            "Text 8"
        ],
        [
            "So some underspecified but numerous equations to describe this.",
            "Our previous state is going to be the previous state of the controller.",
            "The previous state of the memory, which is now time dependent and the previous read from the memory.",
            "Although there you could obviously do a late Fusion kind of thing as well.",
            "Your inputs to the control are going to be the concatenation of the inputs to the overall recurrent cell and the read, and again this is just one of many ways to do this.",
            "The controller based on this and its previous state outputs a bunch of stuff, so the overall output to the network.",
            "Vector for the purpose of producing a distribution of a memory for the purpose of reading one for writing, we call these keys, typically value and a hidden layer.",
            "This updated internal state and the values was going to be written.",
            "The read is just going to be some read operation.",
            "For example attention the right is going to be.",
            "For example, you could either have an erase operation, the simplest kind of right I can imagine as you use the right attention from the right key to gate between a previous value of the memory and the vector that you want to write to the memory, and a is a softmax over the positions.",
            "But there are many ways to do this and then assuming that someone talked about the MTM in a bit more detail during his talks that I won't.",
            "I wanna bore you with the finer details."
        ],
        [
            "But there are a lot of extensions you can put into this idea.",
            "We can explore location based addressing where the state is now containing something that we don't actually care about.",
            "The differentiability of it like A1 hot vector over positions in the memory and the controller simply emits a differentiable shift operation.",
            "For example circular convolution.",
            "Allows you to update the position that the read head is happening at each time step.",
            "You can mix location and content based addressing.",
            "You could explore harder dressing using reinforce.",
            "I believe Cho and Joshua have a paper on this and and and other people here I'm in the presence of genius is great and then there are more esoteric heuristic addressing mechanisms, but ideally you want this differential and I I leave you to read those in the literature and upcoming literature.",
            "Other things that can be looked in there, factorizing the key and content factorize."
        ],
        [
            "Factorizing the key addressing mechanism in the memory and is this something you guys did in your paper too?",
            "Yeah, so if you think about the memory being empty and you want to write right and the right key is a vector and you're doing a content based look up on an empty memory, everything is 0.",
            "So by definition there the attention is simply going to return a uniform distribution over all the values in the matrix, and at that point you're just distributing your vector over the entire contents of the matrix, and that doesn't sound very sensible.",
            "So one thing you can do is reserve part of your.",
            "The matrix for either a fixed or learnable prefix schema that you initialize with orthogonality constraints so that each position, even when the memory is empty, has some sort of value that's going to be used for the key in order to be able to select a position.",
            "Anne."
        ],
        [
            "Plenty of plenty of things you can do in this space."
        ],
        [
            "Their relation to actual Turing machines is there, but perhaps a little 10 years.",
            "So here, as you also pointed out earlier, you can think about the RNN cells internalising the tape.",
            "This is definitely the paradigm being espoused here, where part of the tape at least is internalized in the form of the addressable register based memory part of it still outside in the symbols.",
            "You're right, you're reading from, so it's a little weird the controller can control the tape motion Vyas, various mechanisms like content based addressing.",
            "Or, or shifting an index.",
            "You are an end could therefore learn to simply focus on modeling the state transitions, but there's still a weird split between the external and internal step.",
            "Internal tape in the sequence of sequence paradigm that I I don't quite like, so I'm not super convinced that this is going to be modeling and machine.",
            "The most crucial reason why is the number of computational steps in the standard in standard sequence modeling for, uh, neural Turing machine, for example, is tide to the data.",
            "So imagine you're trying to sort with.",
            "Does anyone read that neutering machines paper where they show that it could learn to sort sequences that blew my mind?",
            "Because it's learning a problem that I thought was ON log NON time and so I thought OK that's impossible.",
            "But actually it's probably doing is approximating something like Radix sort which is oh and then has particular bounds on the data.",
            "I can sort if you wanted to learn quick sort with a neural.",
            "You have a bit of a problem in the sequence to sequence setting because the neural Turing machine would need to read in the input symbols and then choose how much time to start, rearranging things internally before it started out putting the sorted sequence.",
            "And currently, unless you explore a latent variable modeling approach to pondering, there's no easy way to do that, so I think there's a bunch of limitations that prevent you from modeling natural Turing machine, and this applies to the stacks as well.",
            "So you can imagine the controller.",
            "With two stacks being capable of doing the copy problem quite well, and that encoder would push everything to one stack, then you'd have any steps where the controller swaps data from one stack to the other, and then you pop everything out and it comes out in the right order.",
            "But the crucial point here is the model would lean.",
            "Need to learn that it needs to think or ponder fourth steps in a data dependent way in order to be able to shift the data around in central representations.",
            "And there's no elegant way to do this in our standard maximum likelihood based training and.",
            "We need to quickly wrap up, so I think it's still unlikely to learn a general algorithm, but the experiments show that there is better generalization on symbolic tasks.",
            "So is it reaching the level of Turing machines?",
            "Probably not, but we're getting some computational benefit out of it."
        ],
        [
            "I can summarize this slide is saying there's there's not many there haven't been many applications of this to natural language understanding, but there's fascinating research to be done here.",
            "So this is kind of the peak."
        ],
        [
            "Research, so I'd like to quickly conclude this talk with some sort of general recommendations.",
            "We've seen various intuitions about examining the limitations and capabilities of different kinds of models around the theme of recurrent cells.",
            "It's easy to design an overly complex model, and it feels very nice and powerful.",
            "I'm going to write the next crazy model that's going to place an STM.",
            "It's rarely worth it unless you actually spend time trying to understand the limits of your current architectures.",
            "Within the context of the problem you're studying, and that's probably one of the most informative ways of developing new models by looking at existing architectures, understanding the limitations in their nature, often better solutions and extensions just pop out of this, and the best example of this for me, this chapters one through three of Felix Curious thesis on LCMS, and I read this.",
            "It starts with an analysis of the vanishing gradient problem, the exploding gradient problem for RN ends, and then you just very clearly shows how an obvious solution is the gating mechanism, and it's very obvious idea.",
            "Once you see it written like that, and I thought that was really beautiful and I'm sure there are many papers by people in this in Montreal who they also have that property, by the way, because there are lot of grad students here and you're typically grumpy when you're reviewing.",
            "When you see a paper and the idea seems really obvious, that's usually the mark that it's a good idea.",
            "That doesn't mean it's a trivial extension to something, so remember that when you're viewing our papers for NIPS.",
            "So I think I think not just about the model and but think about the complexity of the problem you want to solve, and whether the model using, say, a simple RNN is going to be appropriate for that level of complexity.",
            "And then finally just not to be too grumpy about examining complex models.",
            "Don't be afraid to be creative, there are plenty of problems to solve in natural language understanding and in other domains that will require creative new models rather than the models we currently have.",
            "So don't just exploit, but explore thank you."
        ],
        [
            "Much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for the invitation.",
                    "label": 0
                },
                {
                    "sent": "I went for beers with coming in last night and I'm kind of worried about what I said was going to be my talk because he was like it's promising.",
                    "label": 0
                },
                {
                    "sent": "A lot of stuff I talked about.",
                    "label": 0
                },
                {
                    "sent": "I was like, oh, I don't remember saying that so.",
                    "label": 0
                },
                {
                    "sent": "If you go and look at the proceedings of ACL, which is one of the main natural language processing conferences or MLP, which is another one.",
                    "label": 0
                },
                {
                    "sent": "A few years back, you would have seen when word avec LA's hip and everyone was using it.",
                    "label": 0
                },
                {
                    "sent": "A bunch of papers in the semantics track on new way of doing word embedding.",
                    "label": 0
                },
                {
                    "sent": "So the topic that I personally find really dull.",
                    "label": 0
                },
                {
                    "sent": "Now if you look at this year and some of last year LS Tiens or recurrent Nets or attention or deep learning, just generic, they are in many many titles and I think it's fair to say that we've reached a point where using RNS or variance on RN ends to model sequences to model sequences, sequence mappings.",
                    "label": 0
                },
                {
                    "sent": "And to learn representations of units of texts of sequences, whole sequences in order to label them or classify or perform interesting and exotic tasks have become one of the dominant paradigms.",
                    "label": 0
                },
                {
                    "sent": "In terms of like applications of deep learning to natural language processing, I was thinking what can I talk about?",
                    "label": 0
                },
                {
                    "sent": "You could just go to the proceedings of these conferences and read a bunch of dozen great ideas from each conference and just draw inspiration for your own research from that.",
                    "label": 0
                },
                {
                    "sent": "Obviously in the previous lectures this week, we've covered a lot of the maths behind RNS.",
                    "label": 0
                },
                {
                    "sent": "Some sequences, sequence modeling and some memory, so I thought, OK, I'm not going to revisit that, so I thought to myself, what could I write for this summer school?",
                    "label": 0
                },
                {
                    "sent": "That would be interesting for people here, and I thought what would I like to have at this stage in my research when I'm starting my PhD or about to start a PhD or in the early stages of PhD, or even something I'd like to hear now, which is how?",
                    "label": 0
                },
                {
                    "sent": "What sort of intuitions guide the selection of a particular model for a particular problem?",
                    "label": 0
                },
                {
                    "sent": "What's with intuitions drive us to improve or extend models, and decide how we're going to improve and extend them?",
                    "label": 0
                },
                {
                    "sent": "And I'm not going to answer that question because I said that's the open question of research, but I'm going to try and give you some insight into.",
                    "label": 0
                },
                {
                    "sent": "My opinions on the matter, my intuitions about the matter, a lot of stuff I'm going to say might be controversial or or intentionally, just a little sort of high level, so don't take it all at face value.",
                    "label": 0
                },
                {
                    "sent": "Take it with a grain of salt, but hopefully it will be interesting so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to talk a bit about, oh, this is not good start, so today I'm going to talk a bit about the limitations of RNS for standard sequence modeling and sequences secret mapping.",
                    "label": 0
                },
                {
                    "sent": "How some of these limitations have been addressed by augmenting them with various forms of memory.",
                    "label": 0
                },
                {
                    "sent": "So there will be some topic overlap with whom it talked about.",
                    "label": 0
                },
                {
                    "sent": "The other day, I wasn't actually at lecture, so I'm hoping the content overlap.",
                    "label": 0
                },
                {
                    "sent": "Won't be too high.",
                    "label": 0
                },
                {
                    "sent": "And I'll try and use this to illustrate what sort of at least what my thinking process in designing or analyzing models is, and hopefully that will be somewhat informative when you're working on adapting existing models like RNZ STM's attention to your own problems, and thinking about how to fix, extend improve these models so it will be 6 parts.",
                    "label": 0
                },
                {
                    "sent": "I'm going to sort of take a break 1/3 of the way through in 2/3 of the way through.",
                    "label": 0
                },
                {
                    "sent": "These are fairly bite size, so don't.",
                    "label": 0
                },
                {
                    "sent": "I'm hoping I'll get.",
                    "label": 0
                },
                {
                    "sent": "I'll get through all of this.",
                    "label": 0
                },
                {
                    "sent": "In the time allotted, I'm going to start by talking about the transduction bottleneck in sequence to sequence mapping, so this will be in relation to the idea of having an encoder in a decoder.",
                    "label": 1
                },
                {
                    "sent": "Nothing fancy, no attention why that works in spite of the model moving on.",
                    "label": 0
                },
                {
                    "sent": "From that, I'm going to try and talk about the limitations of recurrent neural networks of the slightly higher level, and this will be not so much mathematical and more on the sort of intuitive side on the basis of that, I'm going to revisit the notion of recurrent network not as a specific kind of cell or specific set of functions.",
                    "label": 0
                },
                {
                    "sent": "But more as an API for putting a subnetwork structure into.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk about how attention stack enhanced darn ends and register machines like neural Turing Machines at API and what's interesting about them with regard to the original limitations.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the trans.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And bottleneck, I'm going to go quickly through this sort of like background section, since you this is the NTH time you've heard the definition of an RNN, Chris Dyer pointed out that the RNN diagram sort of looks like a little standing man.",
                    "label": 0
                },
                {
                    "sent": "So I had a little head.",
                    "label": 0
                },
                {
                    "sent": "This is the only joke in the lecture, I promise.",
                    "label": 0
                },
                {
                    "sent": "I'm just so proud that I made it looks OK.",
                    "label": 0
                },
                {
                    "sent": "So recurrent neural network cells are going to output the distribution over a variety of things like the next symbol in a sequence or label, or in some cases we're not going to actually care about the output and they connect back into themselves.",
                    "label": 0
                },
                {
                    "sent": "And the idea here is that this hidden layer that connects back into itself by looping preserves state and then can therefore model the history of the sequence or long range dependencies within a sequence.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our names are very popular language for obvious reasons because you can unroll them across the sequence to fit basically form a sort of feedforward network.",
                    "label": 0
                },
                {
                    "sent": "They dynamically adapt to variable length sequences.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of those in language.",
                    "label": 0
                },
                {
                    "sent": "We have no way of controlling how long a sentence will be in our data set, how long, how many sentence is there going to be in a dialogue?",
                    "label": 0
                },
                {
                    "sent": "How many sentence is going to be in a paragraph or document?",
                    "label": 0
                },
                {
                    "sent": "So it's nice to have this nice adaptive capacity.",
                    "label": 0
                },
                {
                    "sent": "And they're very popular, because recent research has shown that they can capture fairly long range dependencies.",
                    "label": 1
                },
                {
                    "sent": "As Kenyon talked about earlier today.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They have obvious applications in natural language processing and again Ken talked about quite a few of these earlier, so I'm not going to dwell on these too much, but we can model.",
                    "label": 0
                },
                {
                    "sent": "We can do language modeling, so we can use the chain rule for probabilities to infer the probability of the sequence.",
                    "label": 1
                },
                {
                    "sent": "According to Corpus statistics of the corpus, we can do sequence labeling for online.",
                    "label": 0
                },
                {
                    "sent": "So we start reading in symbol by symbol and for each symbol we output a particular label.",
                    "label": 0
                },
                {
                    "sent": "For example, if you're doing speech to text.",
                    "label": 1
                },
                {
                    "sent": "Can do sentence graphics classification so we can roll unroll an RNN across the sequence and then either take the final state or take new mean pooling.",
                    "label": 0
                },
                {
                    "sent": "So averaging across all the outputs in order to get a single representation of that sentence and then classify classifying to sentiment, classifying the topic if you want.",
                    "label": 1
                },
                {
                    "sent": "But for a lot of these approaches there are simpler or better models that exist both in traditional statistical sequence modeling and neural networks.",
                    "label": 0
                },
                {
                    "sent": "Maybe a little controversial 'cause we've had a lot of recent advances across the last two years, but I still think that Destiny language models are cool.",
                    "label": 0
                },
                {
                    "sent": "I still think that log bilinear models are cool, so there are sometimes better, simpler solutions.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But makes earnings really cool.",
                    "label": 0
                },
                {
                    "sent": "Is that this idea of state being able to capture dependencies with the history within the history of the sequence extends to being able to capture dependencies outside of the history of the sequence, or put it another way we can condition.",
                    "label": 0
                },
                {
                    "sent": "We can condition the output sequence on arbitrary vector representations.",
                    "label": 0
                },
                {
                    "sent": "So for example, instead of just if we ignore the red vector and we on the right would be simply unrolling and across the sequence to do, say, language modeling.",
                    "label": 0
                },
                {
                    "sent": "If we initialize the start state or.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Feed in an arbitrary vector.",
                    "label": 0
                },
                {
                    "sent": "At each input, we can skew the distribution of strings.",
                    "label": 0
                },
                {
                    "sent": "That's going to be output by the model towards something conditioned on arbitrary data, such as another language string from another language such as a vector representation of the topic of a document or anything you want.",
                    "label": 0
                },
                {
                    "sent": "So in this respect, I actually I agree fully with show that being able to model the likelihood of a sequence, the conditional likelihood of a sequence is kind of a fundamental part of a.",
                    "label": 0
                },
                {
                    "sent": "Natural language understanding.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This idea of being.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Able to condition the generation based on arbitrary representations that you might have obtained from another part of your network.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Has led to this idea of doing sequence sequence mapping with recurrent neural networks.",
                    "label": 0
                },
                {
                    "sent": "Many natural language processing and other tasks are in fact castable is sequence sequence transduction problems, Cho famously in several lectures as boldly claimed, everything really boils down the translation and this is kind of my version of that outrageous claim.",
                    "label": 1
                },
                {
                    "sent": "But translation obviously is an example of English to French transaction.",
                    "label": 0
                },
                {
                    "sent": "For example, parsing can be seen this taking a string and then transforming it into a tree.",
                    "label": 0
                },
                {
                    "sent": "And if you really want to drink the koolaid, computation itself could be seen as taking input data and mapping that sequence of data into the result of the computation.",
                    "label": 0
                },
                {
                    "sent": "And we'll see some examples of that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More formally, the goal of sequence sequence mapping is to transform vectorial representation of some sort sequence, which might have a different length than your target sequence, and we want to make that into a vector into the target sequence.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is typically done with R and ends.",
                    "label": 0
                },
                {
                    "sent": "We're going to represent the source sequence is a single vector and then model the conditional distribution over the next target token with recurrent neural networks.",
                    "label": 0
                },
                {
                    "sent": "So we read in the source in the most naive form, rereading the source sequence produces the vector representation by taking, let's say the final hidden layer.",
                    "label": 1
                },
                {
                    "sent": "Then we train a separate RNN or maybe an RN with similar parameters to maximize the likelihood of the target sequence.",
                    "label": 1
                },
                {
                    "sent": "Given that conditioning vector and the test time.",
                    "label": 0
                },
                {
                    "sent": "We can generate the target we we feed in the source vector and then we can generate the target sequence Gridley, or use beam search or a variety of methods to get better output.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This another way of another way this has been presented as an encoder decoder model, so you can think of.",
                    "label": 0
                },
                {
                    "sent": "But this is mostly we've talked about there, sorry, So what?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The an example of how this works for translation, which again overlaps with shows I won't talk about this too much is going to have an RNN that reads in the words of your French sentence, and maybe you can stack the RNN in order to capture dependencies better or have a more expressive model.",
                    "label": 0
                },
                {
                    "sent": "Reason the French words token by token until you reach a delimiting symbol or an end of sequence symbol, and at that point it's going to start out putting the words of the English translation at.",
                    "label": 0
                },
                {
                    "sent": "Running time, we're going to feed in the correct output as the next input.",
                    "label": 0
                },
                {
                    "sent": "at Test time, we're going to take the argmax for sample from the distribution over words here, and feed that in.",
                    "label": 0
                },
                {
                    "sent": "And so that's been quite successful as Alias keyword.",
                    "label": 0
                },
                {
                    "sent": "In fact, I'm going to argue that it's successful despite the architecture of this model.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another cool application again, so again talking just about simple sequence to sequence mapping models with R and ends was this paper by Wojtek Saramba, analysis skier in 2014, where they read simple Python scripts, character by character, and then would output the numerical result of computing that Python script character by character into.",
                    "label": 1
                },
                {
                    "sent": "And this when it came out, blew everyone's minds and it is a cool paper.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's a cool result, but I mean you obviously have to look into what the data is a bit more carefully.",
                    "label": 0
                },
                {
                    "sent": "There short Python scripts.",
                    "label": 0
                },
                {
                    "sent": "It's actually a very very restricted subset of Python.",
                    "label": 0
                },
                {
                    "sent": "For loops are never nested, and in fact always have this kind of formulation of iterating over a particular range and then incrementing, multiplying in particular variable.",
                    "label": 0
                },
                {
                    "sent": "So there, in functional programming parlance, closer to fold or reduce.",
                    "label": 0
                },
                {
                    "sent": "But it was very cool that this worked at all.",
                    "label": 0
                },
                {
                    "sent": "In fact, when in 2014 I was like this, we've solved AI.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But that sounds really cool, so we thought we can we solve machine translation with this.",
                    "label": 0
                },
                {
                    "sent": "Can we solve computation well?",
                    "label": 0
                },
                {
                    "sent": "Impressive results notwithstanding, there is an architectural defect in this simple sequence to sequence mapping model with LVM, which I like to call transmission bottleneck.",
                    "label": 0
                },
                {
                    "sent": "So if you look at what's happening in this simple case of translation.",
                    "label": 0
                },
                {
                    "sent": "You're going to ignore the output of the encoder RNN until you reach some separator symbol, and then you're going to start decoding into English.",
                    "label": 0
                },
                {
                    "sent": "Then when you're training the model, you're going to get gradient with regard to the log likelihood or negative log likelihood of the correct token.",
                    "label": 0
                },
                {
                    "sent": "That's going to go back into the network and flow back.",
                    "label": 0
                },
                {
                    "sent": "This bottleneck goes two ways, so you can think of when you're decoding all the information required to produce the English sentence.",
                    "label": 0
                },
                {
                    "sent": "All the information from the French sentence needs to be in the hidden representation at this state.",
                    "label": 0
                },
                {
                    "sent": "This is a fixed size and representation, so it means that guess what if you have longer and longer sequences on the encoding end and you have a fixed size, then you're going to compress more and more and you're going to expect increasingly lossy decompression when you're decoding, but it's really the effect on the backward pass, which is the bottleneck gradient that the encoder and in fact these representations are going to get has to flow via the single point on the code or end you're going to get happily get gradient flowing.",
                    "label": 0
                },
                {
                    "sent": "Your two sources, one through the forward history and two through the actual predictions, whereas here it has to just go through, possibly crushing, crushing 10 inches.",
                    "label": 0
                },
                {
                    "sent": "Or if you have something like a GR, your LTM through a bunch of gates and it's reasonable.",
                    "label": 0
                },
                {
                    "sent": "To expect that still be an imperfect transmission of gradient back to the encoder.",
                    "label": 0
                },
                {
                    "sent": "What this means in practice is when you're training machine translation model again without attention, without any sort of fancy stuff, the majority of the training is dominated by learning the target sequence language model.",
                    "label": 0
                },
                {
                    "sent": "So if you take your model when it's 80% trained because you know how long it's going to rain for an you sample and you sample like what sort of strings are going to come out of it, you'll find that it's typically going to be ignoring the encoder altogether, and it's only towards the later stages of training when gradient has started to propagate to the encoder that the right sort of behavior of conditional.",
                    "label": 0
                },
                {
                    "sent": "Generation of the English.",
                    "label": 0
                },
                {
                    "sent": "Given the French is going to happen.",
                    "label": 0
                },
                {
                    "sent": "So I think this is a fairly intuitive and again if you disagree, we can talk about this later, but I think it's fairly obvious that while it's a cool model, it's translating in spite of the architectural constraints, so we're kind of fighting against a bottleneck that shouldn't be there.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's sort of the empirical sort of intuitive explanation of a problem with this very simple approach to sequences, sequence modeling with RNS, but I'd like to talk about the limitations of Narendra more computational perspective.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So who here is taking a class on theoretical computer science?",
                    "label": 0
                },
                {
                    "sent": "OK, if you you good good is this is this completely foreign to anyone.",
                    "label": 0
                },
                {
                    "sent": "This kind of hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Give a very quick explanation.",
                    "label": 0
                },
                {
                    "sent": "So in theoretical computer science we have a hierarchy of automatic of like theoretical computing mechanisms that can perform different sort of degrees of complexity of computation or in formal language theory, which has nothing to do or little to do with that actual natural language.",
                    "label": 0
                },
                {
                    "sent": "There's an equivalence between automata and classes of languages.",
                    "label": 0
                },
                {
                    "sent": "So for example finite state machines where you simply have sort of a set of States and transition matrix between states.",
                    "label": 0
                },
                {
                    "sent": "Based on the symbols that you're reading in a string, that's equivalent to the class of regular languages.",
                    "label": 0
                },
                {
                    "sent": "These are languages that can be recognized by regular expression.",
                    "label": 0
                },
                {
                    "sent": "If you take a step up and you add a stack to your finite state machine, you have a pushdown tamada and that actually allows you to recognize or larger class of formal languages.",
                    "label": 0
                },
                {
                    "sent": "Context free grammars, context free languages such as those generated by context free grammars, and we typically think that actual natural languages are formal languages of this or higher and then skipping up a bit at the top of the computational hierarchy are Turing machines, which are sort of one of the fundamental models.",
                    "label": 0
                },
                {
                    "sent": "Of modern computation and their equivalence classes, recursively innumerable languages, or in mathematical parlance, all the computable functions or primitive recursive functions of arithmetic.",
                    "label": 0
                },
                {
                    "sent": "So those are super powerful, and we want like these are computers.",
                    "label": 0
                },
                {
                    "sent": "These are sort of like slightly more crippled computers.",
                    "label": 0
                },
                {
                    "sent": "Quick show of hands.",
                    "label": 0
                },
                {
                    "sent": "Where do you think RN's are on this kind of hierarchy in terms of what they can be trained to do?",
                    "label": 0
                },
                {
                    "sent": "Who thinks finite state machines?",
                    "label": 1
                },
                {
                    "sent": "OK, one who thinks pushdown automata.",
                    "label": 0
                },
                {
                    "sent": "OK, and who thinks Turing machines?",
                    "label": 0
                },
                {
                    "sent": "OK, great, there's been.",
                    "label": 0
                },
                {
                    "sent": "There's a bit of no hands up here you guys or is this a uninformed question?",
                    "label": 0
                },
                {
                    "sent": "Who is the Turing machine?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of what our names can express, there's this paper by Siegelman, Sontag 1995, that shows that RNS can express Turing machines.",
                    "label": 0
                },
                {
                    "sent": "The hopefully vaguely correct sketch of this proof, since I haven't read this paper in five years.",
                    "label": 0
                },
                {
                    "sent": "In fact, this might not be exactly their proof, but any Turing machine that's basically a set of states finite set of states finite language for the tape, and does anyone know Turing machines?",
                    "label": 0
                },
                {
                    "sent": "I need expanding machine.",
                    "label": 0
                },
                {
                    "sent": "So it's controller and it has an infinite tape and you can move over the tape and write symbols and read symbols and conditioning on what symbol you are.",
                    "label": 0
                },
                {
                    "sent": "You transition states.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "So instead of states a set of symbols that can be written to the tape or present on the tape and the transition function which reads the symbol that's currently at the read head on the tape and decides what action to perform, which might be out putting another single symbol or shifting left and right along the tape, and it's fairly obvious that you can translate this determining deterministically intern RNN which will.",
                    "label": 0
                },
                {
                    "sent": "Do the same thing so you take one hot encoding of the hidden layer, one hot encoding of states are going to be the contents of your hidden layer, which will have size Q.",
                    "label": 0
                },
                {
                    "sent": "You have one hot encoding of your symbols as input so that the tape is external to the RN.",
                    "label": 0
                },
                {
                    "sent": "You have one hot encoding of symbols and left and right as outputs and identity as a recurrence.",
                    "label": 0
                },
                {
                    "sent": "And then finally your transition function as an update matrix.",
                    "label": 0
                },
                {
                    "sent": "And you can therefore, for any Turing machine, map it into an organ, and by extension most RN's will be able to express at least a set of Turing machines.",
                    "label": 1
                },
                {
                    "sent": "It might not be a very interesting sewing machines, but you can if everything goes to something one hot and you have appropriate output functions.",
                    "label": 0
                },
                {
                    "sent": "You can say look, there's a turning machine that can be expressed here.",
                    "label": 0
                },
                {
                    "sent": "You learn about neural networks.",
                    "label": 0
                },
                {
                    "sent": "You hear the function approximation theorem and everyone thinks hurray, we're going to solve everything within your own network.",
                    "label": 0
                },
                {
                    "sent": "And obviously it's cool that you can approximate any function with a deep multilayer perceptrons.",
                    "label": 0
                },
                {
                    "sent": "That doesn't mean you're going to be able to learn that function, and likewise here expressivity is definitely different than learnability.",
                    "label": 0
                },
                {
                    "sent": "So just because an RN can express a class of Turing machines doesn't mean you should expect it to learn those Turing machines, and in fact, I think it would be absurd.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there's a few reasons why.",
                    "label": 0
                },
                {
                    "sent": "Simple RNN's, and this includes Gru, zanele, STM.",
                    "label": 1
                },
                {
                    "sent": "By simple, I don't mean trivially simple because there are some of the authors of these models here, but I mean ones not extended with some form of external memory or attention.",
                    "label": 0
                },
                {
                    "sent": "I think they cannot learn Turing machines through normal sequence based maximum likelihood training.",
                    "label": 1
                },
                {
                    "sent": "The first reason is that our names do not control the tape, so typically the training when you're training these models, you unroll the RNN across the sequence at.",
                    "label": 0
                },
                {
                    "sent": "The state is the thing.",
                    "label": 0
                },
                {
                    "sent": "You could simulate that there would be.",
                    "label": 0
                },
                {
                    "sent": "Machine.",
                    "label": 0
                },
                {
                    "sent": "That the memory is part of the bit big current net.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean so formally a Turing machine was still need an infinite tape, but but OK, I take I take away, I'll get to whether or not near you entering machines are Turing machines and a bit later in this talk I'm not going to prove any of these things like take this.",
                    "label": 0
                },
                {
                    "sent": "Take this with a grain of salt.",
                    "label": 0
                },
                {
                    "sent": "But if you think of the the tape is being outside the RNN, then we don't control the tape sequence is the sequence has to be exposed in the force order during the during the training, and so the ability of the model to output left right doesn't actually affect what you're going to be exposed to.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Obviously This is why I say with that little asterisks during normal sequence based training you could look into using reinforcement learning to actually control how the RNN is exposed to the environment, which would be the text, but I won't go into that now even if you think about other ways in which.",
                    "label": 0
                },
                {
                    "sent": "It could simulate the tape internally and it's hidden layer.",
                    "label": 0
                },
                {
                    "sent": "The fact that we train them with a maximum likelihood objective so that we want to maximize the likelihood of the data or the labels given the input condition.",
                    "label": 0
                },
                {
                    "sent": "On some parameters, this produces models that are close to the training data distribution, so they haven't.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "When you say they can't learn them, do you mean that the because of the maximum likelihood objective they can't?",
                    "label": 0
                },
                {
                    "sent": "Or because you need additional assumption?",
                    "label": 0
                },
                {
                    "sent": "For example on how the data is collected?",
                    "label": 0
                },
                {
                    "sent": "No, I think I think it is.",
                    "label": 0
                },
                {
                    "sent": "That would allow this, which I guess was coming up with.",
                    "label": 0
                },
                {
                    "sent": "Suggest that there are a set of assumptions under which you can just maybe you're not willing to make these assumptions in practice, yeah?",
                    "label": 0
                },
                {
                    "sent": "We have to think about that a bit more, but yeah, I think definitely the way the way we typically collect data and expose data to the RNN doesn't fit the assumptions that I think you'd want in the sketch of that of that earlier proof.",
                    "label": 0
                },
                {
                    "sent": "I don't know that I would suggest I don't think there's like a typical way in which we collected.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Do you mean if the data is collected under a fixed policy that is not necessarily the one that you wish to evaluate?",
                    "label": 0
                },
                {
                    "sent": "I'm sorry I misunderstood your question, so you're suggesting that.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the data that we presented on our end with regard to, let's say, classifying it, doesn't actually want to actually induce a Turing machine just because of the set of examples we present to it.",
                    "label": 0
                },
                {
                    "sent": "I guess I'm suggesting more broadly, there are cases assumptions under which data is collected in which we may be able to learn it.",
                    "label": 0
                },
                {
                    "sent": "There's other assumptions under which we may not OK, I think I have to think about this further, so this isn't this isn't particularly meant to be an authoritative proof.",
                    "label": 0
                },
                {
                    "sent": "This is kind of my hand WAVY argument against the proposition that our names are going, so I think.",
                    "label": 1
                },
                {
                    "sent": "Maximum like using a maximum likelihood objective is going to produce a model that's very close to the training data distribution, so there.",
                    "label": 0
                },
                {
                    "sent": "They're bad at generalizing data simple data despite they do it better than some other models, but they're not going to generalize to the extent of learning a general computation mechanism.",
                    "label": 0
                },
                {
                    "sent": "We want to encourage models to fight against the maximum likelihood objective, and generalized by introducing forms of regularization, such as dropout, such as putting an L2 norm on the weights or an L1 norm which encourages waits to be sparse, which would actually bring us closer to what we want for encoding.",
                    "label": 0
                },
                {
                    "sent": "Turning machine in an RNN, but I still think it's insane to expect simple regularization mechanisms like that.",
                    "label": 0
                },
                {
                    "sent": "To just dump battering machine in your in your model weights magically.",
                    "label": 0
                },
                {
                    "sent": "Now if feel free to prove me wrong, but I think it's good to have, like reasonable expectations about what your model is going to learn and disregard magic is.",
                    "label": 0
                },
                {
                    "sent": "So I think there's the SM Tadic case.",
                    "label": 0
                },
                {
                    "sent": "If you have Internet data computation to train and optimize, and you should be able to learn any kind of Turing machines, supposing that was big enough.",
                    "label": 0
                },
                {
                    "sent": "But then of course, in practice we run along with a finite amount of data and finite computation, and so all of these architectural.",
                    "label": 0
                },
                {
                    "sent": "Books or constraints that are putting in could work well when the kinds of Turing machine things we use in computer science are appropriate, so this is what has happened in the last couple of years.",
                    "label": 0
                },
                {
                    "sent": "And you know more will come, but it's it's more like an encouragement to find solutions that fit this computational model.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the fact that we have limited data not not sort of we don't provide infinite data to our models means yeah, they're going to find an adequate enough solution.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to argue and this is again very.",
                    "label": 0
                },
                {
                    "sent": "This is an opinion definitely in the realm of opinion that ends when we in practice are just going to learn to approximate, albiet fairly large but finite finite state machines.",
                    "label": 0
                },
                {
                    "sent": "This is simply because in a lot of problems I've tried to train, RNN, LCM, Gru, baselines to do something as simple as copying large sequences.",
                    "label": 0
                },
                {
                    "sent": "There's absolutely terrible out of sample performance in that you can learn to copy sequences that is never seen before as long as the length of those sequences is within the bounds of what it's seen during training and as soon as you make them a little longer it starts to forget.",
                    "label": 0
                },
                {
                    "sent": "So the empirical empirically.",
                    "label": 0
                },
                {
                    "sent": "Live from the strong belief that RNS, Ellis teams, and Gru's are just going to approximate large finite state machines with a finite end, so they're effectively modeling order and Markov chain, but unlike N gram models, say you don't need to specify in advance, so they're very flexible in that if you get if you get some larger data that has longer range dependencies is going to be able to extend that end and adapt.",
                    "label": 0
                },
                {
                    "sent": "So if indeed you buy, you bite the bullet and buy the RNS or approximating large finite state machines.",
                    "label": 0
                },
                {
                    "sent": "This means that in theory memory list, but in final state machines you can simulate memory through dependencies.",
                    "label": 1
                },
                {
                    "sent": "So for example, if you take a regex like dot star, A .8 which means ignore a bunch of characters, and then if I at the end of the string I see a followed by three symbols, we don't care which ones and another A then recognize you can translate this into an actual sort of.",
                    "label": 0
                },
                {
                    "sent": "Generative model over the last character.",
                    "label": 0
                },
                {
                    "sent": "If you feed in the field in the string until you need to predict that last character, and then that will induce a distribution over whether or not the final character should be a given.",
                    "label": 0
                },
                {
                    "sent": "That I've seen a for symbols ago, which should be one in this case, so you can capture this very limited bounded form of memory through these dependencies even in finite state machines, and that's what I think we've been seeing in these simple sequence to sequence models that allows them to translate.",
                    "label": 0
                },
                {
                    "sent": "But there's no incentive under the maximum likelihood objectives that we train sequence models in sequence to sequence models with simple RNN's.",
                    "label": 0
                },
                {
                    "sent": "To learn dependencies beyond the sort and range observed during training and so.",
                    "label": 1
                },
                {
                    "sent": "A lot of times, especially foreign networks require sort of inductive bias that forms that that may be able to learn larger dependencies than watcher training in the sense that yeah, sure, it can potentially learn just those dependencies in in the in the range or examine it, but that would mean for the Ireland to ignore it.",
                    "label": 0
                },
                {
                    "sent": "To ignore it, it's sort of periodic order and the developers Casa Aficionado Unix system in the sense that.",
                    "label": 0
                },
                {
                    "sent": "Imagine, for example, you have a very simple model that's only able to learn rotations, right?",
                    "label": 0
                },
                {
                    "sent": "In what you're saying is, yeah, of course you can.",
                    "label": 0
                },
                {
                    "sent": "You can potentially learn something a up to one time story, time, story, time there, but if you're running a rotation you keep on posting devastation, you will learn some very recently, which is an arbitrary longer.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but typically you'll find even those models they want their ability to like.",
                    "label": 0
                },
                {
                    "sent": "Keep the period going and output ABC, ABC, ABC.",
                    "label": 0
                },
                {
                    "sent": "It might degrade with time, so you always getting a bit of a bit of noise in the process is never, I think, unless you.",
                    "label": 0
                },
                {
                    "sent": "Fire it and it's never perfect.",
                    "label": 0
                },
                {
                    "sent": "OK so I take I take your point that it couldn't for some constraint for this is obviously this is a not a proof and be this is what we're talking about in the general case.",
                    "label": 0
                },
                {
                    "sent": "I think that in most cases you'll find difficult difficulty learning longer range dependencies than those observed during training.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "It's true that the system might be recorded, control it because it's very nonlinear and eventually an epsilon of change will degrade overtime but still doesn't meet at the time it needs to be constrained by by the range of your examples.",
                    "label": 0
                },
                {
                    "sent": "We could still even, even if it's finite, he could still be way way beyond your registration.",
                    "label": 0
                },
                {
                    "sent": "OK, I agree with that certain extent, but I also think that there still there still going to be a limit that's driven by the training examples after we're going to find a sharp drop in degradation of performance.",
                    "label": 0
                },
                {
                    "sent": "But again, I'm sure that there are corner cases where we can happily train.",
                    "label": 0
                },
                {
                    "sent": "Earnings on fairly synthetic data where they will generalize.",
                    "label": 0
                },
                {
                    "sent": "Beyond that, I'm just saying in most cases you will find a pretty severe drop in performance when you extend the length of the sequences, but there's nice generalization within the bounds of that length, which is why I say they're approximating large finite state machines.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's wrong with our names approximating large finite state machines?",
                    "label": 1
                },
                {
                    "sent": "Well, there's a few issues, both architectural and architectural.",
                    "label": 0
                },
                {
                    "sent": "The first is that the RNN is there an end state is acting both as the controller, so determining what state it's in, but also is the memory saying OK.",
                    "label": 0
                },
                {
                    "sent": "This is like the history of this is my complex state, right?",
                    "label": 0
                },
                {
                    "sent": "This is what I've seen so far.",
                    "label": 0
                },
                {
                    "sent": "The longer range dependency there, longer the range of the dependencies you want to store in the RNN or recognize with yarn and the more memory you're going to require, so the larger your representation is going to be.",
                    "label": 0
                },
                {
                    "sent": "This should be fairly trivial.",
                    "label": 0
                },
                {
                    "sent": "Tracking more dependencies also requires more memory.",
                    "label": 1
                },
                {
                    "sent": "So yes, we've shown in like machine reading, I'll give you some examples that you can track very long range dependencies, but one with an RNN happily.",
                    "label": 0
                },
                {
                    "sent": "But if you're doing something like copying, copying even fairly short sequences where each symbol has a dependency from the input as a dependency to a point in the output.",
                    "label": 0
                },
                {
                    "sent": "As you would expect in machine translation, then you need larger and larger representations.",
                    "label": 0
                },
                {
                    "sent": "The more of these dependencies you're tracking.",
                    "label": 0
                },
                {
                    "sent": "If you have more complex or more structured dependencies, so you want to output C because you've seen A&B within a particular local dependency with each other, you're going to need more memory and finally, final state machines are pretty basic from a computational perspective, so maybe we can do better.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bring us back to language for a second and natural language.",
                    "label": 0
                },
                {
                    "sent": "As I said earlier, is arguably if you ask linguist.",
                    "label": 0
                },
                {
                    "sent": "Depending on which linguist you ask at least context free, which means we need something at least as expressive as a pushdown automata to model the syntax and.",
                    "label": 0
                },
                {
                    "sent": "Arguably the semantics of language successfully an across all human languages.",
                    "label": 0
                },
                {
                    "sent": "I don't actually believe this proposition, but I'm not going to argue that point today.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say simply that even if it's not so, even if natural language to English is a regular language in formal language parlance, rule parsimony matters quite a lot for the intractability of your learning problem.",
                    "label": 1
                },
                {
                    "sent": "For your ability, the ability of your model to learn a set of rules.",
                    "label": 0
                },
                {
                    "sent": "So here's an example of what I mean by that.",
                    "label": 0
                },
                {
                    "sent": "Let's take the smallest.",
                    "label": 0
                },
                {
                    "sent": "Well, at least one of the simplest context free languages that will not be recognized by.",
                    "label": 0
                },
                {
                    "sent": "Finite state machine, which is A&BN, so it's the set of all strings where you have a number of days and then the same number of these.",
                    "label": 1
                },
                {
                    "sent": "If in practice N is bounded by some capital N, so there's only a finite set of strings, then this becomes a regular language.",
                    "label": 0
                },
                {
                    "sent": "Again, the regex that recognizes that regular language is actually fairly complex is going to be empty string, AAB or ABB or AAB B, up until a nesan bees.",
                    "label": 0
                },
                {
                    "sent": "So you're going to implement rules there.",
                    "label": 0
                },
                {
                    "sent": "Context free grammar that will recognize that entire set of strings and then obviously up to the infinite case just has two rules.",
                    "label": 0
                },
                {
                    "sent": "So if you're saying OK in practice is bounded, and I don't care if I can recognize longer ones.",
                    "label": 0
                },
                {
                    "sent": "You still have a nice incentive for using a context free grammar, because you only need to write down to rolls versus a regular language where you have to write down a large number of them.",
                    "label": 0
                },
                {
                    "sent": "If N is large.",
                    "label": 0
                },
                {
                    "sent": "So if you think about what your model is expressing what it's actual, practically speaking, expressive capabilities are what would you pick for a problem that is describable as if you had a choice between one that can solve your problem but is going to need to basically internalize N + 1 rules in its weights.",
                    "label": 0
                },
                {
                    "sent": "And one that internalizes 2 rules.",
                    "label": 0
                },
                {
                    "sent": "Which one do you think is going to be easier to train?",
                    "label": 0
                },
                {
                    "sent": "It's obviously the one on the right, so even if your problem is regular, it's worth looking into slightly more expressive models.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just sort of summarize this point.",
                    "label": 0
                },
                {
                    "sent": "Where I'm arguing here and again, this is an opinion, so if you strongly disagree then you definitely you might be completely right, but I'm arguing that we're kind of here with simple RNN.",
                    "label": 0
                },
                {
                    "sent": "L. Stimson, Gru's, at the level finite state machines, a lot of interesting problems in computation and natural language understanding probably require us to be somewhere up here if we want to simulate a computer with a neural network as the dream of the neural Turing machine authors, we arguably want to be up here.",
                    "label": 1
                },
                {
                    "sent": "So how do we get there?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've had quite a few questions, so if you just in the sake of time we can skip to the next session and less someone has extra questions at this point.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, perfect, so I'm going to talk about how we can at least try the step up the step.",
                    "label": 0
                },
                {
                    "sent": "Take a step up in the computational hierarchy by augmenting RNS with various forms of memories.",
                    "label": 0
                },
                {
                    "sent": "And again, this you'll have had concrete examples of this assumes lecture yesterday.",
                    "label": 0
                },
                {
                    "sent": "I'm going to not talk too much about specific models and more about the sort of generic principles behind different forms of memory and leave it to you to read the papers or invent your own within this kind of framework.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I revisit our names.",
                    "label": 0
                },
                {
                    "sent": "We typically have seen our nenzel Steam VR uses this kind of beast where it's you have some sort of hidden representation it loose back onto itself.",
                    "label": 0
                },
                {
                    "sent": "You feed in vectors and you pop out vectors which might model distributions, and that's definitely a recurrent neural network, but perhaps influenced by the development of libraries like torch, Theano and Tensorflow.",
                    "label": 0
                },
                {
                    "sent": "Come to try and see.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At least convince myself that our names are more like a generic kind of API rather than a specific kind of cell, so we can fit several things inside this generic box, but the API is that it's going to be expecting inputs and outputs, and it's going to have a previous data next state.",
                    "label": 0
                },
                {
                    "sent": "So in the case of a simple RNN, you'll have your inputs and outputs, and you have your previous and next state, but we're going to talk about in the rest of the.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lecture about what else you can put sort of more structured representations in here and how that's going to help with having a more expressive.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "LCMS.",
                    "label": 0
                },
                {
                    "sent": "You could also see as like fitting into, so there's a bunch of this is purposefully complex, but there's a bunch of different wirings that you put to get the gates to work, but at the end of the day they still fit this generic API that you get inputs and outputs previous state in the next day or now, pairs of vectors, so it's a slightly more complex API.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mathematically, we're going to be talking just generically, as this RNN cell is a function which Maps things from an input domain paired with a previous state domain and produces an output elements of an output domain and next state domain.",
                    "label": 1
                },
                {
                    "sent": "The elements of these states can be vectors.",
                    "label": 0
                },
                {
                    "sent": "They can be tensors, they can be scalars.",
                    "label": 1
                },
                {
                    "sent": "They can be nested or grouped, sets of scalars, and we're going to see a lot of examples about when they're not just simple representations.",
                    "label": 1
                },
                {
                    "sent": "Also, typically you're going to find that in most applications that the previous state domain in the next state domain, these are types.",
                    "label": 0
                },
                {
                    "sent": "Obviously are going to be the same, so you want your recurrence to always produce the same class of vectors, but I'm going to show you at least one example of where that constraint doesn't necessarily met and everything is fine.",
                    "label": 0
                },
                {
                    "sent": "Also, in a lot of cases like language modeling, your input domain in your output domain are going to be the same and you might have some non differentiable function that Maps your outputs back into your inputs, like taking the argmax of a distribution over symbols.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This API needs to satisfy a few constraints at least, and there are a few exceptions for this, but basically we want the gradient with regard to the next state.",
                    "label": 0
                },
                {
                    "sent": "The gradient of the next state with regard to the inputs, the gradient of the next state with regard to the previous state, the gradient of the output with regard to the inputs, and with regard to the previous state, all to be well defined to be well defined.",
                    "label": 0
                },
                {
                    "sent": "So we want those partial derivatives to be existing.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is summarized briefly, but where I'm going with this whole idea of like stepping up into the API, stepping away from individual cells into this generic API, we want this.",
                    "label": 0
                },
                {
                    "sent": "We're introducing this kind of framework where you want to basically just keep everything differentiable, use your previous and next.",
                    "label": 0
                },
                {
                    "sent": "P inputs on your end outputs to track state within the cell.",
                    "label": 1
                },
                {
                    "sent": "You can do whatever you want, and we're going to see some very structured sort of functions for this, because these sort of like see these as modules so you can nest and you can stack them.",
                    "label": 0
                },
                {
                    "sent": "That's quite nice and I'm going to show you in the rest of the in the next few slides how we can address some of the issues discussed before in this talk with this API.",
                    "label": 1
                },
                {
                    "sent": "By separating within the cell the controller, so whatever is responsible for tracking state and making decisions about what to do with input and output from the memory.",
                    "label": 0
                },
                {
                    "sent": "Which is going to get which is going to contain some information pertaining to the history of the sequence for the containing elements or the conditioning.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Illustrating this within this kind of box like framework is we're not going to have a control which might be an RN or an LCM or Gru deal with the input and output.",
                    "label": 0
                },
                {
                    "sent": "And we're going to have some interaction with the memory.",
                    "label": 0
                },
                {
                    "sent": "And that the form of this interaction is going to be the main difference between attention and tians and stacks.",
                    "label": 0
                },
                {
                    "sent": "And then the previous state is now going to be pairs or triples or N tuples that we unpack into the memory in the controller hidden States and then we re pack them into the next state.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start with an illustration of how attention or read only memory in computational parlance fits.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This API and you've had an introduction to attention in one of the previous lectures, right?",
                    "label": 0
                },
                {
                    "sent": "Yes, in two minutes.",
                    "label": 0
                },
                {
                    "sent": "OK, so this will this will be revisiting some of the things you've seen, but generically attention you can think it attention you can think as read only memory so you have an array of vectors that represent some data.",
                    "label": 0
                },
                {
                    "sent": "You have a controller, that's your memory.",
                    "label": 1
                },
                {
                    "sent": "You have a controller that deals with the input output logic and you want your controller to be able to read your data array at each time step and consume that data in some form by either using it to update its own internal representation or.",
                    "label": 1
                },
                {
                    "sent": "Influence the output of the controller and finally you want to be able to accumulate gradients in your memory, so you want the way in which you interact with this array of vectors to be differentiable in order to pass information back in the form of gradients to the source of those vectors.",
                    "label": 0
                },
                {
                    "sent": "So an encoder, for example.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How this fits into this API is there are various ways the sort of early Fusion that is to hear?",
                    "label": 0
                },
                {
                    "sent": "Did you come up with the term early Fusion?",
                    "label": 1
                },
                {
                    "sent": "Yeah cool.",
                    "label": 0
                },
                {
                    "sent": "OK, I like this term so there are various ways in which you can sort of like hook up your memory.",
                    "label": 0
                },
                {
                    "sent": "Your read only memory to your controller.",
                    "label": 0
                },
                {
                    "sent": "Within this API.",
                    "label": 0
                },
                {
                    "sent": "The first simple approach, which is kind of the idea that your controller is going to be conditioned the updates on your controller state are going to be conditioned, not just on the inputs and its previous state, but also.",
                    "label": 0
                },
                {
                    "sent": "On some read from the memory.",
                    "label": 0
                },
                {
                    "sent": "And then and then from that the controller will decide what the output is.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This kind of this is going to have a lot of Maps in this lecture, but this is as far as I'm going to go as very underspecified mathematics.",
                    "label": 0
                },
                {
                    "sent": "But the early Fusion approach is going to have a previous state, which is going to be the previous state of the controller, and some fixed memory which is independent of the time step you're at.",
                    "label": 0
                },
                {
                    "sent": "The controller, which might be an LTM is going to take in some inputs and the previous previous state updated representation and produce an output.",
                    "label": 0
                },
                {
                    "sent": "The next state is simply going to be the next state of the controller and the memory unchanged, and the inputs in the early Fusion case are going to be concatenating the actual overall inputs to the model to the recurrent cell.",
                    "label": 0
                },
                {
                    "sent": "Sorry with some read from the memory, which will be a function of the previous state of the model and.",
                    "label": 0
                },
                {
                    "sent": "The memory, so in the early Fusion case, the state of the model at the previous time step is sort of looking ahead saying I'm going to need this information from the memory at the next timestep, fed into my input.",
                    "label": 0
                },
                {
                    "sent": "A typical way of modeling attention is to simply take the matrix vector product of.",
                    "label": 0
                },
                {
                    "sent": "If you want you want to 1st compute a distribution over the rows of the matrix, so you're going to do that by taking your hidden layer and perhaps transforming it with a linear transform, multiplying that by the matrix.",
                    "label": 1
                },
                {
                    "sent": "You can put an nature of sigmoid there if you want you softmax over that to get a distribution over the rows or columns of the matrix depending on if you're doing row major or column major and by multiplying this distribution by the matrix, you simply take the expectation of the rows of the values of the rows of the matrix according to that distribution.",
                    "label": 0
                },
                {
                    "sent": "So you're going to get a single vector out of that.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other option is to do late Fusion, so whereas early Fusion was saying I'm using the memory to update my internal state, Late Fusion is saying OK.",
                    "label": 0
                },
                {
                    "sent": "It's like my internal state is going to tell me what to look for in the memory and I'm going to use that to influence or the content of the output vector of the controller.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how this works mathematically is we're going to the same sort of notion for the previous state and the next state we're going to update those with the controller.",
                    "label": 0
                },
                {
                    "sent": "The controller output isn't going to be directly, isn't going to directly serve as the overall output of the overall cell, but instead we're going to use that to the hidden state of the of the controller in order to look at the memory, get a vector out of it, and then compose those through.",
                    "label": 0
                },
                {
                    "sent": "You know addition or take the mean or take whatever sort of differentiable function of two vectors that you that you want and use that as the overall output.",
                    "label": 0
                },
                {
                    "sent": "Then yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How does this fit in with the encoder decoder sort of schema that we've been discussing earlier?",
                    "label": 0
                },
                {
                    "sent": "Well, here the encoder is what's going to be responsible for producing the array representations that will serve as a memory.",
                    "label": 0
                },
                {
                    "sent": "So for example, for for a variable with sequence, so the source language string of French, for example, you're going to have one vector for token that's going to be a representation of the sequence at that position, you're going to pack those into an attention matrix, and the decoder is going to be a perhaps separately parameterized RNN.",
                    "label": 1
                },
                {
                    "sent": "And plus memory model in our framework with the attention matrix produced by the encoder as the memory so great.",
                    "label": 0
                },
                {
                    "sent": "Now because we're accumulating during decoding, gradients up the error with regard to the memory.",
                    "label": 0
                },
                {
                    "sent": "We can then simply just pass those back to the encoder and those serve as gradients of the error with regard to the encoder.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is nice.",
                    "label": 0
                },
                {
                    "sent": "Now where the encoder is no longer gradient starved is getting gradients not just through the connection to the decoder.",
                    "label": 0
                },
                {
                    "sent": "If you want to pass the final state of the encoder is start state for the decoder, but it has all the gradients coming through the memory.",
                    "label": 0
                },
                {
                    "sent": "This allows us to compute soft alignments between sequences, show discussed, and that's nice from a linguistic perspective, because alignments do exist in sequence to sequence data.",
                    "label": 1
                },
                {
                    "sent": "In particular transition translation and be able to infer those from the data is powerful augmentation to your model.",
                    "label": 0
                },
                {
                    "sent": "We can also use attention to search for information in larger sequences, and I'll give an example of that, and finally memory on the forward pass isn't touched, so once you've produced your memory matrix, you don't have to.",
                    "label": 1
                },
                {
                    "sent": "You don't have to update it during the forward pass during decoding, so you can just have that.",
                    "label": 0
                },
                {
                    "sent": "It might seem kind of expensive in.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of schema to pass a big block of vectors through all time steps, but this could just be a pointer to a particular matrix.",
                    "label": 0
                },
                {
                    "sent": "I don't know how easy that is to implement in tensor flow.",
                    "label": 0
                },
                {
                    "sent": "Is Jeff Dean here.",
                    "label": 0
                },
                {
                    "sent": "But and Tortoise was super easy to do.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's nice.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sufficient?",
                    "label": 0
                },
                {
                    "sent": "And this allows us to directly address this problem that we discussed in the very first part of this talk with this bottleneck, so no longer do we need to just pass gradients through the single vector all the way back into the encoder.",
                    "label": 0
                },
                {
                    "sent": "But if you're doing attention at each time step you can see.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That information flows forward through the recurrent state, but also through these very thin arrows I've added here to sort of indicate attention.",
                    "label": 0
                },
                {
                    "sent": "And that means that gradient and is no longer just flowing back through the encoder.",
                    "label": 0
                },
                {
                    "sent": "So through the decoder states back into the encoder, but also at each point you're actually getting the same benefits in a sense as the decoder was getting, you're getting gradient with regard to the output of the encoder at each time step, so that is like one of the reasons I think attention has been so successful.",
                    "label": 0
                },
                {
                    "sent": "And such a nice general versatile.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model I'm going so talked a bit about the application of attention to translation is just want to illustrate this with a few examples outside of machine translation that are very similar that fit.",
                    "label": 0
                },
                {
                    "sent": "Sometimes a sequence sequence modeling, sometimes not textual entailment, is one of the applications where we found that our ends with attention have yielded great empirical improvements over other methods.",
                    "label": 0
                },
                {
                    "sent": "If you're not familiar with this task, recognizing textual entailment is you get a pair of sentences and you need to say whether or not there's a contradiction between these sentence is whether there's no connection, or whether there's an entailment relation, and in the simplest case, you don't have to specify the nature of the contradiction or direction of the entailment.",
                    "label": 0
                },
                {
                    "sent": "So for example, if your hypothesis is a man is crowd surfing at the concert, that contradicts the man is at a football game that is no relation to the man is drunk, although that's possible.",
                    "label": 1
                },
                {
                    "sent": "Sorry there was another crap joke and finally that entails.",
                    "label": 1
                },
                {
                    "sent": "The man is at a concert so you need to be able to reduce sentences in classifying to three classes.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We rocked Asheville, who is an Internet deep mind, last year adapted word by word attention mechanism very similar to the one Yoshi's Lab used for machine translation, but here we're not using it for sequence to sequence mapping.",
                    "label": 0
                },
                {
                    "sent": "We're reading in a pair of sequences and we're only just classifying based on the final state, so all you care about is this final representation, and then you have a multilayer perceptrons which classifieds them to entailment contradiction or no relation.",
                    "label": 0
                },
                {
                    "sent": "But the attention here is serving a very similar role as in machine translation.",
                    "label": 0
                },
                {
                    "sent": "It's allowing you to provide alignments between the hypothesis and the premise on a word by word basis, and we can visit.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Place these assignments using similar sort of heat map that you've seen in the machine translation alignment cases would find interesting connections.",
                    "label": 0
                },
                {
                    "sent": "So for example, the hypothesis had kids and the premise had young girl and a young boy and it's even though they are discontiguous, had found that look, there's a discontiguous alignment between these two things that matches the entities being mentioned in the premise, and I found this really cool.",
                    "label": 0
                },
                {
                    "sent": "Now you always have to take these sort of visualizations with a grain of salt, and that is like you can cherry pick them and.",
                    "label": 0
                },
                {
                    "sent": "You know we tried not to, but we have had a bias to produce nice ones.",
                    "label": 0
                },
                {
                    "sent": "In the paper we did show examples of failure and this is far too easy to read too much into it, like the expression like seeing the face of your favorite didian burnt toast.",
                    "label": 0
                },
                {
                    "sent": "So I always take this with a grain of salt, but I found it very convincing that at least very encouraging that we could capture these sort of structured dependencies with attention rather than straight through the Ardennes layer.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I talked about using attention not just as a method for computing alignments, but for searching possibly very large bit of text condition on like a question.",
                    "label": 0
                },
                {
                    "sent": "A small bit of text we produced last year, or about a year and a half ago.",
                    "label": 0
                },
                {
                    "sent": "A large scale supervised reading comprehension data set where you'd read a news article and then you get a closed form question where you need to predict what the missing word here was, and we had an optimization strategy which prevented simple degenerate language modeling solution so.",
                    "label": 0
                },
                {
                    "sent": "In fact, you wouldn't see Oisin Tymon or Friday or Clarkson, but you'd see random strings that were entity markers, but this was quite.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you could.",
                    "label": 0
                },
                {
                    "sent": "We had a simple attention model which we trained over this to produce representations.",
                    "label": 0
                },
                {
                    "sent": "The encoder would read the text and produce representations of each position in the text.",
                    "label": 0
                },
                {
                    "sent": "The decoder would read a smaller bit of text, so it was an encoder, encoder, decoder, kind of model decoder.",
                    "label": 0
                },
                {
                    "sent": "Decoder would produce another read another small bit of text, which was the question and then classify into the answer.",
                    "label": 0
                },
                {
                    "sent": "And again we.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And see heat Maps of a different sort.",
                    "label": 0
                },
                {
                    "sent": "We can sort of visualize the strength of the attention over the text conditioned on a particular query, and we see that it checks it doesn't pay attention to a few likely hypothesis, but focuses on the correct one.",
                    "label": 0
                },
                {
                    "sent": "In this example was nice because the answering this question.",
                    "label": 0
                },
                {
                    "sent": "Required some degree of NFR resolution.",
                    "label": 0
                },
                {
                    "sent": "See that things like he referred to the sailor in the text, so that was quite cool again.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's a little cherry picking here, who knows, but it's cool to see that there are examples like this where you find complex dependencies in the text so attend.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And is quickly summarized successfully mitigates some of the limitations of the original sequences sequence modeling, so obviously it's being used in every every other paper these days.",
                    "label": 0
                },
                {
                    "sent": "Were you wanting to do sequence to sequence modeling, but I hope this provides some intuition as to as to why, what limitations it's overcoming.",
                    "label": 0
                },
                {
                    "sent": "It's versatile and adaptable to many problems that fit the sequence to sequence paradigm, and in fact just symptoms classification paradigm.",
                    "label": 0
                },
                {
                    "sent": "As we've seen in textual entailment, it can be tailored to specific sorts of process is so you can.",
                    "label": 1
                },
                {
                    "sent": "Have kind of esoteric, precisely structured forms of attention, like pointer networks, which instead of producing the expectation of the vectors according to the attention variable, actually produce a distribution over those which your network uses directly as the output.",
                    "label": 0
                },
                {
                    "sent": "Helpful for learning a good source of representations, because by propagating gradients back to the encoder, you're forcing the encoder to learn good representations of these position in the text.",
                    "label": 0
                },
                {
                    "sent": "But these are very strongly tide to the task at hand, so maybe they're not the sort of generic sort of language representations we'd like to be learning.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, this read only kind of model puts a strong onus on the controller during decoding to track what's been read.",
                    "label": 0
                },
                {
                    "sent": "So if you're doing iterative attention because you're not changing anything in the attention matrix, the controller needs to learn the logic of saying OK.",
                    "label": 0
                },
                {
                    "sent": "If I'm trying to capture the fact I've read this and then I've linked this factor, that fact and I've learned that fact that this fact and now I'm drawing a conclusion.",
                    "label": 0
                },
                {
                    "sent": "It needs to remember that it's red fact, one fact 2IN factory, so that does put a lot of pressure on the controller.",
                    "label": 0
                },
                {
                    "sent": "That would be nice to alleviate.",
                    "label": 0
                },
                {
                    "sent": "Readonly also means that the encoder needs to do a lot of the legwork.",
                    "label": 1
                },
                {
                    "sent": "Fortunately, it's getting a lot more gradient than in the standard sequence to sequence set up, but it still needs to learn good sort of index vectors for the decoder to search over, and that may also be a limitation of this type of model.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's where 2/3 of the way.",
                    "label": 0
                },
                {
                    "sent": "If you are there any questions about what I've talked there or was there so much overlap with some it talked about yesterday that there are no questions?",
                    "label": 0
                },
                {
                    "sent": "Good OK cool.",
                    "label": 0
                },
                {
                    "sent": "So this brings us doing well with time.",
                    "label": 0
                },
                {
                    "sent": "So briefly in the last part of this talk.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To talk about taking a we've talked about so far how attention overcomes this sort of transduction bottleneck.",
                    "label": 0
                },
                {
                    "sent": "I talked in the first part of this talk and finish.",
                    "label": 0
                },
                {
                    "sent": "I want to talk a bit about overcoming the limitations.",
                    "label": 0
                },
                {
                    "sent": "I've.",
                    "label": 0
                },
                {
                    "sent": "Hopefully vaguely successfully argued for, although I completely understand why they were just something views in the room that our names are approximating finite state machines and we'd like them to be more expressive.",
                    "label": 1
                },
                {
                    "sent": "I know that soon it didn't actually get around to talking about stacks yesterday, so hopefully this will be completely new.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we want to take a step up in the hierarchy from finite state machine to push down automata, which means that OK, if our RNN is modeling a finite state machine and a finite state machine plus a stack is a pushdown automata, let's just add it.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Check and bring us up to this level.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is going to bring bring us back to this general schema of the controller memory factorization.",
                    "label": 0
                },
                {
                    "sent": "This is going to provide a slight difference with attention where interaction was one way in so far as the information was coming from here to here based on keys or search vectors being produced by the controller.",
                    "label": 0
                },
                {
                    "sent": "Now the controller is going to not just read from the memory but also write to it in a structured way as you would with the classical stack.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This interaction is going to basically take this sort of form and illustrated slightly differently.",
                    "label": 0
                },
                {
                    "sent": "You're going to have an R&M cell that is read the input from your document and produces output over labels or the next talk over the next symbol, or during decoding is going to read in the previously generated symbol and then produce a distribution over the next symbol, and at each time step it's going to write to the stack by pushing vectors onto it, or popping vectors off.",
                    "label": 0
                },
                {
                    "sent": "And it's going to obtain at the next time step of read from the head of the stack.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The stack itself fits the RN API, so this is a case of like nesting RNS, except one of the RNS has no.",
                    "label": 0
                },
                {
                    "sent": "This has no parameters, and it's just arithmetic updates forward and backwards, but at each time step is going to be expecting an input, which is going to be a push signal, a pop signal, which are scalars and value vector, which is what the vector being pushed onto the stack is going to be, and it's going to have a state which is going to be separated into a value matrix, which is all the vectors stored on the stack.",
                    "label": 0
                },
                {
                    "sent": "And a strength matrix, which is how much each of those vectors is at that time step still on the stack.",
                    "label": 0
                },
                {
                    "sent": "So there's a weird sort of disconnect between the mathematical implementation of the stack, which is going to have all the vectors every pushed onto the stack on it, and then that combined with the strength, is going to produce the logical stack, which is actually allowing you to say this is what's actually currently on the stack.",
                    "label": 0
                },
                {
                    "sent": "But this is the actual state.",
                    "label": 0
                },
                {
                    "sent": "The output is simply going to be a read of the head, so it's going to be a vector with the same dimensions as the values.",
                    "label": 0
                },
                {
                    "sent": "Stored on it and there are no free parameters.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How you hook this up into and this is quite well.",
                    "label": 0
                },
                {
                    "sent": "I liked with starting to think about our net.",
                    "label": 0
                },
                {
                    "sent": "These this RN API is being something you can put a lot of structure into it.",
                    "label": 0
                },
                {
                    "sent": "You can start to think about this is wiring up these diagrams like you like.",
                    "label": 0
                },
                {
                    "sent": "I did in school with electricity is very cool.",
                    "label": 0
                },
                {
                    "sent": "So the way this fits into this overall API of input, previous state, next day output is that the input.",
                    "label": 0
                },
                {
                    "sent": "From the previous state, is going to be the previous read from the STACK, which is going to be concatenated with.",
                    "label": 0
                },
                {
                    "sent": "The input is going to be the previous hidden layer of your Arden controller.",
                    "label": 0
                },
                {
                    "sent": "By using the list again and it's going to be the actual previous state of the stack, which will be patched into the stack.",
                    "label": 0
                },
                {
                    "sent": "The RNN is going to based on the previous read and the input at a particular time step output the overall output of the network.",
                    "label": 0
                },
                {
                    "sent": "So prediction over the symbols is also going to output the inputs to the neural stack, so a vector.",
                    "label": 0
                },
                {
                    "sent": "Put onto it and push and pop signals and then based on that the arithmetic updates in the neural stacker going to police produce deterministically the next read and the next state which will be concatenated with the hidden layer of the RNN to produce your next date.",
                    "label": 0
                },
                {
                    "sent": "So it's fairly complex sort of wiring, but this is just and this is not the only way you could wire these two things together, and in fact this is not the only way you could model a stack, but this is serves to illustrate how inside this fairly simple API which previously we used to model A single STM single RNN or Gru.",
                    "label": 0
                },
                {
                    "sent": "You can put in very complex arbitrary structure and as long as everything is differentiable, you can unroll this across time and be happy, yes.",
                    "label": 0
                },
                {
                    "sent": "Come to think of it, you should keep it.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you could get an even more powerful model, but instead of just pushing updates not on here that you would have a separate stack for the weights matrices of the higher net.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you can push our, but what vectors you put on to that?",
                    "label": 0
                },
                {
                    "sent": "This is as I said first, this is only one way to implement all illustrate about how the stack works in a second, but I'll list also other stack based approaches, so there definitely there's more than one way to model stack.",
                    "label": 0
                },
                {
                    "sent": "There's more than one way to formalize the interaction between the stack and the controller, so if you wanted to model some sort of recursive computational state, so actually model this as a computational stack in the Von Neumann architecture style, you could push at each time step the entire RNN state onto this, and then instead of just concatenating it here, like have a gating mechanism and do super fancy kind of stuff.",
                    "label": 0
                },
                {
                    "sent": "Or the weights?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, you could do that too.",
                    "label": 0
                },
                {
                    "sent": "I mean, yes, because you could do that differentially.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes you should.",
                    "label": 0
                },
                {
                    "sent": "You should be creating with these things.",
                    "label": 0
                },
                {
                    "sent": "You can definitely, definitely a valid way to do this is just this is an overly complex diagram to show that you can put incredibly complex structure within this API and and get sort of slightly more complex computational behavior, and it's up to you how to design it.",
                    "label": 0
                },
                {
                    "sent": "As long as you satisfy the constraints that you get differential pathways between the outputs in the states.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to quickly illustrate, because I've been talking about a stack without explaining how we can actually make a stack differentiable, it's a classically discrete data structure.",
                    "label": 0
                },
                {
                    "sent": "We're going to simulate a discrete stack, or at least approximated discrete stack by allowing the push and pop signals, which are typically discrete operations in the classical stack to be continuous values between zero and one.",
                    "label": 0
                },
                {
                    "sent": "And I'll give you an illustration about this.",
                    "label": 0
                },
                {
                    "sent": "I'll talk briefly about Facebook's about Julian Michaels approach in a few minutes as well.",
                    "label": 0
                },
                {
                    "sent": "So at particular time step, the controller, let's say, wants to push a vector V with a push strength D for down of 0.8.",
                    "label": 0
                },
                {
                    "sent": "So you can interpret this.",
                    "label": 0
                },
                {
                    "sent": "It's not really probabilistic, but you can interpret this as how certain the controller thinks that vector needs to be pushed to the top of the stack.",
                    "label": 0
                },
                {
                    "sent": "So we're simply going to add that vector to our matrix of zeros, so there will be a matrix and a vector of scalars.",
                    "label": 0
                },
                {
                    "sent": "Here we're going to logically put this vector onto the stack and associate with.",
                    "label": 0
                },
                {
                    "sent": "A strength scalar which is simply going to be the push signal when we read from the stack, we're going to read from the top of the stack, down up until a cumulative strength of 1, right?",
                    "label": 0
                },
                {
                    "sent": "So here you have a vector V that has a strength 0.8.",
                    "label": 0
                },
                {
                    "sent": "That's all that's on the stack, so may read Vector is simply going to be that vector scaled by its associated strength, so you think here about the about discrete push and pop, where that's either zero or one.",
                    "label": 0
                },
                {
                    "sent": "If I pushed it with zero, there'd be a strength of 0.",
                    "label": 0
                },
                {
                    "sent": "So then my read would be an empty stack.",
                    "label": 0
                },
                {
                    "sent": "Even though this is mathematically on the stack, but not logically so, and if it was a one, my read would be the most recently pushed vector onto the stack.",
                    "label": 0
                },
                {
                    "sent": "The scale and the ability to have continuous values here makes everything differentiable, but I talk about about discretization in a second at the next time step we want to pop a bit so the controller says OK.",
                    "label": 0
                },
                {
                    "sent": "I want to Pop Zero point with certainty 0.1 an.",
                    "label": 0
                },
                {
                    "sent": "After that I'm going to push.",
                    "label": 0
                },
                {
                    "sent": "You want to pop before pushing if you want to do these things simultaneously, because otherwise you're destroying information you just pushed onto the stack.",
                    "label": 0
                },
                {
                    "sent": "But I want to pop with string 0.1 and so popping just removes just by.",
                    "label": 0
                },
                {
                    "sent": "Simple subtraction strength from the head of the stack.",
                    "label": 0
                },
                {
                    "sent": "So we take what with the previous head of the stack was and we remove that popping energy to down to 0.7.",
                    "label": 0
                },
                {
                    "sent": "Following which I'm going to push the second vector with a strength signal of 0.5.",
                    "label": 0
                },
                {
                    "sent": "So now we have this stack of incompletely pushed vectors in the sense because I popped a bit off the previous one and I pushed in part of the vector and I'm going to read down to a cumulative strength of 1 again.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to read V2 scaled by Zero point 5.",
                    "label": 0
                },
                {
                    "sent": "And to that I'm going to add V1 scaled by Zero point 5 rather than 0.7 because I'm reading only down to the particular unit.",
                    "label": 0
                },
                {
                    "sent": "So one is like saying this is what I think is at the head of the stack.",
                    "label": 0
                },
                {
                    "sent": "Again, if you want to think about what's going to happen happen if you discretize at inference time.",
                    "label": 0
                },
                {
                    "sent": "For example, the push and pop operations, then this.",
                    "label": 0
                },
                {
                    "sent": "If this was pushed on with string 0.1 with strength one.",
                    "label": 0
                },
                {
                    "sent": "Sorry, you have a strength of 1 associated with like the second vector, and the read would once again.",
                    "label": 0
                },
                {
                    "sent": "Just be that second vector and if it was pushed with zero you would ignore it during the read and read through to V1.",
                    "label": 0
                },
                {
                    "sent": "And then finally just illustrated removal from the stack, so the third time step, the controller says I want to pop with strength 0.9, so I'm pretty certain I want to remove a vector from the stack.",
                    "label": 0
                },
                {
                    "sent": "Again, this is not proper probabilistic partial parlance, so that I use certainty with scare quotes.",
                    "label": 0
                },
                {
                    "sent": "So this means I'm going to remove a 0.9 of strength from the head of the stack, so 0.5 goes to zero, which means we have 0.4 left of popping energy.",
                    "label": 0
                },
                {
                    "sent": "We remove zero point 4.703.",
                    "label": 0
                },
                {
                    "sent": "And finally we push the final vector.",
                    "label": 0
                },
                {
                    "sent": "So now this vector here is logically removed from the stack when we read the head of the stack, we're going to read 0.9 V 3, so that's going to dominate the return on my read.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to ignore V2 because of the strength of 0 N V1.",
                    "label": 0
                },
                {
                    "sent": "You're going to read 0.1 of it left, so here this is almost already discrete operations, simply because the controller has, for example, learn to be very certain about whether it wants to push or pop.",
                    "label": 0
                },
                {
                    "sent": "And we're getting the read is almost the same as just reading the top of the discrete on top of the stack.",
                    "label": 0
                },
                {
                    "sent": "The push has actually completely removed.",
                    "label": 0
                },
                {
                    "sent": "So again, this is a slightly perhaps overly complex way of modeling a stack at inference time.",
                    "label": 0
                },
                {
                    "sent": "You may want to discretize the operations if your model hasn't properly learn to push and pop the spring Cedar Point one, which in experiments we've seen that it does for fairly trivial data transduction problems.",
                    "label": 0
                },
                {
                    "sent": "But or you could, or you could bite the bullet and just leave it, leave it with the ability to have a sort of.",
                    "label": 0
                },
                {
                    "sent": "No, so we basically bound.",
                    "label": 0
                },
                {
                    "sent": "We have a relatively on that operation, so you can model.",
                    "label": 0
                },
                {
                    "sent": "These are all differentiable operations there piecewise differentiable because you have to put reluz in, but everything is kosher with regard to producing gradients with regard to the push and pop operations given reads.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we applied this to synthetic transduction tasks.",
                    "label": 1
                },
                {
                    "sent": "So these are perfectly trivial problems to solve with one line of Python, we provided a large sequences of symbols from some alphabet, and the task was to read those in and then copy them.",
                    "label": 0
                },
                {
                    "sent": "And this is where you see LTM start to fail to generalize quite well, and you can make these sequences very, very long.",
                    "label": 0
                },
                {
                    "sent": "It's artificial data, so we can generate as much as we want, which is nice similar problem which is reversing a long sequence.",
                    "label": 0
                },
                {
                    "sent": "Again, from an actual scientific perspective, pretty boring problem, but it's a nice test case for exploring the limitations of.",
                    "label": 0
                },
                {
                    "sent": "Sequence sequence setting and seeing how adding a stack and can help with that.",
                    "label": 0
                },
                {
                    "sent": "We tried slightly more linguistically motivated.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem, so this is completely artificial language.",
                    "label": 0
                },
                {
                    "sent": "We're just trying to simulate aspects of real language, but I won't go so far as to claim that we're going to solve machine translation with a stack enhanced oranges.",
                    "label": 0
                },
                {
                    "sent": "Yet we simulated a subject verb object to subject object, verb reordering.",
                    "label": 0
                },
                {
                    "sent": "So that would be, for example, translating English into Japanese.",
                    "label": 0
                },
                {
                    "sent": "So we use inversion transduction grammars, which if you don't know what those are, don't worry to generate a bunch of data where everything there was.",
                    "label": 0
                },
                {
                    "sent": "Valid latent grammatical mapping between the left sequences in the right sequences and we wanted to see if the stack could model that sort of like treated 3 strand transduction.",
                    "label": 0
                },
                {
                    "sent": "We also tried to see if it could model transaction with a bit of contextual shifting, so we simulated a generalist of gendered grammar task like translating.",
                    "label": 0
                },
                {
                    "sent": "English into German, where in English we say that regardless of whether we're talking about a man or woman or or something else, and in German, I don't actually speak German, but I'm told that there's these best didar Does anyone here speak German?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is simulating English to German translation, but again is not fully translation.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ann, we got some interesting results out of this.",
                    "label": 0
                },
                {
                    "sent": "So for copying in reverse.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We alongside stacks we could model using similar dynamics cues and double ended queues.",
                    "label": 0
                },
                {
                    "sent": "So for copying using a Q solves the problem and my solved.",
                    "label": 0
                },
                {
                    "sent": "I mean you get 100% accuracy even if you test on sequences that are two times the length of the training sequences.",
                    "label": 0
                },
                {
                    "sent": "So general generalize is not within not just within the bounds of the training sequence length, but outside it.",
                    "label": 0
                },
                {
                    "sent": "Then something else.",
                    "label": 0
                },
                {
                    "sent": "Teams for these tasks completely failed to do for reversal stack solves the problem and double ended queue solve both.",
                    "label": 0
                },
                {
                    "sent": "This should be fairly obvious why you get these poor performances here, because if you're copying with a stack, you're pushing all the data onto the stack when you're encoding, and then it comes out in the wrong order and similarly for Q, so it was reassuring to see that you had bad results here because it just backs off to ignoring the stack and using the controller as an LTM.",
                    "label": 0
                },
                {
                    "sent": "And for the SVO DESCOVY problems which were there was much less diversity in the boundaries.",
                    "label": 0
                },
                {
                    "sent": "We could get all seems to converge after a long time.",
                    "label": 0
                },
                {
                    "sent": "But for most of these, the stack, queue, and deque, or at least the queue in the double ended queue, which is one we can push and pop on both ends, could solve them and.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bringing us back to this idea about rule parsimony affecting how tractable your learning problem is.",
                    "label": 0
                },
                {
                    "sent": "This is particularly the street when you look at the training, and in fact even the validation set curves for these models.",
                    "label": 0
                },
                {
                    "sent": "So this is you.",
                    "label": 0
                },
                {
                    "sent": "See this across all these experiments.",
                    "label": 0
                },
                {
                    "sent": "So for example, in the gender conjugation thing, these two things that rocket to the top.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are the queue in the double ended queue?",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And by exploiting the memory within a few epochs, they shoot up to 100% accuracy because they're learning something similar to just two or three rules that are required to solve the problem.",
                    "label": 0
                },
                {
                    "sent": "It's not a fairly artificial problem.",
                    "label": 0
                },
                {
                    "sent": "James and the STACK, which for some reason isn't particularly well matched to this particular problem, take a long time to start vaguely approximating with like 9899% accuracy, training, distribution, and then fail to generalize when we make it longer.",
                    "label": 0
                },
                {
                    "sent": "So again, this is not a proof, but I think it's a strong sort of empirical nudge towards the fact that if you enhance an RNN with a data structure which allows us to take a step up in the computational hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Even if your problem is regular, which these problems weren't that we always had a boundary on the training data, and on the test data.",
                    "label": 0
                },
                {
                    "sent": "Able, they're able to learn accurate solutions and general solutions much faster because they're learning just internalizing a small set of rules rather than having to approximate a very large set of rules.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just like I said, there are different ways to produce a stack, so there's the paper I the model I talked about right here is from our NIPS paper in 2015.",
                    "label": 0
                },
                {
                    "sent": "This is another very interesting paper by our module and Thomas Mikolov in 2015, where they simulated stacks and lists, which I think was kind of like double ended queues.",
                    "label": 1
                },
                {
                    "sent": "The stack semantics are similar, a bit more lightweight, and that they don't factorize the strength of the values and the actual value is like we did, but have those both contiguous in that when you're.",
                    "label": 0
                },
                {
                    "sent": "Pushing you have an array of vectors and you shift those up by the value of your push, your interleaving your state.",
                    "label": 0
                },
                {
                    "sent": "Likewise popping is shift down or the other way around, which at training time is fine because it allows everything to be continuous at Test time, is problematic in that if you push with certainty 0.5 and then pop with certainty 0.5, every vector becomes an average of the value before and after, so push and pop are destructive unless you do them with close to.",
                    "label": 0
                },
                {
                    "sent": "Perfect certainty, which is why in at inference time they discretize those decisions and you only use the continuous nature of pushing pop to train the model.",
                    "label": 0
                },
                {
                    "sent": "And that's actually perfectly valid.",
                    "label": 0
                },
                {
                    "sent": "In my mind.",
                    "label": 0
                },
                {
                    "sent": "It's like it's a perfectly valid thing to do.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experiments we didn't need to discretize, but they were very synthetic experiments.",
                    "label": 0
                },
                {
                    "sent": "I suspect if we wanted to train a continuous stack of the form I presented on more complex problems, we might also want to discretize and only exploit the continuous nature of the stack during.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But that's completely fine.",
                    "label": 0
                },
                {
                    "sent": "There's also an worth mentioning.",
                    "label": 0
                },
                {
                    "sent": "It doesn't quite fit this controller memory framework I talked about earlier, but there's a really nice idea of using a pointer over previous states of an STM to simulate a stack in this paper by Chris Dyer, an aisle in 2015.",
                    "label": 0
                },
                {
                    "sent": "Really worth reading 'cause there's a very cool experiment applying this to learn shift reduce parsing.",
                    "label": 0
                },
                {
                    "sent": "Likewise, experiments in Julian Michaelov, I thought were quite sophisticated compared to the ones I presented.",
                    "label": 0
                },
                {
                    "sent": "Very artificial ones I presented.",
                    "label": 0
                },
                {
                    "sent": "So read these papers.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, just to summarize quickly what we've talked about with pushdown atomic, these are decent approximations of classical pushdown automata, so I think that were I'm not.",
                    "label": 1
                },
                {
                    "sent": "I think it would be brave to claim that these various stack based mechanisms I just referred to take us completely from a finite state land to push down automata, but I think convincing step towards this, yes.",
                    "label": 0
                },
                {
                    "sent": "So before you were arguing that Ascended Alex team couldn't learn to generalize beyond the sequences, it's all in your stack model.",
                    "label": 0
                },
                {
                    "sent": "Your controller is an LTM, right?",
                    "label": 0
                },
                {
                    "sent": "So if it's only learn to see examples writing to push N times, how can it learn to generalize to 2 * N so you don't need to only use the rule?",
                    "label": 1
                },
                {
                    "sent": "You need to learn as much simpler.",
                    "label": 0
                },
                {
                    "sent": "You don't need the account if your sample if you're.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so so you're arguing that is still going to be a problem for.",
                    "label": 0
                },
                {
                    "sent": "So for copying and for reversal as trivial and that it just needs to learn that.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm either in this mode which is popping pushing mode where I'm in popping mode, so I'm in my encoder, my decoder and it basically pushes until its season end of sequence symbol and then pops until until generating and sequence symbol.",
                    "label": 1
                },
                {
                    "sent": "You're correct that if it needed to track the stack depth within the controller rather than having that be modeled by a register or something like that.",
                    "label": 0
                },
                {
                    "sent": "That the model would probably not generalize as well beyond links where it's already learned account, so that that's a valid counterargument to the thesis that this is directly taking us up a class in the computational hierarchy, but I think there's definitely more potential to generalize in the way that we want by being.",
                    "label": 0
                },
                {
                    "sent": "Put your bottlenecks.",
                    "label": 0
                },
                {
                    "sent": "The license back, yeah yeah, OK, yes, yes, but so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean just to bring us back to my point about it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "For still modeling regular languages, if there's just fewer rules to learn, even if it's not going to unboundedly generalize, I still think it's it's it's.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is evidence that, at least for the simple transaction problems, it's learning a solution in a much quicker, much different way than what the teams are doing, and it's a bit of a sleight of hand to say, look.",
                    "label": 0
                },
                {
                    "sent": "It's learning this rather than this.",
                    "label": 0
                },
                {
                    "sent": "It might not be learning the general case of this, but it's obviously bringing something to the table.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My question is about the fact that we're forcing ourselves into differentiable approximations of these data structures.",
                    "label": 0
                },
                {
                    "sent": "Are we losing something when you do that in terms of expressive power or something else?",
                    "label": 0
                },
                {
                    "sent": "Where do you see?",
                    "label": 0
                },
                {
                    "sent": "Is this a hurting or it's not a big deal?",
                    "label": 0
                },
                {
                    "sent": "Well, I don't think it's a big deal, but the experiments we've done are trivial, and as I'm pointing out here.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think there's been a little work on applying these architectures, so aside from and that's partly because I've done other stuff personally and I'm sure our model is.",
                    "label": 1
                },
                {
                    "sent": "My question was, do you have intuition go wrong because of these sort of smooth stock operation?",
                    "label": 0
                },
                {
                    "sent": "OK, so I mean it's like.",
                    "label": 0
                },
                {
                    "sent": "When you pick what?",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Structure using.",
                    "label": 0
                },
                {
                    "sent": "You're imposing an architectural bias that isn't there in the case of an LC am, so if you pick the wrong data structure.",
                    "label": 0
                },
                {
                    "sent": "Then you're going to, you know, suffer.",
                    "label": 0
                },
                {
                    "sent": "You can obviously hook up a controller to several data structures that makes a learning problem more confusing and you lose some of maybe the elegance of modeling and push down to directly.",
                    "label": 0
                },
                {
                    "sent": "So I feel like we're introducing bias at the architectural level in a way that might be harmful, but I'll be honest, I haven't actually experimented with these beyond this initial paper, enough to form a strong intuition about whether this will come and bite us, and that's the exciting part of research is we need to get around to doing that, yes?",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You just you don't get any.",
                    "label": 0
                },
                {
                    "sent": "You don't get any signal.",
                    "label": 0
                },
                {
                    "sent": "There's an interesting question and answer that answer.",
                    "label": 0
                },
                {
                    "sent": "How do you penalize against popping when there's nothing in it?",
                    "label": 0
                },
                {
                    "sent": "So in fact, there is a degenerate sort of training case.",
                    "label": 0
                },
                {
                    "sent": "If your models initialized such that the controller aggressively pops early in training so that it's constantly removing stuff from the stack is never going to get a lot of gradient on that pop operation, shifting it away from popping aggressively, which is bad.",
                    "label": 0
                },
                {
                    "sent": "So when we started training these models, I don't know if our modeling and Thomas found similar problems.",
                    "label": 0
                },
                {
                    "sent": "We found that it's like sometimes if you initialize them randomly, got lucky and it took that beautiful behavior of going straight to one and sometimes it sucked and this is really a case for against automatic differentiation.",
                    "label": 0
                },
                {
                    "sent": "So what happened is really just for the paper I had written.",
                    "label": 0
                },
                {
                    "sent": "The backward dynamics of the stack in the appendix and I look back upon them.",
                    "label": 0
                },
                {
                    "sent": "I thought, OK, why isn't it not learning to populous?",
                    "label": 0
                },
                {
                    "sent": "Why is it not learning to use the stack?",
                    "label": 0
                },
                {
                    "sent": "And it's fairly obvious from looking at the gradient equations that it's like, OK, it's just.",
                    "label": 0
                },
                {
                    "sent": "It's going to reach this like degenerate state where it's never going to get signal, so that led us to formulate an initialization strategy for the stack, which is described in the appendix, which is you simply initialize the bias on popping to minus one, and then that's learned away so that early in the early stages of training the model, the controller is discouraged from not from ignoring the stack, or in another way of putting it is encouraged to pop stuff to push stuff onto the stack, even if it's a wrong decision, because if it pushes stuff on, if it learns to manipulate.",
                    "label": 0
                },
                {
                    "sent": "This tool it's given wrong, then it wrongly it moves away from that behavior.",
                    "label": 0
                },
                {
                    "sent": "If it was biased against using it in the 1st place, then it will never explore its capabilities.",
                    "label": 0
                },
                {
                    "sent": "So there's a nice sort of story there.",
                    "label": 0
                },
                {
                    "sent": "I don't know if that answers your question, but I've shoehorned something I wanted to say into it, so that works for me.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me very quickly finish.",
                    "label": 0
                },
                {
                    "sent": "I got 5 minutes left.",
                    "label": 0
                },
                {
                    "sent": "To talk about register machines, so I don't want to end this is neural Turing machines and similar architectures, which I love.",
                    "label": 1
                },
                {
                    "sent": "The work I'm not a huge fan of the name, so I like to just think of these registers and this is the random acts.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Memory two attentions read only memory.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An arguably, if you drink the Koolaid, we're talking about being here in the computational hierarchy, but I'm not going to endorse that claim.",
                    "label": 0
                },
                {
                    "sent": "I'll leave it to you to think about whether or not, or anywhere near that.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the structure here is.",
                    "label": 0
                },
                {
                    "sent": "Compare this to attention where at each time step the memory would based on a key on a query vector produced by the controller, either at the previous time step where the current time step, the memory would return a vector.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In register based memories, you're going to still have that, so you're going to read the memory in a manner possibly similar to attention, or to a variant on attention.",
                    "label": 0
                },
                {
                    "sent": "But at each time step control is also going to update the memory, so the memory is now like dependent on the time step.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The generic sort of setup of this is that the controller, as with attention augmented referral networks, deals with the information the input output from the environment from the text.",
                    "label": 0
                },
                {
                    "sent": "The controller also produces distributions over memory registers for the purpose of reading from it a lot attention.",
                    "label": 1
                },
                {
                    "sent": "An writing to it, the controller decides what to write an, possibly how much to erase, and finally the controller and memory states are updated using the right, the right operation and the read operation an as well.",
                    "label": 1
                },
                {
                    "sent": "As the input from the environment, which allows the recurrent cell produce the output in.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Text 8",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So some underspecified but numerous equations to describe this.",
                    "label": 0
                },
                {
                    "sent": "Our previous state is going to be the previous state of the controller.",
                    "label": 0
                },
                {
                    "sent": "The previous state of the memory, which is now time dependent and the previous read from the memory.",
                    "label": 0
                },
                {
                    "sent": "Although there you could obviously do a late Fusion kind of thing as well.",
                    "label": 0
                },
                {
                    "sent": "Your inputs to the control are going to be the concatenation of the inputs to the overall recurrent cell and the read, and again this is just one of many ways to do this.",
                    "label": 0
                },
                {
                    "sent": "The controller based on this and its previous state outputs a bunch of stuff, so the overall output to the network.",
                    "label": 0
                },
                {
                    "sent": "Vector for the purpose of producing a distribution of a memory for the purpose of reading one for writing, we call these keys, typically value and a hidden layer.",
                    "label": 0
                },
                {
                    "sent": "This updated internal state and the values was going to be written.",
                    "label": 0
                },
                {
                    "sent": "The read is just going to be some read operation.",
                    "label": 0
                },
                {
                    "sent": "For example attention the right is going to be.",
                    "label": 0
                },
                {
                    "sent": "For example, you could either have an erase operation, the simplest kind of right I can imagine as you use the right attention from the right key to gate between a previous value of the memory and the vector that you want to write to the memory, and a is a softmax over the positions.",
                    "label": 0
                },
                {
                    "sent": "But there are many ways to do this and then assuming that someone talked about the MTM in a bit more detail during his talks that I won't.",
                    "label": 0
                },
                {
                    "sent": "I wanna bore you with the finer details.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there are a lot of extensions you can put into this idea.",
                    "label": 0
                },
                {
                    "sent": "We can explore location based addressing where the state is now containing something that we don't actually care about.",
                    "label": 0
                },
                {
                    "sent": "The differentiability of it like A1 hot vector over positions in the memory and the controller simply emits a differentiable shift operation.",
                    "label": 0
                },
                {
                    "sent": "For example circular convolution.",
                    "label": 0
                },
                {
                    "sent": "Allows you to update the position that the read head is happening at each time step.",
                    "label": 0
                },
                {
                    "sent": "You can mix location and content based addressing.",
                    "label": 0
                },
                {
                    "sent": "You could explore harder dressing using reinforce.",
                    "label": 0
                },
                {
                    "sent": "I believe Cho and Joshua have a paper on this and and and other people here I'm in the presence of genius is great and then there are more esoteric heuristic addressing mechanisms, but ideally you want this differential and I I leave you to read those in the literature and upcoming literature.",
                    "label": 0
                },
                {
                    "sent": "Other things that can be looked in there, factorizing the key and content factorize.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Factorizing the key addressing mechanism in the memory and is this something you guys did in your paper too?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so if you think about the memory being empty and you want to write right and the right key is a vector and you're doing a content based look up on an empty memory, everything is 0.",
                    "label": 0
                },
                {
                    "sent": "So by definition there the attention is simply going to return a uniform distribution over all the values in the matrix, and at that point you're just distributing your vector over the entire contents of the matrix, and that doesn't sound very sensible.",
                    "label": 0
                },
                {
                    "sent": "So one thing you can do is reserve part of your.",
                    "label": 0
                },
                {
                    "sent": "The matrix for either a fixed or learnable prefix schema that you initialize with orthogonality constraints so that each position, even when the memory is empty, has some sort of value that's going to be used for the key in order to be able to select a position.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Plenty of plenty of things you can do in this space.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Their relation to actual Turing machines is there, but perhaps a little 10 years.",
                    "label": 0
                },
                {
                    "sent": "So here, as you also pointed out earlier, you can think about the RNN cells internalising the tape.",
                    "label": 0
                },
                {
                    "sent": "This is definitely the paradigm being espoused here, where part of the tape at least is internalized in the form of the addressable register based memory part of it still outside in the symbols.",
                    "label": 0
                },
                {
                    "sent": "You're right, you're reading from, so it's a little weird the controller can control the tape motion Vyas, various mechanisms like content based addressing.",
                    "label": 1
                },
                {
                    "sent": "Or, or shifting an index.",
                    "label": 0
                },
                {
                    "sent": "You are an end could therefore learn to simply focus on modeling the state transitions, but there's still a weird split between the external and internal step.",
                    "label": 0
                },
                {
                    "sent": "Internal tape in the sequence of sequence paradigm that I I don't quite like, so I'm not super convinced that this is going to be modeling and machine.",
                    "label": 0
                },
                {
                    "sent": "The most crucial reason why is the number of computational steps in the standard in standard sequence modeling for, uh, neural Turing machine, for example, is tide to the data.",
                    "label": 0
                },
                {
                    "sent": "So imagine you're trying to sort with.",
                    "label": 0
                },
                {
                    "sent": "Does anyone read that neutering machines paper where they show that it could learn to sort sequences that blew my mind?",
                    "label": 0
                },
                {
                    "sent": "Because it's learning a problem that I thought was ON log NON time and so I thought OK that's impossible.",
                    "label": 1
                },
                {
                    "sent": "But actually it's probably doing is approximating something like Radix sort which is oh and then has particular bounds on the data.",
                    "label": 0
                },
                {
                    "sent": "I can sort if you wanted to learn quick sort with a neural.",
                    "label": 0
                },
                {
                    "sent": "You have a bit of a problem in the sequence to sequence setting because the neural Turing machine would need to read in the input symbols and then choose how much time to start, rearranging things internally before it started out putting the sorted sequence.",
                    "label": 0
                },
                {
                    "sent": "And currently, unless you explore a latent variable modeling approach to pondering, there's no easy way to do that, so I think there's a bunch of limitations that prevent you from modeling natural Turing machine, and this applies to the stacks as well.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine the controller.",
                    "label": 0
                },
                {
                    "sent": "With two stacks being capable of doing the copy problem quite well, and that encoder would push everything to one stack, then you'd have any steps where the controller swaps data from one stack to the other, and then you pop everything out and it comes out in the right order.",
                    "label": 0
                },
                {
                    "sent": "But the crucial point here is the model would lean.",
                    "label": 0
                },
                {
                    "sent": "Need to learn that it needs to think or ponder fourth steps in a data dependent way in order to be able to shift the data around in central representations.",
                    "label": 0
                },
                {
                    "sent": "And there's no elegant way to do this in our standard maximum likelihood based training and.",
                    "label": 0
                },
                {
                    "sent": "We need to quickly wrap up, so I think it's still unlikely to learn a general algorithm, but the experiments show that there is better generalization on symbolic tasks.",
                    "label": 0
                },
                {
                    "sent": "So is it reaching the level of Turing machines?",
                    "label": 0
                },
                {
                    "sent": "Probably not, but we're getting some computational benefit out of it.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can summarize this slide is saying there's there's not many there haven't been many applications of this to natural language understanding, but there's fascinating research to be done here.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of the peak.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Research, so I'd like to quickly conclude this talk with some sort of general recommendations.",
                    "label": 0
                },
                {
                    "sent": "We've seen various intuitions about examining the limitations and capabilities of different kinds of models around the theme of recurrent cells.",
                    "label": 1
                },
                {
                    "sent": "It's easy to design an overly complex model, and it feels very nice and powerful.",
                    "label": 0
                },
                {
                    "sent": "I'm going to write the next crazy model that's going to place an STM.",
                    "label": 0
                },
                {
                    "sent": "It's rarely worth it unless you actually spend time trying to understand the limits of your current architectures.",
                    "label": 0
                },
                {
                    "sent": "Within the context of the problem you're studying, and that's probably one of the most informative ways of developing new models by looking at existing architectures, understanding the limitations in their nature, often better solutions and extensions just pop out of this, and the best example of this for me, this chapters one through three of Felix Curious thesis on LCMS, and I read this.",
                    "label": 0
                },
                {
                    "sent": "It starts with an analysis of the vanishing gradient problem, the exploding gradient problem for RN ends, and then you just very clearly shows how an obvious solution is the gating mechanism, and it's very obvious idea.",
                    "label": 0
                },
                {
                    "sent": "Once you see it written like that, and I thought that was really beautiful and I'm sure there are many papers by people in this in Montreal who they also have that property, by the way, because there are lot of grad students here and you're typically grumpy when you're reviewing.",
                    "label": 0
                },
                {
                    "sent": "When you see a paper and the idea seems really obvious, that's usually the mark that it's a good idea.",
                    "label": 0
                },
                {
                    "sent": "That doesn't mean it's a trivial extension to something, so remember that when you're viewing our papers for NIPS.",
                    "label": 0
                },
                {
                    "sent": "So I think I think not just about the model and but think about the complexity of the problem you want to solve, and whether the model using, say, a simple RNN is going to be appropriate for that level of complexity.",
                    "label": 0
                },
                {
                    "sent": "And then finally just not to be too grumpy about examining complex models.",
                    "label": 0
                },
                {
                    "sent": "Don't be afraid to be creative, there are plenty of problems to solve in natural language understanding and in other domains that will require creative new models rather than the models we currently have.",
                    "label": 1
                },
                {
                    "sent": "So don't just exploit, but explore thank you.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Much.",
                    "label": 0
                }
            ]
        }
    }
}