{
    "id": "wdyeeqie6t7fibahnhiber4vdt3yqup5",
    "title": "The Use of Randomization and Statistical Significance in Data Mining",
    "info": {
        "author": [
            "Kai Puolam\u00e4ki, Finnish Institute of Occupational Health"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Data Mining->Statistical & Consensus Methods"
        ]
    },
    "url": "http://videolectures.net/ptdm2012_puolamaeki_statistical_significance/",
    "segmentation": [
        [
            "So so."
        ],
        [
            "Here, to minimize excitement.",
            "So I give you overview of my talk, so I will give first tell about people here.",
            "Talk about approaches to the learning problem.",
            "And then go to statistical significance testing, which is the main topic of this talk, and essentially I will go through with examples from three of our little recent papers and then then I will somehow try to sketch out what could be the relation of this approach to Bayesian approach."
        ],
        [
            "OK.",
            "So.",
            "So here is to data mining group like beginning this year.",
            "As you notice there lots of Emory ptosis here emeritus Emeritus Jeffries there, now alone good for him.",
            "So so I mean there has been some changes and went oil ostomy to kind of make this talk.",
            "So then I thought that I would like to take a bit broader review and summarize things that we have been doing in this group in relation to the topic of the talk and why and what would be kind of the future directions.",
            "So I was running this group for couple of years, so while Haiti serving at president positions elsewhere.",
            "And our topics have been kind of kind of also one of one of our kind of main main models or brands have been that we have been working closely with scientists from humanities and natural Sciences.",
            "And then we have tried to come together with new computational concepts and develop on them and hopefully prevent contribution to the both of."
        ],
        [
            "Fields and an OK on here then the sum.",
            "Are there other other people that are somehow involved in this particular testing and randomization topics that I'm going to talk about?",
            "So Sammy Haun here with his peers had multiple hypothesis testing as until about Nestedness San Marcos Ollendick of work out randomization techniques.",
            "So how to sample from normal hypothesis efficiently and kind of data mining scenarios?",
            "An OK.",
            "So then we have some former postdocs here on our SG honest, so he will start as a professor at our Department next year.",
            "So what it will kind of continue the work in his path in Helsinki also.",
            "OK, and also so this is.",
            "This is kind of this is I mean the view and I would like to."
        ],
        [
            "Take a bit some liberties here and take a bit personal view and maybe.",
            "And So what is this all about?"
        ],
        [
            "OK. Because my training is in a theoretical physics, so I thought that this is a very clarifying analogue of what I'm going to do.",
            "As you all well know.",
            "So in quantum quantum mechanics is a very kind of weird field that I mean this everyday intuition doesn't serve you.",
            "So I mean, this cat is alive and dead at the same time, which is so strange.",
            "And OK, so in a quantum mechanics have several formulations.",
            "Mathematical formulations like wavefunction operator, particle formulation and all of these formulations.",
            "They give you the same predictions, but some formulations are better for some other tasks and some other submitted father talks.",
            "So if, for example if you want to do numerical simulations, so path in the clouds are very nice because they are kind of part of the particles etc.",
            "So.",
            "I mean they naturally switch between these presentations depending on what they're trying to do and what they're trying to understand.",
            "Of course, they are really wrong theories, also, but let's not talk about those here."
        ],
        [
            "OK. Then what is the analogue?",
            "So learning problem is you know where you're very similar to this quantum mechanics in some very, very kind of.",
            "Big picture, so learning problem.",
            "It has at least two parts or one is 1 is the optimization problem that you define some objective and then you want to, which you might be able to give a mathematical definition that you almost minimize some clustering cost or something, and then you solve this optimization problem.",
            "So it is often very difficult problem.",
            "But I mean at least the problem is well defined and you know the answer exists even though you can't find it, but it exists.",
            "So the second part is kind of more fuzzy so.",
            "So we have to watch the trailer lights or estimate something that has not been seen or we have to take noise into account or whatever you want to call that.",
            "And I mean it's also then as you know, so there are lots of theories theorems, no free lunch theorems about this topic.",
            "That which say that in one way or another you must include some prior information because you can solve it and then you could argue that hold this problem is kind of in defined that you can't even mathematically.",
            "Divide it and OK and this is.",
            "This is an to solve this.",
            "So it is important problem to solve this so you have a kind of several different approaches to do that.",
            "And different formulations, and the formulation that I'm going to talk about today.",
            "So so is the significance testing formulation.",
            "So how to solve this estimation or learning or not taking into account part of the problem?",
            "And then I tried to kind of sketch out the relations of the relations of this significance, testing to the other."
        ],
        [
            "I mean other approaches.",
            "So as an example, so about Bayesian learning is 1 approach, so it's very nice.",
            "Start the.",
            "Very principled way and you can take the priority information into account in very nice way.",
            "And it's a robust way to handle noise, and it makes sense.",
            "So I mean it's not like quantum mechanics that you could actually actually probability is very nice, but it has problems.",
            "So usually the models are incorrect.",
            "So when you make a linear fit so your curve is rarely align and this leads all kinds of problems, so it's breaks one of the kind of axiomatic foundations of this whole whole biggest thing.",
            "And then I mean things are getting a bit difficult so.",
            "And here's why you have disconnected learning to do classifiers.",
            "Interactive learning to.",
            "I mean because you know the model is incorrect.",
            "So then you better.",
            "I mean, do something else about it and as you know, the models are always incorrect, so you never can model very complex phenomena.",
            "I mean fully accurately.",
            "And this is an interesting topic, but I removed remove slides about this because I mean I think the time is short and OK. Then what is difficult that is difficult to formulate.",
            "The prior knowledge that you can make a graphical model or whatever, but how to actually tell that tell you so, I mean.",
            "Earlier talk this morning so the biologist tells you something about the data.",
            "So how do you include that into the model?",
            "So this is difficult?",
            "And often the models are very computationally heavy becausw.",
            "I mean they are restricted by this kind of quite strict theoretical."
        ],
        [
            "Your work, so these are kind of.",
            "Various models to solve the same problem called collaborative filtering problem, and so it's very difficult now to say that OK argue that which one is better or another and then to solve this problem.",
            "So it might it might take time if you do some Markov chain Monte Carlo for example.",
            "So this is not for immediate interaction.",
            "At least this things.",
            "OK."
        ],
        [
            "So this is Bayesian approach.",
            "So the second approach that.",
            "If this kind of algorithmic approach, which is essentially just this fitting part of the learning problem, that example started, find the item sets whose frequencies above some threshold or define accost functioning for clustering, clustering and then solve this problem, and you might show that OK, you can solve it to some accuracy and you are happy, or then you can define some new new computational concepts like bucket ordering and then you can say that, OK, let's see what is the efficient way to find this bucket ordering.",
            "And this is."
        ],
        [
            "I mean the good thing about this algorithm, cockroaches that often.",
            "I mean, because now you are not restricted by this probability.",
            "Stuff so then you can make your stuff much more efficient and fast, even though you can make it very slow.",
            "If you want to, but it's possible to make it fast and it's kind of easy to explore all types of new things because you don't have to worry about probabilistic's or anything, but the problems with this approach.",
            "I mean, if you just look at the model version, is that the noise that I mean how to take that into account?",
            "And the primary information insertion that information is difficult and now the."
        ],
        [
            "Like in this quantum mechanics example, so I guess that none of them is perfect, but all of them describe some part of the problem and are useful for some purposes.",
            "OK."
        ],
        [
            "Made.",
            "So this is kind of the.",
            "And motivation.",
            "And background.",
            "So so so.",
            "So then I will kind of give you an overview of statistical significance testing, so I guess most of you know what is about what I mean.",
            "I want to introduce my kind of notation and so I will be.",
            "I will not be talking about rejecting null hypothesis for example, but I I need some pictures.",
            "I will show you."
        ],
        [
            "What is the kind of our setup?",
            "And this is so I'm describing now several papers and you don't fight this set up in this paper.",
            "So this is some some sort of unified presentation of this work that we have been doing, or what I have been doing as part of this work.",
            "So first we have.",
            "Now we ask you that now rule started.",
            "OK, we have space of possible observations which we call Capital, Omega and so here you have a single observation which could be like real number or some instance of a matrix or whatever.",
            "And then we have some observations that we can we do actually observe out of the space of possible observations.",
            "And then because we are doing statistical significance testing, so then we have a new distribution.",
            "So this is our kind of prior belief of where these observations are.",
            "And so these are the screen things of probability there and then we have some test statistic which is a very weird concept and for it means that for each of the each of you can somehow partition this, in this case continuously.",
            "But you can partition this space of observations into kind of parts with different test statistic.",
            "Or I mean all all kind of observations possible observations have a value of the test statistic and.",
            "It could be that in some case, so you can kind of this kind of hear you have logistical here, the hightest autistic, and this summer now and now.",
            "My assumption here is that higher test values more interesting, so we kind of try to find things with more high test statistic and OK Now our observation as a test statistic, something like maybe 4 and half.",
            "And now we divide P value and the P values probability mass here.",
            "So now if the probability mass.",
            "A small P value is small and now this pattern or this observation is interesting because it has a long P value and then we have some threshold.",
            "We say that if the P value is less than 5%'s for example, so there we are significant."
        ],
        [
            "OK, and here is kind of a very, very textbook example of what this means.",
            "So special observation is real numbers and your distribution is normal distribution and then test statistic is just this real number.",
            "And now if we have observation at the Omega, Zero is 1.64.",
            "So then 5% of the probability mass is at this tail and we say that P value is 5%.",
            "OK, so this is, I mean the signal hypothesis testing."
        ],
        [
            "And then often in data mining, you have multiple hypothesis testing, multiple hypothesis to test them, some in here we wrote a thesis about those as I mentioned, but I just briefly tell you the kind of the setup here.",
            "Is that the only difference between the single and multiple hypothesis testing is that you actually have several test statistic.",
            "So instead of one test artistic plastic, you have like K test statistic or empty statistic, and for each of them you have a P value and so then so then the question is that OK is this?",
            "Observation is significant under the same condition the answer is."
        ],
        [
            "No, because now the problem is that if I sample 20P values from new hypothesis, so then I in this case I got like 3 significant findings P values at most 5% and this of course because there are so many many numbers that by random chance or some of them are like significant then they are.",
            "This will established techniques of how to handle with this.",
            "So essentially you make a correction.",
            "So you make this make this bigger under some rules and then you've got bigger P values and.",
            "After this correction, so actually none of these are significant, so these corrections compensate for the effect that so many observations.",
            "And of course these methods they lose power if more.",
            "I think you have so much more power you lose, so I mean.",
            "More these increase these P values from from here.",
            "But I'm not going to again, this is not one of the.",
            "Kind of the.",
            "Main focuses of this talk.",
            "Even though this is I'm in a very kind of important problem in data mining, where you will typically have like even millions of patterns.",
            "Thought he was very untrivial."
        ],
        [
            "OK, so then if you have.",
            "Now have their.",
            "Set up for significant testing.",
            "So how do you apply it for?",
            "Data mining."
        ],
        [
            "So much in a.",
            "So what you do is that?",
            "I mean this is similar to Inspirit.",
            "What tile installed was talking about.",
            "So you introduce you kind of modify the standard significant in area by adding these constraints and these constraints.",
            "They are just some sets of subsets of all sets.",
            "Of these all sets of observations and in principle you can arbitrarily define these constraints.",
            "The only thing is that the observed value must be included.",
            "In the set of constraints and what these constraints do is that they shrink string.",
            "So basically, for example, if you observe something so it kind of.",
            "You observe something something, so it is.",
            "It shrinks the base available.",
            "Observations and so mathematically, so when you kind of update update this set of observations by intersecting it by the set of constraints always when you introduce a new constraint, and now the P value is conditional."
        ],
        [
            "On this on this new set of possible observations.",
            "So.",
            "So if you have like full.",
            "Set of observation and then you can.",
            "Your constraint is that OK?",
            "This is your constraint set.",
            "Here, so like in this full data, so the P value of this.",
            "Omega Zero is quite small because the probability mass mass above here is quite small.",
            "But now if our space of observation is just this slice, so it means that OK. Now the P value of this observation is actually actually quite high, maybe about 1/2, which means that this observation is no longer significant under this constraint.",
            "And and our kind of claim is that so then we have kind of explained this observation by this constraints, because it's no longer kind of special special for the special.",
            "Further.",
            "In the sense of this significance testing."
        ],
        [
            "OK. Correlations on coil currencies.",
            "So now I will kind of demonstrate this recent example from."
        ],
        [
            "A paper by Alex Collier, Haiti, Manila and Michael for Tellius, which was about.",
            "About how to kind of.",
            "How?"
        ],
        [
            "To make controlled experiments when you can't make a controlled experiments, which is often case if you implement logic course.",
            "I made apostles added 10 million years ago and it's difficult to make a controlled experiment 10 million years ago.",
            "So now the problem problem is simply that we have some fine sites.",
            "There are two species here, A&B, so you can see them, but I mean a."
        ],
        [
            "A is kind of back\\ or and B is a/ or other way around.",
            "I don't remember, but anyway you notice that all species live in Africa.",
            "The sites both species exist, and on some sites only one of the species exists."
        ],
        [
            "So then we asked, OK, are these pieces correlated?",
            "This is a balanced budget, is fairly interesting question.",
            "So now if we make this binary Matic species A&B and sites there are 55 sites.",
            "By strange coincidence here.",
            "And if we compute, correlation coefficient is a point or four, so there is a small positive correlation.",
            "OK, very nice, So what?"
        ],
        [
            "Can we tell about that?",
            "So now we can of course I mean transfer this matrix to matrix of counts.",
            "Just by kind of finding that.",
            "Columns with A and B = 1, so you have three of those.",
            "an A equals one, B = 0.",
            "You have nine of those, etc, and then you do Fishers exact test meet P, which is a standard not not exactly standard, but anyway.",
            "Textbook, textbook significance testing methods which should give you 0 if there's a strong negative correlation and close to one if there's a strong positive correlation.",
            "So OK, we conclude that there is a weak positive correlation, as with the Pearson correlation test.",
            "So very nice.",
            "So appears that these species are not correlating."
        ],
        [
            "At all.",
            "So now the.",
            "Possible reason for the correlation?",
            "Even apologize.",
            "Good thing is that OK, the both species live in Africa, so it might be the explanation that that I mean for this result that why are we looking at?",
            "Sites outside Africa.",
            "So if we now we want to exclude the information that species lead from in Africa from our analysis.",
            "So how do we do that?",
            "So OK, we impose the constraint that says that OK."
        ],
        [
            "Hey, we know that the species live in Africa.",
            "So formally, so we say that OK, at the rate I have the columns that outside Africa and Europe or AC or somewhere and I kind of they actually all 00 here.",
            "So I just I mean fix this.",
            "And then I kind of only considered kind of the items that are in African essentially, so I didn't mention to you, but I made this feature.",
            "Exa test is excellent.",
            "Append permuting or the items in a row.",
            "So sampling random 01 matrices so that roll marching so the numbers of species are satisfied.",
            "So this is kind of the randomization implementation of that.",
            "But we don't have to use randomization here.",
            "So OK, so if we exclude what is the same thing that we exclude?",
            "Europe and Asia are Australian OK, so then we have a P value of .04.",
            "So now we have a significant negative correlation.",
            "So OK, so so now if there's actually something when we kind of exclude the knowledge that the species live in Africa."
        ],
        [
            "I don't know now if we can.",
            "So it's other kinds of contracts.",
            "Or if you have no constraints, so we have a significant positive correlation.",
            "So if you have a constraint that we only take into account African location, so we have a negative correlation, and then if we look at the Union of the areas of curious so then we had actually very strong negative correlation and then we have intersection of the areas of a curious so then we have a weak negative correlation.",
            "Now this test that.",
            "So this is a very strong negative correlation that say that OK, this is actually.",
            "Roses other unknown process that causes this species to exhibit different areas of a curious.",
            "So maybe it's something to do with climate.",
            "For example, we don't know, but we can say that there's something something that is kind of not explained just by the areas of occurrence and this this week and formalize with constraints.",
            "So this is like using constraints.",
            "And then if you look at the look at finally this final area, so we notice that the number of data points is so small that it loses its power.",
            "But if we would still notice some correlation here, for example, a negative correlation between that maybe this PC is kind of expelled each other, so maybe they eat each other or something that they can't just live in their neighborhood or something."
        ],
        [
            "And So what I discuss?"
        ],
        [
            "Here was this kind of spatial spatial constraints, but."
        ],
        [
            "We also kind of use temporal constraints, so we look at the species who live at the same temporal interval or use taxonomic constraints, which means that, for example, look at areas of accuracies of order.",
            "So can this correlation be explained by the fact that these two species are in particular taxonomic orders?",
            "And if so then then I mean that you can see no correlation after this order.",
            "Taxonomic constraints.",
            "Then when you do that so then you can.",
            "Genesis and then you can find the positive and negative correlations which are significant under multiple hypothesis correction.",
            "Credit here and then you can kind of analyze it and and this way this is I mean now kind of kind of way to way to add make controlled experiments and way to kind of exclude prior information or this kind of noisy information.",
            "And as an individual to get some statistical significance which I mean kind of respectable way of saying that.",
            "You actually have some scientific finding.",
            "OK, but this is this is.",
            "I mean now the idea of.",
            "That constraint, So what does it mean?",
            "We can use it to kind of insert prior information and see."
        ],
        [
            "What else there is in the data?",
            "And this was very paleontological, faucille fossilized application."
        ],
        [
            "So more like data mining like application of the same idea is this our KDD paper like?",
            "Three years ago.",
            "Quotes tell me something I don't know.",
            "Now instead of not now, the idea in this paper was to see if different data mining results would present the same information.",
            "So if you have a frequent itemsets, infrequent clustering, so do they tell you the same thing, or or is there something new in one and that is not expressed by the other approach?"
        ],
        [
            "And the idea here is that.",
            "We have again again I took this fossil example.",
            "We also have some other than fossil examples.",
            "In a paper is that we observe some like sites of occurrence in some animals.",
            "Here again so Red Gorilla, Burton alternate sites and now if we look at the frequent frequencies of all combinations of these animals.",
            "So we notice that.",
            "An OK that aesthetic is here.",
            "The kind of the frequency of for each pair of each combination of this animal.",
            "So today's topic is the frequency of occurrence of this occurrence of this of this set and then all distribution is uniform.",
            "Distribution of binary matrices where we keep the row column margins fixed.",
            "So essentially permute or columns to get randomized matrix.",
            "So now what we can do is that we for example look at the accuracy of coral and red so.",
            "OK, the frequency is 4 what we observe."
        ],
        [
            "And then we kind of randomized it.",
            "So we compute the frequencies and they're usually less than four.",
            "So which means that these Red Gorilla combination is signal."
        ],
        [
            "Account and find that actually all of the combinations for the frequencies are significant, which is kind of a nightmare because you hope to find some patterns and now all patterns are significant, so they are kind of useless in that sense that what can you do so OK.",
            "So now we set a constraint that, OK, let's say that OK, we fixed fixed frequency of this rattan gorilla.",
            "And then with this constraint we construct null hypothesis so we can sample."
        ],
        [
            "Sample from the data.",
            "OK so here are for randomization.",
            "So where this this frequency is now kind of fixed but everything else is random.",
            "Using this column permutations.",
            "And."
        ],
        [
            "And then we find that OK, there are still kind of four frequent items that are significant.",
            "But these three significant items are no longer seeking difficult, so they have a small big P value.",
            "And then after that.",
            "So if we then set a constraint Bo, so we constrain the frequency of disappear two so so then we find that OK, nothing is significant.",
            "So actually these two patterns below an Archie.",
            "So if they are enough to explain all the data and so then we can be happy and happy and go to home and everything is nice again.",
            "So now this was kind of a way to compress this.",
            "This huge amounts of partner patterns into two patterns that actually contain the information that is in the oil patterns."
        ],
        [
            "And second kind of thing that we asked is that OK?",
            "So this was kind of frequent sets so so then if you look at the clustering so so if you look at this kind of binary matrix and we do roll and color matching preserving normalization so we sample from set of binary matrices which have fixed row and column margins.",
            "So which you can technically implement body swap randomisation.",
            "So this is kind of a baseline randomization and then we add some prior information, this baseline.",
            "So then we kind of test.",
            "Is that OK?",
            "We do.",
            "We can do clustering for the same data and then we can set a constraint that constraints such that preserve this clustering structure, which can be technically technically kind of made by doing this map randomization so that the steps occur only between the clusters.",
            "So this this kind of this randomization keeps the clustering error constant, so it doesn't affect the clustering solution.",
            "And what?"
        ],
        [
            "We find this that when we do this clustering randomization that so here is the data.",
            "So we cluster the species into three clusters and after clustering to these three clusters, so actually no.",
            "Two items that are significant, so the clustering actually explains everything that is explained by the item sets, which is rather surprising, so I guess there are some size 3 items at the last year, signficant.",
            "And Anna second approach is that this kind of iterative data mining skenario where you kind of you start with a.",
            "Did you check on the example?",
            "Was that OK?",
            "Now we start with the full data.",
            "We count the frequent sets of size 2 and three and then we take take some number of the most significant frequent sets an set a constraint at the frequency of these items that must be be fixed, and then we randomize again an at each iteration we we, I mean set a constraint that number of the.",
            "At the most significant items sold, their frequencies fixed and continue this and and when we do this so then we notice that when we at each iteration so well.",
            "I mean, I trained so the number of significant items it's kind of decrease.",
            "So this corresponds to the fact that, OK, we find some patterns and then we said OK, we know these patterns, let's keep them fixed.",
            "What else is there in the data?",
            "Also, for this way we can also kind of prune out throughout patterns patterns.",
            "Pattern space."
        ],
        [
            "OK.",
            "So, so this is kind of the.",
            "So this is something that I don't know what kind of implementation of this implementation of these constraints to data mining skenario's and then we can kind of find some data mining results like frequencies of the items it's or cluster structure, and then we what we did was that we set this cluster structure items that frequency constraint and then we can ask a question that.",
            "Is there something is.",
            "Is that this example cluster structure doesn't explain in the data?",
            "And also then we notice that OK, this clustering kind of explains what Itemsets explains, at least with our measures.",
            "OK."
        ],
        [
            "OK, so.",
            "Kind of 1/3 third."
        ],
        [
            "Well, my last paper so this this is this paper is by Jeffrey Lifelight binoculars, Petro and me and it's kind of it will appear at data mining and knowledge discovery at some point, hopefully.",
            "Well then published yet?",
            "So so here we kind of want to kind of develop this.",
            "Set up that.",
            "OK, so now we all know we want to take a bit different.",
            "Set up so unlike in this in this, tell me something.",
            "What I don't know approach, so I mean we had a different test statistic for all all kind of frequent item sets, for example, so testing scenario.",
            "But now in this approach, we wanted to kind of take one global test statistic, which is a.",
            "Kind of a measure of interest, so I will come back to later.",
            "Or maybe on how to choose that and then then that OK. Then we have a set of NC potential constraints.",
            "So we don't have just one cost range, but the set of constraints.",
            "And now the mathematical problem definition is that find a set of constraints of a given size size K out of these sets of NC constraints such that the global P value of the whole data is maximized.",
            "And this is, I mean very nice, because this gives you some kind of a theoretical.",
            "Theoretical kind of.",
            "Framework for this whole significance testing thing.",
            "And maybe also."
        ],
        [
            "So so if I if I if I show this kind of visually what this means that?",
            "OK So what we have here is that we have the space of all possible observations and then we have our observation here and here are the statistic values and as you know that P value is quite small becausw they are very few points that have a higher statistic value than our observations.",
            "And now let's say for simplicity that we have two possible constraints."
        ],
        [
            "Constraint one which is kind of.",
            "And this one.",
            "This area and constraint 2, which is kind of this part of the space and now by our problem definition, so we want to find a set of.",
            "In this case one constraint that maximizes the global P value.",
            "So now if we pick up I mean this constraint two, so the P value is very small, because I mean you notice that there is a very small slice here that is higher.",
            "But then if we make this quick discourse right one, so the P values around 1/2.",
            "So OK this C one is the correct answer.",
            "So it explains the data."
        ],
        [
            "This is our result.",
            "So, So what happened here is that we want to five sets of constraints that somehow put this our data point in the middle of this probability, or middle of the space in terms of the statistics that we have somehow taken out of our hat.",
            "OK, and then of course I mean so this is I mean there.",
            "Problem definition.",
            "Now then, about the algorithms to solve it."
        ],
        [
            "So.",
            "There are kind of three algorithms which are quite.",
            "I mean start not type, so one is exhaustive search so we can just test all combinations of K constraints and compute P value for all and then we can find the optimal result.",
            "So this is of course computationally very heavy.",
            "But I mean there is an optimal algorithm.",
            "OK, that the simplest is independent that we just look at the individual constraints by the time.",
            "So let's say."
        ],
        [
            "We only look at the constraint C1 and compute the P value."
        ],
        [
            "And then we take a look at the cost range C2 and compute the P value and if they are more constrained.",
            "So we do this Walter constraints and we pick the K. Largest P values from here so we don't take any kind of interaction between these countries."
        ],
        [
            "Into account and what will happen is that we end up with the independent algorithm, which is surprisingly good in some cases.",
            "And then what we kind of proposes this greedy algorithm that we do this, I mean in iteration, so we add constraints one by one and so that each stage.",
            "So the kind of the global P value is maximized.",
            "So at this step, so we take into account all the previously added constraint and then we add a new constraint so that the global P value is maximized and we.",
            "Repeat until we have the K constraints.",
            "OK, so then we can show some theoretic kind of computation or."
        ],
        [
            "Thoughts about this topic.",
            "So there's a reduction to set cover problem, so this is NP hard and then we can show that there is no approximation algorithm that is more efficient than the exhaustive search.",
            "And this is, I mean very easy, easy."
        ],
        [
            "After you find a counterexample.",
            "So what you do is that we can find a counterexample where, so here here we have the sets of constraints.",
            "And let's say that our job is to find the size 3 set of three constraints, case 3, and here are the P values and now the optimal solution is construct 1, two and three because they have the highest P value out of the out of the kind of sets of three.",
            "But as soon as you notice.",
            "So by this weird construction.",
            "So I mean, you really have no choice but to look at each of these P values 1 by 1.",
            "In order to find this, find this maximum and.",
            "This means that by looking at P values alone.",
            "So I mean you can't do better than exhaustive search general case.",
            "So this is a very kind of difficult problem.",
            "But Luckily, so usually we are not in a general case because if we were so, no learning would be."
        ],
        [
            "Possible.",
            "So we can.",
            "In this paper, which you can read from our unpublished paper.",
            "So we kind of show show that under some under some mathematical conditions the constraints can be non descript, discriminative, or kind of independent.",
            "And if the constraints are non discriminate, it means that this independent algorithm gives you already optimal solution.",
            "OK this is full.",
            "Non discrimination is very restrictive assumption because it really means that the test statistic and constraints must be coupled.",
            "But then then we can show that OK for many real world problems.",
            "So the constraints are approximately nondiscriminatory.",
            "They kind of approximately satisfy this independence and then we can give quantify this approximate ability with a number and we can kind of give our algorithm with the approximate quite good approximation guarantee.",
            "An important example of a proxy but not creation is that OK if the space space of space of the observations, all binary matrices and the constraints of the constraints are like row sums of the metrics and the test statistic is just like some of ones in this matrix.",
            "So then which is kind of a simple example, but I mean is simplest example of a case where I mean.",
            "Interesting independent algorithm is almost optimal.",
            "And.",
            "Kind of term.",
            "Kind of the idea.",
            "Idea here is that now these constraints, so they are so weakly couples that they don't kind of that much affect the big picture and so so This is why this independent of the greedy algorithm works quite well, even though in general case I mean this problem is quite unsolvable.",
            "OK, so this is, I mean the set up and then these are the kind of theoretic results and.",
            "OK, So what kind of so?",
            "Is this any useful you might ask?",
            "Right, right?"
        ],
        [
            "Actually.",
            "So so OK. Our first experiment is kind of motivated by the tell me tell me something.",
            "I don't know experiment that we have a frequent sets frequent sets and this again this fossil data here.",
            "As an example, an hour no hypothesis is this this swept randomization, so we assume that set of all binary matrix, fixed row and column margins, and then we take 118.",
            "Item sets which have a satisfied that they have a support of at least 10% and lift of at least one.",
            "So to solve this lift is if you don't know what lift is, so the lift is kind of measure of interestingness.",
            "For the frequent itemset.",
            "So it's kind of a frequency, but normalized.",
            "And if we take so, they won 18 kind of items that satisfy these constraints and satisfy these conditions.",
            "And now we take the frequencies of these items as a constraint, so.",
            "Constraint means that we require that the frequency of a given items.",
            "It must be the same as interregional data.",
            "Annizah test artistic, so which isn't interested so difficult in this approach, is to choose a nice global test statistic, but we choose is the sum of lifts of all of these 118 patterns.",
            "And OK, So what happens?",
            "So then we run it kind of iteratively like with creating algorithms.",
            "So we add these constraints one by one, maximizing at each time the global P value until we run out of constraints."
        ],
        [
            "And here is kind of what happens.",
            "So as a first kind of set we get the set of three animals which are pictured here.",
            "I found a stamp.",
            "Of 1 looks like an ape.",
            "But what you notice here is that if you look at the lift, so you notice the first one is not with the largest lift.",
            "So actually the second one has a larger lift, and the explanation for this is that OK, this first item set, which was chosen first, so it might kind of explain something about the lives of the other patterns, so it kind of chose that.",
            "So these are.",
            "These are species that are long lived general that are associated with Western Europe.",
            "Enclosed habitats as I understand, most happy that means for it."
        ],
        [
            "But I'm not sure.",
            "So, so this is their static.",
            "So you notice that maybe maybe this."
        ],
        [
            "First one explains the most of the sum of the lift, so it kind of takes this correlation now into account.",
            "OK, so this was kind of.",
            "Kind of this one example."
        ],
        [
            "And the second example, I think is rather nice.",
            "So now now we move out of from the frequent sets or for seals.",
            "And now the problem is very classic machine learning problem or data mining problem is that please segment time series into K segments and as you have as people learn at the basic courses so you have dynamic programming and other approaches that you can use to solve this problem.",
            "But the problem is that like dynamic programming, is there a quadratic cost function?",
            "So I mean I mean of course ways to control the noise but I mean it's not kind of unique.",
            "So now the question is can we can we define this type of plastic?",
            "Still learning problem with significance testing.",
            "And the answer is yes we can."
        ],
        [
            "So what we have to do is that.",
            "So OK, so let's say that now we have a time series, so it's a sequence of real numbers.",
            "Now what is then in order to formulate this, we have to define the set of possible observations.",
            "Now distribution and test statistics.",
            "So after we have done that, so everything is kind of fixed after that.",
            "So all possible observations are all permutation of permutations of this time series.",
            "So you notice that we have even though we have real data.",
            "So we have kind of finite space here, which is nice and normal distribution is uniform distribution over all permutations.",
            "And this statistic is order is.",
            "So this is something that is proportional to auto because this - So if you have a very strong autocorrelation so then this is small and this is kind of big.",
            "So this is kind of autocorrelation.",
            "So difference between the consecutive items into time series.",
            "OK, and then we have to have to define also constraints.",
            "That somehow kind of partition this space Omega."
        ],
        [
            "Now the constraints are kind of defined by.",
            "We use kind of icon strengths and we define the my splits point split points so they are in the minus one points between the items in the time series and the constraint is defined that if we have a constraint CI so then we can permute only kind of items within the first I and the last N -- I -- 1.",
            "So I kind of items, so these forms when we if we make this permutation at this permutation.",
            "So this is subset of kind of all permutations.",
            "So this constraint is actually a subset of tops."
        ],
        [
            "Nations.",
            "And OK, what does this mean in practice?",
            "So I have here a time series which are generated synthetically, so you notice that they're kind of two.",
            "You should put the cut here, obviously.",
            "So high values are low values."
        ],
        [
            "And now I do the randomization so this red lights are results of five randomizations where I have just permuted.",
            "This permitted these items, and you notice that there's lots of noise, and for each of these randomized curve I can compute the test statistic, which is this auto correlation, which is quite small.",
            "And here I have a distribution of the test statistic and the test statistic of the original data is minus 42 something.",
            "So it's here.",
            "So P value of one person so highly.",
            "Difficult finding so.",
            "So then let's."
        ],
        [
            "Try to add a constraint here.",
            "And now under this randomized, are these constraints?",
            "Are randomizations only within here and within here, and you notice that OK, now the test statistic values for all of our random samples, they actually pretty close for the original data, and now we make the plot so we notice that, OK, the randomized statistical around origonal test autistic, so P values .7, so there's no non significance here.",
            "And this is, I think, quite nice result, cause I mean so.",
            "This shows that we can formulate this kind of classic problems in significance testing.",
            "And.",
            "And OK, so and what this is, is this so this campaign now be used as a regularization?",
            "So now, because the test exploits ever, it means that it makes no sense to add further constrains.",
            "So of course there are other ways to find the correct number of split points, but I mean this is, I mean, one way and Furthermore so as we show in this paper.",
            "So so this looks like a strictly guassian distribution.",
            "So actually you can make an analytic approximation of this.",
            "Distribution of test artistic and then what you can do.",
            "Do we start you actually segment your time series with your favorite algorithm like dynamic programming?",
            "Then you just use this for to compute the P value, which gives you kind of the regularization to know whether you have enough kind of split points.",
            "And then for demo so we."
        ],
        [
            "Now show that under some kind of loose loose kind of assumptions.",
            "So this approach is equivalent to the Standard Time series segmentation on the quadratic cost function.",
            "So we have really kind of formulated.",
            "Classic problem in terms of.",
            "Significance testing formulation."
        ],
        [
            "And I guess one thing that.",
            "One thing is that the randomization's here are quite fast, or they're transportations, and you can also even do them analytically approximately, so I mean you don't have to spend hours of computer time to do this.",
            "OK and then."
        ],
        [
            "So in the I will not kind of describe it, but we have done the same for collaborative hierarchical clustering, which is a standard clustering approach.",
            "And we have kind of a defined set of possible possible.",
            "Observations and this statistic, which is essentially a sum of correlations between the row and doing the same way.",
            "We can show that OK.",
            "I mean, we can formulate this clustering problem in terms of significance testing in the same way.",
            "And I mean the same comments As for the.",
            "Time series segmentation apply here that I mean you can.",
            "I mean get some insight inside this.",
            "I mean you might use it for as regularization for clustering.",
            "If you would like to.",
            "But the I don't want to call."
        ],
        [
            "In lot of detail in that and OK and so there's some summary.",
            "So summary so.",
            "That one thing that might be independent might be discussed a lot is that how do you choose the test statistic so it's something to do with utility.",
            "So what you define as an interesting yes, but I mean I'm not totally happy with, I mean not with any kind of definition.",
            "So I mean I would like to understand it better.",
            "And this global test statistic.",
            "So if we use this approach so it's sometimes a bit difficult to find a good global test statistic that I mean we could do it for this time series, segmentation, segmentation, clustering problem.",
            "But I mean there are lots of problems for which global test very difficult to file, so I'm not sure if this can be applied to all possible problems and one variation could be that we would modify the problem problem setting by adding more static or something.",
            "But I mean this is something for future work."
        ],
        [
            "OK, and after you know in the last minute so I will present my results from the airplane on the trip to trip to."
        ],
        [
            "Trip to.",
            "Here, so now I will present you with the relation to the Bayesian learning, so please take this with grain of salt.",
            "So this is very untested at.",
            "This is just an idea of how this would go, and this doesn't include final answer so.",
            "So the claim is that this has.",
            "This is very related to the maximum posteriori approximation, and there might be.",
            "Some relation to these other work by tile and his collaborators."
        ],
        [
            "So OK, so.",
            "Now just forget everything that I said that now I start the same with the Bayesian learning.",
            "So we have space of possible observations.",
            "Surprising that notation.",
            "Then we have a space of models.",
            "And then we have a probability of observation and then we have a prior probability.",
            "So this this you have to kind of give before hand.",
            "And then we compute the posterior probability by biggest rule by multiplying the priority.",
            "This likelihood and then we get the posterior probability of the model."
        ],
        [
            "OK, So what does this mean graphically?",
            "So we have space of all possible observations and now observation is here and then we have a kind of we can compute the kind of the prior probability of the observation in terms of the probability of the model.",
            "So this is, I mean just a simple computation.",
            "And now what this?",
            "What this significant testing thing does is that it kind of splits the space of observations."
        ],
        [
            "And of course with Bayesian computation.",
            "So we can also split the space of the observations conditional conditional on the observation.",
            "And we can do this by marginalizing over the models.",
            "Again, it's just an application of the.",
            "This kind of chain rule.",
            "But this is, I mean, very difficult.",
            "This contains some an etc so very complex."
        ],
        [
            "So what we want to do is that we want to take maximum posterior approximation, so we just find the model with the highest posterior probability.",
            "This is again a very standard thing."
        ],
        [
            "And OK, and now with this approximation, so we don't have a sum here, but now the probability of the observation probability of possible observations after this observation is something like this probability conditional.",
            "On this map maximum battery model."
        ],
        [
            "And visually, it's something like that that I mean, we kind of split part of the observation observation kind of strings the space of possible observations.",
            "OK, so one key."
        ],
        [
            "Dismissing yeah.",
            "So now if we have, let's say two possible models, Model 1 and Model 2, usually of course we have kind of infinite number of models or something.",
            "But let's say that we only have two models and these models are like this Model 1 which kind of constraints has high values here in Model 1 which has high values here and now, I mean so pricing.",
            "So now this Bayesian learning is picking up the model which has the highest value for this.",
            "Observation and then we pick it up.",
            "And.",
            "Pick it up.",
            "And then this is."
        ],
        [
            "Our model and if we make a further simplifying assumption that so let's make a connection between this significance testing and display a thing that, let's assume that these models are such that the probability is 0.",
            "So let's map the constraints to probabilities so that all these models, so that if the.",
            "Observation is not in the constraint, so then we say that OK, the probability probability is also zero under that model.",
            "So in a way now we get this kind of very tight cuts as we had in our significance testing formulation and now and then if we make this further theoretical physics like assumption that surely.",
            "And then you kind of see if anybody says something.",
            "You say that OK, probability probability is kind of.",
            "It's kind of small at the edges here and high at the middle of the area.",
            "So I mean, if you have more than one, so it's kind of highest here.",
            "So obviously you tend to choose models which have this observation at the middle, which is exactly the same thing.",
            "What happens in this significance testing formulation?",
            "So in this kind of very loosely, how do we level?",
            "So there is some kind of relation here, but I'm not as I said, so this is somewhat preliminary work.",
            "So, but I guess that this is kind of a nice topic for this."
        ],
        [
            "Fashion.",
            "And OK, so so this is I mean what I had to say and what I kind of in this sort summary so.",
            "I I say that OK, so this is I mean as I see it, I hope to see it justice.",
            "It is significant.",
            "Testing would be kind of 1 formulation for learning problem and it would have some advantages over some other approaches.",
            "And I mean it could be used to solve some problems more easily than the current methods.",
            "And I mean the benefits English, credibility and ease of installing prior information and the ability to use the efficient algorithms and letting their significant testing take care of the noise.",
            "But I guess this is what I had to say, so thank you for your time."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here, to minimize excitement.",
                    "label": 0
                },
                {
                    "sent": "So I give you overview of my talk, so I will give first tell about people here.",
                    "label": 0
                },
                {
                    "sent": "Talk about approaches to the learning problem.",
                    "label": 1
                },
                {
                    "sent": "And then go to statistical significance testing, which is the main topic of this talk, and essentially I will go through with examples from three of our little recent papers and then then I will somehow try to sketch out what could be the relation of this approach to Bayesian approach.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So here is to data mining group like beginning this year.",
                    "label": 0
                },
                {
                    "sent": "As you notice there lots of Emory ptosis here emeritus Emeritus Jeffries there, now alone good for him.",
                    "label": 0
                },
                {
                    "sent": "So so I mean there has been some changes and went oil ostomy to kind of make this talk.",
                    "label": 0
                },
                {
                    "sent": "So then I thought that I would like to take a bit broader review and summarize things that we have been doing in this group in relation to the topic of the talk and why and what would be kind of the future directions.",
                    "label": 0
                },
                {
                    "sent": "So I was running this group for couple of years, so while Haiti serving at president positions elsewhere.",
                    "label": 0
                },
                {
                    "sent": "And our topics have been kind of kind of also one of one of our kind of main main models or brands have been that we have been working closely with scientists from humanities and natural Sciences.",
                    "label": 0
                },
                {
                    "sent": "And then we have tried to come together with new computational concepts and develop on them and hopefully prevent contribution to the both of.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fields and an OK on here then the sum.",
                    "label": 0
                },
                {
                    "sent": "Are there other other people that are somehow involved in this particular testing and randomization topics that I'm going to talk about?",
                    "label": 0
                },
                {
                    "sent": "So Sammy Haun here with his peers had multiple hypothesis testing as until about Nestedness San Marcos Ollendick of work out randomization techniques.",
                    "label": 0
                },
                {
                    "sent": "So how to sample from normal hypothesis efficiently and kind of data mining scenarios?",
                    "label": 0
                },
                {
                    "sent": "An OK.",
                    "label": 0
                },
                {
                    "sent": "So then we have some former postdocs here on our SG honest, so he will start as a professor at our Department next year.",
                    "label": 0
                },
                {
                    "sent": "So what it will kind of continue the work in his path in Helsinki also.",
                    "label": 0
                },
                {
                    "sent": "OK, and also so this is.",
                    "label": 0
                },
                {
                    "sent": "This is kind of this is I mean the view and I would like to.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take a bit some liberties here and take a bit personal view and maybe.",
                    "label": 0
                },
                {
                    "sent": "And So what is this all about?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Because my training is in a theoretical physics, so I thought that this is a very clarifying analogue of what I'm going to do.",
                    "label": 0
                },
                {
                    "sent": "As you all well know.",
                    "label": 0
                },
                {
                    "sent": "So in quantum quantum mechanics is a very kind of weird field that I mean this everyday intuition doesn't serve you.",
                    "label": 0
                },
                {
                    "sent": "So I mean, this cat is alive and dead at the same time, which is so strange.",
                    "label": 0
                },
                {
                    "sent": "And OK, so in a quantum mechanics have several formulations.",
                    "label": 0
                },
                {
                    "sent": "Mathematical formulations like wavefunction operator, particle formulation and all of these formulations.",
                    "label": 0
                },
                {
                    "sent": "They give you the same predictions, but some formulations are better for some other tasks and some other submitted father talks.",
                    "label": 0
                },
                {
                    "sent": "So if, for example if you want to do numerical simulations, so path in the clouds are very nice because they are kind of part of the particles etc.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I mean they naturally switch between these presentations depending on what they're trying to do and what they're trying to understand.",
                    "label": 0
                },
                {
                    "sent": "Of course, they are really wrong theories, also, but let's not talk about those here.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Then what is the analogue?",
                    "label": 0
                },
                {
                    "sent": "So learning problem is you know where you're very similar to this quantum mechanics in some very, very kind of.",
                    "label": 1
                },
                {
                    "sent": "Big picture, so learning problem.",
                    "label": 1
                },
                {
                    "sent": "It has at least two parts or one is 1 is the optimization problem that you define some objective and then you want to, which you might be able to give a mathematical definition that you almost minimize some clustering cost or something, and then you solve this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So it is often very difficult problem.",
                    "label": 0
                },
                {
                    "sent": "But I mean at least the problem is well defined and you know the answer exists even though you can't find it, but it exists.",
                    "label": 0
                },
                {
                    "sent": "So the second part is kind of more fuzzy so.",
                    "label": 1
                },
                {
                    "sent": "So we have to watch the trailer lights or estimate something that has not been seen or we have to take noise into account or whatever you want to call that.",
                    "label": 1
                },
                {
                    "sent": "And I mean it's also then as you know, so there are lots of theories theorems, no free lunch theorems about this topic.",
                    "label": 1
                },
                {
                    "sent": "That which say that in one way or another you must include some prior information because you can solve it and then you could argue that hold this problem is kind of in defined that you can't even mathematically.",
                    "label": 0
                },
                {
                    "sent": "Divide it and OK and this is.",
                    "label": 0
                },
                {
                    "sent": "This is an to solve this.",
                    "label": 0
                },
                {
                    "sent": "So it is important problem to solve this so you have a kind of several different approaches to do that.",
                    "label": 0
                },
                {
                    "sent": "And different formulations, and the formulation that I'm going to talk about today.",
                    "label": 0
                },
                {
                    "sent": "So so is the significance testing formulation.",
                    "label": 0
                },
                {
                    "sent": "So how to solve this estimation or learning or not taking into account part of the problem?",
                    "label": 0
                },
                {
                    "sent": "And then I tried to kind of sketch out the relations of the relations of this significance, testing to the other.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I mean other approaches.",
                    "label": 0
                },
                {
                    "sent": "So as an example, so about Bayesian learning is 1 approach, so it's very nice.",
                    "label": 1
                },
                {
                    "sent": "Start the.",
                    "label": 0
                },
                {
                    "sent": "Very principled way and you can take the priority information into account in very nice way.",
                    "label": 1
                },
                {
                    "sent": "And it's a robust way to handle noise, and it makes sense.",
                    "label": 1
                },
                {
                    "sent": "So I mean it's not like quantum mechanics that you could actually actually probability is very nice, but it has problems.",
                    "label": 0
                },
                {
                    "sent": "So usually the models are incorrect.",
                    "label": 0
                },
                {
                    "sent": "So when you make a linear fit so your curve is rarely align and this leads all kinds of problems, so it's breaks one of the kind of axiomatic foundations of this whole whole biggest thing.",
                    "label": 0
                },
                {
                    "sent": "And then I mean things are getting a bit difficult so.",
                    "label": 0
                },
                {
                    "sent": "And here's why you have disconnected learning to do classifiers.",
                    "label": 0
                },
                {
                    "sent": "Interactive learning to.",
                    "label": 0
                },
                {
                    "sent": "I mean because you know the model is incorrect.",
                    "label": 1
                },
                {
                    "sent": "So then you better.",
                    "label": 0
                },
                {
                    "sent": "I mean, do something else about it and as you know, the models are always incorrect, so you never can model very complex phenomena.",
                    "label": 0
                },
                {
                    "sent": "I mean fully accurately.",
                    "label": 0
                },
                {
                    "sent": "And this is an interesting topic, but I removed remove slides about this because I mean I think the time is short and OK. Then what is difficult that is difficult to formulate.",
                    "label": 0
                },
                {
                    "sent": "The prior knowledge that you can make a graphical model or whatever, but how to actually tell that tell you so, I mean.",
                    "label": 0
                },
                {
                    "sent": "Earlier talk this morning so the biologist tells you something about the data.",
                    "label": 0
                },
                {
                    "sent": "So how do you include that into the model?",
                    "label": 0
                },
                {
                    "sent": "So this is difficult?",
                    "label": 0
                },
                {
                    "sent": "And often the models are very computationally heavy becausw.",
                    "label": 0
                },
                {
                    "sent": "I mean they are restricted by this kind of quite strict theoretical.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your work, so these are kind of.",
                    "label": 0
                },
                {
                    "sent": "Various models to solve the same problem called collaborative filtering problem, and so it's very difficult now to say that OK argue that which one is better or another and then to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "So it might it might take time if you do some Markov chain Monte Carlo for example.",
                    "label": 0
                },
                {
                    "sent": "So this is not for immediate interaction.",
                    "label": 0
                },
                {
                    "sent": "At least this things.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is Bayesian approach.",
                    "label": 0
                },
                {
                    "sent": "So the second approach that.",
                    "label": 0
                },
                {
                    "sent": "If this kind of algorithmic approach, which is essentially just this fitting part of the learning problem, that example started, find the item sets whose frequencies above some threshold or define accost functioning for clustering, clustering and then solve this problem, and you might show that OK, you can solve it to some accuracy and you are happy, or then you can define some new new computational concepts like bucket ordering and then you can say that, OK, let's see what is the efficient way to find this bucket ordering.",
                    "label": 0
                },
                {
                    "sent": "And this is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I mean the good thing about this algorithm, cockroaches that often.",
                    "label": 0
                },
                {
                    "sent": "I mean, because now you are not restricted by this probability.",
                    "label": 0
                },
                {
                    "sent": "Stuff so then you can make your stuff much more efficient and fast, even though you can make it very slow.",
                    "label": 0
                },
                {
                    "sent": "If you want to, but it's possible to make it fast and it's kind of easy to explore all types of new things because you don't have to worry about probabilistic's or anything, but the problems with this approach.",
                    "label": 1
                },
                {
                    "sent": "I mean, if you just look at the model version, is that the noise that I mean how to take that into account?",
                    "label": 0
                },
                {
                    "sent": "And the primary information insertion that information is difficult and now the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like in this quantum mechanics example, so I guess that none of them is perfect, but all of them describe some part of the problem and are useful for some purposes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Made.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of the.",
                    "label": 0
                },
                {
                    "sent": "And motivation.",
                    "label": 0
                },
                {
                    "sent": "And background.",
                    "label": 0
                },
                {
                    "sent": "So so so.",
                    "label": 0
                },
                {
                    "sent": "So then I will kind of give you an overview of statistical significance testing, so I guess most of you know what is about what I mean.",
                    "label": 0
                },
                {
                    "sent": "I want to introduce my kind of notation and so I will be.",
                    "label": 0
                },
                {
                    "sent": "I will not be talking about rejecting null hypothesis for example, but I I need some pictures.",
                    "label": 0
                },
                {
                    "sent": "I will show you.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is the kind of our setup?",
                    "label": 0
                },
                {
                    "sent": "And this is so I'm describing now several papers and you don't fight this set up in this paper.",
                    "label": 1
                },
                {
                    "sent": "So this is some some sort of unified presentation of this work that we have been doing, or what I have been doing as part of this work.",
                    "label": 0
                },
                {
                    "sent": "So first we have.",
                    "label": 0
                },
                {
                    "sent": "Now we ask you that now rule started.",
                    "label": 0
                },
                {
                    "sent": "OK, we have space of possible observations which we call Capital, Omega and so here you have a single observation which could be like real number or some instance of a matrix or whatever.",
                    "label": 0
                },
                {
                    "sent": "And then we have some observations that we can we do actually observe out of the space of possible observations.",
                    "label": 1
                },
                {
                    "sent": "And then because we are doing statistical significance testing, so then we have a new distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is our kind of prior belief of where these observations are.",
                    "label": 0
                },
                {
                    "sent": "And so these are the screen things of probability there and then we have some test statistic which is a very weird concept and for it means that for each of the each of you can somehow partition this, in this case continuously.",
                    "label": 0
                },
                {
                    "sent": "But you can partition this space of observations into kind of parts with different test statistic.",
                    "label": 0
                },
                {
                    "sent": "Or I mean all all kind of observations possible observations have a value of the test statistic and.",
                    "label": 0
                },
                {
                    "sent": "It could be that in some case, so you can kind of this kind of hear you have logistical here, the hightest autistic, and this summer now and now.",
                    "label": 0
                },
                {
                    "sent": "My assumption here is that higher test values more interesting, so we kind of try to find things with more high test statistic and OK Now our observation as a test statistic, something like maybe 4 and half.",
                    "label": 0
                },
                {
                    "sent": "And now we divide P value and the P values probability mass here.",
                    "label": 0
                },
                {
                    "sent": "So now if the probability mass.",
                    "label": 0
                },
                {
                    "sent": "A small P value is small and now this pattern or this observation is interesting because it has a long P value and then we have some threshold.",
                    "label": 0
                },
                {
                    "sent": "We say that if the P value is less than 5%'s for example, so there we are significant.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and here is kind of a very, very textbook example of what this means.",
                    "label": 0
                },
                {
                    "sent": "So special observation is real numbers and your distribution is normal distribution and then test statistic is just this real number.",
                    "label": 0
                },
                {
                    "sent": "And now if we have observation at the Omega, Zero is 1.64.",
                    "label": 0
                },
                {
                    "sent": "So then 5% of the probability mass is at this tail and we say that P value is 5%.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is, I mean the signal hypothesis testing.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then often in data mining, you have multiple hypothesis testing, multiple hypothesis to test them, some in here we wrote a thesis about those as I mentioned, but I just briefly tell you the kind of the setup here.",
                    "label": 0
                },
                {
                    "sent": "Is that the only difference between the single and multiple hypothesis testing is that you actually have several test statistic.",
                    "label": 0
                },
                {
                    "sent": "So instead of one test artistic plastic, you have like K test statistic or empty statistic, and for each of them you have a P value and so then so then the question is that OK is this?",
                    "label": 0
                },
                {
                    "sent": "Observation is significant under the same condition the answer is.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, because now the problem is that if I sample 20P values from new hypothesis, so then I in this case I got like 3 significant findings P values at most 5% and this of course because there are so many many numbers that by random chance or some of them are like significant then they are.",
                    "label": 0
                },
                {
                    "sent": "This will established techniques of how to handle with this.",
                    "label": 0
                },
                {
                    "sent": "So essentially you make a correction.",
                    "label": 0
                },
                {
                    "sent": "So you make this make this bigger under some rules and then you've got bigger P values and.",
                    "label": 0
                },
                {
                    "sent": "After this correction, so actually none of these are significant, so these corrections compensate for the effect that so many observations.",
                    "label": 0
                },
                {
                    "sent": "And of course these methods they lose power if more.",
                    "label": 0
                },
                {
                    "sent": "I think you have so much more power you lose, so I mean.",
                    "label": 0
                },
                {
                    "sent": "More these increase these P values from from here.",
                    "label": 0
                },
                {
                    "sent": "But I'm not going to again, this is not one of the.",
                    "label": 0
                },
                {
                    "sent": "Kind of the.",
                    "label": 0
                },
                {
                    "sent": "Main focuses of this talk.",
                    "label": 0
                },
                {
                    "sent": "Even though this is I'm in a very kind of important problem in data mining, where you will typically have like even millions of patterns.",
                    "label": 0
                },
                {
                    "sent": "Thought he was very untrivial.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so then if you have.",
                    "label": 0
                },
                {
                    "sent": "Now have their.",
                    "label": 0
                },
                {
                    "sent": "Set up for significant testing.",
                    "label": 0
                },
                {
                    "sent": "So how do you apply it for?",
                    "label": 0
                },
                {
                    "sent": "Data mining.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So much in a.",
                    "label": 0
                },
                {
                    "sent": "So what you do is that?",
                    "label": 0
                },
                {
                    "sent": "I mean this is similar to Inspirit.",
                    "label": 0
                },
                {
                    "sent": "What tile installed was talking about.",
                    "label": 0
                },
                {
                    "sent": "So you introduce you kind of modify the standard significant in area by adding these constraints and these constraints.",
                    "label": 0
                },
                {
                    "sent": "They are just some sets of subsets of all sets.",
                    "label": 0
                },
                {
                    "sent": "Of these all sets of observations and in principle you can arbitrarily define these constraints.",
                    "label": 0
                },
                {
                    "sent": "The only thing is that the observed value must be included.",
                    "label": 0
                },
                {
                    "sent": "In the set of constraints and what these constraints do is that they shrink string.",
                    "label": 0
                },
                {
                    "sent": "So basically, for example, if you observe something so it kind of.",
                    "label": 0
                },
                {
                    "sent": "You observe something something, so it is.",
                    "label": 0
                },
                {
                    "sent": "It shrinks the base available.",
                    "label": 0
                },
                {
                    "sent": "Observations and so mathematically, so when you kind of update update this set of observations by intersecting it by the set of constraints always when you introduce a new constraint, and now the P value is conditional.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On this on this new set of possible observations.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So if you have like full.",
                    "label": 0
                },
                {
                    "sent": "Set of observation and then you can.",
                    "label": 0
                },
                {
                    "sent": "Your constraint is that OK?",
                    "label": 0
                },
                {
                    "sent": "This is your constraint set.",
                    "label": 0
                },
                {
                    "sent": "Here, so like in this full data, so the P value of this.",
                    "label": 0
                },
                {
                    "sent": "Omega Zero is quite small because the probability mass mass above here is quite small.",
                    "label": 0
                },
                {
                    "sent": "But now if our space of observation is just this slice, so it means that OK. Now the P value of this observation is actually actually quite high, maybe about 1/2, which means that this observation is no longer significant under this constraint.",
                    "label": 0
                },
                {
                    "sent": "And and our kind of claim is that so then we have kind of explained this observation by this constraints, because it's no longer kind of special special for the special.",
                    "label": 0
                },
                {
                    "sent": "Further.",
                    "label": 0
                },
                {
                    "sent": "In the sense of this significance testing.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Correlations on coil currencies.",
                    "label": 0
                },
                {
                    "sent": "So now I will kind of demonstrate this recent example from.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A paper by Alex Collier, Haiti, Manila and Michael for Tellius, which was about.",
                    "label": 0
                },
                {
                    "sent": "About how to kind of.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To make controlled experiments when you can't make a controlled experiments, which is often case if you implement logic course.",
                    "label": 0
                },
                {
                    "sent": "I made apostles added 10 million years ago and it's difficult to make a controlled experiment 10 million years ago.",
                    "label": 0
                },
                {
                    "sent": "So now the problem problem is simply that we have some fine sites.",
                    "label": 0
                },
                {
                    "sent": "There are two species here, A&B, so you can see them, but I mean a.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A is kind of back\\ or and B is a/ or other way around.",
                    "label": 0
                },
                {
                    "sent": "I don't remember, but anyway you notice that all species live in Africa.",
                    "label": 0
                },
                {
                    "sent": "The sites both species exist, and on some sites only one of the species exists.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then we asked, OK, are these pieces correlated?",
                    "label": 0
                },
                {
                    "sent": "This is a balanced budget, is fairly interesting question.",
                    "label": 0
                },
                {
                    "sent": "So now if we make this binary Matic species A&B and sites there are 55 sites.",
                    "label": 0
                },
                {
                    "sent": "By strange coincidence here.",
                    "label": 0
                },
                {
                    "sent": "And if we compute, correlation coefficient is a point or four, so there is a small positive correlation.",
                    "label": 0
                },
                {
                    "sent": "OK, very nice, So what?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can we tell about that?",
                    "label": 0
                },
                {
                    "sent": "So now we can of course I mean transfer this matrix to matrix of counts.",
                    "label": 0
                },
                {
                    "sent": "Just by kind of finding that.",
                    "label": 0
                },
                {
                    "sent": "Columns with A and B = 1, so you have three of those.",
                    "label": 0
                },
                {
                    "sent": "an A equals one, B = 0.",
                    "label": 0
                },
                {
                    "sent": "You have nine of those, etc, and then you do Fishers exact test meet P, which is a standard not not exactly standard, but anyway.",
                    "label": 0
                },
                {
                    "sent": "Textbook, textbook significance testing methods which should give you 0 if there's a strong negative correlation and close to one if there's a strong positive correlation.",
                    "label": 0
                },
                {
                    "sent": "So OK, we conclude that there is a weak positive correlation, as with the Pearson correlation test.",
                    "label": 0
                },
                {
                    "sent": "So very nice.",
                    "label": 0
                },
                {
                    "sent": "So appears that these species are not correlating.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At all.",
                    "label": 0
                },
                {
                    "sent": "So now the.",
                    "label": 0
                },
                {
                    "sent": "Possible reason for the correlation?",
                    "label": 0
                },
                {
                    "sent": "Even apologize.",
                    "label": 0
                },
                {
                    "sent": "Good thing is that OK, the both species live in Africa, so it might be the explanation that that I mean for this result that why are we looking at?",
                    "label": 0
                },
                {
                    "sent": "Sites outside Africa.",
                    "label": 0
                },
                {
                    "sent": "So if we now we want to exclude the information that species lead from in Africa from our analysis.",
                    "label": 0
                },
                {
                    "sent": "So how do we do that?",
                    "label": 0
                },
                {
                    "sent": "So OK, we impose the constraint that says that OK.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hey, we know that the species live in Africa.",
                    "label": 0
                },
                {
                    "sent": "So formally, so we say that OK, at the rate I have the columns that outside Africa and Europe or AC or somewhere and I kind of they actually all 00 here.",
                    "label": 0
                },
                {
                    "sent": "So I just I mean fix this.",
                    "label": 0
                },
                {
                    "sent": "And then I kind of only considered kind of the items that are in African essentially, so I didn't mention to you, but I made this feature.",
                    "label": 0
                },
                {
                    "sent": "Exa test is excellent.",
                    "label": 0
                },
                {
                    "sent": "Append permuting or the items in a row.",
                    "label": 0
                },
                {
                    "sent": "So sampling random 01 matrices so that roll marching so the numbers of species are satisfied.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of the randomization implementation of that.",
                    "label": 0
                },
                {
                    "sent": "But we don't have to use randomization here.",
                    "label": 0
                },
                {
                    "sent": "So OK, so if we exclude what is the same thing that we exclude?",
                    "label": 0
                },
                {
                    "sent": "Europe and Asia are Australian OK, so then we have a P value of .04.",
                    "label": 0
                },
                {
                    "sent": "So now we have a significant negative correlation.",
                    "label": 0
                },
                {
                    "sent": "So OK, so so now if there's actually something when we kind of exclude the knowledge that the species live in Africa.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I don't know now if we can.",
                    "label": 0
                },
                {
                    "sent": "So it's other kinds of contracts.",
                    "label": 0
                },
                {
                    "sent": "Or if you have no constraints, so we have a significant positive correlation.",
                    "label": 0
                },
                {
                    "sent": "So if you have a constraint that we only take into account African location, so we have a negative correlation, and then if we look at the Union of the areas of curious so then we had actually very strong negative correlation and then we have intersection of the areas of a curious so then we have a weak negative correlation.",
                    "label": 1
                },
                {
                    "sent": "Now this test that.",
                    "label": 0
                },
                {
                    "sent": "So this is a very strong negative correlation that say that OK, this is actually.",
                    "label": 1
                },
                {
                    "sent": "Roses other unknown process that causes this species to exhibit different areas of a curious.",
                    "label": 0
                },
                {
                    "sent": "So maybe it's something to do with climate.",
                    "label": 1
                },
                {
                    "sent": "For example, we don't know, but we can say that there's something something that is kind of not explained just by the areas of occurrence and this this week and formalize with constraints.",
                    "label": 0
                },
                {
                    "sent": "So this is like using constraints.",
                    "label": 0
                },
                {
                    "sent": "And then if you look at the look at finally this final area, so we notice that the number of data points is so small that it loses its power.",
                    "label": 0
                },
                {
                    "sent": "But if we would still notice some correlation here, for example, a negative correlation between that maybe this PC is kind of expelled each other, so maybe they eat each other or something that they can't just live in their neighborhood or something.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what I discuss?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here was this kind of spatial spatial constraints, but.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also kind of use temporal constraints, so we look at the species who live at the same temporal interval or use taxonomic constraints, which means that, for example, look at areas of accuracies of order.",
                    "label": 0
                },
                {
                    "sent": "So can this correlation be explained by the fact that these two species are in particular taxonomic orders?",
                    "label": 0
                },
                {
                    "sent": "And if so then then I mean that you can see no correlation after this order.",
                    "label": 0
                },
                {
                    "sent": "Taxonomic constraints.",
                    "label": 0
                },
                {
                    "sent": "Then when you do that so then you can.",
                    "label": 0
                },
                {
                    "sent": "Genesis and then you can find the positive and negative correlations which are significant under multiple hypothesis correction.",
                    "label": 1
                },
                {
                    "sent": "Credit here and then you can kind of analyze it and and this way this is I mean now kind of kind of way to way to add make controlled experiments and way to kind of exclude prior information or this kind of noisy information.",
                    "label": 1
                },
                {
                    "sent": "And as an individual to get some statistical significance which I mean kind of respectable way of saying that.",
                    "label": 0
                },
                {
                    "sent": "You actually have some scientific finding.",
                    "label": 0
                },
                {
                    "sent": "OK, but this is this is.",
                    "label": 0
                },
                {
                    "sent": "I mean now the idea of.",
                    "label": 0
                },
                {
                    "sent": "That constraint, So what does it mean?",
                    "label": 0
                },
                {
                    "sent": "We can use it to kind of insert prior information and see.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What else there is in the data?",
                    "label": 0
                },
                {
                    "sent": "And this was very paleontological, faucille fossilized application.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So more like data mining like application of the same idea is this our KDD paper like?",
                    "label": 0
                },
                {
                    "sent": "Three years ago.",
                    "label": 0
                },
                {
                    "sent": "Quotes tell me something I don't know.",
                    "label": 1
                },
                {
                    "sent": "Now instead of not now, the idea in this paper was to see if different data mining results would present the same information.",
                    "label": 0
                },
                {
                    "sent": "So if you have a frequent itemsets, infrequent clustering, so do they tell you the same thing, or or is there something new in one and that is not expressed by the other approach?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the idea here is that.",
                    "label": 0
                },
                {
                    "sent": "We have again again I took this fossil example.",
                    "label": 0
                },
                {
                    "sent": "We also have some other than fossil examples.",
                    "label": 0
                },
                {
                    "sent": "In a paper is that we observe some like sites of occurrence in some animals.",
                    "label": 0
                },
                {
                    "sent": "Here again so Red Gorilla, Burton alternate sites and now if we look at the frequent frequencies of all combinations of these animals.",
                    "label": 0
                },
                {
                    "sent": "So we notice that.",
                    "label": 0
                },
                {
                    "sent": "An OK that aesthetic is here.",
                    "label": 0
                },
                {
                    "sent": "The kind of the frequency of for each pair of each combination of this animal.",
                    "label": 0
                },
                {
                    "sent": "So today's topic is the frequency of occurrence of this occurrence of this of this set and then all distribution is uniform.",
                    "label": 0
                },
                {
                    "sent": "Distribution of binary matrices where we keep the row column margins fixed.",
                    "label": 0
                },
                {
                    "sent": "So essentially permute or columns to get randomized matrix.",
                    "label": 0
                },
                {
                    "sent": "So now what we can do is that we for example look at the accuracy of coral and red so.",
                    "label": 0
                },
                {
                    "sent": "OK, the frequency is 4 what we observe.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we kind of randomized it.",
                    "label": 0
                },
                {
                    "sent": "So we compute the frequencies and they're usually less than four.",
                    "label": 0
                },
                {
                    "sent": "So which means that these Red Gorilla combination is signal.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Account and find that actually all of the combinations for the frequencies are significant, which is kind of a nightmare because you hope to find some patterns and now all patterns are significant, so they are kind of useless in that sense that what can you do so OK.",
                    "label": 0
                },
                {
                    "sent": "So now we set a constraint that, OK, let's say that OK, we fixed fixed frequency of this rattan gorilla.",
                    "label": 0
                },
                {
                    "sent": "And then with this constraint we construct null hypothesis so we can sample.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample from the data.",
                    "label": 0
                },
                {
                    "sent": "OK so here are for randomization.",
                    "label": 0
                },
                {
                    "sent": "So where this this frequency is now kind of fixed but everything else is random.",
                    "label": 0
                },
                {
                    "sent": "Using this column permutations.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we find that OK, there are still kind of four frequent items that are significant.",
                    "label": 0
                },
                {
                    "sent": "But these three significant items are no longer seeking difficult, so they have a small big P value.",
                    "label": 0
                },
                {
                    "sent": "And then after that.",
                    "label": 0
                },
                {
                    "sent": "So if we then set a constraint Bo, so we constrain the frequency of disappear two so so then we find that OK, nothing is significant.",
                    "label": 0
                },
                {
                    "sent": "So actually these two patterns below an Archie.",
                    "label": 0
                },
                {
                    "sent": "So if they are enough to explain all the data and so then we can be happy and happy and go to home and everything is nice again.",
                    "label": 0
                },
                {
                    "sent": "So now this was kind of a way to compress this.",
                    "label": 0
                },
                {
                    "sent": "This huge amounts of partner patterns into two patterns that actually contain the information that is in the oil patterns.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And second kind of thing that we asked is that OK?",
                    "label": 0
                },
                {
                    "sent": "So this was kind of frequent sets so so then if you look at the clustering so so if you look at this kind of binary matrix and we do roll and color matching preserving normalization so we sample from set of binary matrices which have fixed row and column margins.",
                    "label": 1
                },
                {
                    "sent": "So which you can technically implement body swap randomisation.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a baseline randomization and then we add some prior information, this baseline.",
                    "label": 0
                },
                {
                    "sent": "So then we kind of test.",
                    "label": 0
                },
                {
                    "sent": "Is that OK?",
                    "label": 0
                },
                {
                    "sent": "We do.",
                    "label": 0
                },
                {
                    "sent": "We can do clustering for the same data and then we can set a constraint that constraints such that preserve this clustering structure, which can be technically technically kind of made by doing this map randomization so that the steps occur only between the clusters.",
                    "label": 1
                },
                {
                    "sent": "So this this kind of this randomization keeps the clustering error constant, so it doesn't affect the clustering solution.",
                    "label": 0
                },
                {
                    "sent": "And what?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We find this that when we do this clustering randomization that so here is the data.",
                    "label": 0
                },
                {
                    "sent": "So we cluster the species into three clusters and after clustering to these three clusters, so actually no.",
                    "label": 0
                },
                {
                    "sent": "Two items that are significant, so the clustering actually explains everything that is explained by the item sets, which is rather surprising, so I guess there are some size 3 items at the last year, signficant.",
                    "label": 0
                },
                {
                    "sent": "And Anna second approach is that this kind of iterative data mining skenario where you kind of you start with a.",
                    "label": 0
                },
                {
                    "sent": "Did you check on the example?",
                    "label": 0
                },
                {
                    "sent": "Was that OK?",
                    "label": 0
                },
                {
                    "sent": "Now we start with the full data.",
                    "label": 0
                },
                {
                    "sent": "We count the frequent sets of size 2 and three and then we take take some number of the most significant frequent sets an set a constraint at the frequency of these items that must be be fixed, and then we randomize again an at each iteration we we, I mean set a constraint that number of the.",
                    "label": 1
                },
                {
                    "sent": "At the most significant items sold, their frequencies fixed and continue this and and when we do this so then we notice that when we at each iteration so well.",
                    "label": 0
                },
                {
                    "sent": "I mean, I trained so the number of significant items it's kind of decrease.",
                    "label": 0
                },
                {
                    "sent": "So this corresponds to the fact that, OK, we find some patterns and then we said OK, we know these patterns, let's keep them fixed.",
                    "label": 0
                },
                {
                    "sent": "What else is there in the data?",
                    "label": 0
                },
                {
                    "sent": "Also, for this way we can also kind of prune out throughout patterns patterns.",
                    "label": 0
                },
                {
                    "sent": "Pattern space.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So, so this is kind of the.",
                    "label": 0
                },
                {
                    "sent": "So this is something that I don't know what kind of implementation of this implementation of these constraints to data mining skenario's and then we can kind of find some data mining results like frequencies of the items it's or cluster structure, and then we what we did was that we set this cluster structure items that frequency constraint and then we can ask a question that.",
                    "label": 1
                },
                {
                    "sent": "Is there something is.",
                    "label": 0
                },
                {
                    "sent": "Is that this example cluster structure doesn't explain in the data?",
                    "label": 1
                },
                {
                    "sent": "And also then we notice that OK, this clustering kind of explains what Itemsets explains, at least with our measures.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Kind of 1/3 third.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, my last paper so this this is this paper is by Jeffrey Lifelight binoculars, Petro and me and it's kind of it will appear at data mining and knowledge discovery at some point, hopefully.",
                    "label": 0
                },
                {
                    "sent": "Well then published yet?",
                    "label": 0
                },
                {
                    "sent": "So so here we kind of want to kind of develop this.",
                    "label": 0
                },
                {
                    "sent": "Set up that.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we all know we want to take a bit different.",
                    "label": 0
                },
                {
                    "sent": "Set up so unlike in this in this, tell me something.",
                    "label": 0
                },
                {
                    "sent": "What I don't know approach, so I mean we had a different test statistic for all all kind of frequent item sets, for example, so testing scenario.",
                    "label": 0
                },
                {
                    "sent": "But now in this approach, we wanted to kind of take one global test statistic, which is a.",
                    "label": 0
                },
                {
                    "sent": "Kind of a measure of interest, so I will come back to later.",
                    "label": 0
                },
                {
                    "sent": "Or maybe on how to choose that and then then that OK. Then we have a set of NC potential constraints.",
                    "label": 0
                },
                {
                    "sent": "So we don't have just one cost range, but the set of constraints.",
                    "label": 0
                },
                {
                    "sent": "And now the mathematical problem definition is that find a set of constraints of a given size size K out of these sets of NC constraints such that the global P value of the whole data is maximized.",
                    "label": 1
                },
                {
                    "sent": "And this is, I mean very nice, because this gives you some kind of a theoretical.",
                    "label": 0
                },
                {
                    "sent": "Theoretical kind of.",
                    "label": 0
                },
                {
                    "sent": "Framework for this whole significance testing thing.",
                    "label": 0
                },
                {
                    "sent": "And maybe also.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so if I if I if I show this kind of visually what this means that?",
                    "label": 0
                },
                {
                    "sent": "OK So what we have here is that we have the space of all possible observations and then we have our observation here and here are the statistic values and as you know that P value is quite small becausw they are very few points that have a higher statistic value than our observations.",
                    "label": 0
                },
                {
                    "sent": "And now let's say for simplicity that we have two possible constraints.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Constraint one which is kind of.",
                    "label": 0
                },
                {
                    "sent": "And this one.",
                    "label": 0
                },
                {
                    "sent": "This area and constraint 2, which is kind of this part of the space and now by our problem definition, so we want to find a set of.",
                    "label": 0
                },
                {
                    "sent": "In this case one constraint that maximizes the global P value.",
                    "label": 0
                },
                {
                    "sent": "So now if we pick up I mean this constraint two, so the P value is very small, because I mean you notice that there is a very small slice here that is higher.",
                    "label": 0
                },
                {
                    "sent": "But then if we make this quick discourse right one, so the P values around 1/2.",
                    "label": 0
                },
                {
                    "sent": "So OK this C one is the correct answer.",
                    "label": 0
                },
                {
                    "sent": "So it explains the data.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is our result.",
                    "label": 0
                },
                {
                    "sent": "So, So what happened here is that we want to five sets of constraints that somehow put this our data point in the middle of this probability, or middle of the space in terms of the statistics that we have somehow taken out of our hat.",
                    "label": 0
                },
                {
                    "sent": "OK, and then of course I mean so this is I mean there.",
                    "label": 0
                },
                {
                    "sent": "Problem definition.",
                    "label": 0
                },
                {
                    "sent": "Now then, about the algorithms to solve it.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There are kind of three algorithms which are quite.",
                    "label": 0
                },
                {
                    "sent": "I mean start not type, so one is exhaustive search so we can just test all combinations of K constraints and compute P value for all and then we can find the optimal result.",
                    "label": 1
                },
                {
                    "sent": "So this is of course computationally very heavy.",
                    "label": 0
                },
                {
                    "sent": "But I mean there is an optimal algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, that the simplest is independent that we just look at the individual constraints by the time.",
                    "label": 0
                },
                {
                    "sent": "So let's say.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We only look at the constraint C1 and compute the P value.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we take a look at the cost range C2 and compute the P value and if they are more constrained.",
                    "label": 0
                },
                {
                    "sent": "So we do this Walter constraints and we pick the K. Largest P values from here so we don't take any kind of interaction between these countries.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Into account and what will happen is that we end up with the independent algorithm, which is surprisingly good in some cases.",
                    "label": 0
                },
                {
                    "sent": "And then what we kind of proposes this greedy algorithm that we do this, I mean in iteration, so we add constraints one by one and so that each stage.",
                    "label": 1
                },
                {
                    "sent": "So the kind of the global P value is maximized.",
                    "label": 1
                },
                {
                    "sent": "So at this step, so we take into account all the previously added constraint and then we add a new constraint so that the global P value is maximized and we.",
                    "label": 0
                },
                {
                    "sent": "Repeat until we have the K constraints.",
                    "label": 0
                },
                {
                    "sent": "OK, so then we can show some theoretic kind of computation or.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thoughts about this topic.",
                    "label": 0
                },
                {
                    "sent": "So there's a reduction to set cover problem, so this is NP hard and then we can show that there is no approximation algorithm that is more efficient than the exhaustive search.",
                    "label": 1
                },
                {
                    "sent": "And this is, I mean very easy, easy.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After you find a counterexample.",
                    "label": 0
                },
                {
                    "sent": "So what you do is that we can find a counterexample where, so here here we have the sets of constraints.",
                    "label": 0
                },
                {
                    "sent": "And let's say that our job is to find the size 3 set of three constraints, case 3, and here are the P values and now the optimal solution is construct 1, two and three because they have the highest P value out of the out of the kind of sets of three.",
                    "label": 0
                },
                {
                    "sent": "But as soon as you notice.",
                    "label": 0
                },
                {
                    "sent": "So by this weird construction.",
                    "label": 0
                },
                {
                    "sent": "So I mean, you really have no choice but to look at each of these P values 1 by 1.",
                    "label": 0
                },
                {
                    "sent": "In order to find this, find this maximum and.",
                    "label": 0
                },
                {
                    "sent": "This means that by looking at P values alone.",
                    "label": 0
                },
                {
                    "sent": "So I mean you can't do better than exhaustive search general case.",
                    "label": 0
                },
                {
                    "sent": "So this is a very kind of difficult problem.",
                    "label": 0
                },
                {
                    "sent": "But Luckily, so usually we are not in a general case because if we were so, no learning would be.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Possible.",
                    "label": 0
                },
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "In this paper, which you can read from our unpublished paper.",
                    "label": 0
                },
                {
                    "sent": "So we kind of show show that under some under some mathematical conditions the constraints can be non descript, discriminative, or kind of independent.",
                    "label": 0
                },
                {
                    "sent": "And if the constraints are non discriminate, it means that this independent algorithm gives you already optimal solution.",
                    "label": 0
                },
                {
                    "sent": "OK this is full.",
                    "label": 0
                },
                {
                    "sent": "Non discrimination is very restrictive assumption because it really means that the test statistic and constraints must be coupled.",
                    "label": 0
                },
                {
                    "sent": "But then then we can show that OK for many real world problems.",
                    "label": 0
                },
                {
                    "sent": "So the constraints are approximately nondiscriminatory.",
                    "label": 1
                },
                {
                    "sent": "They kind of approximately satisfy this independence and then we can give quantify this approximate ability with a number and we can kind of give our algorithm with the approximate quite good approximation guarantee.",
                    "label": 0
                },
                {
                    "sent": "An important example of a proxy but not creation is that OK if the space space of space of the observations, all binary matrices and the constraints of the constraints are like row sums of the metrics and the test statistic is just like some of ones in this matrix.",
                    "label": 1
                },
                {
                    "sent": "So then which is kind of a simple example, but I mean is simplest example of a case where I mean.",
                    "label": 0
                },
                {
                    "sent": "Interesting independent algorithm is almost optimal.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Kind of term.",
                    "label": 0
                },
                {
                    "sent": "Kind of the idea.",
                    "label": 0
                },
                {
                    "sent": "Idea here is that now these constraints, so they are so weakly couples that they don't kind of that much affect the big picture and so so This is why this independent of the greedy algorithm works quite well, even though in general case I mean this problem is quite unsolvable.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is, I mean the set up and then these are the kind of theoretic results and.",
                    "label": 0
                },
                {
                    "sent": "OK, So what kind of so?",
                    "label": 0
                },
                {
                    "sent": "Is this any useful you might ask?",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "So so OK. Our first experiment is kind of motivated by the tell me tell me something.",
                    "label": 0
                },
                {
                    "sent": "I don't know experiment that we have a frequent sets frequent sets and this again this fossil data here.",
                    "label": 0
                },
                {
                    "sent": "As an example, an hour no hypothesis is this this swept randomization, so we assume that set of all binary matrix, fixed row and column margins, and then we take 118.",
                    "label": 1
                },
                {
                    "sent": "Item sets which have a satisfied that they have a support of at least 10% and lift of at least one.",
                    "label": 0
                },
                {
                    "sent": "So to solve this lift is if you don't know what lift is, so the lift is kind of measure of interestingness.",
                    "label": 0
                },
                {
                    "sent": "For the frequent itemset.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a frequency, but normalized.",
                    "label": 0
                },
                {
                    "sent": "And if we take so, they won 18 kind of items that satisfy these constraints and satisfy these conditions.",
                    "label": 0
                },
                {
                    "sent": "And now we take the frequencies of these items as a constraint, so.",
                    "label": 0
                },
                {
                    "sent": "Constraint means that we require that the frequency of a given items.",
                    "label": 0
                },
                {
                    "sent": "It must be the same as interregional data.",
                    "label": 0
                },
                {
                    "sent": "Annizah test artistic, so which isn't interested so difficult in this approach, is to choose a nice global test statistic, but we choose is the sum of lifts of all of these 118 patterns.",
                    "label": 0
                },
                {
                    "sent": "And OK, So what happens?",
                    "label": 0
                },
                {
                    "sent": "So then we run it kind of iteratively like with creating algorithms.",
                    "label": 0
                },
                {
                    "sent": "So we add these constraints one by one, maximizing at each time the global P value until we run out of constraints.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is kind of what happens.",
                    "label": 0
                },
                {
                    "sent": "So as a first kind of set we get the set of three animals which are pictured here.",
                    "label": 0
                },
                {
                    "sent": "I found a stamp.",
                    "label": 0
                },
                {
                    "sent": "Of 1 looks like an ape.",
                    "label": 0
                },
                {
                    "sent": "But what you notice here is that if you look at the lift, so you notice the first one is not with the largest lift.",
                    "label": 0
                },
                {
                    "sent": "So actually the second one has a larger lift, and the explanation for this is that OK, this first item set, which was chosen first, so it might kind of explain something about the lives of the other patterns, so it kind of chose that.",
                    "label": 0
                },
                {
                    "sent": "So these are.",
                    "label": 0
                },
                {
                    "sent": "These are species that are long lived general that are associated with Western Europe.",
                    "label": 0
                },
                {
                    "sent": "Enclosed habitats as I understand, most happy that means for it.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "So, so this is their static.",
                    "label": 0
                },
                {
                    "sent": "So you notice that maybe maybe this.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First one explains the most of the sum of the lift, so it kind of takes this correlation now into account.",
                    "label": 0
                },
                {
                    "sent": "OK, so this was kind of.",
                    "label": 0
                },
                {
                    "sent": "Kind of this one example.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the second example, I think is rather nice.",
                    "label": 0
                },
                {
                    "sent": "So now now we move out of from the frequent sets or for seals.",
                    "label": 0
                },
                {
                    "sent": "And now the problem is very classic machine learning problem or data mining problem is that please segment time series into K segments and as you have as people learn at the basic courses so you have dynamic programming and other approaches that you can use to solve this problem.",
                    "label": 1
                },
                {
                    "sent": "But the problem is that like dynamic programming, is there a quadratic cost function?",
                    "label": 0
                },
                {
                    "sent": "So I mean I mean of course ways to control the noise but I mean it's not kind of unique.",
                    "label": 0
                },
                {
                    "sent": "So now the question is can we can we define this type of plastic?",
                    "label": 0
                },
                {
                    "sent": "Still learning problem with significance testing.",
                    "label": 0
                },
                {
                    "sent": "And the answer is yes we can.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we have to do is that.",
                    "label": 0
                },
                {
                    "sent": "So OK, so let's say that now we have a time series, so it's a sequence of real numbers.",
                    "label": 0
                },
                {
                    "sent": "Now what is then in order to formulate this, we have to define the set of possible observations.",
                    "label": 1
                },
                {
                    "sent": "Now distribution and test statistics.",
                    "label": 0
                },
                {
                    "sent": "So after we have done that, so everything is kind of fixed after that.",
                    "label": 0
                },
                {
                    "sent": "So all possible observations are all permutation of permutations of this time series.",
                    "label": 0
                },
                {
                    "sent": "So you notice that we have even though we have real data.",
                    "label": 0
                },
                {
                    "sent": "So we have kind of finite space here, which is nice and normal distribution is uniform distribution over all permutations.",
                    "label": 0
                },
                {
                    "sent": "And this statistic is order is.",
                    "label": 0
                },
                {
                    "sent": "So this is something that is proportional to auto because this - So if you have a very strong autocorrelation so then this is small and this is kind of big.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of autocorrelation.",
                    "label": 0
                },
                {
                    "sent": "So difference between the consecutive items into time series.",
                    "label": 1
                },
                {
                    "sent": "OK, and then we have to have to define also constraints.",
                    "label": 0
                },
                {
                    "sent": "That somehow kind of partition this space Omega.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the constraints are kind of defined by.",
                    "label": 0
                },
                {
                    "sent": "We use kind of icon strengths and we define the my splits point split points so they are in the minus one points between the items in the time series and the constraint is defined that if we have a constraint CI so then we can permute only kind of items within the first I and the last N -- I -- 1.",
                    "label": 1
                },
                {
                    "sent": "So I kind of items, so these forms when we if we make this permutation at this permutation.",
                    "label": 0
                },
                {
                    "sent": "So this is subset of kind of all permutations.",
                    "label": 0
                },
                {
                    "sent": "So this constraint is actually a subset of tops.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nations.",
                    "label": 0
                },
                {
                    "sent": "And OK, what does this mean in practice?",
                    "label": 0
                },
                {
                    "sent": "So I have here a time series which are generated synthetically, so you notice that they're kind of two.",
                    "label": 0
                },
                {
                    "sent": "You should put the cut here, obviously.",
                    "label": 0
                },
                {
                    "sent": "So high values are low values.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now I do the randomization so this red lights are results of five randomizations where I have just permuted.",
                    "label": 0
                },
                {
                    "sent": "This permitted these items, and you notice that there's lots of noise, and for each of these randomized curve I can compute the test statistic, which is this auto correlation, which is quite small.",
                    "label": 0
                },
                {
                    "sent": "And here I have a distribution of the test statistic and the test statistic of the original data is minus 42 something.",
                    "label": 0
                },
                {
                    "sent": "So it's here.",
                    "label": 0
                },
                {
                    "sent": "So P value of one person so highly.",
                    "label": 0
                },
                {
                    "sent": "Difficult finding so.",
                    "label": 0
                },
                {
                    "sent": "So then let's.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Try to add a constraint here.",
                    "label": 0
                },
                {
                    "sent": "And now under this randomized, are these constraints?",
                    "label": 0
                },
                {
                    "sent": "Are randomizations only within here and within here, and you notice that OK, now the test statistic values for all of our random samples, they actually pretty close for the original data, and now we make the plot so we notice that, OK, the randomized statistical around origonal test autistic, so P values .7, so there's no non significance here.",
                    "label": 0
                },
                {
                    "sent": "And this is, I think, quite nice result, cause I mean so.",
                    "label": 0
                },
                {
                    "sent": "This shows that we can formulate this kind of classic problems in significance testing.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And OK, so and what this is, is this so this campaign now be used as a regularization?",
                    "label": 1
                },
                {
                    "sent": "So now, because the test exploits ever, it means that it makes no sense to add further constrains.",
                    "label": 0
                },
                {
                    "sent": "So of course there are other ways to find the correct number of split points, but I mean this is, I mean, one way and Furthermore so as we show in this paper.",
                    "label": 0
                },
                {
                    "sent": "So so this looks like a strictly guassian distribution.",
                    "label": 0
                },
                {
                    "sent": "So actually you can make an analytic approximation of this.",
                    "label": 0
                },
                {
                    "sent": "Distribution of test artistic and then what you can do.",
                    "label": 1
                },
                {
                    "sent": "Do we start you actually segment your time series with your favorite algorithm like dynamic programming?",
                    "label": 0
                },
                {
                    "sent": "Then you just use this for to compute the P value, which gives you kind of the regularization to know whether you have enough kind of split points.",
                    "label": 0
                },
                {
                    "sent": "And then for demo so we.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now show that under some kind of loose loose kind of assumptions.",
                    "label": 0
                },
                {
                    "sent": "So this approach is equivalent to the Standard Time series segmentation on the quadratic cost function.",
                    "label": 1
                },
                {
                    "sent": "So we have really kind of formulated.",
                    "label": 0
                },
                {
                    "sent": "Classic problem in terms of.",
                    "label": 0
                },
                {
                    "sent": "Significance testing formulation.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I guess one thing that.",
                    "label": 0
                },
                {
                    "sent": "One thing is that the randomization's here are quite fast, or they're transportations, and you can also even do them analytically approximately, so I mean you don't have to spend hours of computer time to do this.",
                    "label": 0
                },
                {
                    "sent": "OK and then.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the I will not kind of describe it, but we have done the same for collaborative hierarchical clustering, which is a standard clustering approach.",
                    "label": 0
                },
                {
                    "sent": "And we have kind of a defined set of possible possible.",
                    "label": 0
                },
                {
                    "sent": "Observations and this statistic, which is essentially a sum of correlations between the row and doing the same way.",
                    "label": 0
                },
                {
                    "sent": "We can show that OK.",
                    "label": 0
                },
                {
                    "sent": "I mean, we can formulate this clustering problem in terms of significance testing in the same way.",
                    "label": 0
                },
                {
                    "sent": "And I mean the same comments As for the.",
                    "label": 0
                },
                {
                    "sent": "Time series segmentation apply here that I mean you can.",
                    "label": 0
                },
                {
                    "sent": "I mean get some insight inside this.",
                    "label": 0
                },
                {
                    "sent": "I mean you might use it for as regularization for clustering.",
                    "label": 0
                },
                {
                    "sent": "If you would like to.",
                    "label": 0
                },
                {
                    "sent": "But the I don't want to call.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In lot of detail in that and OK and so there's some summary.",
                    "label": 0
                },
                {
                    "sent": "So summary so.",
                    "label": 0
                },
                {
                    "sent": "That one thing that might be independent might be discussed a lot is that how do you choose the test statistic so it's something to do with utility.",
                    "label": 0
                },
                {
                    "sent": "So what you define as an interesting yes, but I mean I'm not totally happy with, I mean not with any kind of definition.",
                    "label": 0
                },
                {
                    "sent": "So I mean I would like to understand it better.",
                    "label": 0
                },
                {
                    "sent": "And this global test statistic.",
                    "label": 0
                },
                {
                    "sent": "So if we use this approach so it's sometimes a bit difficult to find a good global test statistic that I mean we could do it for this time series, segmentation, segmentation, clustering problem.",
                    "label": 1
                },
                {
                    "sent": "But I mean there are lots of problems for which global test very difficult to file, so I'm not sure if this can be applied to all possible problems and one variation could be that we would modify the problem problem setting by adding more static or something.",
                    "label": 0
                },
                {
                    "sent": "But I mean this is something for future work.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and after you know in the last minute so I will present my results from the airplane on the trip to trip to.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Trip to.",
                    "label": 0
                },
                {
                    "sent": "Here, so now I will present you with the relation to the Bayesian learning, so please take this with grain of salt.",
                    "label": 1
                },
                {
                    "sent": "So this is very untested at.",
                    "label": 0
                },
                {
                    "sent": "This is just an idea of how this would go, and this doesn't include final answer so.",
                    "label": 0
                },
                {
                    "sent": "So the claim is that this has.",
                    "label": 0
                },
                {
                    "sent": "This is very related to the maximum posteriori approximation, and there might be.",
                    "label": 0
                },
                {
                    "sent": "Some relation to these other work by tile and his collaborators.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK, so.",
                    "label": 0
                },
                {
                    "sent": "Now just forget everything that I said that now I start the same with the Bayesian learning.",
                    "label": 0
                },
                {
                    "sent": "So we have space of possible observations.",
                    "label": 1
                },
                {
                    "sent": "Surprising that notation.",
                    "label": 1
                },
                {
                    "sent": "Then we have a space of models.",
                    "label": 1
                },
                {
                    "sent": "And then we have a probability of observation and then we have a prior probability.",
                    "label": 0
                },
                {
                    "sent": "So this this you have to kind of give before hand.",
                    "label": 0
                },
                {
                    "sent": "And then we compute the posterior probability by biggest rule by multiplying the priority.",
                    "label": 0
                },
                {
                    "sent": "This likelihood and then we get the posterior probability of the model.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what does this mean graphically?",
                    "label": 0
                },
                {
                    "sent": "So we have space of all possible observations and now observation is here and then we have a kind of we can compute the kind of the prior probability of the observation in terms of the probability of the model.",
                    "label": 0
                },
                {
                    "sent": "So this is, I mean just a simple computation.",
                    "label": 0
                },
                {
                    "sent": "And now what this?",
                    "label": 0
                },
                {
                    "sent": "What this significant testing thing does is that it kind of splits the space of observations.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course with Bayesian computation.",
                    "label": 0
                },
                {
                    "sent": "So we can also split the space of the observations conditional conditional on the observation.",
                    "label": 0
                },
                {
                    "sent": "And we can do this by marginalizing over the models.",
                    "label": 0
                },
                {
                    "sent": "Again, it's just an application of the.",
                    "label": 0
                },
                {
                    "sent": "This kind of chain rule.",
                    "label": 0
                },
                {
                    "sent": "But this is, I mean, very difficult.",
                    "label": 0
                },
                {
                    "sent": "This contains some an etc so very complex.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we want to do is that we want to take maximum posterior approximation, so we just find the model with the highest posterior probability.",
                    "label": 0
                },
                {
                    "sent": "This is again a very standard thing.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And OK, and now with this approximation, so we don't have a sum here, but now the probability of the observation probability of possible observations after this observation is something like this probability conditional.",
                    "label": 0
                },
                {
                    "sent": "On this map maximum battery model.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And visually, it's something like that that I mean, we kind of split part of the observation observation kind of strings the space of possible observations.",
                    "label": 0
                },
                {
                    "sent": "OK, so one key.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dismissing yeah.",
                    "label": 0
                },
                {
                    "sent": "So now if we have, let's say two possible models, Model 1 and Model 2, usually of course we have kind of infinite number of models or something.",
                    "label": 0
                },
                {
                    "sent": "But let's say that we only have two models and these models are like this Model 1 which kind of constraints has high values here in Model 1 which has high values here and now, I mean so pricing.",
                    "label": 0
                },
                {
                    "sent": "So now this Bayesian learning is picking up the model which has the highest value for this.",
                    "label": 0
                },
                {
                    "sent": "Observation and then we pick it up.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Pick it up.",
                    "label": 0
                },
                {
                    "sent": "And then this is.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our model and if we make a further simplifying assumption that so let's make a connection between this significance testing and display a thing that, let's assume that these models are such that the probability is 0.",
                    "label": 0
                },
                {
                    "sent": "So let's map the constraints to probabilities so that all these models, so that if the.",
                    "label": 0
                },
                {
                    "sent": "Observation is not in the constraint, so then we say that OK, the probability probability is also zero under that model.",
                    "label": 0
                },
                {
                    "sent": "So in a way now we get this kind of very tight cuts as we had in our significance testing formulation and now and then if we make this further theoretical physics like assumption that surely.",
                    "label": 0
                },
                {
                    "sent": "And then you kind of see if anybody says something.",
                    "label": 0
                },
                {
                    "sent": "You say that OK, probability probability is kind of.",
                    "label": 0
                },
                {
                    "sent": "It's kind of small at the edges here and high at the middle of the area.",
                    "label": 0
                },
                {
                    "sent": "So I mean, if you have more than one, so it's kind of highest here.",
                    "label": 0
                },
                {
                    "sent": "So obviously you tend to choose models which have this observation at the middle, which is exactly the same thing.",
                    "label": 0
                },
                {
                    "sent": "What happens in this significance testing formulation?",
                    "label": 0
                },
                {
                    "sent": "So in this kind of very loosely, how do we level?",
                    "label": 0
                },
                {
                    "sent": "So there is some kind of relation here, but I'm not as I said, so this is somewhat preliminary work.",
                    "label": 0
                },
                {
                    "sent": "So, but I guess that this is kind of a nice topic for this.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fashion.",
                    "label": 0
                },
                {
                    "sent": "And OK, so so this is I mean what I had to say and what I kind of in this sort summary so.",
                    "label": 0
                },
                {
                    "sent": "I I say that OK, so this is I mean as I see it, I hope to see it justice.",
                    "label": 0
                },
                {
                    "sent": "It is significant.",
                    "label": 0
                },
                {
                    "sent": "Testing would be kind of 1 formulation for learning problem and it would have some advantages over some other approaches.",
                    "label": 0
                },
                {
                    "sent": "And I mean it could be used to solve some problems more easily than the current methods.",
                    "label": 0
                },
                {
                    "sent": "And I mean the benefits English, credibility and ease of installing prior information and the ability to use the efficient algorithms and letting their significant testing take care of the noise.",
                    "label": 0
                },
                {
                    "sent": "But I guess this is what I had to say, so thank you for your time.",
                    "label": 0
                }
            ]
        }
    }
}