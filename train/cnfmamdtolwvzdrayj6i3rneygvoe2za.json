{
    "id": "cnfmamdtolwvzdrayj6i3rneygvoe2za",
    "title": "Augmenting the Generalized Hough Transform to Enable the Mining of Petroglyphs",
    "info": {
        "author": [
            "Qiang Zhu, LinkedIn Corporation"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "July 2009",
        "category": [
            "Top->Computer Science->Data Mining->Applications",
            "Top->Computer Science->Computer Vision->Shape Analysis"
        ]
    },
    "url": "http://videolectures.net/kdd09_zhu_aghtemp/",
    "segmentation": [
        [
            "It's my pleasure to introduce our third speaker, Mr Challenger, Mr Drew, the PhD student from University of California, Riverside, thanks.",
            "Hello everyone, thanks for still being here to attend my talk because this is one of the very last session in this year's KDD.",
            "So today my talk is about data mining the Patrick."
        ],
        [
            "Leaves OK here.",
            "As you can see are some examples of beautiful Patrick lives and the final goal of our project is to index all the words, rock routes and to enable the efficient and effective data mining on."
        ],
        [
            "Slaves.",
            "Well, petroglyphs are one of the earliest expressions of abstract thinking, and they can provide us a rich source of information like changing of climate, existence of a certain species, or patterns of humans, migration, and."
        ],
        [
            "Interactions.",
            "However, the progress in petroglyphs research has been very slow in the past decade.",
            "We believe it is because of their extraordinary diversity and complexity, and we find most current matching algorithm cannot capture the similarity of petroglyphs.",
            "So one of the key problem is to find efficient and effective dispatch."
        ],
        [
            "Full Patch believe.",
            "OK, I'd like to introduce our approach by answering three questions, so the first one is how to preprocess the real data.",
            "While this is not our main contribution of our work, the preprocessing is a necessary step and I will focus on how we define distance measure for petroglyph and how we do the speed up."
        ],
        [
            "OK, first, let's see why we need the preprocessing.",
            "Let's take a look at this image.",
            "Although it's very clear and it's in high contrast, but the border of this rock may be recognized at the age of the petroglyph by most of the automatic segmentation algorithms.",
            "So to extract such data, we have built a human."
        ],
        [
            "Amputation 2 codes petrol annotator are some of you may have attended the human Computation Workshop on Sunday and the essence of human computation is to outsource some certain critical steps from computers to human.",
            "Such steps might be very hard or even impossible for computer on the programs, but they are very easy for humans.",
            "So in our petrol notator we first loads the role image into."
        ],
        [
            "Our system and we ask volunteers to draw the boundary around the about objects and then trust the ship for the 1st."
        ],
        [
            "Error analysis OK, here's another very important step of preprocessing.",
            "It's called downsampling.",
            "Here I'll give you a very simple intuition by by example.",
            "We ask two volunteers to draw a bighorn sheep in our petrol notator and you can see the results on left in figure A and although they are from the same image and they do similar seems very similar but are only less than 3.5% of the pixels from each image.",
            "Overlap and let's compare it with the downsampled version on the right.",
            "In figure big we find more than 75% of pixels are coming to both and I will show you some experimental results in our evaluation section to show that downsampling can increase the accuracy in our objects tab."
        ],
        [
            "OK, now we are in a position to discuss about the distance measure.",
            "I'm not going to do an exhaustive discussion about why we have this content.",
            "May be other measures actually we have tried and compared more than 10 different measures here.",
            "I'd like to make a brief review of why we have chosen the GHD which is short for generalized Hough transform because THT can makes can make no assumption about.",
            "The data, and Secondly it can correctly capture the similarity of petroglyph and the third one is we can tightly lower bounds the GHT based distance as I will show you later, the original THT is time consuming.",
            "So one classic data mining solution to this problem is to find a efficiently computable and tight lower bounds and use this lower bound to prune of some promising candidates.",
            "So if we can find such a lower bounds, it can allow us to do the efficient searches in very large."
        ],
        [
            "Dead set.",
            "OK, let's briefly review the classic GHT.",
            "Well GHD is a useful method for 2 dimensional arbitrary ship detection and our illustrates are the basic examine by a very simple example.",
            "So here as you can see it's a query image on left and candidate image, see on the right and each cell is pixel and the dark color denotes the edge points of the ship.",
            "And the goal of their GHD is to find the best match between Q and see an what what I mean by best matches.",
            "If I put Q on to see the overlapped eight points will be maximal between QNC so."
        ],
        [
            "This is our goal.",
            "The first step is to find the stop pattern for the query image.",
            "So are we first rotate all the edge points around a reference point R, which is usually the center of mass of all age points by 180 degrees.",
            "So we get this one and then we draw vectors from reference points are to each edge points."
        ],
        [
            "And second step is we superimpose the star pattern onto each edge points of the candidate image like this and we also use accumulator 2 records.",
            "The number of dead ends which fall into corresponding sales.",
            "So after first are updating we can see four of their sales are increased by one and we just put this factor onto the remaining edge points and after four rounds.",
            "We obtained a final."
        ],
        [
            "Accumulator.",
            "And the third step is fine at the peak of the accumulator, so it's the sale with the value three.",
            "It corresponds to our priming candid images.",
            "See and now if we put Q on to see by overlapping or an R prime, we can find this share three overlap points.",
            "So this is the best match between Q and."
        ],
        [
            "See.",
            "Well, the classic GHT can correctly capture or detector the ship, but it doesn't lay explicitly encodes similarity measure, but we can simply define such GHT best distance measure, which we code minimal.",
            "Unmatched Edge points anet equals to the number of eight points in Q minus the maximum matches points.",
            "In our toy example, it's 4 -- 3."
        ],
        [
            "Just one.",
            "OK, now following, I'd like to discuss about how we can find a lower bound to speed up the calculation.",
            "So here comes a question.",
            "When can we obtain the value of a particular cell in the accumulator?",
            "In a classic GHT we need to wait until the end of all the implementation, right?",
            "So is it possible to obtain the value 1 by 1?",
            "I want requirement is we need to check all positions that are possible to increase the sale value.",
            "We can do this by simply reverse the direction of the vectors so we obtain the new vector pattern."
        ],
        [
            "OK, here is our example.",
            "So if we want to estimate lower bounds for the central column of candid image C, Our method first extract 1 dimensional signature from Q&C and the value is just simply the number of counts of edge points an after we get the signature.",
            "We just align this the center of Signature QX on to the corresponding.",
            "Position of Signature CX and you can imagine this like we put the star pattern of Q on to the certain sale of candid Images C and the next.",
            "We just check column by column.",
            "So for the first column we can see our query image needs two pixels to be matched.",
            "But in the candid image there are three.",
            "So in the best Case No points will be missed.",
            "So the minimum is the point in this column is 0, so let's check the second one.",
            "It's the same case in this column Kunitz two pixels to be matched and she has two, so the minimum is, the point is zero.",
            "However, in the third column we find Q needs four pixels, but she only has two.",
            "So even in the best case there will be at least two missed right?",
            "So the minimum Mr Points in this column is 2 and we can do this for the following two.",
            "Columns like this.",
            "And after we finish all this step, we can get a submission.",
            "It's equals 2 two.",
            "So it's the our estimation for the lower bound on this column and we can do the same thing for all the remaining columns and we just pick the smallest one as a global lower bounds.",
            "And of course if you want to get a title lower bound, we can do the same thing in the row wise."
        ],
        [
            "OK, our because we in our lower bound THT we only need to compare 1 dimensional signature so we can reduce their time complexity and we actually we can further reduce it by only abandoned and shifting order and in our evaluation section I will show you that we can achieve one to two orders of MAG."
        ],
        [
            "To speed up.",
            "And MUE is a very useful basic.",
            "This is special and we further did some adjustment and normalization before we apply it to several high level data mining algorithms.",
            "So especially for the third one finding motives motives are.",
            "Most similar pairs in a collection of objects and our distance measure for finding the motives also satisfy the triangular inequality.",
            "So with this nice quality property, we can efficiently find the exact motives."
        ],
        [
            "OK, next I'm going to evaluate our distance measure and lower bound axiom of utility and accuracy and."
        ],
        [
            "Capability so we start from our very simple sanity check.",
            "So here is a clustering of a typical thousand Western US petroglyph.",
            "And as you can see, our distance measure, again correctly groups all seven pairs.",
            "And what's more, in the high level structure of the dental Graham?",
            "Our distance measure also correctly groups similar petroglyphs and more Interestingly, as you can see in their second pair, our measure is also invariant to the.",
            "Hollow and solid character."
        ],
        [
            "Tractor.",
            "OK, in their next evaluation we want to test whether our distance measure is robust to the human variability.",
            "So we asked two volunteers to draw these four pairs of Patrick left in our petrol annotator and you can find some significant variation between them.",
            "So especially for this Dell E the user SC draw it as our outline of the deal but the user WY draw it as a sticker."
        ],
        [
            "Figure.",
            "And here is the results of our by our distance measure and you can see all the applications from the same image are grouped together and also including the one I mentioned."
        ],
        [
            "In the third evaluation, we want to test whether our distance special can find meaningful motives.",
            "So we did a test on the collection of real petroglyphs and they are prospered more than 4 million pairs.",
            "An here is all the pairwise distance, and if we set value 40 as a motive cut off and we want to find all the pairs with the distance that is smaller than this motive cut off.",
            "If we use a brute Force One, we need to calculate all these 4 million possible pairs, right, but?",
            "If we use our distance measure with the triangular inequality, we only need to calculate a tiny fraction of all the calculations.",
            "So we quickly find all the 52 top motives.",
            "By this motive cutoff, an here are five representative from them, and as you can see they are visually visually very similar and we can see some useful in variation and more Interestingly some of them are in fact known to be true meaning."
        ],
        [
            "Formatives OK, because there are no large collections of Patrick life with labels, so we to evaluate the accuracy of our distance measure.",
            "We tested on two publicly available datasets and as you can see, these two datasets are both large in their size and they are very similar to or in some sense of Patrick."
        ],
        [
            "And remember that the only parameter of our algorithm is a downsampling size.",
            "So we first use the training set to test different downsampling size and the Y access of these two plots is the error rates.",
            "So the lower the better.",
            "And the X axis is different.",
            "Downsampling as we tested.",
            "And in both datasets you can find that the error rate varies only a little.",
            "Once the resolution is greater than 10 by 10, right?",
            "That means our distance measure is relatively insensitive to the downsampling size."
        ],
        [
            "And with the downsides as we gates based on the training set, we achieved a competitive accuracy to several specially designed."
        ],
        [
            "Absence.",
            "OK, finally we evaluate the scalability and to do this we made eight synthetic petroglyph.",
            "That said, they are based on 22 classic petroglyphs and he has some examples and we ask 10 volunteers to duplicate them.",
            "And finally we applied a random polynomial transform to generate the data that we want, and the largest one contains up to more than one."
        ],
        [
            "Million objects.",
            "OK, first we did leave one out.",
            "One years neighbor test and because the testing sample is randomly choose.",
            "So we repeat this test for 10 times on each data set and this is the 1st results.",
            "the Y axis is the prime rate, which is the percentage of exact distance which can be pruned.",
            "So the more the better.",
            "And as we can see, while the size of the Patrick of datasets increase the prime rate.",
            "Also increases, and especially when we exam the one with 80,000 objects.",
            "Even the minimal prune rates already exceeded the 90s, six percent.",
            "Also, we compared our method with the brute Force One, and as you can see here for the largest data set, our time is only 2% to the."
        ],
        [
            "Brute force time.",
            "And we also tested as the finding motives and again by using the triangular inequality of our distance measure, we only need to calculate a very tiny fraction of exact distance, so we can expect a huge speedup VS the brute Force One.",
            "So here is the results.",
            "And even for the smallest death that our algorithm is more than 700 times faster because we can prove more than 99% of the cake."
        ],
        [
            "Relation.",
            "OK, here comes a conclusion are in this work we considered for the first time the problem of mining large collections of rock arts and we introduce a novel distance measure based on GHT and we find efficiently computable tight lower bounds to this measure and based on these two we enabled many large that archives."
        ],
        [
            "Efficiently.",
            "OK, thanks for listening and I will be happy to answer any questions.",
            "Thank you.",
            "Katie.",
            "Yeah, work is skating violence.",
            "You mean the scale invariants, right?",
            "Because we did a downsampling to a certain size, so we can.",
            "There is a scale invariants actually for our future work we are still working on supporting the partial matching and the rotation invariants.",
            "But yes, for this work we enabled the scale invariants and also the translation invariants.",
            "Yeah.",
            "Thank you again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's my pleasure to introduce our third speaker, Mr Challenger, Mr Drew, the PhD student from University of California, Riverside, thanks.",
                    "label": 1
                },
                {
                    "sent": "Hello everyone, thanks for still being here to attend my talk because this is one of the very last session in this year's KDD.",
                    "label": 0
                },
                {
                    "sent": "So today my talk is about data mining the Patrick.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Leaves OK here.",
                    "label": 0
                },
                {
                    "sent": "As you can see are some examples of beautiful Patrick lives and the final goal of our project is to index all the words, rock routes and to enable the efficient and effective data mining on.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slaves.",
                    "label": 0
                },
                {
                    "sent": "Well, petroglyphs are one of the earliest expressions of abstract thinking, and they can provide us a rich source of information like changing of climate, existence of a certain species, or patterns of humans, migration, and.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interactions.",
                    "label": 0
                },
                {
                    "sent": "However, the progress in petroglyphs research has been very slow in the past decade.",
                    "label": 1
                },
                {
                    "sent": "We believe it is because of their extraordinary diversity and complexity, and we find most current matching algorithm cannot capture the similarity of petroglyphs.",
                    "label": 1
                },
                {
                    "sent": "So one of the key problem is to find efficient and effective dispatch.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Full Patch believe.",
                    "label": 0
                },
                {
                    "sent": "OK, I'd like to introduce our approach by answering three questions, so the first one is how to preprocess the real data.",
                    "label": 1
                },
                {
                    "sent": "While this is not our main contribution of our work, the preprocessing is a necessary step and I will focus on how we define distance measure for petroglyph and how we do the speed up.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, first, let's see why we need the preprocessing.",
                    "label": 0
                },
                {
                    "sent": "Let's take a look at this image.",
                    "label": 0
                },
                {
                    "sent": "Although it's very clear and it's in high contrast, but the border of this rock may be recognized at the age of the petroglyph by most of the automatic segmentation algorithms.",
                    "label": 1
                },
                {
                    "sent": "So to extract such data, we have built a human.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Amputation 2 codes petrol annotator are some of you may have attended the human Computation Workshop on Sunday and the essence of human computation is to outsource some certain critical steps from computers to human.",
                    "label": 0
                },
                {
                    "sent": "Such steps might be very hard or even impossible for computer on the programs, but they are very easy for humans.",
                    "label": 0
                },
                {
                    "sent": "So in our petrol notator we first loads the role image into.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our system and we ask volunteers to draw the boundary around the about objects and then trust the ship for the 1st.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Error analysis OK, here's another very important step of preprocessing.",
                    "label": 0
                },
                {
                    "sent": "It's called downsampling.",
                    "label": 0
                },
                {
                    "sent": "Here I'll give you a very simple intuition by by example.",
                    "label": 0
                },
                {
                    "sent": "We ask two volunteers to draw a bighorn sheep in our petrol notator and you can see the results on left in figure A and although they are from the same image and they do similar seems very similar but are only less than 3.5% of the pixels from each image.",
                    "label": 1
                },
                {
                    "sent": "Overlap and let's compare it with the downsampled version on the right.",
                    "label": 0
                },
                {
                    "sent": "In figure big we find more than 75% of pixels are coming to both and I will show you some experimental results in our evaluation section to show that downsampling can increase the accuracy in our objects tab.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now we are in a position to discuss about the distance measure.",
                    "label": 1
                },
                {
                    "sent": "I'm not going to do an exhaustive discussion about why we have this content.",
                    "label": 0
                },
                {
                    "sent": "May be other measures actually we have tried and compared more than 10 different measures here.",
                    "label": 0
                },
                {
                    "sent": "I'd like to make a brief review of why we have chosen the GHD which is short for generalized Hough transform because THT can makes can make no assumption about.",
                    "label": 1
                },
                {
                    "sent": "The data, and Secondly it can correctly capture the similarity of petroglyph and the third one is we can tightly lower bounds the GHT based distance as I will show you later, the original THT is time consuming.",
                    "label": 0
                },
                {
                    "sent": "So one classic data mining solution to this problem is to find a efficiently computable and tight lower bounds and use this lower bound to prune of some promising candidates.",
                    "label": 1
                },
                {
                    "sent": "So if we can find such a lower bounds, it can allow us to do the efficient searches in very large.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dead set.",
                    "label": 0
                },
                {
                    "sent": "OK, let's briefly review the classic GHT.",
                    "label": 1
                },
                {
                    "sent": "Well GHD is a useful method for 2 dimensional arbitrary ship detection and our illustrates are the basic examine by a very simple example.",
                    "label": 1
                },
                {
                    "sent": "So here as you can see it's a query image on left and candidate image, see on the right and each cell is pixel and the dark color denotes the edge points of the ship.",
                    "label": 0
                },
                {
                    "sent": "And the goal of their GHD is to find the best match between Q and see an what what I mean by best matches.",
                    "label": 0
                },
                {
                    "sent": "If I put Q on to see the overlapped eight points will be maximal between QNC so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is our goal.",
                    "label": 0
                },
                {
                    "sent": "The first step is to find the stop pattern for the query image.",
                    "label": 1
                },
                {
                    "sent": "So are we first rotate all the edge points around a reference point R, which is usually the center of mass of all age points by 180 degrees.",
                    "label": 0
                },
                {
                    "sent": "So we get this one and then we draw vectors from reference points are to each edge points.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And second step is we superimpose the star pattern onto each edge points of the candidate image like this and we also use accumulator 2 records.",
                    "label": 0
                },
                {
                    "sent": "The number of dead ends which fall into corresponding sales.",
                    "label": 0
                },
                {
                    "sent": "So after first are updating we can see four of their sales are increased by one and we just put this factor onto the remaining edge points and after four rounds.",
                    "label": 0
                },
                {
                    "sent": "We obtained a final.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Accumulator.",
                    "label": 0
                },
                {
                    "sent": "And the third step is fine at the peak of the accumulator, so it's the sale with the value three.",
                    "label": 0
                },
                {
                    "sent": "It corresponds to our priming candid images.",
                    "label": 0
                },
                {
                    "sent": "See and now if we put Q on to see by overlapping or an R prime, we can find this share three overlap points.",
                    "label": 0
                },
                {
                    "sent": "So this is the best match between Q and.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "Well, the classic GHT can correctly capture or detector the ship, but it doesn't lay explicitly encodes similarity measure, but we can simply define such GHT best distance measure, which we code minimal.",
                    "label": 1
                },
                {
                    "sent": "Unmatched Edge points anet equals to the number of eight points in Q minus the maximum matches points.",
                    "label": 1
                },
                {
                    "sent": "In our toy example, it's 4 -- 3.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just one.",
                    "label": 0
                },
                {
                    "sent": "OK, now following, I'd like to discuss about how we can find a lower bound to speed up the calculation.",
                    "label": 0
                },
                {
                    "sent": "So here comes a question.",
                    "label": 0
                },
                {
                    "sent": "When can we obtain the value of a particular cell in the accumulator?",
                    "label": 1
                },
                {
                    "sent": "In a classic GHT we need to wait until the end of all the implementation, right?",
                    "label": 1
                },
                {
                    "sent": "So is it possible to obtain the value 1 by 1?",
                    "label": 1
                },
                {
                    "sent": "I want requirement is we need to check all positions that are possible to increase the sale value.",
                    "label": 0
                },
                {
                    "sent": "We can do this by simply reverse the direction of the vectors so we obtain the new vector pattern.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here is our example.",
                    "label": 0
                },
                {
                    "sent": "So if we want to estimate lower bounds for the central column of candid image C, Our method first extract 1 dimensional signature from Q&C and the value is just simply the number of counts of edge points an after we get the signature.",
                    "label": 0
                },
                {
                    "sent": "We just align this the center of Signature QX on to the corresponding.",
                    "label": 0
                },
                {
                    "sent": "Position of Signature CX and you can imagine this like we put the star pattern of Q on to the certain sale of candid Images C and the next.",
                    "label": 0
                },
                {
                    "sent": "We just check column by column.",
                    "label": 0
                },
                {
                    "sent": "So for the first column we can see our query image needs two pixels to be matched.",
                    "label": 0
                },
                {
                    "sent": "But in the candid image there are three.",
                    "label": 0
                },
                {
                    "sent": "So in the best Case No points will be missed.",
                    "label": 0
                },
                {
                    "sent": "So the minimum is the point in this column is 0, so let's check the second one.",
                    "label": 1
                },
                {
                    "sent": "It's the same case in this column Kunitz two pixels to be matched and she has two, so the minimum is, the point is zero.",
                    "label": 0
                },
                {
                    "sent": "However, in the third column we find Q needs four pixels, but she only has two.",
                    "label": 0
                },
                {
                    "sent": "So even in the best case there will be at least two missed right?",
                    "label": 1
                },
                {
                    "sent": "So the minimum Mr Points in this column is 2 and we can do this for the following two.",
                    "label": 0
                },
                {
                    "sent": "Columns like this.",
                    "label": 1
                },
                {
                    "sent": "And after we finish all this step, we can get a submission.",
                    "label": 0
                },
                {
                    "sent": "It's equals 2 two.",
                    "label": 0
                },
                {
                    "sent": "So it's the our estimation for the lower bound on this column and we can do the same thing for all the remaining columns and we just pick the smallest one as a global lower bounds.",
                    "label": 0
                },
                {
                    "sent": "And of course if you want to get a title lower bound, we can do the same thing in the row wise.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, our because we in our lower bound THT we only need to compare 1 dimensional signature so we can reduce their time complexity and we actually we can further reduce it by only abandoned and shifting order and in our evaluation section I will show you that we can achieve one to two orders of MAG.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To speed up.",
                    "label": 0
                },
                {
                    "sent": "And MUE is a very useful basic.",
                    "label": 0
                },
                {
                    "sent": "This is special and we further did some adjustment and normalization before we apply it to several high level data mining algorithms.",
                    "label": 0
                },
                {
                    "sent": "So especially for the third one finding motives motives are.",
                    "label": 0
                },
                {
                    "sent": "Most similar pairs in a collection of objects and our distance measure for finding the motives also satisfy the triangular inequality.",
                    "label": 0
                },
                {
                    "sent": "So with this nice quality property, we can efficiently find the exact motives.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, next I'm going to evaluate our distance measure and lower bound axiom of utility and accuracy and.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Capability so we start from our very simple sanity check.",
                    "label": 0
                },
                {
                    "sent": "So here is a clustering of a typical thousand Western US petroglyph.",
                    "label": 1
                },
                {
                    "sent": "And as you can see, our distance measure, again correctly groups all seven pairs.",
                    "label": 1
                },
                {
                    "sent": "And what's more, in the high level structure of the dental Graham?",
                    "label": 0
                },
                {
                    "sent": "Our distance measure also correctly groups similar petroglyphs and more Interestingly, as you can see in their second pair, our measure is also invariant to the.",
                    "label": 0
                },
                {
                    "sent": "Hollow and solid character.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tractor.",
                    "label": 0
                },
                {
                    "sent": "OK, in their next evaluation we want to test whether our distance measure is robust to the human variability.",
                    "label": 0
                },
                {
                    "sent": "So we asked two volunteers to draw these four pairs of Patrick left in our petrol annotator and you can find some significant variation between them.",
                    "label": 0
                },
                {
                    "sent": "So especially for this Dell E the user SC draw it as our outline of the deal but the user WY draw it as a sticker.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Figure.",
                    "label": 0
                },
                {
                    "sent": "And here is the results of our by our distance measure and you can see all the applications from the same image are grouped together and also including the one I mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the third evaluation, we want to test whether our distance special can find meaningful motives.",
                    "label": 1
                },
                {
                    "sent": "So we did a test on the collection of real petroglyphs and they are prospered more than 4 million pairs.",
                    "label": 0
                },
                {
                    "sent": "An here is all the pairwise distance, and if we set value 40 as a motive cut off and we want to find all the pairs with the distance that is smaller than this motive cut off.",
                    "label": 0
                },
                {
                    "sent": "If we use a brute Force One, we need to calculate all these 4 million possible pairs, right, but?",
                    "label": 0
                },
                {
                    "sent": "If we use our distance measure with the triangular inequality, we only need to calculate a tiny fraction of all the calculations.",
                    "label": 1
                },
                {
                    "sent": "So we quickly find all the 52 top motives.",
                    "label": 0
                },
                {
                    "sent": "By this motive cutoff, an here are five representative from them, and as you can see they are visually visually very similar and we can see some useful in variation and more Interestingly some of them are in fact known to be true meaning.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formatives OK, because there are no large collections of Patrick life with labels, so we to evaluate the accuracy of our distance measure.",
                    "label": 0
                },
                {
                    "sent": "We tested on two publicly available datasets and as you can see, these two datasets are both large in their size and they are very similar to or in some sense of Patrick.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And remember that the only parameter of our algorithm is a downsampling size.",
                    "label": 0
                },
                {
                    "sent": "So we first use the training set to test different downsampling size and the Y access of these two plots is the error rates.",
                    "label": 0
                },
                {
                    "sent": "So the lower the better.",
                    "label": 0
                },
                {
                    "sent": "And the X axis is different.",
                    "label": 0
                },
                {
                    "sent": "Downsampling as we tested.",
                    "label": 0
                },
                {
                    "sent": "And in both datasets you can find that the error rate varies only a little.",
                    "label": 1
                },
                {
                    "sent": "Once the resolution is greater than 10 by 10, right?",
                    "label": 1
                },
                {
                    "sent": "That means our distance measure is relatively insensitive to the downsampling size.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with the downsides as we gates based on the training set, we achieved a competitive accuracy to several specially designed.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Absence.",
                    "label": 0
                },
                {
                    "sent": "OK, finally we evaluate the scalability and to do this we made eight synthetic petroglyph.",
                    "label": 1
                },
                {
                    "sent": "That said, they are based on 22 classic petroglyphs and he has some examples and we ask 10 volunteers to duplicate them.",
                    "label": 1
                },
                {
                    "sent": "And finally we applied a random polynomial transform to generate the data that we want, and the largest one contains up to more than one.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Million objects.",
                    "label": 0
                },
                {
                    "sent": "OK, first we did leave one out.",
                    "label": 0
                },
                {
                    "sent": "One years neighbor test and because the testing sample is randomly choose.",
                    "label": 0
                },
                {
                    "sent": "So we repeat this test for 10 times on each data set and this is the 1st results.",
                    "label": 1
                },
                {
                    "sent": "the Y axis is the prime rate, which is the percentage of exact distance which can be pruned.",
                    "label": 0
                },
                {
                    "sent": "So the more the better.",
                    "label": 0
                },
                {
                    "sent": "And as we can see, while the size of the Patrick of datasets increase the prime rate.",
                    "label": 0
                },
                {
                    "sent": "Also increases, and especially when we exam the one with 80,000 objects.",
                    "label": 0
                },
                {
                    "sent": "Even the minimal prune rates already exceeded the 90s, six percent.",
                    "label": 0
                },
                {
                    "sent": "Also, we compared our method with the brute Force One, and as you can see here for the largest data set, our time is only 2% to the.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Brute force time.",
                    "label": 0
                },
                {
                    "sent": "And we also tested as the finding motives and again by using the triangular inequality of our distance measure, we only need to calculate a very tiny fraction of exact distance, so we can expect a huge speedup VS the brute Force One.",
                    "label": 1
                },
                {
                    "sent": "So here is the results.",
                    "label": 1
                },
                {
                    "sent": "And even for the smallest death that our algorithm is more than 700 times faster because we can prove more than 99% of the cake.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Relation.",
                    "label": 0
                },
                {
                    "sent": "OK, here comes a conclusion are in this work we considered for the first time the problem of mining large collections of rock arts and we introduce a novel distance measure based on GHT and we find efficiently computable tight lower bounds to this measure and based on these two we enabled many large that archives.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Efficiently.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks for listening and I will be happy to answer any questions.",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Katie.",
                    "label": 0
                },
                {
                    "sent": "Yeah, work is skating violence.",
                    "label": 0
                },
                {
                    "sent": "You mean the scale invariants, right?",
                    "label": 0
                },
                {
                    "sent": "Because we did a downsampling to a certain size, so we can.",
                    "label": 1
                },
                {
                    "sent": "There is a scale invariants actually for our future work we are still working on supporting the partial matching and the rotation invariants.",
                    "label": 0
                },
                {
                    "sent": "But yes, for this work we enabled the scale invariants and also the translation invariants.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Thank you again.",
                    "label": 0
                }
            ]
        }
    }
}