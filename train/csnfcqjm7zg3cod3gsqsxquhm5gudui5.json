{
    "id": "csnfcqjm7zg3cod3gsqsxquhm5gudui5",
    "title": "Automatic Image Annotation Using Group Sparsity",
    "info": {
        "author": [
            "Shaoting Zhang, Department of Computer Science, Rutgers, The State University of New Jersey"
        ],
        "published": "July 19, 2010",
        "recorded": "June 2010",
        "category": [
            "Top->Computer Science->Computer Vision->Image & Video Retrieval"
        ]
    },
    "url": "http://videolectures.net/cvpr2010_zhang_aiau/",
    "segmentation": [
        [
            "Hello everyone, my name is Shelton John.",
            "My advisor is diminishing my taxes from Rogers.",
            "I work is to annotate images using group sparsity."
        ],
        [
            "The goal of image annotation is to assign relevant keywords to any given image and we hope those images can be as simple as part of those keywords can be as simple as possible to human annotation, like this example.",
            "The first row is predicted keywords from our method.",
            "The same role is the Hue."
        ],
        [
            "The annotation alot of previous methods have been proposed.",
            "We only listen to some of them here.",
            "Most of them try to solve the so-called semantic gap problem.",
            "We limitation is that features are often pre selected.",
            "So the properties of features are not well studied.",
            "This limitation motivates our work.",
            "We try to solve the annotation problem as a as a feature selection one and we use clustering prior and sparsity prior to guide the selection.",
            "Here is the outline.",
            "First we will introduce how to formulate the annotation framework, let's say."
        ],
        [
            "It's a feature selection one and also where we introduce how to use different types of priors."
        ],
        [
            "Assuming we have similar and dissimilar image pairs P1 and P2, Please note that the similarity here is not only depends on the feature distance, but also depends on the keywords.",
            "We don't have that as ground truth.",
            "We will come back to this topic later and discuss how to get some.",
            "For now, we assume we have it.",
            "Then we have the corresponding feature matrix.",
            "FP1 and FP2.",
            "Each role of the feature matrix is the feature vector for a specific image.",
            "We did find the difference between these two feature matrix."
        ],
        [
            "To the axe, we also define a target value.",
            "If two images are similar than the target value will be 1, otherwise it will be negative one.",
            "Of course we can't have this this equation unless we multiply X with which W. So the feature selection just means we want to find a better weights W by minimizing the difference between XWNY for the removing of features.",
            "Assuming we have elements in the W to be 0, then we can remove the whole corresponding.",
            "Conning that's difference matrix, so on so fast.",
            "It just means that we can remove the corresponding calling in the origin originel feature matrix.",
            "For the annotation went gave."
        ],
        [
            "A new test image.",
            "We first duplicate it into a whole matrix, then guess the difference between the matrix with the whole training data set.",
            "Then wait times with the weights, get similarity since the similarities based on the keyword information, we just transfer the keywords from the highest similarities to the test image.",
            "That's how we do the annotation here.",
            "Now we will."
        ],
        [
            "Choose how to add the priors to guide us in action.",
            "The first prior can be able to NOM applied on the weights W since the original formula, the X maybe singular, so we may not get a good solution, but after those error 2 number we can guarantee to have a robust solution.",
            "The problem is that these types of prior prefer only small values off weight, but does not necessarily produce sparse solution."
        ],
        [
            "So we also tried L1 norm here.",
            "This one is convex optimization and we can use different types of methods to get a good solution, and this one will produce smart solution which means most of the elements will be zero.",
            "We prefer such types of the results because we can save time and storage, combine them together we can get."
        ],
        [
            "Do an AMA suchart group sparsity, which was proposing the statistical community in 2006.",
            "So we first manually divides the features.",
            "Two different groups denote SG-12 GM and the Basic Idea interchange the intuition is that we use L1 norm inside of the same group and use their own norm for different groups.",
            "When good thing for these types of definition is such.",
            "We can remove the whole group in the same time, like in this case we can remove the whole RGB.",
            "Or the whole group of the future will be nonzero.",
            "Compared to less, so this one is good because even in the testing stage we do not need these types of feature seems time to extract the features.",
            "And we tried to different types of online sort of the servers to deal with this problem.",
            "Most of them gets the same accuracy.",
            "The only difference is the speech.",
            "But since that's the testing or that's the training stage, so the speech is not a very important factor.",
            "Now we."
        ],
        [
            "I will move on to the obtain the image appears since that's a crucial, is a crucial step for our framework.",
            "The previous methods only."
        ],
        [
            "Relies on the keyword similarity, such as.",
            "If two images has more than three similar keywords, then they're similar pair.",
            "If they do not share any common keywords, then just similar.",
            "These types of definition may induce a lot of noise, like in the left figure.",
            "It's the histogram of distance of the similar pairs will see most of the distance, small values, but some of them are very large.",
            "The right figure are the histogram of both similar and dissimilar pairs will see two types of.",
            "The samples mixed with each other.",
            "Since the classification is still based on the feature distance, so these types of the histogram will be very difficult to deal with using these types of method, we achieve about 15% accuracy only.",
            "Our method using."
        ],
        [
            "Side by the relevance feedback and the method.",
            "Assuming that we get K1 nearest neighbor and K2 fathers neighbors first as the candidates for the similar herzan just similar pairs.",
            "Then we prove them according to the feedback information such as the keywords or the category information.",
            "If they do not share the same key, what are they?",
            "They're not in the same category.",
            "Then we just remove them and we use that as."
        ],
        [
            "This acts and calculates the W and we use the W to find those candidates one more time and repeat this procedure.",
            "We didn't prove that this."
        ],
        [
            "Procedure will converge, but in real practice, after three iterations we achieve much better result like this.",
            "In this case, the left figure most of them."
        ],
        [
            "My source removed and put two types of samples together.",
            "We can still see a gap here, and these types of image pairs can improve the performance of our method.",
            "Finally, let's move on to the experiment.",
            "We did extensive experiments here, but we only put some of them here for more experiment results, please come to our post session for the data set, we use the current data set which has."
        ],
        [
            "I've sound images and the IPL data set which has 20,000 images, not starts for the API data set.",
            "We do not have any ground shoes for the keyword annotation.",
            "We only have a description for this image, so we need some pre process to get these keywords.",
            "Full evaluation we reported the average precision recall per keyword and also the keywords record denoted as N plus.",
            "For the features we use, we tested a lot of features such as different types of color histogram from the whole image.",
            "Or the same kind of histogram from ceiling zoning, like the local information.",
            "And also a lot of texture and appearance information.",
            "We reported the performance of each single features and feature combinations and a lot of interesting thing happened there.",
            "So put that in the post.",
            "Let's move on to the comparisons of the methods directly.",
            "Recall that we use Lambda to control the importance of the prior."
        ],
        [
            "So the X axis will be the changing of the land value from one to 100 and the Y axis will be the performance such as precision recalling N plus.",
            "The blue line is average weights, which means we use all of the features and we assume that the equally contributed to the performance.",
            "It's quite stable because we use everything the green line is from the least square solution, which means we do not have any prior.",
            "It works relatively well."
        ],
        [
            "Here in the core data set, but in the IP in UNIPI data set it does not do a good job.",
            "The Black Line is the national solution.",
            "The not so produce can produce over over sparse solution, which means we can only have few names of non sales elements which sacrificed the performance.",
            "L2 norm did fairly well but it does not produce any sparsity we.",
            "For the group now, so it does the best."
        ],
        [
            "And also it's less sensitive through the tuning parameter that we has.",
            "We use the structure prior.",
            "Awesome yeah so.",
            "Now it's the evaluation for the generic T. We compute the weights from the current data set, then apply them directly through the IPL data set to see what we will see.",
            "So the result is surprisingly good.",
            "It's even better than using the weights calculated from the API directly.",
            "That's because in the current assets we have both types of information.",
            "The keyword information and also the category information using both of them we can discover batch repairs using our in our method, which improves the performance.",
            "Finally, put something for the annotate."
        ],
        [
            "Showing results since every time we transfer 5 keywords to the input image, but human annotation, we only have two 2 three keywords, so we always have more keywords than the human annotation.",
            "Some of them are just redundancy like the bird in this in English image, because it's it's it's nearest neighbors containing those keywords, so it also has it we do not create a semantic semantic model, so we have such problem, But some of them make sense.",
            "Can you explain the image well like this term here?",
            "Although it's not in the human annotation.",
            "For the conclusion, so we proposed a feature selection framework using both sparsity and clustering priors.",
            "In our experiments, RGB was removed how feature was removed and the hug feature was removed, but we do not want to see that these features are not important.",
            "It depends on the feature combination where use.",
            "Assuming large without the SIFT feature hug feature will have a very high weights here, so we propose that we just leave everything to this framework.",
            "This framework will select the best features for us and also we probably prefer this past solution since it can improve the scalability here and also the image appears performs much better than the previous method for the future work we we can still have a lot of work to do like test on different grouping methods.",
            "For now we just define the group as the feature types so we can automatically find groups instead of manually.",
            "We can try more priors combined with other methods.",
            "Also, can you extend this framework to other types of tasks?",
            "So thanks for listening and welcome to the post session.",
            "Questions.",
            "So I have a question in your objective function, you're taking linear combinations.",
            "Of the image features, yeah, But if you were to.",
            "Do some nonlinear combinations.",
            "How would your method change?",
            "Let's say very good point.",
            "Actually that's the same stuff we want to try in the future.",
            "For now, we're just using something like L1 distance to combine them together.",
            "It's a linear combination and we do want to try some nonlinear steps singlepoint.",
            "Anymore question.",
            "Alright, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everyone, my name is Shelton John.",
                    "label": 0
                },
                {
                    "sent": "My advisor is diminishing my taxes from Rogers.",
                    "label": 0
                },
                {
                    "sent": "I work is to annotate images using group sparsity.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The goal of image annotation is to assign relevant keywords to any given image and we hope those images can be as simple as part of those keywords can be as simple as possible to human annotation, like this example.",
                    "label": 1
                },
                {
                    "sent": "The first row is predicted keywords from our method.",
                    "label": 0
                },
                {
                    "sent": "The same role is the Hue.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The annotation alot of previous methods have been proposed.",
                    "label": 0
                },
                {
                    "sent": "We only listen to some of them here.",
                    "label": 0
                },
                {
                    "sent": "Most of them try to solve the so-called semantic gap problem.",
                    "label": 0
                },
                {
                    "sent": "We limitation is that features are often pre selected.",
                    "label": 0
                },
                {
                    "sent": "So the properties of features are not well studied.",
                    "label": 1
                },
                {
                    "sent": "This limitation motivates our work.",
                    "label": 0
                },
                {
                    "sent": "We try to solve the annotation problem as a as a feature selection one and we use clustering prior and sparsity prior to guide the selection.",
                    "label": 1
                },
                {
                    "sent": "Here is the outline.",
                    "label": 0
                },
                {
                    "sent": "First we will introduce how to formulate the annotation framework, let's say.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a feature selection one and also where we introduce how to use different types of priors.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assuming we have similar and dissimilar image pairs P1 and P2, Please note that the similarity here is not only depends on the feature distance, but also depends on the keywords.",
                    "label": 0
                },
                {
                    "sent": "We don't have that as ground truth.",
                    "label": 0
                },
                {
                    "sent": "We will come back to this topic later and discuss how to get some.",
                    "label": 0
                },
                {
                    "sent": "For now, we assume we have it.",
                    "label": 0
                },
                {
                    "sent": "Then we have the corresponding feature matrix.",
                    "label": 0
                },
                {
                    "sent": "FP1 and FP2.",
                    "label": 0
                },
                {
                    "sent": "Each role of the feature matrix is the feature vector for a specific image.",
                    "label": 0
                },
                {
                    "sent": "We did find the difference between these two feature matrix.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the axe, we also define a target value.",
                    "label": 0
                },
                {
                    "sent": "If two images are similar than the target value will be 1, otherwise it will be negative one.",
                    "label": 0
                },
                {
                    "sent": "Of course we can't have this this equation unless we multiply X with which W. So the feature selection just means we want to find a better weights W by minimizing the difference between XWNY for the removing of features.",
                    "label": 0
                },
                {
                    "sent": "Assuming we have elements in the W to be 0, then we can remove the whole corresponding.",
                    "label": 0
                },
                {
                    "sent": "Conning that's difference matrix, so on so fast.",
                    "label": 0
                },
                {
                    "sent": "It just means that we can remove the corresponding calling in the origin originel feature matrix.",
                    "label": 0
                },
                {
                    "sent": "For the annotation went gave.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A new test image.",
                    "label": 0
                },
                {
                    "sent": "We first duplicate it into a whole matrix, then guess the difference between the matrix with the whole training data set.",
                    "label": 1
                },
                {
                    "sent": "Then wait times with the weights, get similarity since the similarities based on the keyword information, we just transfer the keywords from the highest similarities to the test image.",
                    "label": 0
                },
                {
                    "sent": "That's how we do the annotation here.",
                    "label": 0
                },
                {
                    "sent": "Now we will.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Choose how to add the priors to guide us in action.",
                    "label": 0
                },
                {
                    "sent": "The first prior can be able to NOM applied on the weights W since the original formula, the X maybe singular, so we may not get a good solution, but after those error 2 number we can guarantee to have a robust solution.",
                    "label": 0
                },
                {
                    "sent": "The problem is that these types of prior prefer only small values off weight, but does not necessarily produce sparse solution.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we also tried L1 norm here.",
                    "label": 0
                },
                {
                    "sent": "This one is convex optimization and we can use different types of methods to get a good solution, and this one will produce smart solution which means most of the elements will be zero.",
                    "label": 0
                },
                {
                    "sent": "We prefer such types of the results because we can save time and storage, combine them together we can get.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do an AMA suchart group sparsity, which was proposing the statistical community in 2006.",
                    "label": 1
                },
                {
                    "sent": "So we first manually divides the features.",
                    "label": 0
                },
                {
                    "sent": "Two different groups denote SG-12 GM and the Basic Idea interchange the intuition is that we use L1 norm inside of the same group and use their own norm for different groups.",
                    "label": 1
                },
                {
                    "sent": "When good thing for these types of definition is such.",
                    "label": 0
                },
                {
                    "sent": "We can remove the whole group in the same time, like in this case we can remove the whole RGB.",
                    "label": 0
                },
                {
                    "sent": "Or the whole group of the future will be nonzero.",
                    "label": 0
                },
                {
                    "sent": "Compared to less, so this one is good because even in the testing stage we do not need these types of feature seems time to extract the features.",
                    "label": 0
                },
                {
                    "sent": "And we tried to different types of online sort of the servers to deal with this problem.",
                    "label": 0
                },
                {
                    "sent": "Most of them gets the same accuracy.",
                    "label": 0
                },
                {
                    "sent": "The only difference is the speech.",
                    "label": 0
                },
                {
                    "sent": "But since that's the testing or that's the training stage, so the speech is not a very important factor.",
                    "label": 0
                },
                {
                    "sent": "Now we.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will move on to the obtain the image appears since that's a crucial, is a crucial step for our framework.",
                    "label": 0
                },
                {
                    "sent": "The previous methods only.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Relies on the keyword similarity, such as.",
                    "label": 1
                },
                {
                    "sent": "If two images has more than three similar keywords, then they're similar pair.",
                    "label": 0
                },
                {
                    "sent": "If they do not share any common keywords, then just similar.",
                    "label": 0
                },
                {
                    "sent": "These types of definition may induce a lot of noise, like in the left figure.",
                    "label": 1
                },
                {
                    "sent": "It's the histogram of distance of the similar pairs will see most of the distance, small values, but some of them are very large.",
                    "label": 0
                },
                {
                    "sent": "The right figure are the histogram of both similar and dissimilar pairs will see two types of.",
                    "label": 0
                },
                {
                    "sent": "The samples mixed with each other.",
                    "label": 0
                },
                {
                    "sent": "Since the classification is still based on the feature distance, so these types of the histogram will be very difficult to deal with using these types of method, we achieve about 15% accuracy only.",
                    "label": 0
                },
                {
                    "sent": "Our method using.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Side by the relevance feedback and the method.",
                    "label": 1
                },
                {
                    "sent": "Assuming that we get K1 nearest neighbor and K2 fathers neighbors first as the candidates for the similar herzan just similar pairs.",
                    "label": 0
                },
                {
                    "sent": "Then we prove them according to the feedback information such as the keywords or the category information.",
                    "label": 0
                },
                {
                    "sent": "If they do not share the same key, what are they?",
                    "label": 0
                },
                {
                    "sent": "They're not in the same category.",
                    "label": 0
                },
                {
                    "sent": "Then we just remove them and we use that as.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This acts and calculates the W and we use the W to find those candidates one more time and repeat this procedure.",
                    "label": 0
                },
                {
                    "sent": "We didn't prove that this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Procedure will converge, but in real practice, after three iterations we achieve much better result like this.",
                    "label": 0
                },
                {
                    "sent": "In this case, the left figure most of them.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My source removed and put two types of samples together.",
                    "label": 0
                },
                {
                    "sent": "We can still see a gap here, and these types of image pairs can improve the performance of our method.",
                    "label": 0
                },
                {
                    "sent": "Finally, let's move on to the experiment.",
                    "label": 0
                },
                {
                    "sent": "We did extensive experiments here, but we only put some of them here for more experiment results, please come to our post session for the data set, we use the current data set which has.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I've sound images and the IPL data set which has 20,000 images, not starts for the API data set.",
                    "label": 0
                },
                {
                    "sent": "We do not have any ground shoes for the keyword annotation.",
                    "label": 0
                },
                {
                    "sent": "We only have a description for this image, so we need some pre process to get these keywords.",
                    "label": 0
                },
                {
                    "sent": "Full evaluation we reported the average precision recall per keyword and also the keywords record denoted as N plus.",
                    "label": 1
                },
                {
                    "sent": "For the features we use, we tested a lot of features such as different types of color histogram from the whole image.",
                    "label": 0
                },
                {
                    "sent": "Or the same kind of histogram from ceiling zoning, like the local information.",
                    "label": 0
                },
                {
                    "sent": "And also a lot of texture and appearance information.",
                    "label": 0
                },
                {
                    "sent": "We reported the performance of each single features and feature combinations and a lot of interesting thing happened there.",
                    "label": 0
                },
                {
                    "sent": "So put that in the post.",
                    "label": 0
                },
                {
                    "sent": "Let's move on to the comparisons of the methods directly.",
                    "label": 0
                },
                {
                    "sent": "Recall that we use Lambda to control the importance of the prior.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the X axis will be the changing of the land value from one to 100 and the Y axis will be the performance such as precision recalling N plus.",
                    "label": 0
                },
                {
                    "sent": "The blue line is average weights, which means we use all of the features and we assume that the equally contributed to the performance.",
                    "label": 0
                },
                {
                    "sent": "It's quite stable because we use everything the green line is from the least square solution, which means we do not have any prior.",
                    "label": 0
                },
                {
                    "sent": "It works relatively well.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here in the core data set, but in the IP in UNIPI data set it does not do a good job.",
                    "label": 0
                },
                {
                    "sent": "The Black Line is the national solution.",
                    "label": 0
                },
                {
                    "sent": "The not so produce can produce over over sparse solution, which means we can only have few names of non sales elements which sacrificed the performance.",
                    "label": 0
                },
                {
                    "sent": "L2 norm did fairly well but it does not produce any sparsity we.",
                    "label": 0
                },
                {
                    "sent": "For the group now, so it does the best.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also it's less sensitive through the tuning parameter that we has.",
                    "label": 0
                },
                {
                    "sent": "We use the structure prior.",
                    "label": 0
                },
                {
                    "sent": "Awesome yeah so.",
                    "label": 0
                },
                {
                    "sent": "Now it's the evaluation for the generic T. We compute the weights from the current data set, then apply them directly through the IPL data set to see what we will see.",
                    "label": 0
                },
                {
                    "sent": "So the result is surprisingly good.",
                    "label": 0
                },
                {
                    "sent": "It's even better than using the weights calculated from the API directly.",
                    "label": 0
                },
                {
                    "sent": "That's because in the current assets we have both types of information.",
                    "label": 0
                },
                {
                    "sent": "The keyword information and also the category information using both of them we can discover batch repairs using our in our method, which improves the performance.",
                    "label": 0
                },
                {
                    "sent": "Finally, put something for the annotate.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Showing results since every time we transfer 5 keywords to the input image, but human annotation, we only have two 2 three keywords, so we always have more keywords than the human annotation.",
                    "label": 0
                },
                {
                    "sent": "Some of them are just redundancy like the bird in this in English image, because it's it's it's nearest neighbors containing those keywords, so it also has it we do not create a semantic semantic model, so we have such problem, But some of them make sense.",
                    "label": 0
                },
                {
                    "sent": "Can you explain the image well like this term here?",
                    "label": 0
                },
                {
                    "sent": "Although it's not in the human annotation.",
                    "label": 0
                },
                {
                    "sent": "For the conclusion, so we proposed a feature selection framework using both sparsity and clustering priors.",
                    "label": 0
                },
                {
                    "sent": "In our experiments, RGB was removed how feature was removed and the hug feature was removed, but we do not want to see that these features are not important.",
                    "label": 0
                },
                {
                    "sent": "It depends on the feature combination where use.",
                    "label": 0
                },
                {
                    "sent": "Assuming large without the SIFT feature hug feature will have a very high weights here, so we propose that we just leave everything to this framework.",
                    "label": 0
                },
                {
                    "sent": "This framework will select the best features for us and also we probably prefer this past solution since it can improve the scalability here and also the image appears performs much better than the previous method for the future work we we can still have a lot of work to do like test on different grouping methods.",
                    "label": 0
                },
                {
                    "sent": "For now we just define the group as the feature types so we can automatically find groups instead of manually.",
                    "label": 0
                },
                {
                    "sent": "We can try more priors combined with other methods.",
                    "label": 0
                },
                {
                    "sent": "Also, can you extend this framework to other types of tasks?",
                    "label": 0
                },
                {
                    "sent": "So thanks for listening and welcome to the post session.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "So I have a question in your objective function, you're taking linear combinations.",
                    "label": 0
                },
                {
                    "sent": "Of the image features, yeah, But if you were to.",
                    "label": 0
                },
                {
                    "sent": "Do some nonlinear combinations.",
                    "label": 0
                },
                {
                    "sent": "How would your method change?",
                    "label": 0
                },
                {
                    "sent": "Let's say very good point.",
                    "label": 0
                },
                {
                    "sent": "Actually that's the same stuff we want to try in the future.",
                    "label": 0
                },
                {
                    "sent": "For now, we're just using something like L1 distance to combine them together.",
                    "label": 0
                },
                {
                    "sent": "It's a linear combination and we do want to try some nonlinear steps singlepoint.",
                    "label": 0
                },
                {
                    "sent": "Anymore question.",
                    "label": 0
                },
                {
                    "sent": "Alright, thank you.",
                    "label": 0
                }
            ]
        }
    }
}