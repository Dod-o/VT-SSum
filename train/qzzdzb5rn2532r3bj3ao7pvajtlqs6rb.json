{
    "id": "qzzdzb5rn2532r3bj3ao7pvajtlqs6rb",
    "title": "Minimax Fixed-Design Linear Regression",
    "info": {
        "author": [
            "Alan Malek, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_malek_linear_regression/",
    "segmentation": [
        [
            "This is joint work with Peter Bartlet, founder, cooling Agent, Akimoto, and manage Warmoth.",
            "And this talk is about minimax fixes and linear regression.",
            "So let's go through what that means piece by piece."
        ],
        [
            "So linear regression.",
            "I'm sure you're all familiar with you have a data sequence of covariates X an labels why?",
            "And your goal is to find the best linear predictor.",
            "Best meaning that it minimizes the sum of squared residuals, and we all know the classical result that the best linear predictor hindsight is a pseudo inverse of the data matrix X times your labels Y.",
            "Now what does fix design online linear regression mean well?",
            "Fix design means that the covariates X one through XT are known from the start.",
            "And we need in the online part means we need to predict yhat T having only seen the wise of the past Y one through Y T -- 1 right and again we want to penalize ourselves with the squared Euclidean distance.",
            "So just to be explicit, here is the game protocol."
        ],
        [
            "Given some covariates sequence X, one through XT for every round T first learner, predict some yhat.",
            "Then the adversary reveals some YT and then the learner incurs the squared distance between them.",
            "An like in most online learning application we want to control the regret and a lot of you may know that there are plenty of strategies followed.",
            "Leader will get the asymptotic correct rate of log T, but in this paper we're concerned with the minimax regret."
        ],
        [
            "This is not to say the minimax rate, but the actual minimax regret, which is the strongest guarantee we can kind of make about the regret over algorithm, specifically the minimax algorithm has regret exactly equal to this expression, and to sort of summarize what all these min and Max is mean, it says that if you have any other algorithm, you can make it suffer at least as much regret as a minimax algorithm.",
            "So here we have the standard regret.",
            "This is the loss of the algorithm.",
            "This is the loss of the competitor.",
            "Again, the competitor is the same.",
            "Lambda or sorry the same thing that we saw on slide one and this is the minimax formulation and generally for calculating minimax strategies and getting a strategy that achieves regret, you can always proceed by backwards induction and resolve them in's and the Max is computationally, but in general this creates an exponential blowup in the complexity.",
            "For example normalized maximum likelihood has complexity that's generally exponential in T and the question we wanted to answer in this.",
            "Presentation or in this paper is does the minimax strategy for fixes on linear regression have a nice sufficient solution?",
            "And you can probably guess the answer is yes, or I wouldn't be talking to you about it, but it turns out actually that is linear, that your minimax.",
            "Reggie plays Yhat T as some St, which is just a representation of the past data times some coefficient matrix P times occur in covariant, and if you look at the.",
            "Form of this PT.",
            "It's actually really nice if you look at the inverse.",
            "The first term is what you would expect by doing just online least squares.",
            "It's the outer product of the past data you've seen, but the minimax ride, you also has a second term which is a re weighted future sequence of the future covariates that you will see but haven't seen yet.",
            "So that's sort of evidence that this algorithms hedging against the future and therefore getting the minimax rate correct.",
            "I'd like to emphasize that this is computationally tractable because you compute.",
            "All these PTS.",
            "There's a nice recursive formula for them.",
            "You can compute all of them before you see any data there.",
            "Only a function of the covariates, and that the dependence on the past is only through this S. And this is just a very simple past.",
            "Data times past labels, so it's very, very simple to compute.",
            "And the sort of minimax guarantees we get our well being two of AM.",
            "One is that if you assume that the Whitey's are bounded, there's lemon condition needs to hold.",
            "But more specifically, we actually show that for any or."
        ],
        [
            "Terry sequence of Whitey's.",
            "Our algorithm will get regret exactly equal to this expression.",
            "It's a quadratic form with your label size and then this ex CPT XT which.",
            "Is the dependent only on the covariance and this or algorithms?",
            "Minimax in this set?",
            "Which is to say that if you look at all whitey's that belong to the set and if you look at any other algorithm you can find a sequence of whitey's that cause the other algorithm to experience at least as much regret as ours.",
            "And once you have this expression, it kind of shed some light onto the update for PT that I copied over from the previous slide.",
            "This sort of future related potential is a function of the future potential regret.",
            "This XQPQXQ is how much regret you could incur, potentially in round Q anwari waiting by that.",
            "So again, the minimax strategy doesn't depend on R and it sufficient.",
            "Can we also show that our grows approximately order log of T?",
            "So if you want to know how all the math works out, you're having any questions, please come visit us at the posted.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is joint work with Peter Bartlet, founder, cooling Agent, Akimoto, and manage Warmoth.",
                    "label": 0
                },
                {
                    "sent": "And this talk is about minimax fixes and linear regression.",
                    "label": 1
                },
                {
                    "sent": "So let's go through what that means piece by piece.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So linear regression.",
                    "label": 0
                },
                {
                    "sent": "I'm sure you're all familiar with you have a data sequence of covariates X an labels why?",
                    "label": 0
                },
                {
                    "sent": "And your goal is to find the best linear predictor.",
                    "label": 0
                },
                {
                    "sent": "Best meaning that it minimizes the sum of squared residuals, and we all know the classical result that the best linear predictor hindsight is a pseudo inverse of the data matrix X times your labels Y.",
                    "label": 0
                },
                {
                    "sent": "Now what does fix design online linear regression mean well?",
                    "label": 0
                },
                {
                    "sent": "Fix design means that the covariates X one through XT are known from the start.",
                    "label": 1
                },
                {
                    "sent": "And we need in the online part means we need to predict yhat T having only seen the wise of the past Y one through Y T -- 1 right and again we want to penalize ourselves with the squared Euclidean distance.",
                    "label": 1
                },
                {
                    "sent": "So just to be explicit, here is the game protocol.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Given some covariates sequence X, one through XT for every round T first learner, predict some yhat.",
                    "label": 0
                },
                {
                    "sent": "Then the adversary reveals some YT and then the learner incurs the squared distance between them.",
                    "label": 1
                },
                {
                    "sent": "An like in most online learning application we want to control the regret and a lot of you may know that there are plenty of strategies followed.",
                    "label": 0
                },
                {
                    "sent": "Leader will get the asymptotic correct rate of log T, but in this paper we're concerned with the minimax regret.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is not to say the minimax rate, but the actual minimax regret, which is the strongest guarantee we can kind of make about the regret over algorithm, specifically the minimax algorithm has regret exactly equal to this expression, and to sort of summarize what all these min and Max is mean, it says that if you have any other algorithm, you can make it suffer at least as much regret as a minimax algorithm.",
                    "label": 0
                },
                {
                    "sent": "So here we have the standard regret.",
                    "label": 0
                },
                {
                    "sent": "This is the loss of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is the loss of the competitor.",
                    "label": 0
                },
                {
                    "sent": "Again, the competitor is the same.",
                    "label": 0
                },
                {
                    "sent": "Lambda or sorry the same thing that we saw on slide one and this is the minimax formulation and generally for calculating minimax strategies and getting a strategy that achieves regret, you can always proceed by backwards induction and resolve them in's and the Max is computationally, but in general this creates an exponential blowup in the complexity.",
                    "label": 0
                },
                {
                    "sent": "For example normalized maximum likelihood has complexity that's generally exponential in T and the question we wanted to answer in this.",
                    "label": 0
                },
                {
                    "sent": "Presentation or in this paper is does the minimax strategy for fixes on linear regression have a nice sufficient solution?",
                    "label": 1
                },
                {
                    "sent": "And you can probably guess the answer is yes, or I wouldn't be talking to you about it, but it turns out actually that is linear, that your minimax.",
                    "label": 0
                },
                {
                    "sent": "Reggie plays Yhat T as some St, which is just a representation of the past data times some coefficient matrix P times occur in covariant, and if you look at the.",
                    "label": 0
                },
                {
                    "sent": "Form of this PT.",
                    "label": 0
                },
                {
                    "sent": "It's actually really nice if you look at the inverse.",
                    "label": 1
                },
                {
                    "sent": "The first term is what you would expect by doing just online least squares.",
                    "label": 0
                },
                {
                    "sent": "It's the outer product of the past data you've seen, but the minimax ride, you also has a second term which is a re weighted future sequence of the future covariates that you will see but haven't seen yet.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of evidence that this algorithms hedging against the future and therefore getting the minimax rate correct.",
                    "label": 0
                },
                {
                    "sent": "I'd like to emphasize that this is computationally tractable because you compute.",
                    "label": 0
                },
                {
                    "sent": "All these PTS.",
                    "label": 0
                },
                {
                    "sent": "There's a nice recursive formula for them.",
                    "label": 0
                },
                {
                    "sent": "You can compute all of them before you see any data there.",
                    "label": 0
                },
                {
                    "sent": "Only a function of the covariates, and that the dependence on the past is only through this S. And this is just a very simple past.",
                    "label": 0
                },
                {
                    "sent": "Data times past labels, so it's very, very simple to compute.",
                    "label": 0
                },
                {
                    "sent": "And the sort of minimax guarantees we get our well being two of AM.",
                    "label": 0
                },
                {
                    "sent": "One is that if you assume that the Whitey's are bounded, there's lemon condition needs to hold.",
                    "label": 0
                },
                {
                    "sent": "But more specifically, we actually show that for any or.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Terry sequence of Whitey's.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm will get regret exactly equal to this expression.",
                    "label": 0
                },
                {
                    "sent": "It's a quadratic form with your label size and then this ex CPT XT which.",
                    "label": 0
                },
                {
                    "sent": "Is the dependent only on the covariance and this or algorithms?",
                    "label": 0
                },
                {
                    "sent": "Minimax in this set?",
                    "label": 0
                },
                {
                    "sent": "Which is to say that if you look at all whitey's that belong to the set and if you look at any other algorithm you can find a sequence of whitey's that cause the other algorithm to experience at least as much regret as ours.",
                    "label": 0
                },
                {
                    "sent": "And once you have this expression, it kind of shed some light onto the update for PT that I copied over from the previous slide.",
                    "label": 0
                },
                {
                    "sent": "This sort of future related potential is a function of the future potential regret.",
                    "label": 0
                },
                {
                    "sent": "This XQPQXQ is how much regret you could incur, potentially in round Q anwari waiting by that.",
                    "label": 0
                },
                {
                    "sent": "So again, the minimax strategy doesn't depend on R and it sufficient.",
                    "label": 1
                },
                {
                    "sent": "Can we also show that our grows approximately order log of T?",
                    "label": 0
                },
                {
                    "sent": "So if you want to know how all the math works out, you're having any questions, please come visit us at the posted.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}