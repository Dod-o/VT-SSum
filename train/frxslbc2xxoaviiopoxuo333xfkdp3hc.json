{
    "id": "frxslbc2xxoaviiopoxuo333xfkdp3hc",
    "title": "AGDISTIS - Graph-Based Disambiguation of Named Entities using Linked Data (Best Research Paper nominee)",
    "info": {
        "author": [
            "Axel-Cyrille Ngonga Ngomo, University of Leipzig"
        ],
        "published": "Dec. 19, 2014",
        "recorded": "October 2014",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2014_ngonga_ngomo_agdistis/",
    "segmentation": [
        [
            "For those who do not know me, my name is AXA.",
            "I'm from the University of Leipzig in Germany and today I'm going to talk about artists graph based disambiguation approach for named entities, and this is joint work with Ricardo Respect.",
            "Michelle Coda, Daniel Galba, Sandro query.",
            "Oh so now and Andreas boat.",
            "That's quite a long list."
        ],
        [
            "Alright, so if you look at the web every minute you get 270,000 tweets, 41,000 Facebook posts, 500 new websites, 300 new blogs created, and so on and so forth.",
            "So basically the web is a very dynamic place, so we create a lot of data so that that's the document web.",
            "In contrast, the link data web is actually mostly static content, mostly integrated kind of data, and actually even lacks in actuality and the question.",
            "That we were interested in is how can we bridge between those two and that's the whole point of Knowledge X."
        ],
        [
            "Action, so basically what we do is we try to deploy scalable knowledge extraction algorithms, dialogues to bridge between this unstructured data and is structured data and the classical knowledge extraction approach contains 3 parts.",
            "You have named entity recognition where you have these text that comes in you want to find the entities in there.",
            "Then you want to link these entities to a reference knowledge base that's basically entity linking.",
            "They're all named entities disambiguation.",
            "That's what we're going to be concerned with in this talk.",
            "And then usually do some form of.",
            "Relation extraction."
        ],
        [
            "Now if you look at the literature pertaining to an image disambiguation, we basically have a lot of voters pointing to the fact that we still perform quite poorly on web documents and that the approaches that we have actually tend not to scare.",
            "They tend to be quite complex, some of them even going into exponential runtimes, and yet they also partly difficult to port to other languages, and those are actually the problems that we wanted to tackle.",
            "So what we wanted to do was we wanted to design an accurate knowledge base agnostic approach that had a polynomial time complexity.",
            "And was easy to port to other languages.",
            "So we wanted an approach that is simple and works well.",
            "That's basically what we were after."
        ],
        [
            "And that is what artist this is all about.",
            "It is quite a simple approach, but as I will show, it actually performs quite well and we can easily put it to other languages.",
            "The idea behind artist is actually straightforward.",
            "We assume that given some text and we have some form of named entity recognition tool that gives us the labels that were interested in the first thing that we do is actually choosing candidates.",
            "And we do that in a greedy man and explain what greedy is in the second or what I mean by greedy.",
            "Once we have a bunch of candidates, what we then do is we do a BFS with breadth first search within the reference craft that are interested in and we simply employ the hits algorithm.",
            "Then to determine the nodes for each candidate for each label.",
            "Basically, they have the highest authority, and those are the ones that we give back, so it's as straightforward as that, and as you all know that this approach should have approximately a quadratic time complexity, so event squared.",
            "That's actually what."
        ],
        [
            "Draft.",
            "So one example used to sentence Barack Obama arrived this afternoon in Washington DC.",
            "So we imagine this is our input.",
            "The first thing that we have to do is give it to a named entity recognition tool.",
            "So by default we use the Fox framework which is going to be talked about doing the last talk in this session.",
            "So stay here and we've got to get all the details.",
            "But the reason why we use it to simply be cause it outperforms the other state of the art tools respect to F measure.",
            "So basically default wise we use this.",
            "And what?"
        ],
        [
            "Else would do it would it would tell us Barack Obama is an entity in Washington DC is an entity now give me the ur.",
            "I swear these guys and throughout the examples are used."
        ],
        [
            "Arise.",
            "So as I said, the candidate generation approaches greedy, so we assume you're giving a set of labels and not supposed to return a set of candidate research resources for each label.",
            "We call our approach greedy simply because we take everything that we can get to actually generate these candidates so that there's been a paper on labels on the web of data that basically listed a long list of possible predicates someone can use.",
            "For that, we can use those, but we can also use surface forms and the like that are generated.",
            "By order, tools such as spotlight, for example, we take all these lists so we have a bunch of strings.",
            "We know how they map the resources and we basically say if a string.",
            "So basically label Maps to the label that we have that came from the named entity Recognition tool with the similarity larger than Tera, then we assume that this is a candidate is as simple as that.",
            "So if you go back to example with Barack Obama arrived this afternoon in Washington DC, while we would get would be at least of candidates, we get something like Barack Obama, the president, who will get his father.",
            "And so on.",
            "And for Washington DC, we get things as diverse as novels.",
            "And as the city of Washington, DC.",
            "So the question now is, how do we pick the right?",
            "You are out of this bunch."
        ],
        [
            "And the approach here as well, is very straightforward.",
            "What we do is we take each of these candidates and we basically expand it through a BFS.",
            "Three breadth first search.",
            "And once we've expanded the conjugate, that is all of the conjugate, we run the hits algorithm and simply choose the candidate that has the high authority across these different nodes so."
        ],
        [
            "So if we go to our example, we have these nodes as input.",
            "So we have Barack Obama senior and Barack Obama, Ralph Washington DC and the DC novel.",
            "If you want to grab breakfast search of depth, one will basically just extend Washington DC and say OK is connected.",
            "Several federal district San White House.",
            "Barack Obama will be connected with Hawaiian.",
            "Had done nothing that his mother and the novel will go to the United Kingdom, go down and so on.",
            "So for this example we have a depth of two and we basically get this graph on this graph.",
            "We then run the hits algorithm with the standard initialization.",
            "That is the hope school and authoritie.",
            "For us at one, we iterate until we get a convergence."
        ],
        [
            "And these are the scores that we get.",
            "For this example, we basically get Barack Obama.",
            "That is this guy.",
            "With the highest authorities costed two and Washington DC, that is basically a photo of that with that particular authority score so we can see these are the two ur eyes that are most probably important or."
        ],
        [
            "Valid for the input that we received."
        ],
        [
            "Alright.",
            "That's that simple.",
            "The question is, does it work?",
            "Cause if it works, basically would have approaches we can throw in really large documents and actually get your eyes.",
            "We need time that makes sense.",
            "So we wanted to measure the accuracy of activities on different languages.",
            "'cause we also said we want to put it in different languages and as baseline we use state of the art frameworks such as Ida Spotlight and tag Me Too.",
            "Those are basically frameworks that are widely used or basically came up as winners in other competitions such as the depth of that people of 2013 by Colonel Chad R and as evaluation measure.",
            "Reduce the micro F measure simply because district are strict under Microsoft measure and we used nine different datasets in three languages.",
            "That is 7 times English, Chinese and German.",
            "And we had default settings, obviously for experiments, and that was the depth of two and enter value of 0.82."
        ],
        [
            "And we came about with these values simply by measuring how our system behaved on a different data sets.",
            "So this basically what this picture shows on the X axis you have the different values of Terra and you have the F measures for the different depths.",
            "This is basically we do not use any black squares is when we do not use any expansion.",
            "So any breakfast search and what we see is that across the different data sets the best setter value is between 0809.",
            "Actually pinpoint it to zero point.",
            "8 two, so that's the value we used and equals 2 works best across all experiments.",
            "So that's what we used as."
        ],
        [
            "In our first series of experiments, we looked at English datasets and redeployed activities on two different knowledge bases just to see whether it actually works on different knowledge bases.",
            "As we promised at the beginning of the talk and what we saw is that it works well on the PG and Jago, but it works best on TPG.",
            "We simply have to do with the topology of the knowledge base so when we do the expansion we actually get relevant information for disambiguation.",
            "We compared in this case with.",
            "Spotlight and Ida Ida works on Jago.",
            "Spotlight works on the pedia and what we saw restart on two of the datasets.",
            "That is, the Reuters data set on the RS500 data set.",
            "We outperformed these two frameworks, the third data set, the Iyago data set is basically the data set on which idea was trained and their idols perform better than we do.",
            "So we thought this is nice.",
            "These are nice results, but.",
            "Are there any benchmarks or basically benchmark datasets where we can run this approach and we did so?"
        ],
        [
            "So we looked at the benchmark datasets from the framework.",
            "Those are from the... 2013 paper by chronology at all, and we run again against the best systems there.",
            "That is, taking two and we feature spotlight.",
            "And we basically saw something very interesting.",
            "First of all, on three of the four datasets we do or perform this state of the art.",
            "So basically, as you can see here, the bold values are basically the best values.",
            "That was simply due to us having on our four data sets the best precision.",
            "So basically our main problem was recall, especially on these data set where we had quite conforming code and that simply has to do with the data value that we chose.",
            "So because we have such a high tech to value the candidates is.",
            "Are most probably across the candidates that we choose sort of right candidate, but the problem is sometimes we just chop off too many possible candidates and the recall then since so whenever we get these guys right guy, then we usually gets it right.",
            "But in many cases we don't even have him not list of candidates simply cause that add value that we choose is so high.",
            "But again, the output performs well on both datasets that we had before and in the benchmark and basically outperforms the state of the art."
        ],
        [
            "Then we asked ourselves can report this approach to other languages and we chose Chinese number one because it's a non European language and #2 simply because it's the most spoken language on the planet.",
            "So we have more than one billion native speakers.",
            "What we did is we took the data from the Curity four benchmarking question on stream benchmark.",
            "It contains 250 questions in the training data sets and 50 questions in the test data set.",
            "We took this data.",
            "We had a native speaker translate.",
            "The English questions into Chinese and basically mark the entities and we basically measured how well acted this performs there, and we obtain F measures between 65 and 70%.",
            "So basically on this language on Chinese we are sure that we perform also quite well and obviously 'cause we are at the University of Leipzig in Germany.",
            "We also."
        ],
        [
            "So run the whole thing on German.",
            "For this we basically crawl data from the Newsday Day, a portal and there were 53 documents and 627 named entities, and we run activities on this data set and compare it with Spotlight.",
            "Here, Spotlight performed quite well.",
            "We outperformed spotlight, though by 3% in F measure.",
            "What?"
        ],
        [
            "Realized during experiments was that running this whole business is quite tricky for most people.",
            "It is really, really difficult to get the datasets to convert them and to get them actually into a format that you can actually use for experiments so."
        ],
        [
            "We decided to help the community a bit and we also provide this gerbil porthole that allows you to basically evaluate your system against the benchmark datasets that we have or even against more datasets so you can either submit a data set or submit system an all you need to do is basically give us your URL and abide by the specification of the portal and it would benchmark your service for you and send you results back.",
            "So this is quite a nice tool that came out of the hole.",
            "Building act is this really?"
        ],
        [
            "And we also have a demo which is online.",
            "The URL is at the bottom of the presentation.",
            "If you want to try it out, it is there.",
            "You can use it via code or via programmatically, basically.",
            "Or you can simply try in the normal way by using the web page that we provide."
        ],
        [
            "Right?",
            "Yeah, wanted to give enough time for questions.",
            "So what I did is I presented it as it really does have a polynomial time complexity, which is what we are most interested in.",
            "We can basically send documents in there with thousands of entities an actually get results within a time that does make sense.",
            "We showed that is it is multilingual.",
            "That is you can deploy it on several different languages.",
            "We show that it works well in English on German and Chinese and we're currently deploying it on other languages.",
            "The approach has a high accuracy and as we showed it does outperform the state of the art on most of your datasets.",
            "We had a look at what we want to do in the next steps is basically include graph summarization into their approach to basically make the breakfast search perform even better.",
            "Extended to more languages as said before and maybe even combine it with other approaches that are polynomial in time complexity simply to get an approach that's even better.",
            "Best your skills well."
        ],
        [
            "We have a demo stand 79 so please do come to the demo session and have a look at our demo.",
            "Asked all the questions that you might want to ask the technical questions.",
            "How do I call this thing with curve for example and so on?",
            "Here are the links so the whole thing is open source.",
            "Please feel free to use it.",
            "Said not comments.",
            "Would love to know what you think about this software and how we can improve it.",
            "Give us your feedback and if you want to go to demo page it is here and once you've seen a wonderful demo please to vote for us will be delighted.",
            "To have your own so thank you very much for this thing."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For those who do not know me, my name is AXA.",
                    "label": 0
                },
                {
                    "sent": "I'm from the University of Leipzig in Germany and today I'm going to talk about artists graph based disambiguation approach for named entities, and this is joint work with Ricardo Respect.",
                    "label": 1
                },
                {
                    "sent": "Michelle Coda, Daniel Galba, Sandro query.",
                    "label": 0
                },
                {
                    "sent": "Oh so now and Andreas boat.",
                    "label": 0
                },
                {
                    "sent": "That's quite a long list.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so if you look at the web every minute you get 270,000 tweets, 41,000 Facebook posts, 500 new websites, 300 new blogs created, and so on and so forth.",
                    "label": 1
                },
                {
                    "sent": "So basically the web is a very dynamic place, so we create a lot of data so that that's the document web.",
                    "label": 1
                },
                {
                    "sent": "In contrast, the link data web is actually mostly static content, mostly integrated kind of data, and actually even lacks in actuality and the question.",
                    "label": 0
                },
                {
                    "sent": "That we were interested in is how can we bridge between those two and that's the whole point of Knowledge X.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Action, so basically what we do is we try to deploy scalable knowledge extraction algorithms, dialogues to bridge between this unstructured data and is structured data and the classical knowledge extraction approach contains 3 parts.",
                    "label": 1
                },
                {
                    "sent": "You have named entity recognition where you have these text that comes in you want to find the entities in there.",
                    "label": 0
                },
                {
                    "sent": "Then you want to link these entities to a reference knowledge base that's basically entity linking.",
                    "label": 0
                },
                {
                    "sent": "They're all named entities disambiguation.",
                    "label": 0
                },
                {
                    "sent": "That's what we're going to be concerned with in this talk.",
                    "label": 0
                },
                {
                    "sent": "And then usually do some form of.",
                    "label": 0
                },
                {
                    "sent": "Relation extraction.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now if you look at the literature pertaining to an image disambiguation, we basically have a lot of voters pointing to the fact that we still perform quite poorly on web documents and that the approaches that we have actually tend not to scare.",
                    "label": 0
                },
                {
                    "sent": "They tend to be quite complex, some of them even going into exponential runtimes, and yet they also partly difficult to port to other languages, and those are actually the problems that we wanted to tackle.",
                    "label": 1
                },
                {
                    "sent": "So what we wanted to do was we wanted to design an accurate knowledge base agnostic approach that had a polynomial time complexity.",
                    "label": 1
                },
                {
                    "sent": "And was easy to port to other languages.",
                    "label": 0
                },
                {
                    "sent": "So we wanted an approach that is simple and works well.",
                    "label": 0
                },
                {
                    "sent": "That's basically what we were after.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that is what artist this is all about.",
                    "label": 0
                },
                {
                    "sent": "It is quite a simple approach, but as I will show, it actually performs quite well and we can easily put it to other languages.",
                    "label": 0
                },
                {
                    "sent": "The idea behind artist is actually straightforward.",
                    "label": 0
                },
                {
                    "sent": "We assume that given some text and we have some form of named entity recognition tool that gives us the labels that were interested in the first thing that we do is actually choosing candidates.",
                    "label": 0
                },
                {
                    "sent": "And we do that in a greedy man and explain what greedy is in the second or what I mean by greedy.",
                    "label": 0
                },
                {
                    "sent": "Once we have a bunch of candidates, what we then do is we do a BFS with breadth first search within the reference craft that are interested in and we simply employ the hits algorithm.",
                    "label": 0
                },
                {
                    "sent": "Then to determine the nodes for each candidate for each label.",
                    "label": 0
                },
                {
                    "sent": "Basically, they have the highest authority, and those are the ones that we give back, so it's as straightforward as that, and as you all know that this approach should have approximately a quadratic time complexity, so event squared.",
                    "label": 0
                },
                {
                    "sent": "That's actually what.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Draft.",
                    "label": 0
                },
                {
                    "sent": "So one example used to sentence Barack Obama arrived this afternoon in Washington DC.",
                    "label": 1
                },
                {
                    "sent": "So we imagine this is our input.",
                    "label": 0
                },
                {
                    "sent": "The first thing that we have to do is give it to a named entity recognition tool.",
                    "label": 0
                },
                {
                    "sent": "So by default we use the Fox framework which is going to be talked about doing the last talk in this session.",
                    "label": 0
                },
                {
                    "sent": "So stay here and we've got to get all the details.",
                    "label": 0
                },
                {
                    "sent": "But the reason why we use it to simply be cause it outperforms the other state of the art tools respect to F measure.",
                    "label": 0
                },
                {
                    "sent": "So basically default wise we use this.",
                    "label": 0
                },
                {
                    "sent": "And what?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Else would do it would it would tell us Barack Obama is an entity in Washington DC is an entity now give me the ur.",
                    "label": 0
                },
                {
                    "sent": "I swear these guys and throughout the examples are used.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Arise.",
                    "label": 0
                },
                {
                    "sent": "So as I said, the candidate generation approaches greedy, so we assume you're giving a set of labels and not supposed to return a set of candidate research resources for each label.",
                    "label": 0
                },
                {
                    "sent": "We call our approach greedy simply because we take everything that we can get to actually generate these candidates so that there's been a paper on labels on the web of data that basically listed a long list of possible predicates someone can use.",
                    "label": 0
                },
                {
                    "sent": "For that, we can use those, but we can also use surface forms and the like that are generated.",
                    "label": 0
                },
                {
                    "sent": "By order, tools such as spotlight, for example, we take all these lists so we have a bunch of strings.",
                    "label": 0
                },
                {
                    "sent": "We know how they map the resources and we basically say if a string.",
                    "label": 0
                },
                {
                    "sent": "So basically label Maps to the label that we have that came from the named entity Recognition tool with the similarity larger than Tera, then we assume that this is a candidate is as simple as that.",
                    "label": 0
                },
                {
                    "sent": "So if you go back to example with Barack Obama arrived this afternoon in Washington DC, while we would get would be at least of candidates, we get something like Barack Obama, the president, who will get his father.",
                    "label": 1
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "And for Washington DC, we get things as diverse as novels.",
                    "label": 0
                },
                {
                    "sent": "And as the city of Washington, DC.",
                    "label": 0
                },
                {
                    "sent": "So the question now is, how do we pick the right?",
                    "label": 0
                },
                {
                    "sent": "You are out of this bunch.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the approach here as well, is very straightforward.",
                    "label": 0
                },
                {
                    "sent": "What we do is we take each of these candidates and we basically expand it through a BFS.",
                    "label": 0
                },
                {
                    "sent": "Three breadth first search.",
                    "label": 0
                },
                {
                    "sent": "And once we've expanded the conjugate, that is all of the conjugate, we run the hits algorithm and simply choose the candidate that has the high authority across these different nodes so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we go to our example, we have these nodes as input.",
                    "label": 0
                },
                {
                    "sent": "So we have Barack Obama senior and Barack Obama, Ralph Washington DC and the DC novel.",
                    "label": 0
                },
                {
                    "sent": "If you want to grab breakfast search of depth, one will basically just extend Washington DC and say OK is connected.",
                    "label": 0
                },
                {
                    "sent": "Several federal district San White House.",
                    "label": 0
                },
                {
                    "sent": "Barack Obama will be connected with Hawaiian.",
                    "label": 0
                },
                {
                    "sent": "Had done nothing that his mother and the novel will go to the United Kingdom, go down and so on.",
                    "label": 0
                },
                {
                    "sent": "So for this example we have a depth of two and we basically get this graph on this graph.",
                    "label": 0
                },
                {
                    "sent": "We then run the hits algorithm with the standard initialization.",
                    "label": 0
                },
                {
                    "sent": "That is the hope school and authoritie.",
                    "label": 0
                },
                {
                    "sent": "For us at one, we iterate until we get a convergence.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are the scores that we get.",
                    "label": 0
                },
                {
                    "sent": "For this example, we basically get Barack Obama.",
                    "label": 0
                },
                {
                    "sent": "That is this guy.",
                    "label": 0
                },
                {
                    "sent": "With the highest authorities costed two and Washington DC, that is basically a photo of that with that particular authority score so we can see these are the two ur eyes that are most probably important or.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Valid for the input that we received.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "That's that simple.",
                    "label": 0
                },
                {
                    "sent": "The question is, does it work?",
                    "label": 0
                },
                {
                    "sent": "Cause if it works, basically would have approaches we can throw in really large documents and actually get your eyes.",
                    "label": 0
                },
                {
                    "sent": "We need time that makes sense.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to measure the accuracy of activities on different languages.",
                    "label": 1
                },
                {
                    "sent": "'cause we also said we want to put it in different languages and as baseline we use state of the art frameworks such as Ida Spotlight and tag Me Too.",
                    "label": 0
                },
                {
                    "sent": "Those are basically frameworks that are widely used or basically came up as winners in other competitions such as the depth of that people of 2013 by Colonel Chad R and as evaluation measure.",
                    "label": 1
                },
                {
                    "sent": "Reduce the micro F measure simply because district are strict under Microsoft measure and we used nine different datasets in three languages.",
                    "label": 0
                },
                {
                    "sent": "That is 7 times English, Chinese and German.",
                    "label": 0
                },
                {
                    "sent": "And we had default settings, obviously for experiments, and that was the depth of two and enter value of 0.82.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we came about with these values simply by measuring how our system behaved on a different data sets.",
                    "label": 0
                },
                {
                    "sent": "So this basically what this picture shows on the X axis you have the different values of Terra and you have the F measures for the different depths.",
                    "label": 0
                },
                {
                    "sent": "This is basically we do not use any black squares is when we do not use any expansion.",
                    "label": 0
                },
                {
                    "sent": "So any breakfast search and what we see is that across the different data sets the best setter value is between 0809.",
                    "label": 0
                },
                {
                    "sent": "Actually pinpoint it to zero point.",
                    "label": 0
                },
                {
                    "sent": "8 two, so that's the value we used and equals 2 works best across all experiments.",
                    "label": 1
                },
                {
                    "sent": "So that's what we used as.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our first series of experiments, we looked at English datasets and redeployed activities on two different knowledge bases just to see whether it actually works on different knowledge bases.",
                    "label": 0
                },
                {
                    "sent": "As we promised at the beginning of the talk and what we saw is that it works well on the PG and Jago, but it works best on TPG.",
                    "label": 0
                },
                {
                    "sent": "We simply have to do with the topology of the knowledge base so when we do the expansion we actually get relevant information for disambiguation.",
                    "label": 0
                },
                {
                    "sent": "We compared in this case with.",
                    "label": 0
                },
                {
                    "sent": "Spotlight and Ida Ida works on Jago.",
                    "label": 0
                },
                {
                    "sent": "Spotlight works on the pedia and what we saw restart on two of the datasets.",
                    "label": 0
                },
                {
                    "sent": "That is, the Reuters data set on the RS500 data set.",
                    "label": 0
                },
                {
                    "sent": "We outperformed these two frameworks, the third data set, the Iyago data set is basically the data set on which idea was trained and their idols perform better than we do.",
                    "label": 0
                },
                {
                    "sent": "So we thought this is nice.",
                    "label": 0
                },
                {
                    "sent": "These are nice results, but.",
                    "label": 0
                },
                {
                    "sent": "Are there any benchmarks or basically benchmark datasets where we can run this approach and we did so?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we looked at the benchmark datasets from the framework.",
                    "label": 0
                },
                {
                    "sent": "Those are from the... 2013 paper by chronology at all, and we run again against the best systems there.",
                    "label": 0
                },
                {
                    "sent": "That is, taking two and we feature spotlight.",
                    "label": 0
                },
                {
                    "sent": "And we basically saw something very interesting.",
                    "label": 0
                },
                {
                    "sent": "First of all, on three of the four datasets we do or perform this state of the art.",
                    "label": 0
                },
                {
                    "sent": "So basically, as you can see here, the bold values are basically the best values.",
                    "label": 0
                },
                {
                    "sent": "That was simply due to us having on our four data sets the best precision.",
                    "label": 0
                },
                {
                    "sent": "So basically our main problem was recall, especially on these data set where we had quite conforming code and that simply has to do with the data value that we chose.",
                    "label": 0
                },
                {
                    "sent": "So because we have such a high tech to value the candidates is.",
                    "label": 0
                },
                {
                    "sent": "Are most probably across the candidates that we choose sort of right candidate, but the problem is sometimes we just chop off too many possible candidates and the recall then since so whenever we get these guys right guy, then we usually gets it right.",
                    "label": 0
                },
                {
                    "sent": "But in many cases we don't even have him not list of candidates simply cause that add value that we choose is so high.",
                    "label": 0
                },
                {
                    "sent": "But again, the output performs well on both datasets that we had before and in the benchmark and basically outperforms the state of the art.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we asked ourselves can report this approach to other languages and we chose Chinese number one because it's a non European language and #2 simply because it's the most spoken language on the planet.",
                    "label": 0
                },
                {
                    "sent": "So we have more than one billion native speakers.",
                    "label": 0
                },
                {
                    "sent": "What we did is we took the data from the Curity four benchmarking question on stream benchmark.",
                    "label": 0
                },
                {
                    "sent": "It contains 250 questions in the training data sets and 50 questions in the test data set.",
                    "label": 1
                },
                {
                    "sent": "We took this data.",
                    "label": 0
                },
                {
                    "sent": "We had a native speaker translate.",
                    "label": 0
                },
                {
                    "sent": "The English questions into Chinese and basically mark the entities and we basically measured how well acted this performs there, and we obtain F measures between 65 and 70%.",
                    "label": 0
                },
                {
                    "sent": "So basically on this language on Chinese we are sure that we perform also quite well and obviously 'cause we are at the University of Leipzig in Germany.",
                    "label": 0
                },
                {
                    "sent": "We also.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So run the whole thing on German.",
                    "label": 0
                },
                {
                    "sent": "For this we basically crawl data from the Newsday Day, a portal and there were 53 documents and 627 named entities, and we run activities on this data set and compare it with Spotlight.",
                    "label": 1
                },
                {
                    "sent": "Here, Spotlight performed quite well.",
                    "label": 0
                },
                {
                    "sent": "We outperformed spotlight, though by 3% in F measure.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Realized during experiments was that running this whole business is quite tricky for most people.",
                    "label": 0
                },
                {
                    "sent": "It is really, really difficult to get the datasets to convert them and to get them actually into a format that you can actually use for experiments so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We decided to help the community a bit and we also provide this gerbil porthole that allows you to basically evaluate your system against the benchmark datasets that we have or even against more datasets so you can either submit a data set or submit system an all you need to do is basically give us your URL and abide by the specification of the portal and it would benchmark your service for you and send you results back.",
                    "label": 0
                },
                {
                    "sent": "So this is quite a nice tool that came out of the hole.",
                    "label": 0
                },
                {
                    "sent": "Building act is this really?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we also have a demo which is online.",
                    "label": 0
                },
                {
                    "sent": "The URL is at the bottom of the presentation.",
                    "label": 0
                },
                {
                    "sent": "If you want to try it out, it is there.",
                    "label": 0
                },
                {
                    "sent": "You can use it via code or via programmatically, basically.",
                    "label": 0
                },
                {
                    "sent": "Or you can simply try in the normal way by using the web page that we provide.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, wanted to give enough time for questions.",
                    "label": 0
                },
                {
                    "sent": "So what I did is I presented it as it really does have a polynomial time complexity, which is what we are most interested in.",
                    "label": 0
                },
                {
                    "sent": "We can basically send documents in there with thousands of entities an actually get results within a time that does make sense.",
                    "label": 0
                },
                {
                    "sent": "We showed that is it is multilingual.",
                    "label": 0
                },
                {
                    "sent": "That is you can deploy it on several different languages.",
                    "label": 0
                },
                {
                    "sent": "We show that it works well in English on German and Chinese and we're currently deploying it on other languages.",
                    "label": 1
                },
                {
                    "sent": "The approach has a high accuracy and as we showed it does outperform the state of the art on most of your datasets.",
                    "label": 0
                },
                {
                    "sent": "We had a look at what we want to do in the next steps is basically include graph summarization into their approach to basically make the breakfast search perform even better.",
                    "label": 0
                },
                {
                    "sent": "Extended to more languages as said before and maybe even combine it with other approaches that are polynomial in time complexity simply to get an approach that's even better.",
                    "label": 1
                },
                {
                    "sent": "Best your skills well.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have a demo stand 79 so please do come to the demo session and have a look at our demo.",
                    "label": 1
                },
                {
                    "sent": "Asked all the questions that you might want to ask the technical questions.",
                    "label": 0
                },
                {
                    "sent": "How do I call this thing with curve for example and so on?",
                    "label": 0
                },
                {
                    "sent": "Here are the links so the whole thing is open source.",
                    "label": 0
                },
                {
                    "sent": "Please feel free to use it.",
                    "label": 0
                },
                {
                    "sent": "Said not comments.",
                    "label": 0
                },
                {
                    "sent": "Would love to know what you think about this software and how we can improve it.",
                    "label": 0
                },
                {
                    "sent": "Give us your feedback and if you want to go to demo page it is here and once you've seen a wonderful demo please to vote for us will be delighted.",
                    "label": 0
                },
                {
                    "sent": "To have your own so thank you very much for this thing.",
                    "label": 1
                }
            ]
        }
    }
}