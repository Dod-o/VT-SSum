{
    "id": "gy7pzd5c76p7a6p7hy2o7sjtsijepxdk",
    "title": "A Maximum-Likelihood Formulation and EM Algorithm for the Protein Multiple Alignment Problem",
    "info": {
        "author": [
            "Nikolay Razin, Moscow Institute of Physics and Technology"
        ],
        "published": "Oct. 14, 2010",
        "recorded": "September 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/prib2010_razin_mfea/",
    "segmentation": [
        [
            "Good morning, let me present our strictly formal approach to the overlast, everlasting problem of multiple alignment."
        ],
        [
            "My name is Nikolai Reisen.",
            "I'm PhD student of Moscow, Institute of Physics and Technology since the 1st of September and this is my master status presented."
        ],
        [
            "Two months ago, Valentina Cinema and Virgin Professors weren't in the stream and video model are my scientific advisors."
        ],
        [
            "And the professors ileum, which Nick and consumer Krakowsky are our scientific partners."
        ],
        [
            "United States first of all, why we decided to start?"
        ],
        [
            "This work?",
            "Only a few existing multiple alignment methods.",
            "I run delayed by mathematically strict.",
            "Formalism so."
        ],
        [
            "But all these methods are computationally too hard."
        ],
        [
            "On the other hand, are fast heuristic algorithms.",
            "List relevant from the biological."
        ],
        [
            "Point of view.",
            "So what's the main idea of our?"
        ],
        [
            "Approach the proceeds from the famous Margaret Deck of Pam Model.",
            "This model was presented by Margaret Decker in the last century and our idea is deliberately straightforward, maybe native generalization of her model want the case of amino acid sequence."
        ],
        [
            "This.",
            "The amino acid sequences.",
            "To be aligned are treated as a result of independent random random insertions substitutions applied to random hidden ancestors of the same preset smaller lens."
        ],
        [
            "The immediate goal of our analysis is not the final multiple alignment, but estimating the common profile, which is.",
            "Is a sequence of independent probabilistic."
        ],
        [
            "Solutions, but alone with a common problem.",
            "Probabilistic profile our algorithm yields.",
            "A posterior distribution over the set of all multiple alignments between the sequences, so the final alignment would be.",
            "The final result will be the most probable aligned."
        ],
        [
            "Before I will explain our approach in details, let me briefly remind you that Margaret Day hopes."
        ],
        [
            "Mom model.",
            "Let a big A and a note, then alphabet of amino acids."
        ],
        [
            "So Margaret dayhoff works up a quantitative measure of similarity between amino acids as predisposition predispositions towards mutative."
        ],
        [
            "Information, so she proposed to consider the mutative process as a Markov chain.",
            "Which is represented by its matrix of conditional transition probabilities."
        ],
        [
            "And she in doubt this.",
            "Markov process these two properties, the Organicity and the reversibility."
        ],
        [
            "So further will use the following notations.",
            "The Bolt Omega will denote the amino acid sequence of the lens capital and Omega and the lower case and will stand for the number of columns in common profile or in the other words.",
            "Or another terminology.",
            "The order of alignment.",
            "Also, we must note that will will distinguish the set of all amino acids with length less than and lower case, and the set of amino acids with fixed length."
        ],
        [
            "So.",
            "Now I'll describe the hypothesis underlying our approach.",
            "So the first hypothesis is.",
            "It isn't.",
            "Is that each sequence has evolved from a specific ancestor of of the lens, lower case, and so the.",
            "Via the symbol fight we denote the distribution the conditional distribution over such over such a aminoacids this condition 2.",
            "Concrete."
        ],
        [
            "And sister this second hypothesis.",
            "The second hypothesis is that.",
            "All the hidden ancestor.",
            "Randomly generated in the same manner, namely, according to the independent probability distributions beta.",
            "Which are good?",
            "Visscher contain from which the common ancestor."
        ],
        [
            "Consists so.",
            "So let PN be the respective visa respective parametric distribution family."
        ],
        [
            "Under who is the first intermediate goal of the analysis?",
            "You can see it on this thread is that we.",
            "This is real estimating the common common profile beta of the press."
        ],
        [
            "Glance.",
            "But our algorithm.",
            "Yields the probabilistic and the final multiple alignment.",
            "Will be then the combination of individual pairwise alignments of the given sequences with the farm."
        ],
        [
            "Oh no, no.",
            "I'll explain the above mentioned random non compressing transformation of."
        ],
        [
            "The ancestor it consists of free."
        ],
        [
            "Regions, the first constituents is random structure of the transformation, which is unilateral alignment of the ancestor to the resulting sequence.",
            "You can see it in the picture and let's the symbol QN will stand for the family of distributions of this random structures."
        ],
        [
            "The second constituents.",
            "Is like this since we have them.",
            "Unilateral alignment it cuts from the original.",
            "This is it.",
            "I mean, I said sequence.",
            "How?",
            "Several positions, so will this positions form the subsequence and visual calls this subsequence as key subsequence?",
            "So the amino acids in this positions will be generated according to the classical Margaret deck of Pam model.",
            "So you can see the probability distribution on the right part."
        ],
        [
            "Or the slide?",
            "And the third constituent that the rest positions are.",
            "Are filled by randomly generating from the alphabet of."
        ],
        [
            "Any noises?",
            "All in all, we have the results in parametric conditional distribution family over single protein in terms of the unknown common probabilistic profile."
        ],
        [
            "So will probably will solve this problem by the maximum likelihood principle."
        ],
        [
            "So the likelihood the likelihood function, maybe Rd, maybe reason as the product of independent densities of probabilities dependent on the common."
        ],
        [
            "File beta.",
            "Here is the respective estimate.",
            "For the common profile beta, and then we'll use the well known expectation maximization procedure to solve this optimization task."
        ],
        [
            "The main essence of the EM procedure is the.",
            "Exploiting that fact that the given set of proteins is only one part of two component random process.",
            "It's the part is Omega and the hidden parts of this process is the collection of sequence specific transformation structures which is denoted as epsilon on this."
        ],
        [
            "This is a picture for you too.",
            "To clearly understand the structure of transformation."
        ],
        [
            "And the let's be to S and of course, EM algorithm is an iterative procedure, so let beta SB approximation to the solution at Step S. Then a posterior probabilities of."
        ],
        [
            "Of the events that I indicated in the frame are complete."
        ],
        [
            "Defined so the EM procedure boils down to independent computing of each column of common profile.",
            "It is.",
            "Strictly written on the."
        ],
        [
            "Select and it can be proved that the session iteration procedure just stands to increase the probability function until the maximum point will be reached will be achieved."
        ],
        [
            "As I said, the lens of the common profile must be preset.",
            "Before the algorithm, before the algorithm starts, so we have to choose it.",
            "Children's algorithm and the criteria of choosing is written on the slide.",
            "It's the minimum.",
            "Entropy is the minimum entropy during all columns."
        ],
        [
            "Also, the most probable multiple alignment can be found."
        ],
        [
            "Via.",
            "Well known dynamic programming."
        ],
        [
            "Principle, it's it's standard dynamic programming problem, so it can be found with an error linear."
        ],
        [
            "Complexity, so I'll tell you about the experiments.",
            "We used the Valley based valid database for our."
        ],
        [
            "Experiments and it's we.",
            "I must mention that.",
            "There is a difference essential difference between classical structure of alignment."
        ],
        [
            "The the alignment in this paper werlein only ungrouped columns."
        ],
        [
            "Open.",
            "Between the amino acid sequences.",
            "So for prediction, prediction accuracy assessment are the standard metrics.",
            "Some of their score and total."
        ],
        [
            "Local score.",
            "Is the word internalize the entire table?"
        ],
        [
            "Of the experimental results, but I'll I want you to pay attention to the to this summary of the experiments.",
            "The proposed approach in average increases as a methods, but it doesn't yield the best result.",
            "In all cases we have tried to test, but as a rule it decreases slightly when it loses to some other methods.",
            "And it increases high when it wins."
        ],
        [
            "So here you can see some conclusions of this paper and thank you for your attention.",
            "OK. How much the accuracy of the alignment of the approach depends on the number of gaps.",
            "Dependent of gaps."
        ],
        [
            "So yeah, all this algorithm is doing is trying to automatically depending the optimal length of the optimal order of the alignment and the number of groups between which you can see on this picture between the parts of the common ancestor.",
            "Depends on the amino acid sequences.",
            "That's we didn't.",
            "We did not.",
            "Experimentally.",
            "If you did not try to.",
            "Experimentally.",
            "Make this dependency 22.",
            "To study this dependency to build this, yes yes."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good morning, let me present our strictly formal approach to the overlast, everlasting problem of multiple alignment.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Nikolai Reisen.",
                    "label": 0
                },
                {
                    "sent": "I'm PhD student of Moscow, Institute of Physics and Technology since the 1st of September and this is my master status presented.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two months ago, Valentina Cinema and Virgin Professors weren't in the stream and video model are my scientific advisors.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the professors ileum, which Nick and consumer Krakowsky are our scientific partners.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "United States first of all, why we decided to start?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This work?",
                    "label": 0
                },
                {
                    "sent": "Only a few existing multiple alignment methods.",
                    "label": 1
                },
                {
                    "sent": "I run delayed by mathematically strict.",
                    "label": 0
                },
                {
                    "sent": "Formalism so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But all these methods are computationally too hard.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the other hand, are fast heuristic algorithms.",
                    "label": 0
                },
                {
                    "sent": "List relevant from the biological.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Point of view.",
                    "label": 0
                },
                {
                    "sent": "So what's the main idea of our?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approach the proceeds from the famous Margaret Deck of Pam Model.",
                    "label": 0
                },
                {
                    "sent": "This model was presented by Margaret Decker in the last century and our idea is deliberately straightforward, maybe native generalization of her model want the case of amino acid sequence.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "The amino acid sequences.",
                    "label": 0
                },
                {
                    "sent": "To be aligned are treated as a result of independent random random insertions substitutions applied to random hidden ancestors of the same preset smaller lens.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The immediate goal of our analysis is not the final multiple alignment, but estimating the common profile, which is.",
                    "label": 0
                },
                {
                    "sent": "Is a sequence of independent probabilistic.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solutions, but alone with a common problem.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic profile our algorithm yields.",
                    "label": 0
                },
                {
                    "sent": "A posterior distribution over the set of all multiple alignments between the sequences, so the final alignment would be.",
                    "label": 0
                },
                {
                    "sent": "The final result will be the most probable aligned.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before I will explain our approach in details, let me briefly remind you that Margaret Day hopes.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mom model.",
                    "label": 0
                },
                {
                    "sent": "Let a big A and a note, then alphabet of amino acids.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Margaret dayhoff works up a quantitative measure of similarity between amino acids as predisposition predispositions towards mutative.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Information, so she proposed to consider the mutative process as a Markov chain.",
                    "label": 0
                },
                {
                    "sent": "Which is represented by its matrix of conditional transition probabilities.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And she in doubt this.",
                    "label": 0
                },
                {
                    "sent": "Markov process these two properties, the Organicity and the reversibility.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So further will use the following notations.",
                    "label": 0
                },
                {
                    "sent": "The Bolt Omega will denote the amino acid sequence of the lens capital and Omega and the lower case and will stand for the number of columns in common profile or in the other words.",
                    "label": 1
                },
                {
                    "sent": "Or another terminology.",
                    "label": 0
                },
                {
                    "sent": "The order of alignment.",
                    "label": 0
                },
                {
                    "sent": "Also, we must note that will will distinguish the set of all amino acids with length less than and lower case, and the set of amino acids with fixed length.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now I'll describe the hypothesis underlying our approach.",
                    "label": 0
                },
                {
                    "sent": "So the first hypothesis is.",
                    "label": 0
                },
                {
                    "sent": "It isn't.",
                    "label": 0
                },
                {
                    "sent": "Is that each sequence has evolved from a specific ancestor of of the lens, lower case, and so the.",
                    "label": 1
                },
                {
                    "sent": "Via the symbol fight we denote the distribution the conditional distribution over such over such a aminoacids this condition 2.",
                    "label": 0
                },
                {
                    "sent": "Concrete.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And sister this second hypothesis.",
                    "label": 0
                },
                {
                    "sent": "The second hypothesis is that.",
                    "label": 0
                },
                {
                    "sent": "All the hidden ancestor.",
                    "label": 0
                },
                {
                    "sent": "Randomly generated in the same manner, namely, according to the independent probability distributions beta.",
                    "label": 0
                },
                {
                    "sent": "Which are good?",
                    "label": 0
                },
                {
                    "sent": "Visscher contain from which the common ancestor.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consists so.",
                    "label": 0
                },
                {
                    "sent": "So let PN be the respective visa respective parametric distribution family.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Under who is the first intermediate goal of the analysis?",
                    "label": 1
                },
                {
                    "sent": "You can see it on this thread is that we.",
                    "label": 1
                },
                {
                    "sent": "This is real estimating the common common profile beta of the press.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Glance.",
                    "label": 0
                },
                {
                    "sent": "But our algorithm.",
                    "label": 0
                },
                {
                    "sent": "Yields the probabilistic and the final multiple alignment.",
                    "label": 1
                },
                {
                    "sent": "Will be then the combination of individual pairwise alignments of the given sequences with the farm.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh no, no.",
                    "label": 0
                },
                {
                    "sent": "I'll explain the above mentioned random non compressing transformation of.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The ancestor it consists of free.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regions, the first constituents is random structure of the transformation, which is unilateral alignment of the ancestor to the resulting sequence.",
                    "label": 0
                },
                {
                    "sent": "You can see it in the picture and let's the symbol QN will stand for the family of distributions of this random structures.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second constituents.",
                    "label": 0
                },
                {
                    "sent": "Is like this since we have them.",
                    "label": 0
                },
                {
                    "sent": "Unilateral alignment it cuts from the original.",
                    "label": 0
                },
                {
                    "sent": "This is it.",
                    "label": 0
                },
                {
                    "sent": "I mean, I said sequence.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "Several positions, so will this positions form the subsequence and visual calls this subsequence as key subsequence?",
                    "label": 0
                },
                {
                    "sent": "So the amino acids in this positions will be generated according to the classical Margaret deck of Pam model.",
                    "label": 0
                },
                {
                    "sent": "So you can see the probability distribution on the right part.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or the slide?",
                    "label": 0
                },
                {
                    "sent": "And the third constituent that the rest positions are.",
                    "label": 0
                },
                {
                    "sent": "Are filled by randomly generating from the alphabet of.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any noises?",
                    "label": 0
                },
                {
                    "sent": "All in all, we have the results in parametric conditional distribution family over single protein in terms of the unknown common probabilistic profile.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So will probably will solve this problem by the maximum likelihood principle.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the likelihood the likelihood function, maybe Rd, maybe reason as the product of independent densities of probabilities dependent on the common.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "File beta.",
                    "label": 0
                },
                {
                    "sent": "Here is the respective estimate.",
                    "label": 0
                },
                {
                    "sent": "For the common profile beta, and then we'll use the well known expectation maximization procedure to solve this optimization task.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The main essence of the EM procedure is the.",
                    "label": 1
                },
                {
                    "sent": "Exploiting that fact that the given set of proteins is only one part of two component random process.",
                    "label": 1
                },
                {
                    "sent": "It's the part is Omega and the hidden parts of this process is the collection of sequence specific transformation structures which is denoted as epsilon on this.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a picture for you too.",
                    "label": 0
                },
                {
                    "sent": "To clearly understand the structure of transformation.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the let's be to S and of course, EM algorithm is an iterative procedure, so let beta SB approximation to the solution at Step S. Then a posterior probabilities of.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the events that I indicated in the frame are complete.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Defined so the EM procedure boils down to independent computing of each column of common profile.",
                    "label": 1
                },
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "Strictly written on the.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Select and it can be proved that the session iteration procedure just stands to increase the probability function until the maximum point will be reached will be achieved.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I said, the lens of the common profile must be preset.",
                    "label": 1
                },
                {
                    "sent": "Before the algorithm, before the algorithm starts, so we have to choose it.",
                    "label": 0
                },
                {
                    "sent": "Children's algorithm and the criteria of choosing is written on the slide.",
                    "label": 0
                },
                {
                    "sent": "It's the minimum.",
                    "label": 1
                },
                {
                    "sent": "Entropy is the minimum entropy during all columns.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, the most probable multiple alignment can be found.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Via.",
                    "label": 0
                },
                {
                    "sent": "Well known dynamic programming.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Principle, it's it's standard dynamic programming problem, so it can be found with an error linear.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Complexity, so I'll tell you about the experiments.",
                    "label": 0
                },
                {
                    "sent": "We used the Valley based valid database for our.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experiments and it's we.",
                    "label": 0
                },
                {
                    "sent": "I must mention that.",
                    "label": 0
                },
                {
                    "sent": "There is a difference essential difference between classical structure of alignment.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The the alignment in this paper werlein only ungrouped columns.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Open.",
                    "label": 0
                },
                {
                    "sent": "Between the amino acid sequences.",
                    "label": 0
                },
                {
                    "sent": "So for prediction, prediction accuracy assessment are the standard metrics.",
                    "label": 1
                },
                {
                    "sent": "Some of their score and total.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Local score.",
                    "label": 0
                },
                {
                    "sent": "Is the word internalize the entire table?",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of the experimental results, but I'll I want you to pay attention to the to this summary of the experiments.",
                    "label": 0
                },
                {
                    "sent": "The proposed approach in average increases as a methods, but it doesn't yield the best result.",
                    "label": 1
                },
                {
                    "sent": "In all cases we have tried to test, but as a rule it decreases slightly when it loses to some other methods.",
                    "label": 0
                },
                {
                    "sent": "And it increases high when it wins.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here you can see some conclusions of this paper and thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "OK. How much the accuracy of the alignment of the approach depends on the number of gaps.",
                    "label": 0
                },
                {
                    "sent": "Dependent of gaps.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, all this algorithm is doing is trying to automatically depending the optimal length of the optimal order of the alignment and the number of groups between which you can see on this picture between the parts of the common ancestor.",
                    "label": 0
                },
                {
                    "sent": "Depends on the amino acid sequences.",
                    "label": 0
                },
                {
                    "sent": "That's we didn't.",
                    "label": 0
                },
                {
                    "sent": "We did not.",
                    "label": 0
                },
                {
                    "sent": "Experimentally.",
                    "label": 0
                },
                {
                    "sent": "If you did not try to.",
                    "label": 0
                },
                {
                    "sent": "Experimentally.",
                    "label": 0
                },
                {
                    "sent": "Make this dependency 22.",
                    "label": 0
                },
                {
                    "sent": "To study this dependency to build this, yes yes.",
                    "label": 0
                }
            ]
        }
    }
}